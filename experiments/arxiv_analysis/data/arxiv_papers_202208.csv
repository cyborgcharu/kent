id,title,abstract,authors,categories,date
http://arxiv.org/abs/2209.10709v2,A data-driven interpretation of the stability of molecular crystals,"Due to the subtle balance of intermolecular interactions that govern
structure-property relations, predicting the stability of crystal structures
formed from molecular building blocks is a highly non-trivial scientific
problem. A particularly active and fruitful approach involves classifying the
different combinations of interacting chemical moieties, as understanding the
relative energetics of different interactions enables the design of molecular
crystals and fine-tuning their stabilities. While this is usually performed
based on the empirical observation of the most commonly encountered motifs in
known crystal structures, we propose to apply a combination of supervised and
unsupervised machine-learning techniques to automate the construction of an
extensive library of molecular building blocks. We introduce a structural
descriptor tailored to the prediction of the binding (lattice) energy and apply
it to a curated dataset of organic crystals and exploit its atom-centered
nature to obtain a data-driven assessment of the contribution of different
chemical groups to the lattice energy of the crystal. We then interpret this
library using a low-dimensional representation of the structure-energy
landscape and discuss selected examples of the insights into crystal
engineering that can be extracted from this analysis, providing a complete
database to guide the design of molecular materials.","['Rose K. Cersonsky', 'Maria Pakhnova', 'Edgar A. Engel', 'Michele Ceriotti']","['physics.chem-ph', 'cond-mat.mtrl-sci', 'stat.ML']",2022-09-21 23:32:53+00:00
http://arxiv.org/abs/2209.10707v3,Gaussian Process Hydrodynamics,"We present a Gaussian Process (GP) approach (Gaussian Process Hydrodynamics,
GPH) for approximating the solution of the Euler and Navier-Stokes equations.
As in Smoothed Particle Hydrodynamics (SPH), GPH is a Lagrangian particle-based
approach involving the tracking of a finite number of particles transported by
the flow. However, these particles do not represent mollified particles of
matter but carry discrete/partial information about the continuous flow.
Closure is achieved by placing a divergence-free GP prior $\xi$ on the velocity
field and conditioning on vorticity at particle locations. Known physics (e.g.,
the Richardson cascade and velocity-increments power laws) is incorporated into
the GP prior through physics-informed additive kernels. This approach allows us
to coarse-grain turbulence in a statistical manner rather than a deterministic
one. By enforcing incompressibility and fluid/structure boundary conditions
through the selection of the kernel, GPH requires much fewer particles than
SPH. Since GPH has a natural probabilistic interpretation, numerical results
come with uncertainty estimates enabling their incorporation into a UQ pipeline
and the adding/removing of particles (quantas of information) in an adapted
manner. The proposed approach is amenable to analysis, it inherits the
complexity of state-of-the-art solvers for dense kernel matrices, and it leads
to a natural definition of turbulence as information loss. Numerical
experiments support the importance of selecting physics-informed kernels and
illustrate the major impact of such kernels on accuracy and stability. Since
the proposed approach has a Bayesian interpretation, it naturally enables data
assimilation and making predictions and estimations based on mixing simulation
data with experimental data.",['Houman Owhadi'],"['physics.flu-dyn', 'cs.NA', 'math.NA', 'stat.ML', '35Q30, 76D05, 60G15, 65M75, 65N75, 65N35, 47B34, 41A15, 34B15']",2022-09-21 23:27:21+00:00
http://arxiv.org/abs/2209.10675v2,A Validation Approach to Over-parameterized Matrix and Image Recovery,"This paper studies the problem of recovering a low-rank matrix from several
noisy random linear measurements. We consider the setting where the rank of the
ground-truth matrix is unknown a priori and use an objective function built
from a rank-overspecified factored representation of the matrix variable, where
the global optimal solutions overfit and do not correspond to the underlying
ground truth. We then solve the associated nonconvex problem using gradient
descent with small random initialization. We show that as long as the
measurement operators satisfy the restricted isometry property (RIP) with its
rank parameter scaling with the rank of the ground-truth matrix rather than
scaling with the overspecified matrix rank, gradient descent iterations are on
a particular trajectory towards the ground-truth matrix and achieve nearly
information-theoretically optimal recovery when it is stopped appropriately. We
then propose an efficient stopping strategy based on the common hold-out method
and show that it detects a nearly optimal estimator provably. Moreover,
experiments show that the proposed validation approach can also be efficiently
used for image restoration with deep image prior, which over-parameterizes an
image with a deep network.","['Lijun Ding', 'Zhen Qin', 'Liwei Jiang', 'Jinxin Zhou', 'Zhihui Zhu']","['math.OC', 'cs.LG', 'eess.IV', 'stat.ML']",2022-09-21 22:01:23+00:00
http://arxiv.org/abs/2209.10666v3,Adaptive Bias Correction for Improved Subseasonal Forecasting,"Subseasonal forecasting -- predicting temperature and precipitation 2 to 6
weeks ahead -- is critical for effective water allocation, wildfire management,
and drought and flood mitigation. Recent international research efforts have
advanced the subseasonal capabilities of operational dynamical models, yet
temperature and precipitation prediction skills remain poor, partly due to
stubborn errors in representing atmospheric dynamics and physics inside
dynamical models. Here, to counter these errors, we introduce an adaptive bias
correction (ABC) method that combines state-of-the-art dynamical forecasts with
observations using machine learning. We show that, when applied to the leading
subseasonal model from the European Centre for Medium-Range Weather Forecasts
(ECMWF), ABC improves temperature forecasting skill by 60-90% (over baseline
skills of 0.18-0.25) and precipitation forecasting skill by 40-69% (over
baseline skills of 0.11-0.15) in the contiguous U.S. We couple these
performance improvements with a practical workflow to explain ABC skill gains
and identify higher-skill windows of opportunity based on specific climate
conditions.","['Soukayna Mouatadid', 'Paulo Orenstein', 'Genevieve Flaspohler', 'Judah Cohen', 'Miruna Oprescu', 'Ernest Fraenkel', 'Lester Mackey']","['cs.LG', 'physics.ao-ph', 'stat.ML']",2022-09-21 21:22:44+00:00
http://arxiv.org/abs/2209.10634v2,Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation,"Early sensory systems in the brain rapidly adapt to fluctuating input
statistics, which requires recurrent communication between neurons.
Mechanistically, such recurrent communication is often indirect and mediated by
local interneurons. In this work, we explore the computational benefits of
mediating recurrent communication via interneurons compared with direct
recurrent connections. To this end, we consider two mathematically tractable
recurrent linear neural networks that statistically whiten their inputs -- one
with direct recurrent connections and the other with interneurons that mediate
recurrent communication. By analyzing the corresponding continuous synaptic
dynamics and numerically simulating the networks, we show that the network with
interneurons is more robust to initialization than the network with direct
recurrent connections in the sense that the convergence time for the synaptic
dynamics in the network with interneurons (resp. direct recurrent connections)
scales logarithmically (resp. linearly) with the spectrum of their
initialization. Our results suggest that interneurons are computationally
useful for rapid adaptation to changing input statistics. Interestingly, the
network with interneurons is an overparameterized solution of the whitening
objective for the network with direct recurrent connections, so our results can
be viewed as a recurrent linear neural network analogue of the implicit
acceleration phenomenon observed in overparameterized feedforward linear neural
networks.","['David Lipshutz', 'Cengiz Pehlevan', 'Dmitri B. Chklovskii']","['q-bio.NC', 'cs.LG', 'cs.NE', 'stat.ML']",2022-09-21 20:03:58+00:00
http://arxiv.org/abs/2209.10584v3,Continuous Mixtures of Tractable Probabilistic Models,"Probabilistic models based on continuous latent spaces, such as variational
autoencoders, can be understood as uncountable mixture models where components
depend continuously on the latent code. They have proven to be expressive tools
for generative and probabilistic modelling, but are at odds with tractable
probabilistic inference, that is, computing marginals and conditionals of the
represented probability distribution. Meanwhile, tractable probabilistic models
such as probabilistic circuits (PCs) can be understood as hierarchical discrete
mixture models, and thus are capable of performing exact inference efficiently
but often show subpar performance in comparison to continuous latent-space
models. In this paper, we investigate a hybrid approach, namely continuous
mixtures of tractable models with a small latent dimension. While these models
are analytically intractable, they are well amenable to numerical integration
schemes based on a finite set of integration points. With a large enough number
of integration points the approximation becomes de-facto exact. Moreover, for a
finite set of integration points, the integration method effectively compiles
the continuous mixture into a standard PC. In experiments, we show that this
simple scheme proves remarkably effective, as PCs learnt this way set new state
of the art for tractable models on many standard density estimation benchmarks.","['Alvaro H. C. Correia', 'Gennaro Gala', 'Erik Quaeghebeur', 'Cassio de Campos', 'Robert Peharz']","['cs.LG', 'cs.AI', 'stat.ML']",2022-09-21 18:18:32+00:00
http://arxiv.org/abs/2209.12667v1,Shape And Structure Preserving Differential Privacy,"It is common for data structures such as images and shapes of 2D objects to
be represented as points on a manifold. The utility of a mechanism to produce
sanitized differentially private estimates from such data is intimately linked
to how compatible it is with the underlying structure and geometry of the
space. In particular, as recently shown, utility of the Laplace mechanism on a
positively curved manifold, such as Kendall's 2D shape space, is significantly
influences by the curvature. Focusing on the problem of sanitizing the
Fr\'echet mean of a sample of points on a manifold, we exploit the
characterisation of the mean as the minimizer of an objective function
comprised of the sum of squared distances and develop a K-norm gradient
mechanism on Riemannian manifolds that favors values that produce gradients
close to the the zero of the objective function. For the case of positively
curved manifolds, we describe how using the gradient of the squared distance
function offers better control over sensitivity than the Laplace mechanism, and
demonstrate this numerically on a dataset of shapes of corpus callosa. Further
illustrations of the mechanism's utility on a sphere and the manifold of
symmetric positive definite matrices are also presented.","['Carlos Soto', 'Karthik Bharath', 'Matthew Reimherr', 'Aleksandra Slavkovic']","['stat.ML', 'cs.LG', 'math.DG', 'math.ST', 'stat.TH']",2022-09-21 18:14:38+00:00
http://arxiv.org/abs/2209.10578v1,Algorithm-Agnostic Interpretations for Clustering,"A clustering outcome for high-dimensional data is typically interpreted via
post-processing, involving dimension reduction and subsequent visualization.
This destroys the meaning of the data and obfuscates interpretations. We
propose algorithm-agnostic interpretation methods to explain clustering
outcomes in reduced dimensions while preserving the integrity of the data. The
permutation feature importance for clustering represents a general framework
based on shuffling feature values and measuring changes in cluster assignments
through custom score functions. The individual conditional expectation for
clustering indicates observation-wise changes in the cluster assignment due to
changes in the data. The partial dependence for clustering evaluates average
changes in cluster assignments for the entire feature space. All methods can be
used with any clustering algorithm able to reassign instances through soft or
hard labels. In contrast to common post-processing methods such as principal
component analysis, the introduced methods maintain the original structure of
the features.","['Christian A. Scholbeck', 'Henri Funk', 'Giuseppe Casalicchio']","['cs.LG', 'stat.ML']",2022-09-21 18:08:40+00:00
http://arxiv.org/abs/2209.10516v1,Tab2vox: CNN-Based Multivariate Multilevel Demand Forecasting Framework by Tabular-To-Voxel Image Conversion,"Since demand is influenced by a wide variety of causes, it is necessary to
decompose the explana-tory variables into different levels, extract their
relationships effectively, and reflect them in the forecast. In particular,
this contextual information can be very useful in demand forecasting with large
demand volatility or intermittent demand patterns. Convolutional neural
networks (CNNs) have been successfully used in many fields where important
information in data is represented by images. CNNs are powerful because they
accept samples as images and use adjacent voxel sets to integrate
multi-dimensional important information and learn important features. On the
other hand, although the demand-forecasting model has been improved, the input
data is still limited in its tabular form and is not suitable for CNN modeling.
In this study, we propose a Tab2vox neural architecture search (NAS) model as a
method to convert a high-dimensional tabular sam-ple into a well-formed 3D
voxel image and use it in a 3D CNN network. For each image repre-sentation, the
3D CNN forecasting model proposed from the Tab2vox framework showed supe-rior
performance, compared to the existing time series and machine learning
techniques using tabular data, and the latest image transformation studies.","['Euna Lee', 'Myungwoo Nam', 'Hongchul Lee']","['stat.ML', 'cs.LG']",2022-09-21 17:29:59+00:00
http://arxiv.org/abs/2209.10512v1,Improved Marginal Unbiased Score Expansion (MUSE) via Implicit Differentiation,"We apply the technique of implicit differentiation to boost performance,
reduce numerical error, and remove required user-tuning in the Marginal
Unbiased Score Expansion (MUSE) algorithm for hierarchical Bayesian inference.
We demonstrate these improvements on three representative inference problems:
1) an extended Neal's funnel 2) Bayesian neural networks, and 3) probabilistic
principal component analysis. On our particular test cases, MUSE with implicit
differentiation is faster than Hamiltonian Monte Carlo by factors of 155, 397,
and 5, respectively, or factors of 65, 278, and 1 without implicit
differentiation, and yields good approximate marginal posteriors. The Julia and
Python MUSE packages have been updated to use implicit differentiation, and can
solve problems defined by hand or with any of a number of popular probabilistic
programming languages and automatic differentiation backends.",['Marius Millea'],"['stat.ML', 'cs.AI', 'cs.LG']",2022-09-21 17:20:20+00:00
http://arxiv.org/abs/2209.10477v1,Transition to Adulthood for Young People with Intellectual or Developmental Disabilities: Emotion Detection and Topic Modeling,"Transition to Adulthood is an essential life stage for many families. The
prior research has shown that young people with intellectual or development
disabil-ities (IDD) have more challenges than their peers. This study is to
explore how to use natural language processing (NLP) methods, especially
unsupervised machine learning, to assist psychologists to analyze emotions and
sentiments and to use topic modeling to identify common issues and challenges
that young people with IDD and their families have. Additionally, the results
were compared to those obtained from young people without IDD who were in
tran-sition to adulthood. The findings showed that NLP methods can be very
useful for psychologists to analyze emotions, conduct cross-case analysis, and
sum-marize key topics from conversational data. Our Python code is available at
https://github.com/mlaricheva/emotion_topic_modeling.","['Yan Liu', 'Maria Laricheva', 'Chiyu Zhang', 'Patrick Boutet', 'Guanyu Chen', 'Terence Tracey', 'Giuseppe Carenini', 'Richard Young']","['cs.CL', 'stat.AP', 'stat.ML']",2022-09-21 16:23:45+00:00
http://arxiv.org/abs/2209.10444v1,Off-Policy Risk Assessment in Markov Decision Processes,"Addressing such diverse ends as safety alignment with human preferences, and
the efficiency of learning, a growing line of reinforcement learning research
focuses on risk functionals that depend on the entire distribution of returns.
Recent work on \emph{off-policy risk assessment} (OPRA) for contextual bandits
introduced consistent estimators for the target policy's CDF of returns along
with finite sample guarantees that extend to (and hold simultaneously over) all
risk. In this paper, we lift OPRA to Markov decision processes (MDPs), where
importance sampling (IS) CDF estimators suffer high variance on longer
trajectories due to small effective sample size. To mitigate these problems, we
incorporate model-based estimation to develop the first doubly robust (DR)
estimator for the CDF of returns in MDPs. This estimator enjoys significantly
less variance and, when the model is well specified, achieves the Cramer-Rao
variance lower bound. Moreover, for many risk functionals, the downstream
estimates enjoy both lower bias and lower variance. Additionally, we derive the
first minimax lower bounds for off-policy CDF and risk estimation, which match
our error bounds up to a constant factor. Finally, we demonstrate the precision
of our DR CDF estimates experimentally on several different environments.","['Audrey Huang', 'Liu Leqi', 'Zachary Chase Lipton', 'Kamyar Azizzadenesheli']","['cs.LG', 'cs.AI', 'stat.ML']",2022-09-21 15:40:59+00:00
http://arxiv.org/abs/2209.10438v2,A Measure of the Complexity of Neural Representations based on Partial Information Decomposition,"In neural networks, task-relevant information is represented jointly by
groups of neurons. However, the specific way in which this mutual information
about the classification label is distributed among the individual neurons is
not well understood: While parts of it may only be obtainable from specific
single neurons, other parts are carried redundantly or synergistically by
multiple neurons. We show how Partial Information Decomposition (PID), a recent
extension of information theory, can disentangle these different contributions.
From this, we introduce the measure of ""Representational Complexity"", which
quantifies the difficulty of accessing information spread across multiple
neurons. We show how this complexity is directly computable for smaller layers.
For larger layers, we propose subsampling and coarse-graining procedures and
prove corresponding bounds on the latter. Empirically, for quantized deep
neural networks solving the MNIST and CIFAR10 tasks, we observe that
representational complexity decreases both through successive hidden layers and
over training, and compare the results to related measures. Overall, we propose
representational complexity as a principled and interpretable summary statistic
for analyzing the structure and evolution of neural representations and complex
systems in general.","['David A. Ehrlich', 'Andreas C. Schneider', 'Viola Priesemann', 'Michael Wibral', 'Abdullah Makkeh']","['cs.IT', 'cs.AI', 'cs.LG', 'math.IT', 'stat.ML', '94A15, 68T07']",2022-09-21 15:33:11+00:00
http://arxiv.org/abs/2209.10176v2,Large-Sample Properties of Non-Stationary Source Separation for Gaussian Signals,"Non-stationary source separation is a well-established branch of blind source
separation with many different methods. However, for none of these methods
large-sample results are available. To bridge this gap, we develop large-sample
theory for NSS-JD, a popular method of non-stationary source separation based
on the joint diagonalization of block-wise covariance matrices. We work under
an instantaneous linear mixing model for independent Gaussian non-stationary
source signals together with a very general set of assumptions: besides
boundedness conditions, the only assumptions we make are that the sources
exhibit finite dependency and that their variance functions differ sufficiently
to be asymptotically separable. The consistency of the unmixing estimator and
its convergence to a limiting Gaussian distribution at the standard square root
rate are shown to hold under the previous conditions. Simulation experiments
are used to verify the theoretical results and to study the impact of block
length on the separation.","['François Bachoc', 'Christoph Muehlmann', 'Klaus Nordhausen', 'Joni Virta']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2022-09-21 08:13:20+00:00
http://arxiv.org/abs/2209.10166v3,Chaotic Hedging with Iterated Integrals and Neural Networks,"In this paper, we extend the Wiener-Ito chaos decomposition to the class of
continuous semimartingales that are exponentially integrable, which includes in
particular affine and some polynomial diffusion processes. By omitting the
orthogonality in the expansion, we are able to show that every $p$-integrable
functional of the semimartingale, for $p \in [1,\infty)$, can be represented as
a sum of iterated integrals thereof. Using finitely many terms of this
expansion and (possibly random) neural networks for the integrands, whose
parameters are learned in a machine learning setting, we show that every
financial derivative can be approximated arbitrarily well in the $L^p$-sense.
In particular, for $p = 2$, we recover the optimal hedging strategy in the
sense of quadratic hedging. Moreover, since the hedging strategy of the
approximating option can be computed in closed form, we obtain an efficient
algorithm to approximately replicate any sufficiently integrable financial
derivative within short runtime.","['Ariel Neufeld', 'Philipp Schmocker']","['q-fin.MF', 'cs.LG', 'math.PR', 'q-fin.CP', 'stat.ML']",2022-09-21 07:57:07+00:00
http://arxiv.org/abs/2209.10105v1,Distributed Online Non-convex Optimization with Composite Regret,"Regret has been widely adopted as the metric of choice for evaluating the
performance of online optimization algorithms for distributed, multi-agent
systems. However, data/model variations associated with agents can
significantly impact decisions and requires consensus among agents. Moreover,
most existing works have focused on developing approaches for (either strongly
or non-strongly) convex losses, and very few results have been obtained
regarding regret bounds in distributed online optimization for general
non-convex losses. To address these two issues, we propose a novel composite
regret with a new network regret-based metric to evaluate distributed online
optimization algorithms. We concretely define static and dynamic forms of the
composite regret. By leveraging the dynamic form of our composite regret, we
develop a consensus-based online normalized gradient (CONGD) approach for
pseudo-convex losses, and it provably shows a sublinear behavior relating to a
regularity term for the path variation of the optimizer. For general non-convex
losses, we first shed light on the regret for the setting of distributed online
non-convex learning based on recent advances such that no deterministic
algorithm can achieve the sublinear regret. We then develop the distributed
online non-convex optimization with composite regret (DINOCO) without access to
the gradients, depending on an offline optimization oracle. DINOCO is shown to
achieve sublinear regret; to our knowledge, this is the first regret bound for
general distributed online non-convex learning.","['Zhanhong Jiang', 'Aditya Balu', 'Xian Yeow Lee', 'Young M. Lee', 'Chinmay Hegde', 'Soumik Sarkar']","['cs.LG', 'cs.DC', 'stat.ML']",2022-09-21 04:16:33+00:00
http://arxiv.org/abs/2209.10093v1,Projected Gradient Descent Algorithms for Solving Nonlinear Inverse Problems with Generative Priors,"In this paper, we propose projected gradient descent (PGD) algorithms for
signal estimation from noisy nonlinear measurements. We assume that the unknown
$p$-dimensional signal lies near the range of an $L$-Lipschitz continuous
generative model with bounded $k$-dimensional inputs. In particular, we
consider two cases when the nonlinear link function is either unknown or known.
For unknown nonlinearity, similarly to \cite{liu2020generalized}, we make the
assumption of sub-Gaussian observations and propose a linear least-squares
estimator. We show that when there is no representation error and the sensing
vectors are Gaussian, roughly $O(k \log L)$ samples suffice to ensure that a
PGD algorithm converges linearly to a point achieving the optimal statistical
rate using arbitrary initialization. For known nonlinearity, we assume
monotonicity as in \cite{yang2016sparse}, and make much weaker assumptions on
the sensing vectors and allow for representation error. We propose a nonlinear
least-squares estimator that is guaranteed to enjoy an optimal statistical
rate. A corresponding PGD algorithm is provided and is shown to also converge
linearly to the estimator using arbitrary initialization. In addition, we
present experimental results on image datasets to demonstrate the performance
of our PGD algorithms.","['Zhaoqiang Liu', 'Jun Han']","['stat.ML', 'cs.LG']",2022-09-21 04:05:12+00:00
http://arxiv.org/abs/2209.10091v1,Variational Inference for Infinitely Deep Neural Networks,"We introduce the unbounded depth neural network (UDN), an infinitely deep
probabilistic model that adapts its complexity to the training data. The UDN
contains an infinite sequence of hidden layers and places an unbounded prior on
a truncation L, the layer from which it produces its data. Given a dataset of
observations, the posterior UDN provides a conditional distribution of both the
parameters of the infinite neural network and its truncation. We develop a
novel variational inference algorithm to approximate this posterior, optimizing
a distribution of the neural network weights and of the truncation depth L, and
without any upper limit on L. To this end, the variational family has a special
structure: it models neural network weights of arbitrary depth, and it
dynamically creates or removes free variational parameters as its distribution
of the truncation is optimized. (Unlike heuristic approaches to model search,
it is solely through gradient-based optimization that this algorithm explores
the space of truncations.) We study the UDN on real and synthetic data. We find
that the UDN adapts its posterior depth to the dataset complexity; it
outperforms standard neural networks of similar computational complexity; and
it outperforms other approaches to infinite-depth neural networks.","['Achille Nazaret', 'David Blei']","['cs.LG', 'stat.ML']",2022-09-21 03:54:34+00:00
http://arxiv.org/abs/2209.10080v4,Deep Double Descent via Smooth Interpolation,"The ability of overparameterized deep networks to interpolate noisy data,
while at the same time showing good generalization performance, has been
recently characterized in terms of the double descent curve for the test error.
Common intuition from polynomial regression suggests that overparameterized
networks are able to sharply interpolate noisy data, without considerably
deviating from the ground-truth signal, thus preserving generalization ability.
At present, a precise characterization of the relationship between
interpolation and generalization for deep networks is missing. In this work, we
quantify sharpness of fit of the training data interpolated by neural network
functions, by studying the loss landscape w.r.t. to the input variable locally
to each training point, over volumes around cleanly- and noisily-labelled
training samples, as we systematically increase the number of model parameters
and training epochs. Our findings show that loss sharpness in the input space
follows both model- and epoch-wise double descent, with worse peaks observed
around noisy labels. While small interpolating models sharply fit both clean
and noisy data, large interpolating models express a smooth loss landscape,
where noisy targets are predicted over large volumes around training data
points, in contrast to existing intuition.","['Matteo Gamba', 'Erik Englesson', 'Mårten Björkman', 'Hossein Azizpour']","['cs.LG', 'stat.ML']",2022-09-21 02:46:13+00:00
http://arxiv.org/abs/2209.10064v2,Off-Policy Evaluation for Episodic Partially Observable Markov Decision Processes under Non-Parametric Models,"We study the problem of off-policy evaluation (OPE) for episodic Partially
Observable Markov Decision Processes (POMDPs) with continuous states. Motivated
by the recently proposed proximal causal inference framework, we develop a
non-parametric identification result for estimating the policy value via a
sequence of so-called V-bridge functions with the help of time-dependent proxy
variables. We then develop a fitted-Q-evaluation-type algorithm to estimate
V-bridge functions recursively, where a non-parametric instrumental variable
(NPIV) problem is solved at each step. By analyzing this challenging sequential
NPIV problem, we establish the finite-sample error bounds for estimating the
V-bridge functions and accordingly that for evaluating the policy value, in
terms of the sample size, length of horizon and so-called (local) measure of
ill-posedness at each step. To the best of our knowledge, this is the first
finite-sample error bound for OPE in POMDPs under non-parametric models.","['Rui Miao', 'Zhengling Qi', 'Xiaoke Zhang']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-09-21 01:44:45+00:00
http://arxiv.org/abs/2209.10058v1,Mutual Information Learned Classifiers: an Information-theoretic Viewpoint of Training Deep Learning Classification Systems,"Deep learning systems have been reported to achieve state-of-the-art
performances in many applications, and a key is the existence of well trained
classifiers on benchmark datasets. As a main-stream loss function, the cross
entropy can easily lead us to find models which demonstrate severe overfitting
behavior. In this paper, we show that the existing cross entropy loss
minimization problem essentially learns the label conditional entropy (CE) of
the underlying data distribution of the dataset. However, the CE learned in
this way does not characterize well the information shared by the label and the
input. In this paper, we propose a mutual information learning framework where
we train deep neural network classifiers via learning the mutual information
between the label and the input. Theoretically, we give the population
classification error lower bound in terms of the mutual information. In
addition, we derive the mutual information lower and upper bounds for a
concrete binary classification data model in $\mathbb{R}^n$, and also the error
probability lower bound in this scenario. Empirically, we conduct extensive
experiments on several benchmark datasets to support our theory. The mutual
information learned classifiers (MILCs) achieve far better generalization
performances than the conditional entropy learned classifiers (CELCs) with an
improvement which can exceed more than 10\% in testing accuracy.","['Jirong Yi', 'Qiaosheng Zhang', 'Zhen Chen', 'Qiao Liu', 'Wei Shao']","['cs.LG', 'cs.CV', 'cs.IT', 'math.IT', 'stat.ML']",2022-09-21 01:06:30+00:00
http://arxiv.org/abs/2209.10053v6,Instance-dependent uniform tail bounds for empirical processes,"We formulate a uniform tail bound for empirical processes indexed by a class
of functions, in terms of the individual deviations of the functions rather
than the worst-case deviation in the considered class. The tail bound is
established by introducing an initial ``deflation'' step to the standard
generic chaining argument. The resulting tail bound is the sum of the
complexity of the ``deflated function class'' in terms of a generalization of
Talagrand's $\gamma$ functional, and the deviation of the function instance,
both of which are formulated based on the natural seminorm induced by the
corresponding Cram\'{e}r functions. Leveraging another less demanding natural
seminorm, we also show similar bounds, though with implicit dependence on the
sample size, in the more general case where finite exponential moments cannot
be assumed. We also provide approximations of the tail bounds in terms of the
more prevalent Orlicz norms or their ``incomplete'' versions under suitable
moment conditions.",['Sohail Bahmani'],"['math.PR', 'math.ST', 'stat.ML', 'stat.TH']",2022-09-21 00:44:20+00:00
http://arxiv.org/abs/2209.09963v1,Learning Acceptance Regions for Many Classes with Anomaly Detection,"Set-valued classification, a new classification paradigm that aims to
identify all the plausible classes that an observation belongs to, can be
obtained by learning the acceptance regions for all classes. Many existing
set-valued classification methods do not consider the possibility that a new
class that never appeared in the training data appears in the test data.
Moreover, they are computationally expensive when the number of classes is
large. We propose a Generalized Prediction Set (GPS) approach to estimate the
acceptance regions while considering the possibility of a new class in the test
data. The proposed classifier minimizes the expected size of the prediction set
while guaranteeing that the class-specific accuracy is at least a pre-specified
value. Unlike previous methods, the proposed method achieves a good balance
between accuracy, efficiency, and anomaly detection rate. Moreover, our method
can be applied in parallel to all the classes to alleviate the computational
burden. Both theoretical analysis and numerical experiments are conducted to
illustrate the effectiveness of the proposed method.","['Zhou Wang', 'Xingye Qiao']","['stat.ML', 'cs.LG']",2022-09-20 19:40:33+00:00
http://arxiv.org/abs/2209.09893v1,Deep Generalized Schrödinger Bridge,"Mean-Field Game (MFG) serves as a crucial mathematical framework in modeling
the collective behavior of individual agents interacting stochastically with a
large population. In this work, we aim at solving a challenging class of MFGs
in which the differentiability of these interacting preferences may not be
available to the solver, and the population is urged to converge exactly to
some desired distribution. These setups are, despite being well-motivated for
practical purposes, complicated enough to paralyze most (deep) numerical
solvers. Nevertheless, we show that Schr\""odinger Bridge - as an
entropy-regularized optimal transport model - can be generalized to accepting
mean-field structures, hence solving these MFGs. This is achieved via the
application of Forward-Backward Stochastic Differential Equations theory,
which, intriguingly, leads to a computational framework with a similar
structure to Temporal Difference learning. As such, it opens up novel
algorithmic connections to Deep Reinforcement Learning that we leverage to
facilitate practical training. We show that our proposed objective function
provides necessary and sufficient conditions to the mean-field problem. Our
method, named Deep Generalized Schr\""odinger Bridge (DeepGSB), not only
outperforms prior methods in solving classical population navigation MFGs, but
is also capable of solving 1000-dimensional opinion depolarization, setting a
new state-of-the-art numerical solver for high-dimensional MFGs. Our code will
be made available at https://github.com/ghliu/DeepGSB.","['Guan-Horng Liu', 'Tianrong Chen', 'Oswin So', 'Evangelos A. Theodorou']","['stat.ML', 'cs.GT', 'cs.LG', 'math.OC']",2022-09-20 17:47:15+00:00
http://arxiv.org/abs/2209.09845v3,Relational Reasoning via Set Transformers: Provable Efficiency and Applications to MARL,"The cooperative Multi-A gent R einforcement Learning (MARL) with permutation
invariant agents framework has achieved tremendous empirical successes in
real-world applications. Unfortunately, the theoretical understanding of this
MARL problem is lacking due to the curse of many agents and the limited
exploration of the relational reasoning in existing works. In this paper, we
verify that the transformer implements complex relational reasoning, and we
propose and analyze model-free and model-based offline MARL algorithms with the
transformer approximators. We prove that the suboptimality gaps of the
model-free and model-based algorithms are independent of and logarithmic in the
number of agents respectively, which mitigates the curse of many agents. These
results are consequences of a novel generalization error bound of the
transformer and a novel analysis of the Maximum Likelihood Estimate (MLE) of
the system dynamics with the transformer. Our model-based algorithm is the
first provably efficient MARL algorithm that explicitly exploits the
permutation invariance of the agents. Our improved generalization bound may be
of independent interest and is applicable to other regression problems related
to the transformer beyond MARL.","['Fengzhuo Zhang', 'Boyi Liu', 'Kaixin Wang', 'Vincent Y. F. Tan', 'Zhuoran Yang', 'Zhaoran Wang']","['cs.LG', 'cs.MA', 'stat.ML']",2022-09-20 16:42:59+00:00
http://arxiv.org/abs/2209.09811v1,Predictive Scale-Bridging Simulations through Active Learning,"Throughout computational science, there is a growing need to utilize the
continual improvements in raw computational horsepower to achieve greater
physical fidelity through scale-bridging over brute-force increases in the
number of mesh elements. For instance, quantitative predictions of transport in
nanoporous media, critical to hydrocarbon extraction from tight shale
formations, are impossible without accounting for molecular-level interactions.
Similarly, inertial confinement fusion simulations rely on numerical diffusion
to simulate molecular effects such as non-local transport and mixing without
truly accounting for molecular interactions. With these two disparate
applications in mind, we develop a novel capability which uses an active
learning approach to optimize the use of local fine-scale simulations for
informing coarse-scale hydrodynamics. Our approach addresses three challenges:
forecasting continuum coarse-scale trajectory to speculatively execute new
fine-scale molecular dynamics calculations, dynamically updating coarse-scale
from fine-scale calculations, and quantifying uncertainty in neural network
models.","['Satish Karra', 'Mohamed Mehana', 'Nicholas Lubbers', 'Yu Chen', 'Abdourahmane Diaw', 'Javier E. Santos', 'Aleksandra Pachalieva', 'Robert S. Pavel', 'Jeffrey R. Haack', 'Michael McKerns', 'Christoph Junghans', 'Qinjun Kang', 'Daniel Livescu', 'Timothy C. Germann', 'Hari S. Viswanathan']","['cs.LG', 'cs.AI', 'stat.AP', 'stat.CO', 'stat.ML']",2022-09-20 15:58:50+00:00
http://arxiv.org/abs/2209.09810v2,The boosted HP filter is more general than you might think,"The global financial crisis and Covid recession have renewed discussion
concerning trend-cycle discovery in macroeconomic data, and boosting has
recently upgraded the popular HP filter to a modern machine learning device
suited to data-rich and rapid computational environments. This paper extends
boosting's trend determination capability to higher order integrated processes
and time series with roots that are local to unity. The theory is established
by understanding the asymptotic effect of boosting on a simple exponential
function. Given a universe of time series in FRED databases that exhibit
various dynamic patterns, boosting timely captures downturns at crises and
recoveries that follow.","['Ziwei Mei', 'Peter C. B. Phillips', 'Zhentao Shi']","['econ.EM', 'stat.ML']",2022-09-20 15:58:37+00:00
http://arxiv.org/abs/2209.09750v1,Deep Physics Corrector: A physics enhanced deep learning architecture for solving stochastic differential equations,"We propose a novel gray-box modeling algorithm for physical systems governed
by stochastic differential equations (SDE). The proposed approach, referred to
as the Deep Physics Corrector (DPC), blends approximate physics represented in
terms of SDE with deep neural network (DNN). The primary idea here is to
exploit DNN to model the missing physics. We hypothesize that combining
incomplete physics with data will make the model interpretable and allow better
generalization. The primary bottleneck associated with training surrogate
models for stochastic simulators is often associated with selecting the
suitable loss function. Among the different loss functions available in the
literature, we use the conditional maximum mean discrepancy (CMMD) loss
function in DPC because of its proven performance. Overall, physics-data fusion
and CMMD allow DPC to learn from sparse data. We illustrate the performance of
the proposed DPC on four benchmark examples from the literature. The results
obtained are highly accurate, indicating its possible application as a
surrogate model for stochastic simulators.","['Tushar', 'Souvik Chakraborty']","['stat.ML', 'cs.LG']",2022-09-20 14:30:07+00:00
http://arxiv.org/abs/2209.09692v1,Personalized Longitudinal Assessment of Multiple Sclerosis Using Smartphones,"Personalized longitudinal disease assessment is central to quickly
diagnosing, appropriately managing, and optimally adapting the therapeutic
strategy of multiple sclerosis (MS). It is also important for identifying the
idiosyncratic subject-specific disease profiles. Here, we design a novel
longitudinal model to map individual disease trajectories in an automated way
using sensor data that may contain missing values. First, we collect digital
measurements related to gait and balance, and upper extremity functions using
sensor-based assessments administered on a smartphone. Next, we treat missing
data via imputation. We then discover potential markers of MS by employing a
generalized estimation equation. Subsequently, parameters learned from multiple
training datasets are ensembled to form a simple, unified longitudinal
predictive model to forecast MS over time in previously unseen people with MS.
To mitigate potential underestimation for individuals with severe disease
scores, the final model incorporates additional subject-specific fine-tuning
using data from the first day. The results show that the proposed model is
promising to achieve personalized longitudinal MS assessment; they also suggest
that features related to gait and balance as well as upper extremity function,
remotely collected from sensor-based assessments, may be useful digital markers
for predicting MS over time.","['Oliver Y. Chén', 'Florian Lipsmeier', 'Huy Phan', 'Frank Dondelinger', 'Andrew Creagh', 'Christian Gossens', 'Michael Lindemann', 'Maarten de Vos']","['stat.ME', 'stat.AP', 'stat.ML', '62P10, 62P30, 62H12, 62J02, 62D10']",2022-09-20 12:56:29+00:00
http://arxiv.org/abs/2209.09617v2,Seq2Seq Surrogates of Epidemic Models to Facilitate Bayesian Inference,"Epidemic models are powerful tools in understanding infectious disease.
However, as they increase in size and complexity, they can quickly become
computationally intractable. Recent progress in modelling methodology has shown
that surrogate models can be used to emulate complex epidemic models with a
high-dimensional parameter space. We show that deep sequence-to-sequence
(seq2seq) models can serve as accurate surrogates for complex epidemic models
with sequence based model parameters, effectively replicating seasonal and
long-term transmission dynamics. Once trained, our surrogate can predict
scenarios a several thousand times faster than the original model, making them
ideal for policy exploration. We demonstrate that replacing a traditional
epidemic model with a learned simulator facilitates robust Bayesian inference.","['Giovanni Charles', 'Timothy M. Wolock', 'Peter Winskill', 'Azra Ghani', 'Samir Bhatt', 'Seth Flaxman']","['cs.LG', 'cs.AI', 'math.PR', 'q-bio.PE', 'stat.ML']",2022-09-20 11:23:19+00:00
http://arxiv.org/abs/2209.09493v3,A framework for benchmarking clustering algorithms,"The evaluation of clustering algorithms can involve running them on a variety
of benchmark problems, and comparing their outputs to the reference,
ground-truth groupings provided by experts. Unfortunately, many research papers
and graduate theses consider only a small number of datasets. Also, the fact
that there can be many equally valid ways to cluster a given problem set is
rarely taken into account. In order to overcome these limitations, we have
developed a framework whose aim is to introduce a consistent methodology for
testing clustering algorithms. Furthermore, we have aggregated, polished, and
standardised many clustering benchmark dataset collections referred to across
the machine learning and data mining literature, and included new datasets of
different dimensionalities, sizes, and cluster types. An interactive datasets
explorer, the documentation of the Python API, a description of the ways to
interact with the framework from other programming languages such as R or
MATLAB, and other details are all provided at
<https://clustering-benchmarks.gagolewski.com>.",['Marek Gagolewski'],"['cs.LG', 'stat.ML']",2022-09-20 06:10:41+00:00
http://arxiv.org/abs/2209.09419v4,Multi-armed Bandit Learning on a Graph,"The multi-armed bandit(MAB) problem is a simple yet powerful framework that
has been extensively studied in the context of decision-making under
uncertainty. In many real-world applications, such as robotic applications,
selecting an arm corresponds to a physical action that constrains the choices
of the next available arms (actions). Motivated by this, we study an extension
of MAB called the graph bandit, where an agent travels over a graph to maximize
the reward collected from different nodes. The graph defines the agent's
freedom in selecting the next available nodes at each step. We assume the graph
structure is fully available, but the reward distributions are unknown. Built
upon an offline graph-based planning algorithm and the principle of optimism,
we design a learning algorithm, G-UCB, that balances long-term
exploration-exploitation using the principle of optimism. We show that our
proposed algorithm achieves $O(\sqrt{|S|T\log(T)}+D|S|\log T)$ learning regret,
where $|S|$ is the number of nodes and $D$ is the diameter of the graph, which
matches the theoretical lower bound $\Omega(\sqrt{|S|T})$ up to logarithmic
factors. To our knowledge, this result is among the first tight regret bounds
in non-episodic, un-discounted learning problems with known deterministic
transitions. Numerical experiments confirm that our algorithm outperforms
several benchmarks.","['Tianpeng Zhang', 'Kasper Johansson', 'Na Li']","['cs.LG', 'cs.AI', 'stat.ML']",2022-09-20 02:31:42+00:00
http://arxiv.org/abs/2209.09400v2,Polynomial-Time Reachability for LTI Systems with Two-Level Lattice Neural Network Controllers,"In this paper, we consider the computational complexity of bounding the
reachable set of a Linear Time-Invariant (LTI) system controlled by a Rectified
Linear Unit (ReLU) Two-Level Lattice (TLL) Neural Network (NN) controller. In
particular, we show that for such a system and controller, it is possible to
compute the exact one-step reachable set in polynomial time in the size of the
TLL NN controller (number of neurons). Additionally, we show that a tight
bounding box of the reachable set is computable via two polynomial-time
methods: one with polynomial complexity in the size of the TLL and the other
with polynomial complexity in the Lipschitz constant of the controller and
other problem parameters. Finally, we propose a pragmatic algorithm that
adaptively combines the benefits of (semi-)exact reachability and approximate
reachability, which we call L-TLLBox. We evaluate L-TLLBox with an empirical
comparison to a state-of-the-art NN controller reachability tool. In our
experiments, L-TLLBox completed reachability analysis as much as 5000x faster
than this tool on the same network/system, while producing reach boxes that
were from 0.08 to 1.42 times the area.","['James Ferlez', 'Yasser Shoukry']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2022-09-20 01:12:31+00:00
http://arxiv.org/abs/2209.09362v1,Analyzing Machine Learning Models for Credit Scoring with Explainable AI and Optimizing Investment Decisions,"This paper examines two different yet related questions related to
explainable AI (XAI) practices. Machine learning (ML) is increasingly important
in financial services, such as pre-approval, credit underwriting, investments,
and various front-end and back-end activities. Machine Learning can
automatically detect non-linearities and interactions in training data,
facilitating faster and more accurate credit decisions. However, machine
learning models are opaque and hard to explain, which are critical elements
needed for establishing a reliable technology. The study compares various
machine learning models, including single classifiers (logistic regression,
decision trees, LDA, QDA), heterogeneous ensembles (AdaBoost, Random Forest),
and sequential neural networks. The results indicate that ensemble classifiers
and neural networks outperform. In addition, two advanced post-hoc model
agnostic explainability techniques - LIME and SHAP are utilized to assess
ML-based credit scoring models using the open-access datasets offered by
US-based P2P Lending Platform, Lending Club. For this study, we are also using
machine learning algorithms to develop new investment models and explore
portfolio strategies that can maximize profitability while minimizing risk.",['Swati Tyagi'],"['cs.LG', 'cs.CY', 'stat.ML']",2022-09-19 21:44:42+00:00
http://arxiv.org/abs/2209.09349v1,Physics-Informed Machine Learning of Dynamical Systems for Efficient Bayesian Inference,"Although the no-u-turn sampler (NUTS) is a widely adopted method for
performing Bayesian inference, it requires numerous posterior gradients which
can be expensive to compute in practice. Recently, there has been a significant
interest in physics-based machine learning of dynamical (or Hamiltonian)
systems and Hamiltonian neural networks (HNNs) is a noteworthy architecture.
But these types of architectures have not been applied to solve Bayesian
inference problems efficiently. We propose the use of HNNs for performing
Bayesian inference efficiently without requiring numerous posterior gradients.
We introduce latent variable outputs to HNNs (L-HNNs) for improved expressivity
and reduced integration errors. We integrate L-HNNs in NUTS and further propose
an online error monitoring scheme to prevent sampling degeneracy in regions
where L-HNNs may have little training data. We demonstrate L-HNNs in NUTS with
online error monitoring considering several complex high-dimensional posterior
densities and compare its performance to NUTS.","['Somayajulu L. N. Dhulipala', 'Yifeng Che', 'Michael D. Shields']","['stat.ML', 'cs.LG']",2022-09-19 21:17:23+00:00
http://arxiv.org/abs/2209.09326v2,Sparse Interaction Additive Networks via Feature Interaction Detection and Sparse Selection,"There is currently a large gap in performance between the statistically
rigorous methods like linear regression or additive splines and the powerful
deep methods using neural networks. Previous works attempting to close this gap
have failed to fully investigate the exponentially growing number of feature
combinations which deep networks consider automatically during training. In
this work, we develop a tractable selection algorithm to efficiently identify
the necessary feature combinations by leveraging techniques in feature
interaction detection. Our proposed Sparse Interaction Additive Networks (SIAN)
construct a bridge from these simple and interpretable models to fully
connected neural networks. SIAN achieves competitive performance against
state-of-the-art methods across multiple large-scale tabular datasets and
consistently finds an optimal tradeoff between the modeling capacity of neural
networks and the generalizability of simpler methods.","['James Enouen', 'Yan Liu']","['cs.LG', 'stat.ML']",2022-09-19 19:57:17+00:00
http://arxiv.org/abs/2209.09315v2,Deep Linear Networks can Benignly Overfit when Shallow Ones Do,"We bound the excess risk of interpolating deep linear networks trained using
gradient flow. In a setting previously used to establish risk bounds for the
minimum $\ell_2$-norm interpolant, we show that randomly initialized deep
linear networks can closely approximate or even match known bounds for the
minimum $\ell_2$-norm interpolant. Our analysis also reveals that interpolating
deep linear models have exactly the same conditional variance as the minimum
$\ell_2$-norm solution. Since the noise affects the excess risk only through
the conditional variance, this implies that depth does not improve the
algorithm's ability to ""hide the noise"". Our simulations verify that aspects of
our bounds reflect typical behavior for simple data distributions. We also find
that similar phenomena are seen in simulations with ReLU networks, although the
situation there is more nuanced.","['Niladri S. Chatterji', 'Philip M. Long']","['cs.LG', 'cs.AI', 'math.ST', 'stat.ML', 'stat.TH']",2022-09-19 19:23:04+00:00
http://arxiv.org/abs/2209.09298v1,Stability and Generalization Analysis of Gradient Methods for Shallow Neural Networks,"While significant theoretical progress has been achieved, unveiling the
generalization mystery of overparameterized neural networks still remains
largely elusive. In this paper, we study the generalization behavior of shallow
neural networks (SNNs) by leveraging the concept of algorithmic stability. We
consider gradient descent (GD) and stochastic gradient descent (SGD) to train
SNNs, for both of which we develop consistent excess risk bounds by balancing
the optimization and generalization via early-stopping. As compared to existing
analysis on GD, our new analysis requires a relaxed overparameterization
assumption and also applies to SGD. The key for the improvement is a better
estimation of the smallest eigenvalues of the Hessian matrices of the empirical
risks and the loss function along the trajectories of GD and SGD by providing a
refined estimation of their iterates.","['Yunwen Lei', 'Rong Jin', 'Yiming Ying']","['cs.LG', 'stat.ML']",2022-09-19 18:48:00+00:00
http://arxiv.org/abs/2209.09211v2,Neural Collapse with Normalized Features: A Geometric Analysis over the Riemannian Manifold,"When training overparameterized deep networks for classification tasks, it
has been widely observed that the learned features exhibit a so-called ""neural
collapse"" phenomenon. More specifically, for the output features of the
penultimate layer, for each class the within-class features converge to their
means, and the means of different classes exhibit a certain tight frame
structure, which is also aligned with the last layer's classifier. As feature
normalization in the last layer becomes a common practice in modern
representation learning, in this work we theoretically justify the neural
collapse phenomenon for normalized features. Based on an unconstrained feature
model, we simplify the empirical loss function in a multi-class classification
task into a nonconvex optimization problem over the Riemannian manifold by
constraining all features and classifiers over the sphere. In this context, we
analyze the nonconvex landscape of the Riemannian optimization problem over the
product of spheres, showing a benign global landscape in the sense that the
only global minimizers are the neural collapse solutions while all other
critical points are strict saddles with negative curvature. Experimental
results on practical deep networks corroborate our theory and demonstrate that
better representations can be learned faster via feature normalization.","['Can Yaras', 'Peng Wang', 'Zhihui Zhu', 'Laura Balzano', 'Qing Qu']","['cs.LG', 'cs.CV', 'cs.IT', 'eess.SP', 'math.IT', 'stat.ML']",2022-09-19 17:26:32+00:00
http://arxiv.org/abs/2209.09190v2,Robust leave-one-out cross-validation for high-dimensional Bayesian models,"Leave-one-out cross-validation (LOO-CV) is a popular method for estimating
out-of-sample predictive accuracy. However, computing LOO-CV criteria can be
computationally expensive due to the need to fit the model multiple times. In
the Bayesian context, importance sampling provides a possible solution but
classical approaches can easily produce estimators whose asymptotic variance is
infinite, making them potentially unreliable. Here we propose and analyze a
novel mixture estimator to compute Bayesian LOO-CV criteria. Our method retains
the simplicity and computational convenience of classical approaches, while
guaranteeing finite asymptotic variance of the resulting estimators. Both
theoretical and numerical results are provided to illustrate the improved
robustness and efficiency. The computational benefits are particularly
significant in high-dimensional problems, allowing to perform Bayesian LOO-CV
for a broader range of models, and datasets with highly influential
observations. The proposed methodology is easily implementable in standard
probabilistic programming software and has a computational cost roughly
equivalent to fitting the original model once.","['Luca Silva', 'Giacomo Zanella']","['stat.CO', 'stat.ME', 'stat.ML']",2022-09-19 17:14:52+00:00
http://arxiv.org/abs/2209.09168v1,Application of Neural Network in the Prediction of NOx Emissions from Degrading Gas Turbine,"This paper is aiming to apply neural network algorithm for predicting the
process response (NOx emissions) from degrading natural gas turbines. Nine
different process variables, or predictors, are considered in the predictive
modelling. It is found out that the model trained by neural network algorithm
should use part of recent data in the training and validation sets accounting
for the impact of the system degradation. R-Square values of the training and
validation sets demonstrate the validity of the model. The residue plot,
without any clear pattern, shows the model is appropriate. The ranking of the
importance of the process variables are demonstrated and the prediction profile
confirms the significance of the process variables. The model trained by using
neural network algorithm manifests the optimal settings of the process
variables to reach the minimum value of NOx emissions from the degrading gas
turbine system.","['Zhenkun Zheng', 'Alan Rezazadeh']","['cs.LG', 'stat.ML']",2022-09-19 16:44:44+00:00
http://arxiv.org/abs/2209.09658v2,Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty,"Among attempts at giving a theoretical account of the success of deep neural
networks, a recent line of work has identified a so-called lazy training regime
in which the network can be well approximated by its linearization around
initialization. Here we investigate the comparative effect of the lazy (linear)
and feature learning (non-linear) regimes on subgroups of examples based on
their difficulty. Specifically, we show that easier examples are given more
weight in feature learning mode, resulting in faster training compared to more
difficult ones. In other words, the non-linear dynamics tends to sequentialize
the learning of examples of increasing difficulty. We illustrate this
phenomenon across different ways to quantify example difficulty, including
c-score, label noise, and in the presence of easy-to-learn spurious
correlations. Our results reveal a new understanding of how deep networks
prioritize resources across example difficulty.","['Thomas George', 'Guillaume Lajoie', 'Aristide Baratin']","['cs.LG', 'stat.ML']",2022-09-19 16:10:47+00:00
http://arxiv.org/abs/2209.09060v3,Deep Metric Learning with Chance Constraints,"Deep metric learning (DML) aims to minimize empirical expected loss of the
pairwise intra-/inter- class proximity violations in the embedding space. We
relate DML to feasibility problem of finite chance constraints. We show that
minimizer of proxy-based DML satisfies certain chance constraints, and that the
worst case generalization performance of the proxy-based methods can be
characterized by the radius of the smallest ball around a class proxy to cover
the entire domain of the corresponding class samples, suggesting multiple
proxies per class helps performance. To provide a scalable algorithm as well as
exploiting more proxies, we consider the chance constraints implied by the
minimizers of proxy-based DML instances and reformulate DML as finding a
feasible point in intersection of such constraints, resulting in a problem to
be approximately solved by iterative projections. Simply put, we repeatedly
train a regularized proxy-based loss and re-initialize the proxies with the
embeddings of the deliberately selected new samples. We applied our method with
4 well-accepted DML losses and show the effectiveness with extensive
evaluations on 4 popular DML benchmarks. Code is available at:
https://github.com/yetigurbuz/ccp-dml","['Yeti Z. Gurbuz', 'Ogul Can', 'A. Aydin Alatan']","['cs.CV', 'cs.LG', 'stat.ML']",2022-09-19 14:50:48+00:00
http://arxiv.org/abs/2209.08951v1,Generalization Bounds for Stochastic Gradient Descent via Localized $\varepsilon$-Covers,"In this paper, we propose a new covering technique localized for the
trajectories of SGD. This localization provides an algorithm-specific
complexity measured by the covering number, which can have
dimension-independent cardinality in contrast to standard uniform covering
arguments that result in exponential dimension dependency. Based on this
localized construction, we show that if the objective function is a finite
perturbation of a piecewise strongly convex and smooth function with $P$
pieces, i.e. non-convex and non-smooth in general, the generalization error can
be upper bounded by $O(\sqrt{(\log n\log(nP))/n})$, where $n$ is the number of
data samples. In particular, this rate is independent of dimension and does not
require early stopping and decaying step size. Finally, we employ these results
in various contexts and derive generalization bounds for multi-index linear
models, multi-class support vector machines, and $K$-means clustering for both
hard and soft label setups, improving the known state-of-the-art rates.","['Sejun Park', 'Umut Şimşekli', 'Murat A. Erdogdu']","['stat.ML', 'cs.LG']",2022-09-19 12:11:07+00:00
http://arxiv.org/abs/2209.08945v1,A novel approach for wafer defect pattern classification based on topological data analysis,"In semiconductor manufacturing, wafer map defect pattern provides critical
information for facility maintenance and yield management, so the
classification of defect patterns is one of the most important tasks in the
manufacturing process. In this paper, we propose a novel way to represent the
shape of the defect pattern as a finite-dimensional vector, which will be used
as an input for a neural network algorithm for classification. The main idea is
to extract the topological features of each pattern by using the theory of
persistent homology from topological data analysis (TDA). Through some
experiments with a simulated dataset, we show that the proposed method is
faster and much more efficient in training with higher accuracy, compared with
the method using convolutional neural networks (CNN) which is the most common
approach for wafer map defect pattern classification. Moreover, our method
outperforms the CNN-based method when the number of training data is not enough
and is imbalanced.","['Seungchan Ko', 'Dowan Koo']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2022-09-19 11:54:13+00:00
http://arxiv.org/abs/2209.08901v3,Global Optimization for Cardinality-constrained Minimum Sum-of-Squares Clustering via Semidefinite Programming,"The minimum sum-of-squares clustering (MSSC), or k-means type clustering, has
been recently extended to exploit prior knowledge on the cardinality of each
cluster. Such knowledge is used to increase performance as well as solution
quality. In this paper, we propose a global optimization approach based on the
branch-and-cut technique to solve the cardinality-constrained MSSC. For the
lower bound routine, we use the semidefinite programming (SDP) relaxation
recently proposed by Rujeerapaiboon et al. [SIAM J. Optim. 29(2), 1211-1239,
(2019)]. However, this relaxation can be used in a branch-and-cut method only
for small-size instances. Therefore, we derive a new SDP relaxation that scales
better with the instance size and the number of clusters. In both cases, we
strengthen the bound by adding polyhedral cuts. Benefiting from a tailored
branching strategy which enforces pairwise constraints, we reduce the
complexity of the problems arising in the children nodes. For the upper bound,
instead, we present a local search procedure that exploits the solution of the
SDP relaxation solved at each node. Computational results show that the
proposed algorithm globally solves, for the first time, real-world instances of
size 10 times larger than those solved by state-of-the-art exact methods.","['Veronica Piccialli', 'Antonio M. Sudoso']","['math.OC', 'cs.LG', 'stat.ML']",2022-09-19 10:19:06+00:00
http://arxiv.org/abs/2209.08860v6,A Survey of Deep Causal Models and Their Industrial Applications,"The notion of causality assumes a paramount position within the realm of
human cognition. Over the past few decades, there has been significant
advancement in the domain of causal effect estimation across various
disciplines, including but not limited to computer science, medicine,
economics, and industrial applications. Given the continous advancements in
deep learning methodologies, there has been a notable surge in its utilization
for the estimation of causal effects using counterfactual data. Typically, deep
causal models map the characteristics of covariates to a representation space
and then design various objective functions to estimate counterfactual data
unbiasedly. Different from the existing surveys on causal models in machine
learning, this review mainly focuses on the overview of the deep causal models
based on neural networks, and its core contributions are as follows: 1) we cast
insight on a comprehensive overview of deep causal models from both timeline of
development and method classification perspectives; 2) we outline some typical
applications of causal effect estimation to industry; 3) we also endeavor to
present a detailed categorization and analysis on relevant datasets, source
codes and experiments.","['Zongyu Li', 'Xiaobo Guo', 'Siwei Qiang']","['stat.ML', 'cs.LG']",2022-09-19 09:05:05+00:00
http://arxiv.org/abs/2209.08858v1,Rethinking Knowledge Graph Evaluation Under the Open-World Assumption,"Most knowledge graphs (KGs) are incomplete, which motivates one important
research topic on automatically complementing knowledge graphs. However,
evaluation of knowledge graph completion (KGC) models often ignores the
incompleteness -- facts in the test set are ranked against all unknown triplets
which may contain a large number of missing facts not included in the KG yet.
Treating all unknown triplets as false is called the closed-world assumption.
This closed-world assumption might negatively affect the fairness and
consistency of the evaluation metrics. In this paper, we study KGC evaluation
under a more realistic setting, namely the open-world assumption, where unknown
triplets are considered to include many missing facts not included in the
training or test sets. For the currently most used metrics such as mean
reciprocal rank (MRR) and Hits@K, we point out that their behavior may be
unexpected under the open-world assumption. Specifically, with not many missing
facts, their numbers show a logarithmic trend with respect to the true strength
of the model, and thus, the metric increase could be insignificant in terms of
reflecting the true model improvement. Further, considering the variance, we
show that the degradation in the reported numbers may result in incorrect
comparisons between different models, where stronger models may have lower
metric numbers. We validate the phenomenon both theoretically and
experimentally. Finally, we suggest possible causes and solutions for this
problem. Our code and data are available at
https://github.com/GraphPKU/Open-World-KG .","['Haotong Yang', 'Zhouchen Lin', 'Muhan Zhang']","['cs.AI', 'cs.LG', 'stat.ML']",2022-09-19 09:01:29+00:00
http://arxiv.org/abs/2209.11740v2,On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks,"This paper focuses on improving the mathematical interpretability of
convolutional neural networks (CNNs) in the context of image classification.
Specifically, we tackle the instability issue arising in their first layer,
which tends to learn parameters that closely resemble oriented band-pass
filters when trained on datasets like ImageNet. Subsampled convolutions with
such Gabor-like filters are prone to aliasing, causing sensitivity to small
input shifts. In this context, we establish conditions under which the max
pooling operator approximates a complex modulus, which is nearly shift
invariant. We then derive a measure of shift invariance for subsampled
convolutions followed by max pooling. In particular, we highlight the crucial
role played by the filter's frequency and orientation in achieving stability.
We experimentally validate our theory by considering a deterministic feature
extractor based on the dual-tree complex wavelet packet transform, a particular
case of discrete Gabor-like decomposition.","['Hubert Leterme', 'Kévin Polisano', 'Valérie Perrier', 'Karteek Alahari']","['cs.CV', 'cs.AI', 'eess.SP', 'stat.ML']",2022-09-19 08:15:30+00:00
http://arxiv.org/abs/2209.08745v2,Importance Tempering: Group Robustness for Overparameterized Models,"Although overparameterized models have shown their success on many machine
learning tasks, the accuracy could drop on the testing distribution that is
different from the training one. This accuracy drop still limits applying
machine learning in the wild. At the same time, importance weighting, a
traditional technique to handle distribution shifts, has been demonstrated to
have less or even no effect on overparameterized models both empirically and
theoretically. In this paper, we propose importance tempering to improve the
decision boundary and achieve consistently better results for overparameterized
models. Theoretically, we justify that the selection of group temperature can
be different under label shift and spurious correlation setting. At the same
time, we also prove that properly selected temperatures can extricate the
minority collapse for imbalanced classification. Empirically, we achieve
state-of-the-art results on worst group classification tasks using importance
tempering.","['Yiping Lu', 'Wenlong Ji', 'Zachary Izzo', 'Lexing Ying']","['cs.LG', 'cs.AI', 'math.ST', 'stat.ML', 'stat.TH']",2022-09-19 03:41:30+00:00
http://arxiv.org/abs/2209.08739v1,Adaptive Multi-stage Density Ratio Estimation for Learning Latent Space Energy-based Model,"This paper studies the fundamental problem of learning energy-based model
(EBM) in the latent space of the generator model. Learning such prior model
typically requires running costly Markov Chain Monte Carlo (MCMC). Instead, we
propose to use noise contrastive estimation (NCE) to discriminatively learn the
EBM through density ratio estimation between the latent prior density and
latent posterior density. However, the NCE typically fails to accurately
estimate such density ratio given large gap between two densities. To
effectively tackle this issue and learn more expressive prior models, we
develop the adaptive multi-stage density ratio estimation which breaks the
estimation into multiple stages and learn different stages of density ratio
sequentially and adaptively. The latent prior model can be gradually learned
using ratio estimated in previous stage so that the final latent space EBM
prior can be naturally formed by product of ratios in different stages. The
proposed method enables informative and much sharper prior than existing
baselines, and can be trained efficiently. Our experiments demonstrate strong
performances in image generation and reconstruction as well as anomaly
detection.","['Zhisheng Xiao', 'Tian Han']","['cs.LG', 'cs.CV', 'stat.ML']",2022-09-19 03:20:15+00:00
http://arxiv.org/abs/2209.08737v1,Heterogeneous Federated Learning on a Graph,"Federated learning, where algorithms are trained across multiple
decentralized devices without sharing local data, is increasingly popular in
distributed machine learning practice. Typically, a graph structure $G$ exists
behind local devices for communication. In this work, we consider parameter
estimation in federated learning with data distribution and communication
heterogeneity, as well as limited computational capacity of local devices. We
encode the distribution heterogeneity by parametrizing distributions on local
devices with a set of distinct $p$-dimensional vectors. We then propose to
jointly estimate parameters of all devices under the $M$-estimation framework
with the fused Lasso regularization, encouraging an equal estimate of
parameters on connected devices in $G$. We provide a general result for our
estimator depending on $G$, which can be further calibrated to obtain
convergence rates for various specific problem setups. Surprisingly, our
estimator attains the optimal rate under certain graph fidelity condition on
$G$, as if we could aggregate all samples sharing the same distribution. If the
graph fidelity condition is not met, we propose an edge selection procedure via
multiple testing to ensure the optimality. To ease the burden of local
computation, a decentralized stochastic version of ADMM is provided, with
convergence rate $O(T^{-1}\log T)$ where $T$ denotes the number of iterations.
We highlight that, our algorithm transmits only parameters along edges of $G$
at each iteration, without requiring a central machine, which preserves
privacy. We further extend it to the case where devices are randomly
inaccessible during the training process, with a similar algorithmic
convergence guarantee. The computational and statistical efficiency of our
method is evidenced by simulation experiments and the 2020 US presidential
election data set.","['Huiyuan Wang', 'Xuyang Zhao', 'Wei Lin']","['stat.ML', 'cs.LG', 'stat.ME']",2022-09-19 03:18:10+00:00
http://arxiv.org/abs/2209.08729v1,Data-driven and machine-learning based prediction of wave propagation behavior in dam-break flood,"The computational prediction of wave propagation in dam-break floods is a
long-standing problem in hydrodynamics and hydrology. Until now, conventional
numerical models based on Saint-Venant equations are the dominant approaches.
Here we show that a machine learning model that is well-trained on a minimal
amount of data, can help predict the long-term dynamic behavior of a
one-dimensional dam-break flood with satisfactory accuracy. For this purpose,
we solve the Saint-Venant equations for a one-dimensional dam-break flood
scenario using the Lax-Wendroff numerical scheme and train the reservoir
computing echo state network (RC-ESN) with the dataset by the simulation
results consisting of time-sequence flow depths. We demonstrate a good
prediction ability of the RC-ESN model, which ahead predicts wave propagation
behavior 286 time-steps in the dam-break flood with a root mean square error
(RMSE) smaller than 0.01, outperforming the conventional long short-term memory
(LSTM) model which reaches a comparable RMSE of only 81 time-steps ahead. To
show the performance of the RC-ESN model, we also provide a sensitivity
analysis of the prediction accuracy concerning the key parameters including
training set size, reservoir size, and spectral radius. Results indicate that
the RC-ESN are less dependent on the training set size, a medium reservoir size
K=1200~2600 is sufficient. We confirm that the spectral radius \r{ho} shows a
complex influence on the prediction accuracy and suggest a smaller spectral
radius \r{ho} currently. By changing the initial flow depth of the dam break,
we also obtained the conclusion that the prediction horizon of RC-ESN is larger
than that of LSTM.","['Changli Li', 'Zheng Han', 'Yange Li', 'Ming Li', 'Weidong Wang']","['physics.flu-dyn', 'stat.ML']",2022-09-19 02:58:31+00:00
http://arxiv.org/abs/2209.08682v1,Towards Robust Off-Policy Evaluation via Human Inputs,"Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies
in high-stakes domains such as healthcare, where direct deployment is often
infeasible, unethical, or expensive. When deployment environments are expected
to undergo changes (that is, dataset shifts), it is important for OPE methods
to perform robust evaluation of the policies amidst such changes. Existing
approaches consider robustness against a large class of shifts that can
arbitrarily change any observable property of the environment. This often
results in highly pessimistic estimates of the utilities, thereby invalidating
policies that might have been useful in deployment. In this work, we address
the aforementioned problem by investigating how domain knowledge can help
provide more realistic estimates of the utilities of policies. We leverage
human inputs on which aspects of the environments may plausibly change, and
adapt the OPE methods to only consider shifts on these aspects. Specifically,
we propose a novel framework, Robust OPE (ROPE), which considers shifts on a
subset of covariates in the data based on user inputs, and estimates worst-case
utility under these shifts. We then develop computationally efficient
algorithms for OPE that are robust to the aforementioned shifts for contextual
bandits and Markov decision processes. We also theoretically analyze the sample
complexity of these algorithms. Extensive experimentation with synthetic and
real world datasets from the healthcare domain demonstrates that our approach
not only captures realistic dataset shifts accurately, but also results in less
pessimistic policy evaluations.","['Harvineet Singh', 'Shalmali Joshi', 'Finale Doshi-Velez', 'Himabindu Lakkaraju']","['cs.LG', 'stat.ML']",2022-09-18 23:59:55+00:00
http://arxiv.org/abs/2209.08680v1,HiPart: Hierarchical Divisive Clustering Toolbox,"This paper presents the HiPart package, an open-source native python library
that provides efficient and interpret-able implementations of divisive
hierarchical clustering algorithms. HiPart supports interactive visualizations
for the manipulation of the execution steps allowing the direct intervention of
the clustering outcome. This package is highly suited for Big Data applications
as the focus has been given to the computational efficiency of the implemented
clustering methodologies. The dependencies used are either Python build-in
packages or highly maintained stable external packages. The software is
provided under the MIT license. The package's source code and documentation can
be found at https://github.com/panagiotisanagnostou/HiPart.","['Panagiotis Anagnostou', 'Sotiris Tasoulis', 'Vassilis Plagianakos', 'Dimitris Tasoulis']","['stat.ML', 'cs.AI', 'cs.LG']",2022-09-18 23:48:43+00:00
http://arxiv.org/abs/2209.08601v2,Comparative study of machine learning and deep learning methods on ASD classification,"The autism dataset is studied to identify the differences between autistic
and healthy groups. For this, the resting-state Functional Magnetic Resonance
Imaging (rs-fMRI) data of the two groups are analyzed, and networks of
connections between brain regions were created. Several classification
frameworks are developed to distinguish the connectivity patterns between the
groups. The best models for statistical inference and precision were compared,
and the tradeoff between precision and model interpretability was analyzed.
Finally, the classification accuracy measures were reported to justify the
performance of our framework. Our best model can classify autistic and healthy
patients on the multisite ABIDE I data with 71% accuracy.","['Ramchandra Rimal', 'Mitchell Brannon', 'Yingxin Wang', 'Xin Yang']","['eess.IV', 'cs.LG', 'q-bio.NC', 'stat.ML', '62-08, 62P11']",2022-09-18 16:39:10+00:00
http://arxiv.org/abs/2209.08579v2,Bivariate Causal Discovery for Categorical Data via Classification with Optimal Label Permutation,"Causal discovery for quantitative data has been extensively studied but less
is known for categorical data. We propose a novel causal model for categorical
data based on a new classification model, termed classification with optimal
label permutation (COLP). By design, COLP is a parsimonious classifier, which
gives rise to a provably identifiable causal model. A simple learning algorithm
via comparing likelihood functions of causal and anti-causal models suffices to
learn the causal direction. Through experiments with synthetic and real data,
we demonstrate the favorable performance of the proposed COLP-based causal
model compared to state-of-the-art methods. We also make available an
accompanying R package COLP, which contains the proposed causal discovery
algorithm and a benchmark dataset of categorical cause-effect pairs.",['Yang Ni'],"['stat.ML', 'cs.LG', 'stat.ME']",2022-09-18 15:04:55+00:00
http://arxiv.org/abs/2209.11172v1,EEG-Based Epileptic Seizure Prediction Using Temporal Multi-Channel Transformers,"Epilepsy is one of the most common neurological diseases, characterized by
transient and unprovoked events called epileptic seizures. Electroencephalogram
(EEG) is an auxiliary method used to perform both the diagnosis and the
monitoring of epilepsy. Given the unexpected nature of an epileptic seizure,
its prediction would improve patient care, optimizing the quality of life and
the treatment of epilepsy. Predicting an epileptic seizure implies the
identification of two distinct states of EEG in a patient with epilepsy: the
preictal and the interictal. In this paper, we developed two deep learning
models called Temporal Multi-Channel Transformer (TMC-T) and Vision Transformer
(TMC-ViT), adaptations of Transformer-based architectures for multi-channel
temporal signals. Moreover, we accessed the impact of choosing different
preictal duration, since its length is not a consensus among experts, and also
evaluated how the sample size benefits each model. Our models are compared with
fully connected, convolutional, and recurrent networks. The algorithms were
patient-specific trained and evaluated on raw EEG signals from the CHB-MIT
database. Experimental results and statistical validation demonstrated that our
TMC-ViT model surpassed the CNN architecture, state-of-the-art in seizure
prediction.","['Ricardo V. Godoy', 'Tharik J. S. Reis', 'Paulo H. Polegato', 'Gustavo J. G. Lahr', 'Ricardo L. Saute', 'Frederico N. Nakano', 'Helio R. Machado', 'Americo C. Sakamoto', 'Marcelo Becker', 'Glauco A. P. Caurin']","['eess.SP', 'cs.AI', 'cs.LG', 'stat.ML', '92C55 (Primary)', 'I.5.4']",2022-09-18 03:03:47+00:00
http://arxiv.org/abs/2209.08436v1,Estimating and Explaining Model Performance When Both Covariates and Labels Shift,"Deployed machine learning (ML) models often encounter new user data that
differs from their training data. Therefore, estimating how well a given model
might perform on the new data is an important step toward reliable ML
applications. This is very challenging, however, as the data distribution can
change in flexible ways, and we may not have any labels on the new data, which
is often the case in monitoring settings. In this paper, we propose a new
distribution shift model, Sparse Joint Shift (SJS), which considers the joint
shift of both labels and a few features. This unifies and generalizes several
existing shift models including label shift and sparse covariate shift, where
only marginal feature or label distribution shifts are considered. We describe
mathematical conditions under which SJS is identifiable. We further propose
SEES, an algorithmic framework to characterize the distribution shift under SJS
and to estimate a model's performance on new data without any labels. We
conduct extensive experiments on several real-world datasets with various ML
models. Across different datasets and distribution shifts, SEES achieves
significant (up to an order of magnitude) shift estimation error improvements
over existing approaches.","['Lingjiao Chen', 'Matei Zaharia', 'James Zou']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.AP']",2022-09-18 01:16:16+00:00
http://arxiv.org/abs/2209.08422v1,Computed Decision Weights and a New Learning Algorithm for Neural Classifiers,"In this paper we consider the possibility of computing rather than training
the decision layer weights of a neural classifier. Such a possibility arises in
two way, from making an appropriate choice of loss function and by solving a
problem of constrained optimization. The latter formulation leads to a
promising new learning process for pre-decision weights with both simplicity
and efficacy.",['Eugene Wong'],"['cs.LG', 'stat.ML']",2022-09-17 22:59:42+00:00
http://arxiv.org/abs/2209.08411v3,DynaConF: Dynamic Forecasting of Non-Stationary Time Series,"Deep learning has shown impressive results in a variety of time series
forecasting tasks, where modeling the conditional distribution of the future
given the past is the essence. However, when this conditional distribution is
non-stationary, it poses challenges for these models to learn consistently and
to predict accurately. In this work, we propose a new method to model
non-stationary conditional distributions over time by clearly decoupling
stationary conditional distribution modeling from non-stationary dynamics
modeling. Our method is based on a Bayesian dynamic model that can adapt to
conditional distribution changes and a deep conditional distribution model that
handles multivariate time series using a factorized output space. Our
experimental results on synthetic and real-world datasets show that our model
can adapt to non-stationary time series better than state-of-the-art deep
learning solutions.","['Siqi Liu', 'Andreas Lehrmann']","['cs.LG', 'stat.ML']",2022-09-17 21:40:02+00:00
http://arxiv.org/abs/2209.08399v1,Approximation results for Gradient Descent trained Shallow Neural Networks in $1d$,"Two aspects of neural networks that have been extensively studied in the
recent literature are their function approximation properties and their
training by gradient descent methods. The approximation problem seeks accurate
approximations with a minimal number of weights. In most of the current
literature these weights are fully or partially hand-crafted, showing the
capabilities of neural networks but not necessarily their practical
performance. In contrast, optimization theory for neural networks heavily
relies on an abundance of weights in over-parametrized regimes.
  This paper balances these two demands and provides an approximation result
for shallow networks in $1d$ with non-convex weight optimization by gradient
descent. We consider finite width networks and infinite sample limits, which is
the typical setup in approximation theory. Technically, this problem is not
over-parametrized, however, some form of redundancy reappears as a loss in
approximation rate compared to best possible rates.","['R. Gentile', 'G. Welper']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML', '41A46, 65K10, 68T07']",2022-09-17 20:26:19+00:00
http://arxiv.org/abs/2209.08307v2,A review of predictive uncertainty estimation with machine learning,"Predictions and forecasts of machine learning models should take the form of
probability distributions, aiming to increase the quantity of information
communicated to end users. Although applications of probabilistic prediction
and forecasting with machine learning models in academia and industry are
becoming more frequent, related concepts and methods have not been formalized
and structured under a holistic view of the entire field. Here, we review the
topic of predictive uncertainty estimation with machine learning algorithms, as
well as the related metrics (consistent scoring functions and proper scoring
rules) for assessing probabilistic predictions. The review covers a time period
spanning from the introduction of early statistical (linear regression and time
series models, based on Bayesian statistics or quantile regression) to recent
machine learning algorithms (including generalized additive models for
location, scale and shape, random forests, boosting and deep learning
algorithms) that are more flexible by nature. The review of the progress in the
field, expedites our understanding on how to develop new algorithms tailored to
users' needs, since the latest advancements are based on some fundamental
concepts applied to more complex algorithms. We conclude by classifying the
material and discussing challenges that are becoming a hot topic of research.","['Hristos Tyralis', 'Georgia Papacharalampous']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-09-17 10:36:30+00:00
http://arxiv.org/abs/2209.08273v2,Low-Rank Covariance Completion for Graph Quilting with Applications to Functional Connectivity,"As a tool for estimating networks in high dimensions, graphical models are
commonly applied to calcium imaging data to estimate functional neuronal
connectivity, i.e. relationships between the activities of neurons. However, in
many calcium imaging data sets, the full population of neurons is not recorded
simultaneously, but instead in partially overlapping blocks. This leads to the
Graph Quilting problem, as first introduced by (Vinci et.al. 2019), in which
the goal is to infer the structure of the full graph when only subsets of
features are jointly observed. In this paper, we study a novel two-step
approach to Graph Quilting, which first imputes the complete covariance matrix
using low-rank covariance completion techniques before estimating the graph
structure. We introduce three approaches to solve this problem: block singular
value decomposition, nuclear norm penalization, and non-convex low-rank
factorization. While prior works have studied low-rank matrix completion, we
address the challenges brought by the block-wise missingness and are the first
to investigate the problem in the context of graph learning. We discuss
theoretical properties of the two-step procedure, showing graph selection
consistency of one proposed approach by proving novel L infinity-norm error
bounds for matrix completion with block-missingness. We then investigate the
empirical performance of the proposed methods on simulations and on real-world
data examples, through which we show the efficacy of these methods for
estimating functional connectivity from calcium imaging data.","['Andersen Chang', 'Lili Zheng', 'Genevera I. Allen']","['stat.ME', 'stat.ML']",2022-09-17 08:03:46+00:00
http://arxiv.org/abs/2209.08223v1,Joint Network Topology Inference via a Shared Graphon Model,"We consider the problem of estimating the topology of multiple networks from
nodal observations, where these networks are assumed to be drawn from the same
(unknown) random graph model. We adopt a graphon as our random graph model,
which is a nonparametric model from which graphs of potentially different sizes
can be drawn. The versatility of graphons allows us to tackle the joint
inference problem even for the cases where the graphs to be recovered contain
different number of nodes and lack precise alignment across the graphs. Our
solution is based on combining a maximum likelihood penalty with graphon
estimation schemes and can be used to augment existing network inference
methods. The proposed joint network and graphon estimation is further enhanced
with the introduction of a robust method for noisy graph sampling information.
We validate our proposed approach by comparing its performance against
competing methods in synthetic and real-world datasets.","['Madeline Navarro', 'Santiago Segarra']","['stat.ML', 'cs.LG', 'eess.SP']",2022-09-17 02:38:58+00:00
http://arxiv.org/abs/2209.08173v3,Covariance regression with random forests,"Capturing the conditional covariances or correlations among the elements of a
multivariate response vector based on covariates is important to various fields
including neuroscience, epidemiology and biomedicine. We propose a new method
called Covariance Regression with Random Forests (CovRegRF) to estimate the
covariance matrix of a multivariate response given a set of covariates, using a
random forest framework. Random forest trees are built with a splitting rule
specially designed to maximize the difference between the sample covariance
matrix estimates of the child nodes. We also propose a significance test for
the partial effect of a subset of covariates. We evaluate the performance of
the proposed method and significance test through a simulation study which
shows that the proposed method provides accurate covariance matrix estimates
and that the Type-1 error is well controlled. An application of the proposed
method to thyroid disease data is also presented. CovRegRF is implemented in a
freely available R package on CRAN.","['Cansu Alakus', 'Denis Larocque', 'Aurelie Labbe']","['stat.ME', 'stat.AP', 'stat.ML']",2022-09-16 21:21:18+00:00
http://arxiv.org/abs/2209.08139v5,Sparse high-dimensional linear regression with a partitioned empirical Bayes ECM algorithm,"Bayesian variable selection methods are powerful techniques for fitting and
inferring on sparse high-dimensional linear regression models. However, many
are computationally intensive or require restrictive prior distributions on
model parameters. In this paper, we proposed a computationally efficient and
powerful Bayesian approach for sparse high-dimensional linear regression.
Minimal prior assumptions on the parameters are required through the use of
plug-in empirical Bayes estimates of hyperparameters. Efficient maximum a
posteriori (MAP) estimation is completed through a Parameter-Expanded
Expectation-Conditional-Maximization (PX-ECM) algorithm. The PX-ECM results in
a robust computationally efficient coordinate-wise optimization which -- when
updating the coefficient for a particular predictor -- adjusts for the impact
of other predictor variables. The completion of the E-step uses an approach
motivated by the popular two-group approach to multiple testing. The result is
a PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied to sparse
high-dimensional linear regression, which can be completed using one-at-a-time
or all-at-once type optimization. We compare the empirical properties of PROBE
to comparable approaches with numerous simulation studies and analyses of
cancer cell drug responses. The proposed approach is implemented in the R
package probe.","['Alexander C. McLain', 'Anja Zgodic', 'Howard Bondell']","['stat.ME', 'stat.ML']",2022-09-16 19:15:50+00:00
http://arxiv.org/abs/2209.08037v3,DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization,"The combinatorial problem of learning directed acyclic graphs (DAGs) from
data was recently framed as a purely continuous optimization problem by
leveraging a differentiable acyclicity characterization of DAGs based on the
trace of a matrix exponential function. Existing acyclicity characterizations
are based on the idea that powers of an adjacency matrix contain information
about walks and cycles. In this work, we propose a new acyclicity
characterization based on the log-determinant (log-det) function, which
leverages the nilpotency property of DAGs. To deal with the inherent
asymmetries of a DAG, we relate the domain of our log-det characterization to
the set of $\textit{M-matrices}$, which is a key difference to the classical
log-det function defined over the cone of positive definite matrices. Similar
to acyclicity functions previously proposed, our characterization is also exact
and differentiable. However, when compared to existing characterizations, our
log-det function: (1) Is better at detecting large cycles; (2) Has
better-behaved gradients; and (3) Its runtime is in practice about an order of
magnitude faster. From the optimization side, we drop the typically used
augmented Lagrangian scheme and propose DAGMA ($\textit{DAGs via M-matrices for
Acyclicity}$), a method that resembles the central path for barrier methods.
Each point in the central path of DAGMA is a solution to an unconstrained
problem regularized by our log-det function, then we show that at the limit of
the central path the solution is guaranteed to be a DAG. Finally, we provide
extensive experiments for $\textit{linear}$ and $\textit{nonlinear}$ SEMs and
show that our approach can reach large speed-ups and smaller structural Hamming
distances against state-of-the-art methods. Code implementing the proposed
method is open-source and publicly available at
https://github.com/kevinsbello/dagma.","['Kevin Bello', 'Bryon Aragam', 'Pradeep Ravikumar']","['cs.LG', 'stat.ME', 'stat.ML']",2022-09-16 16:31:27+00:00
http://arxiv.org/abs/2209.08030v2,Detection of Interacting Variables for Generalized Linear Models via Neural Networks,"The quality of generalized linear models (GLMs), frequently used by insurance
companies, depends on the choice of interacting variables. The search for
interactions is time-consuming, especially for data sets with a large number of
variables, depends much on expert judgement of actuaries, and often relies on
visual performance indicators. Therefore, we present an approach to automating
the process of finding interactions that should be added to GLMs to improve
their predictive power. Our approach relies on neural networks and a
model-specific interaction detection method, which is computationally faster
than the traditionally used methods like Friedman H-Statistic or SHAP values.
In numerical studies, we provide the results of our approach on artificially
generated data as well as open-source data.","['Yevhen Havrylenko', 'Julia Heger']","['stat.ML', 'cs.LG', 'stat.AP', '62P05']",2022-09-16 16:16:45+00:00
http://arxiv.org/abs/2209.08005v1,Stability and Generalization for Markov Chain Stochastic Gradient Methods,"Recently there is a large amount of work devoted to the study of Markov chain
stochastic gradient methods (MC-SGMs) which mainly focus on their convergence
analysis for solving minimization problems. In this paper, we provide a
comprehensive generalization analysis of MC-SGMs for both minimization and
minimax problems through the lens of algorithmic stability in the framework of
statistical learning theory. For empirical risk minimization (ERM) problems, we
establish the optimal excess population risk bounds for both smooth and
non-smooth cases by introducing on-average argument stability. For minimax
problems, we develop a quantitative connection between on-average argument
stability and generalization error which extends the existing results for
uniform stability \cite{lei2021stability}. We further develop the first nearly
optimal convergence rates for convex-concave problems both in expectation and
with high probability, which, combined with our stability results, show that
the optimal generalization bounds can be attained for both smooth and
non-smooth cases. To the best of our knowledge, this is the first
generalization analysis of SGMs when the gradients are sampled from a Markov
process.","['Puyu Wang', 'Yunwen Lei', 'Yiming Ying', 'Ding-Xuan Zhou']","['stat.ML', 'cs.LG']",2022-09-16 15:42:51+00:00
http://arxiv.org/abs/2209.08004v2,Robust Inference of Manifold Density and Geometry by Doubly Stochastic Scaling,"The Gaussian kernel and its traditional normalizations (e.g., row-stochastic)
are popular approaches for assessing similarities between data points. Yet,
they can be inaccurate under high-dimensional noise, especially if the noise
magnitude varies considerably across the data, e.g., under heteroskedasticity
or outliers. In this work, we investigate a more robust alternative -- the
doubly stochastic normalization of the Gaussian kernel. We consider a setting
where points are sampled from an unknown density on a low-dimensional manifold
embedded in high-dimensional space and corrupted by possibly strong,
non-identically distributed, sub-Gaussian noise. We establish that the doubly
stochastic affinity matrix and its scaling factors concentrate around certain
population forms, and provide corresponding finite-sample probabilistic error
bounds. We then utilize these results to develop several tools for robust
inference under general high-dimensional noise. First, we derive a robust
density estimator that reliably infers the underlying sampling density and can
substantially outperform the standard kernel density estimator under
heteroskedasticity and outliers. Second, we obtain estimators for the pointwise
noise magnitudes, the pointwise signal magnitudes, and the pairwise Euclidean
distances between clean data points. Lastly, we derive robust graph Laplacian
normalizations that accurately approximate various manifold Laplacians,
including the Laplace Beltrami operator, improving over traditional
normalizations in noisy settings. We exemplify our results in simulations and
on real single-cell RNA-sequencing data. For the latter, we show that in
contrast to traditional methods, our approach is robust to variability in
technical noise levels across cell types.","['Boris Landa', 'Xiuyuan Cheng']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2022-09-16 15:39:11+00:00
http://arxiv.org/abs/2209.07787v2,Double logistic regression approach to biased positive-unlabeled data,"Positive and unlabelled learning is an important problem which arises
naturally in many applications. The significant limitation of almost all
existing methods lies in assuming that the propensity score function is
constant (SCAR assumption), which is unrealistic in many practical situations.
Avoiding this assumption, we consider parametric approach to the problem of
joint estimation of posterior probability and propensity score functions. We
show that under mild assumptions when both functions have the same parametric
form (e.g. logistic with different parameters) the corresponding parameters are
identifiable. Motivated by this, we propose two approaches to their estimation:
joint maximum likelihood method and the second approach based on alternating
maximization of two Fisher consistent expressions. Our experimental results
show that the proposed methods are comparable or better than the existing
methods based on Expectation-Maximisation scheme.","['Konrad Furmańczyk', 'Jan Mielniczuk', 'Wojciech Rejchel', 'Paweł Teisseyre']","['stat.ML', 'cs.LG']",2022-09-16 08:32:53+00:00
http://arxiv.org/abs/2209.07587v2,Theoretical Insight into Batch Normalization: Data Dependant Auto-Tuning of Regularization Rate,"Batch normalization is widely used in deep learning to normalize intermediate
activations. Deep networks suffer from notoriously increased training
complexity, mandating careful initialization of weights, requiring lower
learning rates, etc. These issues have been addressed by Batch Normalization
(\textbf{BN}), by normalizing the inputs of activations to zero mean and unit
standard deviation. Making this batch normalization part of the training
process dramatically accelerates the training process of very deep networks. A
new field of research has been going on to examine the exact theoretical
explanation behind the success of \textbf{BN}. Most of these theoretical
insights attempt to explain the benefits of \textbf{BN} by placing them on its
influence on optimization, weight scale invariance, and regularization. Despite
\textbf{BN} undeniable success in accelerating generalization, the gap of
analytically relating the effect of \textbf{BN} to the regularization parameter
is still missing. This paper aims to bring out the data-dependent auto-tuning
of the regularization parameter by \textbf{BN} with analytical proofs. We have
posed \textbf{BN} as a constrained optimization imposed on non-\textbf{BN}
weights through which we demonstrate its data statistics dependant auto-tuning
of regularization parameter. We have also given analytical proof for its
behavior under a noisy input scenario, which reveals the signal vs. noise
tuning of the regularization parameter. We have also substantiated our claim
with empirical results from the MNIST dataset experiments.","['Lakshmi Annamalai', 'Chetan Singh Thakur']","['stat.ML', 'cs.LG']",2022-09-15 19:51:02+00:00
http://arxiv.org/abs/2209.07508v1,Information Theoretic Measures of Causal Influences during Transient Neural Events,"Transient phenomena play a key role in coordinating brain activity at
multiple scales, however,their underlying mechanisms remain largely unknown. A
key challenge for neural data science is thus to characterize the network
interactions at play during these events. Using the formalism of Structural
Causal Models and their graphical representation, we investigate the
theoretical and empirical properties of Information Theory based causal
strength measures in the context of recurring spontaneous transient events.
After showing the limitations of Transfer Entropy and Dynamic Causal Strength
in such a setting, we introduce a novel measure, relative Dynamic Causal
Strength, and provide theoretical and empirical support for its benefits. These
methods are applied to simulated and experimentally recorded neural time
series, and provide results in agreement with our current understanding of the
underlying brain circuits.","['Kaidi Shao', 'Nikos K. Logothetis', 'Michel Besserve']","['q-bio.NC', 'stat.ME', 'stat.ML']",2022-09-15 17:51:46+00:00
http://arxiv.org/abs/2209.07481v3,Variational Representations of Annealing Paths: Bregman Information under Monotonic Embedding,"Markov Chain Monte Carlo methods for sampling from complex distributions and
estimating normalization constants often simulate samples from a sequence of
intermediate distributions along an annealing path, which bridges between a
tractable initial distribution and a target density of interest. Prior works
have constructed annealing paths using quasi-arithmetic means, and interpreted
the resulting intermediate densities as minimizing an expected divergence to
the endpoints. To analyze these variational representations of annealing paths,
we extend known results showing that the arithmetic mean over arguments
minimizes the expected Bregman divergence to a single representative point. In
particular, we obtain an analogous result for quasi-arithmetic means, when the
inputs to the Bregman divergence are transformed under a monotonic embedding
function. Our analysis highlights the interplay between quasi-arithmetic means,
parametric families, and divergence functionals using the rho-tau
representational Bregman divergence framework, and associates common divergence
functionals with intermediate densities along an annealing path.","['Rob Brekelmans', 'Frank Nielsen']","['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2022-09-15 17:22:04+00:00
http://arxiv.org/abs/2209.07438v3,On the Dissipation of Ideal Hamiltonian Monte Carlo Sampler,"We report on what seems to be an intriguing connection between variable
integration time and partial velocity refreshment of Ideal Hamiltonian Monte
Carlo samplers, both of which can be used for reducing the dissipative behavior
of the dynamics. More concretely, we show that on quadratic potentials,
efficiency can be improved through these means by a $\sqrt{\kappa}$ factor in
Wasserstein-2 distance, compared to classical constant integration time, fully
refreshed HMC. We additionally explore the benefit of randomized integrators
for simulating the Hamiltonian dynamics under higher order regularity
conditions.",['Qijia Jiang'],"['stat.CO', 'stat.ML']",2022-09-15 16:33:54+00:00
http://arxiv.org/abs/2209.07436v2,Statistical process monitoring of artificial neural networks,"The rapid advancement of models based on artificial intelligence demands
innovative monitoring techniques which can operate in real time with low
computational costs. In machine learning, especially if we consider artificial
neural networks (ANNs), the models are often trained in a supervised manner.
Consequently, the learned relationship between the input and the output must
remain valid during the model's deployment. If this stationarity assumption
holds, we can conclude that the ANN provides accurate predictions. Otherwise,
the retraining or rebuilding of the model is required. We propose considering
the latent feature representation of the data (called ""embedding"") generated by
the ANN to determine the time when the data stream starts being nonstationary.
In particular, we monitor embeddings by applying multivariate control charts
based on the data depth calculation and normalized ranks. The performance of
the introduced method is compared with benchmark approaches for various ANN
architectures and different underlying data formats.","['Anna Malinovskaya', 'Pavlo Mozharovskyi', 'Philipp Otto']","['stat.ME', 'cs.LG', 'stat.AP', 'stat.ML']",2022-09-15 16:33:36+00:00
http://arxiv.org/abs/2209.07403v6,Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter,"We study differentially private (DP) stochastic optimization (SO) with loss
functions whose worst-case Lipschitz parameter over all data may be extremely
large or infinite. To date, the vast majority of work on DP SO assumes that the
loss is uniformly Lipschitz continuous (i.e. stochastic gradients are uniformly
bounded) over data. While this assumption is convenient, it often leads to
pessimistic risk bounds. In many practical problems, the worst-case (uniform)
Lipschitz parameter of the loss over all data may be huge due to outliers
and/or heavy-tailed data. In such cases, the risk bounds for DP SO, which scale
with the worst-case Lipschitz parameter, are vacuous. To address these
limitations, we provide improved risk bounds that do not depend on the uniform
Lipschitz parameter. Following a recent line of work [WXDX20, KLZ22], we assume
that stochastic gradients have bounded $k$-th order moments for some $k \geq
2$. Compared with works on uniformly Lipschitz DP SO, our risk bounds scale
with the $k$-th moment instead of the uniform Lipschitz parameter of the loss,
allowing for significantly faster rates in the presence of outliers and/or
heavy-tailed data.
  For smooth convex loss functions, we provide linear-time algorithms with
state-of-the-art excess risk. We complement our excess risk upper bounds with
novel lower bounds. In certain parameter regimes, our linear-time excess risk
bounds are minimax optimal. Second, we provide the first algorithm to handle
non-smooth convex loss functions. To do so, we develop novel algorithmic and
stability-based proof techniques, which we believe will be useful for future
work in obtaining optimal excess risk. Finally, our work is the first to
address non-convex non-uniformly Lipschitz loss functions satisfying the
Proximal-PL inequality; this covers some practical machine learning models. Our
Proximal-PL algorithm has near-optimal excess risk.","['Andrew Lowy', 'Meisam Razaviyayn']","['cs.LG', 'cs.CR', 'math.OC', 'stat.ML']",2022-09-15 16:03:23+00:00
http://arxiv.org/abs/2209.07397v2,From algorithms to action: improving patient care requires causality,"In cancer research there is much interest in building and validating outcome
predicting outcomes to support treatment decisions. However, because most
outcome prediction models are developed and validated without regard to the
causal aspects of treatment decision making, many published outcome prediction
models may cause harm when used for decision making, despite being found
accurate in validation studies. Guidelines on prediction model validation and
the checklist for risk model endorsement by the American Joint Committee on
Cancer do not protect against prediction models that are accurate during
development and validation but harmful when used for decision making. We
explain why this is the case and how to build and validate models that are
useful for decision making.","['Wouter A. C. van Amsterdam', 'Pim A. de Jong', 'Joost J. C. Verhoeff', 'Tim Leiner', 'Rajesh Ranganath']","['cs.LG', 'cs.CY', 'stat.ML']",2022-09-15 15:57:17+00:00
http://arxiv.org/abs/2209.07396v2,Towards Healing the Blindness of Score Matching,"Score-based divergences have been widely used in machine learning and
statistics applications. Despite their empirical success, a blindness problem
has been observed when using these for multi-modal distributions. In this work,
we discuss the blindness problem and propose a new family of divergences that
can mitigate the blindness problem. We illustrate our proposed divergence in
the context of density estimation and report improved performance compared to
traditional approaches.","['Mingtian Zhang', 'Oscar Key', 'Peter Hayes', 'David Barber', 'Brooks Paige', 'François-Xavier Briol']","['stat.ML', 'cs.LG']",2022-09-15 15:56:42+00:00
http://arxiv.org/abs/2209.07370v2,A Geometric Perspective on Variational Autoencoders,"This paper introduces a new interpretation of the Variational Autoencoder
framework by taking a fully geometric point of view. We argue that vanilla VAE
models unveil naturally a Riemannian structure in their latent space and that
taking into consideration those geometrical aspects can lead to better
interpolations and an improved generation procedure. This new proposed sampling
method consists in sampling from the uniform distribution deriving
intrinsically from the learned Riemannian latent space and we show that using
this scheme can make a vanilla VAE competitive and even better than more
advanced versions on several benchmark datasets. Since generative models are
known to be sensitive to the number of training samples we also stress the
method's robustness in the low data regime.","['Clément Chadebec', 'Stéphanie Allassonnière']","['stat.ML', 'cs.LG']",2022-09-15 15:32:43+00:00
http://arxiv.org/abs/2209.07369v1,Adversarially Robust Learning: A Generic Minimax Optimal Learner and Characterization,"We present a minimax optimal learner for the problem of learning predictors
robust to adversarial examples at test-time. Interestingly, we find that this
requires new algorithmic ideas and approaches to adversarially robust learning.
In particular, we show, in a strong negative sense, the suboptimality of the
robust learner proposed by Montasser, Hanneke, and Srebro (2019) and a broader
family of learners we identify as local learners. Our results are enabled by
adopting a global perspective, specifically, through a key technical
contribution: the global one-inclusion graph, which may be of independent
interest, that generalizes the classical one-inclusion graph due to Haussler,
Littlestone, and Warmuth (1994). Finally, as a byproduct, we identify a
dimension characterizing qualitatively and quantitatively what classes of
predictors $\mathcal{H}$ are robustly learnable. This resolves an open problem
due to Montasser et al. (2019), and closes a (potentially) infinite gap between
the established upper and lower bounds on the sample complexity of
adversarially robust learning.","['Omar Montasser', 'Steve Hanneke', 'Nathan Srebro']","['cs.LG', 'stat.ML']",2022-09-15 15:32:42+00:00
http://arxiv.org/abs/2209.07330v4,Best Arm Identification with Contextual Information under a Small Gap,"We study the best-arm identification (BAI) problem with a fixed budget and
contextual (covariate) information. In each round of an adaptive experiment,
after observing contextual information, we choose a treatment arm using past
observations and current context. Our goal is to identify the best treatment
arm, which is a treatment arm with the maximal expected reward marginalized
over the contextual distribution, with a minimal probability of
misidentification. In this study, we consider a class of nonparametric bandit
models that converge to location-shift models when the gaps go to zero. First,
we derive lower bounds of the misidentification probability for a certain class
of strategies and bandit models (probabilistic models of potential outcomes)
under a small-gap regime. A small-gap regime is a situation where gaps of the
expected rewards between the best and suboptimal treatment arms go to zero,
which corresponds to one of the worst cases in identifying the best treatment
arm. We then develop the ``Random Sampling (RS)-Augmented Inverse Probability
weighting (AIPW) strategy,'' which is asymptotically optimal in the sense that
the probability of misidentification under the strategy matches the lower bound
when the budget goes to infinity in the small-gap regime. The RS-AIPW strategy
consists of the RS rule tracking a target sample allocation ratio and the
recommendation rule using the AIPW estimator.","['Masahiro Kato', 'Masaaki Imaizumi', 'Takuya Ishihara', 'Toru Kitagawa']","['cs.LG', 'econ.EM', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2022-09-15 14:38:47+00:00
http://arxiv.org/abs/2209.07303v1,Differentially Private Estimation of Hawkes Process,"Point process models are of great importance in real world applications. In
certain critical applications, estimation of point process models involves
large amounts of sensitive personal data from users. Privacy concerns naturally
arise which have not been addressed in the existing literature. To bridge this
glaring gap, we propose the first general differentially private estimation
procedure for point process models. Specifically, we take the Hawkes process as
an example, and introduce a rigorous definition of differential privacy for
event stream data based on a discretized representation of the Hawkes process.
We then propose two differentially private optimization algorithms, which can
efficiently estimate Hawkes process models with the desired privacy and utility
guarantees under two different settings. Experiments are provided to back up
our theoretical analysis.","['Simiao Zuo', 'Tianyi Liu', 'Tuo Zhao', 'Hongyuan Zha']","['cs.LG', 'cs.CR', 'stat.ML']",2022-09-15 13:59:23+00:00
http://arxiv.org/abs/2209.07230v2,Recovery Guarantees for Distributed-OMP,"We study distributed schemes for high-dimensional sparse linear regression,
based on orthogonal matching pursuit (OMP). Such schemes are particularly
suited for settings where a central fusion center is connected to end machines,
that have both computation and communication limitations. We prove that under
suitable assumptions, distributed-OMP schemes recover the support of the
regression vector with communication per machine linear in its sparsity and
logarithmic in the dimension. Remarkably, this holds even at low
signal-to-noise-ratios, where individual machines are unable to detect the
support. Our simulations show that distributed-OMP schemes are competitive with
more computationally intensive methods, and in some cases even outperform them.","['Chen Amiraz', 'Robert Krauthgamer', 'Boaz Nadler']","['stat.ML', 'cs.LG']",2022-09-15 11:43:33+00:00
http://arxiv.org/abs/2209.07154v2,Risk-aware linear bandits with convex loss,"In decision-making problems such as the multi-armed bandit, an agent learns
sequentially by optimizing a certain feedback. While the mean reward criterion
has been extensively studied, other measures that reflect an aversion to
adverse outcomes, such as mean-variance or conditional value-at-risk (CVaR),
can be of interest for critical applications (healthcare, agriculture).
Algorithms have been proposed for such risk-aware measures under bandit
feedback without contextual information. In this work, we study contextual
bandits where such risk measures can be elicited as linear functions of the
contexts through the minimization of a convex loss. A typical example that fits
within this framework is the expectile measure, which is obtained as the
solution of an asymmetric least-square problem. Using the method of mixtures
for supermartingales, we derive confidence sequences for the estimation of such
risk measures. We then propose an optimistic UCB algorithm to learn optimal
risk-aware actions, with regret guarantees similar to those of generalized
linear bandits. This approach requires solving a convex problem at each round
of the algorithm, which we can relax by allowing only approximated solution
obtained by online gradient descent, at the cost of slightly higher regret. We
conclude by evaluating the resulting algorithms on numerical experiments.","['Patrick Saux', 'Odalric-Ambrym Maillard']","['stat.ML', 'cs.LG']",2022-09-15 09:09:53+00:00
http://arxiv.org/abs/2209.07111v2,$ρ$-GNF: A Copula-based Sensitivity Analysis to Unobserved Confounding Using Normalizing Flows,"We propose a novel sensitivity analysis to unobserved confounding in
observational studies using copulas and normalizing flows. Using the idea of
interventional equivalence of structural causal models, we develop $\rho$-GNF
($\rho$-graphical normalizing flow), where $\rho{\in}[-1,+1]$ is a bounded
sensitivity parameter. This parameter represents the back-door non-causal
association due to unobserved confounding, and which is encoded with a Gaussian
copula. In other words, the $\rho$-GNF enables scholars to estimate the average
causal effect (ACE) as a function of $\rho$, while accounting for various
assumed strengths of the unobserved confounding. The output of the $\rho$-GNF
is what we denote as the $\rho_{curve}$ that provides the bounds for the ACE
given an interval of assumed $\rho$ values. In particular, the $\rho_{curve}$
enables scholars to identify the confounding strength required to nullify the
ACE, similar to other sensitivity analysis methods (e.g., the E-value).
Leveraging on experiments from simulated and real-world data, we show the
benefits of $\rho$-GNF. One benefit is that the $\rho$-GNF uses a Gaussian
copula to encode the distribution of the unobserved causes, which is commonly
used in many applied settings. This distributional assumption produces narrower
ACE bounds compared to other popular sensitivity analysis methods.","['Sourabh Balgi', 'Jose M. Peña', 'Adel Daoud']","['stat.ME', 'cs.AI', 'econ.EM', 'stat.ML']",2022-09-15 07:49:23+00:00
http://arxiv.org/abs/2209.07070v1,Fixed-Point Centrality for Networks,"This paper proposes a family of network centralities called fixed-point
centralities. This centrality family is defined via the fixed point of
permutation equivariant mappings related to the underlying network. Such a
centrality notion is immediately extended to define fixed-point centralities
for infinite graphs characterized by graphons. Variation bounds of such
centralities with respect to the variations of the underlying graphs and
graphons under mild assumptions are established. Fixed-point centralities
connect with a variety of different models on networks including graph neural
networks, static and dynamic games on networks, and Markov decision processes.",['Shuang Gao'],"['eess.SY', 'cs.SI', 'cs.SY', 'stat.ML']",2022-09-15 06:01:12+00:00
http://arxiv.org/abs/2209.07067v4,Efficient learning of nonlinear prediction models with time-series privileged information,"In domains where sample sizes are limited, efficient learning algorithms are
critical. Learning using privileged information (LuPI) offers increased sample
efficiency by allowing prediction models access to auxiliary information at
training time which is unavailable when the models are used. In recent work, it
was shown that for prediction in linear-Gaussian dynamical systems, a LuPI
learner with access to intermediate time series data is never worse and often
better in expectation than any unbiased classical learner. We provide new
insights into this analysis and generalize it to nonlinear prediction tasks in
latent dynamical systems, extending theoretical guarantees to the case where
the map connecting latent variables and observations is known up to a linear
transform. In addition, we propose algorithms based on random features and
representation learning for the case when this map is unknown. A suite of
empirical results confirm theoretical findings and show the potential of using
privileged time-series information in nonlinear prediction.","['Bastian Jung', 'Fredrik D Johansson']","['cs.LG', 'stat.ML']",2022-09-15 05:56:36+00:00
http://arxiv.org/abs/2209.07036v2,Langevin Autoencoders for Learning Deep Latent Variable Models,"Markov chain Monte Carlo (MCMC), such as Langevin dynamics, is valid for
approximating intractable distributions. However, its usage is limited in the
context of deep latent variable models owing to costly datapoint-wise sampling
iterations and slow convergence. This paper proposes the amortized Langevin
dynamics (ALD), wherein datapoint-wise MCMC iterations are entirely replaced
with updates of an encoder that maps observations into latent variables. This
amortization enables efficient posterior sampling without datapoint-wise
iterations. Despite its efficiency, we prove that ALD is valid as an MCMC
algorithm, whose Markov chain has the target posterior as a stationary
distribution under mild assumptions. Based on the ALD, we also present a new
deep latent variable model named the Langevin autoencoder (LAE). Interestingly,
the LAE can be implemented by slightly modifying the traditional autoencoder.
Using multiple synthetic datasets, we first validate that ALD can properly
obtain samples from target posteriors. We also evaluate the LAE on the image
generation task, and show that our LAE can outperform existing methods based on
variational inference, such as the variational autoencoder, and other
MCMC-based methods in terms of the test likelihood.","['Shohei Taniguchi', 'Yusuke Iwasawa', 'Wataru Kumagai', 'Yutaka Matsuo']","['cs.LG', 'stat.ML']",2022-09-15 04:26:22+00:00
http://arxiv.org/abs/2209.07028v4,Estimating large causal polytrees from small samples,"We consider the problem of estimating a large causal polytree from a
relatively small i.i.d. sample. This is motivated by the problem of determining
causal structure when the number of variables is very large compared to the
sample size, such as in gene regulatory networks. We give an algorithm that
recovers the tree with high accuracy in such settings. The algorithm works
under essentially no distributional or modeling assumptions other than some
mild non-degeneracy conditions.","['Sourav Chatterjee', 'Mathukumalli Vidyasagar']","['stat.ME', 'cs.LG', 'math.PR', 'math.ST', 'stat.ML', 'stat.TH', '62D20']",2022-09-15 03:41:09+00:00
http://arxiv.org/abs/2209.07015v2,Upper bounds on the Natarajan dimensions of some function classes,"The Natarajan dimension is a fundamental tool for characterizing multi-class
PAC learnability, generalizing the Vapnik-Chervonenkis (VC) dimension from
binary to multi-class classification problems. This work establishes upper
bounds on Natarajan dimensions for certain function classes, including (i)
multi-class decision tree and random forests, and (ii) multi-class neural
networks with binary, linear and ReLU activations. These results may be
relevant for describing the performance of certain multi-class learning
algorithms.",['Ying Jin'],"['stat.ML', 'cs.LG']",2022-09-15 03:10:42+00:00
http://arxiv.org/abs/2209.07011v3,Error Controlled Feature Selection for Ultrahigh Dimensional and Highly Correlated Feature Space Using Deep Learning,"In recent years, deep learning has been at the center of analytics due to its
impressive empirical success in analyzing complex data objects. Despite this
success, most of the existing tools behave like black-box machines, thus the
increasing interest in interpretable, reliable, and robust deep learning models
applicable to a broad class of applications. Feature-selected deep learning has
emerged as a promising tool in this realm. However, the recent developments do
not accommodate ultra-high dimensional and highly correlated features, in
addition to the high noise level. In this article, we propose a novel screening
and cleaning method with the aid of deep learning for a data-adaptive
multi-resolutional discovery of highly correlated predictors with a controlled
error rate. Extensive empirical evaluations over a wide range of simulated
scenarios and several real datasets demonstrate the effectiveness of the
proposed method in achieving high power while keeping the false discovery rate
at a minimum.","['Arkaprabha Ganguli', 'David Todem', 'Tapabrata Maiti']","['stat.ML', 'cs.LG']",2022-09-15 02:58:42+00:00
http://arxiv.org/abs/2209.09090v2,Uncertainty-aware Efficient Subgraph Isomorphism using Graph Topology,"Subgraph isomorphism or subgraph matching is generally considered as an
NP-complete problem, made more complex in practical applications where the edge
weights take real values and are subject to measurement noise and possible
anomalies. To the best of our knowledge, almost all subgraph matching methods
utilize node labels to perform node-node matching. In the absence of such
labels (in applications such as image matching and map matching among others),
these subgraph matching methods do not work. We propose a method for
identifying the node correspondence between a subgraph and a full graph in the
inexact case without node labels in two steps - (a) extract the minimal unique
topology preserving subset from the subgraph and find its feasible matching in
the full graph, and (b) implement a consensus-based algorithm to expand the
matched node set by pairing unique paths based on boundary commutativity. Going
beyond the existing subgraph matching approaches, the proposed method is shown
to have realistically sub-linear computational efficiency, robustness to random
measurement noise, and good statistical properties. Our method is also readily
applicable to the exact matching case without loss of generality. To
demonstrate the effectiveness of the proposed method, a simulation and a case
study is performed on the Erdos-Renyi random graphs and the image-based affine
covariant features dataset respectively.","['Arpan Kusari', 'Wenbo Sun']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']",2022-09-15 02:45:05+00:00
http://arxiv.org/abs/2209.06998v1,Stochastic Tree Ensembles for Estimating Heterogeneous Effects,"Determining subgroups that respond especially well (or poorly) to specific
interventions (medical or policy) requires new supervised learning methods
tailored specifically for causal inference. Bayesian Causal Forest (BCF) is a
recent method that has been documented to perform well on data generating
processes with strong confounding of the sort that is plausible in many
applications. This paper develops a novel algorithm for fitting the BCF model,
which is more efficient than the previously available Gibbs sampler. The new
algorithm can be used to initialize independent chains of the existing Gibbs
sampler leading to better posterior exploration and coverage of the associated
interval estimates in simulation studies. The new algorithm is compared to
related approaches via simulation studies as well as an empirical analysis.","['Nikolay Krantsevich', 'Jingyu He', 'P. Richard Hahn']","['stat.ML', 'cs.LG']",2022-09-15 01:58:03+00:00
http://arxiv.org/abs/2209.06983v2,Double Doubly Robust Thompson Sampling for Generalized Linear Contextual Bandits,"We propose a novel contextual bandit algorithm for generalized linear rewards
with an $\tilde{O}(\sqrt{\kappa^{-1} \phi T})$ regret over $T$ rounds where
$\phi$ is the minimum eigenvalue of the covariance of contexts and $\kappa$ is
a lower bound of the variance of rewards. In several practical cases where
$\phi=O(d)$, our result is the first regret bound for generalized linear model
(GLM) bandits with the order $\sqrt{d}$ without relying on the approach of Auer
[2002]. We achieve this bound using a novel estimator called double
doubly-robust (DDR) estimator, a subclass of doubly-robust (DR) estimator but
with a tighter error bound. The approach of Auer [2002] achieves independence
by discarding the observed rewards, whereas our algorithm achieves independence
considering all contexts using our DDR estimator. We also provide an
$O(\kappa^{-1} \phi \log (NT) \log T)$ regret bound for $N$ arms under a
probabilistic margin condition. Regret bounds under the margin condition are
given by Bastani and Bayati [2020] and Bastani et al. [2021] under the setting
that contexts are common to all arms but coefficients are arm-specific. When
contexts are different for all arms but coefficients are common, ours is the
first regret bound under the margin condition for linear models or GLMs. We
conduct empirical studies using synthetic data and real examples, demonstrating
the effectiveness of our algorithm.","['Wonyoung Kim', 'Kyungbok Lee', 'Myunghee Cho Paik']","['stat.ML', 'cs.LG']",2022-09-15 00:20:38+00:00
http://arxiv.org/abs/2209.06975v2,Wasserstein $K$-means for clustering probability distributions,"Clustering is an important exploratory data analysis technique to group
objects based on their similarity. The widely used $K$-means clustering method
relies on some notion of distance to partition data into a fewer number of
groups. In the Euclidean space, centroid-based and distance-based formulations
of the $K$-means are equivalent. In modern machine learning applications, data
often arise as probability distributions and a natural generalization to handle
measure-valued data is to use the optimal transport metric. Due to non-negative
Alexandrov curvature of the Wasserstein space, barycenters suffer from
regularity and non-robustness issues. The peculiar behaviors of Wasserstein
barycenters may make the centroid-based formulation fail to represent the
within-cluster data points, while the more direct distance-based $K$-means
approach and its semidefinite program (SDP) relaxation are capable of
recovering the true cluster labels. In the special case of clustering Gaussian
distributions, we show that the SDP relaxed Wasserstein $K$-means can achieve
exact recovery given the clusters are well-separated under the $2$-Wasserstein
metric. Our simulation and real data examples also demonstrate that
distance-based $K$-means can achieve better classification performance over the
standard centroid-based $K$-means for clustering probability distributions and
images.","['Yubo Zhuang', 'Xiaohui Chen', 'Yun Yang']","['stat.ML', 'cs.LG']",2022-09-14 23:43:16+00:00
http://arxiv.org/abs/2209.06950v8,Lossy Image Compression with Conditional Diffusion Models,"This paper outlines an end-to-end optimized lossy image compression framework
using diffusion generative models. The approach relies on the transform coding
paradigm, where an image is mapped into a latent space for entropy coding and,
from there, mapped back to the data space for reconstruction. In contrast to
VAE-based neural compression, where the (mean) decoder is a deterministic
neural network, our decoder is a conditional diffusion model. Our approach thus
introduces an additional ``content'' latent variable on which the reverse
diffusion process is conditioned and uses this variable to store information
about the image. The remaining ``texture'' variables characterizing the
diffusion process are synthesized at decoding time. We show that the model's
performance can be tuned toward perceptual metrics of interest. Our extensive
experiments involving multiple datasets and image quality assessment metrics
show that our approach yields stronger reported FID scores than the GAN-based
model, while also yielding competitive performance with VAE-based models in
several distortion metrics. Furthermore, training the diffusion with
$\mathcal{X}$-parameterization enables high-quality reconstructions in only a
handful of decoding steps, greatly affecting the model's practicality. Our code
is available at: \url{https://github.com/buggyyang/CDC_compression}","['Ruihan Yang', 'Stephan Mandt']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2022-09-14 21:53:27+00:00
http://arxiv.org/abs/2209.06931v1,Robust Transferable Feature Extractors: Learning to Defend Pre-Trained Networks Against White Box Adversaries,"The widespread adoption of deep neural networks in computer vision
applications has brought forth a significant interest in adversarial
robustness. Existing research has shown that maliciously perturbed inputs
specifically tailored for a given model (i.e., adversarial examples) can be
successfully transferred to another independently trained model to induce
prediction errors. Moreover, this property of adversarial examples has been
attributed to features derived from predictive patterns in the data
distribution. Thus, we are motivated to investigate the following question: Can
adversarial defenses, like adversarial examples, be successfully transferred to
other independently trained models? To this end, we propose a deep
learning-based pre-processing mechanism, which we refer to as a robust
transferable feature extractor (RTFE). After examining theoretical motivation
and implications, we experimentally show that our method can provide
adversarial robustness to multiple independently pre-trained classifiers that
are otherwise ineffective against an adaptive white box adversary. Furthermore,
we show that RTFEs can even provide one-shot adversarial robustness to models
independently trained on different datasets.","['Alexander Cann', 'Ian Colbert', 'Ihab Amer']","['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML']",2022-09-14 21:09:34+00:00
http://arxiv.org/abs/2209.06928v2,Limit Cycles of AdaBoost,"The iterative weight update for the AdaBoost machine learning algorithm may
be realized as a dynamical map on a probability simplex. When learning a
low-dimensional data set this algorithm has a tendency towards cycling
behavior, which is the topic of this paper. AdaBoost's cycling behavior lends
itself to direct computational methods that are ineffective in the general,
non-cycling case of the algorithm. From these computational properties we give
a concrete correspondence between AdaBoost's cycling behavior and continued
fractions dynamics. Then we explore the results of this correspondence to
expound on how the algorithm comes to be in this periodic state at all. What we
intend for this work is to be a novel and self-contained explanation for the
cycling dynamics of this machine learning algorithm.",['Conor Snedeker'],"['cs.LG', 'math.DS', 'stat.ML']",2022-09-14 21:02:42+00:00
