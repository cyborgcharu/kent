id,title,abstract,authors,categories,date
http://arxiv.org/abs/1906.03737v2,Factorization Bandits for Online Influence Maximization,"We study the problem of online influence maximization in social networks. In
this problem, a learner aims to identify the set of ""best influencers"" in a
network by interacting with it, i.e., repeatedly selecting seed nodes and
observing activation feedback in the network. We capitalize on an important
property of the influence maximization problem named network assortativity,
which is ignored by most existing works in online influence maximization. To
realize network assortativity, we factorize the activation probability on the
edges into latent factors on the corresponding nodes, including influence
factor on the giving nodes and susceptibility factor on the receiving nodes. We
propose an upper confidence bound based online learning solution to estimate
the latent factors, and therefore the activation probabilities. Considerable
regret reduction is achieved by our factorization based online influence
maximization algorithm. And extensive empirical evaluations on two real-world
networks showed the effectiveness of our proposed solution.","['Qingyun Wu', 'Zhige Li', 'Huazheng Wang', 'Wei Chen', 'Hongning Wang']","['cs.LG', 'cs.SI', 'stat.ML']",2019-06-09 23:43:03+00:00
http://arxiv.org/abs/1906.03735v1,"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning","Off-policy evaluation (OPE) in both contextual bandits and reinforcement
learning allows one to evaluate novel decision policies without needing to
conduct exploration, which is often costly or otherwise infeasible. The
problem's importance has attracted many proposed solutions, including
importance sampling (IS), self-normalized IS (SNIS), and doubly robust (DR)
estimates. DR and its variants ensure semiparametric local efficiency if
Q-functions are well-specified, but if they are not they can be worse than both
IS and SNIS. It also does not enjoy SNIS's inherent stability and boundedness.
We propose new estimators for OPE based on empirical likelihood that are always
more efficient than IS, SNIS, and DR and satisfy the same stability and
boundedness properties as SNIS. On the way, we categorize various properties
and classify existing estimators by them. Besides the theoretical guarantees,
empirical studies suggest the new estimators provide advantages.","['Nathan Kallus', 'Masatoshi Uehara']","['cs.LG', 'stat.ML']",2019-06-09 23:15:49+00:00
http://arxiv.org/abs/1906.03728v4,The Generalization-Stability Tradeoff In Neural Network Pruning,"Pruning neural network parameters is often viewed as a means to compress
models, but pruning has also been motivated by the desire to prevent
overfitting. This motivation is particularly relevant given the perhaps
surprising observation that a wide variety of pruning approaches increase test
accuracy despite sometimes massive reductions in parameter counts. To better
understand this phenomenon, we analyze the behavior of pruning over the course
of training, finding that pruning's benefit to generalization increases with
pruning's instability (defined as the drop in test accuracy immediately
following pruning). We demonstrate that this ""generalization-stability
tradeoff"" is present across a wide variety of pruning settings and propose a
mechanism for its cause: pruning regularizes similarly to noise injection.
Supporting this, we find less pruning stability leads to more model flatness
and the benefits of pruning do not depend on permanent parameter removal. These
results explain the compatibility of pruning-based generalization improvements
and the high generalization recently observed in overparameterized networks.","['Brian R. Bartoldson', 'Ari S. Morcos', 'Adrian Barbu', 'Gordon Erlebacher']","['cs.LG', 'stat.ML']",2019-06-09 22:35:00+00:00
http://arxiv.org/abs/1906.03722v1,Integrative Factorization of Bidimensionally Linked Matrices,"Advances in molecular ""omics'"" technologies have motivated new methodology
for the integration of multiple sources of high-content biomedical data.
However, most statistical methods for integrating multiple data matrices only
consider data shared vertically (one cohort on multiple platforms) or
horizontally (different cohorts on a single platform). This is limiting for
data that take the form of bidimensionally linked matrices (e.g., multiple
cohorts measured on multiple platforms), which are increasingly common in
large-scale biomedical studies. In this paper, we propose BIDIFAC
(Bidimensional Integrative Factorization) for integrative dimension reduction
and signal approximation of bidimensionally linked data matrices. Our method
factorizes the data into (i) globally shared, (ii) row-shared, (iii)
column-shared, and (iv) single-matrix structural components, facilitating the
investigation of shared and unique patterns of variability. For estimation we
use a penalized objective function that extends the nuclear norm penalization
for a single matrix. As an alternative to the complicated rank selection
problem, we use results from random matrix theory to choose tuning parameters.
We apply our method to integrate two genomics platforms (mRNA and miRNA
expression) across two sample cohorts (tumor samples and normal tissue samples)
using the breast cancer data from TCGA. We provide R code for fitting BIDIFAC,
imputing missing values, and generating simulated data.","['Jun Young Park', 'Eric F. Lock']","['stat.ML', 'cs.LG', 'q-bio.QM', 'stat.ME']",2019-06-09 21:50:19+00:00
http://arxiv.org/abs/1906.03711v1,Aggregation of pairwise comparisons with reduction of biases,"We study the problem of ranking from crowdsourced pairwise comparisons.
Answers to pairwise tasks are known to be affected by the position of items on
the screen, however, previous models for aggregation of pairwise comparisons do
not focus on modeling such kind of biases. We introduce a new aggregation model
factorBT for pairwise comparisons, which accounts for certain factors of
pairwise tasks that are known to be irrelevant to the result of comparisons but
may affect workers' answers due to perceptual reasons. By modeling biases that
influence workers, factorBT is able to reduce the effect of biased pairwise
comparisons on the resulted ranking. Our empirical studies on real-world data
sets showed that factorBT produces more accurate ranking from crowdsourced
pairwise comparisons than previously established models.","['Nadezhda Bugakova', 'Valentina Fedorova', 'Gleb Gusev', 'Alexey Drutsa']","['cs.LG', 'stat.ML']",2019-06-09 21:12:43+00:00
http://arxiv.org/abs/1906.03710v1,Curiosity-Driven Multi-Criteria Hindsight Experience Replay,"Dealing with sparse rewards is a longstanding challenge in reinforcement
learning. The recent use of hindsight methods have achieved success on a
variety of sparse-reward tasks, but they fail on complex tasks such as stacking
multiple blocks with a robot arm in simulation. Curiosity-driven exploration
using the prediction error of a learned dynamics model as an intrinsic reward
has been shown to be effective for exploring a number of sparse-reward
environments. We present a method that combines hindsight with curiosity-driven
exploration and curriculum learning in order to solve the challenging
sparse-reward block stacking task. We are the first to stack more than two
blocks using only sparse reward without human demonstrations.","['John B. Lanier', 'Stephen McAleer', 'Pierre Baldi']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2019-06-09 21:11:08+00:00
http://arxiv.org/abs/1906.03708v1,Note on the bias and variance of variational inference,"In this note, we study the relationship between the variational gap and the
variance of the (log) likelihood ratio. We show that the gap can be upper
bounded by some form of dispersion measure of the likelihood ratio, which
suggests the bias of variational inference can be reduced by making the
distribution of the likelihood ratio more concentrated, such as via averaging
and variance reduction.","['Chin-Wei Huang', 'Aaron Courville']","['cs.LG', 'stat.ML']",2019-06-09 21:08:35+00:00
http://arxiv.org/abs/1906.03707v1,Redundancy-Free Computation Graphs for Graph Neural Networks,"Graph Neural Networks (GNNs) are based on repeated aggregations of
information across nodes' neighbors in a graph. However, because common
neighbors are shared between different nodes, this leads to repeated and
inefficient computations. We propose Hierarchically Aggregated computation
Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy
by managing intermediate aggregation results hierarchically, eliminating
repeated computations and unnecessary data transfers in GNN training and
inference. We introduce an accurate cost function to quantitatively evaluate
the runtime performance of different HAGs and use a novel HAG search algorithm
to find optimized HAGs. Experiments show that the HAG representation
significantly outperforms the standard GNN graph representation by increasing
the end-to-end training throughput by up to 2.8x and reducing the aggregations
and data transfers in GNN training by up to 6.3x and 5.6x, while maintaining
the original model accuracy.","['Zhihao Jia', 'Sina Lin', 'Rex Ying', 'Jiaxuan You', 'Jure Leskovec', 'Alex Aiken']","['cs.LG', 'cs.SI', 'stat.ML']",2019-06-09 21:06:28+00:00
http://arxiv.org/abs/1906.03704v2,SVRG for Policy Evaluation with Fewer Gradient Evaluations,"Stochastic variance-reduced gradient (SVRG) is an optimization method
originally designed for tackling machine learning problems with a finite sum
structure. SVRG was later shown to work for policy evaluation, a problem in
reinforcement learning in which one aims to estimate the value function of a
given policy. SVRG makes use of gradient estimates at two scales. At the slower
scale, SVRG computes a full gradient over the whole dataset, which could lead
to prohibitive computation costs. In this work, we show that two variants of
SVRG for policy evaluation could significantly diminish the number of gradient
calculations while preserving a linear convergence speed. More importantly, our
theoretical result implies that one does not need to use the entire dataset in
every epoch of SVRG when it is applied to policy evaluation with linear
function approximation. Our experiments demonstrate large computational savings
provided by the proposed methods.","['Zilun Peng', 'Ahmed Touati', 'Pascal Vincent', 'Doina Precup']","['cs.LG', 'stat.ML']",2019-06-09 20:59:02+00:00
http://arxiv.org/abs/1906.03700v4,Solving general elliptical mixture models through an approximate Wasserstein manifold,"We address the estimation problem for general finite mixture models, with a
particular focus on the elliptical mixture models (EMMs). Compared to the
widely adopted Kullback-Leibler divergence, we show that the Wasserstein
distance provides a more desirable optimisation space. We thus provide a stable
solution to the EMMs that is both robust to initialisations and reaches a
superior optimum by adaptively optimising along a manifold of an approximate
Wasserstein distance. To this end, we first provide a unifying account of
computable and identifiable EMMs, which serves as a basis to rigorously address
the underpinning optimisation problem. Due to a probability constraint, solving
this problem is extremely cumbersome and unstable, especially under the
Wasserstein distance. To relieve this issue, we introduce an efficient
optimisation method on a statistical manifold defined under an approximate
Wasserstein distance, which allows for explicit metrics and computable
operations, thus significantly stabilising and improving the EMM estimation. We
further propose an adaptive method to accelerate the convergence. Experimental
results demonstrate the excellent performance of the proposed EMM solver.","['Shengxi Li', 'Zeyang Yu', 'Min Xiang', 'Danilo Mandic']","['cs.LG', 'stat.ML']",2019-06-09 20:04:59+00:00
http://arxiv.org/abs/1906.03694v4,Balanced off-policy evaluation in general action spaces,"Estimation of importance sampling weights for off-policy evaluation of
contextual bandits often results in imbalance - a mismatch between the desired
and the actual distribution of state-action pairs after weighting. In this work
we present balanced off-policy evaluation (B-OPE), a generic method for
estimating weights which minimize this imbalance. Estimation of these weights
reduces to a binary classification problem regardless of action type. We show
that minimizing the risk of the classifier implies minimization of imbalance to
the desired counterfactual distribution of state-action pairs. The classifier
loss is tied to the error of the off-policy estimate, allowing for easy tuning
of hyperparameters. We provide experimental evidence that B-OPE improves
weighting-based approaches for offline policy evaluation in both discrete and
continuous action spaces.","['Arjun Sondhi', 'David Arbour', 'Drew Dimmery']","['cs.LG', 'stat.ME', 'stat.ML']",2019-06-09 19:25:17+00:00
http://arxiv.org/abs/1906.04586v1,Proposition d'une nouvelle approche d'extraction des motifs fermés fréquents,"This work is done as part of a master's thesis project. The increase in the
volume of data has given rise to various issues related to the collection,
storage, analysis and exploitation of these data in order to create an added
value. In this master, we are interested in the search of frequent closed
patterns in the transaction bases. One way to process data is to partition the
search space into subcontexts, and then explore the subcontexts simultaneously.
In this context, we have proposed a new approach for extracting frequent closed
itemsets. The main idea is to update frequent closed patterns with their
minimal generators by applying a strategy of partitioning of the initial
extraction context. Our new approach called UFCIGs-DAC was designed and
implemented to perform a search in the test bases. The main originality of this
approach is the simultaneous exploration of the research space by the update of
the frequent closed patterns and the minimal generators. Moreover, our approach
can be adapted to any algorithm of extraction of the frequent closed patterns
with their minimal generators.",['Ons Khemiri'],"['cs.LG', 'stat.ML']",2019-06-09 19:07:37+00:00
http://arxiv.org/abs/1906.03691v1,Interpreting Age Effects of Human Fetal Brain from Spontaneous fMRI using Deep 3D Convolutional Neural Networks,"Understanding human fetal neurodevelopment is of great clinical importance as
abnormal development is linked to adverse neuropsychiatric outcomes after
birth. Recent advances in functional Magnetic Resonance Imaging (fMRI) have
provided new insight into development of the human brain before birth, but
these studies have predominately focused on brain functional connectivity (i.e.
Fisher z-score), which requires manual processing steps for feature extraction
from fMRI images. Deep learning approaches (i.e., Convolutional Neural
Networks) have achieved remarkable success on learning directly from image
data, yet have not been applied on fetal fMRI for understanding fetal
neurodevelopment. Here, we bridge this gap by applying a novel application of
deep 3D CNN to fetal blood oxygen-level dependence (BOLD) resting-state fMRI
data. Specifically, we test a supervised CNN framework as a data-driven
approach to isolate variation in fMRI signals that relate to younger v.s. older
fetal age groups. Based on the learned CNN, we further perform sensitivity
analysis to identify brain regions in which changes in BOLD signal are strongly
associated with fetal brain age. The findings demonstrate that deep CNNs are a
promising approach for identifying spontaneous functional patterns in fetal
brain activity that discriminate age groups. Further, we discovered that
regions that most strongly differentiate groups are largely bilateral, share
similar distribution in older and younger age groups, and are areas of
heightened metabolic activity in early human development.","['Xiangrui Li', 'Jasmine Hect', 'Moriah Thomason', 'Dongxiao Zhu']","['eess.IV', 'cs.LG', 'stat.ML']",2019-06-09 19:00:50+00:00
http://arxiv.org/abs/1906.03685v1,Novelty Detection via Network Saliency in Visual-based Deep Learning,"Machine-learning driven safety-critical autonomous systems, such as
self-driving cars, must be able to detect situations where its trained model is
not able to make a trustworthy prediction. Often viewed as a black-box, it is
non-obvious to determine when a model will make a safe decision and when it
will make an erroneous, perhaps life-threatening one. Prior work on novelty
detection deal with highly structured data and do not translate well to
dynamic, real-world situations. This paper proposes a multi-step framework for
the detection of novel scenarios in vision-based autonomous systems by
leveraging information learned by the trained prediction model and a new image
similarity metric. We demonstrate the efficacy of this method through
experiments on a real-world driving dataset as well as on our in-house indoor
racing environment.","['Valerie Chen', 'Man-Ki Yoon', 'Zhong Shao']","['cs.LG', 'cs.CV', 'stat.ML']",2019-06-09 18:23:57+00:00
http://arxiv.org/abs/1906.03674v1,Attention-based Conditioning Methods for External Knowledge Integration,"In this paper, we present a novel approach for incorporating external
knowledge in Recurrent Neural Networks (RNNs). We propose the integration of
lexicon features into the self-attention mechanism of RNN-based architectures.
This form of conditioning on the attention distribution, enforces the
contribution of the most salient words for the task at hand. We introduce three
methods, namely attentional concatenation, feature-based gating and affine
transformation. Experiments on six benchmark datasets show the effectiveness of
our methods. Attentional feature-based gating yields consistent performance
improvement across tasks. Our approach is implemented as a simple add-on module
for RNN-based models with minimal computational overhead and can be adapted to
any deep neural architecture.","['Katerina Margatina', 'Christos Baziotis', 'Alexandros Potamianos']","['cs.LG', 'cs.CL', 'stat.ML']",2019-06-09 17:06:28+00:00
http://arxiv.org/abs/1906.03671v2,"Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds","We design a new algorithm for batch active learning with deep neural network
models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings
(BADGE), samples groups of points that are disparate and high-magnitude when
represented in a hallucinated gradient space, a strategy designed to
incorporate both predictive uncertainty and sample diversity into every
selected batch. Crucially, BADGE trades off between diversity and uncertainty
without requiring any hand-tuned hyperparameters. We show that while other
approaches sometimes succeed for particular batch sizes or architectures, BADGE
consistently performs as well or better, making it a versatile option for
practical active learning problems.","['Jordan T. Ash', 'Chicheng Zhang', 'Akshay Krishnamurthy', 'John Langford', 'Alekh Agarwal']","['cs.LG', 'stat.ML']",2019-06-09 16:52:09+00:00
http://arxiv.org/abs/1906.03667v1,Understanding overfitting peaks in generalization error: Analytical risk curves for $l_2$ and $l_1$ penalized interpolation,"Traditionally in regression one minimizes the number of fitting parameters or
uses smoothing/regularization to trade training (TE) and generalization error
(GE). Driving TE to zero by increasing fitting degrees of freedom (dof) is
expected to increase GE. However modern big-data approaches, including deep
nets, seem to over-parametrize and send TE to zero (data interpolation) without
impacting GE. Overparametrization has the benefit that global minima of the
empirical loss function proliferate and become easier to find. These phenomena
have drawn theoretical attention. Regression and classification algorithms have
been shown that interpolate data but also generalize optimally. An interesting
related phenomenon has been noted: the existence of non-monotonic risk curves,
with a peak in GE with increasing dof. It was suggested that this peak
separates a classical regime from a modern regime where over-parametrization
improves performance. Similar over-fitting peaks were reported previously
(statistical physics approach to learning) and attributed to increased fitting
model flexibility. We introduce a generative and fitting model pair
(""Misparametrized Sparse Regression"" or MiSpaR) and show that the overfitting
peak can be dissociated from the point at which the fitting function gains
enough dof's to match the data generative model and thus provides good
generalization. This complicates the interpretation of overfitting peaks as
separating a ""classical"" from a ""modern"" regime. Data interpolation itself
cannot guarantee good generalization: we need to study the interpolation with
different penalty terms. We present analytical formulae for GE curves for
MiSpaR with $l_2$ and $l_1$ penalties, in the interpolating limit
$\lambda\rightarrow 0$.These risk curves exhibit important differences and help
elucidate the underlying phenomena.",['Partha P Mitra'],"['cs.LG', 'physics.data-an', 'stat.ML']",2019-06-09 16:40:25+00:00
http://arxiv.org/abs/1906.04585v2,Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning,"Multi-simulator training has contributed to the recent success of Deep
Reinforcement Learning by stabilizing learning and allowing for higher training
throughputs. We propose Gossip-based Actor-Learner Architectures (GALA) where
several actor-learners (such as A2C agents) are organized in a peer-to-peer
communication topology, and exchange information through asynchronous gossip in
order to take advantage of a large number of distributed simulators. We prove
that GALA agents remain within an epsilon-ball of one-another during training
when using loosely coupled asynchronous communication. By reducing the amount
of synchronization between agents, GALA is more computationally efficient and
scalable compared to A2C, its fully-synchronous counterpart. GALA also
outperforms A2C, being more robust and sample efficient. We show that we can
run several loosely coupled GALA agents in parallel on a single GPU and achieve
significantly higher hardware utilization and frame-rates than vanilla A2C at
comparable power draws.","['Mahmoud Assran', 'Joshua Romoff', 'Nicolas Ballas', 'Joelle Pineau', 'Michael Rabbat']","['cs.LG', 'cs.AI', 'cs.MA', 'math.OC', 'stat.ML']",2019-06-09 16:15:43+00:00
http://arxiv.org/abs/1906.03663v5,Physics-Informed Probabilistic Learning of Linear Embeddings of Non-linear Dynamics With Guaranteed Stability,"The Koopman operator has emerged as a powerful tool for the analysis of
nonlinear dynamical systems as it provides coordinate transformations to
globally linearize the dynamics. While recent deep learning approaches have
been useful in extracting the Koopman operator from a data-driven perspective,
several challenges remain. In this work, we formalize the problem of learning
the continuous-time Koopman operator with deep neural networks in a
measure-theoretic framework. Our approach induces two types of models:
differential and recurrent form, the choice of which depends on the
availability of the governing equations and data. We then enforce a structural
parameterization that renders the realization of the Koopman operator provably
stable. A new autoencoder architecture is constructed, such that only the
residual of the dynamic mode decomposition is learned. Finally, we employ
mean-field variational inference (MFVI) on the aforementioned framework in a
hierarchical Bayesian setting to quantify uncertainties in the characterization
and prediction of the dynamics of observables. The framework is evaluated on a
simple polynomial system, the Duffing oscillator, and an unstable cylinder wake
flow with noisy measurements.","['Shaowu Pan', 'Karthik Duraisamy']","['math.DS', 'stat.ML']",2019-06-09 16:03:45+00:00
http://arxiv.org/abs/1906.04584v5,Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers,"Recent works have shown the effectiveness of randomized smoothing as a
scalable technique for building neural network-based classifiers that are
provably robust to $\ell_2$-norm adversarial perturbations. In this paper, we
employ adversarial training to improve the performance of randomized smoothing.
We design an adapted attack for smoothed classifiers, and we show how this
attack can be used in an adversarial training setting to boost the provable
robustness of smoothed classifiers. We demonstrate through extensive
experimentation that our method consistently outperforms all existing provably
$\ell_2$-robust classifiers by a significant margin on ImageNet and CIFAR-10,
establishing the state-of-the-art for provable $\ell_2$-defenses. Moreover, we
find that pre-training and semi-supervised learning boost adversarially trained
smoothed classifiers even further. Our code and trained models are available at
http://github.com/Hadisalman/smoothing-adversarial .","['Hadi Salman', 'Greg Yang', 'Jerry Li', 'Pengchuan Zhang', 'Huan Zhang', 'Ilya Razenshteyn', 'Sebastien Bubeck']","['cs.LG', 'cs.CR', 'stat.ML']",2019-06-09 15:55:21+00:00
http://arxiv.org/abs/1906.03647v1,A Variant of Gaussian Process Dynamical Systems,"In order to better model high-dimensional sequential data, we propose a
collaborative multi-output Gaussian process dynamical system (CGPDS), which is
a novel variant of GPDSs. The proposed model assumes that the output on each
dimension is controlled by a shared global latent process and a private local
latent process. Thus, the dependence among different dimensions of the
sequences can be captured, and the unique characteristics of each dimension of
the sequences can be maintained. For training models and making prediction, we
introduce inducing points and adopt stochastic variational inference methods.","['Jing Zhao', 'Jingjing Fei', 'Shiliang Sun']","['stat.ML', 'cs.LG']",2019-06-09 14:22:44+00:00
http://arxiv.org/abs/1906.03644v1,The Implicit Metropolis-Hastings Algorithm,"Recent works propose using the discriminator of a GAN to filter out
unrealistic samples of the generator. We generalize these ideas by introducing
the implicit Metropolis-Hastings algorithm. For any implicit probabilistic
model and a target distribution represented by a set of samples, implicit
Metropolis-Hastings operates by learning a discriminator to estimate the
density-ratio and then generating a chain of samples. Since the approximation
of density ratio introduces an error on every step of the chain, it is crucial
to analyze the stationary distribution of such chain. For that purpose, we
present a theoretical result stating that the discriminator loss upper bounds
the total variation distance between the target distribution and the stationary
distribution. Finally, we validate the proposed algorithm both for independent
and Markov proposals on CIFAR-10 and CelebA datasets.","['Kirill Neklyudov', 'Evgenii Egorov', 'Dmitry Vetrov']","['stat.ML', 'cs.LG']",2019-06-09 14:05:30+00:00
http://arxiv.org/abs/1906.03639v1,Consensus Neural Network for Medical Imaging Denoising with Only Noisy Training Samples,"Deep neural networks have been proved efficient for medical image denoising.
Current training methods require both noisy and clean images. However, clean
images cannot be acquired for many practical medical applications due to
naturally noisy signal, such as dynamic imaging, spectral computed tomography,
arterial spin labeling magnetic resonance imaging, etc. In this paper we
proposed a training method which learned denoising neural networks from noisy
training samples only. Training data in the acquisition domain was split to two
subsets and the network was trained to map one noisy set to the other. A
consensus loss function was further proposed to efficiently combine the outputs
from both subsets. A mathematical proof was provided that the proposed training
scheme was equivalent to training with noisy and clean samples when the noise
in the two subsets was uncorrelated and zero-mean. The method was validated on
Low-dose CT Challenge dataset and NYU MRI dataset and achieved improved
performance compared to existing unsupervised methods.","['Dufan Wu', 'Kuang Gong', 'Kyungsang Kim', 'Quanzheng Li']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2019-06-09 13:37:34+00:00
http://arxiv.org/abs/1906.03626v4,Deep Music Analogy Via Latent Representation Disentanglement,"Analogy-making is a key method for computer algorithms to generate both
natural and creative music pieces. In general, an analogy is made by partially
transferring the music abstractions, i.e., high-level representations and their
relationships, from one piece to another; however, this procedure requires
disentangling music representations, which usually takes little effort for
musicians but is non-trivial for computers. Three sub-problems arise:
extracting latent representations from the observation, disentangling the
representations so that each part has a unique semantic interpretation, and
mapping the latent representations back to actual music. In this paper, we
contribute an explicitly-constrained variational autoencoder (EC$^2$-VAE) as a
unified solution to all three sub-problems. We focus on disentangling the pitch
and rhythm representations of 8-beat music clips conditioned on chords. In
producing music analogies, this model helps us to realize the imaginary
situation of ""what if"" a piece is composed using a different pitch contour,
rhythm pattern, or chord progression by borrowing the representations from
other pieces. Finally, we validate the proposed disentanglement method using
objective measurements and evaluate the analogy examples by a subjective study.","['Ruihan Yang', 'Dingsu Wang', 'Ziyu Wang', 'Tianyao Chen', 'Junyan Jiang', 'Gus Xia']","['cs.SD', 'cs.IR', 'cs.LG', 'eess.AS', 'stat.ML']",2019-06-09 12:22:06+00:00
http://arxiv.org/abs/1906.03612v1,On the Vulnerability of Capsule Networks to Adversarial Attacks,"This paper extensively evaluates the vulnerability of capsule networks to
different adversarial attacks. Recent work suggests that these architectures
are more robust towards adversarial attacks than other neural networks.
However, our experiments show that capsule networks can be fooled as easily as
convolutional neural networks.","['Felix Michels', 'Tobias Uelwer', 'Eric Upschulte', 'Stefan Harmeling']","['cs.LG', 'cs.CR', 'stat.ML']",2019-06-09 10:22:52+00:00
http://arxiv.org/abs/1906.03593v2,Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound,"We improve the over-parametrization size over two beautiful results [Li and
Liang' 2018] and [Du, Zhai, Poczos and Singh' 2019] in deep learning theory.","['Zhao Song', 'Xin Yang']","['cs.LG', 'cs.AI', 'stat.ML']",2019-06-09 08:36:35+00:00
http://arxiv.org/abs/1906.03590v1,Region of Attraction for Power Systems using Gaussian Process and Converse Lyapunov Function -- Part I: Theoretical Framework and Off-line Study,"This paper introduces a novel framework to construct the region of attraction
(ROA) of a power system centered around a stable equilibrium by using stable
state trajectories of system dynamics. Most existing works on estimating ROA
rely on analytical Lyapunov functions, which are subject to two limitations:
the analytic Lyapunov functions may not be always readily available, and the
resulting ROA may be overly conservative. This work overcomes these two
limitations by leveraging the converse Lyapunov theorem in control theory to
eliminate the need of an analytic Lyapunov function and learning the unknown
Lyapunov function with the Gaussian Process (GP) approach. In addition, a
Gaussian Process Upper Confidence Bound (GP-UCB) based sampling algorithm is
designed to reconcile the trade-off between the exploitation for enlarging the
ROA and the exploration for reducing the uncertainty of sampling region. Within
the constructed ROA, it is guaranteed in probability that the system state will
converge to the stable equilibrium with a confidence level. Numerical
simulations are also conducted to validate the assessment approach for the ROA
of the single machine infinite bus system and the New England $39$-bus system.
Numerical results demonstrate that our approach can significantly enlarge the
estimated ROA compared to that of the analytic Lyapunov counterpart.","['Chao Zhai', 'Hung D. Nguyen']","['eess.SY', 'cs.LG', 'cs.SY', 'stat.ML']",2019-06-09 08:13:16+00:00
http://arxiv.org/abs/1906.03586v1,Dynamic Network Embedding via Incremental Skip-gram with Negative Sampling,"Network representation learning, as an approach to learn low dimensional
representations of vertices, has attracted considerable research attention
recently. It has been proven extremely useful in many machine learning tasks
over large graph. Most existing methods focus on learning the structural
representations of vertices in a static network, but cannot guarantee an
accurate and efficient embedding in a dynamic network scenario. To address this
issue, we present an efficient incremental skip-gram algorithm with negative
sampling for dynamic network embedding, and provide a set of theoretical
analyses to characterize the performance guarantee. Specifically, we first
partition a dynamic network into the updated, including addition/deletion of
links and vertices, and the retained networks over time. Then we factorize the
objective function of network embedding into the added, vanished and retained
parts of the network. Next we provide a new stochastic gradient-based method,
guided by the partitions of the network, to update the nodes and the parameter
vectors. The proposed algorithm is proven to yield an objective function value
with a bounded difference to that of the original objective function.
Experimental results show that our proposal can significantly reduce the
training time while preserving the comparable performance. We also demonstrate
the correctness of the theoretical analysis and the practical usefulness of the
dynamic network embedding. We perform extensive experiments on multiple
real-world large network datasets over multi-label classification and link
prediction tasks to evaluate the effectiveness and efficiency of the proposed
framework, and up to 22 times speedup has been achieved.","['Hao Peng', 'Jianxin Li', 'Hao Yan', 'Qiran Gong', 'Senzhang Wang', 'Lin Liu', 'Lihong Wang', 'Xiang Ren']","['cs.LG', 'cs.SI', 'stat.ML']",2019-06-09 07:42:39+00:00
http://arxiv.org/abs/1906.04898v1,Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for Large-Scale Multi-Label Text Classification,"CNNs, RNNs, GCNs, and CapsNets have shown significant insights in
representation learning and are widely used in various text mining tasks such
as large-scale multi-label text classification. However, most existing deep
models for multi-label text classification consider either the non-consecutive
and long-distance semantics or the sequential semantics, but how to consider
them both coherently is less studied. In addition, most existing methods treat
output labels as independent methods, but ignore the hierarchical relations
among them, leading to useful semantic information loss. In this paper, we
propose a novel hierarchical taxonomy-aware and attentional graph capsule
recurrent CNNs framework for large-scale multi-label text classification.
Specifically, we first propose to model each document as a word order preserved
graph-of-words and normalize it as a corresponding words-matrix representation
which preserves both the non-consecutive, long-distance and local sequential
semantics. Then the words-matrix is input to the proposed attentional graph
capsule recurrent CNNs for more effectively learning the semantic features. To
leverage the hierarchical relations among the class labels, we propose a
hierarchical taxonomy embedding method to learn their representations, and
define a novel weighted margin loss by incorporating the label representation
similarity. Extensive evaluations on three datasets show that our model
significantly improves the performance of large-scale multi-label text
classification by comparing with state-of-the-art approaches.","['Hao Peng', 'Jianxin Li', 'Qiran Gong', 'Senzhang Wang', 'Lifang He', 'Bo Li', 'Lihong Wang', 'Philip S. Yu']","['cs.IR', 'cs.LG', 'stat.ML']",2019-06-09 07:23:45+00:00
http://arxiv.org/abs/1906.03580v1,Stochastic In-Face Frank-Wolfe Methods for Non-Convex Optimization and Sparse Neural Network Training,"The Frank-Wolfe method and its extensions are well-suited for delivering
solutions with desirable structural properties, such as sparsity or low-rank
structure. We introduce a new variant of the Frank-Wolfe method that combines
Frank-Wolfe steps and steepest descent steps, as well as a novel modification
of the ""Frank-Wolfe gap"" to measure convergence in the non-convex case. We
further extend this method to incorporate in-face directions for preserving
structured solutions as well as block coordinate steps, and we demonstrate
computational guarantees in terms of the modified Frank-Wolfe gap for all of
these variants. We are particularly motivated by the application of this
methodology to the training of neural networks with sparse properties, and we
apply our block coordinate method to the problem of $\ell_1$ regularized neural
network training. We present the results of several numerical experiments on
both artificial and real datasets demonstrating significant improvements of our
method in training sparse neural networks.","['Paul Grigas', 'Alfonso Lobos', 'Nathan Vermeersch']","['math.OC', 'cs.LG', 'stat.ML']",2019-06-09 07:14:11+00:00
http://arxiv.org/abs/1906.04580v1,Fine-grained Event Categorization with Heterogeneous Graph Convolutional Networks,"Events are happening in real-world and real-time, which can be planned and
organized occasions involving multiple people and objects. Social media
platforms publish a lot of text messages containing public events with
comprehensive topics. However, mining social events is challenging due to the
heterogeneous event elements in texts and explicit and implicit social network
structures. In this paper, we design an event meta-schema to characterize the
semantic relatedness of social events and build an event-based heterogeneous
information network (HIN) integrating information from external knowledge base,
and propose a novel Pair-wise Popularity Graph Convolutional Network (PP-GCN)
based fine-grained social event categorization model. We propose a
Knowledgeable meta-paths Instances based social Event Similarity (KIES) between
events and build a weighted adjacent matrix as input to the PP-GCN model.
Comprehensive experiments on real data collections are conducted to compare
various social event detection and clustering tasks. Experimental results
demonstrate that our proposed framework outperforms other alternative social
event categorization techniques.","['Hao Peng', 'Jianxin Li', 'Qiran Gong', 'Yangqiu Song', 'Yuanxing Ning', 'Kunfeng Lai', 'Philip S. Yu']","['cs.SI', 'cs.CL', 'stat.ML']",2019-06-09 07:08:20+00:00
http://arxiv.org/abs/1906.03579v1,Robust conditional GANs under missing or uncertain labels,"Matching the performance of conditional Generative Adversarial Networks with
little supervision is an important task, especially in venturing into new
domains. We design a new training algorithm, which is robust to missing or
ambiguous labels. The main idea is to intentionally corrupt the labels of
generated examples to match the statistics of the real data, and have a
discriminator process the real and generated examples with corrupted labels. We
showcase the robustness of this proposed approach both theoretically and
empirically. We show that minimizing the proposed loss is equivalent to
minimizing true divergence between real and generated data up to a
multiplicative factor, and characterize this multiplicative factor as a
function of the statistics of the uncertain labels. Experiments on MNIST
dataset demonstrates that proposed architecture is able to achieve high
accuracy in generating examples faithful to the class even with only a few
examples per class.","['Kiran Koshy Thekumparampil', 'Sewoong Oh', 'Ashish Khetan']","['stat.ML', 'cs.LG']",2019-06-09 06:37:06+00:00
http://arxiv.org/abs/1906.03574v1,Transfer Learning by Modeling a Distribution over Policies,"Exploration and adaptation to new tasks in a transfer learning setup is a
central challenge in reinforcement learning. In this work, we build on the idea
of modeling a distribution over policies in a Bayesian deep reinforcement
learning setup to propose a transfer strategy. Recent works have shown to
induce diversity in the learned policies by maximizing the entropy of a
distribution of policies (Bachman et al., 2018; Garnelo et al., 2018) and thus,
we postulate that our proposed approach leads to faster exploration resulting
in improved transfer learning. We support our hypothesis by demonstrating
favorable experimental results on a variety of settings on fully-observable
GridWorld and partially observable MiniGrid (Chevalier-Boisvert et al., 2018)
environments.","['Disha Shrivastava', 'Eeshan Gunesh Dhekane', 'Riashat Islam']","['cs.LG', 'cs.AI', 'stat.ML']",2019-06-09 06:00:50+00:00
http://arxiv.org/abs/1906.03563v3,Adversarial Attack Generation Empowered by Min-Max Optimization,"The worst-case training principle that minimizes the maximal adversarial
loss, also known as adversarial training (AT), has shown to be a
state-of-the-art approach for enhancing adversarial robustness. Nevertheless,
min-max optimization beyond the purpose of AT has not been rigorously explored
in the adversarial context. In this paper, we show how a general framework of
min-max optimization over multiple domains can be leveraged to advance the
design of different types of adversarial attacks. In particular, given a set of
risk sources, minimizing the worst-case attack loss can be reformulated as a
min-max problem by introducing domain weights that are maximized over the
probability simplex of the domain set. We showcase this unified framework in
three attack generation problems -- attacking model ensembles, devising
universal perturbation under multiple inputs, and crafting attacks resilient to
data transformations. Extensive experiments demonstrate that our approach leads
to substantial attack improvement over the existing heuristic strategies as
well as robustness improvement over state-of-the-art defense methods trained to
be robust against multiple perturbation types. Furthermore, we find that the
self-adjusted domain weights learned from our min-max framework can provide a
holistic tool to explain the difficulty level of attack across domains. Code is
available at https://github.com/wangjksjtu/minmax-adv.","['Jingkang Wang', 'Tianyun Zhang', 'Sijia Liu', 'Pin-Yu Chen', 'Jiacen Xu', 'Makan Fardad', 'Bo Li']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2019-06-09 04:32:13+00:00
http://arxiv.org/abs/1906.03559v1,The Implicit Bias of AdaGrad on Separable Data,"We study the implicit bias of AdaGrad on separable linear classification
problems. We show that AdaGrad converges to a direction that can be
characterized as the solution of a quadratic optimization problem with the same
feasible set as the hard SVM problem. We also give a discussion about how
different choices of the hyperparameters of AdaGrad might impact this
direction. This provides a deeper understanding of why adaptive methods do not
seem to have the generalization ability as good as gradient descent does in
practice.","['Qian Qian', 'Xiaoyuan Qian']","['stat.ML', 'cs.LG']",2019-06-09 04:11:32+00:00
http://arxiv.org/abs/1906.03548v2,Four Things Everyone Should Know to Improve Batch Normalization,"A key component of most neural network architectures is the use of
normalization layers, such as Batch Normalization. Despite its common use and
large utility in optimizing deep architectures, it has been challenging both to
generically improve upon Batch Normalization and to understand the
circumstances that lend themselves to other enhancements. In this paper, we
identify four improvements to the generic form of Batch Normalization and the
circumstances under which they work, yielding performance gains across all
batch sizes while requiring no additional computation during training. These
contributions include proposing a method for reasoning about the current
example in inference normalization statistics, fixing a training vs. inference
discrepancy; recognizing and validating the powerful regularization effect of
Ghost Batch Normalization for small and medium batch sizes; examining the
effect of weight decay regularization on the scaling and shifting parameters
gamma and beta; and identifying a new normalization algorithm for very small
batch sizes by combining the strengths of Batch and Group Normalization. We
validate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256,
Oxford Flowers-102, CUB-2011, and ImageNet.","['Cecilia Summers', 'Michael J. Dinneen']","['cs.LG', 'cs.CV', 'stat.ML']",2019-06-09 01:14:48+00:00
http://arxiv.org/abs/1906.03543v1,apricot: Submodular selection for data summarization in Python,"We present apricot, an open source Python package for selecting
representative subsets from large data sets using submodular optimization. The
package implements an efficient greedy selection algorithm that offers strong
theoretical guarantees on the quality of the selected set. Two submodular set
functions are implemented in apricot: facility location, which is broadly
applicable but requires memory quadratic in the number of examples in the data
set, and a feature-based function that is less broadly applicable but can scale
to millions of examples. Apricot is extremely efficient, using both algorithmic
speedups such as the lazy greedy algorithm and code optimizers such as numba.
We demonstrate the use of subset selection by training machine learning models
to comparable accuracy using either the full data set or a representative
subset thereof. This paper presents an explanation of submodular selection, an
overview of the features in apricot, and an application to several data sets.
The code and tutorial Jupyter notebooks are available at
https://github.com/jmschrei/apricot","['Jacob Schreiber', 'Jeffrey Bilmes', 'William Stafford Noble']","['cs.LG', 'stat.ML']",2019-06-08 23:53:57+00:00
http://arxiv.org/abs/1906.03533v3,Proposed Guidelines for the Responsible Use of Explainable Machine Learning,"Explainable machine learning (ML) enables human learning from ML, human
appeal of automated model decisions, regulatory compliance, and security audits
of ML models. Explainable ML (i.e. explainable artificial intelligence or XAI)
has been implemented in numerous open source and commercial packages and
explainable ML is also an important, mandatory, or embedded aspect of
commercial predictive modeling in industries like financial services. However,
like many technologies, explainable ML can be misused, particularly as a faulty
safeguard for harmful black-boxes, e.g. fairwashing or scaffolding, and for
other malevolent purposes like stealing models and sensitive training data. To
promote best-practice discussions for this already in-flight technology, this
short text presents internal definitions and a few examples before covering the
proposed guidelines. This text concludes with a seemingly natural argument for
the use of interpretable models and explanatory, debugging, and disparate
impact testing methods in life- or mission-critical ML systems.","['Patrick Hall', 'Navdeep Gill', 'Nicholas Schmidt']","['stat.ML', 'cs.AI', 'cs.LG']",2019-06-08 22:12:11+00:00
http://arxiv.org/abs/1906.03532v2,Reducing the variance in online optimization by transporting past gradients,"Most stochastic optimization methods use gradients once before discarding
them. While variance reduction methods have shown that reusing past gradients
can be beneficial when there is a finite number of datapoints, they do not
easily extend to the online setting. One issue is the staleness due to using
past gradients. We propose to correct this staleness using the idea of implicit
gradient transport (IGT) which transforms gradients computed at previous
iterates into gradients evaluated at the current iterate without using the
Hessian explicitly. In addition to reducing the variance and bias of our
updates over time, IGT can be used as a drop-in replacement for the gradient
estimate in a number of well-understood methods such as heavy ball or Adam. We
show experimentally that it achieves state-of-the-art results on a wide range
of architectures and benchmarks. Additionally, the IGT gradient estimator
yields the optimal asymptotic convergence rate for online stochastic
optimization in the restricted setting where the Hessians of all component
functions are equal.","['Sébastien M. R. Arnold', 'Pierre-Antoine Manzagol', 'Reza Babanezhad', 'Ioannis Mitliagkas', 'Nicolas Le Roux']","['cs.LG', 'math.OC', 'stat.ML']",2019-06-08 22:02:28+00:00
http://arxiv.org/abs/1906.03526v2,Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks,"The problem of adversarial robustness has been studied extensively for neural
networks. However, for boosted decision trees and decision stumps there are
almost no results, even though they are widely used in practice (e.g. XGBoost)
due to their accuracy, interpretability, and efficiency. We show in this paper
that for boosted decision stumps the \textit{exact} min-max robust loss and
test error for an $l_\infty$-attack can be computed in $O(T\log T)$ time per
input, where $T$ is the number of decision stumps and the optimal update step
of the ensemble can be done in $O(n^2\,T\log T)$, where $n$ is the number of
data points. For boosted trees we show how to efficiently calculate and
optimize an upper bound on the robust loss, which leads to state-of-the-art
robust test error for boosted trees on MNIST (12.5% for $\epsilon_\infty=0.3$),
FMNIST (23.2% for $\epsilon_\infty=0.1$), and CIFAR-10 (74.7% for
$\epsilon_\infty=8/255$). Moreover, the robust test error rates we achieve are
competitive to the ones of provably robust convolutional networks. The code of
all our experiments is available at
http://github.com/max-andr/provably-robust-boosting","['Maksym Andriushchenko', 'Matthias Hein']","['cs.LG', 'cs.CR', 'stat.ML']",2019-06-08 21:44:34+00:00
http://arxiv.org/abs/1906.03518v1,Maximum Weighted Loss Discrepancy,"Though machine learning algorithms excel at minimizing the average loss over
a population, this might lead to large discrepancies between the losses across
groups within the population. To capture this inequality, we introduce and
study a notion we call maximum weighted loss discrepancy (MWLD), the maximum
(weighted) difference between the loss of a group and the loss of the
population. We relate MWLD to group fairness notions and robustness to
demographic shifts. We then show MWLD satisfies the following three properties:
1) It is statistically impossible to estimate MWLD when all groups have equal
weights. 2) For a particular family of weighting functions, we can estimate
MWLD efficiently. 3) MWLD is related to loss variance, a quantity that arises
in generalization bounds. We estimate MWLD with different weighting functions
on four common datasets from the fairness literature. We finally show that loss
variance regularization can halve the loss variance of a classifier and hence
reduce MWLD without suffering a significant drop in accuracy.","['Fereshte Khani', 'Aditi Raghunathan', 'Percy Liang']","['cs.LG', 'stat.ML']",2019-06-08 20:25:34+00:00
http://arxiv.org/abs/1906.03509v4,Outlier Exposure with Confidence Control for Out-of-Distribution Detection,"Deep neural networks have achieved great success in classification tasks
during the last years. However, one major problem to the path towards
artificial intelligence is the inability of neural networks to accurately
detect samples from novel class distributions and therefore, most of the
existent classification algorithms assume that all classes are known prior to
the training stage. In this work, we propose a methodology for training a
neural network that allows it to efficiently detect out-of-distribution (OOD)
examples without compromising much of its classification accuracy on the test
examples from known classes. We propose a novel loss function that gives rise
to a novel method, Outlier Exposure with Confidence Control (OECC), which
achieves superior results in OOD detection with OE both on image and text
classification tasks without requiring access to OOD samples. Additionally, we
experimentally show that the combination of OECC with state-of-the-art
post-training OOD detection methods, like the Mahalanobis Detector (MD) and the
Gramian Matrices (GM) methods, further improves their performance in the OOD
detection task, demonstrating the potential of combining training and
post-training methods for OOD detection.","['Aristotelis-Angelos Papadopoulos', 'Mohammad Reza Rajati', 'Nazim Shaikh', 'Jiamian Wang']","['cs.LG', 'cs.CV', 'stat.ML']",2019-06-08 19:30:24+00:00
http://arxiv.org/abs/1906.03504v3,Convolutional Bipartite Attractor Networks,"In human perception and cognition, a fundamental operation that brains
perform is interpretation: constructing coherent neural states from noisy,
incomplete, and intrinsically ambiguous evidence. The problem of interpretation
is well matched to an early and often overlooked architecture, the attractor
network---a recurrent neural net that performs constraint satisfaction,
imputation of missing features, and clean up of noisy data via energy
minimization dynamics. We revisit attractor nets in light of modern deep
learning methods and propose a convolutional bipartite architecture with a
novel training loss, activation function, and connectivity constraints. We
tackle larger problems than have been previously explored with attractor nets
and demonstrate their potential for image completion and super-resolution. We
argue that this architecture is better motivated than ever-deeper feedforward
models and is a viable alternative to more costly sampling-based generative
methods on a range of supervised and unsupervised tasks.","['Michael Iuzzolino', 'Yoram Singer', 'Michael C. Mozer']","['cs.LG', 'cs.NE', 'stat.ML']",2019-06-08 19:13:26+00:00
http://arxiv.org/abs/1906.03502v2,Attending to Discriminative Certainty for Domain Adaptation,"In this paper, we aim to solve for unsupervised domain adaptation of
classifiers where we have access to label information for the source domain
while these are not available for a target domain. While various methods have
been proposed for solving these including adversarial discriminator based
methods, most approaches have focused on the entire image based domain
adaptation. In an image, there would be regions that can be adapted better, for
instance, the foreground object may be similar in nature. To obtain such
regions, we propose methods that consider the probabilistic certainty estimate
of various regions and specify focus on these during classification for
adaptation. We observe that just by incorporating the probabilistic certainty
of the discriminator while training the classifier, we are able to obtain state
of the art results on various datasets as compared against all the recent
methods. We provide a thorough empirical analysis of the method by providing
ablation analysis, statistical significance test, and visualization of the
attention maps and t-SNE embeddings. These evaluations convincingly demonstrate
the effectiveness of the proposed approach.","['Vinod Kumar Kurmi', 'Shanu Kumar', 'Vinay P Namboodiri']","['cs.CV', 'cs.LG', 'stat.ML']",2019-06-08 19:04:38+00:00
http://arxiv.org/abs/1906.03499v1,ML-LOO: Detecting Adversarial Examples with Feature Attribution,"Deep neural networks obtain state-of-the-art performance on a series of
tasks. However, they are easily fooled by adding a small adversarial
perturbation to input. The perturbation is often human imperceptible on image
data. We observe a significant difference in feature attributions of
adversarially crafted examples from those of original ones. Based on this
observation, we introduce a new framework to detect adversarial examples
through thresholding a scale estimate of feature attribution scores.
Furthermore, we extend our method to include multi-layer feature attributions
in order to tackle the attacks with mixed confidence levels. Through vast
experiments, our method achieves superior performances in distinguishing
adversarial examples from popular attack methods on a variety of real data sets
among state-of-the-art detection methods. In particular, our method is able to
detect adversarial examples of mixed confidence levels, and transfer between
different attacking methods.","['Puyudi Yang', 'Jianbo Chen', 'Cho-Jui Hsieh', 'Jane-Ling Wang', 'Michael I. Jordan']","['cs.LG', 'cs.CR', 'stat.ML']",2019-06-08 18:36:16+00:00
http://arxiv.org/abs/1906.08591v2,Doubly Robust Crowdsourcing,"Large-scale labeled dataset is the indispensable fuel that ignites the AI
revolution as we see today. Most such datasets are constructed using
crowdsourcing services such as Amazon Mechanical Turk which provides noisy
labels from non-experts at a fair price. The sheer size of such datasets
mandates that it is only feasible to collect a few labels per data point. We
formulate the problem of test-time label aggregation as a statistical
estimation problem of inferring the expected voting score. By imitating workers
with supervised learners and using them in a doubly robust estimation
framework, we prove that the variance of estimation can be substantially
reduced, even if the learner is a poor approximation. Synthetic and real-world
experiments show that by combining the doubly robust approach with adaptive
worker/item selection rules, we often need much lower label cost to achieve
nearly the same accuracy as in the ideal world where all workers label all data
points.","['Chong Liu', 'Yu-Xiang Wang']","['cs.HC', 'cs.LG', 'stat.ML']",2019-06-08 18:12:40+00:00
http://arxiv.org/abs/1906.03479v1,Learning Radiative Transfer Models for Climate Change Applications in Imaging Spectroscopy,"According to a recent investigation, an estimated 33-50% of the world's coral
reefs have undergone degradation, believed to be as a result of climate change.
A strong driver of climate change and the subsequent environmental impact are
greenhouse gases such as methane. However, the exact relation climate change
has to the environmental condition cannot be easily established. Remote sensing
methods are increasingly being used to quantify and draw connections between
rapidly changing climatic conditions and environmental impact. A crucial part
of this analysis is processing spectroscopy data using radiative transfer
models (RTMs) which is a computationally expensive process and limits their use
with high volume imaging spectrometers. This work presents an algorithm that
can efficiently emulate RTMs using neural networks leading to a multifold
speedup in processing time, and yielding multiple downstream benefits.","['Shubhankar Deshpande', 'Brian D. Bue', 'David R. Thompson', 'Vijay Natraj', 'Mario Parente']","['cs.LG', 'astro-ph.IM', 'physics.comp-ph', 'stat.ML']",2019-06-08 15:39:32+00:00
http://arxiv.org/abs/1906.03471v2,"A gradual, semi-discrete approach to generative network training via explicit Wasserstein minimization","This paper provides a simple procedure to fit generative networks to target
distributions, with the goal of a small Wasserstein distance (or other optimal
transport costs). The approach is based on two principles: (a) if the source
randomness of the network is a continuous distribution (the ""semi-discrete""
setting), then the Wasserstein distance is realized by a deterministic optimal
transport mapping; (b) given an optimal transport mapping between a generator
network and a target distribution, the Wasserstein distance may be decreased
via a regression between the generated data and the mapped target points. The
procedure here therefore alternates these two steps, forming an optimal
transport and regressing against it, gradually adjusting the generator network
towards the target distribution. Mathematically, this approach is shown to
minimize the Wasserstein distance to both the empirical target distribution,
and also its underlying population counterpart. Empirically, good performance
is demonstrated on the training and testing sets of the MNIST and Thin-8 data.
The paper closes with a discussion of the unsuitability of the Wasserstein
distance for certain tasks, as has been identified in prior work [Arora et al.,
2017, Huang et al., 2017].","['Yucheng Chen', 'Matus Telgarsky', 'Chao Zhang', 'Bolton Bailey', 'Daniel Hsu', 'Jian Peng']","['cs.LG', 'stat.ML']",2019-06-08 14:42:54+00:00
http://arxiv.org/abs/1906.03455v2,Sensitivity of Deep Convolutional Networks to Gabor Noise,"Deep Convolutional Networks (DCNs) have been shown to be sensitive to
Universal Adversarial Perturbations (UAPs): input-agnostic perturbations that
fool a model on large portions of a dataset. These UAPs exhibit interesting
visual patterns, but this phenomena is, as yet, poorly understood. Our work
shows that visually similar procedural noise patterns also act as UAPs. In
particular, we demonstrate that different DCN architectures are sensitive to
Gabor noise patterns. This behaviour, its causes, and implications deserve
further in-depth study.","['Kenneth T. Co', 'Luis Muñoz-González', 'Emil C. Lupu']","['cs.LG', 'cs.CR', 'stat.ML']",2019-06-08 13:41:38+00:00
http://arxiv.org/abs/1906.03412v2,A Two-Step Graph Convolutional Decoder for Molecule Generation,"We propose a simple auto-encoder framework for molecule generation. The
molecular graph is first encoded into a continuous latent representation $z$,
which is then decoded back to a molecule. The encoding process is easy, but the
decoding process remains challenging. In this work, we introduce a simple
two-step decoding process. In a first step, a fully connected neural network
uses the latent vector $z$ to produce a molecular formula, for example CO$_2$
(one carbon and two oxygen atoms). In a second step, a graph convolutional
neural network uses the same latent vector $z$ to place bonds between the atoms
that were produced in the first step (for example a double bond will be placed
between the carbon and each of the oxygens). This two-step process, in which a
bag of atoms is first generated, and then assembled, provides a simple
framework that allows us to develop an efficient molecule auto-encoder.
Numerical experiments on basic tasks such as novelty, uniqueness, validity and
optimized chemical property for the 250k ZINC molecules demonstrate the
performances of the proposed system. Particularly, we achieve the highest
reconstruction rate of 90.5\%, improving the previous rate of 76.7\%. We also
report the best property improvement results when optimization is constrained
by the molecular distance between the original and generated molecules.","['Xavier Bresson', 'Thomas Laurent']","['cs.LG', 'stat.ML']",2019-06-08 07:59:26+00:00
http://arxiv.org/abs/1906.03401v2,Optimal Convergence for Stochastic Optimization with Multiple Expectation Constraints,"In this paper, we focus on the problem of stochastic optimization where the
objective function can be written as an expectation function over a closed
convex set. We also consider multiple expectation constraints which restrict
the domain of the problem. We extend the cooperative stochastic approximation
algorithm from Lan and Zhou [2016] to solve the particular problem. We close
the gaps in the previous analysis and provide a novel proof technique to show
that our algorithm attains the optimal rate of convergence for both optimality
gap and constraint violation when the functions are generally convex. We also
compare our algorithm empirically to the state-of-the-art and show improved
convergence in many situations.","['Kinjal Basu', 'Preetam Nandy']","['math.ST', 'math.OC', 'stat.ME', 'stat.ML', 'stat.TH', '60Gxx, 68W40']",2019-06-08 06:56:39+00:00
http://arxiv.org/abs/1906.03397v1,Making targeted black-box evasion attacks effective and efficient,"We investigate how an adversary can optimally use its query budget for
targeted evasion attacks against deep neural networks in a black-box setting.
We formalize the problem setting and systematically evaluate what benefits the
adversary can gain by using substitute models. We show that there is an
exploration-exploitation tradeoff in that query efficiency comes at the cost of
effectiveness. We present two new attack strategies for using substitute models
and show that they are as effective as previous query-only techniques but
require significantly fewer queries, by up to three orders of magnitude. We
also show that an agile adversary capable of switching through different attack
techniques can achieve pareto-optimal efficiency. We demonstrate our attack
against Google Cloud Vision showing that the difficulty of black-box attacks
against real-world prediction APIs is significantly easier than previously
thought (requiring approximately 500 queries instead of approximately 20,000 as
in previous works).","['Mika Juuti', 'Buse Gul Atli', 'N. Asokan']","['cs.LG', 'cs.CR', 'stat.ML']",2019-06-08 06:22:25+00:00
http://arxiv.org/abs/1906.03393v4,Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling,"Motivated by the many real-world applications of reinforcement learning (RL)
that require safe-policy iterations, we consider the problem of off-policy
evaluation (OPE) -- the problem of evaluating a new policy using the historical
data obtained by different behavior policies -- under the model of
nonstationary episodic Markov Decision Processes (MDP) with a long horizon and
a large action space. Existing importance sampling (IS) methods often suffer
from large variance that depends exponentially on the RL horizon $H$. To solve
this problem, we consider a marginalized importance sampling (MIS) estimator
that recursively estimates the state marginal distribution for the target
policy at every step. MIS achieves a mean-squared error of $$ \frac{1}{n}
\sum\nolimits_{t=1}^H\mathbb{E}_{\mu}\left[\frac{d_t^\pi(s_t)^2}{d_t^\mu(s_t)^2}
\mathrm{Var}_{\mu}\left[\frac{\pi_t(a_t|s_t)}{\mu_t(a_t|s_t)}\big(
V_{t+1}^\pi(s_{t+1}) + r_t\big) \middle| s_t\right]\right] +
\tilde{O}(n^{-1.5}) $$ where $\mu$ and $\pi$ are the logging and target
policies, $d_t^{\mu}(s_t)$ and $d_t^{\pi}(s_t)$ are the marginal distribution
of the state at $t$th step, $H$ is the horizon, $n$ is the sample size and
$V_{t+1}^\pi$ is the value function of the MDP under $\pi$. The result matches
the Cramer-Rao lower bound in \citet{jiang2016doubly} up to a multiplicative
factor of $H$. To the best of our knowledge, this is the first OPE estimation
error bound with a polynomial dependence on $H$. Besides theory, we show
empirical superiority of our method in time-varying, partially observable, and
long-horizon RL environments.","['Tengyang Xie', 'Yifei Ma', 'Yu-Xiang Wang']","['cs.LG', 'cs.AI', 'stat.ML']",2019-06-08 05:15:34+00:00
http://arxiv.org/abs/1906.03374v2,Lift Up and Act! Classifier Performance in Resource-Constrained Applications,"Classification tasks are common across many fields and applications where the
decision maker's action is limited by resource constraints. In direct marketing
only a subset of customers is contacted; scarce human resources limit the
number of interviews to the most promising job candidates; limited donated
organs are prioritized to those with best fit. In such scenarios, performance
measures such as the classification matrix, ROC analysis, and even ranking
metrics such as AUC measures outcomes different from the action of interest. At
the same time, gains and lift that do measure the relevant outcome are rarely
used by machine learners. In this paper we define resource-constrained
classifier performance as a task distinguished from classification and ranking.
We explain how gains and lift can lead to different algorithm choices and
discuss the effect of class distribution.",['Galit Shmueli'],"['stat.ML', 'cs.LG']",2019-06-08 02:34:50+00:00
http://arxiv.org/abs/1906.03367v1,Using learned optimizers to make models robust to input noise,"State-of-the art vision models can achieve superhuman performance on image
classification tasks when testing and training data come from the same
distribution. However, when models are tested on corrupted images (e.g. due to
scale changes, translations, or shifts in brightness or contrast), performance
degrades significantly. Here, we explore the possibility of meta-training a
learned optimizer that can train image classification models such that they are
robust to common image corruptions. Specifically, we are interested training
models that are more robust to noise distributions not present in the training
data. We find that a learned optimizer meta-trained to produce models which are
robust to Gaussian noise trains models that are more robust to Gaussian noise
at other scales compared to traditional optimizers like Adam. The effect of
meta-training is more complicated when targeting a more general set of noise
distributions, but led to improved performance on half of held-out corruption
tasks. Our results suggest that meta-learning provides a novel approach for
studying and improving the robustness of deep learning models.","['Luke Metz', 'Niru Maheswaranathan', 'Jonathon Shlens', 'Jascha Sohl-Dickstein', 'Ekin D. Cubuk']","['cs.LG', 'stat.ML']",2019-06-08 01:08:36+00:00
http://arxiv.org/abs/1906.03364v2,Online Forecasting of Total-Variation-bounded Sequences,"We consider the problem of online forecasting of sequences of length $n$ with
total-variation at most $C_n$ using observations contaminated by independent
$\sigma$-subgaussian noise. We design an $O(n\log n)$-time algorithm that
achieves a cumulative square error of $\tilde{O}(n^{1/3}C_n^{2/3}\sigma^{4/3} +
C_n^2)$ with high probability.We also prove a lower bound that matches the
upper bound in all parameters (up to a $\log(n)$ factor). To the best of our
knowledge, this is the first \emph{polynomial-time} algorithm that achieves the
optimal $O(n^{1/3})$ rate in forecasting total variation bounded sequences and
the first algorithm that \emph{adapts to unknown} $C_n$. Our proof techniques
leverage the special localized structure of Haar wavelet basis and the
adaptivity to unknown smoothness parameters in the classical wavelet smoothing
[Donoho et al., 1998]. We also compare our model to the rich literature of
dynamic regret minimization and nonstationary stochastic optimization, where
our problem can be treated as a special case. We show that the workhorse in
those settings --- online gradient descent and its variants with a fixed
restarting schedule --- are instances of a class of \emph{linear forecasters}
that require a suboptimal regret of $\tilde{\Omega}(\sqrt{n})$. This implies
that the use of more adaptive algorithms is necessary to obtain the optimal
rate.","['Dheeraj Baby', 'Yu-Xiang Wang']","['cs.LG', 'stat.ML']",2019-06-08 00:48:26+00:00
http://arxiv.org/abs/1906.03362v1,Partially Linear Additive Gaussian Graphical Models,"We propose a partially linear additive Gaussian graphical model (PLA-GGM) for
the estimation of associations between random variables distorted by observed
confounders. Model parameters are estimated using an $L_1$-regularized maximal
pseudo-profile likelihood estimator (MaPPLE) for which we prove
$\sqrt{n}$-sparsistency. Importantly, our approach avoids parametric
constraints on the effects of confounders on the estimated graphical model
structure. Empirically, the PLA-GGM is applied to both synthetic and real-world
datasets, demonstrating superior performance compared to competing methods.","['Sinong Geng', 'Minhao Yan', 'Mladen Kolar', 'Oluwasanmi Koyejo']","['cs.LG', 'stat.ML']",2019-06-08 00:30:53+00:00
http://arxiv.org/abs/1906.03361v3,Robust Bi-Tempered Logistic Loss Based on Bregman Divergences,"We introduce a temperature into the exponential function and replace the
softmax output layer of neural nets by a high temperature generalization.
Similarly, the logarithm in the log loss we use for training is replaced by a
low temperature logarithm. By tuning the two temperatures we create loss
functions that are non-convex already in the single layer case. When replacing
the last layer of the neural nets by our bi-temperature generalization of
logistic loss, the training becomes more robust to noise. We visualize the
effect of tuning the two temperatures in a simple setting and show the efficacy
of our method on large data sets. Our methodology is based on Bregman
divergences and is superior to a related two-temperature method using the
Tsallis divergence.","['Ehsan Amid', 'Manfred K. Warmuth', 'Rohan Anil', 'Tomer Koren']","['cs.LG', 'stat.ML']",2019-06-08 00:08:38+00:00
http://arxiv.org/abs/1906.03352v4,"Watch, Try, Learn: Meta-Learning from Demonstrations and Reward","Imitation learning allows agents to learn complex behaviors from
demonstrations. However, learning a complex vision-based task may require an
impractical number of demonstrations. Meta-imitation learning is a promising
approach towards enabling agents to learn a new task from one or a few
demonstrations by leveraging experience from learning similar tasks. In the
presence of task ambiguity or unobserved dynamics, demonstrations alone may not
provide enough information; an agent must also try the task to successfully
infer a policy. In this work, we propose a method that can learn to learn from
both demonstrations and trial-and-error experience with sparse reward feedback.
In comparison to meta-imitation, this approach enables the agent to effectively
and efficiently improve itself autonomously beyond the demonstration data. In
comparison to meta-reinforcement learning, we can scale to substantially
broader distributions of tasks, as the demonstration reduces the burden of
exploration. Our experiments show that our method significantly outperforms
prior approaches on a set of challenging, vision-based control tasks.","['Allan Zhou', 'Eric Jang', 'Daniel Kappler', 'Alex Herzog', 'Mohi Khansari', 'Paul Wohlhart', 'Yunfei Bai', 'Mrinal Kalakrishnan', 'Sergey Levine', 'Chelsea Finn']","['cs.LG', 'cs.AI', 'stat.ML']",2019-06-07 22:46:35+00:00
http://arxiv.org/abs/1906.03351v2,Real or Fake? Learning to Discriminate Machine from Human Generated Text,"Energy-based models (EBMs), a.k.a. un-normalized models, have had recent
successes in continuous spaces. However, they have not been successfully
applied to model text sequences. While decreasing the energy at training
samples is straightforward, mining (negative) samples where the energy should
be increased is difficult. In part, this is because standard gradient-based
methods are not readily applicable when the input is high-dimensional and
discrete. Here, we side-step this issue by generating negatives using
pre-trained auto-regressive language models. The EBM then works in the residual
of the language model; and is trained to discriminate real text from text
generated by the auto-regressive models. We investigate the generalization
ability of residual EBMs, a pre-requisite for using them in other applications.
We extensively analyze generalization for the task of classifying whether an
input is machine or human generated, a natural task given the training loss and
how we mine negatives. Overall, we observe that EBMs can generalize remarkably
well to changes in the architecture of the generators producing negatives.
However, EBMs exhibit more sensitivity to the training set used by such
generators.","['Anton Bakhtin', 'Sam Gross', 'Myle Ott', 'Yuntian Deng', ""Marc'Aurelio Ranzato"", 'Arthur Szlam']","['cs.LG', 'cs.CL', 'stat.ML']",2019-06-07 22:45:33+00:00
http://arxiv.org/abs/1906.03336v1,Benchmarking Minimax Linkage,"Minimax linkage was first introduced by Ao et al. [3] in 2004, as an
alternative to standard linkage methods used in hierarchical clustering.
Minimax linkage relies on distances to a prototype for each cluster; this
prototype can be thought of as a representative object in the cluster, hence
improving the interpretability of clustering results. Bien and Tibshirani
analyzed properties of this method in 2011 [2], popularizing the method within
the statistics community. Additionally, they performed comparisons of minimax
linkage to standard linkage methods, making use of five data sets and two
different evaluation metrics (distance to prototype and misclassification
rate). In an effort to expand upon their work and evaluate minimax linkage more
comprehensively, our benchmark study focuses on thorough method evaluation via
multiple performance metrics on several well-described data sets. We also make
all code and data publicly available through an R package, for full
reproducibility. Similarly to [2], we find that minimax linkage often produces
the smallest maximum minimax radius of all linkage methods, meaning that
minimax linkage produces clusters where objects in a cluster are tightly
clustered around their prototype. This is true across a range of values for the
total number of clusters (k). However, this is not always the case, and special
attention should be paid to the case when k is the true known value. For true
k, minimax linkage does not always perform the best in terms of all the
evaluation metrics studied, including maximum minimax radius. This paper was
motivated by the IFCS Cluster Benchmarking Task Force's call for clustering
benchmark studies and the white paper [5], which put forth guidelines and
principles for comprehensive benchmarking in clustering. Our work is designed
to be a neutral benchmark study of minimax linkage.","['Xiao Hui Tai', 'Kayla Frisoli']","['stat.ML', 'cs.LG', 'stat.ME']",2019-06-07 21:25:12+00:00
http://arxiv.org/abs/1906.03333v1,Efficient Project Gradient Descent for Ensemble Adversarial Attack,"Recent advances show that deep neural networks are not robust to deliberately
crafted adversarial examples which many are generated by adding human
imperceptible perturbation to clear input. Consider $l_2$ norms attacks,
Project Gradient Descent (PGD) and the Carlini and Wagner (C\&W) attacks are
the two main methods, where PGD control max perturbation for adversarial
examples while C\&W approach treats perturbation as a regularization term
optimized it with loss function together. If we carefully set parameters for
any individual input, both methods become similar. In general, PGD attacks
perform faster but obtains larger perturbation to find adversarial examples
than the C\&W when fixing the parameters for all inputs. In this report, we
propose an efficient modified PGD method for attacking ensemble models by
automatically changing ensemble weights and step size per iteration per input.
This method generates smaller perturbation adversarial examples than PGD method
while remains efficient as compared to C\&W method. Our method won the first
place in IJCAI19 Targeted Adversarial Attack competition.","['Fanyou Wu', 'Rado Gazo', 'Eva Haviarova', 'Bedrich Benes']","['cs.LG', 'cs.CR', 'stat.ML']",2019-06-07 21:07:02+00:00
http://arxiv.org/abs/1906.03329v2,Sparse Variational Inference: Bayesian Coresets from Scratch,"The proliferation of automated inference algorithms in Bayesian statistics
has provided practitioners newfound access to fast, reproducible data analysis
and powerful statistical models. Designing automated methods that are also both
computationally scalable and theoretically sound, however, remains a
significant challenge. Recent work on Bayesian coresets takes the approach of
compressing the dataset before running a standard inference algorithm,
providing both scalability and guarantees on posterior approximation error. But
the automation of past coreset methods is limited because they depend on the
availability of a reasonable coarse posterior approximation, which is difficult
to specify in practice. In the present work we remove this requirement by
formulating coreset construction as sparsity-constrained variational inference
within an exponential family. This perspective leads to a novel construction
via greedy optimization, and also provides a unifying information-geometric
view of present and past methods. The proposed Riemannian coreset construction
algorithm is fully automated, requiring no problem-specific inputs aside from
the probabilistic model and dataset. In addition to being significantly easier
to use than past methods, experiments demonstrate that past coreset
constructions are fundamentally limited by the fixed coarse posterior
approximation; in contrast, the proposed algorithm is able to continually
improve the coreset, providing state-of-the-art Bayesian dataset summarization
with orders-of-magnitude reduction in KL divergence to the exact posterior.","['Trevor Campbell', 'Boyan Beronov']","['stat.ML', 'cs.LG', 'stat.CO']",2019-06-07 20:54:35+00:00
http://arxiv.org/abs/1906.04569v1,DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep Networks,"Deep neural networks (DNNs) have achieved state-of-the-art performances in
many important domains, including medical diagnosis, security, and autonomous
driving. In these domains where safety is highly critical, an erroneous
decision can result in serious consequences. While a perfect prediction
accuracy is not always achievable, recent work on Bayesian deep networks shows
that it is possible to know when DNNs are more likely to make mistakes. Knowing
what DNNs do not know is desirable to increase the safety of deep learning
technology in sensitive applications. Bayesian neural networks attempt to
address this challenge. However, traditional approaches are computationally
intractable and do not scale well to large, complex neural network
architectures. In this paper, we develop a theoretical framework to approximate
Bayesian inference for DNNs by imposing a Bernoulli distribution on the model
weights. This method, called MC-DropConnect, gives us a tool to represent the
model uncertainty with little change in the overall model structure or
computational cost. We extensively validate the proposed algorithm on multiple
network architectures and datasets for classification and semantic segmentation
tasks. We also propose new metrics to quantify the uncertainty estimates. This
enables an objective comparison between MC-DropConnect and prior approaches.
Our empirical results demonstrate that the proposed framework yields
significant improvement in both prediction accuracy and uncertainty estimation
quality compared to the state of the art.","['Aryan Mobiny', 'Hien V. Nguyen', 'Supratik Moulik', 'Naveen Garg', 'Carol C. Wu']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2019-06-07 20:51:52+00:00
http://arxiv.org/abs/1906.05657v1,Automatically Evaluating Balance: A Machine Learning Approach,"Compared to in-clinic balance training, in-home training is not as effective.
This is, in part, due to the lack of feedback from physical therapists (PTs).
Here, we analyze the feasibility of using trunk sway data and machine learning
(ML) techniques to automatically evaluate balance, providing accurate
assessments outside of the clinic. We recruited sixteen participants to perform
standing balance exercises. For each exercise, we recorded trunk sway data and
had a PT rate balance performance on a scale of 1 to 5. The rating scale was
adapted from the Functional Independence Measure. From the trunk sway data, we
extracted a 61-dimensional feature vector representing performance of each
exercise. Given these labeled data, we trained a multi-class support vector
machine (SVM) to map trunk sway features to PT ratings. Evaluated in a
leave-one-participant-out scheme, the model achieved a classification accuracy
of 82%. Compared to participant self-assessment ratings, the SVM outputs were
significantly closer to PT ratings. The results of this pilot study suggest
that in the absence of PTs, ML techniques can provide accurate assessments
during standing balance exercises. Such automated assessments could reduce PT
consultation time and increase user compliance outside of the clinic.","['Tian Bao', 'Brooke N. Klatt', 'Susan L. Whitney', 'Kathleen H. Sienko', 'Jenna Wiens']","['cs.CY', 'cs.LG', 'stat.ML']",2019-06-07 20:51:17+00:00
http://arxiv.org/abs/1906.03323v4,Empirical Likelihood for Contextual Bandits,"We propose an estimator and confidence interval for computing the value of a
policy from off-policy data in the contextual bandit setting. To this end we
apply empirical likelihood techniques to formulate our estimator and confidence
interval as simple convex optimization problems. Using the lower bound of our
confidence interval, we then propose an off-policy policy optimization
algorithm that searches for policies with large reward lower bound. We
empirically find that both our estimator and confidence interval improve over
previous proposals in finite sample regimes. Finally, the policy optimization
algorithm we propose outperforms a strong baseline system for learning from
off-policy data.","['Nikos Karampatziakis', 'John Langford', 'Paul Mineiro']","['cs.LG', 'stat.ML']",2019-06-07 20:26:18+00:00
http://arxiv.org/abs/1906.03318v2,Efficient non-conjugate Gaussian process factor models for spike count data using polynomial approximations,"Gaussian Process Factor Analysis (GPFA) has been broadly applied to the
problem of identifying smooth, low-dimensional temporal structure underlying
large-scale neural recordings. However, spike trains are non-Gaussian, which
motivates combining GPFA with discrete observation models for binned spike
count data. The drawback to this approach is that GPFA priors are not conjugate
to count model likelihoods, which makes inference challenging. Here we address
this obstacle by introducing a fast, approximate inference method for
non-conjugate GPFA models. Our approach uses orthogonal second-order
polynomials to approximate the nonlinear terms in the non-conjugate
log-likelihood, resulting in a method we refer to as \textit{polynomial
approximate log-likelihood} (PAL) estimators. This approximation allows for
accurate closed-form evaluation of marginal likelihoods and fast numerical
optimization for parameters and hyperparameters. We derive PAL estimators for
GPFA models with binomial, Poisson, and negative binomial observations and find
the PAL estimation is highly accurate, and achieves faster convergence times
compared to existing state-of-the-art inference methods. We also find that PAL
hyperparameters can provide sensible initialization for black box variational
inference (BBVI), which improves BBVI accuracy. We demonstrate that PAL
estimators achieve fast and accurate extraction of latent structure from
multi-neuron spike train data.","['Stephen L. Keeley', 'David M. Zoltowski', 'Yiyi Yu', 'Jacob L. Yates', 'Spencer L. Smith', 'Jonathan W. Pillow']","['stat.ML', 'cs.LG']",2019-06-07 20:15:00+00:00
http://arxiv.org/abs/1906.03317v1,Optimal Transport Relaxations with Application to Wasserstein GANs,"We propose a family of relaxations of the optimal transport problem which
regularize the problem by introducing an additional minimization step over a
small region around one of the underlying transporting measures. The type of
regularization that we obtain is related to smoothing techniques studied in the
optimization literature. When using our approach to estimate optimal transport
costs based on empirical measures, we obtain statistical learning bounds which
are useful to guide the amount of regularization, while maintaining good
generalization properties. To illustrate the computational advantages of our
regularization approach, we apply our method to training Wasserstein GANs. We
obtain running time improvements, relative to current benchmarks, with no
deterioration in testing performance (via FID). The running time improvement
occurs because our new optimality-based threshold criterion reduces the number
of expensive iterates of the generating networks, while increasing the number
of actor-critic iterations.","['Saied Mahdian', 'Jose Blanchet', 'Peter Glynn']","['stat.ML', 'cs.LG', 'math.OC', 'math.ST', 'stat.TH']",2019-06-07 20:01:56+00:00
http://arxiv.org/abs/1906.04165v1,Leveraging BERT for Extractive Text Summarization on Lectures,"In the last two decades, automatic extractive text summarization on lectures
has demonstrated to be a useful tool for collecting key phrases and sentences
that best represent the content. However, many current approaches utilize dated
approaches, producing sub-par outputs or requiring several hours of manual
tuning to produce meaningful results. Recently, new machine learning
architectures have provided mechanisms for extractive summarization through the
clustering of output embeddings from deep learning models. This paper reports
on the project called Lecture Summarization Service, a python based RESTful
service that utilizes the BERT model for text embeddings and KMeans clustering
to identify sentences closes to the centroid for summary selection. The purpose
of the service was to provide students a utility that could summarize lecture
content, based on their desired number of sentences. On top of the summary
work, the service also includes lecture and summary management, storing content
on the cloud which can be used for collaboration. While the results of
utilizing BERT for extractive summarization were promising, there were still
areas where the model struggled, providing feature research opportunities for
further improvement.",['Derek Miller'],"['cs.CL', 'cs.LG', 'cs.SD', 'eess.AS', 'stat.ML']",2019-06-07 19:50:30+00:00
http://arxiv.org/abs/1906.03310v2,Robustness for Non-Parametric Classification: A Generic Attack and Defense,"Adversarially robust machine learning has received much recent attention.
However, prior attacks and defenses for non-parametric classifiers have been
developed in an ad-hoc or classifier-specific basis. In this work, we take a
holistic look at adversarial examples for non-parametric classifiers, including
nearest neighbors, decision trees, and random forests. We provide a general
defense method, adversarial pruning, that works by preprocessing the dataset to
become well-separated. To test our defense, we provide a novel attack that
applies to a wide range of non-parametric classifiers. Theoretically, we derive
an optimally robust classifier, which is analogous to the Bayes Optimal. We
show that adversarial pruning can be viewed as a finite sample approximation to
this optimal classifier. We empirically show that our defense and attack are
either better than or competitive with prior work on non-parametric
classifiers. Overall, our results provide a strong and broadly-applicable
baseline for future work on robust non-parametrics. Code available at
https://github.com/yangarbiter/adversarial-nonparametrics/ .","['Yao-Yuan Yang', 'Cyrus Rashtchian', 'Yizhen Wang', 'Kamalika Chaudhuri']","['cs.LG', 'cs.CR', 'cs.DS', 'stat.ML']",2019-06-07 19:45:52+00:00
http://arxiv.org/abs/1906.03305v1,Clustering Degree-Corrected Stochastic Block Model with Outliers,"For the degree corrected stochastic block model in the presence of arbitrary
or even adversarial outliers, we develop a convex-optimization-based clustering
algorithm that includes a penalization term depending on the positive deviation
of a node from the expected number of edges to other inliers. We prove that
under mild conditions, this method achieves exact recovery of the underlying
clusters. Our synthetic experiments show that our algorithm performs well on
heterogeneous networks, and in particular those with Pareto degree
distributions, for which outliers have a broad range of possible degrees that
may enhance their adversarial power. We also demonstrate that our method allows
for recovery with significantly lower error rates compared to existing
algorithms.","['Xin Qian', 'Yudong Chen', 'Andreea Minca']","['cs.LG', 'q-fin.ST', 'stat.ML']",2019-06-07 19:28:27+00:00
http://arxiv.org/abs/1906.04164v1,FAKTA: An Automatic End-to-End Fact Checking System,"We present FAKTA which is a unified framework that integrates various
components of a fact checking process: document retrieval from media sources
with various types of reliability, stance detection of documents with respect
to given claims, evidence extraction, and linguistic analysis. FAKTA predicts
the factuality of given claims and provides evidence at the document and
sentence level to explain its predictions","['Moin Nadeem', 'Wei Fang', 'Brian Xu', 'Mitra Mohtarami', 'James Glass']","['cs.CL', 'cs.LG', 'stat.ML']",2019-06-07 18:49:38+00:00
http://arxiv.org/abs/1906.03292v3,On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset,"Learning meaningful and compact representations with disentangled semantic
aspects is considered to be of key importance in representation learning. Since
real-world data is notoriously costly to collect, many recent state-of-the-art
disentanglement models have heavily relied on synthetic toy data-sets. In this
paper, we propose a novel data-set which consists of over one million images of
physical 3D objects with seven factors of variation, such as object color,
shape, size and position. In order to be able to control all the factors of
variation precisely, we built an experimental platform where the objects are
being moved by a robotic arm. In addition, we provide two more datasets which
consist of simulations of the experimental setup. These datasets provide for
the first time the possibility to systematically investigate how well different
disentanglement methods perform on real data in comparison to simulation, and
how simulated data can be leveraged to build better representations of the real
world. We provide a first experimental study of these questions and our results
indicate that learned models transfer poorly, but that model and hyperparameter
selection is an effective means of transferring information to the real world.","['Muhammad Waleed Gondal', 'Manuel Wüthrich', 'Đorđe Miladinović', 'Francesco Locatello', 'Martin Breidt', 'Valentin Volchkov', 'Joel Akpo', 'Olivier Bachem', 'Bernhard Schölkopf', 'Stefan Bauer']","['stat.ML', 'cs.LG']",2019-06-07 18:45:27+00:00
http://arxiv.org/abs/1906.03291v6,Understanding Generalization through Visualizations,"The power of neural networks lies in their ability to generalize to unseen
data, yet the underlying reasons for this phenomenon remain elusive. Numerous
rigorous attempts have been made to explain generalization, but available
bounds are still quite loose, and analysis does not always lead to true
understanding. The goal of this work is to make generalization more intuitive.
Using visualization methods, we discuss the mystery of generalization, the
geometry of loss landscapes, and how the curse (or, rather, the blessing) of
dimensionality causes optimizers to settle into minima that generalize well.","['W. Ronny Huang', 'Zeyad Emam', 'Micah Goldblum', 'Liam Fowl', 'J. K. Terry', 'Furong Huang', 'Tom Goldstein']","['cs.LG', 'cs.NE', 'stat.ML']",2019-06-07 18:43:20+00:00
http://arxiv.org/abs/1906.03288v2,Streaming Adaptive Nonparametric Variational Autoencoder,"We develop a data driven approach to perform clustering and end-to-end
feature learning simultaneously for streaming data that can adaptively detect
novel clusters in emerging data. Our approach, Adaptive Nonparametric
Variational Autoencoder (AdapVAE), learns the cluster membership through a
Bayesian Nonparametric (BNP) modeling framework with Deep Neural Networks
(DNNs) for feature learning. We develop a joint online variational inference
algorithm to learn feature representations and clustering assignments
simultaneously via iteratively optimizing the Evidence Lower Bound (ELBO). We
resolve the catastrophic forgetting \citep{kirkpatrick2017overcoming}
challenges with streaming data by adopting generative samples from the trained
AdapVAE using previous data, which avoids the need of storing and reusing past
data. We demonstrate the advantages of our model including adaptive novel
cluster detection without discarding useful information learned from past data,
high quality sample generation and comparable clustering performance as
end-to-end batch mode clustering methods on both image and text corpora
benchmark datasets.","['Tingting Zhao', 'Zifeng Wang', 'Aria Masoomi', 'Jennifer G. Dy']","['stat.ML', 'cs.LG']",2019-06-07 18:32:46+00:00
http://arxiv.org/abs/1906.03284v3,Equalized odds postprocessing under imperfect group information,"Most approaches aiming to ensure a model's fairness with respect to a
protected attribute (such as gender or race) assume to know the true value of
the attribute for every data point. In this paper, we ask to what extent
fairness interventions can be effective even when only imperfect information
about the protected attribute is available. In particular, we study the
prominent equalized odds postprocessing method of Hardt et al. (2016) under a
perturbation of the attribute. We identify conditions on the perturbation that
guarantee that the bias of a classifier is reduced even by running equalized
odds with the perturbed attribute. We also study the error of the resulting
classifier. We empirically observe that under our identified conditions most
often the error does not suffer from a perturbation of the protected attribute.
For a special case, we formally prove this observation to be true.","['Pranjal Awasthi', 'Matthäus Kleindessner', 'Jamie Morgenstern']","['stat.ML', 'cs.LG']",2019-06-07 18:26:48+00:00
http://arxiv.org/abs/1906.03281v1,Latent feature disentanglement for 3D meshes,"Generative modeling of 3D shapes has become an important problem due to its
relevance to many applications across Computer Vision, Graphics, and VR. In
this paper we build upon recently introduced 3D mesh-convolutional Variational
AutoEncoders which have shown great promise for learning rich representations
of deformable 3D shapes. We introduce a supervised generative 3D mesh model
that disentangles the latent shape representation into independent generative
factors. Our extensive experimental analysis shows that learning an explicitly
disentangled representation can both improve random shape generation as well as
successfully address downstream tasks such as pose and shape transfer,
shape-invariant temporal synchronization, and pose-invariant shape matching.","['Jake Levinson', 'Avneesh Sud', 'Ameesh Makadia']","['cs.LG', 'stat.ML']",2019-06-07 18:19:28+00:00
http://arxiv.org/abs/1906.06329v1,TensorNetwork for Machine Learning,"We demonstrate the use of tensor networks for image classification with the
TensorNetwork open source library. We explain in detail the encoding of image
data into a matrix product state form, and describe how to contract the network
in a way that is parallelizable and well-suited to automatic gradients for
optimization. Applying the technique to the MNIST and Fashion-MNIST datasets we
find out-of-the-box performance of 98% and 88% accuracy, respectively, using
the same tensor network architecture. The TensorNetwork library allows us to
seamlessly move from CPU to GPU hardware, and we see a factor of more than 10
improvement in computational speed using a GPU.","['Stavros Efthymiou', 'Jack Hidary', 'Stefan Leichenauer']","['cs.LG', 'cond-mat.str-el', 'cs.CV', 'physics.comp-ph', 'stat.ML']",2019-06-07 17:58:31+00:00
http://arxiv.org/abs/1906.03255v1,Disentangled State Space Representations,"Sequential data often originates from diverse domains across which
statistical regularities and domain specifics exist. To specifically learn
cross-domain sequence representations, we introduce disentangled state space
models (DSSM) -- a class of SSM in which domain-invariant state dynamics is
explicitly disentangled from domain-specific information governing that
dynamics. We analyze how such separation can improve knowledge transfer to new
domains, and enable robust prediction, sequence manipulation and domain
characterization. We furthermore propose an unsupervised VAE-based training
procedure to implement DSSM in form of Bayesian filters. In our experiments, we
applied VAE-DSSM framework to achieve competitive performance in online ODE
system identification and regression across experimental settings, and
controlled generation and prediction of bouncing ball video sequences across
varying gravitational influences.","['Đorđe Miladinović', 'Muhammad Waleed Gondal', 'Bernhard Schölkopf', 'Joachim M. Buhmann', 'Stefan Bauer']","['stat.ML', 'cs.LG']",2019-06-07 17:48:34+00:00
http://arxiv.org/abs/1906.03247v1,Ensemble Pruning via Margin Maximization,"Ensemble models refer to methods that combine a typically large number of
classifiers into a compound prediction. The output of an ensemble method is the
result of fitting a base-learning algorithm to a given data set, and obtaining
diverse answers by reweighting the observations or by resampling them using a
given probabilistic selection. A key challenge of using ensembles in
large-scale multidimensional data lies in the complexity and the computational
burden associated with them. The models created by ensembles are often
difficult, if not impossible, to interpret and their implementation requires
more computational power than single classifiers. Recent research effort in the
field has concentrated in reducing ensemble size, while maintaining their
predictive accuracy. We propose a method to prune an ensemble solution by
optimizing its margin distribution, while increasing its diversity. The
proposed algorithm results in an ensemble that uses only a fraction of the
original classifiers, with improved or similar generalization performance. We
analyze and test our method on both synthetic and real data sets. The
simulations show that the proposed method compares favorably to the original
ensemble solutions and to other existing ensemble pruning methodologies.",['Waldyn Martinez'],"['stat.ML', 'cs.LG', 'stat.CO']",2019-06-07 17:22:31+00:00
http://arxiv.org/abs/1906.03233v1,Machine Learning Prediction of Accurate Atomization Energies of Organic Molecules from Low-Fidelity Quantum Chemical Calculations,"Recent studies illustrate how machine learning (ML) can be used to bypass a
core challenge of molecular modeling: the tradeoff between accuracy and
computational cost. Here, we assess multiple ML approaches for predicting the
atomization energy of organic molecules. Our resulting models learn the
difference between low-fidelity, B3LYP, and high-accuracy, G4MP2, atomization
energies, and predict the G4MP2 atomization energy to 0.005 eV (mean absolute
error) for molecules with less than 9 heavy atoms and 0.012 eV for a small set
of molecules with between 10 and 14 heavy atoms. Our two best models, which
have different accuracy/speed tradeoffs, enable the efficient prediction of
G4MP2-level energies for large molecules and are available through a simple web
interface.","['Logan Ward', 'Ben Blaiszik', 'Ian Foster', 'Rajeev S. Assary', 'Badri Narayanan', 'Larry Curtiss']","['physics.comp-ph', 'cond-mat.mtrl-sci', 'physics.chem-ph', 'stat.ML']",2019-06-07 16:46:22+00:00
http://arxiv.org/abs/1906.09988v1,Recurrent Registration Neural Networks for Deformable Image Registration,"Parametric spatial transformation models have been successfully applied to
image registration tasks. In such models, the transformation of interest is
parameterized by a fixed set of basis functions as for example B-splines. Each
basis function is located on a fixed regular grid position among the image
domain, because the transformation of interest is not known in advance. As a
consequence, not all basis functions will necessarily contribute to the final
transformation which results in a non-compact representation of the
transformation. We reformulate the pairwise registration problem as a recursive
sequence of successive alignments. For each element in the sequence, a local
deformation defined by its position, shape, and weight is computed by our
recurrent registration neural network. The sum of all local deformations yield
the final spatial alignment of both images. Formulating the registration
problem in this way allows the network to detect non-aligned regions in the
images and to learn how to locally refine the registration properly. In
contrast to current non-sequence-based registration methods, our approach
iteratively applies local spatial deformations to the images until the desired
registration accuracy is achieved. We trained our network on 2D magnetic
resonance images of the lung and compared our method to a standard parametric
B-spline registration. The experiments show, that our method performs on par
for the accuracy but yields a more compact representation of the
transformation. Furthermore, we achieve a speedup of around 15 compared to the
B-spline registration.","['Robin Sandkühler', 'Simon Andermatt', 'Grzegorz Bauman', 'Sylvia Nyilas', 'Christoph Jud', 'Philippe C. Cattin']","['cs.CV', 'cs.LG', 'stat.ML']",2019-06-07 16:44:53+00:00
http://arxiv.org/abs/1906.03231v2,A cryptographic approach to black box adversarial machine learning,"We propose a new randomized ensemble technique with a provable security
guarantee against black-box transfer attacks. Our proof constructs a new
security problem for random binary classifiers which is easier to empirically
verify and a reduction from the security of this new model to the security of
the ensemble classifier. We provide experimental evidence of the security of
our random binary classifiers, as well as empirical results of the adversarial
accuracy of the overall ensemble to black-box attacks. Our construction
crucially leverages hidden randomness in the multiclass-to-binary reduction.","['Kevin Shi', 'Daniel Hsu', 'Allison Bishop']","['cs.LG', 'cs.CR', 'stat.ML']",2019-06-07 16:44:08+00:00
http://arxiv.org/abs/1906.03220v2,Labeled Graph Generative Adversarial Networks,"As a new approach to train generative models, \emph{generative adversarial
networks} (GANs) have achieved considerable success in image generation. This
framework has also recently been applied to data with graph structures. We
propose labeled-graph generative adversarial networks (LGGAN) to train deep
generative models for graph-structured data with node labels. We test the
approach on various types of graph datasets, such as collections of citation
networks and protein graphs. Experiment results show that our model can
generate diverse labeled graphs that match the structural characteristics of
the training data and outperforms all alternative approaches in quality and
generality. To further evaluate the quality of the generated graphs, we use
them on a downstream task of graph classification, and the results show that
LGGAN can faithfully capture the important aspects of the graph structure.","['Shuangfei Fan', 'Bert Huang']","['cs.LG', 'stat.ML']",2019-06-07 16:35:56+00:00
http://arxiv.org/abs/1906.03214v3,Importance Weighted Adversarial Variational Autoencoders for Spike Inference from Calcium Imaging Data,"The Importance Weighted Auto Encoder (IWAE) objective has been shown to
improve the training of generative models over the standard Variational Auto
Encoder (VAE) objective. Here, we derive importance weighted extensions to AVB
and AAE. These latent variable models use implicitly defined inference networks
whose approximate posterior density q_\phi(z|x) cannot be directly evaluated,
an essential ingredient for importance weighting. We show improved training and
inference in latent variable models with our adversarially trained importance
weighting method, and derive new theoretical connections between adversarial
generative model training criteria and marginal likelihood based methods. We
apply these methods to the important problem of inferring spiking neural
activity from calcium imaging data, a challenging posterior inference problem
in neuroscience, and show that posterior samples from the adversarial methods
outperform factorized posteriors used in VAEs.","['Daniel Jiwoong Im', 'Sridhama Prakhya', 'Jinyao Yan', 'Srinivas Turaga', 'Kristin Branson']","['cs.LG', 'eess.IV', 'stat.ML']",2019-06-07 16:24:12+00:00
http://arxiv.org/abs/1906.03200v2,Recurrent Kernel Networks,"Substring kernels are classical tools for representing biological sequences
or text. However, when large amounts of annotated data are available, models
that allow end-to-end training such as neural networks are often preferred.
Links between recurrent neural networks (RNNs) and substring kernels have
recently been drawn, by formally showing that RNNs with specific activation
functions were points in a reproducing kernel Hilbert space (RKHS). In this
paper, we revisit this link by generalizing convolutional kernel
networks---originally related to a relaxation of the mismatch kernel---to model
gaps in sequences. It results in a new type of recurrent neural network which
can be trained end-to-end with backpropagation, or without supervision by using
kernel approximation techniques. We experimentally show that our approach is
well suited to biological sequences, where it outperforms existing methods for
protein classification tasks.","['Dexiong Chen', 'Laurent Jacob', 'Julien Mairal']","['stat.ML', 'cs.LG']",2019-06-07 16:08:50+00:00
http://arxiv.org/abs/1906.03193v1,Fighting Quantization Bias With Bias,"Low-precision representation of deep neural networks (DNNs) is critical for
efficient deployment of deep learning application on embedded platforms,
however, converting the network to low precision degrades its performance.
Crucially, networks that are designed for embedded applications usually suffer
from increased degradation since they have less redundancy. This is most
evident for the ubiquitous MobileNet architecture which requires a costly
quantization-aware training cycle to achieve acceptable performance when
quantized to 8-bits. In this paper, we trace the source of the degradation in
MobileNets to a shift in the mean activation value. This shift is caused by an
inherent bias in the quantization process which builds up across layers,
shifting all network statistics away from the learned distribution. We show
that this phenomenon happens in other architectures as well. We propose a
simple remedy - compensating for the quantization induced shift by adding a
constant to the additive bias term of each channel. We develop two simple
methods for estimating the correction constants - one using iterative
evaluation of the quantized network and one where the constants are set using a
short training phase. Both methods are fast and require only a small amount of
unlabeled data, making them appealing for rapid deployment of neural networks.
Using the above methods we are able to match the performance of training-based
quantization of MobileNets at a fraction of the cost.","['Alexander Finkelstein', 'Uri Almog', 'Mark Grobman']","['cs.LG', 'stat.ML']",2019-06-07 16:00:35+00:00
http://arxiv.org/abs/1906.03164v1,Kernelized Capsule Networks,"Capsule Networks attempt to represent patterns in images in a way that
preserves hierarchical spatial relationships. Additionally, research has
demonstrated that these techniques may be robust against adversarial
perturbations. We present an improvement to training capsule networks with
added robustness via non-parametric kernel methods. The representations learned
through the capsule network are used to construct covariance kernels for
Gaussian processes (GPs). We demonstrate that this approach achieves comparable
prediction performance to Capsule Networks while improving robustness to
adversarial perturbations and providing a meaningful measure of uncertainty
that may aid in the detection of adversarial inputs.","['Taylor Killian', 'Justin Goodwin', 'Olivia Brown', 'Sung-Hyun Son']","['stat.ML', 'cs.LG']",2019-06-07 15:34:00+00:00
http://arxiv.org/abs/1906.03161v1,Structured Variational Inference in Continuous Cox Process Models,"We propose a scalable framework for inference in an inhomogeneous Poisson
process modeled by a continuous sigmoidal Cox process that assumes the
corresponding intensity function is given by a Gaussian process (GP) prior
transformed with a scaled logistic sigmoid function. We present a tractable
representation of the likelihood through augmentation with a superposition of
Poisson processes. This view enables a structured variational approximation
capturing dependencies across variables in the model. Our framework avoids
discretization of the domain, does not require accurate numerical integration
over the input space and is not limited to GPs with squared exponential
kernels. We evaluate our approach on synthetic and real-world data showing that
its benefits are particularly pronounced on multivariate input settings where
it overcomes the limitations of mean-field methods and sampling schemes. We
provide the state of-the-art in terms of speed, accuracy and uncertainty
quantification trade-offs.","['Virginia Aglietti', 'Edwin V. Bonilla', 'Theodoros Damoulas', 'Sally Cripps']","['stat.ML', 'cs.LG', 'stat.AP']",2019-06-07 15:31:02+00:00
http://arxiv.org/abs/1906.03155v5,Towards Sharp Analysis for Distributed Learning with Random Features,"In recent studies, the generalization properties for distributed learning and
random features assumed the existence of the target concept over the hypothesis
space. However, this strict condition is not applicable to the more common
non-attainable case. In this paper, using refined proof techniques, we first
extend the optimal rates for distributed learning with random features to the
non-attainable case. Then, we reduce the number of required random features via
data-dependent generating strategy, and improve the allowed number of
partitions with additional unlabeled data. Theoretical analysis shows these
techniques remarkably reduce computational cost while preserving the optimal
generalization accuracy under standard assumptions. Finally, we conduct several
experiments on both simulated and real-world datasets, and the empirical
results validate our theoretical findings.","['Jian Li', 'Yong Liu', 'Weiping Wang']","['cs.LG', 'cs.DC', 'stat.ML']",2019-06-07 15:19:58+00:00
http://arxiv.org/abs/1906.03139v1,Non-Differentiable Supervised Learning with Evolution Strategies and Hybrid Methods,"In this work we show that Evolution Strategies (ES) are a viable method for
learning non-differentiable parameters of large supervised models. ES are
black-box optimization algorithms that estimate distributions of model
parameters; however they have only been used for relatively small problems so
far. We show that it is possible to scale ES to more complex tasks and models
with millions of parameters. While using ES for differentiable parameters is
computationally impractical (although possible), we show that a hybrid approach
is practically feasible in the case where the model has both differentiable and
non-differentiable parameters. In this approach we use standard gradient-based
methods for learning differentiable weights, while using ES for learning
non-differentiable parameters - in our case sparsity masks of the weights. This
proposed method is surprisingly competitive, and when parallelized over
multiple devices has only negligible training time overhead compared to
training with gradient descent. Additionally, this method allows to train
sparse models from the first training step, so they can be much larger than
when using methods that require training dense models first. We present results
and analysis of supervised feed-forward models (such as MNIST and CIFAR-10
classification), as well as recurrent models, such as SparseWaveRNN for
text-to-speech.","['Karel Lenc', 'Erich Elsen', 'Tom Schaul', 'Karen Simonyan']","['cs.NE', 'cs.LG', 'stat.ML']",2019-06-07 14:52:19+00:00
http://arxiv.org/abs/1906.03123v1,On the Current State of Research in Explaining Ensemble Performance Using Margins,"Empirical evidence shows that ensembles, such as bagging, boosting, random
and rotation forests, generally perform better in terms of their generalization
error than individual classifiers. To explain this performance, Schapire et al.
(1998) developed an upper bound on the generalization error of an ensemble
based on the margins of the training data, from which it was concluded that
larger margins should lead to lower generalization error, everything else being
equal. Many other researchers have backed this assumption and presented tighter
bounds on the generalization error based on either the margins or functions of
the margins. For instance, Shen and Li (2010) provide evidence suggesting that
the generalization error of a voting classifier might be reduced by increasing
the mean and decreasing the variance of the margins. In this article we propose
several techniques and empirically test whether the current state of research
in explaining ensemble performance holds. We evaluate the proposed methods
through experiments with real and simulated data sets.","['Waldyn Martinez', 'J. Brian Gray']","['stat.ML', 'cs.LG', 'stat.CO']",2019-06-07 14:23:29+00:00
http://arxiv.org/abs/1906.04045v2,PHiSeg: Capturing Uncertainty in Medical Image Segmentation,"Segmentation of anatomical structures and pathologies is inherently
ambiguous. For instance, structure borders may not be clearly visible or
different experts may have different styles of annotating. The majority of
current state-of-the-art methods do not account for such ambiguities but rather
learn a single mapping from image to segmentation. In this work, we propose a
novel method to model the conditional probability distribution of the
segmentations given an input image. We derive a hierarchical probabilistic
model, in which separate latent variables are responsible for modelling the
segmentation at different resolutions. Inference in this model can be
efficiently performed using the variational autoencoder framework. We show that
our proposed method can be used to generate significantly more realistic and
diverse segmentation samples compared to recent related work, both, when
trained with annotations from a single or multiple annotators.","['Christian F. Baumgartner', 'Kerem C. Tezcan', 'Krishna Chaitanya', 'Andreas M. Hötker', 'Urs J. Muehlematter', 'Khoschy Schawkat', 'Anton S. Becker', 'Olivio Donati', 'Ender Konukoglu']","['eess.IV', 'cs.LG', 'stat.ML']",2019-06-07 14:17:18+00:00
http://arxiv.org/abs/1906.03118v1,Reliable Estimation of Individual Treatment Effect with Causal Information Bottleneck,"Estimating individual level treatment effects (ITE) from observational data
is a challenging and important area in causal machine learning and is commonly
considered in diverse mission-critical applications. In this paper, we propose
an information theoretic approach in order to find more reliable
representations for estimating ITE. We leverage the Information Bottleneck (IB)
principle, which addresses the trade-off between conciseness and predictive
power of representation. With the introduction of an extended graphical model
for causal information bottleneck, we encourage the independence between the
learned representation and the treatment type. We also introduce an additional
form of a regularizer from the perspective of understanding ITE in the
semi-supervised learning framework to ensure more reliable representations.
Experimental results show that our model achieves the state-of-the-art results
and exhibits more reliable prediction performances with uncertainty information
on real-world datasets.","['Sungyub Kim', 'Yongsu Baek', 'Sung Ju Hwang', 'Eunho Yang']","['cs.LG', 'stat.ML']",2019-06-07 14:15:55+00:00
http://arxiv.org/abs/1906.04284v2,Analyzing the Structure of Attention in a Transformer Language Model,"The Transformer is a fully attention-based alternative to recurrent networks
that has achieved state-of-the-art results across a range of NLP tasks. In this
paper, we analyze the structure of attention in a Transformer language model,
the GPT-2 small pretrained model. We visualize attention for individual
instances and analyze the interaction between attention and syntax over a large
corpus. We find that attention targets different parts of speech at different
layer depths within the model, and that attention aligns with dependency
relations most strongly in the middle layers. We also find that the deepest
layers of the model capture the most distant relationships. Finally, we extract
exemplar sentences that reveal highly specific patterns targeted by particular
attention heads.","['Jesse Vig', 'Yonatan Belinkov']","['cs.CL', 'cs.LG', 'stat.ML']",2019-06-07 13:58:49+00:00
http://arxiv.org/abs/1906.03098v1,Multi-modal Active Learning From Human Data: A Deep Reinforcement Learning Approach,"Human behavior expression and experience are inherently multi-modal, and
characterized by vast individual and contextual heterogeneity. To achieve
meaningful human-computer and human-robot interactions, multi-modal models of
the users states (e.g., engagement) are therefore needed. Most of the existing
works that try to build classifiers for the users states assume that the data
to train the models are fully labeled. Nevertheless, data labeling is costly
and tedious, and also prone to subjective interpretations by the human coders.
This is even more pronounced when the data are multi-modal (e.g., some users
are more expressive with their facial expressions, some with their voice).
Thus, building models that can accurately estimate the users states during an
interaction is challenging. To tackle this, we propose a novel multi-modal
active learning (AL) approach that uses the notion of deep reinforcement
learning (RL) to find an optimal policy for active selection of the users data,
needed to train the target (modality-specific) models. We investigate different
strategies for multi-modal data fusion, and show that the proposed model-level
fusion coupled with RL outperforms the feature-level and modality-specific
models, and the naive AL strategies such as random sampling, and the standard
heuristics such as uncertainty sampling. We show the benefits of this approach
on the task of engagement estimation from real-world child-robot interactions
during an autism therapy. Importantly, we show that the proposed multi-modal AL
approach can be used to efficiently personalize the engagement classifiers to
the target user using a small amount of actively selected users data.","['Ognjen Rudovic', 'Meiru Zhang', 'Bjorn Schuller', 'Rosalind W. Picard']","['cs.LG', 'cs.AI', 'cs.HC', 'cs.RO', 'stat.ML']",2019-06-07 13:46:15+00:00
http://arxiv.org/abs/1906.03049v2,Computing Tight Differential Privacy Guarantees Using FFT,"Differentially private (DP) machine learning has recently become popular. The
privacy loss of DP algorithms is commonly reported using
$(\varepsilon,\delta)$-DP. In this paper, we propose a numerical accountant for
evaluating the privacy loss for algorithms with continuous one dimensional
output. This accountant can be applied to the subsampled multidimensional
Gaussian mechanism which underlies the popular DP stochastic gradient descent.
The proposed method is based on a numerical approximation of an integral
formula which gives the exact $(\varepsilon,\delta)$-values. The approximation
is carried out by discretising the integral and by evaluating discrete
convolutions using the fast Fourier transform algorithm. We give both
theoretical error bounds and numerical error estimates for the approximation.
Experimental comparisons with state-of-the-art techniques demonstrate
significant improvements in bound tightness and/or computation time. Python
code for the method can be found in Github
(https://github.com/DPBayes/PLD-Accountant/).","['Antti Koskela', 'Joonas Jälkö', 'Antti Honkela']","['stat.ML', 'cs.CR', 'cs.LG']",2019-06-07 12:33:46+00:00
http://arxiv.org/abs/1906.03038v3,A Generative Framework for Zero-Shot Learning with Adversarial Domain Adaptation,"We present a domain adaptation based generative framework for zero-shot
learning. Our framework addresses the problem of domain shift between the seen
and unseen class distributions in zero-shot learning and minimizes the shift by
developing a generative model trained via adversarial domain adaptation. Our
approach is based on end-to-end learning of the class distributions of seen
classes and unseen classes. To enable the model to learn the class
distributions of unseen classes, we parameterize these class distributions in
terms of the class attribute information (which is available for both seen and
unseen classes). This provides a very simple way to learn the class
distribution of any unseen class, given only its class attribute information,
and no labeled training data. Training this model with adversarial domain
adaptation further provides robustness against the distribution mismatch
between the data from seen and unseen classes. Our approach also provides a
novel way for training neural net based classifiers to overcome the hubness
problem in zero-shot learning. Through a comprehensive set of experiments, we
show that our model yields superior accuracies as compared to various
state-of-the-art zero shot learning models, on a variety of benchmark datasets.
Code for the experiments is available at github.com/vkkhare/ZSL-ADA","['Varun Khare', 'Divyat Mahajan', 'Homanga Bharadhwaj', 'Vinay Verma', 'Piyush Rai']","['cs.LG', 'cs.CV', 'stat.ML']",2019-06-07 12:11:22+00:00
http://arxiv.org/abs/1906.03028v1,Automatic Reparameterisation of Probabilistic Programs,"Probabilistic programming has emerged as a powerful paradigm in statistics,
applied science, and machine learning: by decoupling modelling from inference,
it promises to allow modellers to directly reason about the processes
generating data. However, the performance of inference algorithms can be
dramatically affected by the parameterisation used to express a model,
requiring users to transform their programs in non-intuitive ways. We argue for
automating these transformations, and demonstrate that mechanisms available in
recent modeling frameworks can implement non-centring and related
reparameterisations. This enables new inference algorithms, and we propose two:
a simple approach using interleaved sampling and a novel variational
formulation that searches over a continuous space of parameterisations. We show
that these approaches enable robust inference across a range of models, and can
yield more efficient samplers than the best fixed parameterisation.","['Maria I. Gorinova', 'Dave Moore', 'Matthew D. Hoffman']","['stat.ML', 'cs.LG', 'cs.PL']",2019-06-07 11:56:42+00:00
http://arxiv.org/abs/1906.03001v1,Online Graph-Based Change-Point Detection for High Dimensional Data,"Online change-point detection (OCPD) is important for application in various
areas such as finance, biology, and the Internet of Things (IoT). However, OCPD
faces major challenges due to high-dimensionality, and it is still rarely
studied in literature. In this paper, we propose a novel, online, graph-based,
change-point detection algorithm to detect change of distribution in low- to
high-dimensional data. We introduce a similarity measure, which is derived from
the graph-spanning ratio, to test statistically if a change occurs. Through
numerical study using artificial online datasets, our data-driven approach
demonstrates high detection power for high-dimensional data, while the false
alarm rate (type I error) is controlled at a nominal significant level. In
particular, our graph-spanning approach has desirable power with small and
multiple scanning window, which allows timely detection of change-point in the
online setting.","['Yang-Wen Sun', 'Katerina Papagiannouli', 'Vladmir Spokoiny']","['stat.ML', 'cs.LG', 'stat.ME']",2019-06-07 10:40:45+00:00
