id,title,abstract,authors,categories,date
http://arxiv.org/abs/2202.11817v1,Benefit of Interpolation in Nearest Neighbor Algorithms,"In some studies \citep[e.g.,][]{zhang2016understanding} of deep learning, it
is observed that over-parametrized deep neural networks achieve a small testing
error even when the training error is almost zero. Despite numerous works
towards understanding this so-called ""double descent"" phenomenon
\citep[e.g.,][]{belkin2018reconciling,belkin2019two}, in this paper, we turn
into another way to enforce zero training error (without over-parametrization)
through a data interpolation mechanism. Specifically, we consider a class of
interpolated weighting schemes in the nearest neighbors (NN) algorithms. By
carefully characterizing the multiplicative constant in the statistical risk,
we reveal a U-shaped performance curve for the level of data interpolation in
both classification and regression setups. This sharpens the existing result
\citep{belkin2018does} that zero training error does not necessarily jeopardize
predictive performances and claims a counter-intuitive result that a mild
degree of data interpolation actually {\em strictly} improve the prediction
performance and statistical stability over those of the (un-interpolated)
$k$-NN algorithm. In the end, the universality of our results, such as change
of distance measure and corrupted testing data, will also be discussed.","['Yue Xing', 'Qifan Song', 'Guang Cheng']","['stat.ML', 'cs.LG']",2022-02-23 22:47:18+00:00
http://arxiv.org/abs/2202.11783v3,"Adversarially-regularized mixed effects deep learning (ARMED) models for improved interpretability, performance, and generalization on clustered data","Natural science datasets frequently violate assumptions of independence.
Samples may be clustered (e.g. by study site, subject, or experimental batch),
leading to spurious associations, poor model fitting, and confounded analyses.
While largely unaddressed in deep learning, this problem has been handled in
the statistics community through mixed effects models, which separate
cluster-invariant fixed effects from cluster-specific random effects. We
propose a general-purpose framework for Adversarially-Regularized Mixed Effects
Deep learning (ARMED) models through non-intrusive additions to existing neural
networks: 1) an adversarial classifier constraining the original model to learn
only cluster-invariant features, 2) a random effects subnetwork capturing
cluster-specific features, and 3) an approach to apply random effects to
clusters unseen during training. We apply ARMED to dense, convolutional, and
autoencoder neural networks on 4 applications including simulated nonlinear
data, dementia prognosis and diagnosis, and live-cell image analysis. Compared
to prior techniques, ARMED models better distinguish confounded from true
associations in simulations and learn more biologically plausible features in
clinical applications. They can also quantify inter-cluster variance and
visualize cluster effects in data. Finally, ARMED improves accuracy on data
from clusters seen during training (up to 28% vs. conventional models) and
generalization to unseen clusters (up to 9% vs. conventional models).","['Kevin P. Nguyen', 'Albert Montillo']","['cs.LG', 'stat.ML']",2022-02-23 20:58:22+00:00
http://arxiv.org/abs/2202.11735v3,Truncated LinUCB for Stochastic Linear Bandits,"This paper considers contextual bandits with a finite number of arms, where
the contexts are independent and identically distributed $d$-dimensional random
vectors, and the expected rewards are linear in both the arm parameters and
contexts. The LinUCB algorithm, which is near minimax optimal for related
linear bandits, is shown to have a cumulative regret that is suboptimal in both
the dimension $d$ and time horizon $T$, due to its over-exploration. A
truncated version of LinUCB is proposed and termed ""Tr-LinUCB"", which follows
LinUCB up to a truncation time $S$ and performs pure exploitation afterwards.
The Tr-LinUCB algorithm is shown to achieve $O(d\log(T))$ regret if $S =
Cd\log(T)$ for a sufficiently large constant $C$, and a matching lower bound is
established, which shows the rate optimality of Tr-LinUCB in both $d$ and $T$
under a low dimensional regime. Further, if $S = d\log^{\kappa}(T)$ for some
$\kappa>1$, the loss compared to the optimal is a multiplicative $\log\log(T)$
factor, which does not depend on $d$. This insensitivity to overshooting in
choosing the truncation time of Tr-LinUCB is of practical importance.","['Yanglei Song', 'Meng zhou']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-02-23 19:01:27+00:00
http://arxiv.org/abs/2202.11685v1,A Class of Geometric Structures in Transfer Learning: Minimax Bounds and Optimality,"We study the problem of transfer learning, observing that previous efforts to
understand its information-theoretic limits do not fully exploit the geometric
structure of the source and target domains. In contrast, our study first
illustrates the benefits of incorporating a natural geometric structure within
a linear regression model, which corresponds to the generalized eigenvalue
problem formed by the Gram matrices of both domains. We next establish a
finite-sample minimax lower bound, propose a refined model interpolation
estimator that enjoys a matching upper bound, and then extend our framework to
multiple source domains and generalized linear models. Surprisingly, as long as
information is available on the distance between the source and target
parameters, negative-transfer does not occur. Simulation studies show that our
proposed interpolation estimator outperforms state-of-the-art transfer learning
methods in both moderate- and high-dimensional settings.","['Xuhui Zhang', 'Jose Blanchet', 'Soumyadip Ghosh', 'Mark S. Squillante']","['cs.LG', 'stat.ME', 'stat.ML']",2022-02-23 18:47:53+00:00
http://arxiv.org/abs/2202.11678v3,"Bayesian Model Selection, the Marginal Likelihood, and Generalization","How do we compare between hypotheses that are entirely consistent with
observations? The marginal likelihood (aka Bayesian evidence), which represents
the probability of generating our observations from a prior, provides a
distinctive approach to this foundational question, automatically encoding
Occam's razor. Although it has been observed that the marginal likelihood can
overfit and is sensitive to prior assumptions, its limitations for
hyperparameter learning and discrete model comparison have not been thoroughly
investigated. We first revisit the appealing properties of the marginal
likelihood for learning constraints and hypothesis testing. We then highlight
the conceptual and practical issues in using the marginal likelihood as a proxy
for generalization. Namely, we show how marginal likelihood can be negatively
correlated with generalization, with implications for neural architecture
search, and can lead to both underfitting and overfitting in hyperparameter
learning. We also re-examine the connection between the marginal likelihood and
PAC-Bayes bounds and use this connection to further elucidate the shortcomings
of the marginal likelihood for model selection. We provide a partial remedy
through a conditional marginal likelihood, which we show is more aligned with
generalization, and practically valuable for large-scale hyperparameter
learning, such as in deep kernel learning.","['Sanae Lotfi', 'Pavel Izmailov', 'Gregory Benton', 'Micah Goldblum', 'Andrew Gordon Wilson']","['cs.LG', 'stat.ML']",2022-02-23 18:38:16+00:00
http://arxiv.org/abs/2202.11672v2,Learning Fast and Slow for Online Time Series Forecasting,"The fast adaptation capability of deep neural networks in non-stationary
environments is critical for online time series forecasting. Successful
solutions require handling changes to new and recurring patterns. However,
training deep neural forecaster on the fly is notoriously challenging because
of their limited ability to adapt to non-stationary environments and the
catastrophic forgetting of old knowledge. In this work, inspired by the
Complementary Learning Systems (CLS) theory, we propose Fast and Slow learning
Networks (FSNet), a holistic framework for online time-series forecasting to
simultaneously deal with abrupt changing and repeating patterns. Particularly,
FSNet improves the slowly-learned backbone by dynamically balancing fast
adaptation to recent changes and retrieving similar old knowledge. FSNet
achieves this mechanism via an interaction between two complementary components
of an adapter to monitor each layer's contribution to the lost, and an
associative memory to support remembering, updating, and recalling repeating
events. Extensive experiments on real and synthetic datasets validate FSNet's
efficacy and robustness to both new and recurring patterns. Our code is
available at \url{https://github.com/salesforce/fsnet}.","['Quang Pham', 'Chenghao Liu', 'Doyen Sahoo', 'Steven C. H. Hoi']","['cs.LG', 'stat.ML']",2022-02-23 18:23:07+00:00
http://arxiv.org/abs/2202.11670v1,Wide Mean-Field Bayesian Neural Networks Ignore the Data,"Bayesian neural networks (BNNs) combine the expressive power of deep learning
with the advantages of Bayesian formalism. In recent years, the analysis of
wide, deep BNNs has provided theoretical insight into their priors and
posteriors. However, we have no analogous insight into their posteriors under
approximate inference. In this work, we show that mean-field variational
inference entirely fails to model the data when the network width is large and
the activation function is odd. Specifically, for fully-connected BNNs with odd
activation functions and a homoscedastic Gaussian likelihood, we show that the
optimal mean-field variational posterior predictive (i.e., function space)
distribution converges to the prior predictive distribution as the width tends
to infinity. We generalize aspects of this result to other likelihoods. Our
theoretical results are suggestive of underfitting behavior previously
observered in BNNs. While our convergence bounds are non-asymptotic and
constants in our analysis can be computed, they are currently too loose to be
applicable in standard training regimes. Finally, we show that the optimal
approximate posterior need not tend to the prior if the activation function is
not odd, showing that our statements cannot be generalized arbitrarily.","['Beau Coker', 'Wessel P. Bruinsma', 'David R. Burt', 'Weiwei Pan', 'Finale Doshi-Velez']","['cs.LG', 'stat.ML']",2022-02-23 18:21:50+00:00
http://arxiv.org/abs/2202.11659v2,Globally Convergent Policy Search over Dynamic Filters for Output Estimation,"We introduce the first direct policy search algorithm which provably
converges to the globally optimal $\textit{dynamic}$ filter for the classical
problem of predicting the outputs of a linear dynamical system, given noisy,
partial observations. Despite the ubiquity of partial observability in
practice, theoretical guarantees for direct policy search algorithms, one of
the backbones of modern reinforcement learning, have proven difficult to
achieve. This is primarily due to the degeneracies which arise when optimizing
over filters that maintain internal state.
  In this paper, we provide a new perspective on this challenging problem based
on the notion of $\textit{informativity}$, which intuitively requires that all
components of a filter's internal state are representative of the true state of
the underlying dynamical system. We show that informativity overcomes the
aforementioned degeneracy. Specifically, we propose a $\textit{regularizer}$
which explicitly enforces informativity, and establish that gradient descent on
this regularized objective - combined with a ``reconditioning step'' -
converges to the globally optimal cost a $\mathcal{O}(1/T)$ rate. Our analysis
relies on several new results which may be of independent interest, including a
new framework for analyzing non-convex gradient descent via convex
reformulation, and novel bounds on the solution to linear Lyapunov equations in
terms of (our quantitative measure of) informativity.","['Jack Umenberger', 'Max Simchowitz', 'Juan C. Perdomo', 'Kaiqing Zhang', 'Russ Tedrake']","['math.OC', 'cs.DS', 'cs.LG', 'stat.ML']",2022-02-23 18:06:20+00:00
http://arxiv.org/abs/2202.11632v1,Mirror Descent Strikes Again: Optimal Stochastic Convex Optimization under Infinite Noise Variance,"We study stochastic convex optimization under infinite noise variance.
Specifically, when the stochastic gradient is unbiased and has uniformly
bounded $(1+\kappa)$-th moment, for some $\kappa \in (0,1]$, we quantify the
convergence rate of the Stochastic Mirror Descent algorithm with a particular
class of uniformly convex mirror maps, in terms of the number of iterations,
dimensionality and related geometric parameters of the optimization problem.
Interestingly this algorithm does not require any explicit gradient clipping or
normalization, which have been extensively used in several recent empirical and
theoretical works. We complement our convergence results with
information-theoretic lower bounds showing that no other algorithm using only
stochastic first-order oracles can achieve improved rates. Our results have
several interesting consequences for devising online/streaming stochastic
approximation algorithms for problems arising in robust statistics and machine
learning.","['Nuri Mert Vural', 'Lu Yu', 'Krishnakumar Balasubramanian', 'Stanislav Volgushev', 'Murat A. Erdogdu']","['stat.ML', 'cs.LG', 'math.OC']",2022-02-23 17:08:40+00:00
http://arxiv.org/abs/2202.11629v1,A Complete Criterion for Value of Information in Soluble Influence Diagrams,"Influence diagrams have recently been used to analyse the safety and fairness
properties of AI systems. A key building block for this analysis is a graphical
criterion for value of information (VoI). This paper establishes the first
complete graphical criterion for VoI in influence diagrams with multiple
decisions. Along the way, we establish two important techniques for proving
properties of multi-decision influence diagrams: ID homomorphisms are
structure-preserving transformations of influence diagrams, while a Tree of
Systems is collection of paths that captures how information and control can
flow in an influence diagram.","['Chris van Merwijk', 'Ryan Carey', 'Tom Everitt']","['cs.AI', 'stat.ML']",2022-02-23 17:06:41+00:00
http://arxiv.org/abs/2202.11612v1,Testing Granger Non-Causality in Panels with Cross-Sectional Dependencies,"This paper proposes a new approach for testing Granger non-causality on panel
data. Instead of aggregating panel member statistics, we aggregate their
corresponding p-values and show that the resulting p-value approximately bounds
the type I error by the chosen significance level even if the panel members are
dependent. We compare our approach against the most widely used Granger
causality algorithm on panel data and show that our approach yields lower FDR
at the same power for large sample sizes and panels with cross-sectional
dependencies. Finally, we examine COVID-19 data about confirmed cases and
deaths measured in countries/regions worldwide and show that our approach is
able to discover the true causal relation between confirmed cases and deaths
while state-of-the-art approaches fail.","['Lenon Minorics', 'Caner Turkmen', 'David Kernert', 'Patrick Bloebaum', 'Laurent Callot', 'Dominik Janzing']","['stat.ME', 'stat.ML']",2022-02-23 16:49:13+00:00
http://arxiv.org/abs/2202.11598v1,A Dimensionality Reduction Method for Finding Least Favorable Priors with a Focus on Bregman Divergence,"A common way of characterizing minimax estimators in point estimation is by
moving the problem into the Bayesian estimation domain and finding a least
favorable prior distribution. The Bayesian estimator induced by a least
favorable prior, under mild conditions, is then known to be minimax. However,
finding least favorable distributions can be challenging due to inherent
optimization over the space of probability distributions, which is
infinite-dimensional. This paper develops a dimensionality reduction method
that allows us to move the optimization to a finite-dimensional setting with an
explicit bound on the dimension. The benefit of this dimensionality reduction
is that it permits the use of popular algorithms such as projected gradient
ascent to find least favorable priors. Throughout the paper, in order to make
progress on the problem, we restrict ourselves to Bayesian risks induced by a
relatively large class of loss functions, namely Bregman divergences.","['Alex Dytso', 'Mario Goldenbaum', 'H. Vincent Poor', 'Shlomo Shamai']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2022-02-23 16:22:28+00:00
http://arxiv.org/abs/2202.11593v2,Finding Safe Zones of policies Markov Decision Processes,"Given a policy of a Markov Decision Process, we define a SafeZone as a subset
of states, such that most of the policy's trajectories are confined to this
subset. The quality of a SafeZone is parameterized by the number of states and
the escape probability, i.e., the probability that a random trajectory will
leave the subset. SafeZones are especially interesting when they have a small
number of states and low escape probability. We study the complexity of finding
optimal SafeZones, and show that in general, the problem is computationally
hard. Our main result is a bi-criteria approximation learning algorithm with a
factor of almost $2$ approximation for both the escape probability and SafeZone
size, using a polynomial size sample complexity.","['Lee Cohen', 'Yishay Mansour', 'Michal Moshkovitz']","['cs.LG', 'cs.AI', 'cs.DS', 'stat.ML']",2022-02-23 16:14:35+00:00
http://arxiv.org/abs/2202.11592v2,A Law of Robustness beyond Isoperimetry,"We study the robust interpolation problem of arbitrary data distributions
supported on a bounded space and propose a two-fold law of robustness. Robust
interpolation refers to the problem of interpolating $n$ noisy training data
points in $\mathbb{R}^d$ by a Lipschitz function. Although this problem has
been well understood when the samples are drawn from an isoperimetry
distribution, much remains unknown concerning its performance under generic or
even the worst-case distributions. We prove a Lipschitzness lower bound
$\Omega(\sqrt{n/p})$ of the interpolating neural network with $p$ parameters on
arbitrary data distributions. With this result, we validate the law of
robustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layer
neural networks with polynomial weights. We then extend our result to arbitrary
interpolating approximators and prove a Lipschitzness lower bound
$\Omega(n^{1/d})$ for robust interpolation. Our results demonstrate a two-fold
law of robustness: i) we show the potential benefit of overparametrization for
smooth data interpolation when $n=\mathrm{poly}(d)$, and ii) we disprove the
potential existence of an $O(1)$-Lipschitz robust interpolating function when
$n=\exp(\omega(d))$.","['Yihan Wu', 'Heng Huang', 'Hongyang Zhang']","['cs.LG', 'stat.ML']",2022-02-23 16:10:23+00:00
http://arxiv.org/abs/2202.11585v1,Amortised Likelihood-free Inference for Expensive Time-series Simulators with Signatured Ratio Estimation,"Simulation models of complex dynamics in the natural and social sciences
commonly lack a tractable likelihood function, rendering traditional
likelihood-based statistical inference impossible. Recent advances in machine
learning have introduced novel algorithms for estimating otherwise intractable
likelihood functions using a likelihood ratio trick based on binary
classifiers. Consequently, efficient likelihood approximations can be obtained
whenever good probabilistic classifiers can be constructed. We propose a kernel
classifier for sequential data using path signatures based on the recently
introduced signature kernel. We demonstrate that the representative power of
signatures yields a highly performant classifier, even in the crucially
important case where sample numbers are low. In such scenarios, our approach
can outperform sophisticated neural networks for common posterior inference
tasks.","['Joel Dyer', 'Patrick Cannon', 'Sebastian M Schmon']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2022-02-23 15:59:34+00:00
http://arxiv.org/abs/2202.11559v2,Bayesian Target-Vector Optimization for Efficient Parameter Reconstruction,"Parameter reconstructions are indispensable in metrology. Here, the objective
is to to explain $K$ experimental measurements by fitting to them a
parameterized model of the measurement process. The model parameters are
regularly determined by least-square methods, i.e., by minimizing the sum of
the squared residuals between the $K$ model predictions and the $K$
experimental observations, $\chi^2$. The model functions often involve
computationally demanding numerical simulations. Bayesian optimization methods
are specifically suited for minimizing expensive model functions. However, in
contrast to least-square methods such as the Levenberg-Marquardt algorithm,
they only take the value of $\chi^2$ into account, and neglect the $K$
individual model outputs. We present a Bayesian target-vector optimization
scheme with improved performance over previous developments, that considers all
$K$ contributions of the model function and that is specifically suited for
parameter reconstruction problems which are often based on hundreds of
observations. Its performance is compared to established methods for an optical
metrology reconstruction problem and two synthetic least-squares problems. The
proposed method outperforms established optimization methods. It also enables
to determine accurate uncertainty estimates with very few observations of the
actual model function by using Markov chain Monte Carlo sampling on a trained
surrogate model.","['Matthias Plock', 'Kas Andrle', 'Sven Burger', 'Philipp-Immanuel Schneider']","['physics.comp-ph', 'physics.data-an', 'stat.ML']",2022-02-23 15:13:32+00:00
http://arxiv.org/abs/2202.11550v3,Robust Geometric Metric Learning,"This paper proposes new algorithms for the metric learning problem. We start
by noticing that several classical metric learning formulations from the
literature can be viewed as modified covariance matrix estimation problems.
Leveraging this point of view, a general approach, called Robust Geometric
Metric Learning (RGML), is then studied. This method aims at simultaneously
estimating the covariance matrix of each class while shrinking them towards
their (unknown) barycenter. We focus on two specific costs functions: one
associated with the Gaussian likelihood (RGML Gaussian), and one with Tyler's M
-estimator (RGML Tyler). In both, the barycenter is defined with the Riemannian
distance, which enjoys nice properties of geodesic convexity and affine
invariance. The optimization is performed using the Riemannian geometry of
symmetric positive definite matrices and its submanifold of unit determinant.
Finally, the performance of RGML is asserted on real datasets. Strong
performance is exhibited while being robust to mislabeled data.","['Antoine Collas', 'Arnaud Breloy', 'Guillaume Ginolhac', 'Chengfang Ren', 'Jean-Philippe Ovarlez']","['stat.ML', 'cs.LG']",2022-02-23 14:55:08+00:00
http://arxiv.org/abs/2202.11539v2,Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut,"Transformers trained with self-supervised learning using self-distillation
loss (DINO) have been shown to produce attention maps that highlight salient
foreground objects. In this paper, we demonstrate a graph-based approach that
uses the self-supervised transformer features to discover an object from an
image. Visual tokens are viewed as nodes in a weighted graph with edges
representing a connectivity score based on the similarity of tokens. Foreground
objects can then be segmented using a normalized graph-cut to group
self-similar regions. We solve the graph-cut problem using spectral clustering
with generalized eigen-decomposition and show that the second smallest
eigenvector provides a cutting solution since its absolute value indicates the
likelihood that a token belongs to a foreground object. Despite its simplicity,
this approach significantly boosts the performance of unsupervised object
discovery: we improve over the recent state of the art LOST by a margin of
6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The
performance can be further improved by adding a second stage class-agnostic
detector (CAD). Our proposed method can be easily extended to unsupervised
saliency detection and weakly supervised object detection. For unsupervised
saliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,
DUT-OMRON respectively compared to previous state of the art. For weakly
supervised object detection, we achieve competitive performance on CUB and
ImageNet.","['Yangtao Wang', 'Xi Shen', 'Shell Hu', 'Yuan Yuan', 'James Crowley', 'Dominique Vaufreydaz']","['cs.CV', 'stat.ML']",2022-02-23 14:27:36+00:00
http://arxiv.org/abs/2202.11474v2,Residual Bootstrap Exploration for Stochastic Linear Bandit,"We propose a new bootstrap-based online algorithm for stochastic linear
bandit problems. The key idea is to adopt residual bootstrap exploration, in
which the agent estimates the next step reward by re-sampling the residuals of
mean reward estimate. Our algorithm, residual bootstrap exploration for
stochastic linear bandit (\texttt{LinReBoot}), estimates the linear reward from
its re-sampling distribution and pulls the arm with the highest reward
estimate. In particular, we contribute a theoretical framework to demystify
residual bootstrap-based exploration mechanisms in stochastic linear bandit
problems. The key insight is that the strength of bootstrap exploration is
based on collaborated optimism between the online-learned model and the
re-sampling distribution of residuals. Such observation enables us to show that
the proposed \texttt{LinReBoot} secure a high-probability $\tilde{O}(d
\sqrt{n})$ sub-linear regret under mild conditions. Our experiments support the
easy generalizability of the \texttt{ReBoot} principle in the various
formulations of linear bandit problems and show the significant computational
efficiency of \texttt{LinReBoot}.","['Shuang Wu', 'Chi-Hua Wang', 'Yuantong Li', 'Guang Cheng']","['stat.ML', 'cs.LG']",2022-02-23 12:47:12+00:00
http://arxiv.org/abs/2202.11461v1,Exponential Tail Local Rademacher Complexity Risk Bounds Without the Bernstein Condition,"The local Rademacher complexity framework is one of the most successful
general-purpose toolboxes for establishing sharp excess risk bounds for
statistical estimators based on the framework of empirical risk minimization.
Applying this toolbox typically requires using the Bernstein condition, which
often restricts applicability to convex and proper settings. Recent years have
witnessed several examples of problems where optimal statistical performance is
only achievable via non-convex and improper estimators originating from
aggregation theory, including the fundamental problem of model selection. These
examples are currently outside of the reach of the classical localization
theory.
  In this work, we build upon the recent approach to localization via offset
Rademacher complexities, for which a general high-probability theory has yet to
be established. Our main result is an exponential-tail excess risk bound
expressed in terms of the offset Rademacher complexity that yields results at
least as sharp as those obtainable via the classical theory. However, our bound
applies under an estimator-dependent geometric condition (the ""offset
condition"") instead of the estimator-independent (but, in general,
distribution-dependent) Bernstein condition on which the classical theory
relies. Our results apply to improper prediction regimes not directly covered
by the classical theory.","['Varun Kanade', 'Patrick Rebeschini', 'Tomas Vaskevicius']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2022-02-23 12:27:53+00:00
http://arxiv.org/abs/2202.11455v1,On PAC-Bayesian reconstruction guarantees for VAEs,"Despite its wide use and empirical successes, the theoretical understanding
and study of the behaviour and performance of the variational autoencoder (VAE)
have only emerged in the past few years. We contribute to this recent line of
work by analysing the VAE's reconstruction ability for unseen test data,
leveraging arguments from the PAC-Bayes theory. We provide generalisation
bounds on the theoretical reconstruction error, and provide insights on the
regularisation effect of VAE objectives. We illustrate our theoretical results
with supporting experiments on classical benchmark datasets.","['Badr-Eddine Chérief-Abdellatif', 'Yuyang Shi', 'Arnaud Doucet', 'Benjamin Guedj']","['cs.LG', 'cs.CV', 'math.ST', 'stat.ML', 'stat.TH']",2022-02-23 12:11:05+00:00
http://arxiv.org/abs/2202.11424v1,Towards Speaker Age Estimation with Label Distribution Learning,"Existing methods for speaker age estimation usually treat it as a multi-class
classification or a regression problem. However, precise age identification
remains a challenge due to label ambiguity, \emph{i.e.}, utterances from
adjacent age of the same person are often indistinguishable. To address this,
we utilize the ambiguous information among the age labels, convert each age
label into a discrete label distribution and leverage the label distribution
learning (LDL) method to fit the data. For each audio data sample, our method
produces a age distribution of its speaker, and on top of the distribution we
also perform two other tasks: age prediction and age uncertainty minimization.
Therefore, our method naturally combines the age classification and regression
approaches, which enhances the robustness of our method. We conduct experiments
on the public NIST SRE08-10 dataset and a real-world dataset, which exhibit
that our method outperforms baseline methods by a relatively large margin,
yielding a 10\% reduction in terms of mean absolute error (MAE) on a real-world
dataset.","['Shijing Si', 'Jianzong Wang', 'Junqing Peng', 'Jing Xiao']","['cs.SD', 'cs.LG', 'eess.AS', 'stat.ML']",2022-02-23 11:11:58+00:00
http://arxiv.org/abs/2202.11393v2,Differential privacy for symmetric log-concave mechanisms,"Adding random noise to database query results is an important tool for
achieving privacy. A challenge is to minimize this noise while still meeting
privacy requirements. Recently, a sufficient and necessary condition for
$(\epsilon, \delta)$-differential privacy for Gaussian noise was published.
This condition allows the computation of the minimum privacy-preserving scale
for this distribution. We extend this work and provide a sufficient and
necessary condition for $(\epsilon, \delta)$-differential privacy for all
symmetric and log-concave noise densities. Our results allow fine-grained
tailoring of the noise distribution to the dimensionality of the query result.
We demonstrate that this can yield significantly lower mean squared errors than
those incurred by the currently used Laplace and Gaussian mechanisms for the
same $\epsilon$ and $\delta$.",['Staal A. Vinterbo'],"['cs.CR', 'stat.ML']",2022-02-23 10:20:29+00:00
http://arxiv.org/abs/2202.11389v2,Fast Sparse Classification for Generalized Linear and Additive Models,"We present fast classification techniques for sparse generalized linear and
additive models. These techniques can handle thousands of features and
thousands of observations in minutes, even in the presence of many highly
correlated features. For fast sparse logistic regression, our computational
speed-up over other best-subset search techniques owes to linear and quadratic
surrogate cuts for the logistic loss that allow us to efficiently screen
features for elimination, as well as use of a priority queue that favors a more
uniform exploration of features. As an alternative to the logistic loss, we
propose the exponential loss, which permits an analytical solution to the line
search at each iteration. Our algorithms are generally 2 to 5 times faster than
previous approaches. They produce interpretable models that have accuracy
comparable to black box models on challenging datasets.","['Jiachang Liu', 'Chudi Zhong', 'Margo Seltzer', 'Cynthia Rudin']","['cs.LG', 'stat.ML']",2022-02-23 10:07:42+00:00
http://arxiv.org/abs/2202.11356v1,Preformer: Predictive Transformer with Multi-Scale Segment-wise Correlations for Long-Term Time Series Forecasting,"Transformer-based methods have shown great potential in long-term time series
forecasting. However, most of these methods adopt the standard point-wise
self-attention mechanism, which not only becomes intractable for long-term
forecasting since its complexity increases quadratically with the length of
time series, but also cannot explicitly capture the predictive dependencies
from contexts since the corresponding key and value are transformed from the
same point. This paper proposes a predictive Transformer-based model called
{\em Preformer}. Preformer introduces a novel efficient {\em Multi-Scale
Segment-Correlation} mechanism that divides time series into segments and
utilizes segment-wise correlation-based attention for encoding time series. A
multi-scale structure is developed to aggregate dependencies at different
temporal scales and facilitate the selection of segment length. Preformer
further designs a predictive paradigm for decoding, where the key and value
come from two successive segments rather than the same segment. In this way, if
a key segment has a high correlation score with the query segment, its
successive segment contributes more to the prediction of the query segment.
Extensive experiments demonstrate that our Preformer outperforms other
Transformer-based methods.","['Dazhao Du', 'Bing Su', 'Zhewei Wei']","['cs.LG', 'stat.ML']",2022-02-23 08:49:35+00:00
http://arxiv.org/abs/2202.11322v2,Efficient CDF Approximations for Normalizing Flows,"Normalizing flows model a complex target distribution in terms of a bijective
transform operating on a simple base distribution. As such, they enable
tractable computation of a number of important statistical quantities,
particularly likelihoods and samples. Despite these appealing properties, the
computation of more complex inference tasks, such as the cumulative
distribution function (CDF) over a complex region (e.g., a polytope) remains
challenging. Traditional CDF approximations using Monte-Carlo techniques are
unbiased but have unbounded variance and low sample efficiency. Instead, we
build upon the diffeomorphic properties of normalizing flows and leverage the
divergence theorem to estimate the CDF over a closed region in target space in
terms of the flux across its \emph{boundary}, as induced by the normalizing
flow. We describe both deterministic and stochastic instances of this
estimator: while the deterministic variant iteratively improves the estimate by
strategically subdividing the boundary, the stochastic variant provides
unbiased estimates. Our experiments on popular flow architectures and UCI
benchmark datasets show a marked improvement in sample efficiency as compared
to traditional estimators.","['Chandramouli Shama Sastry', 'Andreas Lehrmann', 'Marcus Brubaker', 'Alexander Radovic']","['cs.LG', 'stat.ML']",2022-02-23 06:11:49+00:00
http://arxiv.org/abs/2202.11316v2,Multivariate Quantile Function Forecaster,"We propose Multivariate Quantile Function Forecaster (MQF$^2$), a global
probabilistic forecasting method constructed using a multivariate quantile
function and investigate its application to multi-horizon forecasting. Prior
approaches are either autoregressive, implicitly capturing the dependency
structure across time but exhibiting error accumulation with increasing
forecast horizons, or multi-horizon sequence-to-sequence models, which do not
exhibit error accumulation, but also do typically not model the dependency
structure across time steps. MQF$^2$ combines the benefits of both approaches,
by directly making predictions in the form of a multivariate quantile function,
defined as the gradient of a convex function which we parametrize using
input-convex neural networks. By design, the quantile function is monotone with
respect to the input quantile levels and hence avoids quantile crossing. We
provide two options to train MQF$^2$: with energy score or with maximum
likelihood. Experimental results on real-world and synthetic datasets show that
our model has comparable performance with state-of-the-art methods in terms of
single time step metrics while capturing the time dependency structure.","['Kelvin Kan', 'François-Xavier Aubet', 'Tim Januschowski', 'Youngsuk Park', 'Konstantinos Benidis', 'Lars Ruthotto', 'Jan Gasthaus']","['cs.LG', 'stat.ML']",2022-02-23 05:22:03+00:00
http://arxiv.org/abs/2202.11285v1,Neural Generalised AutoRegressive Conditional Heteroskedasticity,"We propose Neural GARCH, a class of methods to model conditional
heteroskedasticity in financial time series. Neural GARCH is a neural network
adaptation of the GARCH 1,1 model in the univariate case, and the diagonal BEKK
1,1 model in the multivariate case. We allow the coefficients of a GARCH model
to be time varying in order to reflect the constantly changing dynamics of
financial markets. The time varying coefficients are parameterised by a
recurrent neural network that is trained with stochastic gradient variational
Bayes. We propose two variants of our model, one with normal innovations and
the other with Students t innovations. We test our models on a wide range of
univariate and multivariate financial time series, and we find that the Neural
Students t model consistently outperforms the others.","['Zexuan Yin', 'Paolo Barucca']","['cs.LG', 'q-fin.ST', 'stat.ML']",2022-02-23 03:23:02+00:00
http://arxiv.org/abs/2202.11277v2,Minimax Optimal Quantization of Linear Models: Information-Theoretic Limits and Efficient Algorithms,"High-dimensional models often have a large memory footprint and must be
quantized after training before being deployed on resource-constrained edge
devices for inference tasks. In this work, we develop an information-theoretic
framework for the problem of quantizing a linear regressor learned from
training data $(\mathbf{X}, \mathbf{y})$, for some underlying statistical
relationship $\mathbf{y} = \mathbf{X}\boldsymbol{\theta} + \mathbf{v}$. The
learned model, which is an estimate of the latent parameter
$\boldsymbol{\theta} \in \mathbb{R}^d$, is constrained to be representable
using only $Bd$ bits, where $B \in (0, \infty)$ is a pre-specified budget and
$d$ is the dimension. We derive an information-theoretic lower bound for the
minimax risk under this setting and propose a matching upper bound using
randomized embedding-based algorithms which is tight up to constant factors.
The lower and upper bounds together characterize the minimum threshold
bit-budget required to achieve a performance risk comparable to the unquantized
setting. We also propose randomized Hadamard embeddings that are
computationally efficient and are optimal up to a mild logarithmic factor of
the lower bound. Our model quantization strategy can be generalized and we show
its efficacy by extending the method and upper-bounds to two-layer ReLU neural
networks for non-linear regression. Numerical simulations show the improved
performance of our proposed scheme as well as its closeness to the lower bound.","['Rajarshi Saha', 'Mert Pilanci', 'Andrea J. Goldsmith']","['cs.IT', 'cs.LG', 'eess.SP', 'math.IT', 'stat.ML']",2022-02-23 02:39:04+00:00
http://arxiv.org/abs/2202.11269v2,NetRCA: An Effective Network Fault Cause Localization Algorithm,"Localizing the root cause of network faults is crucial to network operation
and maintenance. However, due to the complicated network architectures and
wireless environments, as well as limited labeled data, accurately localizing
the true root cause is challenging. In this paper, we propose a novel algorithm
named NetRCA to deal with this problem. Firstly, we extract effective derived
features from the original raw data by considering temporal, directional,
attribution, and interaction characteristics. Secondly, we adopt multivariate
time series similarity and label propagation to generate new training data from
both labeled and unlabeled data to overcome the lack of labeled samples.
Thirdly, we design an ensemble model which combines XGBoost, rule set learning,
attribution model, and graph algorithm, to fully utilize all data information
and enhance performance. Finally, experiments and analysis are conducted on the
real-world dataset from ICASSP 2022 AIOps Challenge to demonstrate the
superiority and effectiveness of our approach.","['Chaoli Zhang', 'Zhiqiang Zhou', 'Yingying Zhang', 'Linxiao Yang', 'Kai He', 'Qingsong Wen', 'Liang Sun']","['cs.LG', 'cs.AI', 'cs.NI', 'eess.SP', 'stat.ML']",2022-02-23 02:03:35+00:00
http://arxiv.org/abs/2202.11258v1,"Many processors, little time: MCMC for partitions via optimal transport couplings","Markov chain Monte Carlo (MCMC) methods are often used in clustering since
they guarantee asymptotically exact expectations in the infinite-time limit. In
finite time, though, slow mixing often leads to poor performance. Modern
computing environments offer massive parallelism, but naive implementations of
parallel MCMC can exhibit substantial bias. In MCMC samplers of continuous
random variables, Markov chain couplings can overcome bias. But these
approaches depend crucially on paired chains meetings after a small number of
transitions. We show that straightforward applications of existing coupling
ideas to discrete clustering variables fail to meet quickly. This failure
arises from the ""label-switching problem"": semantically equivalent cluster
relabelings impede fast meeting of coupled chains. We instead consider chains
as exploring the space of partitions rather than partitions' (arbitrary)
labelings. Using a metric on the partition space, we formulate a practical
algorithm using optimal transport couplings. Our theory confirms our method is
accurate and efficient. In experiments ranging from clustering of genes or
seeds to graph colorings, we show the benefits of our coupling in the highly
parallel, time-limited regime.","['Tin D. Nguyen', 'Brian L. Trippe', 'Tamara Broderick']","['stat.ME', 'stat.CO', 'stat.ML']",2022-02-23 01:20:13+00:00
http://arxiv.org/abs/2202.11219v2,No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling,"For each of $T$ time steps, $m$ experts report probability distributions over
$n$ outcomes; we wish to learn to aggregate these forecasts in a way that
attains a no-regret guarantee. We focus on the fundamental and practical
aggregation method known as logarithmic pooling -- a weighted average of log
odds -- which is in a certain sense the optimal choice of pooling method if one
is interested in minimizing log loss (as we take to be our loss function). We
consider the problem of learning the best set of parameters (i.e. expert
weights) in an online adversarial setting. We assume (by necessity) that the
adversarial choices of outcomes and forecasts are consistent, in the sense that
experts report calibrated forecasts. Imposing this constraint creates a (to our
knowledge) novel semi-adversarial setting in which the adversary retains a
large amount of flexibility. In this setting, we present an algorithm based on
online mirror descent that learns expert weights in a way that attains
$O(\sqrt{T} \log T)$ expected regret as compared with the best weights in
hindsight.","['Eric Neyman', 'Tim Roughgarden']","['cs.LG', 'stat.ML']",2022-02-22 22:27:25+00:00
http://arxiv.org/abs/2203.04695v2,Structured Multi-task Learning for Molecular Property Prediction,"Multi-task learning for molecular property prediction is becoming
increasingly important in drug discovery. However, in contrast to other
domains, the performance of multi-task learning in drug discovery is still not
satisfying as the number of labeled data for each task is too limited, which
calls for additional data to complement the data scarcity. In this paper, we
study multi-task learning for molecular property prediction in a novel setting,
where a relation graph between tasks is available. We first construct a dataset
(ChEMBL-STRING) including around 400 tasks as well as a task relation graph.
Then to better utilize such relation graph, we propose a method called SGNN-EBM
to systematically investigate the structured task modeling from two
perspectives. (1) In the \emph{latent} space, we model the task representations
by applying a state graph neural network (SGNN) on the relation graph. (2) In
the \emph{output} space, we employ structured prediction with the energy-based
model (EBM), which can be efficiently trained through noise-contrastive
estimation (NCE) approach. Empirical results justify the effectiveness of
SGNN-EBM. Code is available on https://github.com/chao1224/SGNN-EBM.","['Shengchao Liu', 'Meng Qu', 'Zuobai Zhang', 'Huiyu Cai', 'Jian Tang']","['q-bio.BM', 'cs.LG', 'stat.ML']",2022-02-22 20:31:23+00:00
http://arxiv.org/abs/2202.11154v2,Parallel MCMC Without Embarrassing Failures,"Embarrassingly parallel Markov Chain Monte Carlo (MCMC) exploits parallel
computing to scale Bayesian inference to large datasets by using a two-step
approach. First, MCMC is run in parallel on (sub)posteriors defined on data
partitions. Then, a server combines local results. While efficient, this
framework is very sensitive to the quality of subposterior sampling. Common
sampling problems such as missing modes or misrepresentation of low-density
regions are amplified -- instead of being corrected -- in the combination
phase, leading to catastrophic failures. In this work, we propose a novel
combination strategy to mitigate this issue. Our strategy, Parallel Active
Inference (PAI), leverages Gaussian Process (GP) surrogate modeling and active
learning. After fitting GPs to subposteriors, PAI (i) shares information
between GP surrogates to cover missing modes; and (ii) uses active sampling to
individually refine subposterior approximations. We validate PAI in challenging
benchmarks, including heavy-tailed and multi-modal posteriors and a real-world
application to computational neuroscience. Empirical results show that PAI
succeeds where previous methods catastrophically fail, with a small
communication overhead.","['Daniel Augusto de Souza', 'Diego Mesquita', 'Samuel Kaski', 'Luigi Acerbi']","['stat.ML', 'cs.LG', 'stat.ME']",2022-02-22 20:17:46+00:00
http://arxiv.org/abs/2202.11141v1,Nonconvex Extension of Generalized Huber Loss for Robust Learning and Pseudo-Mode Statistics,"We propose an extended generalization of the pseudo Huber loss formulation.
We show that using the log-exp transform together with the logistic function,
we can create a loss which combines the desirable properties of the strictly
convex losses with robust loss functions. With this formulation, we show that a
linear convergence algorithm can be utilized to find a minimizer. We further
discuss the creation of a quasi-convex composite loss and provide a
derivative-free exponential convergence rate algorithm.","['Kaan Gokcesu', 'Hakan Gokcesu']","['stat.ML', 'cs.LG', 'stat.CO']",2022-02-22 19:32:02+00:00
http://arxiv.org/abs/2202.11097v1,Message passing all the way up,"The message passing framework is the foundation of the immense success
enjoyed by graph neural networks (GNNs) in recent years. In spite of its
elegance, there exist many problems it provably cannot solve over given input
graphs. This has led to a surge of research on going ""beyond message passing"",
building GNNs which do not suffer from those limitations -- a term which has
become ubiquitous in regular discourse. However, have those methods truly moved
beyond message passing? In this position paper, I argue about the dangers of
using this term -- especially when teaching graph representation learning to
newcomers. I show that any function of interest we want to compute over graphs
can, in all likelihood, be expressed using pairwise message passing -- just
over a potentially modified graph, and argue how most practical implementations
subtly do this kind of trick anyway. Hoping to initiate a productive
discussion, I propose replacing ""beyond message passing"" with a more tame term,
""augmented message passing"".",['Petar Veličković'],"['cs.LG', 'cs.SI', 'stat.ML']",2022-02-22 18:57:54+00:00
http://arxiv.org/abs/2202.11091v2,Efficient and Differentiable Conformal Prediction with General Function Classes,"Quantifying the data uncertainty in learning tasks is often done by learning
a prediction interval or prediction set of the label given the input. Two
commonly desired properties for learned prediction sets are \emph{valid
coverage} and \emph{good efficiency} (such as low length or low cardinality).
Conformal prediction is a powerful technique for learning prediction sets with
valid coverage, yet by default its conformalization step only learns a single
parameter, and does not optimize the efficiency over more expressive function
classes.
  In this paper, we propose a generalization of conformal prediction to
multiple learnable parameters, by considering the constrained empirical risk
minimization (ERM) problem of finding the most efficient prediction set subject
to valid empirical coverage. This meta-algorithm generalizes existing conformal
prediction algorithms, and we show that it achieves approximate valid
population coverage and near-optimal efficiency within class, whenever the
function class in the conformalization step is low-capacity in a certain sense.
Next, this ERM problem is challenging to optimize as it involves a
non-differentiable coverage constraint. We develop a gradient-based algorithm
for it by approximating the original constrained ERM using differentiable
surrogate losses and Lagrangians. Experiments show that our algorithm is able
to learn valid prediction sets and improve the efficiency significantly over
existing approaches in several applications such as prediction intervals with
improved length, minimum-volume prediction sets for multi-output regression,
and label prediction sets for image classification.","['Yu Bai', 'Song Mei', 'Huan Wang', 'Yingbo Zhou', 'Caiming Xiong']","['cs.LG', 'cs.AI', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2022-02-22 18:37:23+00:00
http://arxiv.org/abs/2202.11089v3,Counterfactual Phenotyping with Censored Time-to-Events,"Estimation of treatment efficacy of real-world clinical interventions
involves working with continuous outcomes such as time-to-death,
re-hospitalization, or a composite event that may be subject to censoring.
Counterfactual reasoning in such scenarios requires decoupling the effects of
confounding physiological characteristics that affect baseline survival rates
from the effects of the interventions being assessed. In this paper, we present
a latent variable approach to model heterogeneous treatment effects by
proposing that an individual can belong to one of latent clusters with distinct
response characteristics. We show that this latent structure can mediate the
base survival rates and helps determine the effects of an intervention. We
demonstrate the ability of our approach to discover actionable phenotypes of
individuals based on their treatment response on multiple large randomized
clinical trials originally conducted to assess appropriate treatments to reduce
cardiovascular risk.","['Chirag Nagpal', 'Mononito Goswami', 'Keith Dufendach', 'Artur Dubrawski']","['cs.LG', 'stat.AP', 'stat.ME', 'stat.ML']",2022-02-22 18:34:40+00:00
http://arxiv.org/abs/2202.11043v1,Differentially Private Estimation of Heterogeneous Causal Effects,"Estimating heterogeneous treatment effects in domains such as healthcare or
social science often involves sensitive data where protecting privacy is
important. We introduce a general meta-algorithm for estimating conditional
average treatment effects (CATE) with differential privacy (DP) guarantees. Our
meta-algorithm can work with simple, single-stage CATE estimators such as
S-learner and more complex multi-stage estimators such as DR and R-learner. We
perform a tight privacy analysis by taking advantage of sample splitting in our
meta-algorithm and the parallel composition property of differential privacy.
In this paper, we implement our approach using DP-EBMs as the base learner.
DP-EBMs are interpretable, high-accuracy models with privacy guarantees, which
allow us to directly observe the impact of DP noise on the learned causal
model. Our experiments show that multi-stage CATE estimators incur larger
accuracy loss than single-stage CATE or ATE estimators and that most of the
accuracy loss from differential privacy is due to an increase in variance, not
biased estimates of treatment effects.","['Fengshi Niu', 'Harsha Nori', 'Brian Quistorff', 'Rich Caruana', 'Donald Ngwe', 'Aadharsh Kannan']","['stat.ML', 'cs.CR', 'cs.LG', 'econ.EM']",2022-02-22 17:21:18+00:00
http://arxiv.org/abs/2202.12707v2,Benchmarking Generative Latent Variable Models for Speech,"Stochastic latent variable models (LVMs) achieve state-of-the-art performance
on natural image generation but are still inferior to deterministic models on
speech. In this paper, we develop a speech benchmark of popular temporal LVMs
and compare them against state-of-the-art deterministic models. We report the
likelihood, which is a much used metric in the image domain, but rarely, or
incomparably, reported for speech models. To assess the quality of the learned
representations, we also compare their usefulness for phoneme recognition.
Finally, we adapt the Clockwork VAE, a state-of-the-art temporal LVM for video
generation, to the speech domain. Despite being autoregressive only in latent
space, we find that the Clockwork VAE can outperform previous LVMs and reduce
the gap to deterministic models by using a hierarchy of latent variables.","['Jakob D. Havtorn', 'Lasse Borgholt', 'Søren Hauberg', 'Jes Frellsen', 'Lars Maaløe']","['eess.AS', 'cs.AI', 'cs.LG', 'cs.SD', 'stat.ML']",2022-02-22 14:35:35+00:00
http://arxiv.org/abs/2202.10913v1,Distributed Sparse Multicategory Discriminant Analysis,"This paper proposes a convex formulation for sparse multicategory linear
discriminant analysis and then extend it to the distributed setting when data
are stored across multiple sites. The key observation is that for the purpose
of classification it suffices to recover the discriminant subspace which is
invariant to orthogonal transformations. Theoretically, we establish
statistical properties ensuring that the distributed sparse multicategory
linear discriminant analysis performs as good as the centralized version after
{a few rounds} of communications. Numerical studies lend strong support to our
methodology and theory.","['Hengchao Chen', 'Qiang Sun']","['math.ST', 'stat.ML', 'stat.TH']",2022-02-22 14:23:33+00:00
http://arxiv.org/abs/2202.10903v2,Confident Neural Network Regression with Bootstrapped Deep Ensembles,"With the rise of the popularity and usage of neural networks, trustworthy
uncertainty estimation is becoming increasingly essential. One of the most
prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et
al., 2017) . A classical parametric model has uncertainty in the parameters due
to the fact that the data on which the model is build is a random sample. A
modern neural network has an additional uncertainty component since the
optimization of the network is random. Lakshminarayanan et al. (2017) noted
that Deep Ensembles do not incorporate the classical uncertainty induced by the
effect of finite data. In this paper, we present a computationally cheap
extension of Deep Ensembles for the regression setting, called Bootstrapped
Deep Ensembles, that explicitly takes this classical effect of finite data into
account using a modified version of the parametric bootstrap. We demonstrate
through an experimental study that our method significantly improves upon
standard Deep Ensembles","['Laurens Sluijterman', 'Eric Cator', 'Tom Heskes']","['stat.ML', 'cs.LG', '62F40']",2022-02-22 14:08:24+00:00
http://arxiv.org/abs/2202.10887v5,Policy Evaluation for Temporal and/or Spatial Dependent Experiments,"The aim of this paper is to establish a causal link between the policies
implemented by technology companies and the outcomes they yield within
intricate temporal and/or spatial dependent experiments. We propose a novel
temporal/spatio-temporal Varying Coefficient Decision Process (VCDP) model,
capable of effectively capturing the evolving treatment effects in situations
characterized by temporal and/or spatial dependence. Our methodology
encompasses the decomposition of the Average Treatment Effect (ATE) into the
Direct Effect (DE) and the Indirect Effect (IE). We subsequently devise
comprehensive procedures for estimating and making inferences about both DE and
IE. Additionally, we provide a rigorous analysis of the statistical properties
of these procedures, such as asymptotic power. To substantiate the
effectiveness of our approach, we carry out extensive simulations and real data
analyses.","['Shikai Luo', 'Ying Yang', 'Chengchun Shi', 'Fang Yao', 'Jieping Ye', 'Hongtu Zhu']","['stat.ME', 'cs.LG', 'stat.ML']",2022-02-22 13:38:14+00:00
http://arxiv.org/abs/2202.10885v1,Learning Infomax and Domain-Independent Representations for Causal Effect Inference with Real-World Data,"The foremost challenge to causal inference with real-world data is to handle
the imbalance in the covariates with respect to different treatment options,
caused by treatment selection bias. To address this issue, recent literature
has explored domain-invariant representation learning based on different domain
divergence metrics (e.g., Wasserstein distance, maximum mean discrepancy,
position-dependent metric, and domain overlap). In this paper, we reveal the
weaknesses of these strategies, i.e., they lead to the loss of predictive
information when enforcing the domain invariance; and the treatment effect
estimation performance is unstable, which heavily relies on the characteristics
of the domain distributions and the choice of domain divergence metrics.
Motivated by information theory, we propose to learn the Infomax and
Domain-Independent Representations to solve the above puzzles. Our method
utilizes the mutual information between the global feature representations and
individual feature representations, and the mutual information between feature
representations and treatment assignment predictions, in order to maximally
capture the common predictive information for both treatment and control
groups. Moreover, our method filters out the influence of instrumental and
irrelevant variables, and thus it effectively increases the predictive ability
of potential outcomes. Experimental results on both the synthetic and
real-world datasets show that our method achieves state-of-the-art performance
on causal effect inference. Moreover, our method exhibits reliable prediction
performances when facing data with different characteristics of data
distributions, complicated variable types, and severe covariate imbalance.","['Zhixuan Chu', 'Stephen Rathbun', 'Sheng Li']","['stat.ML', 'cs.LG']",2022-02-22 13:35:15+00:00
http://arxiv.org/abs/2202.10815v1,Robust and Provable Guarantees for Sparse Random Embeddings,"In this work, we improve upon the guarantees for sparse random embeddings, as
they were recently provided and analyzed by Freksen at al. (NIPS'18) and
Jagadeesan (NIPS'19). Specifically, we show that (a) our bounds are explicit as
opposed to the asymptotic guarantees provided previously, and (b) our bounds
are guaranteed to be sharper by practically significant constants across a wide
range of parameters, including the dimensionality, sparsity and dispersion of
the data. Moreover, we empirically demonstrate that our bounds significantly
outperform prior works on a wide range of real-world datasets, such as
collections of images, text documents represented as bags-of-words, and text
sequences vectorized by neural embeddings. Behind our numerical improvements
are techniques of broader interest, which improve upon key steps of previous
analyses in terms of (c) tighter estimates for certain types of quadratic
chaos, (d) establishing extreme properties of sparse linear forms, and (e)
improvements on bounds for the estimation of sums of independent random
variables.","['Maciej Skorski', 'Alessandro Temperoni', 'Martin Theobald']","['cs.LG', 'stat.ML']",2022-02-22 11:15:59+00:00
http://arxiv.org/abs/2202.10806v4,Stochastic Causal Programming for Bounding Treatment Effects,"Causal effect estimation is important for many tasks in the natural and
social sciences. We design algorithms for the continuous partial identification
problem: bounding the effects of multivariate, continuous treatments when
unmeasured confounding makes identification impossible. Specifically, we cast
causal effects as objective functions within a constrained optimization
problem, and minimize/maximize these functions to obtain bounds. We combine
flexible learning algorithms with Monte Carlo methods to implement a family of
solutions under the name of stochastic causal programming. In particular, we
show how the generic framework can be efficiently formulated in settings where
auxiliary variables are clustered into pre-treatment and post-treatment sets,
where no fine-grained causal graph can be easily specified. In these settings,
we can avoid the need for fully specifying the distribution family of hidden
common causes. Monte Carlo computation is also much simplified, leading to
algorithms which are more computationally stable against alternatives.","['Kirtan Padh', 'Jakob Zeitler', 'David Watson', 'Matt Kusner', 'Ricardo Silva', 'Niki Kilbertus']","['stat.ML', 'cs.LG']",2022-02-22 10:55:24+00:00
http://arxiv.org/abs/2202.10793v6,PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and Directed Graphs,"Networks are ubiquitous in many real-world applications (e.g., social
networks encoding trust/distrust relationships, correlation networks arising
from time series data). While many networks are signed or directed, or both,
there is a lack of unified software packages on graph neural networks (GNNs)
specially designed for signed and directed networks. In this paper, we present
PyTorch Geometric Signed Directed (PyGSD), a software package which fills this
gap. Along the way, we evaluate the implemented methods with experiments with a
view to providing insights into which method to choose for a given task. The
deep learning framework consists of easy-to-use GNN models, synthetic and
real-world data, as well as task-specific evaluation metrics and loss functions
for signed and directed networks. As an extension library for PyG, our proposed
software is maintained with open-source releases, detailed documentation,
continuous integration, unit tests and code coverage checks. The GitHub
repository of the library is
https://github.com/SherylHYX/pytorch_geometric_signed_directed.","['Yixuan He', 'Xitong Zhang', 'Junjie Huang', 'Benedek Rozemberczki', 'Mihai Cucuringu', 'Gesine Reinert']","['cs.LG', 'cs.AI', 'cs.SI', 'stat.ML']",2022-02-22 10:25:59+00:00
http://arxiv.org/abs/2202.10788v1,Explicit Regularization via Regularizer Mirror Descent,"Despite perfectly interpolating the training data, deep neural networks
(DNNs) can often generalize fairly well, in part due to the ""implicit
regularization"" induced by the learning algorithm. Nonetheless, various forms
of regularization, such as ""explicit regularization"" (via weight decay), are
often used to avoid overfitting, especially when the data is corrupted. There
are several challenges with explicit regularization, most notably unclear
convergence properties. Inspired by convergence properties of stochastic mirror
descent (SMD) algorithms, we propose a new method for training DNNs with
regularization, called regularizer mirror descent (RMD). In highly
overparameterized DNNs, SMD simultaneously interpolates the training data and
minimizes a certain potential function of the weights. RMD starts with a
standard cost which is the sum of the training loss and a convex regularizer of
the weights. Reinterpreting this cost as the potential of an ""augmented""
overparameterized network and applying SMD yields RMD. As a result, RMD
inherits the properties of SMD and provably converges to a point ""close"" to the
minimizer of this cost. RMD is computationally comparable to stochastic
gradient descent (SGD) and weight decay, and is parallelizable in the same
manner. Our experimental results on training sets with various levels of
corruption suggest that the generalization performance of RMD is remarkably
robust and significantly better than both SGD and weight decay, which
implicitly and explicitly regularize the $\ell_2$ norm of the weights. RMD can
also be used to regularize the weights to a desired weight vector, which is
particularly relevant for continual learning.","['Navid Azizan', 'Sahin Lale', 'Babak Hassibi']","['cs.LG', 'math.OC', 'stat.ML']",2022-02-22 10:21:44+00:00
http://arxiv.org/abs/2202.10746v4,CD-ROM: Complemented Deep-Reduced Order Model,"Model order reduction through the POD-Galerkin method can lead to dramatic
gains in terms of computational efficiency in solving physical problems.
However, the applicability of the method to non linear high-dimensional
dynamical systems such as the Navier-Stokes equations has been shown to be
limited, producing inaccurate and sometimes unstable models. This paper
proposes a deep learning based closure modeling approach for classical
POD-Galerkin reduced order models (ROM). The proposed approach is theoretically
grounded, using neural networks to approximate well studied operators. In
contrast with most previous works, the present CD-ROM approach is based on an
interpretable continuous memory formulation, derived from simple hypotheses on
the behavior of partially observed dynamical systems. The final corrected
models can hence be simulated using most classical time stepping schemes. The
capabilities of the CD-ROM approach are demonstrated on two classical examples
from Computational Fluid Dynamics, as well as a parametric case, the
Kuramoto-Sivashinsky equation.","['Emmanuel Menier', 'Michele Alessandro Bucci', 'Mouadh Yagoubi', 'Lionel Mathelin', 'Marc Schoenauer']","['physics.flu-dyn', 'cs.LG', 'stat.ML']",2022-02-22 09:05:06+00:00
http://arxiv.org/abs/2202.10723v1,Sobolev Transport: A Scalable Metric for Probability Measures with Graph Metrics,"Optimal transport (OT) is a popular measure to compare probability
distributions. However, OT suffers a few drawbacks such as (i) a high
complexity for computation, (ii) indefiniteness which limits its applicability
to kernel machines. In this work, we consider probability measures supported on
a graph metric space and propose a novel Sobolev transport metric. We show that
the Sobolev transport metric yields a closed-form formula for fast computation
and it is negative definite. We show that the space of probability measures
endowed with this transport distance is isometric to a bounded convex set in a
Euclidean space with a weighted $\ell_p$ distance. We further exploit the
negative definiteness of the Sobolev transport to design positive-definite
kernels, and evaluate their performances against other baselines in document
classification with word embeddings and in topological data analysis.","['Tam Le', 'Truyen Nguyen', 'Dinh Phung', 'Viet Anh Nguyen']","['cs.LG', 'cs.AI', 'stat.ML']",2022-02-22 08:27:58+00:00
http://arxiv.org/abs/2202.10670v3,From Optimization Dynamics to Generalization Bounds via Łojasiewicz Gradient Inequality,"Optimization and generalization are two essential aspects of statistical
machine learning. In this paper, we propose a framework to connect optimization
with generalization by analyzing the generalization error based on the
optimization trajectory under the gradient flow algorithm. The key ingredient
of this framework is the Uniform-LGI, a property that is generally satisfied
when training machine learning models. Leveraging the Uniform-LGI, we first
derive convergence rates for gradient flow algorithm, then we give
generalization bounds for a large class of machine learning models. We further
apply our framework to three distinct machine learning models: linear
regression, kernel regression, and two-layer neural networks. Through our
approach, we obtain generalization estimates that match or extend previous
results.","['Fusheng Liu', 'Haizhao Yang', 'Soufiane Hayou', 'Qianxiao Li']","['stat.ML', 'cs.LG']",2022-02-22 04:58:16+00:00
http://arxiv.org/abs/2202.10669v1,On Uncertainty Estimation by Tree-based Surrogate Models in Sequential Model-based Optimization,"Sequential model-based optimization sequentially selects a candidate point by
constructing a surrogate model with the history of evaluations, to solve a
black-box optimization problem. Gaussian process (GP) regression is a popular
choice as a surrogate model, because of its capability of calculating
prediction uncertainty analytically. On the other hand, an ensemble of
randomized trees is another option and has practical merits over GPs due to its
scalability and easiness of handling continuous/discrete mixed variables. In
this paper we revisit various ensembles of randomized trees to investigate
their behavior in the perspective of prediction uncertainty estimation. Then,
we propose a new way of constructing an ensemble of randomized trees, referred
to as BwO forest, where bagging with oversampling is employed to construct
bootstrapped samples that are used to build randomized trees with random
splitting. Experimental results demonstrate the validity and good performance
of BwO forest over existing tree-based models in various circumstances.","['Jungtaek Kim', 'Seungjin Choi']","['stat.ML', 'cs.LG', 'math.OC']",2022-02-22 04:50:37+00:00
http://arxiv.org/abs/2202.10662v2,Random Graph Matching in Geometric Models: the Case of Complete Graphs,"This paper studies the problem of matching two complete graphs with edge
weights correlated through latent geometries, extending a recent line of
research on random graph matching with independent edge weights to geometric
models. Specifically, given a random permutation $\pi^*$ on $[n]$ and $n$ iid
pairs of correlated Gaussian vectors $\{X_{\pi^*(i)}, Y_i\}$ in $\mathbb{R}^d$
with noise parameter $\sigma$, the edge weights are given by
$A_{ij}=\kappa(X_i,X_j)$ and $B_{ij}=\kappa(Y_i,Y_j)$ for some link function
$\kappa$. The goal is to recover the hidden vertex correspondence $\pi^*$ based
on the observation of $A$ and $B$. We focus on the dot-product model with
$\kappa(x,y)=\langle x, y \rangle$ and Euclidean distance model with
$\kappa(x,y)=\|x-y\|^2$, in the low-dimensional regime of $d=o(\log n)$ wherein
the underlying geometric structures are most evident. We derive an approximate
maximum likelihood estimator, which provably achieves, with high probability,
perfect recovery of $\pi^*$ when $\sigma=o(n^{-2/d})$ and almost perfect
recovery with a vanishing fraction of errors when $\sigma=o(n^{-1/d})$.
Furthermore, these conditions are shown to be information-theoretically optimal
even when the latent coordinates $\{X_i\}$ and $\{Y_i\}$ are observed,
complementing the recent results of [DCK19] and [KNW22] in geometric models of
the planted bipartite matching problem. As a side discovery, we show that the
celebrated spectral algorithm of [Ume88] emerges as a further approximation to
the maximum likelihood in the geometric model.","['Haoyu Wang', 'Yihong Wu', 'Jiaming Xu', 'Israel Yolou']","['math.ST', 'math.PR', 'stat.ML', 'stat.TH']",2022-02-22 04:14:45+00:00
http://arxiv.org/abs/2202.10660v1,Batched Dueling Bandits,"The $K$-armed dueling bandit problem, where the feedback is in the form of
noisy pairwise comparisons, has been widely studied. Previous works have only
focused on the sequential setting where the policy adapts after every
comparison. However, in many applications such as search ranking and
recommendation systems, it is preferable to perform comparisons in a limited
number of parallel batches. We study the batched $K$-armed dueling bandit
problem under two standard settings: (i) existence of a Condorcet winner, and
(ii) strong stochastic transitivity and stochastic triangle inequality. For
both settings, we obtain algorithms with a smooth trade-off between the number
of batches and regret. Our regret bounds match the best known sequential regret
bounds (up to poly-logarithmic factors), using only a logarithmic number of
batches. We complement our regret analysis with a nearly-matching lower bound.
Finally, we also validate our theoretical results via experiments on synthetic
and real data.","['Arpit Agarwal', 'Rohan Ghuge', 'Viswanath Nagarajan']","['cs.LG', 'stat.ML']",2022-02-22 04:02:36+00:00
http://arxiv.org/abs/2202.10638v3,Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations,"Data augmentation is commonly applied to improve performance of deep learning
by enforcing the knowledge that certain transformations on the input preserve
the output. Currently, the data augmentation parameters are chosen by human
effort and costly cross-validation, which makes it cumbersome to apply to new
datasets. We develop a convenient gradient-based method for selecting the data
augmentation without validation data during training of a deep neural network.
Our approach relies on phrasing data augmentation as an invariance in the prior
distribution on the functions of a neural network, which allows us to learn it
using Bayesian model selection. This has been shown to work in Gaussian
processes, but not yet for deep neural networks. We propose a differentiable
Kronecker-factored Laplace approximation to the marginal likelihood as our
objective, which can be optimised without human supervision or validation data.
We show that our method can successfully recover invariances present in the
data, and that this improves generalisation and data efficiency on image
datasets.","['Alexander Immer', 'Tycho F. A. van der Ouderaa', 'Gunnar Rätsch', 'Vincent Fortuin', 'Mark van der Wilk']","['stat.ML', 'cs.LG']",2022-02-22 02:51:11+00:00
http://arxiv.org/abs/2202.10615v2,On Average-Case Error Bounds for Kernel-Based Bayesian Quadrature,"In this paper, we study error bounds for {\em Bayesian quadrature} (BQ), with
an emphasis on noisy settings, randomized algorithms, and average-case
performance measures. We seek to approximate the integral of functions in a
{\em Reproducing Kernel Hilbert Space} (RKHS), particularly focusing on the
Mat\'ern-$\nu$ and squared exponential (SE) kernels, with samples from the
function potentially being corrupted by Gaussian noise. We provide a two-step
meta-algorithm that serves as a general tool for relating the average-case
quadrature error with the $L^2$-function approximation error. When specialized
to the Mat\'ern kernel, we recover an existing near-optimal error rate while
avoiding the existing method of repeatedly sampling points. When specialized to
other settings, we obtain new average-case results for settings including the
SE kernel with noise and the Mat\'ern kernel with misspecification. Finally, we
present algorithm-independent lower bounds that have greater generality and/or
give distinct proofs compared to existing ones.","['Xu Cai', 'Chi Thanh Lam', 'Jonathan Scarlett']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",2022-02-22 01:49:41+00:00
http://arxiv.org/abs/2202.10613v3,Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces,"Bayesian learning using Gaussian processes provides a foundational framework
for making decisions in a manner that balances what is known with what could be
learned by gathering data. In this dissertation, we develop techniques for
broadening the applicability of Gaussian processes. This is done in two ways.
Firstly, we develop pathwise conditioning techniques for Gaussian processes,
which allow one to express posterior random functions as prior random functions
plus a dependent update term. We introduce a wide class of efficient
approximations built from this viewpoint, which can be randomly sampled once in
advance, and evaluated at arbitrary locations without any subsequent
stochasticity. This key property improves efficiency and makes it simpler to
deploy Gaussian process models in decision-making settings. Secondly, we
develop a collection of Gaussian process models over non-Euclidean spaces,
including Riemannian manifolds and graphs. We derive fully constructive
expressions for the covariance kernels of scalar-valued Gaussian processes on
Riemannian manifolds and graphs. Building on these ideas, we describe a
formalism for defining vector-valued Gaussian processes on Riemannian
manifolds. The introduced techniques allow all of these models to be trained
using standard computational methods. In total, these contributions make
Gaussian processes easier to work with and allow them to be used within a wider
class of domains in an effective and principled manner. This, in turn, makes it
possible to potentially apply Gaussian processes to novel decision-making
settings.",['Alexander Terenin'],"['stat.ML', 'cs.LG']",2022-02-22 01:42:57+00:00
http://arxiv.org/abs/2202.10600v2,Myriad: a real-world testbed to bridge trajectory optimization and deep learning,"We present Myriad, a testbed written in JAX for learning and planning in
real-world continuous environments. The primary contributions of Myriad are
threefold. First, Myriad provides machine learning practitioners access to
trajectory optimization techniques for application within a typical automatic
differentiation workflow. Second, Myriad presents many real-world optimal
control problems, ranging from biology to medicine to engineering, for use by
the machine learning community. Formulated in continuous space and time, these
environments retain some of the complexity of real-world systems often
abstracted away by standard benchmarks. As such, Myriad strives to serve as a
stepping stone towards application of modern machine learning techniques for
impactful real-world tasks. Finally, we use the Myriad repository to showcase a
novel approach for learning and control tasks. Trained in a fully end-to-end
fashion, our model leverages an implicit planning module over neural ordinary
differential equations, enabling simultaneous learning and planning with
complex environment dynamics.","['Nikolaus H. R. Howe', 'Simon Dufort-Labbé', 'Nitarshan Rajkumar', 'Pierre-Luc Bacon']","['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'stat.ML']",2022-02-22 00:47:14+00:00
http://arxiv.org/abs/2202.10589v5,Off-Policy Confidence Interval Estimation with Confounded Markov Decision Process,"This paper is concerned with constructing a confidence interval for a target
policy's value offline based on a pre-collected observational data in infinite
horizon settings. Most of the existing works assume no unmeasured variables
exist that confound the observed actions. This assumption, however, is likely
to be violated in real applications such as healthcare and technological
industries. In this paper, we show that with some auxiliary variables that
mediate the effect of actions on the system dynamics, the target policy's value
is identifiable in a confounded Markov decision process. Based on this result,
we develop an efficient off-policy value estimator that is robust to potential
model misspecification and provide rigorous uncertainty quantification. Our
method is justified by theoretical results, simulated and real datasets
obtained from ridesharing companies. A Python implementation of the proposed
procedure is available at https://github.com/Mamba413/cope.","['Chengchun Shi', 'Jin Zhu', 'Ye Shen', 'Shikai Luo', 'Hongtu Zhu', 'Rui Song']","['stat.ML', 'cs.LG']",2022-02-22 00:03:48+00:00
http://arxiv.org/abs/2202.10574v4,A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation in Two-sided Markets,"The two-sided markets such as ride-sharing companies often involve a group of
subjects who are making sequential decisions across time and/or location. With
the rapid development of smart phones and internet of things, they have
substantially transformed the transportation landscape of human beings. In this
paper we consider large-scale fleet management in ride-sharing companies that
involve multiple units in different areas receiving sequences of products (or
treatments) over time. Major technical challenges, such as policy evaluation,
arise in those studies because (i) spatial and temporal proximities induce
interference between locations and times; and (ii) the large number of
locations results in the curse of dimensionality. To address both challenges
simultaneously, we introduce a multi-agent reinforcement learning (MARL)
framework for carrying policy evaluation in these studies. We propose novel
estimators for mean outcomes under different products that are consistent
despite the high-dimensionality of state-action space. The proposed estimator
works favorably in simulation experiments. We further illustrate our method
using a real dataset obtained from a two-sided marketplace company to evaluate
the effects of applying different subsidizing policies. A Python implementation
of our proposed method is available at
https://github.com/RunzheStat/CausalMARL.","['Chengchun Shi', 'Runzhe Wan', 'Ge Song', 'Shikai Luo', 'Rui Song', 'Hongtu Zhu']","['stat.ML', 'cs.LG']",2022-02-21 23:36:40+00:00
http://arxiv.org/abs/2202.10506v2,Accelerating Primal-dual Methods for Regularized Markov Decision Processes,"Entropy regularized Markov decision processes have been widely used in
reinforcement learning. This paper is concerned with the primal-dual
formulation of the entropy regularized problems. Standard first-order methods
suffer from slow convergence due to the lack of strict convexity and concavity.
To address this issue, we first introduce a new quadratically convexified
primal-dual formulation. The natural gradient ascent descent of the new
formulation enjoys global convergence guarantee and exponential convergence
rate. We also propose a new interpolating metric that further accelerates the
convergence significantly. Numerical results are provided to demonstrate the
performance of the proposed methods under multiple settings.","['Haoya Li', 'Hsiang-fu Yu', 'Lexing Ying', 'Inderjit Dhillon']","['math.OC', 'cs.LG', 'stat.ML']",2022-02-21 19:38:33+00:00
http://arxiv.org/abs/2202.10464v1,A Globally Convergent Evolutionary Strategy for Stochastic Constrained Optimization with Applications to Reinforcement Learning,"Evolutionary strategies have recently been shown to achieve competing levels
of performance for complex optimization problems in reinforcement learning. In
such problems, one often needs to optimize an objective function subject to a
set of constraints, including for instance constraints on the entropy of a
policy or to restrict the possible set of actions or states accessible to an
agent. Convergence guarantees for evolutionary strategies to optimize
stochastic constrained problems are however lacking in the literature. In this
work, we address this problem by designing a novel optimization algorithm with
a sufficient decrease mechanism that ensures convergence and that is based only
on estimates of the functions. We demonstrate the applicability of this
algorithm on two types of experiments: i) a control task for maximizing rewards
and ii) maximizing rewards subject to a non-relaxable set of constraints.","['Youssef Diouane', 'Aurelien Lucchi', 'Vihang Patil']","['cs.NE', 'cs.LG', 'math.OC', 'stat.ML']",2022-02-21 17:04:51+00:00
http://arxiv.org/abs/2202.10244v1,Stochastic Modeling of Inhomogeneities in the Aortic Wall and Uncertainty Quantification using a Bayesian Encoder-Decoder Surrogate,"Inhomogeneities in the aortic wall can lead to localized stress
accumulations, possibly initiating dissection. In many cases, a dissection
results from pathological changes such as fragmentation or loss of elastic
fibers. But it has been shown that even the healthy aortic wall has an inherent
heterogeneous microstructure. Some parts of the aorta are particularly
susceptible to the development of inhomogeneities due to pathological changes,
however, the distribution in the aortic wall and the spatial extent, such as
size, shape, and type, are difficult to predict. Motivated by this observation,
we describe the heterogeneous distribution of elastic fiber degradation in the
dissected aortic wall using a stochastic constitutive model. For this purpose,
random field realizations, which model the stochastic distribution of degraded
elastic fibers, are generated over a non-equidistant grid. The random field
then serves as input for a uni-axial extension test of the pathological aortic
wall, solved with the finite-element (FE) method. To include the microstructure
of the dissected aortic wall, a constitutive model developed in a previous
study is applied, which also includes an approach to model the degradation of
inter-lamellar elastic fibers. Then to assess the uncertainty in the output
stress distribution due to this stochastic constitutive model, a convolutional
neural network, specifically a Bayesian encoder-decoder, was used as a
surrogate model that maps the random input fields to the output stress
distribution obtained from the FE analysis. The results show that the neural
network is able to predict the stress distribution of the FE analysis while
significantly reducing the computational time. In addition, it provides the
probability for exceeding critical stresses within the aortic wall, which could
allow for the prediction of delamination or fatal rupture.","['Sascha Ranftl', 'Malte Rolf-Pissarczyk', 'Gloria Wolkerstorfer', 'Antonio Pepe', 'Jan Egger', 'Wolfgang von der Linden', 'Gerhard A. Holzapfel']","['stat.ML', 'cs.LG', 'physics.bio-ph']",2022-02-21 13:53:37+00:00
http://arxiv.org/abs/2202.10194v1,Low-Dimensional High-Fidelity Kinetic Models for NOX Formation by a Compute Intensification Method,"A novel compute intensification methodology to the construction of
low-dimensional, high-fidelity ""compact"" kinetic models for NOX formation is
designed and demonstrated. The method adapts the data intensive Machine Learned
Optimization of Chemical Kinetics (MLOCK) algorithm for compact model
generation by the use of a Latin Square method for virtual reaction network
generation. A set of logical rules are defined which construct a minimally
sized virtual reaction network comprising three additional nodes (N, NO, NO2).
This NOX virtual reaction network is appended to a pre-existing compact model
for methane combustion comprising fifteen nodes.
  The resulting eighteen node virtual reaction network is processed by the
MLOCK coded algorithm to produce a plethora of compact model candidates for NOX
formation during methane combustion. MLOCK automatically; populates the terms
of the virtual reaction network with candidate inputs; measures the success of
the resulting compact model candidates (in reproducing a broad set of gas
turbine industry-defined performance targets); selects regions of input
parameters space showing models of best performance; refines the input
parameters to give better performance; and makes an ultimate selection of the
best performing model or models.
  By this method, it is shown that a number of compact model candidates exist
that show fidelities in excess of 75% in reproducing industry defined
performance targets, with one model valid to >75% across fuel/air equivalence
ratios of 0.5-1.0. However, to meet the full fuel/air equivalence ratio
performance envelope defined by industry, we show that with this minimal
virtual reaction network, two further compact models are required.","['Mark Kelly', 'Harry Dunne', 'Gilles Bourque', 'Stephen Dooley']","['physics.chem-ph', 'cs.DC', 'cs.LG', 'physics.comp-ph', 'stat.ML']",2022-02-21 13:08:01+00:00
http://arxiv.org/abs/2202.10153v2,Inferring Lexicographically-Ordered Rewards from Preferences,"Modeling the preferences of agents over a set of alternatives is a principal
concern in many areas. The dominant approach has been to find a single
reward/utility function with the property that alternatives yielding higher
rewards are preferred over alternatives yielding lower rewards. However, in
many settings, preferences are based on multiple, often competing, objectives;
a single reward function is not adequate to represent such preferences. This
paper proposes a method for inferring multi-objective reward-based
representations of an agent's observed preferences. We model the agent's
priorities over different objectives as entering lexicographically, so that
objectives with lower priorities matter only when the agent is indifferent with
respect to objectives with higher priorities. We offer two example applications
in healthcare, one inspired by cancer treatment, the other inspired by organ
transplantation, to illustrate how the lexicographically-ordered rewards we
learn can provide a better understanding of a decision-maker's preferences and
help improve policies when used in reinforcement learning.","['Alihan Hüyük', 'William R. Zame', 'Mihaela van der Schaar']","['cs.LG', 'cs.AI', 'stat.ML']",2022-02-21 12:01:41+00:00
http://arxiv.org/abs/2202.10125v1,ABO3 Perovskites' Formability Prediction and Crystal Structure Classification using Machine Learning,"Renewable energy sources are of great interest to combat global warming, yet
promising sources like photovoltaic (PV) cells are not efficient and cheap
enough to act as an alternative to traditional energy sources. Perovskite has
high potential as a PV material but engineering the right material for a
specific application is often a lengthy process. In this paper, ABO3 type
perovskites' formability is predicted and its crystal structure is classified
using machine learning with high accuracy, which provides a fast screening
process. Although the study was done with solar-cell application in mind, the
prediction framework is generic enough to be used for other purposes.
Formability of perovskite is predicted and its crystal structure is classified
with an accuracy of 98.57% and 90.53% respectively using Random Forest after
5-fold cross-validation. Our machine learning model may aid in the accelerated
development of a desired perovskite structure by providing a quick mechanism to
get insight into the material's properties in advance.","['Minhaj Uddin Ahmad', 'A. Abdur Rahman Akib', 'Md. Mohsin Sarker Raihan', 'Abdullah Bin Shams']","['cond-mat.mtrl-sci', 'stat.ML']",2022-02-21 11:15:10+00:00
http://arxiv.org/abs/2202.10103v2,Robustness and Accuracy Could Be Reconcilable by (Proper) Definition,"The trade-off between robustness and accuracy has been widely studied in the
adversarial literature. Although still controversial, the prevailing view is
that this trade-off is inherent, either empirically or theoretically. Thus, we
dig for the origin of this trade-off in adversarial training and find that it
may stem from the improperly defined robust error, which imposes an inductive
bias of local invariance -- an overcorrection towards smoothness. Given this,
we advocate employing local equivariance to describe the ideal behavior of a
robust model, leading to a self-consistent robust error named SCORE. By
definition, SCORE facilitates the reconciliation between robustness and
accuracy, while still handling the worst-case uncertainty via robust
optimization. By simply substituting KL divergence with variants of distance
metrics, SCORE can be efficiently minimized. Empirically, our models achieve
top-rank performance on RobustBench under AutoAttack. Besides, SCORE provides
instructive insights for explaining the overfitting phenomenon and semantic
input gradients observed on robust models. Code is available at
https://github.com/P2333/SCORE.","['Tianyu Pang', 'Min Lin', 'Xiao Yang', 'Jun Zhu', 'Shuicheng Yan']","['cs.LG', 'cs.CR', 'stat.ML']",2022-02-21 10:36:09+00:00
http://arxiv.org/abs/2202.10066v2,Multi-task Representation Learning with Stochastic Linear Bandits,"We study the problem of transfer-learning in the setting of stochastic linear
bandit tasks. We consider that a low dimensional linear representation is
shared across the tasks, and study the benefit of learning this representation
in the multi-task learning setting. Following recent results to design
stochastic bandit policies, we propose an efficient greedy policy based on
trace norm regularization. It implicitly learns a low dimensional
representation by encouraging the matrix formed by the task regression vectors
to be of low rank. Unlike previous work in the literature, our policy does not
need to know the rank of the underlying matrix. We derive an upper bound on the
multi-task regret of our policy, which is, up to logarithmic factors, of order
$\sqrt{NdT(T+d)r}$, where $T$ is the number of tasks, $r$ the rank, $d$ the
number of variables and $N$ the number of rounds per task. We show the benefit
of our strategy compared to the baseline $Td\sqrt{N}$ obtained by solving each
task independently. We also provide a lower bound to the multi-task regret.
Finally, we corroborate our theoretical findings with preliminary experiments
on synthetic data.","['Leonardo Cella', 'Karim Lounici', 'Grégoire Pacreau', 'Massimiliano Pontil']","['stat.ML', 'cs.LG']",2022-02-21 09:26:34+00:00
http://arxiv.org/abs/2202.10923v1,MSTGD:A Memory Stochastic sTratified Gradient Descent Method with an Exponential Convergence Rate,"The fluctuation effect of gradient expectation and variance caused by
parameter update between consecutive iterations is neglected or confusing by
current mainstream gradient optimization algorithms.Using this fluctuation
effect, combined with the stratified sampling strategy, this paper designs a
novel \underline{M}emory \underline{S}tochastic s\underline{T}ratified Gradient
Descend(\underline{MST}GD) algorithm with an exponential convergence rate.
Specifically, MSTGD uses two strategies for variance reduction: the first
strategy is to perform variance reduction according to the proportion p of used
historical gradient, which is estimated from the mean and variance of sample
gradients before and after iteration, and the other strategy is stratified
sampling by category. The statistic \ $\bar{G}_{mst}$\ designed under these two
strategies can be adaptively unbiased, and its variance decays at a geometric
rate. This enables MSTGD based on $\bar{G}_{mst}$ to obtain an exponential
convergence rate of the form $\lambda^{2(k-k_0)}$($\lambda\in (0,1)$,k is the
number of iteration steps,$\lambda$ is a variable related to proportion
p).Unlike most other algorithms that claim to achieve an exponential
convergence rate, the convergence rate is independent of parameters such as
dataset size N, batch size n, etc., and can be achieved at a constant step
size.Theoretical and experimental results show the effectiveness of MSTGD","['Aixiang', 'Chen', 'Jinting Zhang', 'Zanbo Zhang', 'Zhihong Li']","['stat.ML', 'cs.LG']",2022-02-21 01:36:26+00:00
http://arxiv.org/abs/2202.09931v2,Deconstructing Distributions: A Pointwise Framework of Learning,"In machine learning, we traditionally evaluate the performance of a single
model, averaged over a collection of test inputs. In this work, we propose a
new approach: we measure the performance of a collection of models when
evaluated on a $\textit{single input point}$. Specifically, we study a point's
$\textit{profile}$: the relationship between models' average performance on the
test distribution and their pointwise performance on this individual point. We
find that profiles can yield new insights into the structure of both models and
data -- in and out-of-distribution. For example, we empirically show that real
data distributions consist of points with qualitatively different profiles. On
one hand, there are ""compatible"" points with strong correlation between the
pointwise and average performance. On the other hand, there are points with
weak and even $\textit{negative}$ correlation: cases where improving overall
model accuracy actually $\textit{hurts}$ performance on these inputs. We prove
that these experimental observations are inconsistent with the predictions of
several simplified models of learning proposed in prior work. As an
application, we use profiles to construct a dataset we call CIFAR-10-NEG: a
subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is
$\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This
illustrates, for the first time, an OOD dataset that completely inverts
""accuracy-on-the-line"" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
Liang, Carmon, and Schmidt 2021)","['Gal Kaplun', 'Nikhil Ghosh', 'Saurabh Garg', 'Boaz Barak', 'Preetum Nakkiran']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2022-02-20 23:25:28+00:00
http://arxiv.org/abs/2202.09924v1,Generalized Bayesian Additive Regression Trees Models: Beyond Conditional Conjugacy,"Bayesian additive regression trees have seen increased interest in recent
years due to their ability to combine machine learning techniques with
principled uncertainty quantification. The Bayesian backfitting algorithm used
to fit BART models, however, limits their application to a small class of
models for which conditional conjugacy exists. In this article, we greatly
expand the domain of applicability of BART to arbitrary \emph{generalized BART}
models by introducing a very simple, tuning-parameter-free, reversible jump
Markov chain Monte Carlo algorithm. Our algorithm requires only that the user
be able to compute the likelihood and (optionally) its gradient and Fisher
information. The potential applications are very broad; we consider examples in
survival analysis, structured heteroskedastic regression, and gamma shape
regression.",['Antonio R. Linero'],"['stat.ML', 'cs.LG', 'stat.ME']",2022-02-20 22:52:07+00:00
http://arxiv.org/abs/2202.09889v2,Memorize to Generalize: on the Necessity of Interpolation in High Dimensional Linear Regression,"We examine the necessity of interpolation in overparameterized models, that
is, when achieving optimal predictive risk in machine learning problems
requires (nearly) interpolating the training data. In particular, we consider
simple overparameterized linear regression $y = X \theta + w$ with random
design $X \in \mathbb{R}^{n \times d}$ under the proportional asymptotics $d/n
\to \gamma \in (1, \infty)$. We precisely characterize how prediction (test)
error necessarily scales with training error in this setting. An implication of
this characterization is that as the label noise variance $\sigma^2 \to 0$, any
estimator that incurs at least $\mathsf{c}\sigma^4$ training error for some
constant $\mathsf{c}$ is necessarily suboptimal and will suffer growth in
excess prediction error at least linear in the training error. Thus, optimal
performance requires fitting training data to substantially higher accuracy
than the inherent noise floor of the problem.","['Chen Cheng', 'John Duchi', 'Rohith Kuditipudi']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-02-20 18:51:45+00:00
http://arxiv.org/abs/2202.09885v2,On Optimal Early Stopping: Over-informative versus Under-informative Parametrization,"Early stopping is a simple and widely used method to prevent over-training
neural networks. We develop theoretical results to reveal the relationship
between the optimal early stopping time and model dimension as well as sample
size of the dataset for certain linear models. Our results demonstrate two very
different behaviors when the model dimension exceeds the number of features
versus the opposite scenario. While most previous works on linear models focus
on the latter setting, we observe that the dimension of the model often exceeds
the number of features arising from data in common deep learning tasks and
propose a model to study this setting. We demonstrate experimentally that our
theoretical results on optimal early stopping time corresponds to the training
process of deep neural networks.","['Ruoqi Shen', 'Liyao Gao', 'Yi-An Ma']","['cs.LG', 'stat.ML']",2022-02-20 18:20:06+00:00
http://arxiv.org/abs/2202.09875v4,Trying to Outrun Causality with Machine Learning: Limitations of Model Explainability Techniques for Identifying Predictive Variables,"Machine Learning explainability techniques have been proposed as a means of
`explaining' or interrogating a model in order to understand why a particular
decision or prediction has been made. Such an ability is especially important
at a time when machine learning is being used to automate decision processes
which concern sensitive factors and legal outcomes. Indeed, it is even a
requirement according to EU law. Furthermore, researchers concerned with
imposing overly restrictive functional form (e.g., as would be the case in a
linear regression) may be motivated to use machine learning algorithms in
conjunction with explainability techniques, as part of exploratory research,
with the goal of identifying important variables which are associated with an
outcome of interest. For example, epidemiologists might be interested in
identifying `risk factors' - i.e. factors which affect recovery from disease -
by using random forests and assessing variable relevance using importance
measures. However, and as we demonstrate, machine learning algorithms are not
as flexible as they might seem, and are instead incredibly sensitive to the
underling causal structure in the data. The consequences of this are that
predictors which are, in fact, critical to a causal system and highly
correlated with the outcome, may nonetheless be deemed by explainability
techniques to be unrelated/unimportant/unpredictive of the outcome. Rather than
this being a limitation of explainability techniques per se, we show that it is
rather a consequence of the mathematical implications of regression, and the
interaction of these implications with the associated conditional
independencies of the underlying causal structure. We provide some alternative
recommendations for researchers wanting to explore the data for important
variables.",['Matthew J. Vowels'],"['stat.ML', 'cs.LG']",2022-02-20 17:48:54+00:00
http://arxiv.org/abs/2202.09867v1,Interacting Contour Stochastic Gradient Langevin Dynamics,"We propose an interacting contour stochastic gradient Langevin dynamics
(ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic
gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show
that ICSGLD can be theoretically more efficient than a single-chain CSGLD with
an equivalent computational budget. We also present a novel random-field
function, which facilitates the estimation of self-adapting parameters in big
data and obtains free mode explorations. Empirically, we compare the proposed
algorithm with popular benchmark methods for posterior sampling. The numerical
results show a great potential of ICSGLD for large-scale uncertainty estimation
tasks.","['Wei Deng', 'Siqi Liang', 'Botao Hao', 'Guang Lin', 'Faming Liang']","['stat.ML', 'cs.LG']",2022-02-20 17:23:09+00:00
http://arxiv.org/abs/2202.09778v2,Pseudo Numerical Methods for Diffusion Models on Manifolds,"Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality
samples such as image and audio samples. However, DDPMs require hundreds to
thousands of iterations to produce final samples. Several prior works have
successfully accelerated DDPMs through adjusting the variance schedule (e.g.,
Improved Denoising Diffusion Probabilistic Models) or the denoising equation
(e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these
acceleration methods cannot maintain the quality of samples and even introduce
new noise at a high speedup rate, which limit their practicability. To
accelerate the inference process while keeping the sample quality, we provide a
fresh perspective that DDPMs should be treated as solving differential
equations on manifolds. Under such a perspective, we propose pseudo numerical
methods for diffusion models (PNDMs). Specifically, we figure out how to solve
differential equations on manifolds and show that DDIMs are simple cases of
pseudo numerical methods. We change several classical numerical methods to
corresponding pseudo numerical methods and find that the pseudo linear
multi-step method is the best in most situations. According to our experiments,
by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can
generate higher quality synthetic images with only 50 steps compared with
1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps
(by around 0.4 in FID) and have good generalization on different variance
schedules. Our implementation is available at
https://github.com/luping-liu/PNDM.","['Luping Liu', 'Yi Ren', 'Zhijie Lin', 'Zhou Zhao']","['cs.CV', 'cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2022-02-20 10:37:52+00:00
http://arxiv.org/abs/2202.09753v3,Finite-Time Analysis of Natural Actor-Critic for POMDPs,"We consider the reinforcement learning problem for partially observed Markov
decision processes (POMDPs) with large or even countably infinite state spaces,
where the controller has access to only noisy observations of the underlying
controlled Markov chain. We consider a natural actor-critic method that employs
a finite internal memory for policy parameterization, and a multi-step temporal
difference learning algorithm for policy evaluation. We establish, to the best
of our knowledge, the first non-asymptotic global convergence of actor-critic
methods for partially observed systems under function approximation. In
particular, in addition to the function approximation and statistical errors
that also arise in MDPs, we explicitly characterize the error due to the use of
finite-state controllers. This additional error is stated in terms of the total
variation distance between the traditional belief state in POMDPs and the
posterior distribution of the hidden state when using a finite-state
controller. Further, we show that this error can be made small in the case of
sliding-block controllers by using larger block sizes.","['Semih Cayci', 'Niao He', 'R. Srikant']","['cs.LG', 'math.OC', 'stat.ML']",2022-02-20 07:42:00+00:00
http://arxiv.org/abs/2202.09724v5,Bayes-Optimal Classifiers under Group Fairness,"Machine learning algorithms are becoming integrated into more and more
high-stakes decision-making processes, such as in social welfare issues. Due to
the need of mitigating the potentially disparate impacts from algorithmic
predictions, many approaches have been proposed in the emerging area of fair
machine learning. However, the fundamental problem of characterizing
Bayes-optimal classifiers under various group fairness constraints has only
been investigated in some special cases. Based on the classical Neyman-Pearson
argument (Neyman and Pearson, 1933; Shao, 2003) for optimal hypothesis testing,
this paper provides a unified framework for deriving Bayes-optimal classifiers
under group fairness. This enables us to propose a group-based thresholding
method we call FairBayes, that can directly control disparity, and achieve an
essentially optimal fairness-accuracy tradeoff. These advantages are supported
by thorough experiments.","['Xianli Zeng', 'Edgar Dobriban', 'Guang Cheng']","['stat.ML', 'cs.LG']",2022-02-20 03:35:44+00:00
http://arxiv.org/abs/2202.09699v1,Selective Credit Assignment,"Efficient credit assignment is essential for reinforcement learning
algorithms in both prediction and control settings. We describe a unified view
on temporal-difference algorithms for selective credit assignment. These
selective algorithms apply weightings to quantify the contribution of learning
updates. We present insights into applying weightings to value-based learning
and planning algorithms, and describe their role in mediating the backward
credit distribution in prediction and control. Within this space, we identify
some existing online learning algorithms that can assign credit selectively as
special cases, as well as add new algorithms that assign credit backward in
time counterfactually, allowing credit to be assigned off-trajectory and
off-policy.","['Veronica Chelu', 'Diana Borsa', 'Doina Precup', 'Hado van Hasselt']","['cs.LG', 'cs.AI', 'stat.ML']",2022-02-20 00:07:57+00:00
http://arxiv.org/abs/2202.09674v2,Generalized Optimistic Methods for Convex-Concave Saddle Point Problems,"The optimistic gradient method has seen increasing popularity for solving
convex-concave saddle point problems. To analyze its iteration complexity, a
recent work [arXiv:1906.01115] proposed an interesting perspective that
interprets this method as an approximation to the proximal point method. In
this paper, we follow this approach and distill the underlying idea of optimism
to propose a generalized optimistic method, which includes the optimistic
gradient method as a special case. Our general framework can handle constrained
saddle point problems with composite objective functions and can work with
arbitrary norms using Bregman distances. Moreover, we develop a backtracking
line search scheme to select the step sizes without knowledge of the smoothness
coefficients. We instantiate our method with first-, second- and higher-order
oracles and give best-known global iteration complexity bounds. For our
first-order method, we show that the averaged iterates converge at a rate of
$O(1/N)$ when the objective function is convex-concave, and it achieves linear
convergence when the objective is strongly-convex-strongly-concave. For our
second- and higher-order methods, under the additional assumption that the
distance-generating function has Lipschitz gradient, we prove a complexity
bound of $O(1/\epsilon^\frac{2}{p+1})$ in the convex-concave setting and a
complexity bound of
$O((L_pD^\frac{p-1}{2}/\mu)^\frac{2}{p+1}+\log\log\frac{1}{\epsilon})$ in the
strongly-convex-strongly-concave setting, where $L_p$ ($p\geq 2$) is the
Lipschitz constant of the $p$-th-order derivative, $\mu$ is the strong
convexity parameter, and $D$ is the initial Bregman distance to the saddle
point. Moreover, our line search scheme provably only requires a constant
number of calls to a subproblem solver per iteration on average, making our
first- and second-order methods particularly amenable to implementation.","['Ruichen Jiang', 'Aryan Mokhtari']","['math.OC', 'cs.LG', 'stat.ML', '90C25, 90C33, 90C47']",2022-02-19 20:31:05+00:00
http://arxiv.org/abs/2202.09673v2,A Behavior Regularized Implicit Policy for Offline Reinforcement Learning,"Offline reinforcement learning enables learning from a fixed dataset, without
further interactions with the environment. The lack of environmental
interactions makes the policy training vulnerable to state-action pairs far
from the training dataset and prone to missing rewarding actions. For training
more effective agents, we propose a framework that supports learning a flexible
yet well-regularized fully-implicit policy. We further propose a simple
modification to the classical policy-matching methods for regularizing with
respect to the dual form of the Jensen--Shannon divergence and the integral
probability metrics. We theoretically show the correctness of the
policy-matching approach, and the correctness and a good finite-sample property
of our modification. An effective instantiation of our framework through the
GAN structure is provided, together with techniques to explicitly smooth the
state-action mapping for robust generalization beyond the static dataset.
Extensive experiments and ablation study on the D4RL benchmark validate our
framework and the effectiveness of our algorithmic designs.","['Shentao Yang', 'Zhendong Wang', 'Huangjie Zheng', 'Yihao Feng', 'Mingyuan Zhou']","['stat.ML', 'cs.LG']",2022-02-19 20:22:04+00:00
http://arxiv.org/abs/2202.09671v4,Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders,"Employing a forward diffusion chain to gradually map the data to a noise
distribution, diffusion-based generative models learn how to generate the data
by inferring a reverse diffusion chain. However, this approach is slow and
costly because it needs many forward and reverse steps. We propose a faster and
cheaper approach that adds noise not until the data become pure random noise,
but until they reach a hidden noisy data distribution that we can confidently
learn. Then, we use fewer reverse steps to generate data by starting from this
hidden distribution that is made similar to the noisy data. We reveal that the
proposed model can be cast as an adversarial auto-encoder empowered by both the
diffusion process and a learnable implicit prior. Experimental results show
even with a significantly smaller number of reverse diffusion steps, the
proposed truncated diffusion probabilistic models can provide consistent
improvements over the non-truncated ones in terms of performance in both
unconditional and text-guided image generations.","['Huangjie Zheng', 'Pengcheng He', 'Weizhu Chen', 'Mingyuan Zhou']","['stat.ML', 'cs.LG']",2022-02-19 20:18:49+00:00
http://arxiv.org/abs/2202.09667v2,Doubly Robust Distributionally Robust Off-Policy Evaluation and Learning,"Off-policy evaluation and learning (OPE/L) use offline observational data to
make better decisions, which is crucial in applications where online
experimentation is limited. However, depending entirely on logged data, OPE/L
is sensitive to environment distribution shifts -- discrepancies between the
data-generating environment and that where policies are deployed.
\citet{si2020distributional} proposed distributionally robust OPE/L (DROPE/L)
to address this, but the proposal relies on inverse-propensity weighting, whose
estimation error and regret will deteriorate if propensities are
nonparametrically estimated and whose variance is suboptimal even if not. For
standard, non-robust, OPE/L, this is solved by doubly robust (DR) methods, but
they do not naturally extend to the more complex DROPE/L, which involves a
worst-case expectation. In this paper, we propose the first DR algorithms for
DROPE/L with KL-divergence uncertainty sets. For evaluation, we propose
Localized Doubly Robust DROPE (LDR$^2$OPE) and show that it achieves
semiparametric efficiency under weak product rates conditions. Thanks to a
localization technique, LDR$^2$OPE only requires fitting a small number of
regressions, just like DR methods for standard OPE. For learning, we propose
Continuum Doubly Robust DROPL (CDR$^2$OPL) and show that, under a product rate
condition involving a continuum of regressions, it enjoys a fast regret rate of
$\mathcal{O}\left(N^{-1/2}\right)$ even when unknown propensities are
nonparametrically estimated. We empirically validate our algorithms in
simulations and further extend our results to general $f$-divergence
uncertainty sets.","['Nathan Kallus', 'Xiaojie Mao', 'Kaiwen Wang', 'Zhengyuan Zhou']","['cs.LG', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",2022-02-19 20:00:44+00:00
http://arxiv.org/abs/2202.09664v1,Accurate Prediction and Uncertainty Estimation using Decoupled Prediction Interval Networks,"We propose a network architecture capable of reliably estimating uncertainty
of regression based predictions without sacrificing accuracy. The current
state-of-the-art uncertainty algorithms either fall short of achieving
prediction accuracy comparable to the mean square error optimization or
underestimate the variance of network predictions. We propose a decoupled
network architecture that is capable of accomplishing both at the same time. We
achieve this by breaking down the learning of prediction and prediction
interval (PI) estimations into a two-stage training process. We use a custom
loss function for learning a PI range around optimized mean estimation with a
desired coverage of a proportion of the target labels within the PI range. We
compare the proposed method with current state-of-the-art uncertainty
quantification algorithms on synthetic datasets and UCI benchmarks, reducing
the error in the predictions by 23 to 34% while maintaining 95% Prediction
Interval Coverage Probability (PICP) for 7 out of 9 UCI benchmark datasets. We
also examine the quality of our predictive uncertainty by evaluating on Active
Learning and demonstrating 17 to 36% error reduction on UCI benchmarks.","['Kinjal Patel', 'Steven Waslander']","['cs.LG', 'stat.ML']",2022-02-19 19:31:36+00:00
http://arxiv.org/abs/2202.09653v2,The Pareto Frontier of Instance-Dependent Guarantees in Multi-Player Multi-Armed Bandits with no Communication,"We study the stochastic multi-player multi-armed bandit problem. In this
problem, $m$ players cooperate to maximize their total reward from $K > m$
arms. However the players cannot communicate and are penalized (e.g. receive no
reward) if they pull the same arm at the same time. We ask whether it is
possible to obtain optimal instance-dependent regret $\tilde{O}(1/\Delta)$
where $\Delta$ is the gap between the $m$-th and $m+1$-st best arms. Such
guarantees were recently achieved in a model allowing the players to implicitly
communicate through intentional collisions.
  Surprisingly, we show that with no communication at all, such guarantees are
not achievable. In fact, obtaining the optimal $\tilde{O}(1/\Delta)$ regret for
some values of $\Delta$ necessarily implies strictly sub-optimal regret in
other regimes. Our main result is a complete characterization of the Pareto
optimal instance-dependent trade-offs that are possible with no communication.
Our algorithm generalizes that of Bubeck, Budzinski, and the second author. As
there, our algorithm succeeds even when feedback upon collision can be
corrupted by an adaptive adversary, thanks to a strong no-collision property.
Our lower bound is based on topological obstructions at multiple scales and is
completely new.","['Allen Liu', 'Mark Sellke']","['cs.LG', 'cs.MA', 'stat.ML']",2022-02-19 18:19:36+00:00
http://arxiv.org/abs/2202.11199v1,Differentially Private Regression with Unbounded Covariates,"We provide computationally efficient, differentially private algorithms for
the classical regression settings of Least Squares Fitting, Binary Regression
and Linear Regression with unbounded covariates. Prior to our work, privacy
constraints in such regression settings were studied under strong a priori
bounds on covariates. We consider the case of Gaussian marginals and extend
recent differentially private techniques on mean and covariance estimation
(Kamath et al., 2019; Karwa and Vadhan, 2018) to the sub-gaussian regime. We
provide a novel technical analysis yielding differentially private algorithms
for the above classical regression settings. Through the case of Binary
Regression, we capture the fundamental and widely-studied models of logistic
regression and linearly-separable SVMs, learning an unbiased estimate of the
true regression vector, up to a scaling factor.","['Jason Milionis', 'Alkis Kalavasis', 'Dimitris Fotakis', 'Stratis Ioannidis']","['cs.CR', 'cs.DS', 'cs.LG', 'stat.ML']",2022-02-19 17:31:38+00:00
http://arxiv.org/abs/2202.09638v1,Polytopic Matrix Factorization: Determinant Maximization Based Criterion and Identifiability,"We introduce Polytopic Matrix Factorization (PMF) as a novel data
decomposition approach. In this new framework, we model input data as unknown
linear transformations of some latent vectors drawn from a polytope. In this
sense, the article considers a semi-structured data model, in which the input
matrix is modeled as the product of a full column rank matrix and a matrix
containing samples from a polytope as its column vectors. The choice of
polytope reflects the presumed features of the latent components and their
mutual relationships. As the factorization criterion, we propose the
determinant maximization (Det-Max) for the sample autocorrelation matrix of the
latent vectors. We introduce a sufficient condition for identifiability, which
requires that the convex hull of the latent vectors contains the maximum volume
inscribed ellipsoid of the polytope with a particular tightness constraint.
Based on the Det-Max criterion and the proposed identifiability condition, we
show that all polytopes that satisfy a particular symmetry restriction qualify
for the PMF framework. Having infinitely many polytope choices provides a form
of flexibility in characterizing latent vectors. In particular, it is possible
to define latent vectors with heterogeneous features, enabling the assignment
of attributes such as nonnegativity and sparsity at the subvector level. The
article offers examples illustrating the connection between polytope choices
and the corresponding feature representations.","['Gokcan Tatli', 'Alper T. Erdogan']","['stat.ML', 'cs.LG', 'eess.SP']",2022-02-19 16:49:24+00:00
http://arxiv.org/abs/2202.09497v8,Gradient Estimation with Discrete Stein Operators,"Gradient estimation -- approximating the gradient of an expectation with
respect to the parameters of a distribution -- is central to the solution of
many machine learning problems. However, when the distribution is discrete,
most common gradient estimators suffer from excessive variance. To improve the
quality of gradient estimation, we introduce a variance reduction technique
based on Stein operators for discrete distributions. We then use this technique
to build flexible control variates for the REINFORCE leave-one-out estimator.
Our control variates can be adapted online to minimize variance and do not
require extra evaluations of the target function. In benchmark generative
modeling tasks such as training binary variational autoencoders, our gradient
estimator achieves substantially lower variance than state-of-the-art
estimators with the same number of function evaluations.","['Jiaxin Shi', 'Yuhao Zhou', 'Jessica Hwang', 'Michalis K. Titsias', 'Lester Mackey']","['stat.ML', 'cs.LG']",2022-02-19 02:22:23+00:00
http://arxiv.org/abs/2202.09459v1,Interactive Visual Pattern Search on Graph Data via Graph Representation Learning,"Graphs are a ubiquitous data structure to model processes and relations in a
wide range of domains. Examples include control-flow graphs in programs and
semantic scene graphs in images. Identifying subgraph patterns in graphs is an
important approach to understanding their structural properties. We propose a
visual analytics system GraphQ to support human-in-the-loop, example-based,
subgraph pattern search in a database containing many individual graphs. To
support fast, interactive queries, we use graph neural networks (GNNs) to
encode a graph as fixed-length latent vector representation, and perform
subgraph matching in the latent space. Due to the complexity of the problem, it
is still difficult to obtain accurate one-to-one node correspondences in the
matching results that are crucial for visualization and interpretation. We,
therefore, propose a novel GNN for node-alignment called NeuroAlign, to
facilitate easy validation and interpretation of the query results. GraphQ
provides a visual query interface with a query editor and a multi-scale
visualization of the results, as well as a user feedback mechanism for refining
the results with additional constraints. We demonstrate GraphQ through two
example usage scenarios: analyzing reusable subroutines in program workflows
and semantic scene graph search in images. Quantitative experiments show that
NeuroAlign achieves 19-29% improvement in node-alignment accuracy compared to
baseline GNN and provides up to 100x speedup compared to combinatorial
algorithms. Our qualitative study with domain experts confirms the
effectiveness for both usage scenarios.","['Huan Song', 'Zeng Dai', 'Panpan Xu', 'Liu Ren']","['cs.LG', 'cs.HC', 'stat.ML']",2022-02-18 22:30:28+00:00
http://arxiv.org/abs/2202.11527v1,A new LDA formulation with covariates,"The Latent Dirichlet Allocation (LDA) model is a popular method for creating
mixed-membership clusters. Despite having been originally developed for text
analysis, LDA has been used for a wide range of other applications. We propose
a new formulation for the LDA model which incorporates covariates. In this
model, a negative binomial regression is embedded within LDA, enabling
straight-forward interpretation of the regression coefficients and the analysis
of the quantity of cluster-specific elements in each sampling units (instead of
the analysis being focused on modeling the proportion of each cluster, as in
Structural Topic Models). We use slice sampling within a Gibbs sampling
algorithm to estimate model parameters. We rely on simulations to show how our
algorithm is able to successfully retrieve the true parameter values and the
ability to make predictions for the abundance matrix using the information
given by the covariates. The model is illustrated using real data sets from
three different areas: text-mining of Coronavirus articles, analysis of grocery
shopping baskets, and ecology of tree species on Barro Colorado Island
(Panama). This model allows the identification of mixed-membership clusters in
discrete data and provides inference on the relationship between covariates and
the abundance of these clusters.","['Gilson Shimizu', 'Rafael Izbicki', 'Denis Valle']","['cs.IR', 'cs.LG', 'stat.ME', 'stat.ML']",2022-02-18 19:58:24+00:00
http://arxiv.org/abs/2202.09312v2,Learning Predictions for Algorithms with Predictions,"A burgeoning paradigm in algorithm design is the field of algorithms with
predictions, in which algorithms can take advantage of a possibly-imperfect
prediction of some aspect of the problem. While much work has focused on using
predictions to improve competitive ratios, running times, or other performance
measures, less effort has been devoted to the question of how to obtain the
predictions themselves, especially in the critical online setting. We introduce
a general design approach for algorithms that learn predictors: (1) identify a
functional dependence of the performance measure on the prediction quality and
(2) apply techniques from online learning to learn predictors, tune
robustness-consistency trade-offs, and bound the sample complexity. We
demonstrate the effectiveness of our approach by applying it to bipartite
matching, ski-rental, page migration, and job scheduling. In several settings
we improve upon multiple existing results while utilizing a much simpler
analysis, while in the others we provide the first learning-theoretic
guarantees.","['Mikhail Khodak', 'Maria-Florina Balcan', 'Ameet Talwalkar', 'Sergei Vassilvitskii']","['cs.LG', 'cs.AI', 'cs.DS', 'stat.ML']",2022-02-18 17:25:43+00:00
http://arxiv.org/abs/2202.09305v1,Masked prediction tasks: a parameter identifiability view,"The vast majority of work in self-supervised learning, both theoretical and
empirical (though mostly the latter), have largely focused on recovering good
features for downstream tasks, with the definition of ""good"" often being
intricately tied to the downstream task itself. This lens is undoubtedly very
interesting, but suffers from the problem that there isn't a ""canonical"" set of
downstream tasks to focus on -- in practice, this problem is usually resolved
by competing on the benchmark dataset du jour.
  In this paper, we present an alternative lens: one of parameter
identifiability. More precisely, we consider data coming from a parametric
probabilistic model, and train a self-supervised learning predictor with a
suitably chosen parametric form. Then, we ask whether we can read off the
ground truth parameters of the probabilistic model from the optimal predictor.
We focus on the widely used self-supervised learning method of predicting
masked tokens, which is popular for both natural languages and visual data.
  While incarnations of this approach have already been successfully used for
simpler probabilistic models (e.g. learning fully-observed undirected graphical
models), we focus instead on latent-variable models capturing sequential
structures -- namely Hidden Markov Models with both discrete and conditionally
Gaussian observations. We show that there is a rich landscape of possibilities,
out of which some prediction tasks yield identifiability, while others do not.
Our results, borne of a theoretical grounding of self-supervised learning,
could thus potentially beneficially inform practice. Moreover, we uncover close
connections with uniqueness of tensor rank decompositions -- a widely used tool
in studying identifiability through the lens of the method of moments.","['Bingbin Liu', 'Daniel Hsu', 'Pradeep Ravikumar', 'Andrej Risteski']","['cs.LG', 'stat.ML']",2022-02-18 17:09:32+00:00
http://arxiv.org/abs/2202.09233v1,Nonstationary multi-output Gaussian processes via harmonizable spectral mixtures,"Kernel design for Multi-output Gaussian Processes (MOGP) has received
increased attention recently. In particular, the Multi-Output Spectral Mixture
kernel (MOSM) arXiv:1709.01298 approach has been praised as a general model in
the sense that it extends other approaches such as Linear Model of
Corregionalization, Intrinsic Corregionalization Model and Cross-Spectral
Mixture. MOSM relies on Cram\'er's theorem to parametrise the power spectral
densities (PSD) as a Gaussian mixture, thus, having a structural restriction:
by assuming the existence of a PSD, the method is only suited for multi-output
stationary applications. We develop a nonstationary extension of MOSM by
proposing the family of harmonizable kernels for MOGPs, a class of kernels that
contains both stationary and a vast majority of non-stationary processes. A
main contribution of the proposed harmonizable kernels is that they
automatically identify a possible nonstationary behaviour meaning that
practitioners do not need to choose between stationary or non-stationary
kernels. The proposed method is first validated on synthetic data with the
purpose of illustrating the key properties of our approach, and then compared
to existing MOGP methods on two real-world settings from finance and
electroencephalography.","['Matías Altamirano', 'Felipe Tobar']","['stat.ML', 'cs.LG', 'eess.SP']",2022-02-18 15:00:08+00:00
http://arxiv.org/abs/2202.09188v1,Testing the boundaries: Normalizing Flows for higher dimensional data sets,"Normalizing Flows (NFs) are emerging as a powerful class of generative
models, as they not only allow for efficient sampling, but also deliver, by
construction, density estimation. They are of great potential usage in High
Energy Physics (HEP), where complex high dimensional data and probability
distributions are everyday's meal. However, in order to fully leverage the
potential of NFs it is crucial to explore their robustness as data
dimensionality increases. Thus, in this contribution, we discuss the
performances of some of the most popular types of NFs on the market, on some
toy data sets with increasing number of dimensions.","['Humberto Reyes-Gonzalez', 'Riccardo Torre']","['stat.ML', 'cs.LG', 'hep-ph']",2022-02-18 13:31:24+00:00
http://arxiv.org/abs/2202.09182v1,Churn modeling of life insurance policies via statistical and machine learning methods -- Analysis of important features,"Life assurance companies typically possess a wealth of data covering multiple
systems and databases. These data are often used for analyzing the past and for
describing the present. Taking account of the past, the future is mostly
forecasted by traditional statistical methods. So far, only a few attempts were
undertaken to perform estimations by means of machine learning approaches. In
this work, the individual contract cancellation behavior of customers within
two partial stocks is modeled by the aid of various classification methods.
Partial stocks of private pension and endowment policy are considered. We
describe the data used for the modeling, their structured and in which way they
are cleansed. The utilized models are calibrated on the basis of an extensive
tuning process, then graphically evaluated regarding their goodness-of-fit and
with the help of a variable relevance concept, we investigate which features
notably affect the individual contract cancellation behavior.","['Andreas Groll', 'Carsten Wasserfuhr', 'Leonid Zeldin']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.CO']",2022-02-18 13:20:47+00:00
http://arxiv.org/abs/2202.09134v3,Data Augmentation in the Underparameterized and Overparameterized Regimes,"We provide results that exactly quantify how data augmentation affects the
variance and limiting distribution of estimates, and analyze several specific
models in detail. The results confirm some observations made in machine
learning practice, but also lead to unexpected findings: Data augmentation may
increase rather than decrease the uncertainty of estimates, such as the
empirical prediction risk. It can act as a regularizer, but fails to do so in
certain high-dimensional problems, and it may shift the double-descent peak of
an empirical risk. Overall, the analysis shows that several properties data
augmentation has been attributed with are not either true or false, but rather
depend on a combination of factors -- notably the data distribution, the
properties of the estimator, and the interplay of sample size, number of
augmentations, and dimension. Our main theoretical tool is a limit theorem for
functions of randomly transformed, high-dimensional random vectors. The proof
draws on work in probability on noise stability of functions of many variables.","['Kevin Han Huang', 'Peter Orbanz', 'Morgane Austern']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2022-02-18 11:32:41+00:00
http://arxiv.org/abs/2202.09096v2,A Free Lunch with Influence Functions? Improving Neural Network Estimates with Concepts from Semiparametric Statistics,"Parameter estimation in empirical fields is usually undertaken using
parametric models, and such models readily facilitate statistical inference.
Unfortunately, they are unlikely to be sufficiently flexible to be able to
adequately model real-world phenomena, and may yield biased estimates.
Conversely, non-parametric approaches are flexible but do not readily
facilitate statistical inference and may still exhibit residual bias. We
explore the potential for Influence Functions (IFs) to (a) improve initial
estimators without needing more data (b) increase model robustness and (c)
facilitate statistical inference. We begin with a broad introduction to IFs,
and propose a neural network method 'MultiNet', which seeks the diversity of an
ensemble using a single architecture. We also introduce variants on the IF
update step which we call 'MultiStep', and provide a comprehensive evaluation
of different approaches. The improvements are found to be dataset dependent,
indicating an interaction between the methods used and nature of the data
generating process. Our experiments highlight the need for practitioners to
check the consistency of their findings, potentially by undertaking multiple
analyses with different combinations of estimators. We also show that it is
possible to improve existing neural networks for `free', without needing more
data, and without needing to retrain them.","['Matthew J. Vowels', 'Sina Akbari', 'Necati Cihan Camgoz', 'Richard Bowden']","['cs.LG', 'stat.ME', 'stat.ML']",2022-02-18 09:35:51+00:00
http://arxiv.org/abs/2202.09054v1,Interpolation and Regularization for Causal Learning,"We study the problem of learning causal models from observational data
through the lens of interpolation and its counterpart -- regularization. A
large volume of recent theoretical, as well as empirical work, suggests that,
in highly complex model classes, interpolating estimators can have good
statistical generalization properties and can even be optimal for statistical
learning. Motivated by an analogy between statistical and causal learning
recently highlighted by Janzing (2019), we investigate whether interpolating
estimators can also learn good causal models. To this end, we consider a simple
linearly confounded model and derive precise asymptotics for the *causal risk*
of the min-norm interpolator and ridge-regularized regressors in the
high-dimensional regime. Under the principle of independent causal mechanisms,
a standard assumption in causal learning, we find that interpolators cannot be
optimal and causal learning requires stronger regularization than statistical
learning. This resolves a recent conjecture in Janzing (2019). Beyond this
assumption, we find a larger range of behavior that can be precisely
characterized with a new measure of *confounding strength*. If the confounding
strength is negative, causal learning requires weaker regularization than
statistical learning, interpolators can be optimal, and the optimal
regularization can even be negative. If the confounding strength is large, the
optimal regularization is infinite, and learning from observational data is
actively harmful.","['Leena Chennuru Vankadara', 'Luca Rendsburg', 'Ulrike von Luxburg', 'Debarghya Ghoshdastidar']","['stat.ML', 'cs.LG']",2022-02-18 07:37:31+00:00
http://arxiv.org/abs/2202.09052v1,Tackling benign nonconvexity with smoothing and stochastic gradients,"Non-convex optimization problems are ubiquitous in machine learning,
especially in Deep Learning. While such complex problems can often be
successfully optimized in practice by using stochastic gradient descent (SGD),
theoretical analysis cannot adequately explain this success. In particular, the
standard analyses do not show global convergence of SGD on non-convex
functions, and instead show convergence to stationary points (which can also be
local minima or saddle points). We identify a broad class of nonconvex
functions for which we can show that perturbed SGD (gradient descent perturbed
by stochastic noise -- covering SGD as a special case) converges to a global
minimum (or a neighborhood thereof), in contrast to gradient descent without
noise that can get stuck in local minima far from a global solution. For
example, on non-convex functions that are relatively close to a convex-like
(strongly convex or PL) function we show that SGD can converge linearly to a
global optimum.","['Harsh Vardhan', 'Sebastian U. Stich']","['cs.LG', 'stat.ML']",2022-02-18 07:27:32+00:00
http://arxiv.org/abs/2202.09036v4,Adaptive Experimentation in the Presence of Exogenous Nonstationary Variation,"We investigate experiments that are designed to select a treatment arm for
population deployment. Multi-armed bandit algorithms can enhance efficiency by
dynamically allocating measurement effort towards higher performing arms based
on observed feedback. However, such dynamics can result in brittle behavior in
the face of nonstationary exogenous factors influencing arms' performance
during the experiment. To counter this, we propose deconfounded Thompson
sampling (DTS), a more robust variant of the prominent Thompson sampling
algorithm. As observations accumulate, DTS projects the population-level
performance of an arm while controlling for the context within which observed
treatment decisions were made. Contexts here might capture a comprehensible
source of variation, such as the country of a treated individual, or simply
record the time of treatment. We provide bounds on both within-experiment and
post-experiment regret of DTS, illustrating its resilience to exogenous
variation and the delicate balance it strikes between exploration and
exploitation. Our proofs leverage inverse propensity weights to analyze the
evolution of the posterior distribution, a departure from established methods
in the literature. Hinting that new understanding is indeed necessary, we show
that a deconfounded variant of the popular upper confidence bound algorithm can
fail completely.","['Chao Qin', 'Daniel Russo']","['cs.LG', 'stat.ML']",2022-02-18 06:09:04+00:00
