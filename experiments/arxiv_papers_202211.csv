id,title,abstract,authors,categories,date
http://arxiv.org/abs/2212.10649v2,Inversion of Bayesian Networks,"Variational autoencoders and Helmholtz machines use a recognition network
(encoder) to approximate the posterior distribution of a generative model
(decoder). In this paper we study the necessary and sufficient properties of a
recognition network so that it can model the true posterior distribution
exactly. These results are derived in the general context of probabilistic
graphical modelling / Bayesian networks, for which the network represents a set
of conditional independence statements. We derive both global conditions, in
terms of d-separation, and local conditions for the recognition network to have
the desired qualities. It turns out that for the local conditions the property
perfectness (for every node, all parents are joined) plays an important role.","['Jesse van Oostrum', 'Peter van Hintum', 'Nihat Ay']","['cs.LG', 'cs.AI', 'stat.ML']",2022-12-20 20:56:18+00:00
http://arxiv.org/abs/2212.10579v1,Resonant Anomaly Detection with Multiple Reference Datasets,"An important class of techniques for resonant anomaly detection in high
energy physics builds models that can distinguish between reference and target
datasets, where only the latter has appreciable signal. Such techniques,
including Classification Without Labels (CWoLa) and Simulation Assisted
Likelihood-free Anomaly Detection (SALAD) rely on a single reference dataset.
They cannot take advantage of commonly-available multiple datasets and thus
cannot fully exploit available information. In this work, we propose
generalizations of CWoLa and SALAD for settings where multiple reference
datasets are available, building on weak supervision techniques. We demonstrate
improved performance in a number of settings with realistic and synthetic data.
As an added benefit, our generalizations enable us to provide finite-sample
guarantees, improving on existing asymptotic analyses.","['Mayee F. Chen', 'Benjamin Nachman', 'Frederic Sala']","['hep-ph', 'cs.LG', 'hep-ex', 'stat.ML']",2022-12-20 19:00:03+00:00
http://arxiv.org/abs/2212.10538v2,HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes,"Bayesian optimization (BO), while proved highly effective for many black-box
function optimization tasks, requires practitioners to carefully select priors
that well model their functions of interest. Rather than specifying by hand,
researchers have investigated transfer learning based methods to automatically
learn the priors, e.g. multi-task BO (Swersky et al., 2013), few-shot BO
(Wistuba and Grabocka, 2021) and HyperBO (Wang et al., 2022). However, those
prior learning methods typically assume that the input domains are the same for
all tasks, weakening their ability to use observations on functions with
different domains or generalize the learned priors to BO on different search
spaces. In this work, we present HyperBO+: a pre-training approach for
hierarchical Gaussian processes that enables the same prior to work universally
for Bayesian optimization on functions with different domains. We propose a
two-step pre-training method and analyze its appealing asymptotic properties
and benefits to BO both theoretically and empirically. On real-world
hyperparameter tuning tasks that involve multiple search spaces, we demonstrate
that HyperBO+ is able to generalize to unseen search spaces and achieves lower
regrets than competitive baselines.","['Zhou Fan', 'Xinran Han', 'Zi Wang']","['cs.LG', 'stat.ML']",2022-12-20 18:47:10+00:00
http://arxiv.org/abs/2212.10477v2,Generalized Simultaneous Perturbation-based Gradient Search with Reduced Estimator Bias,"We present in this paper a family of generalized simultaneous
perturbation-based gradient search (GSPGS) estimators that use noisy function
measurements. The number of function measurements required by each estimator is
guided by the desired level of accuracy. We first present in detail unbalanced
generalized simultaneous perturbation stochastic approximation (GSPSA)
estimators and later present the balanced versions (B-GSPSA) of these. We
extend this idea further and present the generalized smoothed functional (GSF)
and generalized random directions stochastic approximation (GRDSA) estimators,
respectively, as well as their balanced variants. We show that estimators
within any specified class requiring more number of function measurements
result in lower estimator bias. We present a detailed analysis of both the
asymptotic and non-asymptotic convergence of the resulting stochastic
approximation schemes. We further present a series of experimental results with
the various GSPGS estimators on the Rastrigin and quadratic function
objectives. Our experiments are seen to validate our theoretical findings.","['Soumen Pachal', 'Shalabh Bhatnagar', 'L. A. Prashanth']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2022-12-20 17:50:36+00:00
http://arxiv.org/abs/2212.10426v7,Deep Riemannian Networks for End-to-End EEG Decoding,"State-of-the-art performance in electroencephalography (EEG) decoding tasks
is currently often achieved with either Deep-Learning (DL) or
Riemannian-Geometry-based decoders (RBDs). Recently, there is growing interest
in Deep Riemannian Networks (DRNs) possibly combining the advantages of both
previous classes of methods. However, there are still a range of topics where
additional insight is needed to pave the way for a more widespread application
of DRNs in EEG. These include architecture design questions such as network
size and end-to-end ability. How these factors affect model performance has not
been explored. Additionally, it is not clear how the data within these networks
is transformed, and whether this would correlate with traditional EEG decoding.
Our study aims to lay the groundwork in the area of these topics through the
analysis of DRNs for EEG with a wide range of hyperparameters. Networks were
tested on five public EEG datasets and compared with state-of-the-art ConvNets.
  Here we propose EE(G)-SPDNet, and we show that this wide, end-to-end DRN can
outperform the ConvNets, and in doing so use physiologically plausible
frequency regions. We also show that the end-to-end approach learns more
complex filters than traditional band-pass filters targeting the classical
alpha, beta, and gamma frequency bands of the EEG, and that performance can
benefit from channel specific filtering approaches. Additionally, architectural
analysis revealed areas for further improvement due to the possible under
utilisation of Riemannian specific information throughout the network. Our
study thus shows how to design and train DRNs to infer task-related information
from the raw EEG without the need of handcrafted filterbanks and highlights the
potential of end-to-end DRNs such as EE(G)-SPDNet for high-performance EEG
decoding.","['Daniel Wilson', 'Robin Tibor Schirrmeister', 'Lukas Alexander Wilhelm Gemein', 'Tonio Ball']","['cs.LG', 'eess.SP', 'stat.ML']",2022-12-20 17:04:50+00:00
http://arxiv.org/abs/2212.13902v2,Likelihood-based generalization of Markov parameter estimation and multiple shooting objectives in system identification,"This paper considers the problem of system identification (ID) of linear and
nonlinear non-autonomous systems from noisy and sparse data. We propose and
analyze an objective function derived from a Bayesian formulation for learning
a hidden Markov model with stochastic dynamics. We then analyze this objective
function in the context of several state-of-the-art approaches for both linear
and nonlinear system ID. In the former, we analyze least squares approaches for
Markov parameter estimation, and in the latter, we analyze the multiple
shooting approach. We demonstrate the limitations of the optimization problems
posed by these existing methods by showing that they can be seen as special
cases of the proposed optimization objective under certain simplifying
assumptions: conditional independence of data and zero model error.
Furthermore, we observe that our proposed approach has improved smoothness and
inherent regularization that make it well-suited for system ID and provide
mathematical explanations for these characteristics' origins. Finally,
numerical simulations demonstrate a mean squared error over 8.7 times lower
compared to multiple shooting when data are noisy and/or sparse. Moreover, the
proposed approach can identify accurate and generalizable models even when
there are more parameters than data or when the underlying system exhibits
chaotic behavior.","['Nicholas Galioto', 'Alex Arkady Gorodetsky']","['eess.SY', 'cs.LG', 'cs.SY', 'math.DS', 'math.OC', 'physics.data-an', 'stat.CO', 'stat.ML']",2022-12-20 16:36:23+00:00
http://arxiv.org/abs/2212.10301v3,Probabilistic Quantile Factor Analysis,"This paper extends quantile factor analysis to a probabilistic variant that
incorporates regularization and computationally efficient variational
approximations. We establish through synthetic and real data experiments that
the proposed estimator can, in many cases, achieve better accuracy than a
recently proposed loss-based estimator. We contribute to the factor analysis
literature by extracting new indexes of \emph{low}, \emph{medium}, and
\emph{high} economic policy uncertainty, as well as \emph{loose},
\emph{median}, and \emph{tight} financial conditions. We show that the high
uncertainty and tight financial conditions indexes have superior predictive
ability for various measures of economic activity. In a high-dimensional
exercise involving about 1000 daily financial series, we find that quantile
factors also provide superior out-of-sample information compared to mean or
median factors.","['Dimitris Korobilis', 'Maximilian Schr√∂der']","['econ.EM', 'stat.ML']",2022-12-20 14:49:27+00:00
http://arxiv.org/abs/2212.10299v1,Cell-Free Data Power Control Via Scalable Multi-Objective Bayesian Optimisation,"Cell-free multi-user multiple input multiple output networks are a promising
alternative to classical cellular architectures, since they have the potential
to provide uniform service quality and high resource utilisation over the
entire coverage area of the network. To realise this potential, previous works
have developed radio resource management mechanisms using various optimisation
engines. In this work, we consider the problem of overall ergodic spectral
efficiency maximisation in the context of uplink-downlink data power control in
cell-free networks. To solve this problem in large networks, and to address
convergence-time limitations, we apply scalable multi-objective Bayesian
optimisation. Furthermore, we discuss how an intersection of multi-fidelity
emulation and Bayesian optimisation can improve radio resource management in
cell-free networks.","['Sergey S. Tambovskiy', 'G√°bor Fodor', 'Hugo Tullberg']","['eess.SY', 'cs.LG', 'cs.SY', 'stat.AP', 'stat.ML']",2022-12-20 14:46:44+00:00
http://arxiv.org/abs/2212.10287v1,Strong uniform convergence of Laplacians of random geometric and directed kNN graphs on compact manifolds,"Consider $n$ points independently sampled from a density $p$ of class
$\mathcal{C}^2$ on a smooth compact $d$-dimensional sub-manifold $\mathcal{M}$
of $\mathbb{R}^m$, and consider the generator of a random walk visiting these
points according to a transition kernel $K$. We study the almost sure uniform
convergence of this operator to the diffusive Laplace-Beltrami operator when
$n$ tends to infinity. This work extends known results of the past 15 years. In
particular, our result does not require the kernel $K$ to be continuous, which
covers the cases of walks exploring $k$NN-random and geometric graphs, and
convergence rates are given. The distance between the random walk generator and
the limiting operator is separated into several terms: a statistical term,
related to the law of large numbers, is treated with concentration tools and an
approximation term that we control with tools from differential geometry. The
convergence of $k$NN Laplacians is detailed.","['H√©l√®ne Gu√©rin', 'Dinh-Toan Nguyen', 'Viet-Chi Tran']","['math.PR', 'stat.ML', '60F05, 05C81, 62G05']",2022-12-20 14:31:06+00:00
http://arxiv.org/abs/2212.10259v2,Nonparametric plug-in classifier for multiclass classification of S.D.E. paths,"We study the multiclass classification problem where the features come from
the mixture of time-homogeneous diffusions. Specifically, the classes are
discriminated by their drift functions while the diffusion coefficient is
common to all classes and unknown. In this framework, we build a plug-in
classifier which relies on nonparametric estimators of the drift and diffusion
functions. We first establish the consistency of our classification procedure
under mild assumptions and then provide rates of cnvergence under different set
of assumptions. Finally, a numerical study supports our theoretical findings.","['Christophe Denis', 'Charlotte Dion-Blanc', 'Eddy Ella Mintsa', 'Viet-Chi Tran']","['math.ST', 'stat.ML', 'stat.TH']",2022-12-20 14:08:05+00:00
http://arxiv.org/abs/2212.09962v3,Distributional Robustness Bounds Generalization Errors,"Bayesian methods, distributionally robust optimization methods, and
regularization methods are three pillars of trustworthy machine learning
combating distributional uncertainty, e.g., the uncertainty of an empirical
distribution compared to the true underlying distribution. This paper
investigates the connections among the three frameworks and, in particular,
explores why these frameworks tend to have smaller generalization errors.
Specifically, first, we suggest a quantitative definition for ""distributional
robustness"", propose the concept of ""robustness measure"", and formalize several
philosophical concepts in distributionally robust optimization. Second, we show
that Bayesian methods are distributionally robust in the probably approximately
correct (PAC) sense; in addition, by constructing a Dirichlet-process-like
prior in Bayesian nonparametrics, it can be proven that any regularized
empirical risk minimization method is equivalent to a Bayesian method. Third,
we show that generalization errors of machine learning models can be
characterized using the distributional uncertainty of the nominal distribution
and the robustness measures of these machine learning models, which is a new
perspective to bound generalization errors, and therefore, explain the reason
why distributionally robust machine learning models, Bayesian models, and
regularization models tend to have smaller generalization errors in a unified
manner.","['Shixiong Wang', 'Haowei Wang']","['cs.LG', 'stat.ML', '62F35, 62G35, 62C12']",2022-12-20 02:30:13+00:00
http://arxiv.org/abs/2212.09961v2,Uncertainty Quantification of MLE for Entity Ranking with Covariates,"This paper concerns with statistical estimation and inference for the ranking
problems based on pairwise comparisons with additional covariate information
such as the attributes of the compared items. Despite extensive studies, few
prior literatures investigate this problem under the more realistic setting
where covariate information exists. To tackle this issue, we propose a novel
model, Covariate-Assisted Ranking Estimation (CARE) model, that extends the
well-known Bradley-Terry-Luce (BTL) model, by incorporating the covariate
information. Specifically, instead of assuming every compared item has a fixed
latent score $\{\theta_i^*\}_{i=1}^n$, we assume the underlying scores are
given by $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$, where $\alpha_i^*$ and
${x}_i^\top\beta^*$ represent latent baseline and covariate score of the $i$-th
item, respectively. We impose natural identifiability conditions and derive the
$\ell_{\infty}$- and $\ell_2$-optimal rates for the maximum likelihood
estimator of $\{\alpha_i^*\}_{i=1}^{n}$ and $\beta^*$ under a sparse comparison
graph, using a novel `leave-one-out' technique (Chen et al., 2019) . To conduct
statistical inferences, we further derive asymptotic distributions for the MLE
of $\{\alpha_i^*\}_{i=1}^n$ and $\beta^*$ with minimal sample complexity. This
allows us to answer the question whether some covariates have any explanation
power for latent scores and to threshold some sparse parameters to improve the
ranking performance. We improve the approximation method used in (Gao et al.,
2021) for the BLT model and generalize it to the CARE model. Moreover, we
validate our theoretical results through large-scale numerical studies and an
application to the mutual fund stock holding dataset.","['Jianqing Fan', 'Jikai Hou', 'Mengxin Yu']","['stat.ME', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2022-12-20 02:28:27+00:00
http://arxiv.org/abs/2212.09957v1,Beyond Surrogate Modeling: Learning the Local Volatility Via Shape Constraints,"We explore the abilities of two machine learning approaches for no-arbitrage
interpolation of European vanilla option prices, which jointly yield the
corresponding local volatility surface: a finite dimensional Gaussian process
(GP) regression approach under no-arbitrage constraints based on prices, and a
neural net (NN) approach with penalization of arbitrages based on implied
volatilities. We demonstrate the performance of these approaches relative to
the SSVI industry standard. The GP approach is proven arbitrage-free, whereas
arbitrages are only penalized under the SSVI and NN approaches. The GP approach
obtains the best out-of-sample calibration error and provides uncertainty
quantification.The NN approach yields a smoother local volatility and a better
backtesting performance, as its training criterion incorporates a local
volatility regularization term.","['Marc Chataigner', 'Areski Cousin', 'St√©phane Cr√©pey', 'Matthew Dixon', 'Djibril Gueye']","['q-fin.MF', 'q-fin.CP', 'stat.ML']",2022-12-20 02:17:47+00:00
http://arxiv.org/abs/2212.09931v3,A Generalized Variable Importance Metric and Estimator for Black Box Machine Learning Models,"In this paper we define a population parameter, ``Generalized Variable
Importance Metric (GVIM)'', to measure importance of predictors for black box
machine learning methods, where the importance is not represented by
model-based parameter. GVIM is defined for each input variable, using the true
conditional expectation function, and it measures the variable's importance in
affecting a continuous or a binary response. We extend previously published
results to show that the defined GVIM can be represented as a function of the
Conditional Average Treatment Effect (CATE) for any kind of a predictor, which
gives it a causal interpretation and further justification as an alternative to
classical measures of significance that are only available in simple parametric
models. Extensive set of simulations using realistically complex relationships
between covariates and outcomes and number of regression techniques of varying
degree of complexity show the performance of our proposed estimator of the
GVIM.","['Mohammad Kaviul Anam Khan', 'Olli Saarela', 'Rafal Kustra']","['stat.CO', 'stat.ML']",2022-12-20 00:50:28+00:00
http://arxiv.org/abs/2212.09900v3,"Policy learning ""without"" overlap: Pessimism and generalized empirical Bernstein's inequality","This paper studies offline policy learning, which aims at utilizing
observations collected a priori (from either fixed or adaptively evolving
behavior policies) to learn an optimal individualized decision rule that
achieves the best overall outcomes for a given population. Existing policy
learning methods rely on a uniform overlap assumption, i.e., the propensities
of exploring all actions for all individual characteristics must be lower
bounded. As one has no control over the data collection process, this
assumption can be unrealistic in many situations, especially when the behavior
policies are allowed to evolve over time with diminishing propensities for
certain actions.
  In this paper, we propose Pessimistic Policy Learning (PPL), a new algorithm
that optimizes lower confidence bounds (LCBs) -- instead of point estimates --
of the policy values. The LCBs are constructed using knowledge of the behavior
policies for collecting the offline data. Without assuming any uniform overlap
condition, we establish a data-dependent upper bound for the suboptimality of
our algorithm, which only depends on (i) the overlap for the optimal policy,
and (ii) the complexity of the policy class we optimize over. As an
implication, for adaptively collected data, we ensure efficient policy learning
as long as the propensities for optimal actions are lower bounded over time,
while those for suboptimal ones are allowed to diminish arbitrarily fast. In
our theoretical analysis, we develop a new self-normalized type concentration
inequality for inverse-propensity-weighting estimators, generalizing the
well-known empirical Bernstein's inequality to unbounded and non-i.i.d. data.
We complement our theory with an efficient optimization algorithm via
Majorization-Minimization and policy tree search, as well as extensive
simulation studies and real-world applications that demonstrate the efficacy of
PPL.","['Ying Jin', 'Zhimei Ren', 'Zhuoran Yang', 'Zhaoran Wang']","['cs.LG', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2022-12-19 22:43:08+00:00
http://arxiv.org/abs/2212.09826v2,Fixed and adaptive landmark sets for finite pseudometric spaces,"Topological data analysis (TDA) is an expanding field that leverages
principles and tools from algebraic topology to quantify structural features of
data sets or transform them into more manageable forms. As its theoretical
foundations have been developed, TDA has shown promise in extracting useful
information from high-dimensional, noisy, and complex data such as those used
in biomedicine. To improve efficiency, these techniques may employ landmark
samplers. The heuristic maxmin procedure obtains a roughly even distribution of
sample points by implicitly constructing a cover comprising sets of uniform
radius. However, issues arise with data that vary in density or include points
with multiplicities, as are common in biomedicine. We propose an analogous
procedure, ""lastfirst"" based on ranked distances, which implies a cover
comprising sets of uniform cardinality. We first rigorously define the
procedure and prove that it obtains landmarks with desired properties. We then
perform benchmark tests and compare its performance to that of maxmin, on
feature detection and class prediction tasks involving simulated and real-world
biomedical data. Lastfirst is more general than maxmin in that it can be
applied to any data on which arbitrary (and not necessarily symmetric) pairwise
distances can be computed. Lastfirst is more computationally costly, but our
implementation scales at the same rate as maxmin. We find that lastfirst
achieves comparable performance on prediction tasks and outperforms maxmin on
homology detection tasks. Where the numerical values of similarity measures are
not meaningful, as in many biomedical contexts, lastfirst sampling may also
improve interpretability.","['Jason Cory Brunson', 'Yara Skaf']","['cs.CG', 'math.AT', 'stat.ML', '62R40 (Primary) 54E35, 62P10 (Secondary)', 'G.2.3; H.3.3; J.3']",2022-12-19 19:53:33+00:00
http://arxiv.org/abs/2212.09713v2,A Probabilistic Framework for Lifelong Test-Time Adaptation,"Test-time adaptation (TTA) is the problem of updating a pre-trained source
model at inference time given test input(s) from a different target domain.
Most existing TTA approaches assume the setting in which the target domain is
stationary, i.e., all the test inputs come from a single target domain.
However, in many practical settings, the test input distribution might exhibit
a lifelong/continual shift over time. Moreover, existing TTA approaches also
lack the ability to provide reliable uncertainty estimates, which is crucial
when distribution shifts occur between the source and target domain. To address
these issues, we present PETAL (Probabilistic lifElong Test-time Adaptation
with seLf-training prior), which solves lifelong TTA using a probabilistic
approach, and naturally results in (1) a student-teacher framework, where the
teacher model is an exponential moving average of the student model, and (2)
regularizing the model updates at inference time using the source model as a
regularizer. To prevent model drift in the lifelong/continual TTA setting, we
also propose a data-driven parameter restoration technique which contributes to
reducing the error accumulation and maintaining the knowledge of recent domains
by restoring only the irrelevant parameters. In terms of predictive error rate
as well as uncertainty based metrics such as Brier score and negative
log-likelihood, our method achieves better results than the current
state-of-the-art for online lifelong test-time adaptation across various
benchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC
datasets. The source code for our approach is accessible at
https://github.com/dhanajitb/petal.","['Dhanajit Brahma', 'Piyush Rai']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2022-12-19 18:42:19+00:00
http://arxiv.org/abs/2212.09510v1,Near-optimal Policy Identification in Active Reinforcement Learning,"Many real-world reinforcement learning tasks require control of complex
dynamical systems that involve both costly data acquisition processes and large
state spaces. In cases where the transition dynamics can be readily evaluated
at specified states (e.g., via a simulator), agents can operate in what is
often referred to as planning with a \emph{generative model}. We propose the
AE-LSVI algorithm for best-policy identification, a novel variant of the
kernelized least-squares value iteration (LSVI) algorithm that combines
optimism with pessimism for active exploration (AE). AE-LSVI provably
identifies a near-optimal policy \emph{uniformly} over an entire state space
and achieves polynomial sample complexity guarantees that are independent of
the number of states. When specialized to the recently introduced offline
contextual Bayesian optimization setting, our algorithm achieves improved
sample complexity bounds. Experimentally, we demonstrate that AE-LSVI
outperforms other RL algorithms in a variety of environments when robustness to
the initial state is required.","['Xiang Li', 'Viraj Mehta', 'Johannes Kirschner', 'Ian Char', 'Willie Neiswanger', 'Jeff Schneider', 'Andreas Krause', 'Ilija Bogunovic']","['stat.ML', 'cs.AI', 'cs.LG']",2022-12-19 14:46:57+00:00
http://arxiv.org/abs/2212.09507v1,VC dimensions of group convolutional neural networks,"We study the generalization capacity of group convolutional neural networks.
We identify precise estimates for the VC dimensions of simple sets of group
convolutional neural networks. In particular, we find that for infinite groups
and appropriately chosen convolutional kernels, already two-parameter families
of convolutional neural networks have an infinite VC dimension, despite being
invariant to the action of an infinite group.","['Philipp Christian Petersen', 'Anna Sepliarskaia']","['cs.LG', 'math.FA', 'stat.ML', '68T07, 68Q32, 68T05']",2022-12-19 14:43:22+00:00
http://arxiv.org/abs/2212.09494v3,Optimal Treatment Regimes for Proximal Causal Learning,"A common concern when a policymaker draws causal inferences from and makes
decisions based on observational data is that the measured covariates are
insufficiently rich to account for all sources of confounding, i.e., the
standard no confoundedness assumption fails to hold. The recently proposed
proximal causal inference framework shows that proxy variables that abound in
real-life scenarios can be leveraged to identify causal effects and therefore
facilitate decision-making. Building upon this line of work, we propose a novel
optimal individualized treatment regime based on so-called outcome and
treatment confounding bridges. We then show that the value function of this new
optimal treatment regime is superior to that of existing ones in the
literature. Theoretical guarantees, including identification, superiority,
excess value bound, and consistency of the estimated regime, are established.
Furthermore, we demonstrate the proposed optimal regime via numerical
experiments and a real data application.","['Tao Shen', 'Yifan Cui']","['stat.ME', 'stat.ML']",2022-12-19 14:29:25+00:00
http://arxiv.org/abs/2212.09458v1,Exploring Optimal Substructure for Out-of-distribution Generalization via Feature-targeted Model Pruning,"Recent studies show that even highly biased dense networks contain an
unbiased substructure that can achieve better out-of-distribution (OOD)
generalization than the original model. Existing works usually search the
invariant subnetwork using modular risk minimization (MRM) with out-domain
data. Such a paradigm may bring about two potential weaknesses: 1) Unfairness,
due to the insufficient observation of out-domain data during training; and 2)
Sub-optimal OOD generalization, due to the feature-untargeted model pruning on
the whole data distribution. In this paper, we propose a novel Spurious
Feature-targeted model Pruning framework, dubbed SFP, to automatically explore
invariant substructures without referring to the above weaknesses.
Specifically, SFP identifies in-distribution (ID) features during training
using our theoretically verified task loss, upon which, SFP can perform ID
targeted-model pruning that removes branches with strong dependencies on ID
features. Notably, by attenuating the projections of spurious features into
model space, SFP can push the model learning toward invariant features and pull
that out of environmental features, devising optimal OOD generalization.
Moreover, we also conduct detailed theoretical analysis to provide the
rationality guarantee and a proof framework for OOD structures via model
sparsity, and for the first time, reveal how a highly biased data distribution
affects the model's OOD generalization. Extensive experiments on various OOD
datasets show that SFP can significantly outperform both structure-based and
non-structure OOD generalization SOTAs, with accuracy improvement up to 4.72%
and 23.35%, respectively.","['Yingchun Wang', 'Jingcai Guo', 'Song Guo', 'Weizhan Zhang', 'Jie Zhang']","['cs.LG', 'cs.AI', 'stat.ML', 'I.2.6']",2022-12-19 13:51:06+00:00
http://arxiv.org/abs/2212.09434v1,Quasi-parametric rates for Sparse Multivariate Functional Principal Components Analysis,"This work aims to give non-asymptotic results for estimating the first
principal component of a multivariate random process. We first define the
covariance function and the covariance operator in the multivariate case. We
then define a projection operator. This operator can be seen as a
reconstruction step from the raw data in the functional data analysis context.
Next, we show that the eigenelements can be expressed as the solution to an
optimization problem, and we introduce the LASSO variant of this optimization
problem and the associated plugin estimator. Finally, we assess the estimator's
accuracy. We establish a minimax lower bound on the mean square reconstruction
error of the eigenelement, which proves that the procedure has an optimal
variance in the minimax sense.",['Ryad Belhakem'],"['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2022-12-19 13:17:57+00:00
http://arxiv.org/abs/2212.09429v1,On the Complexity of Representation Learning in Contextual Linear Bandits,"In contextual linear bandits, the reward function is assumed to be a linear
combination of an unknown reward vector and a given embedding of context-arm
pairs. In practice, the embedding is often learned at the same time as the
reward vector, thus leading to an online representation learning problem.
Existing approaches to representation learning in contextual bandits are either
very generic (e.g., model-selection techniques or algorithms for learning with
arbitrary function classes) or specialized to particular structures (e.g.,
nested features or representations with certain spectral properties). As a
result, the understanding of the cost of representation learning in contextual
linear bandit is still limited. In this paper, we take a systematic approach to
the problem and provide a comprehensive study through an instance-dependent
perspective. We show that representation learning is fundamentally more complex
than linear bandits (i.e., learning with a given representation). In
particular, learning with a given set of representations is never simpler than
learning with the worst realizable representation in the set, while we show
cases where it can be arbitrarily harder. We complement this result with an
extensive discussion of how it relates to existing literature and we illustrate
positive instances where representation learning is as complex as learning with
a fixed representation and where sub-logarithmic regret is achievable.","['Andrea Tirinzoni', 'Matteo Pirotta', 'Alessandro Lazaric']","['cs.LG', 'stat.ML']",2022-12-19 13:08:58+00:00
http://arxiv.org/abs/2212.09413v1,Gradient Descent-Type Methods: Background and Simple Unified Convergence Analysis,"In this book chapter, we briefly describe the main components that constitute
the gradient descent method and its accelerated and stochastic variants. We aim
at explaining these components from a mathematical point of view, including
theoretical and practical aspects, but at an elementary level. We will focus on
basic variants of the gradient descent method and then extend our view to
recent variants, especially variance-reduced stochastic gradient schemes (SGD).
Our approach relies on revealing the structures presented inside the problem
and the assumptions imposed on the objective function. Our convergence analysis
unifies several known results and relies on a general, but elementary recursive
expression. We have illustrated this analysis on several common schemes.","['Quoc Tran-Dinh', 'Marten van Dijk']","['math.OC', 'stat.ML']",2022-12-19 12:45:02+00:00
http://arxiv.org/abs/2212.09396v2,Rank-1 Matrix Completion with Gradient Descent and Small Random Initialization,"The nonconvex formulation of matrix completion problem has received
significant attention in recent years due to its affordable complexity compared
to the convex formulation. Gradient descent (GD) is the simplest yet efficient
baseline algorithm for solving nonconvex optimization problems. The success of
GD has been witnessed in many different problems in both theory and practice
when it is combined with random initialization. However, previous works on
matrix completion require either careful initialization or regularizers to
prove the convergence of GD. In this work, we study the rank-1 symmetric matrix
completion and prove that GD converges to the ground truth when small random
initialization is used. We show that in logarithmic amount of iterations, the
trajectory enters the region where local convergence occurs. We provide an
upper bound on the initialization size that is sufficient to guarantee the
convergence and show that a larger initialization can be used as more samples
are available. We observe that implicit regularization effect of GD plays a
critical role in the analysis, and for the entire trajectory, it prevents each
entry from becoming much larger than the others.","['Daesung Kim', 'Hye Won Chung']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2022-12-19 12:05:37+00:00
http://arxiv.org/abs/2212.09385v2,Prediction of Auto Insurance Risk Based on t-SNE Dimensionality Reduction,"Correct risk estimation of policyholders is of great significance to auto
insurance companies. While the current tools used in this field have been
proven in practice to be quite efficient and beneficial, we argue that there is
still a lot of room for development and improvement in the auto insurance risk
estimation process. To this end, we develop a framework based on a combination
of a neural network together with a dimensionality reduction technique t-SNE
(t-distributed stochastic neighbour embedding). This enables us to visually
represent the complex structure of the risk as a two-dimensional surface, while
still preserving the properties of the local region in the features space. The
obtained results, which are based on real insurance data, reveal a clear
contrast between the high and the low risk policy holders, and indeed improve
upon the actual risk estimation performed by the insurer. Due to the visual
accessibility of the portfolio in this approach, we argue that this framework
could be advantageous to the auto insurer, both as a main risk prediction tool
and as an additional validation stage in other approaches.","['Joseph Levitas', 'Konstantin Yavilberg', 'Oleg Korol', 'Genadi Man']","['cs.AI', 'q-fin.RM', 'stat.ML']",2022-12-19 11:50:18+00:00
http://arxiv.org/abs/2212.09328v1,Quantum policy gradient algorithms,"Understanding the power and limitations of quantum access to data in machine
learning tasks is primordial to assess the potential of quantum computing in
artificial intelligence. Previous works have already shown that speed-ups in
learning are possible when given quantum access to reinforcement learning
environments. Yet, the applicability of quantum algorithms in this setting
remains very limited, notably in environments with large state and action
spaces. In this work, we design quantum algorithms to train state-of-the-art
reinforcement learning policies by exploiting quantum interactions with an
environment. However, these algorithms only offer full quadratic speed-ups in
sample complexity over their classical analogs when the trained policies
satisfy some regularity conditions. Interestingly, we find that reinforcement
learning policies derived from parametrized quantum circuits are well-behaved
with respect to these conditions, which showcases the benefit of a
fully-quantum reinforcement learning framework.","['Sofiene Jerbi', 'Arjan Cornelissen', 'MƒÅris Ozols', 'Vedran Dunjko']","['quant-ph', 'cs.AI', 'cs.LG', 'stat.ML']",2022-12-19 09:45:58+00:00
http://arxiv.org/abs/2212.09240v1,Probabilistic machine learning based predictive and interpretable digital twin for dynamical systems,"A framework for creating and updating digital twins for dynamical systems
from a library of physics-based functions is proposed. The sparse Bayesian
machine learning is used to update and derive an interpretable expression for
the digital twin. Two approaches for updating the digital twin are proposed.
The first approach makes use of both the input and output information from a
dynamical system, whereas the second approach utilizes output-only observations
to update the digital twin. Both methods use a library of candidate functions
representing certain physics to infer new perturbation terms in the existing
digital twin model. In both cases, the resulting expressions of updated digital
twins are identical, and in addition, the epistemic uncertainties are
quantified. In the first approach, the regression problem is derived from a
state-space model, whereas in the latter case, the output-only information is
treated as a stochastic process. The concepts of It\^o calculus and
Kramers-Moyal expansion are being utilized to derive the regression equation.
The performance of the proposed approaches is demonstrated using highly
nonlinear dynamical systems such as the crack-degradation problem. Numerical
results demonstrated in this paper almost exactly identify the correct
perturbation terms along with their associated parameters in the dynamical
system. The probabilistic nature of the proposed approach also helps in
quantifying the uncertainties associated with updated models. The proposed
approaches provide an exact and explainable description of the perturbations in
digital twin models, which can be directly used for better cyber-physical
integration, long-term future predictions, degradation monitoring, and
model-agnostic control.","['Tapas Tripura', 'Aarya Sheetal Desai', 'Sondipon Adhikari', 'Souvik Chakraborty']","['stat.ML', 'cs.LG']",2022-12-19 04:25:59+00:00
http://arxiv.org/abs/2212.09201v3,Spectral Regularized Kernel Two-Sample Tests,"Over the last decade, an approach that has gained a lot of popularity to
tackle nonparametric testing problems on general (i.e., non-Euclidean) domains
is based on the notion of reproducing kernel Hilbert space (RKHS) embedding of
probability distributions. The main goal of our work is to understand the
optimality of two-sample tests constructed based on this approach. First, we
show the popular MMD (maximum mean discrepancy) two-sample test to be not
optimal in terms of the separation boundary measured in Hellinger distance.
Second, we propose a modification to the MMD test based on spectral
regularization by taking into account the covariance information (which is not
captured by the MMD test) and prove the proposed test to be minimax optimal
with a smaller separation boundary than that achieved by the MMD test. Third,
we propose an adaptive version of the above test which involves a data-driven
strategy to choose the regularization parameter and show the adaptive test to
be almost minimax optimal up to a logarithmic factor. Moreover, our results
hold for the permutation variant of the test where the test threshold is chosen
elegantly through the permutation of the samples. Through numerical experiments
on synthetic and real data, we demonstrate the superior performance of the
proposed test in comparison to the MMD test and other popular tests in the
literature.","['Omar Hagrass', 'Bharath K. Sriperumbudur', 'Bing Li']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH', 'Primary: 62G10, Secondary: 65J20, 65J22, 46E22, 47A52']",2022-12-19 00:42:21+00:00
http://arxiv.org/abs/2212.09184v1,Faithful Heteroscedastic Regression with Neural Networks,"Heteroscedastic regression models a Gaussian variable's mean and variance as
a function of covariates. Parametric methods that employ neural networks for
these parameter maps can capture complex relationships in the data. Yet,
optimizing network parameters via log likelihood gradients can yield suboptimal
mean and uncalibrated variance estimates. Current solutions side-step this
optimization problem with surrogate objectives or Bayesian treatments. Instead,
we make two simple modifications to optimization. Notably, their combination
produces a heteroscedastic model with mean estimates that are provably as
accurate as those from its homoscedastic counterpart (i.e.~fitting the mean
under squared error loss). For a wide variety of network and task complexities,
we find that mean estimates from existing heteroscedastic solutions can be
significantly less accurate than those from an equivalently expressive
mean-only model. Our approach provably retains the accuracy of an equally
flexible mean-only model while also offering best-in-class variance
calibration. Lastly, we show how to leverage our method to recover the
underlying heteroscedastic noise variance.","['Andrew Stirn', 'Hans-Hermann Wessels', 'Megan Schertzer', 'Laura Pereira', 'Neville E. Sanjana', 'David A. Knowles']","['cs.LG', 'stat.ML']",2022-12-18 22:34:42+00:00
http://arxiv.org/abs/2212.09178v5,Support Vector Regression: Risk Quadrangle Framework,"This paper investigates Support Vector Regression (SVR) in the context of the
fundamental risk quadrangle theory, which links optimization, risk management,
and statistical estimation. It is shown that both formulations of SVR,
$\varepsilon$-SVR and $\nu$-SVR, correspond to the minimization of equivalent
error measures (Vapnik error and CVaR norm, respectively) with a regularization
penalty. These error measures, in turn, define the corresponding risk
quadrangles. By constructing the fundamental risk quadrangle, which corresponds
to SVR, we show that SVR is the asymptotically unbiased estimator of the
average of two symmetric conditional quantiles. Further, we prove the
equivalence of the $\varepsilon$-SVR and $\nu$-SVR in a general stochastic
setting. Additionally, SVR is formulated as a regular deviation minimization
problem with a regularization penalty. Finally, the dual formulation of SVR in
the risk quadrangle framework is derived.","['Anton Malandii', 'Stan Uryasev']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-12-18 21:53:33+00:00
http://arxiv.org/abs/2212.09108v1,A Permutation-Free Kernel Independence Test,"In nonparametric independence testing, we observe i.i.d.\ data
$\{(X_i,Y_i)\}_{i=1}^n$, where $X \in \mathcal{X}, Y \in \mathcal{Y}$ lie in
any general spaces, and we wish to test the null that $X$ is independent of
$Y$. Modern test statistics such as the kernel Hilbert-Schmidt Independence
Criterion (HSIC) and Distance Covariance (dCov) have intractable null
distributions due to the degeneracy of the underlying U-statistics. Thus, in
practice, one often resorts to using permutation testing, which provides a
nonasymptotic guarantee at the expense of recalculating the quadratic-time
statistics (say) a few hundred times. This paper provides a simple but
nontrivial modification of HSIC and dCov (called xHSIC and xdCov, pronounced
``cross'' HSIC/dCov) so that they have a limiting Gaussian distribution under
the null, and thus do not require permutations. This requires building on the
newly developed theory of cross U-statistics by Kim and Ramdas (2020), and in
particular developing several nontrivial extensions of the theory in Shekhar et
al. (2022), which developed an analogous permutation-free kernel two-sample
test. We show that our new tests, like the originals, are consistent against
fixed alternatives, and minimax rate optimal against smooth local alternatives.
Numerical simulations demonstrate that compared to the full dCov or HSIC, our
variants have the same power up to a $\sqrt 2$ factor, giving practitioners a
new option for large problems or data-analysis pipelines where computation, not
sample size, could be the bottleneck.","['Shubhanshu Shekhar', 'Ilmun Kim', 'Aaditya Ramdas']","['stat.ME', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2022-12-18 15:28:16+00:00
http://arxiv.org/abs/2212.09083v1,Influence-Based Mini-Batching for Graph Neural Networks,"Using graph neural networks for large graphs is challenging since there is no
clear way of constructing mini-batches. To solve this, previous methods have
relied on sampling or graph clustering. While these approaches often lead to
good training convergence, they introduce significant overhead due to expensive
random data accesses and perform poorly during inference. In this work we
instead focus on model behavior during inference. We theoretically model batch
construction via maximizing the influence score of nodes on the outputs. This
formulation leads to optimal approximation of the output when we do not have
knowledge of the trained model. We call the resulting method influence-based
mini-batching (IBMB). IBMB accelerates inference by up to 130x compared to
previous methods that reach similar accuracy. Remarkably, with adaptive
optimization and the right training schedule IBMB can also substantially
accelerate training, thanks to precomputed batches and consecutive memory
accesses. This results in up to 18x faster training per epoch and up to 17x
faster convergence per runtime compared to previous methods.","['Johannes Gasteiger', 'Chendi Qian', 'Stephan G√ºnnemann']","['cs.LG', 'cs.AI', 'cs.SI', 'stat.ML']",2022-12-18 13:27:01+00:00
http://arxiv.org/abs/2212.09081v1,Riemannian Optimization for Variance Estimation in Linear Mixed Models,"Variance parameter estimation in linear mixed models is a challenge for many
classical nonlinear optimization algorithms due to the positive-definiteness
constraint of the random effects covariance matrix. We take a completely novel
view on parameter estimation in linear mixed models by exploiting the intrinsic
geometry of the parameter space. We formulate the problem of residual maximum
likelihood estimation as an optimization problem on a Riemannian manifold.
Based on the introduced formulation, we give geometric higher-order information
on the problem via the Riemannian gradient and the Riemannian Hessian. Based on
that, we test our approach with Riemannian optimization algorithms numerically.
Our approach yields a higher quality of the variance parameter estimates
compared to existing approaches.","['Lena Sembach', 'Jan Pablo Burgard', 'Volker H. Schulz']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2022-12-18 13:08:45+00:00
http://arxiv.org/abs/2212.08949v3,Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off,"A default assumption in reinforcement learning (RL) and optimal control is
that observations arrive at discrete time points on a fixed clock cycle. Yet,
many applications involve continuous-time systems where the time
discretization, in principle, can be managed. The impact of time discretization
on RL methods has not been fully characterized in existing theory, but a more
detailed analysis of its effect could reveal opportunities for improving
data-efficiency. We address this gap by analyzing Monte-Carlo policy evaluation
for LQR systems and uncover a fundamental trade-off between approximation and
statistical error in value estimation. Importantly, these two errors behave
differently to time discretization, leading to an optimal choice of temporal
resolution for a given data budget. These findings show that managing the
temporal resolution can provably improve policy evaluation efficiency in LQR
systems with finite data. Empirically, we demonstrate the trade-off in
numerical simulations of LQR instances and standard RL benchmarks for
non-linear continuous control.","['Zichen Zhang', 'Johannes Kirschner', 'Junxi Zhang', 'Francesco Zanini', 'Alex Ayoub', 'Masood Dehghan', 'Dale Schuurmans']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2022-12-17 20:45:34+00:00
http://arxiv.org/abs/2212.08890v1,TCFimt: Temporal Counterfactual Forecasting from Individual Multiple Treatment Perspective,"Determining causal effects of temporal multi-intervention assists
decision-making. Restricted by time-varying bias, selection bias, and
interactions of multiple interventions, the disentanglement and estimation of
multiple treatment effects from individual temporal data is still rare. To
tackle these challenges, we propose a comprehensive framework of temporal
counterfactual forecasting from an individual multiple treatment perspective
(TCFimt). TCFimt constructs adversarial tasks in a seq2seq framework to
alleviate selection and time-varying bias and designs a contrastive
learning-based block to decouple a mixed treatment effect into separated main
treatment effects and causal interactions which further improves estimation
accuracy. Through implementing experiments on two real-world datasets from
distinct fields, the proposed method shows satisfactory performance in
predicting future outcomes with specific treatments and in choosing optimal
treatment type and timing than state-of-the-art methods.","['Pengfei Xi', 'Guifeng Wang', 'Zhipeng Hu', 'Yu Xiong', 'Mingming Gong', 'Wei Huang', 'Runze Wu', 'Yu Ding', 'Tangjie Lv', 'Changjie Fan', 'Xiangnan Feng']","['cs.LG', 'stat.ML']",2022-12-17 15:01:05+00:00
http://arxiv.org/abs/2212.08791v2,Two-Scale Gradient Descent Ascent Dynamics Finds Mixed Nash Equilibria of Continuous Games: A Mean-Field Perspective,"Finding the mixed Nash equilibria (MNE) of a two-player zero sum continuous
game is an important and challenging problem in machine learning. A canonical
algorithm to finding the MNE is the noisy gradient descent ascent method which
in the infinite particle limit gives rise to the {\em Mean-Field Gradient
Descent Ascent} (GDA) dynamics on the space of probability measures. In this
paper, we first study the convergence of a two-scale Mean-Field GDA dynamics
for finding the MNE of the entropy-regularized objective. More precisely we
show that for each finite temperature (or regularization parameter), the
two-scale Mean-Field GDA with a suitable {\em finite} scale ratio converges
exponentially to the unique MNE without assuming the convexity or concavity of
the interaction potential. The key ingredient of our proof lies in the
construction of new Lyapunov functions that dissipate exponentially along the
Mean-Field GDA. We further study the simulated annealing of the Mean-Field GDA
dynamics. We show that with a temperature schedule that decays logarithmically
in time the annealed Mean-Field GDA converges to the MNE of the original
unregularized objective.",['Yulong Lu'],"['math.OC', 'cs.LG', 'math.AP', 'math.PR', 'stat.ML']",2022-12-17 03:44:35+00:00
http://arxiv.org/abs/2212.08765v2,Latent Variable Representation for Reinforcement Learning,"Deep latent variable models have achieved significant empirical successes in
model-based reinforcement learning (RL) due to their expressiveness in modeling
complex transition dynamics. On the other hand, it remains unclear
theoretically and empirically how latent variable models may facilitate
learning, planning, and exploration to improve the sample efficiency of RL. In
this paper, we provide a representation view of the latent variable models for
state-action value functions, which allows both tractable variational learning
algorithm and effective implementation of the optimism/pessimism principle in
the face of uncertainty for exploration. In particular, we propose a
computationally efficient planning algorithm with UCB exploration by
incorporating kernel embeddings of latent variable models. Theoretically, we
establish the sample complexity of the proposed approach in the online and
offline settings. Empirically, we demonstrate superior performance over current
state-of-the-art algorithms across various benchmarks.","['Tongzheng Ren', 'Chenjun Xiao', 'Tianjun Zhang', 'Na Li', 'Zhaoran Wang', 'Sujay Sanghavi', 'Dale Schuurmans', 'Bo Dai']","['cs.LG', 'stat.ML']",2022-12-17 00:26:31+00:00
http://arxiv.org/abs/2212.08697v2,Multi-Task Learning for Sparsity Pattern Heterogeneity: Statistical and Computational Perspectives,"We consider a problem in Multi-Task Learning (MTL) where multiple linear
models are jointly trained on a collection of datasets (""tasks""). A key novelty
of our framework is that it allows the sparsity pattern of regression
coefficients and the values of non-zero coefficients to differ across tasks
while still leveraging partially shared structure. Our methods encourage models
to share information across tasks through separately encouraging 1) coefficient
supports, and/or 2) nonzero coefficient values to be similar. This allows
models to borrow strength during variable selection even when non-zero
coefficient values differ across tasks. We propose a novel mixed-integer
programming formulation for our estimator. We develop custom scalable
algorithms based on block coordinate descent and combinatorial local search to
obtain high-quality (approximate) solutions for our estimator. Additionally, we
propose a novel exact optimization algorithm to obtain globally optimal
solutions. We investigate the theoretical properties of our estimators. We
formally show how our estimators leverage the shared support information across
tasks to achieve better variable selection performance. We evaluate the
performance of our methods in simulations and two biomedical applications. Our
proposed approaches appear to outperform other sparse MTL methods in variable
selection and prediction accuracy. We provide the sMTL package on CRAN.","['Kayhan Behdin', 'Gabriel Loewinger', 'Kenneth T. Kishida', 'Giovanni Parmigiani', 'Rahul Mazumder']","['stat.ME', 'stat.ML']",2022-12-16 19:52:25+00:00
http://arxiv.org/abs/2212.08648v3,Connecting Permutation Equivariant Neural Networks and Partition Diagrams,"Permutation equivariant neural networks are often constructed using tensor
powers of $\mathbb{R}^{n}$ as their layer spaces. We show that all of the
weight matrices that appear in these neural networks can be obtained from
Schur-Weyl duality between the symmetric group and the partition algebra. In
particular, we adapt Schur-Weyl duality to derive a simple, diagrammatic method
for calculating the weight matrices themselves.",['Edward Pearce-Crump'],"['cs.LG', 'math.CO', 'math.RT', 'stat.ML']",2022-12-16 18:48:54+00:00
http://arxiv.org/abs/2212.08645v2,Efficient Conditionally Invariant Representation Learning,"We introduce the Conditional Independence Regression CovariancE (CIRCE), a
measure of conditional independence for multivariate continuous-valued
variables. CIRCE applies as a regularizer in settings where we wish to learn
neural features $\varphi(X)$ of data $X$ to estimate a target $Y$, while being
conditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are
assumed to be continuous-valued but relatively low dimensional, whereas $X$ and
its features may be complex and high dimensional. Relevant settings include
domain-invariant learning, fairness, and causal learning. The procedure
requires just a single ridge regression from $Y$ to kernelized features of $Z$,
which can be done in advance. It is then only necessary to enforce independence
of $\varphi(X)$ from residuals of this regression, which is possible with
attractive estimation properties and consistency guarantees. By contrast,
earlier measures of conditional feature dependence require multiple regressions
for each step of feature learning, resulting in more severe bias and variance,
and greater computational cost. When sufficiently rich features are used, we
establish that CIRCE is zero if and only if $\varphi(X) \perp \!\!\! \perp Z
\mid Y$. In experiments, we show superior performance to previous methods on
challenging benchmarks, including learning conditionally invariant image
features.","['Roman Pogodin', 'Namrata Deka', 'Yazhe Li', 'Danica J. Sutherland', 'Victor Veitch', 'Arthur Gretton']","['cs.LG', 'stat.ML']",2022-12-16 18:39:32+00:00
http://arxiv.org/abs/2212.08642v3,"Estimating Higher-Order Mixed Memberships via the $\ell_{2,\infty}$ Tensor Perturbation Bound","Higher-order multiway data is ubiquitous in machine learning and statistics
and often exhibits community-like structures, where each component (node) along
each different mode has a community membership associated with it. In this
paper we propose the tensor mixed-membership blockmodel, a generalization of
the tensor blockmodel positing that memberships need not be discrete, but
instead are convex combinations of latent communities. We establish the
identifiability of our model and propose a computationally efficient estimation
procedure based on the higher-order orthogonal iteration algorithm (HOOI) for
tensor SVD composed with a simplex corner-finding algorithm. We then
demonstrate the consistency of our estimation procedure by providing a per-node
error bound, which showcases the effect of higher-order structures on
estimation accuracy. To prove our consistency result, we develop the
$\ell_{2,\infty}$ tensor perturbation bound for HOOI under independent,
heteroskedastic, subgaussian noise that may be of independent interest. Our
analysis uses a novel leave-one-out construction for the iterates, and our
bounds depend only on spectral properties of the underlying low-rank tensor
under nearly optimal signal-to-noise ratio conditions such that tensor SVD is
computationally feasible. Finally, we apply our methodology to real and
simulated data, demonstrating some effects not identifiable from the model with
discrete community memberships.","['Joshua Agterberg', 'Anru Zhang']","['math.ST', 'math.OC', 'stat.ME', 'stat.ML', 'stat.TH']",2022-12-16 18:32:20+00:00
http://arxiv.org/abs/2212.08630v2,Brauer's Group Equivariant Neural Networks,"We provide a full characterisation of all of the possible group equivariant
neural networks whose layers are some tensor power of $\mathbb{R}^{n}$ for
three symmetry groups that are missing from the machine learning literature:
$O(n)$, the orthogonal group; $SO(n)$, the special orthogonal group; and
$Sp(n)$, the symplectic group. In particular, we find a spanning set of
matrices for the learnable, linear, equivariant layer functions between such
tensor power spaces in the standard basis of $\mathbb{R}^{n}$ when the group is
$O(n)$ or $SO(n)$, and in the symplectic basis of $\mathbb{R}^{n}$ when the
group is $Sp(n)$.",['Edward Pearce-Crump'],"['cs.LG', 'math.CO', 'math.RT', 'stat.ML']",2022-12-16 18:08:51+00:00
http://arxiv.org/abs/2212.08581v1,Penalised regression with multiple sources of prior effects,"In many high-dimensional prediction or classification tasks, complementary
data on the features are available, e.g. prior biological knowledge on
(epi)genetic markers. Here we consider tasks with numerical prior information
that provide an insight into the importance (weight) and the direction (sign)
of the feature effects, e.g. regression coefficients from previous studies. We
propose an approach for integrating multiple sources of such prior information
into penalised regression. If suitable co-data are available, this improves the
predictive performance, as shown by simulation and application. The proposed
method is implemented in the R package `transreg'
(https://github.com/lcsb-bds/transreg).","['Armin Rauschenberger', 'Zied Landoulsi', 'Mark A. van de Wiel', 'Enrico Glaab']","['stat.ME', 'stat.ML']",2022-12-16 16:58:48+00:00
http://arxiv.org/abs/2212.08580v1,Nested Gradient Codes for Straggler Mitigation in Distributed Machine Learning,"We consider distributed learning in the presence of slow and unresponsive
worker nodes, referred to as stragglers. In order to mitigate the effect of
stragglers, gradient coding redundantly assigns partial computations to the
worker such that the overall result can be recovered from only the
non-straggling workers. Gradient codes are designed to tolerate a fixed number
of stragglers. Since the number of stragglers in practice is random and unknown
a priori, tolerating a fixed number of stragglers can yield a sub-optimal
computation load and can result in higher latency. We propose a gradient coding
scheme that can tolerate a flexible number of stragglers by carefully
concatenating gradient codes for different straggler tolerance. By proper task
scheduling and small additional signaling, our scheme adapts the computation
load of the workers to the actual number of stragglers. We analyze the latency
of our proposed scheme and show that it has a significantly lower latency than
gradient codes.","['Luis Ma√üny', 'Christoph Hofmeister', 'Maximilian Egger', 'Rawad Bitar', 'Antonia Wachter-Zeh']","['cs.IT', 'math.IT', 'stat.ML']",2022-12-16 16:56:51+00:00
http://arxiv.org/abs/2212.08541v1,Learnable Commutative Monoids for Graph Neural Networks,"Graph neural networks (GNNs) have been shown to be highly sensitive to the
choice of aggregation function. While summing over a node's neighbours can
approximate any permutation-invariant function over discrete inputs,
Cohen-Karlik et al. [2020] proved there are set-aggregation problems for which
summing cannot generalise to unbounded inputs, proposing recurrent neural
networks regularised towards permutation-invariance as a more expressive
aggregator. We show that these results carry over to the graph domain: GNNs
equipped with recurrent aggregators are competitive with state-of-the-art
permutation-invariant aggregators, on both synthetic benchmarks and real-world
problems. However, despite the benefits of recurrent aggregators, their $O(V)$
depth makes them both difficult to parallelise and harder to train on large
graphs. Inspired by the observation that a well-behaved aggregator for a GNN is
a commutative monoid over its latent space, we propose a framework for
constructing learnable, commutative, associative binary operators. And with
this, we construct an aggregator of $O(\log V)$ depth, yielding exponential
improvements for both parallelism and dependency length while achieving
performance competitive with recurrent aggregators. Based on our empirical
observations, our proposed learnable commutative monoid (LCM) aggregator
represents a favourable tradeoff between efficient and expressive aggregators.","['Euan Ong', 'Petar Veliƒçkoviƒá']","['cs.LG', 'cs.AI', 'stat.ML']",2022-12-16 15:43:41+00:00
http://arxiv.org/abs/2212.08339v1,Generalization Bounds for Inductive Matrix Completion in Low-noise Settings,"We study inductive matrix completion (matrix completion with side
information) under an i.i.d. subgaussian noise assumption at a low noise
regime, with uniform sampling of the entries. We obtain for the first time
generalization bounds with the following three properties: (1) they scale like
the standard deviation of the noise and in particular approach zero in the
exact recovery case; (2) even in the presence of noise, they converge to zero
when the sample size approaches infinity; and (3) for a fixed dimension of the
side information, they only have a logarithmic dependence on the size of the
matrix. Differently from many works in approximate recovery, we present results
both for bounded Lipschitz losses and for the absolute loss, with the latter
relying on Talagrand-type inequalities. The proofs create a bridge between two
approaches to the theoretical analysis of matrix completion, since they consist
in a combination of techniques from both the exact recovery literature and the
approximate recovery literature.","['Antoine Ledent', 'Rodrigo Alves', 'Yunwen Lei', 'Yann Guermeur', 'Marius Kloft']","['cs.LG', 'stat.ML']",2022-12-16 08:30:41+00:00
http://arxiv.org/abs/2212.08329v1,Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder,"Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of
the factors that have been driving TTS are the advancements of probabilistic
models and latent representation learning. We propose a TTS method based on
latent variable conversion using a diffusion probabilistic model and the
variational autoencoder (VAE). In our TTS method, we use a waveform model based
on VAE, a diffusion model that predicts the distribution of latent variables in
the waveform model from texts, and an alignment model that learns alignments
between the text and speech latent sequences. Our method integrates diffusion
with VAE by modeling both mean and variance parameters with diffusion, where
the target distribution is determined by approximation from VAE. This latent
variable conversion framework potentially enables us to flexibly incorporate
various latent feature extractors. Our experiments show that our method is
robust to linguistic labels with poor orthography and alignment errors.","['Yusuke Yasuda', 'Tomoki Toda']","['eess.AS', 'cs.CL', 'stat.ML']",2022-12-16 08:14:04+00:00
http://arxiv.org/abs/2212.08255v1,A Sieve Quasi-likelihood Ratio Test for Neural Networks with Applications to Genetic Association Studies,"Neural networks (NN) play a central role in modern Artificial intelligence
(AI) technology and has been successfully used in areas such as natural
language processing and image recognition. While majority of NN applications
focus on prediction and classification, there are increasing interests in
studying statistical inference of neural networks. The study of NN statistical
inference can enhance our understanding of NN statistical proprieties.
Moreover, it can facilitate the NN-based hypothesis testing that can be applied
to hypothesis-driven clinical and biomedical research. In this paper, we
propose a sieve quasi-likelihood ratio test based on NN with one hidden layer
for testing complex associations. The test statistic has asymptotic chi-squared
distribution, and therefore it is computationally efficient and easy for
implementation in real data analysis. The validity of the asymptotic
distribution is investigated via simulations. Finally, we demonstrate the use
of the proposed test by performing a genetic association analysis of the
sequencing data from Alzheimer's Disease Neuroimaging Initiative (ADNI).","['Xiaoxi Shen', 'Chang Jiang', 'Lyudmila Sakhanenko', 'Qing Lu']","['stat.ML', 'math.ST', 'stat.TH']",2022-12-16 02:54:46+00:00
http://arxiv.org/abs/2212.08225v1,Materials Discovery using Max K-Armed Bandit,"Search algorithms for the bandit problems are applicable in materials
discovery. However, the objectives of the conventional bandit problem are
different from those of materials discovery. The conventional bandit problem
aims to maximize the total rewards, whereas materials discovery aims to achieve
breakthroughs in material properties. The max K-armed bandit (MKB) problem,
which aims to acquire the single best reward, matches with the discovery tasks
better than the conventional bandit. Thus, here, we propose a search algorithm
for materials discovery based on the MKB problem using a pseudo-value of the
upper confidence bound of expected improvement of the best reward. This
approach is pseudo-guaranteed to be asymptotic oracles that do not depends on
the time horizon. In addition, compared with other MKB algorithms, the proposed
algorithm has only one hyperparameter, which is advantageous in materials
discovery. We applied the proposed algorithm to synthetic problems and
molecular-design demonstrations using a Monte Carlo tree search. According to
the results, the proposed algorithm stably outperformed other bandit algorithms
in the late stage of the search process when the optimal arm of the MKB could
not be determined based on its expectation reward.","['Nobuaki Kikkawa', 'Hiroshi Ohno']","['stat.ML', 'cs.LG', 'physics.chem-ph']",2022-12-16 01:27:42+00:00
http://arxiv.org/abs/2212.08162v2,Huber-energy measure quantization,"We describe a measure quantization procedure i.e., an algorithm which finds
the best approximation of a target probability law (and more generally signed
finite variation measure) by a sum of $Q$ Dirac masses ($Q$ being the
quantization parameter). The procedure is implemented by minimizing the
statistical distance between the original measure and its quantized version;
the distance is built from a negative definite kernel and, if necessary, can be
computed on the fly and feed to a stochastic optimization algorithm (such as
SGD, Adam, ...). We investigate theoretically the fundamental questions of
existence of the optimal measure quantizer and identify what are the required
kernel properties that guarantee suitable behavior. We propose two best linear
unbiased (BLUE) estimators for the squared statistical distance and use them in
an unbiased procedure, called HEMQ, to find the optimal quantization. We test
HEMQ on several databases: multi-dimensional Gaussian mixtures, Wiener space
cubature, Italian wine cultivars and the MNIST image database. The results
indicate that the HEMQ algorithm is robust and versatile and, for the class of
Huber-energy kernels, matches the expected intuitive behavior.",['Gabriel Turinici'],"['stat.ML', 'cs.AI', 'cs.LG', 'cs.NA', 'math.NA', 'math.PR', 'math.ST', 'stat.TH']",2022-12-15 21:50:54+00:00
http://arxiv.org/abs/2212.08123v3,Bayesian posterior approximation with stochastic ensembles,"We introduce ensembles of stochastic neural networks to approximate the
Bayesian posterior, combining stochastic methods such as dropout with deep
ensembles. The stochastic ensembles are formulated as families of distributions
and trained to approximate the Bayesian posterior with variational inference.
We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and
a novel non-parametric version of dropout and evaluate them on a toy problem
and CIFAR image classification. For both tasks, we test the quality of the
posteriors directly against Hamiltonian Monte Carlo simulations. Our results
show that stochastic ensembles provide more accurate posterior estimates than
other popular baselines for Bayesian inference.","['Oleksandr Balabanov', 'Bernhard Mehlig', 'Hampus Linander']","['cs.LG', 'cs.CV', 'stat.ML']",2022-12-15 20:23:09+00:00
http://arxiv.org/abs/2212.08660v1,Learning Inter-Annual Flood Loss Risk Models From Historical Flood Insurance Claims and Extreme Rainfall Data,"Flooding is one of the most disastrous natural hazards, responsible for
substantial economic losses. A predictive model for flood-induced financial
damages is useful for many applications such as climate change adaptation
planning and insurance underwriting. This research assesses the predictive
capability of regressors constructed on the National Flood Insurance Program
(NFIP) dataset using neural networks (Conditional Generative Adversarial
Networks), decision trees (Extreme Gradient Boosting), and kernel-based
regressors (Gaussian Process). The assessment highlights the most informative
predictors for regression. The distribution for claims amount inference is
modeled with a Burr distribution permitting the introduction of a bias
correction scheme and increasing the regressor's predictive capability. Aiming
to study the interaction with physical variables, we incorporate Daymet
rainfall estimation to NFIP as an additional predictor. A study on the coastal
counties in the eight US South-West states resulted in an $R^2=0.807$. Further
analysis of 11 counties with a significant number of claims in the NFIP dataset
reveals that Extreme Gradient Boosting provides the best results, that bias
correction significantly improves the similarity with the reference
distribution, and that the rainfall predictor strengthens the regressor
performance.","['Joaquin Salas', 'Anamitra Saha', 'Sai Ravela']","['cs.LG', 'physics.data-an', 'stat.AP', 'stat.ML']",2022-12-15 19:23:02+00:00
http://arxiv.org/abs/2212.08049v9,Sliced Optimal Partial Transport,"Optimal transport (OT) has become exceedingly popular in machine learning,
data science, and computer vision. The core assumption in the OT problem is the
equal total amount of mass in source and target measures, which limits its
application. Optimal Partial Transport (OPT) is a recently proposed solution to
this limitation. Similar to the OT problem, the computation of OPT relies on
solving a linear programming problem (often in high dimensions), which can
become computationally prohibitive. In this paper, we propose an efficient
algorithm for calculating the OPT problem between two non-negative measures in
one dimension. Next, following the idea of sliced OT distances, we utilize
slicing to define the sliced OPT distance. Finally, we demonstrate the
computational and accuracy benefits of the sliced OPT-based method in various
numerical experiments. In particular, we show an application of our proposed
Sliced-OPT in noisy point cloud registration.","['Yikun Bai', 'Berhnard Schmitzer', 'Mathew Thorpe', 'Soheil Kolouri']","['cs.LG', 'math.OC', 'stat.ML']",2022-12-15 18:55:23+00:00
http://arxiv.org/abs/2212.08018v2,"Privately Estimating a Gaussian: Efficient, Robust and Optimal","In this work, we give efficient algorithms for privately estimating a
Gaussian distribution in both pure and approximate differential privacy (DP)
models with optimal dependence on the dimension in the sample complexity. In
the pure DP setting, we give an efficient algorithm that estimates an unknown
$d$-dimensional Gaussian distribution up to an arbitrary tiny total variation
error using $\widetilde{O}(d^2 \log \kappa)$ samples while tolerating a
constant fraction of adversarial outliers. Here, $\kappa$ is the condition
number of the target covariance matrix. The sample bound matches best
non-private estimators in the dependence on the dimension (up to a
polylogarithmic factor). We prove a new lower bound on differentially private
covariance estimation to show that the dependence on the condition number
$\kappa$ in the above sample bound is also tight. Prior to our work, only
identifiability results (yielding inefficient super-polynomial time algorithms)
were known for the problem. In the approximate DP setting, we give an efficient
algorithm to estimate an unknown Gaussian distribution up to an arbitrarily
tiny total variation error using $\widetilde{O}(d^2)$ samples while tolerating
a constant fraction of adversarial outliers. Prior to our work, all efficient
approximate DP algorithms incurred a super-quadratic sample cost or were not
outlier-robust. For the special case of mean estimation, our algorithm achieves
the optimal sample complexity of $\widetilde O(d)$, improving on a $\widetilde
O(d^{1.5})$ bound from prior work. Our pure DP algorithm relies on a recursive
private preconditioning subroutine that utilizes the recent work on private
mean estimation [Hopkins et al., 2022]. Our approximate DP algorithms are based
on a substantial upgrade of the method of stabilizing convex relaxations
introduced in [Kothari et al., 2022].","['Daniel Alabi', 'Pravesh K. Kothari', 'Pranay Tankala', 'Prayaag Venkat', 'Fred Zhang']","['cs.DS', 'cs.CR', 'cs.IT', 'math.IT', 'stat.ML']",2022-12-15 18:27:39+00:00
http://arxiv.org/abs/2212.07918v1,Construction of a Surrogate Model: Multivariate Time Series Prediction with a Hybrid Model,"Recent developments of advanced driver-assistance systems necessitate an
increasing number of tests to validate new technologies. These tests cannot be
carried out on track in a reasonable amount of time and automotive groups rely
on simulators to perform most tests. The reliability of these simulators for
constantly refined tasks is becoming an issue and, to increase the number of
tests, the industry is now developing surrogate models, that should mimic the
behavior of the simulator while being much faster to run on specific tasks.
  In this paper we aim to construct a surrogate model to mimic and replace the
simulator. We first test several classical methods such as random forests,
ridge regression or convolutional neural networks. Then we build three hybrid
models that use all these methods and combine them to obtain an efficient
hybrid surrogate model.","['Clara Carlier', 'Arnaud Franju', 'Matthieu Lerasle', 'Mathias Obrebski']","['stat.ML', 'cs.LG']",2022-12-15 15:52:18+00:00
http://arxiv.org/abs/2212.07881v1,Identifying AGN host galaxies with convolutional neural networks,"Active galactic nuclei (AGN) are supermassive black holes with luminous
accretion disks found in some galaxies, and are thought to play an important
role in galaxy evolution. However, traditional optical spectroscopy for
identifying AGN requires time-intensive observations. We train a convolutional
neural network (CNN) to distinguish AGN host galaxies from non-active galaxies
using a sample of 210,000 Sloan Digital Sky Survey galaxies. We evaluate the
CNN on 33,000 galaxies that are spectrally classified as composites, and find
correlations between galaxy appearances and their CNN classifications, which
hint at evolutionary processes that affect both galaxy morphology and AGN
activity. With the advent of the Vera C. Rubin Observatory, Nancy Grace Roman
Space Telescope, and other wide-field imaging telescopes, deep learning methods
will be instrumental for quickly and reliably shortlisting AGN samples for
future analyses.","['Ziting Guo', 'John F. Wu', 'Chelsea E. Sharon']","['astro-ph.GA', 'stat.ML']",2022-12-15 15:04:40+00:00
http://arxiv.org/abs/2212.07818v1,Towards Hardware-Specific Automatic Compression of Neural Networks,"Compressing neural network architectures is important to allow the deployment
of models to embedded or mobile devices, and pruning and quantization are the
major approaches to compress neural networks nowadays. Both methods benefit
when compression parameters are selected specifically for each layer. Finding
good combinations of compression parameters, so-called compression policies, is
hard as the problem spans an exponentially large search space. Effective
compression policies consider the influence of the specific hardware
architecture on the used compression methods. We propose an algorithmic
framework called Galen to search such policies using reinforcement learning
utilizing pruning and quantization, thus providing automatic compression for
neural networks. Contrary to other approaches we use inference latency measured
on the target hardware device as an optimization goal. With that, the framework
supports the compression of models specific to a given hardware target. We
validate our approach using three different reinforcement learning agents for
pruning, quantization and joint pruning and quantization. Besides proving the
functionality of our approach we were able to compress a ResNet18 for CIFAR-10,
on an embedded ARM processor, to 20% of the original inference latency without
significant loss of accuracy. Moreover, we can demonstrate that a joint search
and compression using pruning and quantization is superior to an individual
search for policies using a single compression method.","['Torben Krieger', 'Bernhard Klein', 'Holger Fr√∂ning']","['cs.LG', 'cs.PF', 'stat.ML']",2022-12-15 13:34:02+00:00
http://arxiv.org/abs/2212.07632v2,Reinforcement Learning in Credit Scoring and Underwriting,"This paper proposes a novel reinforcement learning (RL) framework for credit
underwriting that tackles ungeneralizable contextual challenges. We adapt RL
principles for credit scoring, incorporating action space renewal and
multi-choice actions. Our work demonstrates that the traditional underwriting
approach aligns with the RL greedy strategy. We introduce two new RL-based
credit underwriting algorithms to enable more informed decision-making.
Simulations show these new approaches outperform the traditional method in
scenarios where the data aligns with the model. However, complex situations
highlight model limitations, emphasizing the importance of powerful machine
learning models for optimal performance. Future research directions include
exploring more sophisticated models alongside efficient exploration mechanisms.","['Seksan Kiatsupaibul', 'Pakawan Chansiripas', 'Pojtanut Manopanjasiri', 'Kantapong Visantavarakul', 'Zheng Wen']","['stat.ML', 'cs.LG']",2022-12-15 06:36:14+00:00
http://arxiv.org/abs/2212.07524v3,Invariant Lipschitz Bandits: A Side Observation Approach,"Symmetry arises in many optimization and decision-making problems, and has
attracted considerable attention from the optimization community: By utilizing
the existence of such symmetries, the process of searching for optimal
solutions can be improved significantly. Despite its success in (offline)
optimization, the utilization of symmetries has not been well examined within
the online optimization settings, especially in the bandit literature. As such,
in this paper we study the invariant Lipschitz bandit setting, a subclass of
the Lipschitz bandits where the reward function and the set of arms are
preserved under a group of transformations. We introduce an algorithm named
\texttt{UniformMesh-N}, which naturally integrates side observations using
group orbits into the \texttt{UniformMesh} algorithm
(\cite{Kleinberg2005_UniformMesh}), which uniformly discretizes the set of
arms. Using the side-observation approach, we prove an improved regret upper
bound, which depends on the cardinality of the group, given that the group is
finite. We also prove a matching regret's lower bound for the invariant
Lipschitz bandit class (up to logarithmic factors). We hope that our work will
ignite further investigation of symmetry in bandit theory and sequential
decision-making theory in general.","['Nam Phuong Tran', 'Long Tran-Thanh']","['cs.LG', 'stat.ML']",2022-12-14 22:12:32+00:00
http://arxiv.org/abs/2212.07414v1,Hierarchical Over-the-Air FedGradNorm,"Multi-task learning (MTL) is a learning paradigm to learn multiple related
tasks simultaneously with a single shared network where each task has a
distinct personalized header network for fine-tuning. MTL can be integrated
into a federated learning (FL) setting if tasks are distributed across clients
and clients have a single shared network, leading to personalized federated
learning (PFL). To cope with statistical heterogeneity in the federated setting
across clients which can significantly degrade the learning performance, we use
a distributed dynamic weighting approach. To perform the communication between
the remote parameter server (PS) and the clients efficiently over the noisy
channel in a power and bandwidth-limited regime, we utilize over-the-air (OTA)
aggregation and hierarchical federated learning (HFL). Thus, we propose
hierarchical over-the-air (HOTA) PFL with a dynamic weighting strategy which we
call HOTA-FedGradNorm. Our algorithm considers the channel conditions during
the dynamic weight selection process. We conduct experiments on a wireless
communication system dataset (RadComDynamic). The experimental results
demonstrate that the training speed with HOTA-FedGradNorm is faster compared to
the algorithms with a naive static equal weighting strategy. In addition,
HOTA-FedGradNorm provides robustness against the negative channel effects by
compensating for the channel conditions during the dynamic weight selection
process.","['Cemil Vahapoglu', 'Matin Mortaheb', 'Sennur Ulukus']","['cs.LG', 'cs.IT', 'eess.SP', 'math.IT', 'stat.ML']",2022-12-14 18:54:46+00:00
http://arxiv.org/abs/2212.07383v3,Sequential Kernelized Independence Testing,"Independence testing is a classical statistical problem that has been
extensively studied in the batch setting when one fixes the sample size before
collecting data. However, practitioners often prefer procedures that adapt to
the complexity of a problem at hand instead of setting sample size in advance.
Ideally, such procedures should (a) stop earlier on easy tasks (and later on
harder tasks), hence making better use of available resources, and (b)
continuously monitor the data and efficiently incorporate statistical evidence
after collecting new data, while controlling the false alarm rate. Classical
batch tests are not tailored for streaming data: valid inference after data
peeking requires correcting for multiple testing which results in low power.
Following the principle of testing by betting, we design sequential kernelized
independence tests that overcome such shortcomings. We exemplify our broad
framework using bets inspired by kernelized dependence measures, e.g., the
Hilbert-Schmidt independence criterion. Our test is also valid under
non-i.i.d., time-varying settings. We demonstrate the power of our approaches
on both simulated and real data.","['Aleksandr Podkopaev', 'Patrick Bl√∂baum', 'Shiva Prasad Kasiviswanathan', 'Aaditya Ramdas']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2022-12-14 18:08:42+00:00
http://arxiv.org/abs/2212.07311v2,Bayesian data fusion with shared priors,"The integration of data and knowledge from several sources is known as data
fusion. When data is only available in a distributed fashion or when different
sensors are used to infer a quantity of interest, data fusion becomes
essential. In Bayesian settings, a priori information of the unknown quantities
is available and, possibly, present among the different distributed estimators.
When the local estimates are fused, the prior knowledge used to construct
several local posteriors might be overused unless the fusion node accounts for
this and corrects it. In this paper, we analyze the effects of shared priors in
Bayesian data fusion contexts. Depending on different common fusion rules, our
analysis helps to understand the performance behavior as a function of the
number of collaborative agents and as a consequence of different types of
priors. The analysis is performed by using two divergences which are common in
Bayesian inference, and the generality of the results allows to analyze very
generic distributions. These theoretical results are corroborated through
experiments in a variety of estimation and classification problems, including
linear and nonlinear models, and federated learning schemes.","['Peng Wu', 'Tales Imbiriba', 'Victor Elvira', 'Pau Closas']","['cs.LG', 'stat.ML']",2022-12-14 16:16:47+00:00
http://arxiv.org/abs/2212.07295v2,Maximal Initial Learning Rates in Deep ReLU Networks,"Training a neural network requires choosing a suitable learning rate, which
involves a trade-off between speed and effectiveness of convergence. While
there has been considerable theoretical and empirical analysis of how large the
learning rate can be, most prior work focuses only on late-stage training. In
this work, we introduce the maximal initial learning rate $\eta^{\ast}$ - the
largest learning rate at which a randomly initialized neural network can
successfully begin training and achieve (at least) a given threshold accuracy.
Using a simple approach to estimate $\eta^{\ast}$, we observe that in
constant-width fully-connected ReLU networks, $\eta^{\ast}$ behaves differently
from the maximum learning rate later in training. Specifically, we find that
$\eta^{\ast}$ is well predicted as a power of depth $\times$ width, provided
that (i) the width of the network is sufficiently large compared to the depth,
and (ii) the input layer is trained at a relatively small learning rate. We
further analyze the relationship between $\eta^{\ast}$ and the sharpness
$\lambda_{1}$ of the network at initialization, indicating they are closely
though not inversely related. We formally prove bounds for $\lambda_{1}$ in
terms of depth $\times$ width that align with our empirical results.","['Gaurav Iyer', 'Boris Hanin', 'David Rolnick']","['stat.ML', 'cs.LG']",2022-12-14 15:58:37+00:00
http://arxiv.org/abs/2212.07201v2,Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice Reduction,"The circular coordinates algorithm of de Silva, Morozov, and
Vejdemo-Johansson takes as input a dataset together with a cohomology class
representing a $1$-dimensional hole in the data; the output is a map from the
data into the circle that captures this hole, and that is of minimum energy in
a suitable sense. However, when applied to several cohomology classes, the
output circle-valued maps can be ""geometrically correlated"" even if the chosen
cohomology classes are linearly independent. It is shown in the original work
that less correlated maps can be obtained with suitable integer linear
combinations of the cohomology classes, with the linear combinations being
chosen by inspection. In this paper, we identify a formal notion of geometric
correlation between circle-valued maps which, in the Riemannian manifold case,
corresponds to the Dirichlet form, a bilinear form derived from the Dirichlet
energy. We describe a systematic procedure for constructing low energy
torus-valued maps on data, starting from a set of linearly independent
cohomology classes. We showcase our procedure with computational examples. Our
main algorithm is based on the Lenstra--Lenstra--Lov\'asz algorithm from
computational number theory.","['Luis Scoccola', 'Hitesh Gakhar', 'Johnathan Bush', 'Nikolas Schonsheck', 'Tatum Rask', 'Ling Zhou', 'Jose A. Perea']","['cs.CG', 'cs.LG', 'math.AT', 'stat.ML']",2022-12-14 12:59:25+00:00
http://arxiv.org/abs/2212.07052v2,On LASSO for High Dimensional Predictive Regression,"This paper examines LASSO, a widely-used $L_{1}$-penalized regression method,
in high dimensional linear predictive regressions, particularly when the number
of potential predictors exceeds the sample size and numerous unit root
regressors are present. The consistency of LASSO is contingent upon two key
components: the deviation bound of the cross product of the regressors and the
error term, and the restricted eigenvalue of the Gram matrix. We present new
probabilistic bounds for these components, suggesting that LASSO's rates of
convergence are different from those typically observed in cross-sectional
cases. When applied to a mixture of stationary, nonstationary, and cointegrated
predictors, LASSO maintains its asymptotic guarantee if predictors are
scale-standardized. Leveraging machine learning and macroeconomic domain
expertise, LASSO demonstrates strong performance in forecasting the
unemployment rate, as evidenced by its application to the FRED-MD database.","['Ziwei Mei', 'Zhentao Shi']","['econ.EM', 'stat.ML']",2022-12-14 06:14:58+00:00
http://arxiv.org/abs/2212.06925v4,On the Relationship Between Explanation and Prediction: A Causal View,"Being able to provide explanations for a model's decision has become a
central requirement for the development, deployment, and adoption of machine
learning models. However, we are yet to understand what explanation methods can
and cannot do. How do upstream factors such as data, model prediction,
hyperparameters, and random initialization influence downstream explanations?
While previous work raised concerns that explanations (E) may have little
relationship with the prediction (Y), there is a lack of conclusive study to
quantify this relationship. Our work borrows tools from causal inference to
systematically assay this relationship. More specifically, we study the
relationship between E and Y by measuring the treatment effect when intervening
on their causal ancestors, i.e., on hyperparameters and inputs used to generate
saliency-based Es or Ys. Our results suggest that the relationships between E
and Y is far from ideal. In fact, the gap between 'ideal' case only increase in
higher-performing models -- models that are likely to be deployed. Our work is
a promising first step towards providing a quantitative measure of the
relationship between E and Y, which could also inform the future development of
methods for E with a quantitative metric.","['Amir-Hossein Karimi', 'Krikamol Muandet', 'Simon Kornblith', 'Bernhard Sch√∂lkopf', 'Been Kim']","['cs.LG', 'stat.ME', 'stat.ML']",2022-12-13 22:42:00+00:00
http://arxiv.org/abs/2212.06803v1,Fair Infinitesimal Jackknife: Mitigating the Influence of Biased Training Data Points Without Refitting,"In consequential decision-making applications, mitigating unwanted biases in
machine learning models that yield systematic disadvantage to members of groups
delineated by sensitive attributes such as race and gender is one key
intervention to strive for equity. Focusing on demographic parity and equality
of opportunity, in this paper we propose an algorithm that improves the
fairness of a pre-trained classifier by simply dropping carefully selected
training data points. We select instances based on their influence on the
fairness metric of interest, computed using an infinitesimal jackknife-based
approach. The dropping of training points is done in principle, but in practice
does not require the model to be refit. Crucially, we find that such an
intervention does not substantially reduce the predictive performance of the
model but drastically improves the fairness metric. Through careful
experiments, we evaluate the effectiveness of the proposed approach on diverse
tasks and find that it consistently improves upon existing alternatives.","['Prasanna Sattigeri', 'Soumya Ghosh', 'Inkit Padhi', 'Pierre Dognin', 'Kush R. Varshney']","['cs.LG', 'cs.CY', 'stat.ML']",2022-12-13 18:36:19+00:00
http://arxiv.org/abs/2212.06757v2,Gradient flow in the gaussian covariate model: exact solution of learning curves and multiple descent structures,"A recent line of work has shown remarkable behaviors of the generalization
error curves in simple learning models. Even the least-squares regression has
shown atypical features such as the model-wise double descent, and further
works have observed triple or multiple descents. Another important
characteristic are the epoch-wise descent structures which emerge during
training. The observations of model-wise and epoch-wise descents have been
analytically derived in limited theoretical settings (such as the random
feature model) and are otherwise experimental. In this work, we provide a full
and unified analysis of the whole time-evolution of the generalization curve,
in the asymptotic large-dimensional regime and under gradient-flow, within a
wider theoretical setting stemming from a gaussian covariate model. In
particular, we cover most cases already disparately observed in the literature,
and also provide examples of the existence of multiple descent structures as a
function of a model parameter or time. Furthermore, we show that our
theoretical predictions adequately match the learning curves obtained by
gradient descent over realistic datasets. Technically we compute averages of
rational expressions involving random matrices using recent developments in
random matrix theory based on ""linear pencils"". Another contribution, which is
also of independent interest in random matrix theory, is a new derivation of
related fixed point equations (and an extension there-off) using Dyson brownian
motions.","['Antoine Bodin', 'Nicolas Macris']","['stat.ML', 'cs.LG']",2022-12-13 17:39:18+00:00
http://arxiv.org/abs/2212.06504v1,Accelerated structured matrix factorization,"Matrix factorization exploits the idea that, in complex high-dimensional
data, the actual signal typically lies in lower-dimensional structures. These
lower dimensional objects provide useful insight, with interpretability favored
by sparse structures. Sparsity, in addition, is beneficial in terms of
regularization and, thus, to avoid over-fitting. By exploiting Bayesian
shrinkage priors, we devise a computationally convenient approach for
high-dimensional matrix factorization. The dependence between row and column
entities is modeled by inducing flexible sparse patterns within factors. The
availability of external information is accounted for in such a way that
structures are allowed while not imposed. Inspired by boosting algorithms, we
pair the the proposed approach with a numerical strategy relying on a
sequential inclusion and estimation of low-rank contributions, with data-driven
stopping rule. Practical advantages of the proposed approach are demonstrated
by means of a simulation study and the analysis of soccer heatmaps obtained
from new generation tracking data.","['Lorenzo Schiavon', 'Bernardo Nipoti', 'Antonio Canale']","['stat.ME', 'stat.ML']",2022-12-13 11:35:01+00:00
http://arxiv.org/abs/2212.06470v3,Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining,"The performance of differentially private machine learning can be boosted
significantly by leveraging the transfer learning capabilities of non-private
models pretrained on large public datasets. We critically review this approach.
  We primarily question whether the use of large Web-scraped datasets should be
viewed as differential-privacy-preserving. We caution that publicizing these
models pretrained on Web data as ""private"" could lead to harm and erode the
public's trust in differential privacy as a meaningful definition of privacy.
  Beyond the privacy considerations of using public data, we further question
the utility of this paradigm. We scrutinize whether existing machine learning
benchmarks are appropriate for measuring the ability of pretrained models to
generalize to sensitive domains, which may be poorly represented in public Web
data. Finally, we notice that pretraining has been especially impactful for the
largest available models -- models sufficiently large to prohibit end users
running them on their own devices. Thus, deploying such models today could be a
net loss for privacy, as it would require (private) data to be outsourced to a
more compute-powerful third party.
  We conclude by discussing potential paths forward for the field of private
learning, as public pretraining becomes more popular and powerful.","['Florian Tram√®r', 'Gautam Kamath', 'Nicholas Carlini']","['cs.LG', 'cs.CR', 'stat.ML']",2022-12-13 10:41:12+00:00
http://arxiv.org/abs/2212.06461v2,A Statistical Model for Predicting Generalization in Few-Shot Classification,"The estimation of the generalization error of classifiers often relies on a
validation set. Such a set is hardly available in few-shot learning scenarios,
a highly disregarded shortcoming in the field. In these scenarios, it is common
to rely on features extracted from pre-trained neural networks combined with
distance-based classifiers such as nearest class mean. In this work, we
introduce a Gaussian model of the feature distribution. By estimating the
parameters of this model, we are able to predict the generalization error on
new classification tasks with few samples. We observe that accurate distance
estimates between class-conditional densities are the key to accurate estimates
of the generalization performance. Therefore, we propose an unbiased estimator
for these distances and integrate it in our numerical analysis. We empirically
show that our approach outperforms alternatives such as the leave-one-out
cross-validation strategy.","['Yassir Bendou', 'Vincent Gripon', 'Bastien Pasdeloup', 'Lukas Mauch', 'Stefan Uhlich', 'Fabien Cardinaux', 'Ghouthi Boukli Hacene', 'Javier Alonso Garcia']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2022-12-13 10:21:15+00:00
http://arxiv.org/abs/2212.06370v4,Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation,"Accurate uncertainty quantification is necessary to enhance the reliability
of deep learning models in real-world applications. In the case of regression
tasks, prediction intervals (PIs) should be provided along with the
deterministic predictions of deep learning models. Such PIs are useful or
""high-quality"" as long as they are sufficiently narrow and capture most of the
probability density. In this paper, we present a method to learn prediction
intervals for regression-based neural networks automatically in addition to the
conventional target predictions. In particular, we train two companion neural
networks: one that uses one output, the target estimate, and another that uses
two outputs, the upper and lower bounds of the corresponding PI. Our main
contribution is the design of a novel loss function for the PI-generation
network that takes into account the output of the target-estimation network and
has two optimization objectives: minimizing the mean prediction interval width
and ensuring the PI integrity using constraints that maximize the prediction
interval probability coverage implicitly. Furthermore, we introduce a
self-adaptive coefficient that balances both objectives within the loss
function, which alleviates the task of fine-tuning. Experiments using a
synthetic dataset, eight benchmark datasets, and a real-world crop yield
prediction dataset showed that our method was able to maintain a nominal
probability coverage and produce significantly narrower PIs without detriment
to its target estimation accuracy when compared to those PIs generated by three
state-of-the-art neural-network-based methods. In other words, our method was
shown to produce higher-quality PIs.","['Giorgio Morales', 'John W. Sheppard']","['cs.LG', 'stat.ML']",2022-12-13 05:03:16+00:00
http://arxiv.org/abs/2212.06355v1,A Review of Off-Policy Evaluation in Reinforcement Learning,"Reinforcement learning (RL) is one of the most vibrant research frontiers in
machine learning and has been recently applied to solve a number of challenging
problems. In this paper, we primarily focus on off-policy evaluation (OPE), one
of the most fundamental topics in RL. In recent years, a number of OPE methods
have been developed in the statistics and computer science literature. We
provide a discussion on the efficiency bound of OPE, some of the existing
state-of-the-art OPE methods, their statistical properties and some other
related research directions that are currently actively explored.","['Masatoshi Uehara', 'Chengchun Shi', 'Nathan Kallus']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2022-12-13 03:38:57+00:00
http://arxiv.org/abs/2212.06339v1,Regularized Optimal Transport Layers for Generalized Global Pooling Operations,"Global pooling is one of the most significant operations in many machine
learning models and tasks, which works for information fusion and structured
data (like sets and graphs) representation. However, without solid mathematical
fundamentals, its practical implementations often depend on empirical
mechanisms and thus lead to sub-optimal, even unsatisfactory performance. In
this work, we develop a novel and generalized global pooling framework through
the lens of optimal transport. The proposed framework is interpretable from the
perspective of expectation-maximization. Essentially, it aims at learning an
optimal transport across sample indices and feature dimensions, making the
corresponding pooling operation maximize the conditional expectation of input
data. We demonstrate that most existing pooling methods are equivalent to
solving a regularized optimal transport (ROT) problem with different
specializations, and more sophisticated pooling operations can be implemented
by hierarchically solving multiple ROT problems. Making the parameters of the
ROT problem learnable, we develop a family of regularized optimal transport
pooling (ROTP) layers. We implement the ROTP layers as a new kind of deep
implicit layer. Their model architectures correspond to different optimization
algorithms. We test our ROTP layers in several representative set-level machine
learning scenarios, including multi-instance learning (MIL), graph
classification, graph set representation, and image classification.
Experimental results show that applying our ROTP layers can reduce the
difficulty of the design and selection of global pooling -- our ROTP layers may
either imitate some existing global pooling methods or lead to some new pooling
layers fitting data better. The code is available at
\url{https://github.com/SDS-Lab/ROT-Pooling}.","['Hongteng Xu', 'Minjie Cheng']","['cs.LG', 'cs.CV', 'stat.ML']",2022-12-13 02:46:36+00:00
http://arxiv.org/abs/2212.06338v2,Minimax Optimal Estimation of Stability Under Distribution Shift,"The performance of decision policies and prediction models often deteriorates
when applied to environments different from the ones seen during training. To
ensure reliable operation, we analyze the stability of a system under
distribution shift, which is defined as the smallest change in the underlying
environment that causes the system's performance to deteriorate beyond a
permissible threshold. In contrast to standard tail risk measures and
distributionally robust losses that require the specification of a plausible
magnitude of distribution shift, the stability measure is defined in terms of a
more intuitive quantity: the level of acceptable performance degradation. We
develop a minimax optimal estimator of stability and analyze its convergence
rate, which exhibits a fundamental phase shift behavior. Our characterization
of the minimax convergence rate shows that evaluating stability against large
performance degradation incurs a statistical cost. Empirically, we demonstrate
the practical utility of our stability framework by using it to compare system
designs on problems where robustness to distribution shift is critical.","['Hongseok Namkoong', 'Yuanzhe Ma', 'Peter W. Glynn']","['stat.ML', 'cs.LG']",2022-12-13 02:40:30+00:00
http://arxiv.org/abs/2212.06327v1,Nonparametric Independent Component Analysis for the Sources with Mixed Spectra,"Independent component analysis (ICA) is a blind source separation method to
recover source signals of interest from their mixtures. Most existing ICA
procedures assume independent sampling. Second-order-statistics-based source
separation methods have been developed based on parametric time series models
for the mixtures from the autocorrelated sources. However, the
second-order-statistics-based methods cannot separate the sources accurately
when the sources have temporal autocorrelations with mixed spectra. To address
this issue, we propose a new ICA method by estimating spectral density
functions and line spectra of the source signals using cubic splines and
indicator functions, respectively. The mixed spectra and the mixing matrix are
estimated by maximizing the Whittle likelihood function. We illustrate the
performance of the proposed method through simulation experiments and an EEG
data application. The numerical results indicate that our approach outperforms
existing ICA methods, including SOBI algorithms. In addition, we investigate
the asymptotic behavior of the proposed method.","['Seonjoo Lee', 'Haipeng Shen', 'Young K. Truong']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2022-12-13 02:13:14+00:00
http://arxiv.org/abs/2212.06319v2,Linear Convergence of ISTA and FISTA,"In this paper, we revisit the class of iterative shrinkage-thresholding
algorithms (ISTA) for solving the linear inverse problem with sparse
representation, which arises in signal and image processing. It is shown in the
numerical experiment to deblur an image that the convergence behavior in the
logarithmic-scale ordinate tends to be linear instead of logarithmic,
approximating to be flat. Making meticulous observations, we find that the
previous assumption for the smooth part to be convex weakens the least-square
model. Specifically, assuming the smooth part to be strongly convex is more
reasonable for the least-square model, even though the image matrix is probably
ill-conditioned. Furthermore, we improve the pivotal inequality tighter for
composite optimization with the smooth part to be strongly convex instead of
general convex, which is first found in [Li et al., 2022]. Based on this
pivotal inequality, we generalize the linear convergence to composite
optimization in both the objective value and the squared proximal subgradient
norm. Meanwhile, we set a simple ill-conditioned matrix which is easy to
compute the singular values instead of the original blur matrix. The new
numerical experiment shows the proximal generalization of Nesterov's
accelerated gradient descent (NAG) for the strongly convex function has a
faster linear convergence rate than ISTA. Based on the tighter pivotal
inequality, we also generalize the faster linear convergence rate to composite
optimization, in both the objective value and the squared proximal subgradient
norm, by taking advantage of the well-constructed Lyapunov function with a
slight modification and the phase-space representation based on the
high-resolution differential equation framework from the implicit-velocity
scheme.","['Bowen Li', 'Bin Shi', 'Ya-xiang Yuan']","['math.OC', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2022-12-13 02:02:50+00:00
http://arxiv.org/abs/2212.06303v1,MAntRA: A framework for model agnostic reliability analysis,"We propose a novel model agnostic data-driven reliability analysis framework
for time-dependent reliability analysis. The proposed approach -- referred to
as MAntRA -- combines interpretable machine learning, Bayesian statistics, and
identifying stochastic dynamic equation to evaluate reliability of
stochastically-excited dynamical systems for which the governing physics is
\textit{apriori} unknown. A two-stage approach is adopted: in the first stage,
an efficient variational Bayesian equation discovery algorithm is developed to
determine the governing physics of an underlying stochastic differential
equation (SDE) from measured output data. The developed algorithm is efficient
and accounts for epistemic uncertainty due to limited and noisy data, and
aleatoric uncertainty because of environmental effect and external excitation.
In the second stage, the discovered SDE is solved using a stochastic
integration scheme and the probability failure is computed. The efficacy of the
proposed approach is illustrated on three numerical examples. The results
obtained indicate the possible application of the proposed approach for
reliability analysis of in-situ and heritage structures from on-site
measurements.","['Yogesh Chandrakant Mathpati', 'Kalpesh Sanjay More', 'Tapas Tripura', 'Rajdip Nayek', 'Souvik Chakraborty']","['stat.ME', 'cs.LG', 'stat.ML']",2022-12-13 00:57:09+00:00
http://arxiv.org/abs/2212.06279v1,Decentralized Stochastic Multi-Player Multi-Armed Walking Bandits,"Multi-player multi-armed bandit is an increasingly relevant decision-making
problem, motivated by applications to cognitive radio systems. Most research
for this problem focuses exclusively on the settings that players have
\textit{full access} to all arms and receive no reward when pulling the same
arm. Hence all players solve the same bandit problem with the goal of
maximizing their cumulative reward. However, these settings neglect several
important factors in many real-world applications, where players have
\textit{limited access} to \textit{a dynamic local subset of arms} (i.e., an
arm could sometimes be ``walking'' and not accessible to the player). To this
end, this paper proposes a \textit{multi-player multi-armed walking bandits}
model, aiming to address aforementioned modeling issues. The goal now is to
maximize the reward, however, players can only pull arms from the local subset
and only collect a full reward if no other players pull the same arm. We adopt
Upper Confidence Bound (UCB) to deal with the exploration-exploitation tradeoff
and employ distributed optimization techniques to properly handle collisions.
By carefully integrating these two techniques, we propose a decentralized
algorithm with near-optimal guarantee on the regret, and can be easily
implemented to obtain competitive empirical performance.","['Guojun Xiong', 'Jian Li']","['cs.LG', 'cs.AI', 'cs.MA', 'stat.ML']",2022-12-12 23:26:02+00:00
http://arxiv.org/abs/2212.06251v2,Autoregressive Bandits,"Autoregressive processes naturally arise in a large variety of real-world
scenarios, including stock markets, sales forecasting, weather prediction,
advertising, and pricing. When facing a sequential decision-making problem in
such a context, the temporal dependence between consecutive observations should
be properly accounted for guaranteeing convergence to the optimal policy. In
this work, we propose a novel online learning setting, namely, Autoregressive
Bandits (ARBs), in which the observed reward is governed by an autoregressive
process of order $k$, whose parameters depend on the chosen action. We show
that, under mild assumptions on the reward process, the optimal policy can be
conveniently computed. Then, we devise a new optimistic regret minimization
algorithm, namely, AutoRegressive Upper Confidence Bound (AR-UCB), that suffers
sublinear regret of order $\widetilde{\mathcal{O}} \left(
\frac{(k+1)^{3/2}\sqrt{nT}}{(1-\Gamma)^2}\right)$, where $T$ is the
optimization horizon, $n$ is the number of actions, and $\Gamma < 1$ is a
stability index of the process. Finally, we empirically validate our algorithm,
illustrating its advantages w.r.t. bandit baselines and its robustness to
misspecification of key parameters.","['Francesco Bacchiocchi', 'Gianmarco Genalti', 'Davide Maran', 'Marco Mussi', 'Marcello Restelli', 'Nicola Gatti', 'Alberto Maria Metelli']","['cs.LG', 'stat.ML']",2022-12-12 21:37:36+00:00
http://arxiv.org/abs/2212.06132v3,Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision Processes,"We study reinforcement learning (RL) with linear function approximation. For
episodic time-inhomogeneous linear Markov decision processes (linear MDPs)
whose transition probability can be parameterized as a linear function of a
given feature mapping, we propose the first computationally efficient algorithm
that achieves the nearly minimax optimal regret $\tilde O(d\sqrt{H^3K})$, where
$d$ is the dimension of the feature mapping, $H$ is the planning horizon, and
$K$ is the number of episodes. Our algorithm is based on a weighted linear
regression scheme with a carefully designed weight, which depends on a new
variance estimator that (1) directly estimates the variance of the optimal
value function, (2) monotonically decreases with respect to the number of
episodes to ensure a better estimation accuracy, and (3) uses a rare-switching
policy to update the value function estimator to control the complexity of the
estimated value function class. Our work provides a complete answer to optimal
RL with linear MDPs, and the developed algorithm and theoretical tools may be
of independent interest.","['Jiafan He', 'Heyang Zhao', 'Dongruo Zhou', 'Quanquan Gu']","['cs.LG', 'math.OC', 'stat.ML']",2022-12-12 18:58:59+00:00
http://arxiv.org/abs/2212.06069v1,VO$Q$L: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation,"We study time-inhomogeneous episodic reinforcement learning (RL) under
general function approximation and sparse rewards. We design a new algorithm,
Variance-weighted Optimistic $Q$-Learning (VO$Q$L), based on $Q$-learning and
bound its regret assuming completeness and bounded Eluder dimension for the
regression function class. As a special case, VO$Q$L achieves
$\tilde{O}(d\sqrt{HT}+d^6H^{5})$ regret over $T$ episodes for a horizon $H$ MDP
under ($d$-dimensional) linear function approximation, which is asymptotically
optimal. Our algorithm incorporates weighted regression-based upper and lower
bounds on the optimal value function to obtain this improved regret. The
algorithm is computationally efficient given a regression oracle over the
function class, making this the first computationally tractable and
statistically optimal approach for linear MDPs.","['Alekh Agarwal', 'Yujia Jin', 'Tong Zhang']","['cs.LG', 'stat.ML']",2022-12-12 17:37:00+00:00
http://arxiv.org/abs/2212.05949v4,Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear Contextual Bandits and Markov Decision Processes,"Despite the significant interest and progress in reinforcement learning (RL)
problems with adversarial corruption, current works are either confined to the
linear setting or lead to an undesired $\tilde{O}(\sqrt{T}\zeta)$ regret bound,
where $T$ is the number of rounds and $\zeta$ is the total amount of
corruption. In this paper, we consider the contextual bandit with general
function approximation and propose a computationally efficient algorithm to
achieve a regret of $\tilde{O}(\sqrt{T}+\zeta)$. The proposed algorithm relies
on the recently developed uncertainty-weighted least-squares regression from
linear contextual bandit and a new weighted estimator of uncertainty for the
general function class. In contrast to the existing analysis that heavily
relies on the linear structure, we develop a novel technique to control the sum
of weighted uncertainty, thus establishing the final regret bounds. We then
generalize our algorithm to the episodic MDP setting and first achieve an
additive dependence on the corruption level $\zeta$ in the scenario of general
function approximation. Notably, our algorithms achieve regret bounds either
nearly match the performance lower bound or improve the existing methods for
all the corruption levels and in both known and unknown $\zeta$ cases.","['Chenlu Ye', 'Wei Xiong', 'Quanquan Gu', 'Tong Zhang']","['stat.ML', 'cs.LG']",2022-12-12 15:04:56+00:00
http://arxiv.org/abs/2212.05866v3,Measuring the Driving Forces of Predictive Performance: Application to Credit Scoring,"In credit scoring, machine learning models are known to outperform standard
parametric models. As they condition access to credit, banking supervisors and
internal model validation teams need to monitor their predictive performance
and to identify the features with the highest impact on performance. To
facilitate this, we introduce the XPER methodology to decompose a performance
metric (e.g., AUC, $R^2$) into specific contributions associated with the
various features of a classification or regression model. XPER is theoretically
grounded on Shapley values and is both model-agnostic and performance
metric-agnostic. Furthermore, it can be implemented either at the model level
or at the individual level. Using a novel dataset of car loans, we decompose
the AUC of a machine-learning model trained to forecast the default probability
of loan applicants. We show that a small number of features can explain a
surprisingly large part of the model performance. Furthermore, we find that the
features that contribute the most to the predictive performance of the model
may not be the ones that contribute the most to individual forecasts (SHAP). We
also show how XPER can be used to deal with heterogeneity issues and
significantly boost out-of-sample performance.","['Hu√© Sullivan', 'Hurlin Christophe', 'P√©rignon Christophe', 'Saurin S√©bastien']","['stat.ML', 'cs.LG', 'econ.EM', 'stat.ME']",2022-12-12 13:09:46+00:00
http://arxiv.org/abs/2212.05836v1,Acceptance Rates of Invertible Neural Networks on Electron Spectra from Near-Critical Laser-Plasmas: A Comparison,"While the interaction of ultra-intense ultra-short laser pulses with near-
and overcritical plasmas cannot be directly observed, experimentally accessible
quantities (observables) often only indirectly give information about the
underlying plasma dynamics. Furthermore, the information provided by
observables is incomplete, making the inverse problem highly ambiguous.
Therefore, in order to infer plasma dynamics as well as experimental parameter,
the full distribution over parameters given an observation needs to considered,
requiring that models are flexible and account for the information lost in the
forward process. Invertible Neural Networks (INNs) have been designed to
efficiently model both the forward and inverse process, providing the full
conditional posterior given a specific measurement. In this work, we benchmark
INNs and standard statistical methods on synthetic electron spectra. First, we
provide experimental results with respect to the acceptance rate, where our
results show increases in acceptance rates up to a factor of 10. Additionally,
we show that this increased acceptance rate also results in an increased
speed-up for INNs to the same extent. Lastly, we propose a composite algorithm
that utilizes INNs and promises low runtimes while preserving high accuracy.","['Thomas Miethlinger', 'Nico Hoffmann', 'Thomas Kluge']","['physics.plasm-ph', 'physics.comp-ph', 'stat.ML']",2022-12-12 11:44:14+00:00
http://arxiv.org/abs/2212.05814v2,GWRBoost:A geographically weighted gradient boosting method for explainable quantification of spatially-varying relationships,"The geographically weighted regression (GWR) is an essential tool for
estimating the spatial variation of relationships between dependent and
independent variables in geographical contexts. However, GWR suffers from the
problem that classical linear regressions, which compose the GWR model, are
more prone to be underfitting, especially for significant volume and complex
nonlinear data, causing inferior comparative performance. Nevertheless, some
advanced models, such as the decision tree and the support vector machine, can
learn features from complex data more effectively while they cannot provide
explainable quantification for the spatial variation of localized
relationships. To address the above issues, we propose a geographically
gradient boosting weighted regression model, GWRBoost, that applies the
localized additive model and gradient boosting optimization method to alleviate
underfitting problems and retains explainable quantification capability for
spatially-varying relationships between geographically located variables.
Furthermore, we formulate the computation method of the Akaike information
score for the proposed model to conduct the comparative analysis with the
classic GWR algorithm. Simulation experiments and the empirical case study are
applied to prove the efficient performance and practical value of GWRBoost. The
results show that our proposed model can reduce the RMSE by 18.3% in parameter
estimation accuracy and AICc by 67.3% in the goodness of fit.","['Han Wang', 'Zhou Huang', 'Ganmin Yin', 'Yi Bao', 'Xiao Zhou', 'Yong Gao']","['cs.LG', 'stat.ML']",2022-12-12 10:24:47+00:00
http://arxiv.org/abs/2212.05772v1,Multi-Dimensional Self Attention based Approach for Remaining Useful Life Estimation,"Remaining Useful Life (RUL) estimation plays a critical role in Prognostics
and Health Management (PHM). Traditional machine health maintenance systems are
often costly, requiring sufficient prior expertise, and are difficult to fit
into highly complex and changing industrial scenarios. With the widespread
deployment of sensors on industrial equipment, building the Industrial Internet
of Things (IIoT) to interconnect these devices has become an inexorable trend
in the development of the digital factory. Using the device's real-time
operational data collected by IIoT to get the estimated RUL through the RUL
prediction algorithm, the PHM system can develop proactive maintenance measures
for the device, thus, reducing maintenance costs and decreasing failure times
during operation. This paper carries out research into the remaining useful
life prediction model for multi-sensor devices in the IIoT scenario. We
investigated the mainstream RUL prediction models and summarized the basic
steps of RUL prediction modeling in this scenario. On this basis, a data-driven
approach for RUL estimation is proposed in this paper. It employs a Multi-Head
Attention Mechanism to fuse the multi-dimensional time-series data output from
multiple sensors, in which the attention on features is used to capture the
interactions between features and attention on sequences is used to learn the
weights of time steps. Then, the Long Short-Term Memory Network is applied to
learn the features of time series. We evaluate the proposed model on two
benchmark datasets (C-MAPSS and PHM08), and the results demonstrate that it
outperforms the state-of-art models. Moreover, through the interpretability of
the multi-head attention mechanism, the proposed model can provide a
preliminary explanation of engine degradation. Therefore, this approach is
promising for predictive maintenance in IIoT scenarios.","['Zhi Lai', 'Mengjuan Liu', 'Yunzhu Pan', 'Dajiang Chen']","['cs.LG', 'stat.ML']",2022-12-12 08:50:27+00:00
http://arxiv.org/abs/2212.05720v1,Tensor-based Sequential Learning via Hankel Matrix Representation for Next Item Recommendations,"Self-attentive transformer models have recently been shown to solve the next
item recommendation task very efficiently. The learned attention weights
capture sequential dynamics in user behavior and generalize well. Motivated by
the special structure of learned parameter space, we question if it is possible
to mimic it with an alternative and more lightweight approach. We develop a new
tensor factorization-based model that ingrains the structural knowledge about
sequential data within the learning process. We demonstrate how certain
properties of a self-attention network can be reproduced with our approach
based on special Hankel matrix representation. The resulting model has a
shallow linear architecture and compares competitively to its neural
counterpart.","['Evgeny Frolov', 'Ivan Oseledets']","['cs.LG', 'cs.AI', 'cs.IR', 'stat.ML']",2022-12-12 05:55:40+00:00
http://arxiv.org/abs/2212.05672v1,Hybrid Censored Quantile Regression Forest to Assess the Heterogeneous Effects,"In many applications, heterogeneous treatment effects on a censored response
variable are of primary interest, and it is natural to evaluate the effects at
different quantiles (e.g., median). The large number of potential effect
modifiers, the unknown structure of the treatment effects, and the presence of
right censoring pose significant challenges. In this paper, we develop a hybrid
forest approach called Hybrid Censored Quantile Regression Forest (HCQRF) to
assess the heterogeneous effects varying with high-dimensional variables. The
hybrid estimation approach takes advantage of the random forests and the
censored quantile regression. We propose a doubly-weighted estimation procedure
that consists of a redistribution-of-mass weight to handle censoring and an
adaptive nearest neighbor weight derived from the forest to handle
high-dimensional effect functions. We propose a variable importance
decomposition to measure the impact of a variable on the treatment effect
function. Extensive simulation studies demonstrate the efficacy and stability
of HCQRF. The result of the simulation study also convinces us of the
effectiveness of the variable importance decomposition. We apply HCQRF to a
clinical trial of colorectal cancer. We achieve insightful estimations of the
treatment effect and meaningful variable importance results. The result of the
variable importance also confirms the necessity of the decomposition.","['Huichen Zhu', 'Yifei Sun', 'Ying Wei']","['stat.ME', 'stat.ML']",2022-12-12 03:01:36+00:00
http://arxiv.org/abs/2212.05663v2,On an Interpretation of ResNets via Solution Constructions,"This paper first constructs a typical solution of ResNets for multi-category
classifications by the principle of gate-network controls and deep-layer
classifications, from which a general interpretation of the ResNet architecture
is given and the performance mechanism is explained. We then use more solutions
to further demonstrate the generality of that interpretation. The
universal-approximation capability of ResNets is proved.",['Changcun Huang'],"['cs.LG', 'cs.AI', 'stat.ML', '68T07', 'I.2.6']",2022-12-12 02:28:54+00:00
http://arxiv.org/abs/2212.05591v1,Random Feature Models for Learning Interacting Dynamical Systems,"Particle dynamics and multi-agent systems provide accurate dynamical models
for studying and forecasting the behavior of complex interacting systems. They
often take the form of a high-dimensional system of differential equations
parameterized by an interaction kernel that models the underlying attractive or
repulsive forces between agents. We consider the problem of constructing a
data-based approximation of the interacting forces directly from noisy
observations of the paths of the agents in time. The learned interaction
kernels are then used to predict the agents behavior over a longer time
interval. The approximation developed in this work uses a randomized feature
algorithm and a sparse randomized feature approach. Sparsity-promoting
regression provides a mechanism for pruning the randomly generated features
which was observed to be beneficial when one has limited data, in particular,
leading to less overfitting than other approaches. In addition, imposing
sparsity reduces the kernel evaluation cost which significantly lowers the
simulation cost for forecasting the multi-agent systems. Our method is applied
to various examples, including first-order systems with homogeneous and
heterogeneous interactions, second order homogeneous systems, and a new sheep
swarming system.","['Yuxuan Liu', 'Scott G. McCalla', 'Hayden Schaeffer']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2022-12-11 20:09:36+00:00
http://arxiv.org/abs/2212.05562v2,Retire: Robust Expectile Regression in High Dimensions,"High-dimensional data can often display heterogeneity due to heteroscedastic
variance or inhomogeneous covariate effects. Penalized quantile and expectile
regression methods offer useful tools to detect heteroscedasticity in
high-dimensional data. The former is computationally challenging due to the
non-smooth nature of the check loss, and the latter is sensitive to
heavy-tailed error distributions. In this paper, we propose and study
(penalized) robust expectile regression (retire), with a focus on iteratively
reweighted $\ell_1$-penalization which reduces the estimation bias from
$\ell_1$-penalization and leads to oracle properties. Theoretically, we
establish the statistical properties of the retire estimator under two regimes:
(i) low-dimensional regime in which $d \ll n$; (ii) high-dimensional regime in
which $s\ll n\ll d$ with $s$ denoting the number of significant predictors. In
the high-dimensional setting, we carefully characterize the solution path of
the iteratively reweighted $\ell_1$-penalized retire estimation, adapted from
the local linear approximation algorithm for folded-concave regularization.
Under a mild minimum signal strength condition, we show that after as many as
$\log(\log d)$ iterations the final iterate enjoys the oracle convergence rate.
At each iteration, the weighted $\ell_1$-penalized convex program can be
efficiently solved by a semismooth Newton coordinate descent algorithm.
Numerical studies demonstrate the competitive performance of the proposed
procedure compared with either non-robust or quantile regression based
alternatives.","['Rebeka Man', 'Kean Ming Tan', 'Zian Wang', 'Wen-Xin Zhou']","['stat.ME', 'stat.ML']",2022-12-11 18:03:12+00:00
http://arxiv.org/abs/2212.05517v1,SchNetPack 2.0: A neural network toolbox for atomistic machine learning,"SchNetPack is a versatile neural networks toolbox that addresses both the
requirements of method development and application of atomistic machine
learning. Version 2.0 comes with an improved data pipeline, modules for
equivariant neural networks as well as a PyTorch implementation of molecular
dynamics. An optional integration with PyTorch Lightning and the Hydra
configuration framework powers a flexible command-line interface. This makes
SchNetPack 2.0 easily extendable with custom code and ready for complex
training task such as generation of 3d molecular structures.","['Kristof T. Sch√ºtt', 'Stefaan S. P. Hessmann', 'Niklas W. A. Gebauer', 'Jonas Lederer', 'Michael Gastegger']","['physics.chem-ph', 'stat.ML']",2022-12-11 14:44:56+00:00
http://arxiv.org/abs/2212.05430v1,Corruption-tolerant Algorithms for Generalized Linear Models,"This paper presents SVAM (Sequential Variance-Altered MLE), a unified
framework for learning generalized linear models under adversarial label
corruption in training data. SVAM extends to tasks such as least squares
regression, logistic regression, and gamma regression, whereas many existing
works on learning with label corruptions focus only on least squares
regression. SVAM is based on a novel variance reduction technique that may be
of independent interest and works by iteratively solving weighted MLEs over
variance-altered versions of the GLM objective. SVAM offers provable model
recovery guarantees superior to the state-of-the-art for robust regression even
when a constant fraction of training labels are adversarially corrupted. SVAM
also empirically outperforms several existing problem-specific techniques for
robust regression and classification. Code for SVAM is available at
https://github.com/purushottamkar/svam/","['Bhaskar P Mukhoty', 'Debojyoti Dey', 'Purushottam Kar']","['cs.LG', 'stat.ML']",2022-12-11 07:08:02+00:00
http://arxiv.org/abs/2212.05427v1,Statistical guarantees for sparse deep learning,"Neural networks are becoming increasingly popular in applications, but our
mathematical understanding of their potential and limitations is still limited.
In this paper, we further this understanding by developing statistical
guarantees for sparse deep learning. In contrast to previous work, we consider
different types of sparsity, such as few active connections, few active nodes,
and other norm-based types of sparsity. Moreover, our theories cover important
aspects that previous theories have neglected, such as multiple outputs,
regularization, and l2-loss. The guarantees have a mild dependence on network
widths and depths, which means that they support the application of sparse but
wide and deep networks from a statistical perspective. Some of the concepts and
tools that we use in our derivations are uncommon in deep learning and, hence,
might be of additional interest.",['Johannes Lederer'],"['cs.LG', 'cs.AI', 'math.ST', 'stat.ML', 'stat.TH']",2022-12-11 06:45:45+00:00
http://arxiv.org/abs/2212.05378v1,Neural Continuous-Time Markov Models,"Continuous-time Markov chains are used to model stochastic systems where
transitions can occur at irregular times, e.g., birth-death processes, chemical
reaction networks, population dynamics, and gene regulatory networks. We
develop a method to learn a continuous-time Markov chain's transition rate
functions from fully observed time series. In contrast with existing methods,
our method allows for transition rates to depend nonlinearly on both state
variables and external covariates. The Gillespie algorithm is used to generate
trajectories of stochastic systems where propensity functions (reaction rates)
are known. Our method can be viewed as the inverse: given trajectories of a
stochastic reaction network, we generate estimates of the propensity functions.
While previous methods used linear or log-linear methods to link transition
rates to covariates, we use neural networks, increasing the capacity and
potential accuracy of learned models. In the chemical context, this enables the
method to learn propensity functions from non-mass-action kinetics. We test our
method with synthetic data generated from a variety of systems with known
transition rates. We show that our method learns these transition rates with
considerably more accuracy than log-linear methods, in terms of mean absolute
error between ground truth and predicted transition rates. We also demonstrate
an application of our methods to open-loop control of a continuous-time Markov
chain.","['Majerle Reeves', 'Harish S. Bhat']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY']",2022-12-11 00:07:41+00:00
http://arxiv.org/abs/2212.05149v1,Stochastic Optimization for Spectral Risk Measures,"Spectral risk objectives - also called $L$-risks - allow for learning systems
to interpolate between optimizing average-case performance (as in empirical
risk minimization) and worst-case performance on a task. We develop stochastic
algorithms to optimize these quantities by characterizing their subdifferential
and addressing challenges such as biasedness of subgradient estimates and
non-smoothness of the objective. We show theoretically and experimentally that
out-of-the-box approaches such as stochastic subgradient and dual averaging are
hindered by bias and that our approach outperforms them.","['Ronak Mehta', 'Vincent Roulet', 'Krishna Pillutla', 'Lang Liu', 'Zaid Harchaoui']","['stat.ML', 'cs.LG', 'math.OC']",2022-12-10 00:03:12+00:00
http://arxiv.org/abs/2212.05015v3,Robustness Implies Privacy in Statistical Estimation,"We study the relationship between adversarial robustness and differential
privacy in high-dimensional algorithmic statistics. We give the first black-box
reduction from privacy to robustness which can produce private estimators with
optimal tradeoffs among sample complexity, accuracy, and privacy for a wide
range of fundamental high-dimensional parameter estimation problems, including
mean and covariance estimation. We show that this reduction can be implemented
in polynomial time in some important special cases. In particular, using
nearly-optimal polynomial-time robust estimators for the mean and covariance of
high-dimensional Gaussians which are based on the Sum-of-Squares method, we
design the first polynomial-time private estimators for these problems with
nearly-optimal samples-accuracy-privacy tradeoffs. Our algorithms are also
robust to a nearly optimal fraction of adversarially-corrupted samples.","['Samuel B. Hopkins', 'Gautam Kamath', 'Mahbod Majid', 'Shyam Narayanan']","['cs.DS', 'cs.CR', 'cs.IT', 'math.IT', 'stat.ML']",2022-12-09 18:07:30+00:00
http://arxiv.org/abs/2212.04955v2,Simulating first-order phase transition with hierarchical autoregressive networks,"We apply the Hierarchical Autoregressive Neural (HAN) network sampling
algorithm to the two-dimensional $Q$-state Potts model and perform simulations
around the phase transition at $Q=12$. We quantify the performance of the
approach in the vicinity of the first-order phase transition and compare it
with that of the Wolff cluster algorithm. We find a significant improvement as
far as the statistical uncertainty is concerned at a similar numerical effort.
In order to efficiently train large neural networks we introduce the technique
of pre-training. It allows to train some neural networks using smaller system
sizes and then employing them as starting configurations for larger system
sizes. This is possible due to the recursive construction of our hierarchical
approach. Our results serve as a demonstration of the performance of the
hierarchical approach for systems exhibiting bimodal distributions.
Additionally, we provide estimates of the free energy and entropy in the
vicinity of the phase transition with statistical uncertainties of the order of
$10^{-7}$ for the former and $10^{-3}$ for the latter based on a statistics of
$10^6$ configurations.","['Piotr Bia≈Ças', 'Paulina Czarnota', 'Piotr Korcyl', 'Tomasz Stebel']","['cond-mat.stat-mech', 'cond-mat.dis-nn', 'hep-lat', 'stat.ML']",2022-12-09 16:04:56+00:00
