id,title,abstract,authors,categories,date
http://arxiv.org/abs/1803.06407v1,Deep Component Analysis via Alternating Direction Neural Networks,"Despite a lack of theoretical understanding, deep neural networks have
achieved unparalleled performance in a wide range of applications. On the other
hand, shallow representation learning with component analysis is associated
with rich intuition and theory, but smaller capacity often limits its
usefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA),
an expressive multilayer model formulation that enforces hierarchical structure
through constraints on latent variables in each layer. For inference, we
propose a differentiable optimization algorithm implemented using recurrent
Alternating Direction Neural Networks (ADNNs) that enable parameter learning
using standard backpropagation. By interpreting feed-forward networks as
single-iteration approximations of inference in our model, we provide both a
novel theoretical perspective for understanding them and a practical technique
for constraining predictions with prior knowledge. Experimentally, we
demonstrate performance improvements on a variety of tasks, including
single-image depth prediction with sparse output constraints.","['Calvin Murdock', 'Ming-Fang Chang', 'Simon Lucey']","['cs.LG', 'cs.CV', 'stat.ML']",2018-03-16 21:40:02+00:00
http://arxiv.org/abs/1803.06401v1,Evaluating Conditional Cash Transfer Policies with Machine Learning Methods,"This paper presents an out-of-sample prediction comparison between major
machine learning models and the structural econometric model. Over the past
decade, machine learning has established itself as a powerful tool in many
prediction applications, but this approach is still not widely adopted in
empirical economic studies. To evaluate the benefits of this approach, I use
the most common machine learning algorithms, CART, C4.5, LASSO, random forest,
and adaboost, to construct prediction models for a cash transfer experiment
conducted by the Progresa program in Mexico, and I compare the prediction
results with those of a previous structural econometric study. Two prediction
tasks are performed in this paper: the out-of-sample forecast and the long-term
within-sample simulation. For the out-of-sample forecast, both the mean
absolute error and the root mean square error of the school attendance rates
found by all machine learning models are smaller than those found by the
structural model. Random forest and adaboost have the highest accuracy for the
individual outcomes of all subgroups. For the long-term within-sample
simulation, the structural model has better performance than do all of the
machine learning models. The poor within-sample fitness of the machine learning
model results from the inaccuracy of the income and pregnancy prediction
models. The result shows that the machine learning model performs better than
does the structural model when there are many data to learn; however, when the
data are limited, the structural model offers a more sensible prediction. The
findings of this paper show promise for adopting machine learning in economic
policy analyses in the era of big data.",['Tzai-Shuen Chen'],"['econ.EM', 'stat.ML']",2018-03-16 21:14:02+00:00
http://arxiv.org/abs/1803.06396v4,Reviving and Improving Recurrent Back-Propagation,"In this paper, we revisit the recurrent back-propagation (RBP) algorithm,
discuss the conditions under which it applies as well as how to satisfy them in
deep neural networks. We show that RBP can be unstable and propose two variants
based on conjugate gradient on the normal equations (CG-RBP) and Neumann series
(Neumann-RBP). We further investigate the relationship between Neumann-RBP and
back propagation through time (BPTT) and its truncated version (TBPTT). Our
Neumann-RBP has the same time complexity as TBPTT but only requires constant
memory, whereas TBPTT's memory cost scales linearly with the number of
truncation steps. We examine all RBP variants along with BPTT and TBPTT in
three different application domains: associative memory with continuous
Hopfield networks, document classification in citation networks using graph
neural networks and hyperparameter optimization for fully connected networks.
All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are
efficient and effective for optimizing convergent recurrent neural networks.
Code is released at: \url{https://github.com/lrjconan/RBP}.","['Renjie Liao', 'Yuwen Xiong', 'Ethan Fetaya', 'Lisa Zhang', 'KiJung Yoon', 'Xaq Pitkow', 'Raquel Urtasun', 'Richard Zemel']","['cs.LG', 'stat.ML']",2018-03-16 20:57:36+00:00
http://arxiv.org/abs/1803.06386v1,Forecasting Economics and Financial Time Series: ARIMA vs. LSTM,"Forecasting time series data is an important subject in economics, business,
and finance. Traditionally, there are several techniques to effectively
forecast the next lag of time series data such as univariate Autoregressive
(AR), univariate Moving Average (MA), Simple Exponential Smoothing (SES), and
more notably Autoregressive Integrated Moving Average (ARIMA) with its many
variations. In particular, ARIMA model has demonstrated its outperformance in
precision and accuracy of predicting the next lags of time series. With the
recent advancement in computational power of computers and more importantly
developing more advanced machine learning algorithms and approaches such as
deep learning, new algorithms are developed to forecast time series data. The
research question investigated in this article is that whether and how the
newly developed deep learning-based algorithms for forecasting time series
data, such as ""Long Short-Term Memory (LSTM)"", are superior to the traditional
algorithms. The empirical studies conducted and reported in this article show
that deep learning-based algorithms such as LSTM outperform traditional-based
algorithms such as ARIMA model. More specifically, the average reduction in
error rates obtained by LSTM is between 84 - 87 percent when compared to ARIMA
indicating the superiority of LSTM to ARIMA. Furthermore, it was noticed that
the number of training times, known as ""epoch"" in deep learning, has no effect
on the performance of the trained forecast model and it exhibits a truly random
behavior.","['Sima Siami-Namini', 'Akbar Siami Namin']","['cs.LG', 'q-fin.ST', 'stat.ML']",2018-03-16 20:01:48+00:00
http://arxiv.org/abs/1803.06373v1,Adversarial Logit Pairing,"In this paper, we develop improved techniques for defending against
adversarial examples at scale. First, we implement the state of the art version
of adversarial training at unprecedented scale on ImageNet and investigate
whether it remains effective in this setting - an important open scientific
question (Athalye et al., 2018). Next, we introduce enhanced defenses using a
technique we call logit pairing, a method that encourages logits for pairs of
examples to be similar. When applied to clean examples and their adversarial
counterparts, logit pairing improves accuracy on adversarial examples over
vanilla adversarial training; we also find that logit pairing on clean examples
only is competitive with adversarial training in terms of accuracy on two
datasets. Finally, we show that adversarial logit pairing achieves the state of
the art defense on ImageNet against PGD white box attacks, with an accuracy
improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully
damages the current state of the art defense against black box attacks on
ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With
this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018)
for the state of the art on black box attacks on ImageNet.","['Harini Kannan', 'Alexey Kurakin', 'Ian Goodfellow']","['cs.LG', 'stat.ML']",2018-03-16 19:03:45+00:00
http://arxiv.org/abs/1803.07416v1,Tensor2Tensor for Neural Machine Translation,"Tensor2Tensor is a library for deep learning models that is well-suited for
neural machine translation and includes the reference implementation of the
state-of-the-art Transformer model.","['Ashish Vaswani', 'Samy Bengio', 'Eugene Brevdo', 'Francois Chollet', 'Aidan N. Gomez', 'Stephan Gouws', 'Llion Jones', '≈Åukasz Kaiser', 'Nal Kalchbrenner', 'Niki Parmar', 'Ryan Sepassi', 'Noam Shazeer', 'Jakob Uszkoreit']","['cs.LG', 'cs.CL', 'stat.ML']",2018-03-16 18:49:22+00:00
http://arxiv.org/abs/1803.06328v2,Nesting Probabilistic Programs,"We formalize the notion of nesting probabilistic programming queries and
investigate the resulting statistical implications. We demonstrate that while
query nesting allows the definition of models which could not otherwise be
expressed, such as those involving agents reasoning about other agents,
existing systems take approaches which lead to inconsistent estimates. We show
how to correct this by delineating possible ways one might want to nest queries
and asserting the respective conditions required for convergence. We further
introduce a new online nested Monte Carlo estimator that makes it substantially
easier to ensure these conditions are met, thereby providing a simple framework
for designing statistically correct inference engines. We prove the correctness
of this online estimator and show that, when using the recommended setup, its
asymptotic variance is always better than that of the equivalent fixed
estimator, while its bias is always within a factor of two.",['Tom Rainforth'],"['stat.ML', 'cs.PL', 'stat.CO']",2018-03-16 17:30:35+00:00
http://arxiv.org/abs/1803.06321v1,A particle-based variational approach to Bayesian Non-negative Matrix Factorization,"Bayesian Non-negative Matrix Factorization (NMF) is a promising approach for
understanding uncertainty and structure in matrix data. However, a large volume
of applied work optimizes traditional non-Bayesian NMF objectives that fail to
provide a principled understanding of the non-identifiability inherent in NMF--
an issue ideally addressed by a Bayesian approach. Despite their suitability,
current Bayesian NMF approaches have failed to gain popularity in an applied
setting; they sacrifice flexibility in modeling for tractable computation, tend
to get stuck in local modes, and require many thousands of samples for
meaningful uncertainty estimates. We address these issues through a
particle-based variational approach to Bayesian NMF that only requires the
joint likelihood to be differentiable for tractability, uses a novel
initialization technique to identify multiple modes in the posterior, and
allows domain experts to inspect a `small' set of factorizations that
faithfully represent the posterior. We introduce and employ a class of
likelihood and prior distributions for NMF that formulate a Bayesian model
using popular non-Bayesian NMF objectives. On several real datasets, we obtain
better particle approximations to the Bayesian NMF posterior in less time than
baselines and demonstrate the significant role that multimodality plays in
NMF-related tasks.","['M. Arjumand Masood', 'Finale Doshi-Velez']",['stat.ML'],2018-03-16 17:20:19+00:00
http://arxiv.org/abs/1803.06320v3,Synchronisation of Partial Multi-Matchings via Non-negative Factorisations,"In this work we study permutation synchronisation for the challenging case of
partial permutations, which plays an important role for the problem of matching
multiple objects (e.g. images or shapes). The term synchronisation refers to
the property that the set of pairwise matchings is cycle-consistent, i.e. in
the full matching case all compositions of pairwise matchings over cycles must
be equal to the identity. Motivated by clustering and matrix factorisation
perspectives of cycle-consistency, we derive an algorithm to tackle the
permutation synchronisation problem based on non-negative factorisations. In
order to deal with the inherent non-convexity of the permutation
synchronisation problem, we use an initialisation procedure based on a novel
rotation scheme applied to the solution of the spectral relaxation. Moreover,
this rotation scheme facilitates a convenient Euclidean projection to obtain a
binary solution after solving our relaxed problem. In contrast to
state-of-the-art methods, our approach is guaranteed to produce
cycle-consistent results. We experimentally demonstrate the efficacy of our
method and show that it achieves better results compared to existing methods.","['Florian Bernard', 'Johan Thunberg', 'Jorge Goncalves', 'Christian Theobalt']","['cs.CV', 'math.OC', 'stat.ML']",2018-03-16 17:17:05+00:00
http://arxiv.org/abs/1803.06272v1,Graph Partition Neural Networks for Semi-Supervised Classification,"We present graph partition neural networks (GPNN), an extension of graph
neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate
between locally propagating information between nodes in small subgraphs and
globally propagating information between the subgraphs. To efficiently
partition graphs, we experiment with several partitioning algorithms and also
propose a novel variant for fast processing of large scale graphs. We
extensively test our model on a variety of semi-supervised node classification
tasks. Experimental results indicate that GPNNs are either superior or
comparable to state-of-the-art methods on a wide variety of datasets for
graph-based semi-supervised classification. We also show that GPNNs can achieve
similar performance as standard GNNs with fewer propagation steps.","['Renjie Liao', 'Marc Brockschmidt', 'Daniel Tarlow', 'Alexander L. Gaunt', 'Raquel Urtasun', 'Richard Zemel']","['cs.LG', 'stat.ML']",2018-03-16 15:34:06+00:00
http://arxiv.org/abs/1803.06247v6,Coordinating users of shared facilities via data-driven predictive assistants and game theory,"We study data-driven assistants that provide congestion forecasts to users of
shared facilities (roads, cafeterias, etc.), to support coordination between
them, and increase efficiency of such collective systems. Key questions are:
(1) when and how much can (accurate) predictions help for coordination, and (2)
which assistant algorithms reach optimal predictions?
  First we lay conceptual ground for this setting where user preferences are a
priori unknown and predictions influence outcomes. Addressing (1), we establish
conditions under which self-fulfilling prophecies, i.e., ""perfect""
(probabilistic) predictions of what will happen, solve the coordination problem
in the game-theoretic sense of selecting a Bayesian Nash equilibrium (BNE).
Next we prove that such prophecies exist even in large-scale settings where
only aggregated statistics about users are available. This entails a new
(nonatomic) BNE existence result. Addressing (2), we propose two assistant
algorithms that sequentially learn from users' reactions, together with
optimality/convergence guarantees. We validate one of them in a large
real-world experiment.","['Philipp Geiger', 'Michel Besserve', 'Justus Winkelmann', 'Claudius Proissl', 'Bernhard Sch√∂lkopf']","['cs.GT', 'stat.ML']",2018-03-16 14:27:12+00:00
http://arxiv.org/abs/1803.06344v1,A Multi-Scheme Ensemble Using Coopetitive Soft-Gating With Application to Power Forecasting for Renewable Energy Generation,"In this article, we propose a novel ensemble technique with a multi-scheme
weighting based on a technique called coopetitive soft gating. This technique
combines both, ensemble member competition and cooperation, in order to
maximize the overall forecasting accuracy of the ensemble. The proposed
algorithm combines the ideas of multiple ensemble paradigms (power forecasting
model ensemble, weather forecasting model ensemble, and lagged ensemble) in a
hierarchical structure. The technique is designed to be used in a flexible
manner on single and multiple weather forecasting models, and for a variety of
lead times. We compare the technique to other power forecasting models and
ensemble techniques with a flexible number of weather forecasting models, which
can have the same, or varying forecasting horizons. It is shown that the model
is able to outperform those models on a number of publicly available data sets.
The article closes with a discussion of properties of the proposed model which
are relevant in its application.","['Andr√© Gensler', 'Bernhard Sick']","['stat.AP', 'stat.ML']",2018-03-16 14:23:37+00:00
http://arxiv.org/abs/1803.06118v4,Gaussian Processes indexed on the symmetric group: prediction and learning,"In the framework of the supervised learning of a real function defined on a
space X , the so called Kriging method stands on a real Gaussian field defined
on X. The Euclidean case is well known and has been widely studied. In this
paper, we explore the less classical case where X is the non commutative finite
group of permutations. In this setting, we propose and study an harmonic
analysis of the covariance operators that enables to consider Gaussian
processes models and forecasting issues. Our theory is motivated by statistical
ranking problems.","['Fran√ßois Bachoc', 'Baptiste Broto', 'Fabrice Gamboa', 'Jean-Michel Loubes']",['stat.ML'],2018-03-16 09:19:36+00:00
http://arxiv.org/abs/1803.06111v1,Vulnerability of Deep Learning,"The Renormalisation Group (RG) provides a framework in which it is possible
to assess whether a deep-learning network is sensitive to small changes in the
input data and hence prone to error, or susceptible to adversarial attack.
Distinct classification outputs are associated with different RG fixed points
and sensitivity to small changes in the input data is due to the presence of
relevant operators at a fixed point. A numerical scheme, based on Monte Carlo
RG ideas, is proposed for identifying the existence of relevant operators and
the corresponding directions of greatest sensitivity in the input data. Thus, a
trained deep-learning network may be tested for its robustness and, if it is
vulnerable to attack, dangerous perturbations of the input data identified.",['Richard Kenway'],"['stat.ML', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.AI', 'cs.LG']",2018-03-16 08:52:04+00:00
http://arxiv.org/abs/1803.06084v2,A Kernel Theory of Modern Data Augmentation,"Data augmentation, a technique in which a training set is expanded with
class-preserving transformations, is ubiquitous in modern machine learning
pipelines. In this paper, we seek to establish a theoretical framework for
understanding data augmentation. We approach this from two directions: First,
we provide a general model of augmentation as a Markov process, and show that
kernels appear naturally with respect to this model, even when we do not employ
kernel classification. Next, we analyze more directly the effect of
augmentation on kernel classifiers, showing that data augmentation can be
approximated by first-order feature averaging and second-order variance
regularization components. These frameworks both serve to illustrate the ways
in which data augmentation affects the downstream learning model, and the
resulting analyses provide novel connections between prior work in invariant
kernels, tangent propagation, and robust optimization. Finally, we provide
several proof-of-concept applications showing that our theory can be useful for
accelerating machine learning workflows, such as reducing the amount of
computation needed to train using augmented data, and predicting the utility of
a transformation prior to training.","['Tri Dao', 'Albert Gu', 'Alexander J. Ratner', 'Virginia Smith', 'Christopher De Sa', 'Christopher R√©']","['cs.LG', 'stat.ML']",2018-03-16 06:05:32+00:00
http://arxiv.org/abs/1803.06905v2,TBD: Benchmarking and Analyzing Deep Neural Network Training,"The recent popularity of deep neural networks (DNNs) has generated a lot of
research interest in performing DNN-related computation efficiently. However,
the primary focus is usually very narrow and limited to (i) inference -- i.e.
how to efficiently execute already trained models and (ii) image classification
networks as the primary benchmark for evaluation.
  Our primary goal in this work is to break this myopic view by (i) proposing a
new benchmark for DNN training, called TBD (TBD is short for Training Benchmark
for DNNs), that uses a representative set of DNN models that cover a wide range
of machine learning applications: image classification, machine translation,
speech recognition, object detection, adversarial networks, reinforcement
learning, and (ii) by performing an extensive performance analysis of training
these different applications on three major deep learning frameworks
(TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU,
multi-GPU, and multi-machine). TBD currently covers six major application
domains and eight different state-of-the-art models.
  We present a new toolchain for performance analysis for these models that
combines the targeted usage of existing performance analysis tools, careful
selection of new and existing metrics and methodologies to analyze the results,
and utilization of domain specific characteristics of DNN training. We also
build a new set of tools for memory profiling in all three major frameworks;
much needed tools that can finally shed some light on precisely how much memory
is consumed by different data structures (weights, activations, gradients,
workspace) in DNN training. By using our tools and methodologies, we make
several important observations and recommendations on where the future research
and optimization of DNN training should be focused.","['Hongyu Zhu', 'Mohamed Akrout', 'Bojian Zheng', 'Andrew Pelegris', 'Amar Phanishayee', 'Bianca Schroeder', 'Gennady Pekhimenko']","['cs.LG', 'stat.ML']",2018-03-16 05:16:06+00:00
http://arxiv.org/abs/1803.06071v2,Impacts of Dirty Data: and Experimental Evaluation,"Data quality issues have attracted widespread attention due to the negative
impacts of dirty data on data mining and machine learning results. The
relationship between data quality and the accuracy of results could be applied
on the selection of the appropriate algorithm with the consideration of data
quality and the determination of the data share to clean. However, rare
research has focused on exploring such relationship. Motivated by this, this
paper conducts an experimental comparison for the effects of missing,
inconsistent and conflicting data on classification and clustering algorithms.
Based on the experimental findings, we provide guidelines for algorithm
selection and data cleaning.","['Zhixin Qi', 'Hongzhi Wang', 'Jianzhong Li', 'Hong Gao']","['cs.DB', 'cs.LG', 'stat.ML']",2018-03-16 04:23:00+00:00
http://arxiv.org/abs/1803.06070v2,"Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data","We propose a novel class of network models for temporal dyadic interaction
data. Our goal is to capture a number of important features often observed in
social interactions: sparsity, degree heterogeneity, community structure and
reciprocity. We propose a family of models based on self-exciting Hawkes point
processes in which events depend on the history of the process. The key
component is the conditional intensity function of the Hawkes Process, which
captures the fact that interactions may arise as a response to past
interactions (reciprocity), or due to shared interests between individuals
(community structure). In order to capture the sparsity and degree
heterogeneity, the base (non time dependent) part of the intensity function
builds on compound random measures following Todeschini et al. (2016). We
conduct experiments on a variety of real-world temporal interaction data and
show that the proposed model outperforms many competing approaches for link
prediction, and leads to interpretable parameters.","['Xenia Miscouridou', 'Fran√ßois Caron', 'Yee Whye Teh']",['stat.ML'],2018-03-16 04:00:41+00:00
http://arxiv.org/abs/1803.06058v4,Constant-Time Predictive Distributions for Gaussian Processes,"One of the most compelling features of Gaussian process (GP) regression is
its ability to provide well-calibrated posterior distributions. Recent advances
in inducing point methods have sped up GP marginal likelihood and posterior
mean computations, leaving posterior covariance estimation and sampling as the
remaining computational bottlenecks. In this paper we address these
shortcomings by using the Lanczos algorithm to rapidly approximate the
predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs
Variance Estimates), substantially improves time and space complexity. In our
experiments, LOVE computes covariances up to 2,000 times faster and draws
samples 18,000 times faster than existing methods, all without sacrificing
accuracy.","['Geoff Pleiss', 'Jacob R. Gardner', 'Kilian Q. Weinberger', 'Andrew Gordon Wilson']","['cs.LG', 'stat.ML']",2018-03-16 02:31:45+00:00
http://arxiv.org/abs/1803.06031v2,Optimal Bipartite Network Clustering,"We study bipartite community detection in networks, or more generally the
network biclustering problem. We present a fast two-stage procedure based on
spectral initialization followed by the application of a pseudo-likelihood
classifier twice. Under mild regularity conditions, we establish the weak
consistency of the procedure (i.e., the convergence of the misclassification
rate to zero) under a general bipartite stochastic block model. We show that
the procedure is optimal in the sense that it achieves the optimal convergence
rate that is achievable by a biclustering oracle, adaptively over the whole
class, up to constants. This is further formalized by deriving a minimax lower
bound over a class of biclustering problems. The optimal rate we obtain
sharpens some of the existing results and generalizes others to a wide regime
of average degree growth, from sparse networks with average degrees growing
arbitrarily slowly to fairly dense networks with average degrees of order
$\sqrt{n}$. As a special case, we recover the known exact recovery threshold in
the $\log n$ regime of sparsity. To obtain the consistency result, as part of
the provable version of the algorithm, we introduce a sub-block partitioning
scheme that is also computationally attractive, allowing for distributed
implementation of the algorithm without sacrificing optimality. The provable
algorithm is derived from a general class of pseudo-likelihood biclustering
algorithms that employ simple EM type updates. We show the effectiveness of
this general class by numerical simulations.","['Zhixin Zhou', 'Arash A. Amini']","['math.ST', 'cs.SI', 'stat.ML', 'stat.TH']",2018-03-15 23:19:30+00:00
http://arxiv.org/abs/1803.06030v2,Estimation of lactate threshold with machine learning techniques in recreational runners,"Lactate threshold is considered an essential parameter when assessing
performance of elite and recreational runners and prescribing training
intensities in endurance sports. However, the measurement of blood lactate
concentration requires expensive equipment and the extraction of blood samples,
which are inconvenient for frequent monitoring. Furthermore, most recreational
runners do not have access to routine assessment of their physical fitness by
the aforementioned equipment so they are not able to calculate the lactate
threshold without resorting to an expensive and specialized centre. Therefore,
the main objective of this study is to create an intelligent system capable of
estimating the lactate threshold of recreational athletes participating in
endurance running sports. The solution here proposed is based on a machine
learning system which models the lactate evolution using recurrent neural
networks and includes the proposal of standardization of the temporal axis as
well as a modification of the stratified sampling method. The results show that
the proposed system accurately estimates the lactate threshold of 89.52% of the
athletes and its correlation with the experimentally measured lactate threshold
is very high (R=0,89). Moreover, its behaviour with the test dataset is as good
as with the training set, meaning that the generalization power of the model is
high. Therefore, in this study a machine learning based system is proposed as
alternative to the traditional invasive lactate threshold measurement tests for
recreational runners.","['Urtats Etxegarai', 'Eva Portillo', 'Jon Irazusta', 'Ander Arriandiaga', 'Itziar Cabanes']",['stat.ML'],2018-03-15 23:11:59+00:00
http://arxiv.org/abs/1803.06024v1,Deep Learning Reconstruction of Ultra-Short Pulses,"Ultra-short laser pulses with femtosecond to attosecond pulse duration are
the shortest systematic events humans can create. Characterization (amplitude
and phase) of these pulses is a key ingredient in ultrafast science, e.g.,
exploring chemical reactions and electronic phase transitions. Here, we propose
and demonstrate, numerically and experimentally, the first deep neural network
technique to reconstruct ultra-short optical pulses. We anticipate that this
approach will extend the range of ultrashort laser pulses that can be
characterized, e.g., enabling to diagnose very weak attosecond pulses.","['Tom Zahavy', 'Alex Dikopoltsev', 'Oren Cohen', 'Shie Mannor', 'Mordechai Segev']","['physics.optics', 'cs.AI', 'cs.LG', 'stat.ML']",2018-03-15 22:37:31+00:00
http://arxiv.org/abs/1803.06010v2,Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling,"Ridge leverage scores provide a balance between low-rank approximation and
regularization, and are ubiquitous in randomized linear algebra and machine
learning. Deterministic algorithms are also of interest in the moderately big
data regime, because deterministic algorithms provide interpretability to the
practitioner by having no failure probability and always returning the same
results.
  We provide provable guarantees for deterministic column sampling using ridge
leverage scores. The matrix sketch returned by our algorithm is a column subset
of the original matrix, yielding additional interpretability. Like the
randomized counterparts, the deterministic algorithm provides (1 + {\epsilon})
error column subset selection, (1 + {\epsilon}) error projection-cost
preservation, and an additive-multiplicative spectral bound. We also show that
under the assumption of power-law decay of ridge leverage scores, this
deterministic algorithm is provably as accurate as randomized algorithms.
  Lastly, ridge regression is frequently used to regularize ill-posed linear
least-squares problems. While ridge regression provides shrinkage for the
regression coefficients, many of the coefficients remain small but non-zero.
Performing ridge regression with the matrix sketch returned by our algorithm
and a particular regularization parameter forces coefficients to zero and has a
provable (1 + {\epsilon}) bound on the statistical risk. As such, it is an
interesting alternative to elastic net regularization.",['Shannon R. McCurdy'],"['math.ST', 'cs.DS', 'stat.CO', 'stat.ML', 'stat.TH']",2018-03-15 21:35:55+00:00
http://arxiv.org/abs/1803.05999v2,Escaping Saddles with Stochastic Gradients,"We analyze the variance of stochastic gradients along negative curvature
directions in certain non-convex machine learning models and show that
stochastic gradients exhibit a strong component along these directions.
Furthermore, we show that - contrary to the case of isotropic noise - this
variance is proportional to the magnitude of the corresponding eigenvalues and
not decreasing in the dimensionality. Based upon this observation we propose a
new assumption under which we show that the injection of explicit, isotropic
noise usually applied to make gradient descent escape saddle points can
successfully be replaced by a simple SGD step. Additionally - and under the
same condition - we derive the first convergence rate for plain SGD to a
second-order stationary point in a number of iterations that is independent of
the problem dimension.","['Hadi Daneshmand', 'Jonas Kohler', 'Aurelien Lucchi', 'Thomas Hofmann']","['cs.LG', 'math.OC', 'stat.ML']",2018-03-15 20:48:06+00:00
http://arxiv.org/abs/1803.05985v1,EEG machine learning with Higuchi fractal dimension and Sample Entropy as features for successful detection of depression,"Reliable diagnosis of depressive disorder is essential for both optimal
treatment and prevention of fatal outcomes. In this study, we aimed to
elucidate the effectiveness of two non-linear measures, Higuchi Fractal
Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders
when applied on EEG. HFD and SampEn of EEG signals were used as features for
seven machine learning algorithms including Multilayer Perceptron, Logistic
Regression, Support Vector Machines with the linear and polynomial kernel,
Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG
between healthy control subjects and patients diagnosed with depression. We
confirmed earlier observations that both non-linear measures can discriminate
EEG signals of patients from healthy control subjects. The results suggest that
good classification is possible even with a small number of principal
components. Average accuracy among classifiers ranged from 90.24% to 97.56%.
Among the two measures, SampEn had better performance. Using HFD and SampEn and
a variety of machine learning techniques we can accurately discriminate
patients diagnosed with depression vs controls which can serve as a highly
sensitive, clinically relevant marker for the diagnosis of depressive
disorders.","['Milena Cukic', 'David Pokrajac', 'Miodrag Stokic', 'slobodan Simic', 'Vlada Radivojevic', 'Milos Ljubisavljevic']","['stat.ML', 'cs.LG', 'q-bio.NC']",2018-03-15 20:13:38+00:00
http://arxiv.org/abs/1803.05976v1,Deep Choice Model Using Pointer Networks for Airline Itinerary Prediction,"Travel providers such as airlines and on-line travel agents are becoming more
and more interested in understanding how passengers choose among alternative
itineraries when searching for flights. This knowledge helps them better
display and adapt their offer, taking into account market conditions and
customer needs. Some common applications are not only filtering and sorting
alternatives, but also changing certain attributes in real-time (e.g., changing
the price). In this paper, we concentrate with the problem of modeling air
passenger choices of flight itineraries. This problem has historically been
tackled using classical Discrete Choice Modelling techniques. Traditional
statistical approaches, in particular the Multinomial Logit model (MNL), is
widely used in industrial applications due to its simplicity and general good
performance. However, MNL models present several shortcomings and assumptions
that might not hold in real applications. To overcome these difficulties, we
present a new choice model based on Pointer Networks. Given an input sequence,
this type of deep neural architecture combines Recurrent Neural Networks with
the Attention Mechanism to learn the conditional probability of an output whose
values correspond to positions in an input sequence. Therefore, given a
sequence of different alternatives presented to a customer, the model can learn
to point to the one most likely to be chosen by the customer. The proposed
method was evaluated on a real dataset that combines on-line user search logs
and airline flight bookings. Experimental results show that the proposed model
outperforms the traditional MNL model on several metrics.","['Alejandro Mottini', 'Rodrigo Acuna-Agost']","['stat.ML', 'cs.LG']",2018-03-15 19:55:56+00:00
http://arxiv.org/abs/1803.05897v1,Contrasting information theoretic decompositions of modulatory and arithmetic interactions in neural information processing systems,"Biological and artificial neural systems are composed of many local
processors, and their capabilities depend upon the transfer function that
relates each local processor's outputs to its inputs. This paper uses a recent
advance in the foundations of information theory to study the properties of
local processors that use contextual input to amplify or attenuate transmission
of information about their driving inputs. This advance enables the information
transmitted by processors with two distinct inputs to be decomposed into those
components unique to each input, that shared between the two inputs, and that
which depends on both though it is in neither, i.e. synergy. The decompositions
that we report here show that contextual modulation has information processing
properties that contrast with those of all four simple arithmetic operators,
that it can take various forms, and that the form used in our previous studies
of artificial neural nets composed of local processors with both driving and
contextual inputs is particularly well-suited to provide the distinctive
capabilities of contextual modulation under a wide range of conditions. We
argue that the decompositions reported here could be compared with those
obtained from empirical neurobiological and psychophysical data under
conditions thought to reflect contextual modulation. That would then shed new
light on the underlying processes involved. Finally, we suggest that such
decompositions could aid the design of context-sensitive machine learning
algorithms.","['Jim W. Kay', 'William A. Phillips']","['cs.IT', 'math.IT', 'q-bio.NC', 'q-bio.QM', 'stat.ML']",2018-03-15 17:51:21+00:00
http://arxiv.org/abs/1803.05867v1,Capturing Structure Implicitly from Time-Series having Limited Data,"Scientific fields such as insider-threat detection and highway-safety
planning often lack sufficient amounts of time-series data to estimate
statistical models for the purpose of scientific discovery. Moreover, the
available limited data are quite noisy. This presents a major challenge when
estimating time-series models that are robust to overfitting and have
well-calibrated uncertainty estimates. Most of the current literature in these
fields involve visualizing the time-series for noticeable structure and hard
coding them into pre-specified parametric functions. This approach is
associated with two limitations. First, given that such trends may not be
easily noticeable in small data, it is difficult to explicitly incorporate
expressive structure into the models during formulation. Second, it is
difficult to know $\textit{a priori}$ the most appropriate functional form to
use. To address these limitations, a nonparametric Bayesian approach was
proposed to implicitly capture hidden structure from time series having limited
data. The proposed model, a Gaussian process with a spectral mixture kernel,
precludes the need to pre-specify a functional form and hard code trends, is
robust to overfitting and has well-calibrated uncertainty estimates.","['Daniel Emaasit', 'Matthew Johnson']","['stat.ML', 'cs.LG']",2018-03-15 17:03:04+00:00
http://arxiv.org/abs/1803.05796v2,Deep Architectures for Learning Context-dependent Ranking Functions,"Object ranking is an important problem in the realm of preference learning.
On the basis of training data in the form of a set of rankings of objects,
which are typically represented as feature vectors, the goal is to learn a
ranking function that predicts a linear order of any new set of objects.
Current approaches commonly focus on ranking by scoring, i.e., on learning an
underlying latent utility function that seeks to capture the inherent utility
of each object. These approaches, however, are not able to take possible
effects of context-dependence into account, where context-dependence means that
the utility or usefulness of an object may also depend on what other objects
are available as alternatives. In this paper, we formalize the problem of
context-dependent ranking and present two general approaches based on two
natural representations of context-dependent ranking functions. Both approaches
are instantiated by means of appropriate neural network architectures, which
are evaluated on suitable benchmark task.","['Karlson Pfannschmidt', 'Pritha Gupta', 'Eyke H√ºllermeier']","['stat.ML', 'cs.IR', 'cs.LG', 'cs.NE']",2018-03-15 15:14:16+00:00
http://arxiv.org/abs/1803.05784v2,Minimax optimal rates for Mondrian trees and forests,"Introduced by Breiman, Random Forests are widely used classification and
regression algorithms. While being initially designed as batch algorithms,
several variants have been proposed to handle online learning. One particular
instance of such forests is the \emph{Mondrian Forest}, whose trees are built
using the so-called Mondrian process, therefore allowing to easily update their
construction in a streaming fashion. In this paper, we provide a thorough
theoretical study of Mondrian Forests in a batch learning setting, based on new
results about Mondrian partitions. Our results include consistency and
convergence rates for Mondrian Trees and Forests, that turn out to be minimax
optimal on the set of $s$-H\""older function with $s \in (0,1]$ (for trees and
forests) and $s \in (1,2]$ (for forests only), assuming a proper tuning of
their complexity parameter in both cases. Furthermore, we prove that an
adaptive procedure (to the unknown $s \in (0, 2]$) can be constructed by
combining Mondrian Forests with a standard model aggregation algorithm. These
results are the first demonstrating that some particular random forests achieve
minimax rates \textit{in arbitrary dimension}. Owing to their remarkably simple
distributional properties, which lead to minimax rates, Mondrian trees are a
promising basis for more sophisticated yet theoretically sound random forests
variants.","['Jaouad Mourtada', 'St√©phane Ga√Øffas', 'Erwan Scornet']","['stat.ML', 'math.ST', 'stat.TH']",2018-03-15 14:43:04+00:00
http://arxiv.org/abs/1803.05776v2,Gaussian Processes Over Graphs,"We propose Gaussian processes for signals over graphs (GPG) using the apriori
knowledge that the target vectors lie over a graph. We incorporate this
information using a graph- Laplacian based regularization which enforces the
target vectors to have a specific profile in terms of graph Fourier transform
coeffcients, for example lowpass or bandpass graph signals. We discuss how the
regularization affects the mean and the variance in the prediction output. In
particular, we prove that the predictive variance of the GPG is strictly
smaller than the conventional Gaussian process (GP) for any non-trivial graph.
We validate our concepts by application to various real-world graph signals.
Our experiments show that the performance of the GPG is superior to GP for
small training data sizes and under noisy training.","['Arun Venkitaraman', 'Saikat Chatterjee', 'Peter H√§ndel']","['stat.ML', 'cs.LG', 'eess.SP']",2018-03-15 14:27:49+00:00
http://arxiv.org/abs/1803.05657v1,Fast Subspace Clustering Based on the Kronecker Product,"Subspace clustering is a useful technique for many computer vision
applications in which the intrinsic dimension of high-dimensional data is often
smaller than the ambient dimension. Spectral clustering, as one of the main
approaches to subspace clustering, often takes on a sparse representation or a
low-rank representation to learn a block diagonal self-representation matrix
for subspace generation. However, existing methods require solving a large
scale convex optimization problem with a large set of data, with computational
complexity reaches O(N^3) for N data points. Therefore, the efficiency and
scalability of traditional spectral clustering methods can not be guaranteed
for large scale datasets. In this paper, we propose a subspace clustering model
based on the Kronecker product. Due to the property that the Kronecker product
of a block diagonal matrix with any other matrix is still a block diagonal
matrix, we can efficiently learn the representation matrix which is formed by
the Kronecker product of k smaller matrices. By doing so, our model
significantly reduces the computational complexity to O(kN^{3/k}). Furthermore,
our model is general in nature, and can be adapted to different regularization
based subspace clustering methods. Experimental results on two public datasets
show that our model significantly improves the efficiency compared with several
state-of-the-art methods. Moreover, we have conducted experiments on synthetic
data to verify the scalability of our model for large scale datasets.","['Lei Zhou', 'Xiao Bai', 'Xianglong Liu', 'Jun Zhou', 'Hancock Edwin']","['cs.LG', 'cs.CV', 'stat.ML']",2018-03-15 09:31:44+00:00
http://arxiv.org/abs/1803.05649v2,Sylvester Normalizing Flows for Variational Inference,"Variational inference relies on flexible approximate posterior distributions.
Normalizing flows provide a general recipe to construct flexible variational
posteriors. We introduce Sylvester normalizing flows, which can be seen as a
generalization of planar flows. Sylvester normalizing flows remove the
well-known single-unit bottleneck from planar flows, making a single
transformation much more flexible. We compare the performance of Sylvester
normalizing flows against planar flows and inverse autoregressive flows and
demonstrate that they compare favorably on several datasets.","['Rianne van den Berg', 'Leonard Hasenclever', 'Jakub M. Tomczak', 'Max Welling']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.ME']",2018-03-15 09:15:14+00:00
http://arxiv.org/abs/1803.05621v2,Proximal SCOPE for Distributed Sparse Learning: Better Data Partition Implies Faster Convergence Rate,"Distributed sparse learning with a cluster of multiple machines has attracted
much attention in machine learning, especially for large-scale applications
with high-dimensional data. One popular way to implement sparse learning is to
use $L_1$ regularization. In this paper, we propose a novel method, called
proximal \mbox{SCOPE}~(\mbox{pSCOPE}), for distributed sparse learning with
$L_1$ regularization. pSCOPE is based on a \underline{c}ooperative
\underline{a}utonomous \underline{l}ocal \underline{l}earning~(\mbox{CALL})
framework. In the \mbox{CALL} framework of \mbox{pSCOPE}, we find that the data
partition affects the convergence of the learning procedure, and subsequently
we define a metric to measure the goodness of a data partition. Based on the
defined metric, we theoretically prove that pSCOPE is convergent with a linear
convergence rate if the data partition is good enough. We also prove that
better data partition implies faster convergence rate. Furthermore, pSCOPE is
also communication efficient. Experimental results on real data sets show that
pSCOPE can outperform other state-of-the-art distributed methods for sparse
learning.","['Shen-Yi Zhao', 'Gong-Duo Zhang', 'Ming-Wei Li', 'Wu-Jun Li']","['stat.ML', 'cs.LG']",2018-03-15 07:38:50+00:00
http://arxiv.org/abs/1803.05598v2,Large Margin Deep Networks for Classification,"We present a formulation of deep learning that aims at producing a large
margin classifier. The notion of margin, minimum distance to a decision
boundary, has served as the foundation of several theoretically profound and
empirically successful results for both classification and regression tasks.
However, most large margin algorithms are applicable only to shallow models
with a preset feature representation; and conventional margin methods for
neural networks only enforce margin at the output layer. Such methods are
therefore not well suited for deep networks.
  In this work, we propose a novel loss function to impose a margin on any
chosen set of layers of a deep network (including input and hidden layers). Our
formulation allows choosing any norm on the metric measuring the margin. We
demonstrate that the decision boundary obtained by our loss has nice properties
compared to standard classification loss functions. Specifically, we show
improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on
multiple tasks: generalization from small training sets, corrupted labels, and
robustness against adversarial perturbations. The resulting loss is general and
complementary to existing data augmentation (such as random/adversarial input
transform) and regularization techniques (such as weight decay, dropout, and
batch norm).","['Gamaleldin F. Elsayed', 'Dilip Krishnan', 'Hossein Mobahi', 'Kevin Regan', 'Samy Bengio']","['stat.ML', 'cs.LG']",2018-03-15 05:33:13+00:00
http://arxiv.org/abs/1803.05591v2,On the insufficiency of existing momentum schemes for Stochastic Optimization,"Momentum based stochastic gradient methods such as heavy ball (HB) and
Nesterov's accelerated gradient descent (NAG) method are widely used in
practice for training deep networks and other supervised learning models, as
they often provide significant improvements over stochastic gradient descent
(SGD). Rigorously speaking, ""fast gradient"" methods have provable improvements
over gradient descent only for the deterministic case, where the gradients are
exact. In the stochastic case, the popular explanations for their wide
applicability is that when these fast gradient methods are applied in the
stochastic case, they partially mimic their exact gradient counterparts,
resulting in some practical gain. This work provides a counterpoint to this
belief by proving that there exist simple problem instances where these methods
cannot outperform SGD despite the best setting of its parameters. These
negative problem instances are, in an informal sense, generic; they do not look
like carefully constructed pathological instances. These results suggest (along
with empirical evidence) that HB or NAG's practical performance gains are a
by-product of mini-batching.
  Furthermore, this work provides a viable (and provable) alternative, which,
on the same set of problem instances, significantly improves over HB, NAG, and
SGD's performance. This algorithm, referred to as Accelerated Stochastic
Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based
on a relatively less popular variant of Nesterov's Acceleration. Extensive
empirical results in this paper show that ASGD has performance gains over HB,
NAG, and SGD.","['Rahul Kidambi', 'Praneeth Netrapalli', 'Prateek Jain', 'Sham M. Kakade']","['cs.LG', 'math.OC', 'stat.ML']",2018-03-15 05:09:51+00:00
http://arxiv.org/abs/1803.05589v2,Variational Message Passing with Structured Inference Networks,"Recent efforts on combining deep models with probabilistic graphical models
are promising in providing flexible models that are also easy to interpret. We
propose a variational message-passing algorithm for variational inference in
such models. We make three contributions. First, we propose structured
inference networks that incorporate the structure of the graphical model in the
inference network of variational auto-encoders (VAE). Second, we establish
conditions under which such inference networks enable fast amortized inference
similar to VAE. Finally, we derive a variational message passing algorithm to
perform efficient natural-gradient inference while retaining the efficiency of
the amortized inference. By simultaneously enabling structured, amortized, and
natural-gradient inference for deep structured models, our method simplifies
and generalizes existing methods.","['Wu Lin', 'Nicolas Hubacher', 'Mohammad Emtiyaz Khan']",['stat.ML'],2018-03-15 04:26:24+00:00
http://arxiv.org/abs/1803.05573v1,Improving GANs Using Optimal Transport,"We present Optimal Transport GAN (OT-GAN), a variant of generative
adversarial nets minimizing a new metric measuring the distance between the
generator distribution and the data distribution. This metric, which we call
mini-batch energy distance, combines optimal transport in primal form with an
energy distance defined in an adversarially learned feature space, resulting in
a highly discriminative distance function with unbiased mini-batch gradients.
Experimentally we show OT-GAN to be highly stable when trained with large
mini-batches, and we present state-of-the-art results on several popular
benchmark problems for image generation.","['Tim Salimans', 'Han Zhang', 'Alec Radford', 'Dimitris Metaxas']","['cs.LG', 'stat.ML']",2018-03-15 02:34:46+00:00
http://arxiv.org/abs/1803.05554v3,Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models,"Learning a Bayesian network (BN) from data can be useful for decision-making
or discovering causal relationships. However, traditional methods often fail in
modern applications, which exhibit a larger number of observed variables than
data points. The resulting uncertainty about the underlying network as well as
the desire to incorporate prior information recommend a Bayesian approach to
learning the BN, but the highly combinatorial structure of BNs poses a striking
challenge for inference. The current state-of-the-art methods such as order
MCMC are faster than previous methods but prevent the use of many natural
structural priors and still have running time exponential in the maximum
indegree of the true directed acyclic graph (DAG) of the BN. We here propose an
alternative posterior approximation based on the observation that, if we
incorporate empirical conditional independence tests, we can focus on a
high-probability DAG associated with each order of the vertices. We show that
our method allows the desired flexibility in prior specification, removes
timing dependence on the maximum indegree and yields provably good posterior
approximations; in addition, we show that it achieves superior accuracy,
scalability, and sampler mixing on several datasets.","['Raj Agrawal', 'Tamara Broderick', 'Caroline Uhler']","['stat.CO', 'cs.LG', 'stat.ME', 'stat.ML']",2018-03-15 00:53:25+00:00
http://arxiv.org/abs/1803.05419v2,Generalised Structural CNNs (SCNNs) for time series data with arbitrary graph topology,"Deep Learning methods, specifically convolutional neural networks (CNNs),
have seen a lot of success in the domain of image-based data, where the data
offers a clearly structured topology in the regular lattice of pixels. This
4-neighbourhood topological simplicity makes the application of convolutional
masks straightforward for time series data, such as video applications, but
many high-dimensional time series data are not organised in regular lattices,
and instead values may have adjacency relationships with non-trivial
topologies, such as small-world networks or trees. In our application case,
human kinematics, it is currently unclear how to generalise convolutional
kernels in a principled manner. Therefore we define and implement here a
framework for general graph-structured CNNs for time series analysis. Our
algorithm automatically builds convolutional layers using the specified
adjacency matrix of the data dimensions and convolutional masks that scale with
the hop distance. In the limit of a lattice-topology our method produces the
well-known image convolutional masks. We test our method first on synthetic
data of arbitrarily-connected graphs and human hand motion capture data, where
the hand is represented by a tree capturing the mechanical dependencies of the
joints. We are able to demonstrate, amongst other things, that inclusion of the
graph structure of the data dimensions improves model prediction significantly,
when compared against a benchmark CNN model with only time convolution layers.","['Thomas Teh', 'Chaiyawan Auepanwiriyakul', 'John Alexander Harston', 'A. Aldo Faisal']","['stat.ML', 'cs.LG']",2018-03-14 17:39:11+00:00
http://arxiv.org/abs/1803.05407v3,Averaging Weights Leads to Wider Optima and Better Generalization,"Deep neural networks are typically trained by optimizing a loss function with
an SGD variant, in conjunction with a decaying learning rate, until
convergence. We show that simple averaging of multiple points along the
trajectory of SGD, with a cyclical or constant learning rate, leads to better
generalization than conventional training. We also show that this Stochastic
Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and
approximates the recent Fast Geometric Ensembling (FGE) approach with a single
model. Using SWA we achieve notable improvement in test accuracy over
conventional SGD training on a range of state-of-the-art residual networks,
PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and
ImageNet. In short, SWA is extremely easy to implement, improves
generalization, and has almost no computational overhead.","['Pavel Izmailov', 'Dmitrii Podoprikhin', 'Timur Garipov', 'Dmitry Vetrov', 'Andrew Gordon Wilson']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2018-03-14 17:09:27+00:00
http://arxiv.org/abs/1803.05402v5,Imitation Learning with Concurrent Actions in 3D Games,"In this work we describe a novel deep reinforcement learning architecture
that allows multiple actions to be selected at every time-step in an efficient
manner. Multi-action policies allow complex behaviours to be learnt that would
otherwise be hard to achieve when using single action selection techniques. We
use both imitation learning and temporal difference (TD) reinforcement learning
(RL) to provide a 4x improvement in training time and 2.5x improvement in
performance over single action selection TD RL. We demonstrate the capabilities
of this network using a complex in-house 3D game. Mimicking the behavior of the
expert teacher significantly improves world state exploration and allows the
agents vision system to be trained more rapidly than TD RL alone. This initial
training technique kick-starts TD learning and the agent quickly learns to
surpass the capabilities of the expert.","['Jack Harmer', 'Linus Gissl√©n', 'Jorge del Val', 'Henrik Holst', 'Joakim Bergdahl', 'Tom Olsson', 'Kristoffer Sj√∂√∂', 'Magnus Nordin']","['cs.AI', 'cs.LG', 'stat.ML']",2018-03-14 16:59:17+00:00
http://arxiv.org/abs/1803.05397v1,Redundancy Techniques for Straggler Mitigation in Distributed Optimization and Learning,"Performance of distributed optimization and learning systems is bottlenecked
by ""straggler"" nodes and slow communication links, which significantly delay
computation. We propose a distributed optimization framework where the dataset
is ""encoded"" to have an over-complete representation with built-in redundancy,
and the straggling nodes in the system are dynamically left out of the
computation at every iteration, whose loss is compensated by the embedded
redundancy. We show that oblivious application of several popular optimization
algorithms on encoded data, including gradient descent, L-BFGS, proximal
gradient under data parallelism, and coordinate descent under model
parallelism, converge to either approximate or exact solutions of the original
problem when stragglers are treated as erasures. These convergence results are
deterministic, i.e., they establish sample path convergence for arbitrary
sequences of delay patterns or distributions on the nodes, and are independent
of the tail behavior of the delay distribution. We demonstrate that equiangular
tight frames have desirable properties as encoding matrices, and propose
efficient mechanisms for encoding large-scale data. We implement the proposed
technique on Amazon EC2 clusters, and demonstrate its performance over several
learning problems, including matrix factorization, LASSO, ridge regression and
logistic regression, and compare the proposed method with uncoded,
asynchronous, and data replication strategies.","['Can Karakus', 'Yifan Sun', 'Suhas Diggavi', 'Wotao Yin']","['stat.ML', 'cs.DC', 'cs.LG', 'math.OC']",2018-03-14 16:48:08+00:00
http://arxiv.org/abs/1803.05391v2,On the Universal Approximation Property and Equivalence of Stochastic Computing-based Neural Networks and Binary Neural Networks,"Large-scale deep neural networks are both memory intensive and
computation-intensive, thereby posing stringent requirements on the computing
platforms. Hardware accelerations of deep neural networks have been extensively
investigated in both industry and academia. Specific forms of binary neural
networks (BNNs) and stochastic computing based neural networks (SCNNs) are
particularly appealing to hardware implementations since they can be
implemented almost entirely with binary operations. Despite the obvious
advantages in hardware implementation, these approximate computing techniques
are questioned by researchers in terms of accuracy and universal applicability.
Also it is important to understand the relative pros and cons of SCNNs and BNNs
in theory and in actual hardware implementations. In order to address these
concerns, in this paper we prove that the ""ideal"" SCNNs and BNNs satisfy the
universal approximation property with probability 1 (due to the stochastic
behavior). The proof is conducted by first proving the property for SCNNs from
the strong law of large numbers, and then using SCNNs as a ""bridge"" to prove
for BNNs. Based on the universal approximation property, we further prove that
SCNNs and BNNs exhibit the same energy complexity. In other words, they have
the same asymptotic energy consumption with the growing of network size. We
also provide a detailed analysis of the pros and cons of SCNNs and BNNs for
hardware implementations and conclude that SCNNs are more suitable for
hardware.","['Yanzhi Wang', 'Zheng Zhan', 'Jiayu Li', 'Jian Tang', 'Bo Yuan', 'Liang Zhao', 'Wujie Wen', 'Siyue Wang', 'Xue Lin']","['cs.LG', 'stat.ML']",2018-03-14 16:40:42+00:00
http://arxiv.org/abs/1803.05340v2,Measurement-based adaptation protocol with quantum reinforcement learning,"Machine learning employs dynamical algorithms that mimic the human capacity
to learn, where the reinforcement learning ones are among the most similar to
humans in this respect. On the other hand, adaptability is an essential aspect
to perform any task efficiently in a changing environment, and it is
fundamental for many purposes, such as natural selection. Here, we propose an
algorithm based on successive measurements to adapt one quantum state to a
reference unknown state, in the sense of achieving maximum overlap. The
protocol naturally provides many identical copies of the reference state, such
that in each measurement iteration more information about it is obtained. In
our protocol, we consider a system composed of three parts, the ""environment""
system, which provides the reference state copies; the register, which is an
auxiliary subsystem that interacts with the environment to acquire information
from it; and the agent, which corresponds to the quantum state that is adapted
by digital feedback with input corresponding to the outcome of the measurements
on the register. With this proposal we can achieve an average fidelity between
the environment and the agent of more than $90\% $ with less than $30$
iterations of the protocol. In addition, we extend the formalism to $ d
$-dimensional states, reaching an average fidelity of around $80\% $ in less
than $400$ iterations for $d=$ 11, for a variety of genuinely quantum and
semiclassical states. This work paves the way for the development of quantum
reinforcement learning protocols using quantum data and for the future
deployment of semi-autonomous quantum systems.","['F. Albarr√°n-Arriagada', 'J. C. Retamal', 'E. Solano', 'L. Lamata']","['quant-ph', 'cond-mat.mes-hall', 'cs.AI', 'cs.LG', 'stat.ML']",2018-03-14 15:06:11+00:00
http://arxiv.org/abs/1803.05339v1,Predicting Oral Disintegrating Tablet Formulations by Neural Network Techniques,"Oral Disintegrating Tablets (ODTs) is a novel dosage form that can be
dissolved on the tongue within 3min or less especially for geriatric and
pediatric patients. Current ODT formulation studies usually rely on the
personal experience of pharmaceutical experts and trial-and-error in the
laboratory, which is inefficient and time-consuming. The aim of current
research was to establish the prediction model of ODT formulations with direct
compression process by Artificial Neural Network (ANN) and Deep Neural Network
(DNN) techniques. 145 formulation data were extracted from Web of Science. All
data sets were divided into three parts: training set (105 data), validation
set (20) and testing set (20). ANN and DNN were compared for the prediction of
the disintegrating time. The accuracy of the ANN model has reached 85.60%,
80.00% and 75.00% on the training set, validation set and testing set
respectively, whereas that of the DNN model was 85.60%, 85.00% and 80.00%,
respectively. Compared with the ANN, DNN showed the better prediction for ODT
formulations. It is the first time that deep neural network with the improved
dataset selection algorithm is applied to formulation prediction on small data.
The proposed predictive approach could evaluate the critical parameters about
quality control of formulation, and guide research and process development. The
implementation of this prediction model could effectively reduce drug product
development timeline and material usage, and proactively facilitate the
development of a robust drug product.","['Run Han', 'Yilong Yang', 'Xiaoshan Li', 'Defang Ouyang']","['stat.ML', 'cs.AI', 'cs.LG']",2018-03-14 15:05:11+00:00
http://arxiv.org/abs/1803.05288v3,Domain Adaptation on Graphs by Learning Aligned Graph Bases,"A common assumption in semi-supervised learning with graph models is that the
class label function varies smoothly on the data graph, resulting in the rather
strict prior that the label function has low-frequency content. Meanwhile, in
many classification problems, the label function may vary abruptly in certain
graph regions, resulting in high-frequency components. Although the
semi-supervised estimation of class labels is an ill-posed problem in general,
in several applications it is possible to find a source graph on which the
label function has similar frequency content to that on the target graph where
the actual classification problem is defined. In this paper, we propose a
method for domain adaptation on graphs motivated by these observations. Our
algorithm is based on learning the spectrum of the label function in a source
graph with many labeled nodes, and transferring the information of the spectrum
to the target graph with fewer labeled nodes. While the frequency content of
the class label function can be identified through the graph Fourier transform,
it is not easy to transfer the Fourier coefficients directly between the two
graphs, since no one-to-one match exists between the Fourier basis vectors of
independently constructed graphs in the domain adaptation setting. We solve
this problem by learning a transformation between the Fourier bases of the two
graphs that flexibly ``aligns'' them. The unknown class label function on the
target graph is then reconstructed such that its spectrum matches that on the
source graph while also ensuring the consistency with the available labels. The
proposed method is tested in the classification of image, online product
review, and social network data sets. Comparative experiments suggest that the
proposed algorithm performs better than recent domain adaptation methods in the
literature in most settings.","['Mehmet Pilanci', 'Elif Vural']","['stat.ML', 'cs.LG']",2018-03-14 14:04:04+00:00
http://arxiv.org/abs/1803.05159v2,Multiplicative Updates for Convolutional NMF Under $Œ≤$-Divergence,"In this letter, we generalize the convolutional NMF by taking the
$\beta$-divergence as the contrast function and present the correct
multiplicative updates for its factors in closed form. The new updates unify
the $\beta$-NMF and the convolutional NMF. We state why almost all of the
existing updates are inexact and approximative w.r.t. the convolutional data
model. We show that our updates are stable and that their convergence
performance is consistent across the most common values of $\beta$.","['Pedro J. Villasana T.', 'Stanislaw Gorlow', 'Arvind T. Hariraman']","['cs.LG', 'cs.DS', 'stat.ML']",2018-03-14 08:11:07+00:00
http://arxiv.org/abs/1803.05130v1,Signal Processing and Piecewise Convex Estimation,"Many problems on signal processing reduce to nonparametric function
estimation. We propose a new methodology, piecewise convex fitting (PCF), and
give a two-stage adaptive estimate. In the first stage, the number and location
of the change points is estimated using strong smoothing. In the second stage,
a constrained smoothing spline fit is performed with the smoothing level chosen
to minimize the MSE. The imposed constraint is that a single change point
occurs in a region about each empirical change point of the first-stage
estimate. This constraint is equivalent to requiring that the third derivative
of the second-stage estimate has a single sign in a small neighborhood about
each first-stage change point. We sketch how PCF may be applied to signal
recovery, instantaneous frequency estimation, surface reconstruction, image
segmentation, spectral estimation and multivariate adaptive regression.",['Kurt Riedel'],"['stat.ME', 'eess.SP', 'math.ST', 'physics.data-an', 'stat.ML', 'stat.TH']",2018-03-14 04:17:21+00:00
http://arxiv.org/abs/1803.05112v5,Uplift Modeling from Separate Labels,"Uplift modeling is aimed at estimating the incremental impact of an action on
an individual's behavior, which is useful in various application domains such
as targeted marketing (advertisement campaigns) and personalized medicine
(medical treatments). Conventional methods of uplift modeling require every
instance to be jointly equipped with two types of labels: the taken action and
its outcome. However, obtaining two labels for each instance at the same time
is difficult or expensive in many real-world problems. In this paper, we
propose a novel method of uplift modeling that is applicable to a more
practical setting where only one type of labels is available for each instance.
We show a mean squared error bound for the proposed estimator and demonstrate
its effectiveness through experiments.","['Ikko Yamane', 'Florian Yger', 'Jamal Atif', 'Masashi Sugiyama']",['stat.ML'],2018-03-14 02:46:17+00:00
http://arxiv.org/abs/1803.05105v1,Ranking with Adaptive Neighbors,"Retrieving the most similar objects in a large-scale database for a given
query is a fundamental building block in many application domains, ranging from
web searches, visual, cross media, and document retrievals. State-of-the-art
approaches have mainly focused on capturing the underlying geometry of the data
manifolds. Graph-based approaches, in particular, define various diffusion
processes on weighted data graphs. Despite success, these approaches rely on
fixed-weight graphs, making ranking sensitive to the input affinity matrix. In
this study, we propose a new ranking algorithm that simultaneously learns the
data affinity matrix and the ranking scores. The proposed optimization
formulation assigns adaptive neighbors to each point in the data based on the
local connectivity, and the smoothness constraint assigns similar ranking
scores to similar data points. We develop a novel and efficient algorithm to
solve the optimization problem. Evaluations using synthetic and real datasets
suggest that the proposed algorithm can outperform the existing methods.","['Muge Li', 'Liangyue Li', 'Feiping Nie']","['cs.LG', 'stat.ML']",2018-03-14 02:23:11+00:00
http://arxiv.org/abs/1803.05104v3,Bucket Renormalization for Approximate Inference,"Probabilistic graphical models are a key tool in machine learning
applications. Computing the partition function, i.e., normalizing constant, is
a fundamental task of statistical inference but it is generally computationally
intractable, leading to extensive study of approximation methods. Iterative
variational methods are a popular and successful family of approaches. However,
even state of the art variational methods can return poor results or fail to
converge on difficult instances. In this paper, we instead consider computing
the partition function via sequential summation over variables. We develop
robust approximate algorithms by combining ideas from mini-bucket elimination
with tensor network and renormalization group methods from statistical physics.
The resulting ""convergence-free"" methods show good empirical performance on
both synthetic and real-world benchmark models, even for difficult instances.","['Sungsoo Ahn', 'Michael Chertkov', 'Adrian Weller', 'Jinwoo Shin']",['stat.ML'],2018-03-14 02:16:54+00:00
http://arxiv.org/abs/1803.05070v1,A Multi-Modal Approach to Infer Image Affect,"The group affect or emotion in an image of people can be inferred by
extracting features about both the people in the picture and the overall makeup
of the scene. The state-of-the-art on this problem investigates a combination
of facial features, scene extraction and even audio tonality. This paper
combines three additional modalities, namely, human pose, text-based tagging
and CNN extracted features / predictions. To the best of our knowledge, this is
the first time all of the modalities were extracted using deep neural networks.
We evaluate the performance of our approach against baselines and identify
insights throughout this paper.","['Ashok Sundaresan', 'Sugumar Murugesan', 'Sean Davis', 'Karthik Kappaganthu', 'ZhongYi Jin', 'Divya Jain', 'Anurag Maunder']","['cs.CV', 'cs.LG', 'stat.ML']",2018-03-13 23:07:45+00:00
http://arxiv.org/abs/1803.05428v5,A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music,"The Variational Autoencoder (VAE) has proven to be an effective model for
producing semantically meaningful latent representations for natural data.
However, it has thus far seen limited application to sequential data, and, as
we demonstrate, existing recurrent VAE models have difficulty modeling
sequences with long-term structure. To address this issue, we propose the use
of a hierarchical decoder, which first outputs embeddings for subsequences of
the input and then uses these embeddings to generate each subsequence
independently. This structure encourages the model to utilize its latent code,
thereby avoiding the ""posterior collapse"" problem, which remains an issue for
recurrent VAEs. We apply this architecture to modeling sequences of musical
notes and find that it exhibits dramatically better sampling, interpolation,
and reconstruction performance than a ""flat"" baseline model. An implementation
of our ""MusicVAE"" is available online at http://g.co/magenta/musicvae-code.","['Adam Roberts', 'Jesse Engel', 'Colin Raffel', 'Curtis Hawthorne', 'Douglas Eck']","['cs.LG', 'cs.SD', 'eess.AS', 'stat.ML']",2018-03-13 21:14:46+00:00
http://arxiv.org/abs/1803.05045v1,Analysis of Nonautonomous Adversarial Systems,"Generative adversarial networks are used to generate images but still their
convergence properties are not well understood. There have been a few studies
who intended to investigate the stability properties of GANs as a dynamical
system. This short writing can be seen in that direction. Among the proposed
methods for stabilizing training of GANs, {\ss}-GAN was the first who proposed
a complete annealing strategy to change high-level conditions of the GAN
objective. In this note, we show by a simple example how annealing strategy
works in GANs. The theoretical analysis is supported by simple simulations.",['Arash Mehrjou'],"['stat.ML', 'cs.LG']",2018-03-13 21:06:58+00:00
http://arxiv.org/abs/1803.05036v1,Variational zero-inflated Gaussian processes with sparse kernels,"Zero-inflated datasets, which have an excess of zero outputs, are commonly
encountered in problems such as climate or rare event modelling. Conventional
machine learning approaches tend to overestimate the non-zeros leading to poor
performance. We propose a novel model family of zero-inflated Gaussian
processes (ZiGP) for such zero-inflated datasets, produced by sparse kernels
through learning a latent probit Gaussian process that can zero out kernel rows
and columns whenever the signal is absent. The ZiGPs are particularly useful
for making the powerful Gaussian process networks more interpretable. We
introduce sparse GP networks where variable-order latent modelling is achieved
through sparse mixing signals. We derive the non-trivial stochastic variational
inference tractably for scalable learning of the sparse kernels in both models.
The novel output-sparse approach improves both prediction of zero-inflated data
and interpretability of latent mixing models.","['Pashupati Hegde', 'Markus Heinonen', 'Samuel Kaski']",['stat.ML'],2018-03-13 20:34:23+00:00
http://arxiv.org/abs/1803.05011v1,A Probabilistic Disease Progression Model for Predicting Future Clinical Outcome,"In this work, we consider the problem of predicting the course of a
progressive disease, such as cancer or Alzheimer's. Progressive diseases often
start with mild symptoms that might precede a diagnosis, and each patient
follows their own trajectory. Patient trajectories exhibit wild variability,
which can be associated with many factors such as genotype, age, or sex. An
additional layer of complexity is that, in real life, the amount and type of
data available for each patient can differ significantly. For example, for one
patient we might have no prior history, whereas for another patient we might
have detailed clinical assessments obtained at multiple prior time-points. This
paper presents a probabilistic model that can handle multiple modalities
(including images and clinical assessments) and variable patient histories with
irregular timings and missing entries, to predict clinical scores at future
time-points. We use a sigmoidal function to model latent disease progression,
which gives rise to clinical observations in our generative model. We
implemented an approximate Bayesian inference strategy on the proposed model to
estimate the parameters on data from a large population of subjects.
Furthermore, the Bayesian framework enables the model to automatically
fine-tune its predictions based on historical observations that might be
available on the test subject. We applied our method to a longitudinal
Alzheimer's disease dataset with more than 3000 subjects [23] and present a
detailed empirical analysis of prediction performance under different
scenarios, with comparisons against several benchmarks. We also demonstrate how
the proposed model can be interrogated to glean insights about temporal
dynamics in Alzheimer's disease.","['Yingying Zhu', 'Mert R. Sabuncu']","['cs.LG', 'cs.CV', 'stat.ML']",2018-03-13 19:05:08+00:00
http://arxiv.org/abs/1803.04929v5,Structural Agnostic Modeling: Adversarial Learning of Causal Graphs,"A new causal discovery method, Structural Agnostic Modeling (SAM), is
presented in this paper. Leveraging both conditional independencies and
distributional asymmetries, SAM aims to find the underlying causal structure
from observational data. The approach is based on a game between different
players estimating each variable distribution conditionally to the others as a
neural net, and an adversary aimed at discriminating the generated data against
the original data. A learning criterion combining distribution estimation,
sparsity and acyclicity constraints is used to enforce the optimization of the
graph structure and parameters through stochastic gradient descent. SAM is
extensively experimentally validated on synthetic and real data.","['Diviyan Kalainathan', 'Olivier Goudet', 'Isabelle Guyon', 'David Lopez-Paz', 'Mich√®le Sebag']",['stat.ML'],2018-03-13 16:40:00+00:00
http://arxiv.org/abs/1803.04926v3,Active Reinforcement Learning with Monte-Carlo Tree Search,"Active Reinforcement Learning (ARL) is a twist on RL where the agent observes
reward information only if it pays a cost. This subtle change makes exploration
substantially more challenging. Powerful principles in RL like optimism,
Thompson sampling, and random exploration do not help with ARL. We relate ARL
in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm
using Monte-Carlo Tree Search that is asymptotically Bayes optimal.
Experimentally, this algorithm is near-optimal on small Bandit problems and
MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised
heuristics for ARL. By analysing exploration behaviour in detail, we uncover
obstacles to scaling up simulation-based algorithms for ARL.","['Sebastian Schulze', 'Owain Evans']","['cs.LG', 'stat.ML']",2018-03-13 16:35:25+00:00
http://arxiv.org/abs/1803.04924v2,Dense Limit of the Dawid-Skene Model for Crowdsourcing and Regions of Sub-optimality of Message Passing Algorithms,"Crowdsourcing is a strategy to categorize data through the contribution of
many individuals. A wide range of theoretical and algorithmic contributions are
based on the model of Dawid and Skene [1]. Recently it was shown in [2,3] that,
in certain regimes, belief propagation is asymptotically optimal for data
generated from the Dawid-Skene model. This paper is motivated by this recent
progress. We analyze the dense limit of the Dawid-Skene model. It is shown that
it belongs to a larger class of low-rank matrix estimation problems for which
it is possible to express the asymptotic, Bayes-optimal, performance in a
simple closed form. In the dense limit the mapping to a low-rank matrix
estimation problem provides an approximate message passing algorithm that
solves the problem algorithmically. We identify the regions where the algorithm
efficiently computes the Bayes-optimal estimates. Our analysis refines the
results of [2,3] about optimality of message passing algorithms by
characterizing regions of parameters where these algorithms do not match the
Bayes-optimal performance. We further study numerically the performance of
approximate message passing, derived in the dense limit, on sparse instances
and carry out experiments on a real world dataset.","['Christian Schmidt', 'Lenka Zdeborov√°']","['stat.ML', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'physics.data-an']",2018-03-13 16:31:37+00:00
http://arxiv.org/abs/1803.05337v1,Learning to Recognize Musical Genre from Audio,"We here summarize our experience running a challenge with open data for
musical genre recognition. Those notes motivate the task and the challenge
design, show some statistics about the submissions, and present the results.","['Micha√´l Defferrard', 'Sharada P. Mohanty', 'Sean F. Carroll', 'Marcel Salath√©']","['cs.SD', 'cs.IR', 'cs.LG', 'eess.AS', 'stat.ML']",2018-03-13 15:58:58+00:00
http://arxiv.org/abs/1803.04899v3,Optimal Transport for Multi-source Domain Adaptation under Target Shift,"In this paper, we propose to tackle the problem of reducing discrepancies
between multiple domains referred to as multi-source domain adaptation and
consider it under the target shift assumption: in all domains we aim to solve a
classification problem with the same output classes, but with labels'
proportions differing across them. This problem, generally ignored in the vast
majority papers on domain adaptation papers, is nevertheless critical in
real-world applications, and we theoretically show its impact on the adaptation
success. To address this issue, we design a method based on optimal transport,
a theory that has been successfully used to tackle adaptation problems in
machine learning. Our method performs multi-source adaptation and target shift
correction simultaneously by learning the class probabilities of the unlabeled
target sample and the coupling allowing to align two (or more) probability
distributions. Experiments on both synthetic and real-world data related to
satellite image segmentation task show the superiority of the proposed method
over the state-of-the-art.","['Ievgen Redko', 'Nicolas Courty', 'R√©mi Flamary', 'Devis Tuia']",['stat.ML'],2018-03-13 15:55:35+00:00
http://arxiv.org/abs/1803.04837v4,Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction,"The availability of a large amount of electronic health records (EHR)
provides huge opportunities to improve health care service by mining these
data. One important application is clinical endpoint prediction, which aims to
predict whether a disease, a symptom or an abnormal lab test will happen in the
future according to patients' history records. This paper develops deep
learning techniques for clinical endpoint prediction, which are effective in
many practical applications. However, the problem is very challenging since
patients' history records contain multiple heterogeneous temporal events such
as lab tests, diagnosis, and drug administrations. The visiting patterns of
different types of events vary significantly, and there exist complex nonlinear
relationships between different events. In this paper, we propose a novel model
for learning the joint representation of heterogeneous temporal events. The
model adds a new gate to control the visiting rates of different events which
effectively models the irregular patterns of different events and their
nonlinear correlations. Experiment results with real-world clinical data on the
tasks of predicting death and abnormal lab tests prove the effectiveness of our
proposed approach over competitive baselines.","['Luchen Liu', 'Jianhao Shen', 'Ming Zhang', 'Zichang Wang', 'Jian Tang']","['cs.AI', 'cs.LG', 'stat.ML']",2018-03-13 14:32:38+00:00
http://arxiv.org/abs/1803.04825v1,Low-Rank Boolean Matrix Approximation by Integer Programming,"Low-rank approximations of data matrices are an important dimensionality
reduction tool in machine learning and regression analysis. We consider the
case of categorical variables, where it can be formulated as the problem of
finding low-rank approximations to Boolean matrices. In this paper we give what
is to the best of our knowledge the first integer programming formulation that
relies on only polynomially many variables and constraints, we discuss how to
solve it computationally and report numerical tests on synthetic and real-world
data.","['Reka Kovacs', 'Oktay Gunluk', 'Raphael Hauser']","['cs.LG', 'cs.DM', 'stat.ML']",2018-03-13 14:17:00+00:00
http://arxiv.org/abs/1803.04765v1,"Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning","Deep neural networks (DNNs) enable innovative applications of machine
learning like image recognition, machine translation, or malware detection.
However, deep learning is often criticized for its lack of robustness in
adversarial settings (e.g., vulnerability to adversarial inputs) and general
inability to rationalize its predictions. In this work, we exploit the
structure of deep learning to enable new learning-based inference and decision
strategies that achieve desirable properties such as robustness and
interpretability. We take a first step in this direction and introduce the Deep
k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest
neighbors algorithm with representations of the data learned by each layer of
the DNN: a test input is compared to its neighboring training points according
to the distance that separates them in the representations. We show the labels
of these neighboring points afford confidence estimates for inputs outside the
model's training manifold, including on malicious inputs like adversarial
examples--and therein provides protections against inputs that are outside the
models understanding. This is because the nearest neighbors can be used to
estimate the nonconformity of, i.e., the lack of support for, a prediction in
the training data. The neighbors also constitute human-interpretable
explanations of predictions. We evaluate the DkNN algorithm on several
datasets, and show the confidence estimates accurately identify inputs outside
the model, and that the explanations provided by nearest neighbors are
intuitive and useful in understanding model failures.","['Nicolas Papernot', 'Patrick McDaniel']","['cs.LG', 'stat.ML']",2018-03-13 13:02:13+00:00
http://arxiv.org/abs/1803.05307v1,Deep CNN based feature extractor for text-prompted speaker recognition,"Deep learning is still not a very common tool in speaker verification field.
We study deep convolutional neural network performance in the text-prompted
speaker verification task. The prompted passphrase is segmented into word
states - i.e. digits -to test each digit utterance separately. We train a
single high-level feature extractor for all states and use cosine similarity
metric for scoring. The key feature of our network is the Max-Feature-Map
activation function, which acts as an embedded feature selector. By using
multitask learning scheme to train the high-level feature extractor we were
able to surpass the classic baseline systems in terms of quality and achieved
impressive results for such a novice approach, getting 2.85% EER on the RSR2015
evaluation set. Fusion of the proposed and the baseline systems improves this
result.","['Sergey Novoselov', 'Oleg Kudashev', 'Vadim Schemelinin', 'Ivan Kremnev', 'Galina Lavrentyeva']","['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD', 'stat.ML']",2018-03-13 10:59:24+00:00
http://arxiv.org/abs/1803.04674v1,Hierarchical Reinforcement Learning: Approximating Optimal Discounted TSP Using Local Policies,"In this work, we provide theoretical guarantees for reward decomposition in
deterministic MDPs. Reward decomposition is a special case of Hierarchical
Reinforcement Learning, that allows one to learn many policies in parallel and
combine them into a composite solution. Our approach builds on mapping this
problem into a Reward Discounted Traveling Salesman Problem, and then deriving
approximate solutions for it. In particular, we focus on approximate solutions
that are local, i.e., solutions that only observe information about the current
state. Local policies are easy to implement and do not require substantial
computational resources as they do not perform planning. While local
deterministic policies, like Nearest Neighbor, are being used in practice for
hierarchical reinforcement learning, we propose three stochastic policies that
guarantee better performance than any deterministic policy.","['Tom Zahavy', 'Avinatan Hasidim', 'Haim Kaplan', 'Yishay Mansour']","['cs.LG', 'cs.AI', 'stat.ML']",2018-03-13 08:13:11+00:00
http://arxiv.org/abs/1803.04967v1,Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection,"Deep learning has recently demonstrated state-of-the art performance on key
tasks related to the maintenance of computer systems, such as intrusion
detection, denial of service attack detection, hardware and software system
failures, and malware detection. In these contexts, model interpretability is
vital for administrator and analyst to trust and act on the automated analysis
of machine learning models. Deep learning methods have been criticized as black
box oracles which allow limited insight into decision factors. In this work we
seek to ""bridge the gap"" between the impressive performance of deep learning
models and the need for interpretable model introspection. To this end we
present recurrent neural network (RNN) language models augmented with attention
for anomaly detection in system logs. Our methods are generally applicable to
any computer system and logging source.
  By incorporating attention variants into our RNN language models we create
opportunities for model introspection and analysis without sacrificing
state-of-the art performance.
  We demonstrate model performance and illustrate model interpretability on an
intrusion detection task using the Los Alamos National Laboratory (LANL) cyber
security dataset, reporting upward of 0.99 area under the receiver operator
characteristic curve despite being trained only on a single day's worth of
data.","['Andy Brown', 'Aaron Tuor', 'Brian Hutchinson', 'Nicole Nichols']","['cs.LG', 'cs.NE', 'stat.ML']",2018-03-13 08:09:20+00:00
http://arxiv.org/abs/1803.04665v1,Pure Exploration in Infinitely-Armed Bandit Models with Fixed-Confidence,"We consider the problem of near-optimal arm identification in the fixed
confidence setting of the infinitely armed bandit problem when nothing is known
about the arm reservoir distribution. We (1) introduce a PAC-like framework
within which to derive and cast results; (2) derive a sample complexity lower
bound for near-optimal arm identification; (3) propose an algorithm that
identifies a nearly-optimal arm with high probability and derive an upper bound
on its sample complexity which is within a log factor of our lower bound; and
(4) discuss whether our log^2(1/delta) dependence is inescapable for
""two-phase"" (select arms first, identify the best later) algorithms in the
infinite setting. This work permits the application of bandit models to a
broader class of problems where fewer assumptions hold.","['Maryam Aziz', 'Jesse Anderton', 'Emilie Kaufmann', 'Javed Aslam']","['stat.ML', 'cs.LG']",2018-03-13 07:36:31+00:00
http://arxiv.org/abs/1803.04663v1,Binary Matrix Completion Using Unobserved Entries,"A matrix completion problem, which aims to recover a complete matrix from its
partial observations, is one of the important problems in the machine learning
field and has been studied actively. However, there is a discrepancy between
the mainstream problem setting, which assumes continuous-valued observations,
and some practical applications such as recommendation systems and SNS link
predictions where observations take discrete or even binary values. To cope
with this problem, Davenport et al. (2014) proposed a binary matrix completion
(BMC) problem, where observations are quantized into binary values. Hsieh et
al. (2015) proposed a PU (Positive and Unlabeled) matrix completion problem,
which is an extension of the BMC problem. This problem targets the setting
where we cannot observe negative values, such as SNS link predictions. In the
construction of their method for this setting, they introduced a methodology of
the classification problem, regarding each matrix entry as a sample. Their
risk, which defines losses over unobserved entries as well, indicates the
possibility of the use of unobserved entries. In this paper, motivated by a
semi-supervised classification method recently proposed by Sakai et al. (2017),
we develop a method for the BMC problem which can use all of positive,
negative, and unobserved entries, by combining the risks of Davenport et al.
(2014) and Hsieh et al. (2015). To the best of our knowledge, this is the first
BMC method which exploits all kinds of matrix entries. We experimentally show
that an appropriate mixture of risks improves the performance.","['Masayoshi Hayashi', 'Tomoya Sakai', 'Masashi Sugiyama']","['stat.ML', 'cs.LG']",2018-03-13 07:26:30+00:00
http://arxiv.org/abs/1803.04654v1,Simulation and Calibration of a Fully Bayesian Marked Multidimensional Hawkes Process with Dissimilar Decays,"We propose a simulation method for multidimensional Hawkes processes based on
superposition theory of point processes. This formulation allows us to design
efficient simulations for Hawkes processes with differing exponentially
decaying intensities. We demonstrate that inter-arrival times can be decomposed
into simpler auxiliary variables that can be sampled directly, giving exact
simulation with no approximation. We establish that the auxiliary variables
provides information on the parent process for each event time. The algorithm
correctness is shown by verifying the simulated intensities with their
theoretical moments. A modular inference procedure consisting of Gibbs samplers
through the auxiliary variable augmentation and adaptive rejection sampling is
presented. Finally, we compare our proposed simulation method against existing
methods, and find significant improvement in terms of algorithm speed. Our
inference algorithm is used to discover the strengths of mutually excitations
in real dark networks.","['Kar Wai Lim', 'Young Lee', 'Leif Hanlen', 'Hongbiao Zhao']",['stat.ML'],2018-03-13 06:44:56+00:00
http://arxiv.org/abs/1803.04585v4,Categorizing Variants of Goodhart's Law,"There are several distinct failure modes for overoptimization of systems on
the basis of metrics. This occurs when a metric which can be used to improve a
system is used to an extent that further optimization is ineffective or
harmful, and is sometimes termed Goodhart's Law. This class of failure is often
poorly understood, partly because terminology for discussing them is ambiguous,
and partly because discussion using this ambiguous terminology ignores
distinctions between different failure modes of this general type. This paper
expands on an earlier discussion by Garrabrant, which notes there are ""(at
least) four different mechanisms"" that relate to Goodhart's Law. This paper is
intended to explore these mechanisms further, and specify more clearly how they
occur. This discussion should be helpful in better understanding these types of
failures in economic regulation, in public policy, in machine learning, and in
Artificial Intelligence alignment. The importance of Goodhart effects depends
on the amount of power directed towards optimizing the proxy, and so the
increased optimization power offered by artificial intelligence makes it
especially critical for that field.","['David Manheim', 'Scott Garrabrant']","['cs.AI', 'q-fin.GN', 'stat.ML', '91E45']",2018-03-13 01:15:39+00:00
http://arxiv.org/abs/1803.04965v1,Coregionalised Locomotion Envelopes - A Qualitative Approach,"'Sharing of statistical strength' is a phrase often employed in machine
learning and signal processing. In sensor networks, for example, missing
signals from certain sensors may be predicted by exploiting their correlation
with observed signals acquired from other sensors. For humans, our hands move
synchronously with our legs, and we can exploit these implicit correlations for
predicting new poses and for generating new natural-looking walking sequences.
We can also go much further and exploit this form of transfer learning, to
develop new control schemas for robust control of rehabilitation robots. In
this short paper we introduce coregionalised locomotion envelopes - a method
for multi-dimensional manifold regression, on human locomotion variates. Herein
we render a qualitative description of this method.","['Neil Dhir', 'Houman Dallali', 'Mo Rastgaar']","['stat.ML', 'cs.RO']",2018-03-13 00:04:40+00:00
http://arxiv.org/abs/1803.04572v2,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.","['Ardavan Afshar', 'Ioakeim Perros', 'Evangelos E. Papalexakis', 'Elizabeth Searles', 'Joyce Ho', 'Jimeng Sun']","['cs.LG', 'stat.ML']",2018-03-12 23:27:06+00:00
http://arxiv.org/abs/1803.04566v2,Compact Convolutional Neural Networks for Classification of Asynchronous Steady-state Visual Evoked Potentials,"Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from
the parietal and occipital regions of the brain that are evoked from flickering
visual stimuli. SSVEPs are robust signals measurable in the
electroencephalogram (EEG) and are commonly used in brain-computer interfaces
(BCIs). However, methods for high-accuracy decoding of SSVEPs usually require
hand-crafted approaches that leverage domain-specific knowledge of the stimulus
signals, such as specific temporal frequencies in the visual stimuli and their
relative spatial arrangement. When this knowledge is unavailable, such as when
SSVEP signals are acquired asynchronously, such approaches tend to fail. In
this paper, we show how a compact convolutional neural network (Compact-CNN),
which only requires raw EEG signals for automatic feature extraction, can be
used to decode signals from a 12-class SSVEP dataset without the need for any
domain-specific knowledge or calibration data. We report across subject mean
accuracy of approximately 80% (chance being 8.3%) and show this is
substantially better than current state-of-the-art hand-crafted approaches
using canonical correlation analysis (CCA) and Combined-CCA. Furthermore, we
analyze our Compact-CNN to examine the underlying feature representation,
discovering that the deep learner extracts additional phase and amplitude
related features associated with the structure of the dataset. We discuss how
our Compact-CNN shows promise for BCI applications that allow users to freely
gaze/attend to any stimulus at any time (e.g., asynchronous BCI) as well as
provides a method for analyzing SSVEP signals in a way that might augment our
understanding about the basic processing in the visual cortex.","['Nicholas R. Waytowich', 'Vernon Lawhern', 'Javier O. Garcia', 'Jennifer Cummings', 'Josef Faller', 'Paul Sajda', 'Jean M. Vettel']","['cs.LG', 'q-bio.NC', 'stat.ML']",2018-03-12 23:03:44+00:00
http://arxiv.org/abs/1803.04547v2,Analysis of spectral clustering algorithms for community detection: the general bipartite setting,"We consider spectral clustering algorithms for community detection under a
general bipartite stochastic block model (SBM). A modern spectral clustering
algorithm consists of three steps: (1) regularization of an appropriate
adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a
k-means type algorithm in the reduced spectral domain. We focus on the
adjacency-based spectral clustering and for the first step, propose a new
data-driven regularization that can restore the concentration of the adjacency
matrix even for the sparse networks. This result is based on recent work on
regularization of random binary matrices, but avoids using unknown population
level parameters, and instead estimates the necessary quantities from the data.
We also propose and study a novel variation of the spectral truncation step and
show how this variation changes the nature of the misclassification rate in a
general SBM. We then show how the consistency results can be extended to models
beyond SBMs, such as inhomogeneous random graph models with approximate
clusters, including a graphon clustering problem, as well as general
sub-Gaussian biclustering. A theme of the paper is providing a better
understanding of the analysis of spectral methods for community detection and
establishing consistency results, under fairly general clustering models and
for a wide regime of degree growths, including sparse cases where the average
expected degree grows arbitrarily slowly.","['Zhixin Zhou', 'Arash A. Amini']","['math.ST', 'cs.SI', 'stat.ML', 'stat.TH']",2018-03-12 21:50:58+00:00
http://arxiv.org/abs/1803.04494v1,Gradient Augmented Information Retrieval with Autoencoders and Semantic Hashing,"This paper will explore the use of autoencoders for semantic hashing in the
context of Information Retrieval. This paper will summarize how to efficiently
train an autoencoder in order to create meaningful and low-dimensional
encodings of data. This paper will demonstrate how computing and storing the
closest encodings to an input query can help speed up search time and improve
the quality of our search results. The novel contributions of this paper
involve using the representation of the data learned by an auto-encoder in
order to augment our search query in various ways. I present and evaluate the
new gradient search augmentation (GSA) approach, as well as the more well-known
pseudo-relevance-feedback (PRF) adjustment. I find that GSA helps to improve
the performance of the TF-IDF based information retrieval system, and PRF
combined with GSA works best overall for the systems compared in this paper.",['Sean Billings'],"['cs.IR', 'cs.LG', 'stat.ML']",2018-03-12 19:49:30+00:00
http://arxiv.org/abs/1803.04489v1,Probabilistic and Regularized Graph Convolutional Networks,"This paper explores the recently proposed Graph Convolutional Network
architecture proposed in (Kipf & Welling, 2016) The key points of their work is
summarized and their results are reproduced. Graph regularization and
alternative graph convolution approaches are explored. I find that explicit
graph regularization was correctly rejected by (Kipf & Welling, 2016). I
attempt to improve the performance of GCN by approximating a k-step transition
matrix in place of the normalized graph laplacian, but I fail to find positive
results. Nonetheless, the performance of several configurations of this GCN
variation is shown for the Cora, Citeseer, and Pubmed datasets.",['Sean Billings'],"['cs.LG', 'stat.ML']",2018-03-12 19:47:16+00:00
http://arxiv.org/abs/1803.04479v1,Machine Learning Harnesses Molecular Dynamics to Discover New $Œº$ Opioid Chemotypes,"Computational chemists typically assay drug candidates by virtually screening
compounds against crystal structures of a protein despite the fact that some
targets, like the $\mu$ Opioid Receptor and other members of the GPCR family,
traverse many non-crystallographic states. We discover new conformational
states of $\mu OR$ with molecular dynamics simulation and then machine learn
ligand-structure relationships to predict opioid ligand function. These
artificial intelligence models identified a novel $\mu$ opioid chemotype.","['Evan N. Feinberg', 'Amir Barati Farimani', 'Rajendra Uprety', 'Amanda Hunkele', 'Gavril W. Pasternak', 'Susruta Majumdar', 'Vijay S. Pande']","['q-bio.BM', 'stat.ML']",2018-03-12 19:32:21+00:00
http://arxiv.org/abs/1803.04475v1,Accuracy-Reliability Cost Function for Empirical Variance Estimation,"In this paper we focus on the problem of assigning uncertainties to
single-point predictions. We introduce a cost function that encodes the
trade-off between accuracy and reliability in probabilistic forecast. We derive
analytic formula for the case of forecasts of continuous scalar variables
expressed in terms of Gaussian distributions. The Accuracy-Reliability cost
function can be used to empirically estimate the variance in heteroskedastic
regression problems (input dependent noise), by solving a two-objective
optimization problem. The simple philosophy behind this strategy is that
predictions based on the estimated variances should be both accurate and
reliable (i.e. statistical consistent with observations). We show several
examples with synthetic data, where the underlying hidden noise function can be
accurately recovered, both in one and multi-dimensional problems. The practical
implementation of the method has been done using a Neural Network and, in the
one-dimensional case, with a simple polynomial fit.",['Enrico Camporeale'],"['stat.ML', 'cs.LG']",2018-03-12 19:24:37+00:00
http://arxiv.org/abs/1803.04464v2,False Discovery Rate Control via Debiased Lasso,"We consider the problem of variable selection in high-dimensional statistical
models where the goal is to report a set of variables, out of many predictors
$X_1, \dotsc, X_p$, that are relevant to a response of interest. For linear
high-dimensional model, where the number of parameters exceeds the number of
samples $(p>n)$, we propose a procedure for variables selection and prove that
it controls the ""directional"" false discovery rate (FDR) below a pre-assigned
significance level $q\in [0,1]$. We further analyze the statistical power of
our framework and show that for designs with subgaussian rows and a common
precision matrix $\Omega\in\mathbb{R}^{p\times p}$, if the minimum nonzero
parameter $\theta_{\min}$ satisfies $$\sqrt{n} \theta_{\min} - \sigma
\sqrt{2(\max_{i\in [p]}\Omega_{ii})\log\left(\frac{2p}{qs_0}\right)} \to
\infty\,,$$ then this procedure achieves asymptotic power one. Our framework is
built upon the debiasing approach and assumes the standard condition $s_0 =
o(\sqrt{n}/(\log p)^2)$, where $s_0$ indicates the number of true positives
among the $p$ features. Notably, this framework achieves exact directional FDR
control without any assumption on the amplitude of unknown regression
parameters, and does not require any knowledge of the distribution of
covariates or the noise level. We test our method in synthetic and real data
experiments to assess its performance and to corroborate our theoretical
results.","['Adel Javanmard', 'Hamid Javadi']","['stat.ME', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2018-03-12 19:03:33+00:00
http://arxiv.org/abs/1803.04431v1,Scalable Algorithms for Learning High-Dimensional Linear Mixed Models,"Linear mixed models (LMMs) are used extensively to model dependecies of
observations in linear regression and are used extensively in many application
areas. Parameter estimation for LMMs can be computationally prohibitive on big
data. State-of-the-art learning algorithms require computational complexity
which depends at least linearly on the dimension $p$ of the covariates, and
often use heuristics that do not offer theoretical guarantees. We present
scalable algorithms for learning high-dimensional LMMs with sublinear
computational complexity dependence on $p$. Key to our approach are novel dual
estimators which use only kernel functions of the data, and fast computational
techniques based on the subsampled randomized Hadamard transform. We provide
theoretical guarantees for our learning algorithms, demonstrating the
robustness of parameter estimation. Finally, we complement the theory with
experiments on large synthetic and real data.","['Zilong Tan', 'Kimberly Roche', 'Xiang Zhou', 'Sayan Mukherjee']","['stat.ML', 'cs.LG']",2018-03-12 18:07:40+00:00
http://arxiv.org/abs/1803.04386v2,Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches,"Stochastic neural net weights are used in a variety of contexts, including
regularization, Bayesian neural nets, exploration in reinforcement learning,
and evolution strategies. Unfortunately, due to the large number of weights,
all the examples in a mini-batch typically share the same weight perturbation,
thereby limiting the variance reduction effect of large mini-batches. We
introduce flipout, an efficient method for decorrelating the gradients within a
mini-batch by implicitly sampling pseudo-independent weight perturbations for
each example. Empirically, flipout achieves the ideal linear variance reduction
for fully connected networks, convolutional networks, and RNNs. We find
significant speedups in training neural networks with multiplicative Gaussian
perturbations. We show that flipout is effective at regularizing LSTMs, and
outperforms previous methods. Flipout also enables us to vectorize evolution
strategies: in our experiments, a single GPU with flipout can handle the same
throughput as at least 40 CPU cores using existing methods, equivalent to a
factor-of-4 cost reduction on Amazon Web Services.","['Yeming Wen', 'Paul Vicol', 'Jimmy Ba', 'Dustin Tran', 'Roger Grosse']","['cs.LG', 'stat.ML']",2018-03-12 17:25:21+00:00
http://arxiv.org/abs/1803.04383v2,Delayed Impact of Fair Machine Learning,"Fairness in machine learning has predominantly been studied in static
classification settings without concern for how decisions change the underlying
population over time. Conventional wisdom suggests that fairness criteria
promote the long-term well-being of those groups they aim to protect.
  We study how static fairness criteria interact with temporal indicators of
well-being, such as long-term improvement, stagnation, and decline in a
variable of interest. We demonstrate that even in a one-step feedback model,
common fairness criteria in general do not promote improvement over time, and
may in fact cause harm in cases where an unconstrained objective would not.
  We completely characterize the delayed impact of three standard criteria,
contrasting the regimes in which these exhibit qualitatively different
behavior. In addition, we find that a natural form of measurement error
broadens the regime in which fairness criteria perform favorably.
  Our results highlight the importance of measurement and temporal modeling in
the evaluation of fairness criteria, suggesting a range of new challenges and
trade-offs.","['Lydia T. Liu', 'Sarah Dean', 'Esther Rolf', 'Max Simchowitz', 'Moritz Hardt']","['cs.LG', 'stat.ML']",2018-03-12 17:20:56+00:00
http://arxiv.org/abs/1803.04371v2,Optimal Rates of Sketched-regularized Algorithms for Least-Squares Regression over Hilbert Spaces,"We investigate regularized algorithms combining with projection for
least-squares regression problem over a Hilbert space, covering nonparametric
regression over a reproducing kernel Hilbert space. We prove convergence
results with respect to variants of norms, under a capacity assumption on the
hypothesis space and a regularity condition on the target function. As a
result, we obtain optimal rates for regularized algorithms with randomized
sketches, provided that the sketch dimension is proportional to the effective
dimension up to a logarithmic factor. As a byproduct, we obtain similar results
for Nystr\""{o}m regularized algorithms. Our results are the first ones with
optimal, distribution-dependent rates that do not have any saturation effect
for sketched/Nystr\""{o}m regularized algorithms, considering both the
attainable and non-attainable cases.","['Junhong Lin', 'Volkan Cevher']","['stat.ML', 'cs.LG', 'math.FA']",2018-03-12 16:57:48+00:00
http://arxiv.org/abs/1803.04347v1,Classifying Online Dating Profiles on Tinder using FaceNet Facial Embeddings,"A method to produce personalized classification models to automatically
review online dating profiles on Tinder is proposed, based on the user's
historical preference. The method takes advantage of a FaceNet facial
classification model to extract features which may be related to facial
attractiveness. The embeddings from a FaceNet model were used as the features
to describe an individual's face. A user reviewed 8,545 online dating profiles.
For each reviewed online dating profile, a feature set was constructed from the
profile images which contained just one face. Two approaches are presented to
go from the set of features for each face, to a set of profile features. A
simple logistic regression trained on the embeddings from just 20 profiles
could obtain a 65% validation accuracy. A point of diminishing marginal returns
was identified to occur around 80 profiles, at which the model accuracy of 73%
would only improve marginally after reviewing a significant number of
additional profiles.","['Charles F Jekel', 'Raphael T. Haftka']","['cs.CV', 'cs.SI', 'eess.IV', 'stat.ML']",2018-03-12 16:14:24+00:00
http://arxiv.org/abs/1803.04304v1,Representation Learning and Recovery in the ReLU Model,"Rectified linear units, or ReLUs, have become the preferred activation
function for artificial neural networks. In this paper we consider two basic
learning problems assuming that the underlying data follow a generative model
based on a ReLU-network -- a neural network with ReLU activations. As a
primarily theoretical study, we limit ourselves to a single-layer network. The
first problem we study corresponds to dictionary-learning in the presence of
nonlinearity (modeled by the ReLU functions). Given a set of observation
vectors $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, we aim to recover
$d\times k$ matrix $A$ and the latent vectors $\{\mathbf{c}^i\} \subset
\mathbb{R}^k$ under the model $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i
+\mathbf{b})$, where $\mathbf{b}\in \mathbb{R}^d$ is a random bias. We show
that it is possible to recover the column space of $A$ within an error of
$O(d)$ (in Frobenius norm) under certain conditions on the probability
distribution of $\mathbf{b}$.
  The second problem we consider is that of robust recovery of the signal in
the presence of outliers, i.e., large but sparse noise. In this setting we are
interested in recovering the latent vector $\mathbf{c}$ from its noisy
nonlinear sketches of the form $\mathbf{v} = \mathrm{ReLU}(A\mathbf{c}) +
\mathbf{e}+\mathbf{w}$, where $\mathbf{e} \in \mathbb{R}^d$ denotes the
outliers with sparsity $s$ and $\mathbf{w} \in \mathbb{R}^d$ denote the dense
but small noise. This line of work has recently been studied (Soltanolkotabi,
2017) without the presence of outliers. For this problem, we show that a
generalized LASSO algorithm is able to recover the signal $\mathbf{c} \in
\mathbb{R}^k$ within an $\ell_2$ error of $O(\sqrt{\frac{(k+s)\log d}{d}})$
when $A$ is a random Gaussian matrix.","['Arya Mazumdar', 'Ankit Singh Rawat']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2018-03-12 15:17:14+00:00
http://arxiv.org/abs/1803.04303v1,Learning unknown ODE models with Gaussian processes,"In conventional ODE modelling coefficients of an equation driving the system
state forward in time are estimated. However, for many complex systems it is
practically impossible to determine the equations or interactions governing the
underlying dynamics. In these settings, parametric ODE model cannot be
formulated. Here, we overcome this issue by introducing a novel paradigm of
nonparametric ODE modelling that can learn the underlying dynamics of arbitrary
continuous-time systems without prior knowledge. We propose to learn
non-linear, unknown differential functions from state observations using
Gaussian process vector fields within the exact ODE formalism. We demonstrate
the model's capabilities to infer dynamics from sparse data and to simulate the
system forward into future.","['Markus Heinonen', 'Cagatay Yildiz', 'Henrik Mannerstr√∂m', 'Jukka Intosalmi', 'Harri L√§hdesm√§ki']",['stat.ML'],2018-03-12 15:13:27+00:00
http://arxiv.org/abs/1803.04300v2,Neural Conditional Gradients,"The move from hand-designed to learned optimizers in machine learning has
been quite successful for gradient-based and -free optimizers. When facing a
constrained problem, however, maintaining feasibility typically requires a
projection step, which might be computationally expensive and not
differentiable. We show how the design of projection-free convex optimization
algorithms can be cast as a learning problem based on Frank-Wolfe Networks:
recurrent networks implementing the Frank-Wolfe algorithm aka. conditional
gradients. This allows them to learn to exploit structure when, e.g.,
optimizing over rank-1 matrices. Our LSTM-learned optimizers outperform
hand-designed as well learned but unconstrained ones. We demonstrate this for
training support vector machines and softmax classifiers.","['Patrick Schramowski', 'Christian Bauckhage', 'Kristian Kersting']","['cs.LG', 'stat.ML']",2018-03-12 15:10:45+00:00
http://arxiv.org/abs/1803.04239v1,FeTa: A DCA Pruning Algorithm with Generalization Error Guarantees,"Recent DNN pruning algorithms have succeeded in reducing the number of
parameters in fully connected layers, often with little or no drop in
classification accuracy. However, most of the existing pruning schemes either
have to be applied during training or require a costly retraining procedure
after pruning to regain classification accuracy. We start by proposing a cheap
pruning algorithm for fully connected DNN layers based on difference of convex
functions (DC) optimisation, that requires little or no retraining. We then
provide a theoretical analysis for the growth in the Generalization Error (GE)
of a DNN for the case of bounded perturbations to the hidden layers, of which
weight pruning is a special case. Our pruning method is orders of magnitude
faster than competing approaches, while our theoretical analysis sheds light to
previously observed problems in DNN pruning. Experiments on commnon feedforward
neural networks validate our results.","['Konstantinos Pitas', 'Mike Davies', 'Pierre Vandergheynst']","['cs.LG', 'cs.NE', 'stat.ML']",2018-03-12 13:19:33+00:00
http://arxiv.org/abs/1803.04232v1,Variational Inference for Gaussian Process with Panel Count Data,"We present the first framework for Gaussian-process-modulated Poisson
processes when the temporal data appear in the form of panel counts. Panel
count data frequently arise when experimental subjects are observed only at
discrete time points and only the numbers of occurrences of the events between
subsequent observation times are available. The exact occurrence timestamps of
the events are unknown. The method of conducting the efficient variational
inference is presented, based on the assumption of a Gaussian-process-modulated
intensity function. We derive a tractable lower bound to alleviate the problems
of the intractable evidence lower bound inherent in the variational inference
framework. Our algorithm outperforms classical methods on both synthetic and
three real panel count sets.","['Hongyi Ding', 'Young Lee', 'Issei Sato', 'Masashi Sugiyama']",['stat.ML'],2018-03-12 13:02:20+00:00
http://arxiv.org/abs/1803.04223v1,Leveraging Crowdsourcing Data For Deep Active Learning - An Application: Learning Intents in Alexa,"This paper presents a generic Bayesian framework that enables any deep
learning model to actively learn from targeted crowds. Our framework inherits
from recent advances in Bayesian deep learning, and extends existing work by
considering the targeted crowdsourcing approach, where multiple annotators with
unknown expertise contribute an uncontrolled amount (often limited) of
annotations. Our framework leverages the low-rank structure in annotations to
learn individual annotator expertise, which then helps to infer the true labels
from noisy and sparse annotations. It provides a unified Bayesian model to
simultaneously infer the true labels and train the deep learning model in order
to reach an optimal learning efficacy. Finally, our framework exploits the
uncertainty of the deep learning model during prediction as well as the
annotators' estimated expertise to minimize the number of required annotations
and annotators for optimally training the deep learning model.
  We evaluate the effectiveness of our framework for intent classification in
Alexa (Amazon's personal assistant), using both synthetic and real-world
datasets. Experiments show that our framework can accurately learn annotator
expertise, infer true labels, and effectively reduce the amount of annotations
in model training as compared to state-of-the-art approaches. We further
discuss the potential of our proposed framework in bridging machine learning
and crowdsourcing towards improved human-in-the-loop systems.","['Jie Yang', 'Thomas Drake', 'Andreas Damianou', 'Yoelle Maarek']","['cs.LG', 'cs.SI', 'stat.ML']",2018-03-12 12:43:41+00:00
http://arxiv.org/abs/1803.04209v1,High Throughput Synchronous Distributed Stochastic Gradient Descent,"We introduce a new, high-throughput, synchronous, distributed, data-parallel,
stochastic-gradient-descent learning algorithm. This algorithm uses amortized
inference in a compute-cluster-specific, deep, generative, dynamical model to
perform joint posterior predictive inference of the mini-batch gradient
computation times of all worker-nodes in a parallel computing cluster. We show
that a synchronous parameter server can, by utilizing such a model, choose an
optimal cutoff time beyond which mini-batch gradient messages from slow workers
are ignored that maximizes overall mini-batch gradient computations per second.
In keeping with earlier findings we observe that, under realistic conditions,
eagerly discarding the mini-batch gradient computations of stragglers not only
increases throughput but actually increases the overall rate of convergence as
a function of wall-clock time by virtue of eliminating idleness. The principal
novel contribution and finding of this work goes beyond this by demonstrating
that using the predicted run-times from a generative model of cluster worker
performance to dynamically adjust the cutoff improves substantially over the
static-cutoff prior art, leading to, among other things, significantly reduced
deep neural net training times on large computer clusters.","['Michael Teng', 'Frank Wood']","['cs.DC', 'cs.LG', 'stat.ML']",2018-03-12 11:51:38+00:00
http://arxiv.org/abs/1803.04204v2,Semiparametric Contextual Bandits,"This paper studies semiparametric contextual bandits, a generalization of the
linear stochastic bandit problem where the reward for an action is modeled as a
linear function of known action features confounded by an non-linear
action-independent term. We design new algorithms that achieve
$\tilde{O}(d\sqrt{T})$ regret over $T$ rounds, when the linear function is
$d$-dimensional, which matches the best known bounds for the simpler
unconfounded case and improves on a recent result of Greenewald et al. (2017).
Via an empirical evaluation, we show that our algorithms outperform prior
approaches when there are non-linear confounding effects on the rewards.
Technically, our algorithms use a new reward estimator inspired by
doubly-robust approaches and our proofs require new concentration inequalities
for self-normalized martingales.","['Akshay Krishnamurthy', 'Zhiwei Steven Wu', 'Vasilis Syrgkanis']","['stat.ML', 'cs.LG']",2018-03-12 11:39:20+00:00
http://arxiv.org/abs/1803.04196v1,Multi-kernel Regression For Graph Signal Processing,"We develop a multi-kernel based regression method for graph signal processing
where the target signal is assumed to be smooth over a graph. In multi-kernel
regression, an effective kernel function is expressed as a linear combination
of many basis kernel functions. We estimate the linear weights to learn the
effective kernel function by appropriate regularization based on graph
smoothness. We show that the resulting optimization problem is shown to be
convex and pro- pose an accelerated projected gradient descent based solution.
Simulation results using real-world graph signals show efficiency of the
multi-kernel based approach over a standard kernel based approach.","['Arun Venkitaraman', 'Saikat Chatterjee', 'Peter H√§ndel']","['stat.ML', 'cs.LG']",2018-03-12 11:20:07+00:00
http://arxiv.org/abs/1803.04193v1,Extreme Learning Machine for Graph Signal Processing,"In this article, we improve extreme learning machines for regression tasks
using a graph signal processing based regularization. We assume that the target
signal for prediction or regression is a graph signal. With this assumption, we
use the regularization to enforce that the output of an extreme learning
machine is smooth over a given graph. Simulation results with real data confirm
that such regularization helps significantly when the available training data
is limited in size and corrupted by noise.","['Arun Venkitaraman', 'Saikat Chatterjee', 'Peter H√§ndel']","['stat.ML', 'cs.LG', 'eess.SP']",2018-03-12 11:12:48+00:00
http://arxiv.org/abs/1803.04189v3,Noise2Noise: Learning Image Restoration without Clean Data,"We apply basic statistical reasoning to signal reconstruction by machine
learning -- learning to map corrupted observations to clean signals -- with a
simple and powerful conclusion: it is possible to learn to restore images by
only looking at corrupted examples, at performance at and sometimes exceeding
training using clean data, without explicit image priors or likelihood models
of the corruption. In practice, we show that a single model learns photographic
noise removal, denoising synthetic Monte Carlo images, and reconstruction of
undersampled MRI scans -- all corrupted by different processes -- based on
noisy data only.","['Jaakko Lehtinen', 'Jacob Munkberg', 'Jon Hasselgren', 'Samuli Laine', 'Tero Karras', 'Miika Aittala', 'Timo Aila']","['cs.CV', 'cs.LG', 'stat.ML']",2018-03-12 11:07:58+00:00
http://arxiv.org/abs/1803.04186v1,"R3Net: Random Weights, Rectifier Linear Units and Robustness for Artificial Neural Network","We consider a neural network architecture with randomized features, a
sign-splitter, followed by rectified linear units (ReLU). We prove that our
architecture exhibits robustness to the input perturbation: the output feature
of the neural network exhibits a Lipschitz continuity in terms of the input
perturbation. We further show that the network output exhibits a discrimination
ability that inputs that are not arbitrarily close generate output vectors
which maintain distance between each other obeying a certain lower bound. This
ensures that two different inputs remain discriminable while contracting the
distance in the output feature space.","['Arun Venkitaraman', 'Alireza M. Javid', 'Saikat Chatterjee']","['stat.ML', 'cs.LG']",2018-03-12 11:04:17+00:00
http://arxiv.org/abs/1803.04087v3,Learning discrete Bayesian networks in polynomial time and sample complexity,"In this paper, we study the problem of structure learning for Bayesian
networks in which nodes take discrete values. The problem is NP-hard in general
but we show that under certain conditions we can recover the true structure of
a Bayesian network with sufficient number of samples. We develop a mathematical
model which does not assume any specific conditional probability distributions
for the nodes. We use a primal-dual witness construction to prove that, under
some technical conditions on the interaction between node pairs, we can do
exact recovery of the parents and children of a node by performing group
l_12-regularized multivariate regression. Thus, we recover the true Bayesian
network structure. If degree of a node is bounded then the sample complexity of
our proposed approach grows logarithmically with respect to the number of nodes
in the Bayesian network. Furthermore, our method runs in polynomial time.","['Adarsh Barik', 'Jean Honorio']","['cs.LG', 'stat.ML']",2018-03-12 01:49:39+00:00
http://arxiv.org/abs/1803.04084v1,Link prediction for egocentrically sampled networks,"Link prediction in networks is typically accomplished by estimating or
ranking the probabilities of edges for all pairs of nodes. In practice,
especially for social networks, the data are often collected by egocentric
sampling, which means selecting a subset of nodes and recording all of their
edges. This sampling mechanism requires different prediction tools than the
typical assumption of links missing at random. We propose a new computationally
efficient link prediction algorithm for egocentrically sampled networks, which
estimates the underlying probability matrix by estimating its row space. For
networks created by sampling rows, our method outperforms many popular link
prediction and graphon estimation techniques.","['Yun-Jhong Wu', 'Elizaveta Levina', 'Ji Zhu']","['stat.CO', 'cs.LG', 'stat.ML']",2018-03-12 01:37:53+00:00
