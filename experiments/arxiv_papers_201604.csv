id,title,abstract,authors,categories,date
http://arxiv.org/abs/1605.08062v2,A PAC RL Algorithm for Episodic POMDPs,"Many interesting real world domains involve reinforcement learning (RL) in
partially observable environments. Efficient learning in such domains is
important, but existing sample complexity bounds for partially observable RL
are at least exponential in the episode length. We give, to our knowledge, the
first partially observable RL algorithm with a polynomial bound on the number
of episodes on which the algorithm may not achieve near-optimal performance.
Our algorithm is suitable for an important class of episodic POMDPs. Our
approach builds on recent advances in method of moments for latent variable
model estimation.","['Zhaohan Daniel Guo', 'Shayan Doroudi', 'Emma Brunskill']","['cs.LG', 'cs.AI', 'stat.ML']",2016-05-25 20:15:38+00:00
http://arxiv.org/abs/1605.08003v3,Tight Complexity Bounds for Optimizing Composite Objectives,"We provide tight upper and lower bounds on the complexity of minimizing the
average of $m$ convex functions using gradient and prox oracles of the
component functions. We show a significant gap between the complexity of
deterministic vs randomized optimization. For smooth functions, we show that
accelerated gradient descent (AGD) and an accelerated variant of SVRG are
optimal in the deterministic and randomized settings respectively, and that a
gradient oracle is sufficient for the optimal rate. For non-smooth functions,
having access to prox oracles reduces the complexity and we present optimal
methods based on smoothing that improve over methods using just gradient
accesses.","['Blake Woodworth', 'Nathan Srebro']","['math.OC', 'cs.LG', 'stat.ML']",2016-05-25 18:44:54+00:00
http://arxiv.org/abs/1605.07999v1,"Toward a general, scaleable framework for Bayesian teaching with applications to topic models","Machines, not humans, are the world's dominant knowledge accumulators but
humans remain the dominant decision makers. Interpreting and disseminating the
knowledge accumulated by machines requires expertise, time, and is prone to
failure. The problem of how best to convey accumulated knowledge from computers
to humans is a critical bottleneck in the broader application of machine
learning. We propose an approach based on human teaching where the problem is
formalized as selecting a small subset of the data that will, with high
probability, lead the human user to the correct inference. This approach,
though successful for modeling human learning in simple laboratory experiments,
has failed to achieve broader relevance due to challenges in formulating
general and scalable algorithms. We propose general-purpose teaching via
pseudo-marginal sampling and demonstrate the algorithm by teaching topic
models. Simulation results show our sampling-based approach: effectively
approximates the probability where ground-truth is possible via enumeration,
results in data that are markedly different from those expected by random
sampling, and speeds learning especially for small amounts of data. Application
to movie synopsis data illustrates differences between teaching and random
sampling for teaching distributions and specific topics, and demonstrates gains
in scalability and applicability to real-world problems.","['Baxter S. Eaves Jr', 'Patrick Shafto']","['cs.LG', 'cs.AI', 'stat.ML']",2016-05-25 18:33:10+00:00
http://arxiv.org/abs/1605.07991v1,Efficient Distributed Learning with Sparsity,"We propose a novel, efficient approach for distributed sparse learning in
high-dimensions, where observations are randomly partitioned across machines.
Computationally, at each round our method only requires the master machine to
solve a shifted ell_1 regularized M-estimation problem, and other workers to
compute the gradient. In respect of communication, the proposed approach
provably matches the estimation error bound of centralized methods within
constant rounds of communications (ignoring logarithmic factors). We conduct
extensive experiments on both simulated and real world datasets, and
demonstrate encouraging performances on high-dimensional regression and
classification tasks.","['Jialei Wang', 'Mladen Kolar', 'Nathan Srebro', 'Tong Zhang']","['stat.ML', 'cs.LG']",2016-05-25 18:15:43+00:00
http://arxiv.org/abs/1605.07950v6,On Fast Convergence of Proximal Algorithms for SQRT-Lasso Optimization: Don't Worry About Its Nonsmooth Loss Function,"Many machine learning techniques sacrifice convenient computational
structures to gain estimation robustness and modeling flexibility. However, by
exploring the modeling structures, we find these ""sacrifices"" do not always
require more computational efforts. To shed light on such a ""free-lunch""
phenomenon, we study the square-root-Lasso (SQRT-Lasso) type regression
problem. Specifically, we show that the nonsmooth loss functions of SQRT-Lasso
type regression ease tuning effort and gain adaptivity to inhomogeneous noise,
but is not necessarily more challenging than Lasso in computation. We can
directly apply proximal algorithms (e.g. proximal gradient descent, proximal
Newton, and proximal Quasi-Newton algorithms) without worrying the
nonsmoothness of the loss function. Theoretically, we prove that the proximal
algorithms combined with the pathwise optimization scheme enjoy fast
convergence guarantees with high probability. Numerical results are provided to
support our theory.","['Xingguo Li', 'Haoming Jiang', 'Jarvis Haupt', 'Raman Arora', 'Han Liu', 'Mingyi Hong', 'Tuo Zhao']","['cs.LG', 'math.OC', 'stat.ML']",2016-05-25 16:08:08+00:00
http://arxiv.org/abs/1605.07913v3,Solution of linear ill-posed problems using random dictionaries,"In the present paper we consider application of overcomplete dictionaries to
solution of general ill-posed linear inverse problems. In the context of
regression problems, there has been enormous amount of effort to recover an
unknown function using such dictionaries. One of the most popular methods,
lasso and its versions, is based on minimizing empirical likelihood and
unfortunately, requires stringent assumptions on the dictionary, the, so
called, compatibility conditions. Though compatibility conditions are hard to
satisfy, it is well known that this can be accomplished by using random
dictionaries. In the present paper, we show how one can apply random
dictionaries to solution of ill-posed linear inverse problems. We put a
theoretical foundation under the suggested methodology and study its
performance via simulations.","['Pawan Gupta', 'Marianna Pensky']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH', '62G05 (Primary), 62C10 (Secondary)']",2016-05-25 14:51:50+00:00
http://arxiv.org/abs/1605.07906v2,How priors of initial hyperparameters affect Gaussian process regression models,"The hyperparameters in Gaussian process regression (GPR) model with a
specified kernel are often estimated from the data via the maximum marginal
likelihood. Due to the non-convexity of marginal likelihood with respect to the
hyperparameters, the optimization may not converge to the global maxima. A
common approach to tackle this issue is to use multiple starting points
randomly selected from a specific prior distribution. As a result the choice of
prior distribution may play a vital role in the predictability of this
approach. However, there exists little research in the literature to study the
impact of the prior distributions on the hyperparameter estimation and the
performance of GPR. In this paper, we provide the first empirical study on this
problem using simulated and real data experiments. We consider different types
of priors for the initial values of hyperparameters for some commonly used
kernels and investigate the influence of the priors on the predictability of
GPR models. The results reveal that, once a kernel is chosen, different priors
for the initial hyperparameters have no significant impact on the performance
of GPR prediction, despite that the estimates of the hyperparameters are very
different to the true values in some cases.","['Zexun Chen', 'Bo Wang']",['stat.ML'],2016-05-25 14:45:26+00:00
http://arxiv.org/abs/1605.07870v1,Simultaneous Sparse Dictionary Learning and Pruning,"Dictionary learning is a cutting-edge area in imaging processing, that has
recently led to state-of-the-art results in many signal processing tasks. The
idea is to conduct a linear decomposition of a signal using a few atoms of a
learned and usually over-completed dictionary instead of a pre-defined basis.
Determining a proper size of the to-be-learned dictionary is crucial for both
precision and efficiency of the process, while most of the existing dictionary
learning algorithms choose the size quite arbitrarily. In this paper, a novel
regularization method called the Grouped Smoothly Clipped Absolute Deviation
(GSCAD) is employed for learning the dictionary. The proposed method can
simultaneously learn a sparse dictionary and select the appropriate dictionary
size. Efficient algorithm is designed based on the alternative direction method
of multipliers (ADMM) which decomposes the joint non-convex problem with the
non-convex penalty into two convex optimization problems. Several examples are
presented for image denoising and the experimental results are compared with
other state-of-the-art approaches.","['Simeng Qu', 'Xiao Wang']",['stat.ML'],2016-05-25 13:24:39+00:00
http://arxiv.org/abs/1605.07826v4,Asymptotically exact inference in differentiable generative models,"Many generative models can be expressed as a differentiable function of
random inputs drawn from some simple probability density. This framework
includes both deep generative architectures such as Variational Autoencoders
and a large class of procedurally defined simulator models. We present a method
for performing efficient MCMC inference in such models when conditioning on
observations of the model output. For some models this offers an asymptotically
exact inference method where Approximate Bayesian Computation might otherwise
be employed. We use the intuition that inference corresponds to integrating a
density across the manifold corresponding to the set of inputs consistent with
the observed outputs. This motivates the use of a constrained variant of
Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to
coherently move between inputs exactly consistent with observations. We
validate the method by performing inference tasks in a diverse set of models.","['Matthew M. Graham', 'Amos J. Storkey']","['stat.CO', 'stat.ML']",2016-05-25 11:10:36+00:00
http://arxiv.org/abs/1605.07784v2,Fast Algorithms for Robust PCA via Gradient Descent,"We consider the problem of Robust PCA in the fully and partially observed
settings. Without corruptions, this is the well-known matrix completion
problem. From a statistical standpoint this problem has been recently
well-studied, and conditions on when recovery is possible (how many
observations do we need, how many corruptions can we tolerate) via
polynomial-time algorithms is by now understood. This paper presents and
analyzes a non-convex optimization approach that greatly reduces the
computational complexity of the above problems, compared to the best available
algorithms. In particular, in the fully observed case, with $r$ denoting rank
and $d$ dimension, we reduce the complexity from
$\mathcal{O}(r^2d^2\log(1/\varepsilon))$ to
$\mathcal{O}(rd^2\log(1/\varepsilon))$ -- a big savings when the rank is big.
For the partially observed case, we show the complexity of our algorithm is no
more than $\mathcal{O}(r^4d \log d \log(1/\varepsilon))$. Not only is this the
best-known run-time for a provable algorithm under partial observation, but in
the setting where $r$ is small compared to $d$, it also allows for
near-linear-in-$d$ run-time that can be exploited in the fully-observed case as
well, by simply running our algorithm on a subset of the observations.","['Xinyang Yi', 'Dohyung Park', 'Yudong Chen', 'Constantine Caramanis']","['cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2016-05-25 09:10:07+00:00
http://arxiv.org/abs/1605.07747v2,NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization,"We study a stochastic and distributed algorithm for nonconvex problems whose
objective consists of a sum of $N$ nonconvex $L_i/N$-smooth functions, plus a
nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT)
algorithm splits the problem into $N$ subproblems, and utilizes an augmented
Lagrangian based primal-dual scheme to solve it in a distributed and stochastic
manner. With a special non-uniform sampling, a version of NESTT achieves
$\epsilon$-stationary solution using
$\mathcal{O}((\sum_{i=1}^N\sqrt{L_i/N})^2/\epsilon)$ gradient evaluations,
which can be up to $\mathcal{O}(N)$ times better than the (proximal) gradient
descent methods. It also achieves Q-linear convergence rate for nonconvex
$\ell_1$ penalized quadratic problems with polyhedral constraints. Further, we
reveal a fundamental connection between primal-dual based methods and a few
primal only methods such as IAG/SAG/SAGA.","['Davood Hajinezhad', 'Mingyi Hong', 'Tuo Zhao', 'Zhaoran Wang']","['math.OC', 'cs.LG', 'stat.ML']",2016-05-25 06:42:51+00:00
http://arxiv.org/abs/1605.07725v4,Adversarial Training Methods for Semi-Supervised Text Classification,"Adversarial training provides a means of regularizing supervised learning
algorithms while virtual adversarial training is able to extend supervised
learning algorithms to the semi-supervised setting. However, both methods
require making small perturbations to numerous entries of the input vector,
which is inappropriate for sparse high-dimensional inputs such as one-hot word
representations. We extend adversarial and virtual adversarial training to the
text domain by applying perturbations to the word embeddings in a recurrent
neural network rather than to the original input itself. The proposed method
achieves state of the art results on multiple benchmark semi-supervised and
purely supervised tasks. We provide visualizations and analysis showing that
the learned word embeddings have improved in quality and that while training,
the model is less prone to overfitting. Code is available at
https://github.com/tensorflow/models/tree/master/research/adversarial_text.","['Takeru Miyato', 'Andrew M. Dai', 'Ian Goodfellow']","['stat.ML', 'cs.LG']",2016-05-25 04:25:45+00:00
http://arxiv.org/abs/1605.07723v3,"Data Programming: Creating Large Training Sets, Quickly","Large labeled training sets are the critical building blocks of supervised
learning methods and are key enablers of deep learning techniques. For some
applications, creating labeled training sets is the most time-consuming and
expensive part of applying machine learning. We therefore propose a paradigm
for the programmatic creation of training sets called data programming in which
users express weak supervision strategies or domain heuristics as labeling
functions, which are programs that label subsets of the data, but that are
noisy and may conflict. We show that by explicitly representing this training
set labeling process as a generative model, we can ""denoise"" the generated
training set, and establish theoretically that we can recover the parameters of
these generative models in a handful of settings. We then show how to modify a
discriminative loss function to make it noise-aware, and demonstrate our method
over a range of discriminative models including logistic regression and LSTMs.
Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data
programming would have led to a new winning score, and also show that applying
data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points
over a state-of-the-art LSTM baseline (and into second place in the
competition). Additionally, in initial user studies we observed that data
programming may be an easier way for non-experts to create machine learning
models when training data is limited or unavailable.","['Alexander Ratner', 'Christopher De Sa', 'Sen Wu', 'Daniel Selsam', 'Christopher Ré']","['stat.ML', 'cs.AI', 'cs.LG']",2016-05-25 04:14:59+00:00
http://arxiv.org/abs/1605.07719v2,Reshaped Wirtinger Flow and Incremental Algorithm for Solving Quadratic System of Equations,"We study the phase retrieval problem, which solves quadratic system of
equations, i.e., recovers a vector $\boldsymbol{x}\in \mathbb{R}^n$ from its
magnitude measurements $y_i=|\langle \boldsymbol{a}_i, \boldsymbol{x}\rangle|,
i=1,..., m$. We develop a gradient-like algorithm (referred to as RWF
representing reshaped Wirtinger flow) by minimizing a nonconvex nonsmooth loss
function. In comparison with existing nonconvex Wirtinger flow (WF) algorithm
\cite{candes2015phase}, although the loss function becomes nonsmooth, it
involves only the second power of variable and hence reduces the complexity. We
show that for random Gaussian measurements, RWF enjoys geometric convergence to
a global optimal point as long as the number $m$ of measurements is on the
order of $n$, the dimension of the unknown $\boldsymbol{x}$. This improves the
sample complexity of WF, and achieves the same sample complexity as truncated
Wirtinger flow (TWF) \cite{chen2015solving}, but without truncation in gradient
loop. Furthermore, RWF costs less computationally than WF, and runs faster
numerically than both WF and TWF. We further develop the incremental
(stochastic) reshaped Wirtinger flow (IRWF) and show that IRWF converges
linearly to the true signal. We further establish performance guarantee of an
existing Kaczmarz method for the phase retrieval problem based on its
connection to IRWF. We also empirically demonstrate that IRWF outperforms
existing ITWF algorithm (stochastic version of TWF) as well as other batch
algorithms.","['Huishuai Zhang', 'Yi Zhou', 'Yingbin Liang', 'Yuejie Chi']","['stat.ML', 'cs.LG']",2016-05-25 03:45:44+00:00
http://arxiv.org/abs/1605.07717v2,Deep Structured Energy Based Models for Anomaly Detection,"In this paper, we attack the anomaly detection problem by directly modeling
the data distribution with deep architectures. We propose deep structured
energy based models (DSEBMs), where the energy function is the output of a
deterministic deep neural network with structure. We develop novel model
architectures to integrate EBMs with different types of data such as static
data, sequential data, and spatial data, and apply appropriate model
architectures to adapt to the data structure. Our training algorithm is built
upon the recent development of score matching \cite{sm}, which connects an EBM
with a regularized autoencoder, eliminating the need for complicated sampling
method. Statistically sound decision criterion can be derived for anomaly
detection purpose from the perspective of the energy landscape of the data
distribution. We investigate two decision criteria for performing anomaly
detection: the energy score and the reconstruction error. Extensive empirical
studies on benchmark tasks demonstrate that our proposed model consistently
matches or outperforms all the competing methods.","['Shuangfei Zhai', 'Yu Cheng', 'Weining Lu', 'Zhongfei Zhang']","['cs.LG', 'stat.ML']",2016-05-25 03:40:18+00:00
http://arxiv.org/abs/1605.07696v2,Exact Exponent in Optimal Rates for Crowdsourcing,"In many machine learning applications, crowdsourcing has become the primary
means for label collection. In this paper, we study the optimal error rate for
aggregating labels provided by a set of non-expert workers. Under the classic
Dawid-Skene model, we establish matching upper and lower bounds with an exact
exponent $mI(\pi)$ in which $m$ is the number of workers and $I(\pi)$ the
average Chernoff information that characterizes the workers' collective
ability. Such an exact characterization of the error exponent allows us to
state a precise sample size requirement
$m>\frac{1}{I(\pi)}\log\frac{1}{\epsilon}$ in order to achieve an $\epsilon$
misclassification error. In addition, our results imply the optimality of
various EM algorithms for crowdsourcing initialized by consistent estimators.","['Chao Gao', 'Yu Lu', 'Dengyong Zhou']","['stat.ML', 'math.ST', 'stat.TH']",2016-05-25 01:16:06+00:00
http://arxiv.org/abs/1605.07689v3,Communication-Efficient Distributed Statistical Inference,"We present a Communication-efficient Surrogate Likelihood (CSL) framework for
solving distributed statistical inference problems. CSL provides a
communication-efficient surrogate to the global likelihood that can be used for
low-dimensional estimation, high-dimensional regularized estimation and
Bayesian inference. For low-dimensional estimation, CSL provably improves upon
naive averaging schemes and facilitates the construction of confidence
intervals. For high-dimensional regularized estimation, CSL leads to a
minimax-optimal estimator with controlled communication cost. For Bayesian
inference, CSL can be used to form a communication-efficient quasi-posterior
distribution that converges to the true posterior. This quasi-posterior
procedure significantly improves the computational efficiency of MCMC
algorithms even in a non-distributed setting. We present both theoretical
analysis and experiments to explore the properties of the CSL approximation.","['Michael I. Jordan', 'Jason D. Lee', 'Yun Yang']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC', 'stat.ME']",2016-05-25 00:12:06+00:00
http://arxiv.org/abs/1605.07604v1,Posterior Dispersion Indices,"Probabilistic modeling is cyclical: we specify a model, infer its posterior,
and evaluate its performance. Evaluation drives the cycle, as we revise our
model based on how it performs. This requires a metric. Traditionally,
predictive accuracy prevails. Yet, predictive accuracy does not tell the whole
story. We propose to evaluate a model through posterior dispersion. The idea is
to analyze how each datapoint fares in relation to posterior uncertainty around
the hidden structure. We propose a family of posterior dispersion indices (PDI)
that capture this idea. A PDI identifies rich patterns of model mismatch in
three real data examples: voting preferences, supermarket shopping, and
population genetics.","['Alp Kucukelbir', 'David M. Blei']","['stat.ML', 'cs.AI', 'stat.CO']",2016-05-24 19:58:02+00:00
http://arxiv.org/abs/1605.07596v3,Local Minimax Complexity of Stochastic Convex Optimization,"We extend the traditional worst-case, minimax analysis of stochastic convex
optimization by introducing a localized form of minimax complexity for
individual functions. Our main result gives function-specific lower and upper
bounds on the number of stochastic subgradient evaluations needed to optimize
either the function or its ""hardest local alternative"" to a given numerical
precision. The bounds are expressed in terms of a localized and computational
analogue of the modulus of continuity that is central to statistical minimax
analysis. We show how the computational modulus of continuity can be explicitly
calculated in concrete cases, and relates to the curvature of the function at
the optimum. We also prove a superefficiency result that demonstrates it is a
meaningful benchmark, acting as a computational analogue of the Fisher
information in statistical estimation. The nature and practical implications of
the results are demonstrated in simulations.","['Yuancheng Zhu', 'Sabyasachi Chatterjee', 'John Duchi', 'John Lafferty']",['stat.ML'],2016-05-24 19:37:41+00:00
http://arxiv.org/abs/1605.07588v3,A Consistent Regularization Approach for Structured Prediction,"We propose and analyze a regularization approach for structured prediction
problems. We characterize a large class of loss functions that allows to
naturally embed structured outputs in a linear space. We exploit this fact to
design learning algorithms using a surrogate loss approach and regularization
techniques. We prove universal consistency and finite sample bounds
characterizing the generalization properties of the proposed methods.
Experimental results are provided to demonstrate the practical usefulness of
the proposed approach.","['Carlo Ciliberto', 'Alessandro Rudi', 'Lorenzo Rosasco']","['cs.LG', 'stat.ML']",2016-05-24 19:06:43+00:00
http://arxiv.org/abs/1605.07583v5,Recursive Sampling for the Nyström Method,"We give the first algorithm for kernel Nystr\""om approximation that runs in
*linear time in the number of training points* and is provably accurate for all
kernel matrices, without dependence on regularity or incoherence conditions.
The algorithm projects the kernel onto a set of $s$ landmark points sampled by
their *ridge leverage scores*, requiring just $O(ns)$ kernel evaluations and
$O(ns^2)$ additional runtime. While leverage score sampling has long been known
to give strong theoretical guarantees for Nystr\""om approximation, by employing
a fast recursive sampling scheme, our algorithm is the first to make the
approach scalable. Empirically we show that it finds more accurate, lower rank
kernel approximations in less time than popular techniques such as uniformly
sampled Nystr\""om approximation and the random Fourier features method.","['Cameron Musco', 'Christopher Musco']","['cs.LG', 'cs.DS', 'stat.ML']",2016-05-24 18:56:57+00:00
http://arxiv.org/abs/1605.07571v2,Sequential Neural Models with Stochastic Layers,"How can we efficiently propagate uncertainty in a latent state representation
with recurrent neural networks? This paper introduces stochastic recurrent
neural networks which glue a deterministic recurrent neural network and a state
space model together to form a stochastic and sequential neural generative
model. The clear separation of deterministic and stochastic layers allows a
structured variational inference network to track the factorization of the
model's posterior distribution. By retaining both the nonlinear recursive
structure of a recurrent neural network and averaging over the uncertainty in a
latent path, like a state space model, we improve the state of the art results
on the Blizzard and TIMIT speech modeling data sets by a large margin, while
achieving comparable performances to competing methods on polyphonic music
modeling.","['Marco Fraccaro', 'Søren Kaae Sønderby', 'Ulrich Paquet', 'Ole Winther']","['stat.ML', 'cs.LG']",2016-05-24 18:23:58+00:00
http://arxiv.org/abs/1605.07541v2,Inductive supervised quantum learning,"In supervised learning, an inductive learning algorithm extracts general
rules from observed training instances, then the rules are applied to test
instances. We show that this splitting of training and application arises
naturally, in the classical setting, from a simple independence requirement
with a physical interpretation of being non-signalling. Thus, two seemingly
different definitions of inductive learning happen to coincide. This follows
from the properties of classical information that break down in the quantum
setup. We prove a quantum de Finetti theorem for quantum channels, which shows
that in the quantum case, the equivalence holds in the asymptotic setting, that
is, for large number of test instances. This reveals a natural analogy between
classical learning protocols and their quantum counterparts, justifying a
similar treatment, and allowing to inquire about standard elements in
computational learning theory, such as structural risk minimization and sample
complexity.","['Alex Monràs', 'Gael Sentís', 'Peter Wittek']","['cs.LG', 'quant-ph', 'stat.ML']",2016-05-24 16:56:46+00:00
http://arxiv.org/abs/1605.07511v1,A note on privacy preserving iteratively reweighted least squares,"Iteratively reweighted least squares (IRLS) is a widely-used method in
machine learning to estimate the parameters in the generalised linear models.
In particular, IRLS for L1 minimisation under the linear model provides a
closed-form solution in each step, which is a simple multiplication between the
inverse of the weighted second moment matrix and the weighted first moment
vector. When dealing with privacy sensitive data, however, developing a privacy
preserving IRLS algorithm faces two challenges. First, due to the inversion of
the second moment matrix, the usual sensitivity analysis in differential
privacy incorporating a single datapoint perturbation gets complicated and
often requires unrealistic assumptions. Second, due to its iterative nature, a
significant cumulative privacy loss occurs. However, adding a high level of
noise to compensate for the privacy loss hinders from getting accurate
estimates. Here, we develop a practical algorithm that overcomes these
challenges and outputs privatised and accurate IRLS solutions. In our method,
we analyse the sensitivity of each moments separately and treat the matrix
inversion and multiplication as a post-processing step, which simplifies the
sensitivity analysis. Furthermore, we apply the {\it{concentrated differential
privacy}} formalism, a more relaxed version of differential privacy, which
requires adding a significantly less amount of noise for the same level of
privacy guarantee, compared to the conventional and advanced compositions of
differentially private mechanisms.","['Mijung Park', 'Max Welling']","['cs.CR', 'cs.AI', 'stat.AP', 'stat.ML']",2016-05-24 15:50:26+00:00
http://arxiv.org/abs/1605.07496v3,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.","['Supratik Paul', 'Konstantinos Chatzilygeroudis', 'Kamil Ciosek', 'Jean-Baptiste Mouret', 'Michael A. Osborne', 'Shimon Whiteson']","['cs.LG', 'cs.AI', 'stat.ML']",2016-05-24 15:15:57+00:00
http://arxiv.org/abs/1605.07427v1,Hierarchical Memory Networks,"Memory networks are neural networks with an explicit memory component that
can be both read and written to by the network. The memory is often addressed
in a soft way using a softmax function, making end-to-end training with
backpropagation possible. However, this is not computationally scalable for
applications which require the network to read from extremely large memories.
On the other hand, it is well known that hard attention mechanisms based on
reinforcement learning are challenging to train successfully. In this paper, we
explore a form of hierarchical memory network, which can be considered as a
hybrid between hard and soft attention memory networks. The memory is organized
in a hierarchical structure such that reading from it is done with less
computation than soft attention over a flat memory, while also being easier to
train than hard attention over a flat memory. Specifically, we propose to
incorporate Maximum Inner Product Search (MIPS) in the training and inference
procedures for our hierarchical memory network. We explore the use of various
state-of-the art approximate MIPS techniques and report results on
SimpleQuestions, a challenging large scale factoid question answering task.","['Sarath Chandar', 'Sungjin Ahn', 'Hugo Larochelle', 'Pascal Vincent', 'Gerald Tesauro', 'Yoshua Bengio']","['stat.ML', 'cs.CL', 'cs.LG', 'cs.NE']",2016-05-24 12:48:19+00:00
http://arxiv.org/abs/1605.07422v3,Computing Web-scale Topic Models using an Asynchronous Parameter Server,"Topic models such as Latent Dirichlet Allocation (LDA) have been widely used
in information retrieval for tasks ranging from smoothing and feedback methods
to tools for exploratory search and discovery. However, classical methods for
inferring topic models do not scale up to the massive size of today's publicly
available Web-scale data sets. The state-of-the-art approaches rely on custom
strategies, implementations and hardware to facilitate their asynchronous,
communication-intensive workloads.
  We present APS-LDA, which integrates state-of-the-art topic modeling with
cluster computing frameworks such as Spark using a novel asynchronous parameter
server. Advantages of this integration include convenient usage of existing
data processing pipelines and eliminating the need for disk writes as data can
be kept in memory from start to finish. Our goal is not to outperform highly
customized implementations, but to propose a general high-performance topic
modeling framework that can easily be used in today's data processing
pipelines. We compare APS-LDA to the existing Spark LDA implementations and
show that our system can, on a 480-core cluster, process up to 135 times more
data and 10 times more topics without sacrificing model quality.","['Rolf Jagerman', 'Carsten Eickhoff', 'Maarten de Rijke']","['cs.DC', 'cs.IR', 'cs.LG', 'stat.ML']",2016-05-24 12:40:29+00:00
http://arxiv.org/abs/1605.07416v2,Refined Lower Bounds for Adversarial Bandits,"We provide new lower bounds on the regret that must be suffered by
adversarial bandit algorithms. The new results show that recent upper bounds
that either (a) hold with high-probability or (b) depend on the total lossof
the best arm or (c) depend on the quadratic variation of the losses, are close
to tight. Besides this we prove two impossibility results. First, the existence
of a single arm that is optimal in every round cannot improve the regret in the
worst case. Second, the regret cannot scale with the effective range of the
losses. In contrast, both results are possible in the full-information setting.","['Sébastien Gerchinovitz', 'Tor Lattimore']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2016-05-24 12:36:47+00:00
http://arxiv.org/abs/1605.07371v1,Semiparametric energy-based probabilistic models,"Probabilistic models can be defined by an energy function, where the
probability of each state is proportional to the exponential of the state's
negative energy. This paper considers a generalization of energy-based models
in which the probability of a state is proportional to an arbitrary positive,
strictly decreasing, and twice differentiable function of the state's energy.
The precise shape of the nonlinear map from energies to unnormalized
probabilities has to be learned from data together with the parameters of the
energy function. As a case study we show that the above generalization of a
fully visible Boltzmann machine yields an accurate model of neural activity of
retinal ganglion cells. We attribute this success to the model's ability to
easily capture distributions whose probabilities span a large dynamic range, a
possible consequence of latent variables that globally couple the system.
Similar features have recently been observed in many datasets, suggesting that
our new method has wide applicability.","['Jan Humplik', 'Gašper Tkačik']","['q-bio.NC', 'cond-mat.stat-mech', 'stat.ML']",2016-05-24 10:51:13+00:00
http://arxiv.org/abs/1605.07367v3,Riemannian stochastic variance reduced gradient on Grassmann manifold,"Stochastic variance reduction algorithms have recently become popular for
minimizing the average of a large, but finite, number of loss functions. In
this paper, we propose a novel Riemannian extension of the Euclidean stochastic
variance reduced gradient algorithm (R-SVRG) to a compact manifold search
space. To this end, we show the developments on the Grassmann manifold. The key
challenges of averaging, addition, and subtraction of multiple gradients are
addressed with notions like logarithm mapping and parallel translation of
vectors on the Grassmann manifold. We present a global convergence analysis of
the proposed algorithm with decay step-sizes and a local convergence rate
analysis under fixed step-size with some natural assumptions. The proposed
algorithm is applied on a number of problems on the Grassmann manifold like
principal components analysis, low-rank matrix completion, and the Karcher mean
computation. In all these cases, the proposed algorithm outperforms the
standard Riemannian stochastic gradient descent algorithm.","['Hiroyuki Kasai', 'Hiroyuki Sato', 'Bamdev Mishra']","['cs.LG', 'cs.NA', 'math.OC', 'stat.ML']",2016-05-24 10:36:32+00:00
http://arxiv.org/abs/1605.07358v1,Consistency Analysis for the Doubly Stochastic Dirichlet Process,"This technical report proves components consistency for the Doubly Stochastic
Dirichlet Process with exponential convergence of posterior probability. We
also present the fundamental properties for DSDP as well as inference
algorithms. Simulation toy experiment and real-world experiment results for
single and multi-cluster also support the consistency proof. This report is
also a support document for the paper ""Computationally Efficient Hyperspectral
Data Learning Based on the Doubly Stochastic Dirichlet Process"".","['Xing Sun', 'Nelson H. C. Yung', 'Edmund Y. Lam', 'Hayden K. -H. So']","['cs.IT', 'math.IT', 'stat.ML']",2016-05-24 10:13:19+00:00
http://arxiv.org/abs/1605.07332v2,Relevant sparse codes with variational information bottleneck,"In many applications, it is desirable to extract only the relevant aspects of
data. A principled way to do this is the information bottleneck (IB) method,
where one seeks a code that maximizes information about a 'relevance' variable,
Y, while constraining the information encoded about the original data, X.
Unfortunately however, the IB method is computationally demanding when data are
high-dimensional and/or non-gaussian. Here we propose an approximate
variational scheme for maximizing a lower bound on the IB objective, analogous
to variational EM. Using this method, we derive an IB algorithm to recover
features that are both relevant and sparse. Finally, we demonstrate how
kernelized versions of the algorithm can be used to address a broad range of
problems with non-linear relation between X and Y.","['Matthew Chalk', 'Olivier Marre', 'Gasper Tkacik']",['stat.ML'],2016-05-24 08:16:54+00:00
http://arxiv.org/abs/1605.07272v4,Matrix Completion has No Spurious Local Minimum,"Matrix completion is a basic machine learning problem that has wide
applications, especially in collaborative filtering and recommender systems.
Simple non-convex optimization algorithms are popular and effective in
practice. Despite recent progress in proving various non-convex algorithms
converge from a good initial point, it remains unclear why random or arbitrary
initialization suffices in practice. We prove that the commonly used non-convex
objective function for \textit{positive semidefinite} matrix completion has no
spurious local minima --- all local minima must also be global. Therefore, many
popular optimization algorithms such as (stochastic) gradient descent can
provably solve positive semidefinite matrix completion with \textit{arbitrary}
initialization in polynomial time. The result can be generalized to the setting
when the observed entries contain noise. We believe that our main proof
strategy can be useful for understanding geometric properties of other
statistical problems involving partial or noisy observations.","['Rong Ge', 'Jason D. Lee', 'Tengyu Ma']","['cs.LG', 'cs.DS', 'stat.ML']",2016-05-24 02:53:27+00:00
http://arxiv.org/abs/1605.07254v2,Convergence guarantees for kernel-based quadrature rules in misspecified settings,"Kernel-based quadrature rules are becoming important in machine learning and
statistics, as they achieve super-$\sqrt{n}$ convergence rates in numerical
integration, and thus provide alternatives to Monte Carlo integration in
challenging settings where integrands are expensive to evaluate or where
integrands are high dimensional. These rules are based on the assumption that
the integrand has a certain degree of smoothness, which is expressed as that
the integrand belongs to a certain reproducing kernel Hilbert space (RKHS).
However, this assumption can be violated in practice (e.g., when the integrand
is a black box function), and no general theory has been established for the
convergence of kernel quadratures in such misspecified settings. Our
contribution is in proving that kernel quadratures can be consistent even when
the integrand does not belong to the assumed RKHS, i.e., when the integrand is
less smooth than assumed. Specifically, we derive convergence rates that depend
on the (unknown) lesser smoothness of the integrand, where the degree of
smoothness is expressed via powers of RKHSs or via Sobolev spaces.","['Motonobu Kanagawa', 'Bharath K. Sriperumbudur', 'Kenji Fukumizu']",['stat.ML'],2016-05-24 01:41:25+00:00
http://arxiv.org/abs/1605.07252v3,Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models,"We consider the problem of learning the underlying graph of an unknown Ising
model on p spins from a collection of i.i.d. samples generated from the model.
We suggest a new estimator that is computationally efficient and requires a
number of samples that is near-optimal with respect to previously established
information-theoretic lower-bound. Our statistical estimator has a physical
interpretation in terms of ""interaction screening"". The estimator is consistent
and is efficiently implemented using convex optimization. We prove that with
appropriate regularization, the estimator recovers the underlying graph using a
number of samples that is logarithmic in the system size p and exponential in
the maximum coupling-intensity and maximum node-degree.","['Marc Vuffray', 'Sidhant Misra', 'Andrey Y. Lokhov', 'Michael Chertkov']","['cs.LG', 'cond-mat.stat-mech', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2016-05-24 01:36:48+00:00
http://arxiv.org/abs/1605.07221v2,Global Optimality of Local Search for Low Rank Matrix Recovery,"We show that there are no spurious local minima in the non-convex factorized
parametrization of low-rank matrix recovery from incoherent linear
measurements. With noisy measurements we show all local minima are very close
to a global optimum. Together with a curvature bound at saddle points, this
yields a polynomial time global convergence guarantee for stochastic gradient
descent {\em from random initialization}.","['Srinadh Bhojanapalli', 'Behnam Neyshabur', 'Nathan Srebro']","['stat.ML', 'cs.LG', 'math.OC']",2016-05-23 22:05:42+00:00
http://arxiv.org/abs/1605.07174v1,Kernel-based Reconstruction of Graph Signals,"A number of applications in engineering, social sciences, physics, and
biology involve inference over networks. In this context, graph signals are
widely encountered as descriptors of vertex attributes or features in
graph-structured data. Estimating such signals in all vertices given noisy
observations of their values on a subset of vertices has been extensively
analyzed in the literature of signal processing on graphs (SPoG). This paper
advocates kernel regression as a framework generalizing popular SPoG modeling
and reconstruction and expanding their capabilities. Formulating signal
reconstruction as a regression task on reproducing kernel Hilbert spaces of
graph signals permeates benefits from statistical learning, offers fresh
insights, and allows for estimators to leverage richer forms of prior
information than existing alternatives. A number of SPoG notions such as
bandlimitedness, graph filters, and the graph Fourier transform are naturally
accommodated in the kernel framework. Additionally, this paper capitalizes on
the so-called representer theorem to devise simpler versions of existing
Thikhonov regularized estimators, and offers a novel probabilistic
interpretation of kernel methods on graphs based on graphical models. Motivated
by the challenges of selecting the bandwidth parameter in SPoG estimators or
the kernel map in kernel-based methods, the present paper further proposes two
multi-kernel approaches with complementary strengths. Whereas the first enables
estimation of the unknown bandwidth of bandlimited signals, the second allows
for efficient graph filter selection. Numerical tests with synthetic as well as
real data demonstrate the merits of the proposed methods relative to
state-of-the-art alternatives.","['Daniel Romero', 'Meng Ma', 'Georgios B. Giannakis']","['stat.ML', 'cs.LG']",2016-05-23 19:47:18+00:00
http://arxiv.org/abs/1605.07156v1,Genetic Architect: Discovering Genomic Structure with Learned Neural Architectures,"Each human genome is a 3 billion base pair set of encoding instructions.
Decoding the genome using deep learning fundamentally differs from most tasks,
as we do not know the full structure of the data and therefore cannot design
architectures to suit it. As such, architectures that fit the structure of
genomics should be learned not prescribed. Here, we develop a novel search
algorithm, applicable across domains, that discovers an optimal architecture
which simultaneously learns general genomic patterns and identifies the most
important sequence motifs in predicting functional genomic outcomes. The
architectures we find using this algorithm succeed at using only RNA expression
data to predict gene regulatory structure, learn human-interpretable
visualizations of key sequence motifs, and surpass state-of-the-art results on
benchmark genomics challenges.","['Laura Deming', 'Sasha Targ', 'Nate Sauder', 'Diogo Almeida', 'Chun Jimmie Ye']","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",2016-05-23 19:43:08+00:00
http://arxiv.org/abs/1605.07145v2,On Optimality Conditions for Auto-Encoder Signal Recovery,"Auto-Encoders are unsupervised models that aim to learn patterns from
observed data by minimizing a reconstruction cost. The useful representations
learned are often found to be sparse and distributed. On the other hand,
compressed sensing and sparse coding assume a data generating process, where
the observed data is generated from some true latent signal source, and try to
recover the corresponding signal from measurements. Looking at auto-encoders
from this \textit{signal recovery perspective} enables us to have a more
coherent view of these techniques. In this paper, in particular, we show that
the \textit{true} hidden representation can be approximately recovered if the
weight matrices are highly incoherent with unit $ \ell^{2} $ row length and the
bias vectors takes the value (approximately) equal to the negative of the data
mean. The recovery also becomes more and more accurate as the sparsity in
hidden signals increases. Additionally, we empirically demonstrate that
auto-encoders are capable of recovering the data generating dictionary when
only data samples are given.","['Devansh Arpit', 'Yingbo Zhou', 'Hung Q. Ngo', 'Nils Napp', 'Venu Govindaraju']","['stat.ML', 'cs.LG', 'cs.NE']",2016-05-23 19:21:53+00:00
http://arxiv.org/abs/1605.07144v2,Actively Learning Hemimetrics with Applications to Eliciting User Preferences,"Motivated by an application of eliciting users' preferences, we investigate
the problem of learning hemimetrics, i.e., pairwise distances among a set of
$n$ items that satisfy triangle inequalities and non-negativity constraints. In
our application, the (asymmetric) distances quantify private costs a user
incurs when substituting one item by another. We aim to learn these distances
(costs) by asking the users whether they are willing to switch from one item to
another for a given incentive offer. Without exploiting structural constraints
of the hemimetric polytope, learning the distances between each pair of items
requires $\Theta(n^2)$ queries. We propose an active learning algorithm that
substantially reduces this sample complexity by exploiting the structural
constraints on the version space of hemimetrics. Our proposed algorithm
achieves provably-optimal sample complexity for various instances of the task.
For example, when the items are embedded into $K$ tight clusters, the sample
complexity of our algorithm reduces to $O(n K)$. Extensive experiments on a
restaurant recommendation data set support the conclusions of our theoretical
analysis.","['Adish Singla', 'Sebastian Tschiatschek', 'Andreas Krause']","['stat.ML', 'cs.LG']",2016-05-23 19:21:35+00:00
http://arxiv.org/abs/1605.07139v2,Fairness in Learning: Classic and Contextual Bandits,"We introduce the study of fairness in multi-armed bandit problems. Our
fairness definition can be interpreted as demanding that given a pool of
applicants (say, for college admission or mortgages), a worse applicant is
never favored over a better one, despite a learning algorithm's uncertainty
over the true payoffs. We prove results of two types.
  First, in the important special case of the classic stochastic bandits
problem (i.e., in which there are no contexts), we provide a provably fair
algorithm based on ""chained"" confidence intervals, and provide a cumulative
regret bound with a cubic dependence on the number of arms. We further show
that any fair algorithm must have such a dependence. When combined with regret
bounds for standard non-fair algorithms such as UCB, this proves a strong
separation between fair and unfair learning, which extends to the general
contextual case.
  In the general contextual case, we prove a tight connection between fairness
and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class
of functions can be transformed into a provably fair contextual bandit
algorithm, and conversely any fair contextual bandit algorithm can be
transformed into a KWIK learning algorithm. This tight connection allows us to
provide a provably fair algorithm for the linear contextual bandit problem with
a polynomial dependence on the dimension, and to show (for a different class of
functions) a worst-case exponential gap in regret between fair and non-fair
learning algorithms","['Matthew Joseph', 'Michael Kearns', 'Jamie Morgenstern', 'Aaron Roth']","['cs.LG', 'stat.ML']",2016-05-23 18:58:24+00:00
http://arxiv.org/abs/1605.07129v5,Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed entries,"Estimation of the covariance matrix has attracted a lot of attention of the
statistical research community over the years, partially due to important
applications such as Principal Component Analysis. However, frequently used
empirical covariance estimator (and its modifications) is very sensitive to
outliers in the data. As P. J. Huber wrote in 1964, ""...This raises a question
which could have been asked already by Gauss, but which was, as far as I know,
only raised a few years ago (notably by Tukey): what happens if the true
distribution deviates slightly from the assumed normal one? As is now well
known, the sample mean then may have a catastrophically bad performance...""
Motivated by this question, we develop a new estimator of the (element-wise)
mean of a random matrix, which includes covariance estimation problem as a
special case. Assuming that the entries of a matrix possess only finite second
moment, this new estimator admits sub-Gaussian or sub-exponential concentration
around the unknown mean in the operator norm. We will explain the key ideas
behind our construction, as well as applications to covariance estimation and
matrix completion problems.",['Stanislav Minsker'],"['math.ST', 'stat.ML', 'stat.TH', '60B20, 62G35 (Primary) 62H12 (Secondary)']",2016-05-23 18:36:28+00:00
http://arxiv.org/abs/1605.07127v3,Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks,"We present an algorithm for model-based reinforcement learning that combines
Bayesian neural networks (BNNs) with random roll-outs and stochastic
optimization for policy learning. The BNNs are trained by minimizing
$\alpha$-divergences, allowing us to capture complicated statistical patterns
in the transition dynamics, e.g. multi-modality and heteroskedasticity, which
are usually missed by other common modeling approaches. We illustrate the
performance of our method by solving a challenging benchmark where model-based
approaches usually fail and by obtaining promising results in a real-world
scenario for controlling a gas turbine.","['Stefan Depeweg', 'José Miguel Hernández-Lobato', 'Finale Doshi-Velez', 'Steffen Udluft']","['stat.ML', 'cs.LG']",2016-05-23 18:28:15+00:00
http://arxiv.org/abs/1605.07110v3,Deep Learning without Poor Local Minima,"In this paper, we prove a conjecture published in 1989 and also partially
address an open problem announced at the Conference on Learning Theory (COLT)
2015. With no unrealistic assumption, we first prove the following statements
for the squared loss function of deep linear neural networks with any depth and
any widths: 1) the function is non-convex and non-concave, 2) every local
minimum is a global minimum, 3) every critical point that is not a global
minimum is a saddle point, and 4) there exist ""bad"" saddle points (where the
Hessian has no negative eigenvalue) for the deeper networks (with more than
three layers), whereas there is no bad saddle point for the shallow networks
(with three layers). Moreover, for deep nonlinear neural networks, we prove the
same four statements via a reduction to a deep linear model under the
independence assumption adopted from recent work. As a result, we present an
instance, for which we can answer the following question: how difficult is it
to directly train a deep model in theory? It is more difficult than the
classical machine learning models (because of the non-convexity), but not too
difficult (because of the nonexistence of poor local minima). Furthermore, the
mathematically proven existence of bad saddle points for deeper models would
suggest a possible open problem. We note that even though we have advanced the
theoretical foundations of deep learning and non-convex optimization, there is
still a gap between theory and practice.",['Kenji Kawaguchi'],"['stat.ML', 'cs.LG', 'math.OC']",2016-05-23 17:34:20+00:00
http://arxiv.org/abs/1605.07094v2,A note on the expected minimum error probability in equientropic channels,"While the channel capacity reflects a theoretical upper bound on the
achievable information transmission rate in the limit of infinitely many bits,
it does not characterise the information transfer of a given encoding routine
with finitely many bits. In this note, we characterise the quality of a code
(i. e. a given encoding routine) by an upper bound on the expected minimum
error probability that can be achieved when using this code. We show that for
equientropic channels this upper bound is minimal for codes with maximal
marginal entropy. As an instructive example we show for the additive white
Gaussian noise (AWGN) channel that random coding---also a capacity achieving
code---indeed maximises the marginal entropy in the limit of infinite messages.","['Sebastian Weichwald', 'Tatiana Fomina', 'Bernhard Schölkopf', 'Moritz Grosse-Wentrup']","['q-bio.NC', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2016-05-23 17:04:57+00:00
http://arxiv.org/abs/1605.07079v2,Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets,"Bayesian optimization has become a successful tool for hyperparameter
optimization of machine learning algorithms, such as support vector machines or
deep neural networks. Despite its success, for large datasets, training and
validating a single configuration often takes hours, days, or even weeks, which
limits the achievable performance. To accelerate hyperparameter optimization,
we propose a generative model for the validation error as a function of
training set size, which is learned during the optimization process and allows
exploration of preliminary configurations on small subsets, by extrapolating to
the full dataset. We construct a Bayesian optimization procedure, dubbed
Fabolas, which models loss and training time as a function of dataset size and
automatically trades off high information gain about the global optimum against
computational cost. Experiments optimizing support vector machines and deep
neural networks show that Fabolas often finds high-quality solutions 10 to 100
times faster than other state-of-the-art Bayesian optimization methods or the
recently proposed bandit strategy Hyperband.","['Aaron Klein', 'Stefan Falkner', 'Simon Bartels', 'Philipp Hennig', 'Frank Hutter']","['cs.LG', 'cs.AI', 'stat.ML']",2016-05-23 16:29:51+00:00
http://arxiv.org/abs/1605.07078v2,Learning Sensor Multiplexing Design through Back-propagation,"Recent progress on many imaging and vision tasks has been driven by the use
of deep feed-forward neural networks, which are trained by propagating
gradients of a loss defined on the final output, back through the network up to
the first layer that operates directly on the image. We propose
back-propagating one step further---to learn camera sensor designs jointly with
networks that carry out inference on the images they capture. In this paper, we
specifically consider the design and inference problems in a typical color
camera---where the sensor is able to measure only one color channel at each
pixel location, and computational inference is required to reconstruct a full
color image. We learn the camera sensor's color multiplexing pattern by
encoding it as layer whose learnable weights determine which color channel,
from among a fixed set, will be measured at each location. These weights are
jointly trained with those of a reconstruction network that operates on the
corresponding sensor measurements to produce a full color image. Our network
achieves significant improvements in accuracy over the traditional Bayer
pattern used in most color cameras. It automatically learns to employ a sparse
color measurement approach similar to that of a recent design, and moreover,
improves upon that design by learning an optimal layout for these measurements.",['Ayan Chakrabarti'],"['cs.LG', 'stat.ML']",2016-05-23 16:26:59+00:00
http://arxiv.org/abs/1605.07066v3,A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation,"Gaussian processes (GPs) are flexible distributions over functions that
enable high-level assumptions about unknown functions to be encoded in a
parsimonious, flexible and general way. Although elegant, the application of
GPs is limited by computational and analytical intractabilities that arise when
data are sufficiently numerous or when employing non-Gaussian models.
Consequently, a wealth of GP approximation schemes have been developed over the
last 15 years to address these key limitations. Many of these schemes employ a
small set of pseudo data points to summarise the actual data. In this paper, we
develop a new pseudo-point approximation framework using Power Expectation
Propagation (Power EP) that unifies a large number of these pseudo-point
approximations. Unlike much of the previous venerable work in this area, the
new framework is built on standard methods for approximate inference
(variational free-energy, EP and Power EP methods) rather than employing
approximations to the probabilistic generative model itself. In this way, all
of approximation is performed at `inference time' rather than at `modelling
time' resolving awkward philosophical and empirical questions that trouble
previous approaches. Crucially, we demonstrate that the new framework includes
new pseudo-point approximation methods that outperform current approaches on
regression and classification tasks.","['Thang D. Bui', 'Josiah Yan', 'Richard E. Turner']","['stat.ML', 'cs.LG']",2016-05-23 15:53:51+00:00
http://arxiv.org/abs/1605.07057v1,Bayesian Model Selection of Stochastic Block Models,"A central problem in analyzing networks is partitioning them into modules or
communities. One of the best tools for this is the stochastic block model,
which clusters vertices into blocks with statistically homogeneous pattern of
links. Despite its flexibility and popularity, there has been a lack of
principled statistical model selection criteria for the stochastic block model.
Here we propose a Bayesian framework for choosing the number of blocks as well
as comparing it to the more elaborate degree- corrected block models,
ultimately leading to a universal model selection framework capable of
comparing multiple modeling combinations. We will also investigate its
connection to the minimum description length principle.",['Xiaoran Yan'],"['stat.ML', 'cs.LG', 'cs.SI']",2016-05-23 15:16:51+00:00
http://arxiv.org/abs/1605.07051v2,Convergence Analysis for Rectangular Matrix Completion Using Burer-Monteiro Factorization and Gradient Descent,"We address the rectangular matrix completion problem by lifting the unknown
matrix to a positive semidefinite matrix in higher dimension, and optimizing a
nonconvex objective over the semidefinite factor using a simple gradient
descent scheme. With $O( \mu r^2 \kappa^2 n \max(\mu, \log n))$ random
observations of a $n_1 \times n_2$ $\mu$-incoherent matrix of rank $r$ and
condition number $\kappa$, where $n = \max(n_1, n_2)$, the algorithm linearly
converges to the global optimum with high probability.","['Qinqing Zheng', 'John Lafferty']","['stat.ML', 'cs.LG']",2016-05-23 15:14:09+00:00
http://arxiv.org/abs/1605.07025v3,Collaborative Filtering with Side Information: a Gaussian Process Perspective,"We tackle the problem of collaborative filtering (CF) with side information,
through the lens of Gaussian Process (GP) regression. Driven by the idea of
using the kernel to explicitly model user-item similarities, we formulate the
GP in a way that allows the incorporation of low-rank matrix factorisation,
arriving at our model, the Tucker Gaussian Process (TGP). Consequently, TGP
generalises classical Bayesian matrix factorisation models, and goes beyond
them to give a natural and elegant method for incorporating side information,
giving enhanced predictive performance for CF problems. Moreover we show that
it is a novel model for regression, especially well-suited to grid-structured
data and problems where the dependence on covariates is close to being
separable.","['Hyunjik Kim', 'Xiaoyu Lu', 'Seth Flaxman', 'Yee Whye Teh']","['stat.ML', 'cs.IR', 'cs.LG']",2016-05-23 14:19:02+00:00
http://arxiv.org/abs/1605.07018v1,Online Learning with Feedback Graphs Without the Graphs,"We study an online learning framework introduced by Mannor and Shamir (2011)
in which the feedback is specified by a graph, in a setting where the graph may
vary from round to round and is \emph{never fully revealed} to the learner. We
show a large gap between the adversarial and the stochastic cases. In the
adversarial case, we prove that even for dense feedback graphs, the learner
cannot improve upon a trivial regret bound obtained by ignoring any additional
feedback besides her own loss. In contrast, in the stochastic case we give an
algorithm that achieves $\widetilde \Theta(\sqrt{\alpha T})$ regret over $T$
rounds, provided that the independence numbers of the hidden feedback graphs
are at most $\alpha$. We also extend our results to a more general feedback
model, in which the learner does not necessarily observe her own loss, and show
that, even in simple cases, concealing the feedback graphs might render a
learnable problem unlearnable.","['Alon Cohen', 'Tamir Hazan', 'Tomer Koren']","['cs.LG', 'stat.ML']",2016-05-23 14:07:43+00:00
http://arxiv.org/abs/1605.06995v2,DP-EM: Differentially Private Expectation Maximization,"The iterative nature of the expectation maximization (EM) algorithm presents
a challenge for privacy-preserving estimation, as each iteration increases the
amount of noise needed. We propose a practical private EM algorithm that
overcomes this challenge using two innovations: (1) a novel moment perturbation
formulation for differentially private EM (DP-EM), and (2) the use of two
recently developed composition methods to bound the privacy ""cost"" of multiple
EM iterations: the moments accountant (MA) and zero-mean concentrated
differential privacy (zCDP). Both MA and zCDP bound the moment generating
function of the privacy loss random variable and achieve a refined tail bound,
which effectively decrease the amount of additive noise. We present empirical
results showing the benefits of our approach, as well as similar performance
between these two composition methods in the DP-EM setting for Gaussian mixture
models. Our approach can be readily extended to many iterative learning
algorithms, opening up various exciting future directions.","['Mijung Park', 'Jimmy Foulds', 'Kamalika Chaudhuri', 'Max Welling']","['cs.LG', 'cs.AI', 'cs.CR', 'stat.ME', 'stat.ML']",2016-05-23 12:36:55+00:00
http://arxiv.org/abs/1605.06950v4,A Sub-Quadratic Exact Medoid Algorithm,"We present a new algorithm, trimed, for obtaining the medoid of a set, that
is the element of the set which minimises the mean distance to all other
elements. The algorithm is shown to have, under certain assumptions, expected
run time O(N^(3/2)) in R^d where N is the set size, making it the first
sub-quadratic exact medoid algorithm for d>1. Experiments show that it performs
very well on spatial network data, frequently requiring two orders of magnitude
fewer distance calculations than state-of-the-art approximate algorithms. As an
application, we show how trimed can be used as a component in an accelerated
K-medoids algorithm, and then how it can be relaxed to obtain further
computational gains with only a minor loss in cluster quality.","['James Newling', 'François Fleuret']","['stat.ML', 'cs.DS', 'cs.LG']",2016-05-23 09:24:59+00:00
http://arxiv.org/abs/1605.06931v3,An Information Criterion for Inferring Coupling in Distributed Dynamical Systems,"The behaviour of many real-world phenomena can be modelled by nonlinear
dynamical systems whereby a latent system state is observed through a filter.
We are interested in interacting subsystems of this form, which we model by a
set of coupled maps as a synchronous update graph dynamical systems.
Specifically, we study the structure learning problem for spatially distributed
dynamical systems coupled via a directed acyclic graph. Unlike established
structure learning procedures that find locally maximum posterior probabilities
of a network structure containing latent variables, our work exploits the
properties of dynamical systems to compute globally optimal approximations of
these distributions. We arrive at this result by the use of time delay
embedding theorems. Taking an information-theoretic perspective, we show that
the log-likelihood has an intuitive interpretation in terms of information
transfer.","['Oliver M. Cliff', 'Mikhail Prokopenko', 'Robert Fitch']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2016-05-23 08:40:58+00:00
http://arxiv.org/abs/1605.06900v1,Fast Stochastic Methods for Nonsmooth Nonconvex Optimization,"We analyze stochastic algorithms for optimizing nonconvex, nonsmooth
finite-sum problems, where the nonconvex part is smooth and the nonsmooth part
is convex. Surprisingly, unlike the smooth case, our knowledge of this
fundamental problem is very limited. For example, it is not known whether the
proximal stochastic gradient method with constant minibatch converges to a
stationary point. To tackle this issue, we develop fast stochastic algorithms
that provably converge to a stationary point for constant minibatches.
Furthermore, using a variant of these algorithms, we show provably faster
convergence than batch proximal gradient descent. Finally, we prove global
linear convergence rate for an interesting subclass of nonsmooth nonconvex
functions, that subsumes several recent works. This paper builds upon our
recent series of papers on fast stochastic methods for smooth nonconvex
optimization [22, 23], with a novel analysis for nonconvex and nonsmooth
functions.","['Sashank J. Reddi', 'Suvrit Sra', 'Barnabas Poczos', 'Alex Smola']","['math.OC', 'cs.LG', 'stat.ML']",2016-05-23 05:49:00+00:00
http://arxiv.org/abs/1605.06892v6,Accelerated Randomized Mirror Descent Algorithms For Composite Non-strongly Convex Optimization,"We consider the problem of minimizing the sum of an average function of a
large number of smooth convex components and a general, possibly
non-differentiable, convex function. Although many methods have been proposed
to solve this problem with the assumption that the sum is strongly convex, few
methods support the non-strongly convex case. Adding a small quadratic
regularization is a common devise used to tackle non-strongly convex problems;
however, it may cause loss of sparsity of solutions or weaken the performance
of the algorithms. Avoiding this devise, we propose an accelerated randomized
mirror descent method for solving this problem without the strongly convex
assumption. Our method extends the deterministic accelerated proximal gradient
methods of Paul Tseng and can be applied even when proximal points are computed
inexactly. We also propose a scheme for solving the problem when the component
functions are non-smooth.","['Le Thi Khanh Hien', 'Cuong V. Nguyen', 'Huan Xu', 'Canyi Lu', 'Jiashi Feng']","['math.OC', 'stat.ML', '65K05, 90C06, 90C30']",2016-05-23 04:21:07+00:00
http://arxiv.org/abs/1605.06886v2,Stochastic Patching Process,"Stochastic partition models tailor a product space into a number of
rectangular regions such that the data within each region exhibit certain types
of homogeneity. Due to constraints of partition strategy, existing models may
cause unnecessary dissections in sparse regions when fitting data in dense
regions. To alleviate this limitation, we propose a parsimonious partition
model, named Stochastic Patching Process (SPP), to deal with multi-dimensional
arrays. SPP adopts an ""enclosing"" strategy to attach rectangular patches to
dense regions. SPP is self-consistent such that it can be extended to infinite
arrays. We apply SPP to relational modeling and the experimental results
validate its merit compared to the state-of-the-arts.","['Xuhui Fan', 'Bin Li', 'Yi Wang', 'Yang Wang', 'Fang Chen']","['cs.AI', 'stat.ML']",2016-05-23 03:43:01+00:00
http://arxiv.org/abs/1605.06855v1,Smart broadcasting: Do you want to be seen?,"Many users in online social networks are constantly trying to gain attention
from their followers by broadcasting posts to them. These broadcasters are
likely to gain greater attention if their posts can remain visible for a longer
period of time among their followers' most recent feeds. Then when to post? In
this paper, we study the problem of smart broadcasting using the framework of
temporal point processes, where we model users feeds and posts as discrete
events occurring in continuous time. Based on such continuous-time model, then
choosing a broadcasting strategy for a user becomes a problem of designing the
conditional intensity of her posting events. We derive a novel formula which
links this conditional intensity with the visibility of the user in her
followers' feeds. Furthermore, by exploiting this formula, we develop an
efficient convex optimization framework for the when-to-post problem. Our
method can find broadcasting strategies that reach a desired visibility level
with provable guarantees. We experimented with data gathered from Twitter, and
show that our framework can consistently make broadcasters' post more visible
than alternatives.","['Mohammad Reza Karimi', 'Erfan Tavakoli', 'Mehrdad Farajtabar', 'Le Song', 'Manuel Gomez-Rodriguez']","['cs.SI', 'cs.LG', 'stat.ML']",2016-05-22 21:18:19+00:00
http://arxiv.org/abs/1605.06838v3,Causality on Longitudinal Data: Stable Specification Search in Constrained Structural Equation Modeling,"A typical problem in causal modeling is the instability of model structure
learning, i.e., small changes in finite data can result in completely different
optimal models. The present work introduces a novel causal modeling algorithm
for longitudinal data, that is robust for finite samples based on recent
advances in stability selection using subsampling and selection algorithms. Our
approach uses exploratory search but allows incorporation of prior knowledge,
e.g., the absence of a particular causal relationship between two specific
variables. We represent causal relationships using structural equation models.
Models are scored along two objectives: the model fit and the model complexity.
Since both objectives are often conflicting we apply a multi-objective
evolutionary algorithm to search for Pareto optimal models. To handle the
instability of small finite data samples, we repeatedly subsample the data and
select those substructures (from the optimal models) that are both stable and
parsimonious. These substructures can be visualized through a causal graph. Our
more exploratory approach achieves at least comparable performance as, but
often a significant improvement over state-of-the-art alternative approaches on
a simulated data set with a known ground truth. We also present the results of
our method on three real-world longitudinal data sets on chronic fatigue
syndrome, Alzheimer disease, and chronic kidney disease. The findings obtained
with our approach are generally in line with results from more
hypothesis-driven analyses in earlier studies and suggest some novel
relationships that deserve further research.","['Ridho Rahmadi', 'Perry Groot', 'Marieke HC van Rijn', 'Jan AJG van den Brand', 'Marianne Heins', 'Hans Knoop', 'Tom Heskes']","['stat.ML', 'cs.AI']",2016-05-22 19:28:25+00:00
http://arxiv.org/abs/1605.06796v2,Interpretable Distribution Features with Maximum Testing Power,"Two semimetrics on probability distributions are proposed, given as the sum
of differences of expectations of analytic functions evaluated at spatial or
frequency locations (i.e, features). The features are chosen so as to maximize
the distinguishability of the distributions, by optimizing a lower bound on
test power for a statistical test using these features. The result is a
parsimonious and interpretable indication of how and where two distributions
differ locally. An empirical estimate of the test power criterion converges
with increasing sample size, ensuring the quality of the returned features. In
real-world benchmarks on high-dimensional text and image data, linear-time
tests using the proposed semimetrics achieve comparable performance to the
state-of-the-art quadratic-time maximum mean discrepancy test, while returning
human-interpretable features that explain the test results.","['Wittawat Jitkrittum', 'Zoltan Szabo', 'Kacper Chwialkowski', 'Arthur Gretton']","['stat.ML', 'cs.LG', '46E22, 62G10', 'G.3; I.2.6']",2016-05-22 14:10:13+00:00
http://arxiv.org/abs/1605.06742v1,A Rapid Pattern-Recognition Method for Driving Types Using Clustering-Based Support Vector Machines,"A rapid pattern-recognition approach to characterize driver's
curve-negotiating behavior is proposed. To shorten the recognition time and
improve the recognition of driving styles, a k-means clustering-based support
vector machine ( kMC-SVM) method is developed and used for classifying drivers
into two types: aggressive and moderate. First, vehicle speed and throttle
opening are treated as the feature parameters to reflect the driving styles.
Second, to discriminate driver curve-negotiating behaviors and reduce the
number of support vectors, the k-means clustering method is used to extract and
gather the two types of driving data and shorten the recognition time. Then,
based on the clustering results, a support vector machine approach is utilized
to generate the hyperplane for judging and predicting to which types the human
driver are subject. Lastly, to verify the validity of the kMC-SVM method, a
cross-validation experiment is designed and conducted. The research results
show that the $ k $MC-SVM is an effective method to classify driving styles
with a short time, compared with SVM method.","['Wenshuo Wang', 'Junqiang Xi']","['stat.ML', 'cs.CV', 'cs.LG']",2016-05-22 06:15:11+00:00
http://arxiv.org/abs/1605.06718v3,The De-Biased Whittle Likelihood,"The Whittle likelihood is a widely used and computationally efficient
pseudo-likelihood. However, it is known to produce biased parameter estimates
for large classes of models. We propose a method for de-biasing Whittle
estimates for second-order stationary stochastic processes. The de-biased
Whittle likelihood can be computed in the same $\mathcal{O}(n\log n)$
operations as the standard approach. We demonstrate the superior performance of
the method in simulation studies and in application to a large-scale
oceanographic dataset, where in both cases the de-biased approach reduces bias
by up to two orders of magnitude, achieving estimates that are close to exact
maximum likelihood, at a fraction of the computational cost. We prove that the
method yields estimates that are consistent at an optimal convergence rate of
$n^{-1/2}$, under weaker assumptions than standard theory, where we do not
require that the power spectral density is continuous in frequency. We describe
how the method can be easily combined with standard methods of bias reduction,
such as tapering and differencing, to further reduce bias in parameter
estimates.","['Adam M. Sykulski', 'Sofia C. Olhede', 'Arthur P. Guillaumin', 'Jonathan M. Lilly', 'Jeffrey J. Early']","['stat.ME', 'math.ST', 'stat.CO', 'stat.ML', 'stat.TH']",2016-05-22 00:47:52+00:00
http://arxiv.org/abs/1605.06715v1,Factored Temporal Sigmoid Belief Networks for Sequence Learning,"Deep conditional generative models are developed to simultaneously learn the
temporal dependencies of multiple sequences. The model is designed by
introducing a three-way weight tensor to capture the multiplicative
interactions between side information and sequences. The proposed model builds
on the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid
Belief Networks (SBNs). The transition matrices are further factored to reduce
the number of parameters and improve generalization. When side information is
not available, a general framework for semi-supervised learning based on the
proposed model is constituted, allowing robust sequence classification.
Experimental results show that the proposed approach achieves state-of-the-art
predictive and classification performance on sequential data, and has the
capacity to synthesize sequences, with controlled style transitioning and
blending.","['Jiaming Song', 'Zhe Gan', 'Lawrence Carin']","['stat.ML', 'cs.LG']",2016-05-22 00:17:31+00:00
http://arxiv.org/abs/1605.06711v1,Learning From Hidden Traits: Joint Factor Analysis and Latent Clustering,"Dimensionality reduction techniques play an essential role in data analytics,
signal processing and machine learning. Dimensionality reduction is usually
performed in a preprocessing stage that is separate from subsequent data
analysis, such as clustering or classification. Finding reduced-dimension
representations that are well-suited for the intended task is more appealing.
This paper proposes a joint factor analysis and latent clustering framework,
which aims at learning cluster-aware low-dimensional representations of matrix
and tensor data. The proposed approach leverages matrix and tensor
factorization models that produce essentially unique latent representations of
the data to unravel latent cluster structure -- which is otherwise obscured
because of the freedom to apply an oblique transformation in latent space. At
the same time, latent cluster structure is used as prior information to enhance
the performance of factorization. Specific contributions include several
custom-built problem formulations, corresponding algorithms, and discussion of
associated convergence properties. Besides extensive simulations, real-world
datasets such as Reuters document data and MNIST image data are also employed
to showcase the effectiveness of the proposed approaches.","['Bo Yang', 'Xiao Fu', 'Nicholas D. Sidiropoulos']","['cs.LG', 'stat.ML']",2016-05-21 23:51:02+00:00
http://arxiv.org/abs/1605.06650v2,Latent Tree Models for Hierarchical Topic Detection,"We present a novel method for hierarchical topic detection where topics are
obtained by clustering documents in multiple ways. Specifically, we model
document collections using a class of graphical models called hierarchical
latent tree models (HLTMs). The variables at the bottom level of an HLTM are
observed binary variables that represent the presence/absence of words in a
document. The variables at other levels are binary latent variables, with those
at the lowest latent level representing word co-occurrence patterns and those
at higher levels representing co-occurrence of patterns at the level below.
Each latent variable gives a soft partition of the documents, and document
clusters in the partitions are interpreted as topics. Latent variables at high
levels of the hierarchy capture long-range word co-occurrence patterns and
hence give thematically more general topics, while those at low levels of the
hierarchy capture short-range word co-occurrence patterns and give thematically
more specific topics. Unlike LDA-based topic models, HLTMs do not refer to a
document generation process and use word variables instead of token variables.
They use a tree structure to model the relationships between topics and words,
which is conducive to the discovery of meaningful topics and topic hierarchies.","['Peixian Chen', 'Nevin L. Zhang', 'Tengfei Liu', 'Leonard K. M. Poon', 'Zhourong Chen', 'Farhan Khawar']","['cs.CL', 'cs.IR', 'cs.LG', 'stat.ML']",2016-05-21 14:36:33+00:00
http://arxiv.org/abs/1605.06636v2,Deep Transfer Learning with Joint Adaptation Networks,"Deep networks have been successfully applied to learn transferable features
for adapting models from a source domain to a different target domain. In this
paper, we present joint adaptation networks (JAN), which learn a transfer
network by aligning the joint distributions of multiple domain-specific layers
across domains based on a joint maximum mean discrepancy (JMMD) criterion.
Adversarial training strategy is adopted to maximize JMMD such that the
distributions of the source and target domains are made more distinguishable.
Learning can be performed by stochastic gradient descent with the gradients
computed by back-propagation in linear-time. Experiments testify that our model
yields state of the art results on standard datasets.","['Mingsheng Long', 'Han Zhu', 'Jianmin Wang', 'Michael I. Jordan']","['cs.LG', 'stat.ML']",2016-05-21 12:56:14+00:00
http://arxiv.org/abs/1605.06619v1,Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic Gradient Descent,"Asynchronous parallel optimization algorithms for solving large-scale machine
learning problems have drawn significant attention from academia to industry
recently. This paper proposes a novel algorithm, decoupled asynchronous
proximal stochastic gradient descent (DAP-SGD), to minimize an objective
function that is the composite of the average of multiple empirical losses and
a regularization term. Unlike the traditional asynchronous proximal stochastic
gradient descent (TAP-SGD) in which the master carries much of the computation
load, the proposed algorithm off-loads the majority of computation tasks from
the master to workers, and leaves the master to conduct simple addition
operations. This strategy yields an easy-to-parallelize algorithm, whose
performance is justified by theoretical convergence analyses. To be specific,
DAP-SGD achieves an $O(\log T/T)$ rate when the step-size is diminishing and an
ergodic $O(1/\sqrt{T})$ rate when the step-size is constant, where $T$ is the
number of total iterations.","['Yitan Li', 'Linli Xu', 'Xiaowei Zhong', 'Qing Ling']","['math.OC', 'cs.DC', 'cs.LG', 'stat.ML']",2016-05-21 10:27:50+00:00
http://arxiv.org/abs/1605.06593v3,Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback,"We study the online influence maximization problem in social networks under
the independent cascade model. Specifically, we aim to learn the set of ""best
influencers"" in a social network online while repeatedly interacting with it.
We address the challenges of (i) combinatorial action space, since the number
of feasible influencer sets grows exponentially with the maximum number of
influencers, and (ii) limited feedback, since only the influenced portion of
the network is observed. Under a stochastic semi-bandit feedback, we propose
and analyze IMLinUCB, a computationally efficient UCB-based algorithm. Our
bounds on the cumulative regret are polynomial in all quantities of interest,
achieve near-optimal dependence on the number of interactions and reflect the
topology of the network and the activation probabilities of its edges, thereby
giving insights on the problem complexity. To the best of our knowledge, these
are the first such results. Our experiments show that in several representative
graph topologies, the regret of IMLinUCB scales as suggested by our upper
bounds. IMLinUCB permits linear generalization and thus is both statistically
and computationally suitable for large-scale problems. Our experiments also
show that IMLinUCB with linear generalization can lead to low regret in
real-world online influence maximization.","['Zheng Wen', 'Branislav Kveton', 'Michal Valko', 'Sharan Vaswani']","['cs.LG', 'cs.AI', 'cs.SI', 'math.OC', 'stat.ML']",2016-05-21 06:07:53+00:00
http://arxiv.org/abs/1605.06457v1,Virtual Worlds as Proxy for Multi-Object Tracking Analysis,"Modern computer vision algorithms typically require expensive data
acquisition and accurate manual labeling. In this work, we instead leverage the
recent progress in computer graphics to generate fully labeled, dynamic, and
photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual
world cloning method, and validate our approach by building and publicly
releasing a new video dataset, called Virtual KITTI (see
http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),
automatically labeled with accurate ground truth for object detection,
tracking, scene and instance segmentation, depth, and optical flow. We provide
quantitative experimental evidence suggesting that (i) modern deep learning
algorithms pre-trained on real data behave similarly in real and virtual
worlds, and (ii) pre-training on virtual data improves performance. As the gap
between real and virtual worlds is small, virtual worlds enable measuring the
impact of various weather and imaging conditions on recognition performance,
all other things being equal. We show these factors may affect drastically
otherwise high-performing deep models for tracking.","['Adrien Gaidon', 'Qiao Wang', 'Yohann Cabon', 'Eleonora Vig']","['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']",2016-05-20 18:03:07+00:00
http://arxiv.org/abs/1605.06451v3,Fixed Points of Belief Propagation -- An Analysis via Polynomial Homotopy Continuation,"Belief propagation (BP) is an iterative method to perform approximate
inference on arbitrary graphical models. Whether BP converges and if the
solution is a unique fixed point depends on both the structure and the
parametrization of the model. To understand this dependence it is interesting
to find \emph{all} fixed points. In this work, we formulate a set of polynomial
equations, the solutions of which correspond to BP fixed points. To solve such
a nonlinear system we present the numerical polynomial-homotopy-continuation
(NPHC) method. Experiments on binary Ising models and on error-correcting codes
show how our method is capable of obtaining all BP fixed points. On Ising
models with fixed parameters we show how the structure influences both the
number of fixed points and the convergence properties. We further asses the
accuracy of the marginals and weighted combinations thereof. Weighting
marginals with their respective partition function increases the accuracy in
all experiments. Contrary to the conjecture that uniqueness of BP fixed points
implies convergence, we find graphs for which BP fails to converge, even though
a unique fixed point exists. Moreover, we show that this fixed point gives a
good approximation, and the NPHC method is able to obtain this fixed point.","['Christian Knoll', 'Franz Pernkopf', 'Dhagash Mehta', 'Tianran Chen']","['stat.ML', 'math.AG']",2016-05-20 17:42:03+00:00
http://arxiv.org/abs/1605.06444v3,Unreasonable Effectiveness of Learning Neural Networks: From Accessible States and Robust Ensembles to Basic Algorithmic Schemes,"In artificial neural networks, learning from data is a computationally
demanding task in which a large number of connection weights are iteratively
tuned through stochastic-gradient-based heuristic processes over a
cost-function. It is not well understood how learning occurs in these systems,
in particular how they avoid getting trapped in configurations with poor
computational performance. Here we study the difficult case of networks with
discrete weights, where the optimization landscape is very rough even for
simple architectures, and provide theoretical and numerical evidence of the
existence of rare - but extremely dense and accessible - regions of
configurations in the network weight space. We define a novel measure, which we
call the ""robust ensemble"" (RE), which suppresses trapping by isolated
configurations and amplifies the role of these dense regions. We analytically
compute the RE in some exactly solvable models, and also provide a general
algorithmic scheme which is straightforward to implement: define a
cost-function given by a sum of a finite number of replicas of the original
cost-function, with a constraint centering the replicas around a driving
assignment. To illustrate this, we derive several powerful new algorithms,
ranging from Markov Chains to message passing to gradient descent processes,
where the algorithms target the robust dense states, resulting in substantial
improvements in performance. The weak dependence on the number of precision
bits of the weights leads us to conjecture that very similar reasoning applies
to more conventional neural networks. Analogous algorithmic schemes can also be
applied to other optimization problems.","['Carlo Baldassi', 'Christian Borgs', 'Jennifer Chayes', 'Alessandro Ingrosso', 'Carlo Lucibello', 'Luca Saglietti', 'Riccardo Zecchina']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG']",2016-05-20 17:27:18+00:00
http://arxiv.org/abs/1605.06443v2,Structured Prediction Theory Based on Factor Graph Complexity,"We present a general theoretical analysis of structured prediction with a
series of new results. We give new data-dependent margin guarantees for
structured prediction for a very wide family of loss functions and a general
family of hypotheses, with an arbitrary factor graph decomposition. These are
the tightest margin bounds known for both standard multi-class and general
structured prediction problems. Our guarantees are expressed in terms of a
data-dependent complexity measure, factor graph complexity, which we show can
be estimated from data and bounded in terms of familiar quantities. We further
extend our theory by leveraging the principle of Voted Risk Minimization (VRM)
and show that learning is possible even with complex factor graphs. We present
new learning bounds for this advanced setting, which we use to design two new
algorithms, Voted Conditional Random Field (VCRF) and Voted Structured Boosting
(StructBoost). These algorithms can make use of complex features and factor
graphs and yet benefit from favorable learning guarantees. We also report the
results of experiments with VCRF on several datasets to validate our theory.","['Corinna Cortes', 'Mehryar Mohri', 'Vitaly Kuznetsov', 'Scott Yang']","['stat.ML', 'cs.LG']",2016-05-20 17:21:17+00:00
http://arxiv.org/abs/1605.06432v3,Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data,"We introduce Deep Variational Bayes Filters (DVBF), a new method for
unsupervised learning and identification of latent Markovian state space
models. Leveraging recent advances in Stochastic Gradient Variational Bayes,
DVBF can overcome intractable inference distributions via variational
inference. Thus, it can handle highly nonlinear input data with temporal and
spatial dependencies such as image sequences without domain knowledge. Our
experiments show that enabling backpropagation through transitions enforces
state space assumptions and significantly improves information content of the
latent embedding. This also enables realistic long-term prediction.","['Maximilian Karl', 'Maximilian Soelch', 'Justin Bayer', 'Patrick van der Smagt']","['stat.ML', 'cs.LG', 'cs.SY']",2016-05-20 16:52:22+00:00
http://arxiv.org/abs/1605.06423v3,Coresets for Scalable Bayesian Logistic Regression,"The use of Bayesian methods in large-scale data settings is attractive
because of the rich hierarchical models, uncertainty quantification, and prior
specification they provide. Standard Bayesian inference algorithms are
computationally expensive, however, making their direct application to large
datasets difficult or infeasible. Recent work on scaling Bayesian inference has
focused on modifying the underlying algorithms to, for example, use only a
random data subsample at each iteration. We leverage the insight that data is
often redundant to instead obtain a weighted subset of the data (called a
coreset) that is much smaller than the original dataset. We can then use this
small coreset in any number of existing posterior inference algorithms without
modification. In this paper, we develop an efficient coreset construction
algorithm for Bayesian logistic regression models. We provide theoretical
guarantees on the size and approximation quality of the coreset -- both for
fixed, known datasets, and in expectation for a wide class of data generative
models. Crucially, the proposed approach also permits efficient construction of
the coreset in both streaming and parallel settings, with minimal additional
effort. We demonstrate the efficacy of our approach on a number of synthetic
and real-world datasets, and find that, in practice, the size of the coreset is
independent of the original dataset size. Furthermore, constructing the coreset
takes a negligible amount of time compared to that required to run MCMC on it.","['Jonathan H. Huggins', 'Trevor Campbell', 'Tamara Broderick']","['stat.CO', 'cs.DS', 'stat.ML']",2016-05-20 16:26:45+00:00
http://arxiv.org/abs/1605.06422v3,Fast Randomized Semi-Supervised Clustering,"We consider the problem of clustering partially labeled data from a minimal
number of randomly chosen pairwise comparisons between the items. We introduce
an efficient local algorithm based on a power iteration of the non-backtracking
operator and study its performance on a simple model. For the case of two
clusters, we give bounds on the classification error and show that a small
error can be achieved from $O(n)$ randomly chosen measurements, where $n$ is
the number of items in the dataset. Our algorithm is therefore efficient both
in terms of time and space complexities. We also investigate numerically the
performance of the algorithm on synthetic and real world data.","['Alaa Saade', 'Florent Krzakala', 'Marc Lelarge', 'Lenka Zdeborová']","['cs.LG', 'math.PR', 'math.ST', 'stat.ML', 'stat.TH']",2016-05-20 16:21:13+00:00
http://arxiv.org/abs/1605.06420v4,Quantifying the accuracy of approximate diffusions and Markov chains,"Markov chains and diffusion processes are indispensable tools in machine
learning and statistics that are used for inference, sampling, and modeling.
With the growth of large-scale datasets, the computational cost associated with
simulating these stochastic processes can be considerable, and many algorithms
have been proposed to approximate the underlying Markov chain or diffusion. A
fundamental question is how the computational savings trade off against the
statistical error incurred due to approximations. This paper develops general
results that address this question. We bound the Wasserstein distance between
the equilibrium distributions of two diffusions as a function of their mixing
rates and the deviation in their drifts. We show that this error bound is tight
in simple Gaussian settings. Our general result on continuous diffusions can be
discretized to provide insights into the computational-statistical trade-off of
Markov chains. As an illustration, we apply our framework to derive
finite-sample error bounds of approximate unadjusted Langevin dynamics. We
characterize computation-constrained settings where, by using fast-to-compute
approximate gradients in the Langevin dynamics, we obtain more accurate samples
compared to using the exact gradients. Finally, as an additional application of
our approach, we quantify the accuracy of approximate zig-zag sampling. Our
theoretical analyses are supported by simulation experiments.","['Jonathan H. Huggins', 'James Zou']","['math.ST', 'math.PR', 'stat.CO', 'stat.ML', 'stat.TH']",2016-05-20 16:17:22+00:00
http://arxiv.org/abs/1605.06416v3,Statistical Inference for Cluster Trees,"A cluster tree provides a highly-interpretable summary of a density function
by representing the hierarchy of its high-density clusters. It is estimated
using the empirical tree, which is the cluster tree constructed from a density
estimator. This paper addresses the basic question of quantifying our
uncertainty by assessing the statistical significance of topological features
of an empirical cluster tree. We first study a variety of metrics that can be
used to compare different trees, analyze their properties and assess their
suitability for inference. We then propose methods to construct and summarize
confidence sets for the unknown true cluster tree. We introduce a partial
ordering on cluster trees which we use to prune some of the statistically
insignificant features of the empirical tree, yielding interpretable and
parsimonious cluster trees. Finally, we illustrate the proposed methods on a
variety of synthetic examples and furthermore demonstrate their utility in the
analysis of a Graft-versus-Host Disease (GvHD) data set.","['Jisu Kim', 'Yen-Chi Chen', 'Sivaraman Balakrishnan', 'Alessandro Rinaldo', 'Larry Wasserman']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2016-05-20 16:04:01+00:00
http://arxiv.org/abs/1605.06376v4,Fast $ε$-free Inference of Simulation Models with Bayesian Conditional Density Estimation,"Many statistical models can be simulated forwards but have intractable
likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer
properties of these models from data. Traditionally these methods approximate
the posterior over parameters by conditioning on data being inside an
$\epsilon$-ball around the observed data, which is only correct in the limit
$\epsilon\!\rightarrow\!0$. Monte Carlo methods can then draw samples from the
approximate posterior to approximate predictions or error bars on parameters.
These algorithms critically slow down as $\epsilon\!\rightarrow\!0$, and in
practice draw samples from a broader distribution than the posterior. We
propose a new approach to likelihood-free inference based on Bayesian
conditional density estimation. Preliminary inferences based on limited
simulation data are used to guide later simulations. In some cases, learning an
accurate parametric representation of the entire true posterior distribution
requires fewer model simulations than Monte Carlo ABC methods need to produce a
single sample from an approximate posterior.","['George Papamakarios', 'Iain Murray']","['stat.ML', 'cs.LG', 'stat.CO']",2016-05-20 14:34:38+00:00
http://arxiv.org/abs/1605.06359v3,Learning to Discover Sparse Graphical Models,"We consider structure discovery of undirected graphical models from
observational data. Inferring likely structures from few examples is a complex
task often requiring the formulation of priors and sophisticated inference
procedures. Popular methods rely on estimating a penalized maximum likelihood
of the precision matrix. However, in these approaches structure recovery is an
indirect consequence of the data-fit term, the penalty can be difficult to
adapt for domain-specific knowledge, and the inference is computationally
demanding. By contrast, it may be easier to generate training samples of data
that arise from graphs with the desired structure properties. We propose here
to leverage this latter source of information as training data to learn a
function, parametrized by a neural network that maps empirical covariance
matrices to estimated graph structures. Learning this function brings two
benefits: it implicitly models the desired structure or sparsity properties to
form suitable priors, and it can be tailored to the specific problem of edge
structure discovery, rather than maximizing data likelihood. Applying this
framework, we find our learnable graph-discovery method trained on synthetic
data generalizes well: identifying relevant edges in both synthetic and real
data, completely unknown at training time. We find that on genetics, brain
imaging, and simulation data we obtain performance generally superior to
analytical methods.","['Eugene Belilovsky', 'Kyle Kastner', 'Gaël Varoquaux', 'Matthew Blaschko']",['stat.ML'],2016-05-20 13:58:21+00:00
http://arxiv.org/abs/1605.06336v1,Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA,"Nonlinear independent component analysis (ICA) provides an appealing
framework for unsupervised feature learning, but the models proposed so far are
not identifiable. Here, we first propose a new intuitive principle of
unsupervised deep learning from time series which uses the nonstationary
structure of the data. Our learning principle, time-contrastive learning (TCL),
finds a representation which allows optimal discrimination of time segments
(windows). Surprisingly, we show how TCL can be related to a nonlinear ICA
model, when ICA is redefined to include temporal nonstationarities. In
particular, we show that TCL combined with linear ICA estimates the nonlinear
ICA model up to point-wise transformations of the sources, and this solution is
unique --- thus providing the first identifiability result for nonlinear ICA
which is rigorous, constructive, as well as very general.","['Aapo Hyvarinen', 'Hiroshi Morioka']","['stat.ML', 'cs.LG']",2016-05-20 12:59:22+00:00
http://arxiv.org/abs/1605.06276v2,Piece-wise quadratic approximations of arbitrary error functions for fast and robust machine learning,"Most of machine learning approaches have stemmed from the application of
minimizing the mean squared distance principle, based on the computationally
efficient quadratic optimization methods. However, when faced with
high-dimensional and noisy data, the quadratic error functionals demonstrated
many weaknesses including high sensitivity to contaminating factors and
dimensionality curse. Therefore, a lot of recent applications in machine
learning exploited properties of non-quadratic error functionals based on $L_1$
norm or even sub-linear potentials corresponding to quasinorms $L_p$ ($0<p<1$).
The back side of these approaches is increase in computational cost for
optimization. Till so far, no approaches have been suggested to deal with {\it
arbitrary} error functionals, in a flexible and computationally efficient
framework. In this paper, we develop a theory and basic universal data
approximation algorithms ($k$-means, principal components, principal manifolds
and graphs, regularized and sparse regression), based on piece-wise quadratic
error potentials of subquadratic growth (PQSQ potentials). We develop a new and
universal framework to minimize {\it arbitrary sub-quadratic error potentials}
using an algorithm with guaranteed fast convergence to the local or global
error minimum. The theory of PQSQ potentials is based on the notion of the cone
of minorant functions, and represents a natural approximation formalism based
on the application of min-plus algebra. The approach can be applied in most of
existing machine learning methods, including methods of data approximation and
regularized and sparse regression, leading to the improvement in the
computational cost/accuracy trade-off. We demonstrate that on synthetic and
real-life datasets PQSQ-based machine learning methods achieve orders of
magnitude faster computational performance than the corresponding
state-of-the-art methods.","['A. N. Gorban', 'E. M. Mirkes', 'A. Zinovyev']","['cs.LG', 'stat.ML']",2016-05-20 10:25:47+00:00
http://arxiv.org/abs/1605.06265v2,End-to-End Kernel Learning with Supervised Convolutional Kernel Networks,"In this paper, we introduce a new image representation based on a multilayer
kernel machine. Unlike traditional kernel methods where data representation is
decoupled from the prediction task, we learn how to shape the kernel with
supervision. We proceed by first proposing improvements of the
recently-introduced convolutional kernel networks (CKNs) in the context of
unsupervised learning; then, we derive backpropagation rules to take advantage
of labeled training data. The resulting model is a new type of convolutional
neural network, where optimizing the filters at each layer is equivalent to
learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We
show that our method achieves reasonably competitive performance for image
classification on some standard ""deep learning"" datasets such as CIFAR-10 and
SVHN, and also for image super-resolution, demonstrating the applicability of
our approach to a large variety of image-related tasks.",['Julien Mairal'],"['stat.ML', 'cs.CV', 'cs.LG']",2016-05-20 09:52:14+00:00
http://arxiv.org/abs/1605.06220v1,Convergence of Contrastive Divergence with Annealed Learning Rate in Exponential Family,"In our recent paper, we showed that in exponential family, contrastive
divergence (CD) with fixed learning rate will give asymptotically consistent
estimates \cite{wu2016convergence}. In this paper, we establish consistency and
convergence rate of CD with annealed learning rate $\eta_t$. Specifically,
suppose CD-$m$ generates the sequence of parameters $\{\theta_t\}_{t \ge 0}$
using an i.i.d. data sample $\mathbf{X}_1^n \sim p_{\theta^*}$ of size $n$,
then $\delta_n(\mathbf{X}_1^n) = \limsup_{t \to \infty} \Vert \sum_{s=t_0}^t
\eta_s \theta_s / \sum_{s=t_0}^t \eta_s - \theta^* \Vert$ converges in
probability to 0 at a rate of $1/\sqrt[3]{n}$. The number ($m$) of MCMC
transitions in CD only affects the coefficient factor of convergence rate. Our
proof is not a simple extension of the one in \cite{wu2016convergence}. which
depends critically on the fact that $\{\theta_t\}_{t \ge 0}$ is a homogeneous
Markov chain conditional on the observed sample $\mathbf{X}_1^n$. Under
annealed learning rate, the homogeneous Markov property is not available and we
have to develop an alternative approach based on super-martingales. Experiment
results of CD on a fully-visible $2\times 2$ Boltzmann Machine are provided to
demonstrate our theoretical results.","['Bai Jiang', 'Tung-yu Wu', 'Wing H. Wong']","['stat.ML', 'cs.LG']",2016-05-20 06:26:38+00:00
http://arxiv.org/abs/1605.06201v4,Adversarial Delays in Online Strongly-Convex Optimization,"We consider the problem of strongly-convex online optimization in presence of
adversarial delays; in a T-iteration online game, the feedback of the player's
query at time t is arbitrarily delayed by an adversary for d_t rounds and
delivered before the game ends, at iteration t+d_t-1. Specifically for
\algo{online-gradient-descent} algorithm we show it has a simple regret bound
of \Oh{\sum_{t=1}^T \log (1+ \frac{d_t}{t})}. This gives a clear and simple
bound without resorting any distributional and limiting assumptions on the
delays. We further show how this result encompasses and generalizes several of
the existing known results in the literature. Specifically it matches the
celebrated logarithmic regret \Oh{\log T} when there are no delays (i.e. d_t =
1) and regret bound of \Oh{\tau \log T} for constant delays d_t = \tau.","['Daniel Khashabi', 'Kent Quanrud', 'Amirhossein Taghvaei']","['cs.LG', 'cs.AI', 'stat.ML']",2016-05-20 02:55:59+00:00
http://arxiv.org/abs/1605.06197v3,Stick-Breaking Variational Autoencoders,"We extend Stochastic Gradient Variational Bayes to perform posterior
inference for the weights of Stick-Breaking processes. This development allows
us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian
nonparametric version of the variational autoencoder that has a latent
representation with stochastic dimensionality. We experimentally demonstrate
that the SB-VAE, and a semi-supervised variant, learn highly discriminative
latent representations that often outperform the Gaussian VAE's.","['Eric Nalisnick', 'Padhraic Smyth']",['stat.ML'],2016-05-20 02:20:13+00:00
http://arxiv.org/abs/1605.06181v1,Variational hybridization and transformation for large inaccurate noisy-or networks,"Variational inference provides approximations to the computationally
intractable posterior distribution in Bayesian networks. A prominent medical
application of noisy-or Bayesian network is to infer potential diseases given
observed symptoms. Previous studies focus on approximating a handful of
complicated pathological cases using variational transformation. Our goal is to
use variational transformation as part of a novel hybridized inference for
serving reliable and real time diagnosis at web scale. We propose a hybridized
inference that allows variational parameters to be estimated without disease
posteriors or priors, making the inference faster and much of its computation
recyclable. In addition, we propose a transformation ranking algorithm that is
very stable to large variances in network prior probabilities, a common issue
that arises in medical applications of Bayesian networks. In experiments, we
perform comparative study on a large real life medical network and scalability
study on a much larger (36,000x) synthesized network.","['Yusheng Xie', 'Nan Du', 'Wei Fan', 'Jing Zhai', 'Weicheng Zhu']","['cs.LG', 'cs.AI', 'stat.ML']",2016-05-20 00:31:07+00:00
http://arxiv.org/abs/1605.06049v2,A Multi-Batch L-BFGS Method for Machine Learning,"The question of how to parallelize the stochastic gradient descent (SGD)
method has received much attention in the literature. In this paper, we focus
instead on batch methods that use a sizeable fraction of the training set at
each iteration to facilitate parallelism, and that employ second-order
information. In order to improve the learning process, we follow a multi-batch
approach in which the batch changes at each iteration. This can cause
difficulties because L-BFGS employs gradient differences to update the Hessian
approximations, and when these gradients are computed using different data
points the process can be unstable. This paper shows how to perform stable
quasi-Newton updating in the multi-batch setting, illustrates the behavior of
the algorithm in a distributed computing platform, and studies its convergence
properties for both the convex and nonconvex cases.","['Albert S. Berahas', 'Jorge Nocedal', 'Martin Takáč']","['math.OC', 'cs.LG', 'stat.ML']",2016-05-19 16:53:50+00:00
http://arxiv.org/abs/1605.05969v3,Randomized Primal-Dual Proximal Block Coordinate Updates,"In this paper we propose a randomized primal-dual proximal block coordinate
updating framework for a general multi-block convex optimization model with
coupled objective function and linear constraints. Assuming mere convexity, we
establish its $O(1/t)$ convergence rate in terms of the objective value and
feasibility measure. The framework includes several existing algorithms as
special cases such as a primal-dual method for bilinear saddle-point problems
(PD-S), the proximal Jacobian ADMM (Prox-JADMM) and a randomized variant of the
ADMM method for multi-block convex optimization. Our analysis recovers and/or
strengthens the convergence properties of several existing algorithms. For
example, for PD-S our result leads to the same order of convergence rate
without the previously assumed boundedness condition on the constraint sets,
and for Prox-JADMM the new result provides convergence rate in terms of the
objective value and the feasibility violation. It is well known that the
original ADMM may fail to converge when the number of blocks exceeds two. Our
result shows that if an appropriate randomization procedure is invoked to
select the updating blocks, then a sublinear rate of convergence in expectation
can be guaranteed for multi-block ADMM, without assuming any strong convexity.
The new approach is also extended to solve problems where only a stochastic
approximation of the (sub-)gradient of the objective is available, and we
establish an $O(1/\sqrt{t})$ convergence rate of the extended approach for
solving stochastic programming.","['Xiang Gao', 'Yangyang Xu', 'Shuzhong Zhang']","['math.OC', 'math.NA', 'stat.ML', '90C25, 95C06, 68W20']",2016-05-19 14:20:20+00:00
http://arxiv.org/abs/1605.05918v2,Bayesian Variable Selection for Globally Sparse Probabilistic PCA,"Sparse versions of principal component analysis (PCA) have imposed themselves
as simple, yet powerful ways of selecting relevant features of high-dimensional
data in an unsupervised manner. However, when several sparse principal
components are computed, the interpretation of the selected variables is
difficult since each axis has its own sparsity pattern and has to be
interpreted separately. To overcome this drawback, we propose a Bayesian
procedure called globally sparse probabilistic PCA (GSPPCA) that allows to
obtain several sparse components with the same sparsity pattern. This allows
the practitioner to identify the original variables which are relevant to
describe the data. To this end, using Roweis' probabilistic interpretation of
PCA and a Gaussian prior on the loading matrix, we provide the first exact
computation of the marginal likelihood of a Bayesian PCA model. To avoid the
drawbacks of discrete model selection, a simple relaxation of this framework is
presented. It allows to find a path of models using a variational
expectation-maximization algorithm. The exact marginal likelihood is then
maximized over this path. This approach is illustrated on real and synthetic
data sets. In particular, using unlabeled microarray data, GSPPCA infers much
more relevant gene subsets than traditional sparse PCA algorithms.","['Charles Bouveyron', 'Pierre Latouche', 'Pierre-Alexandre Mattei']",['stat.ML'],2016-05-19 12:34:34+00:00
http://arxiv.org/abs/1605.05860v3,False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking,"With the rapid growth of crowdsourcing platforms it has become easy and
relatively inexpensive to collect a dataset labeled by multiple annotators in a
short time. However due to the lack of control over the quality of the
annotators, some abnormal annotators may be affected by position bias which can
potentially degrade the quality of the final consensus labels. In this paper we
introduce a statistical framework to model and detect annotator's position bias
in order to control the false discovery rate (FDR) without a prior knowledge on
the amount of biased annotators - the expected fraction of false discoveries
among all discoveries being not too high, in order to assure that most of the
discoveries are indeed true and replicable. The key technical development
relies on some new knockoff filters adapted to our problem and new algorithms
based on the Inverse Scale Space dynamics whose discretization is potentially
suitable for large scale crowdsourcing data analysis. Our studies are supported
by experiments with both simulated examples and real-world data. The proposed
framework provides us a useful tool for quantitatively studying annotator's
abnormal behavior in crowdsourcing data arising from machine learning,
sociology, computer vision, multimedia, etc.","['Qianqian Xu', 'Jiechao Xiong', 'Xiaochun Cao', 'Yuan Yao']",['stat.ML'],2016-05-19 09:14:39+00:00
http://arxiv.org/abs/1605.05799v1,Recurrent Exponential-Family Harmoniums without Backprop-Through-Time,"Exponential-family harmoniums (EFHs), which extend restricted Boltzmann
machines (RBMs) from Bernoulli random variables to other exponential families
(Welling et al., 2005), are generative models that can be trained with
unsupervised-learning techniques, like contrastive divergence (Hinton et al.
2006; Hinton, 2002), as density estimators for static data. Methods for
extending RBMs--and likewise EFHs--to data with temporal dependencies have been
proposed previously (Sutskever and Hinton, 2007; Sutskever et al., 2009), the
learning procedure being validated by qualitative assessment of the generative
model. Here we propose and justify, from a very different perspective, an
alternative training procedure, proving sufficient conditions for optimal
inference under that procedure. The resulting algorithm can be learned with
only forward passes through the data--backprop-through-time is not required, as
in previous approaches. The proof exploits a recent result about information
retention in density estimators (Makin and Sabes, 2015), and applies it to a
""recurrent EFH"" (rEFH) by induction. Finally, we demonstrate optimality by
simulation, testing the rEFH: (1) as a filter on training data generated with a
linear dynamical system, the position of which is noisily reported by a
population of ""neurons"" with Poisson-distributed spike counts; and (2) with the
qualitative experiments proposed by Sutskever et al. (2009).","['Joseph G. Makin', 'Benjamin K. Dichter', 'Philip N. Sabes']","['cs.LG', 'stat.ML']",2016-05-19 03:19:31+00:00
http://arxiv.org/abs/1605.05785v2,Efficient Nonparametric Smoothness Estimation,"Sobolev quantities (norms, inner products, and distances) of probability
density functions are important in the theory of nonparametric statistics, but
have rarely been used in practice, partly due to a lack of practical
estimators. They also include, as special cases, $L^2$ quantities which are
used in many applications. We propose and analyze a family of estimators for
Sobolev quantities of unknown probability density functions. We bound the bias
and variance of our estimators over finite samples, finding that they are
generally minimax rate-optimal. Our estimators are significantly more
computationally tractable than previous estimators, and exhibit a
statistical/computational trade-off allowing them to adapt to computational
constraints. We also draw theoretical connections to recent work on fast
two-sample testing. Finally, we empirically validate our estimators on
synthetic data.","['Shashank Singh', 'Simon S. Du', 'Barnabás Póczos']","['math.ST', 'cs.IT', 'math.IT', 'stat.ML', 'stat.TH']",2016-05-19 00:29:38+00:00
http://arxiv.org/abs/1605.05776v4,The Quality of the Covariance Selection Through Detection Problem and AUC Bounds,"We consider the problem of quantifying the quality of a model selection
problem for a graphical model. We discuss this by formulating the problem as a
detection problem. Model selection problems usually minimize a distance between
the original distribution and the model distribution. For the special case of
Gaussian distributions, the model selection problem simplifies to the
covariance selection problem which is widely discussed in literature by
Dempster [2] where the likelihood criterion is maximized or equivalently the
Kullback-Leibler (KL) divergence is minimized to compute the model covariance
matrix. While this solution is optimal for Gaussian distributions in the sense
of the KL divergence, it is not optimal when compared with other information
divergences and criteria such as Area Under the Curve (AUC).
  In this paper, we analytically compute upper and lower bounds for the AUC and
discuss the quality of model selection problem using the AUC and its bounds as
an accuracy measure in detection problem. We define the correlation
approximation matrix (CAM) and show that analytical computation of the KL
divergence, the AUC and its bounds only depend on the eigenvalues of CAM. We
also show the relationship between the AUC, the KL divergence and the ROC curve
by optimizing with respect to the ROC curve. In the examples provided, we pick
tree structures as the simplest graphical models. We perform simulations on
fully-connected graphs and compute the tree structured models by applying the
widely used Chow-Liu algorithm [3]. Examples show that the quality of tree
approximation models are not good in general based on information divergences,
the AUC and its bounds when the number of nodes in the graphical model is
large. We show both analytically and by simulations that the 1-AUC for the tree
approximation model decays exponentially as the dimension of graphical model
increases.","['Navid Tafaghodi Khajavi', 'Anthony Kuh']","['cs.IT', 'math.IT', 'stat.ML']",2016-05-18 22:22:57+00:00
http://arxiv.org/abs/1605.05775v2,Supervised Learning with Quantum-Inspired Tensor Networks,"Tensor networks are efficient representations of high-dimensional tensors
which have been very successful for physics and mathematics applications. We
demonstrate how algorithms for optimizing such networks can be adapted to
supervised learning tasks by using matrix product states (tensor trains) to
parameterize models for classifying images. For the MNIST data set we obtain
less than 1% test set classification error. We discuss how the tensor network
form imparts additional structure to the learned model and suggest a possible
generative interpretation.","['E. Miles Stoudenmire', 'David J. Schwab']","['stat.ML', 'cond-mat.str-el', 'cs.LG']",2016-05-18 22:20:35+00:00
http://arxiv.org/abs/1605.05721v4,Linearized GMM Kernels and Normalized Random Fourier Features,"The method of ""random Fourier features (RFF)"" has become a popular tool for
approximating the ""radial basis function (RBF)"" kernel. The variance of RFF is
actually large. Interestingly, the variance can be substantially reduced by a
simple normalization step as we theoretically demonstrate. We name the improved
scheme as the ""normalized RFF (NRFF)"".
  We also propose the ""generalized min-max (GMM)"" kernel as a measure of data
similarity. GMM is positive definite as there is an associated hashing method
named ""generalized consistent weighted sampling (GCWS)"" which linearizes this
nonlinear kernel. We provide an extensive empirical evaluation of the RBF
kernel and the GMM kernel on more than 50 publicly available datasets. For a
majority of the datasets, the (tuning-free) GMM kernel outperforms the
best-tuned RBF kernel.
  We conduct extensive experiments for comparing the linearized RBF kernel
using NRFF with the linearized GMM kernel using GCWS. We observe that, to reach
a comparable classification accuracy, GCWS typically requires substantially
fewer samples than NRFF, even on datasets where the original RBF kernel
outperforms the original GMM kernel. The empirical success of GCWS (compared to
NRFF) can also be explained from a theoretical perspective. Firstly, the
relative variance (normalized by the squared expectation) of GCWS is
substantially smaller than that of NRFF, except for the very high similarity
region (where the variances of both methods are close to zero). Secondly, if we
make a model assumption on the data, we can show analytically that GCWS
exhibits much smaller variance than NRFF for estimating the same object (e.g.,
the RBF kernel), except for the very high similarity region.",['Ping Li'],"['cs.LG', 'cs.IR', 'stat.ML']",2016-05-18 19:54:22+00:00
http://arxiv.org/abs/1605.05697v1,Online Algorithms For Parameter Mean And Variance Estimation In Dynamic Regression Models,"We study the problem of estimating the parameters of a regression model from
a set of observations, each consisting of a response and a predictor. The
response is assumed to be related to the predictor via a regression model of
unknown parameters. Often, in such models the parameters to be estimated are
assumed to be constant. Here we consider the more general scenario where the
parameters are allowed to evolve over time, a more natural assumption for many
applications. We model these dynamics via a linear update equation with
additive noise that is often used in a wide range of engineering applications,
particularly in the well-known and widely used Kalman filter (where the system
state it seeks to estimate maps to the parameter values here). We derive an
approximate algorithm to estimate both the mean and the variance of the
parameter estimates in an online fashion for a generic regression model. This
algorithm turns out to be equivalent to the extended Kalman filter. We
specialize our algorithm to the multivariate exponential family distribution to
obtain a generalization of the generalized linear model (GLM). Because the
common regression models encountered in practice such as logistic, exponential
and multinomial all have observations modeled through an exponential family
distribution, our results are used to easily obtain algorithms for online mean
and variance parameter estimation for all these regression models in the
context of time-dependent parameters. Lastly, we propose to use these
algorithms in the contextual multi-armed bandit scenario, where so far model
parameters are assumed static and observations univariate and Gaussian or
Bernoulli. Both of these restrictions can be relaxed using the algorithms
described here, which we combine with Thompson sampling to show the resulting
performance on a simulation.",['Carlos Alberto Gomez-Uribe'],['stat.ML'],2016-05-18 19:00:39+00:00
http://arxiv.org/abs/1605.05622v3,Gaussian variational approximation with sparse precision matrices,"We consider the problem of learning a Gaussian variational approximation to
the posterior distribution for a high-dimensional parameter, where we impose
sparsity in the precision matrix to reflect appropriate conditional
independence structure in the model. Incorporating sparsity in the precision
matrix allows the Gaussian variational distribution to be both flexible and
parsimonious, and the sparsity is achieved through parameterization in terms of
the Cholesky factor. Efficient stochastic gradient methods which make
appropriate use of gradient information for the target distribution are
developed for the optimization. We consider alternative estimators of the
stochastic gradients which have lower variation and are more stable. Our
approach is illustrated using generalized linear mixed models and state space
models for time series.","['Linda S. L. Tan', 'David J. Nott']","['stat.CO', 'stat.ML']",2016-05-18 15:38:16+00:00
http://arxiv.org/abs/1605.05537v5,ABC random forests for Bayesian parameter inference,"This preprint has been reviewed and recommended by Peer Community In
Evolutionary Biology (http://dx.doi.org/10.24072/pci.evolbiol.100036).
Approximate Bayesian computation (ABC) has grown into a standard methodology
that manages Bayesian inference for models associated with intractable
likelihood functions. Most ABC implementations require the preliminary
selection of a vector of informative statistics summarizing raw data.
Furthermore, in almost all existing implementations, the tolerance level that
separates acceptance from rejection of simulated parameter values needs to be
calibrated. We propose to conduct likelihood-free Bayesian inferences about
parameters with no prior selection of the relevant components of the summary
statistics and bypassing the derivation of the associated tolerance level. The
approach relies on the random forest methodology of Breiman (2001) applied in a
(non parametric) regression setting. We advocate the derivation of a new random
forest for each component of the parameter vector of interest. When compared
with earlier ABC solutions, this method offers significant gains in terms of
robustness to the choice of the summary statistics, does not depend on any type
of tolerance level, and is a good trade-off in term of quality of point
estimator precision and credible interval estimations for a given computing
time. We illustrate the performance of our methodological proposal and compare
it with earlier ABC methods on a Normal toy example and a population genetics
example dealing with human population evolution. All methods designed here have
been incorporated in the R package abcrf (version 1.7) available on CRAN.","['Louis Raynal', 'Jean-Michel Marin', 'Pierre Pudlo', 'Mathieu Ribatet', 'Christian P. Robert', 'Arnaud Estoup']","['stat.ME', 'stat.CO', 'stat.ML']",2016-05-18 12:04:38+00:00
http://arxiv.org/abs/1605.05509v2,Learning activation functions from data using cubic spline interpolation,"Neural networks require a careful design in order to perform properly on a
given task. In particular, selecting a good activation function (possibly in a
data-dependent fashion) is a crucial step, which remains an open problem in the
research community. Despite a large amount of investigations, most current
implementations simply select one fixed function from a small set of
candidates, which is not adapted during training, and is shared among all
neurons throughout the different layers. However, neither two of these
assumptions can be supposed optimal in practice. In this paper, we present a
principled way to have data-dependent adaptation of the activation functions,
which is performed independently for each neuron. This is achieved by
leveraging over past and present advances on cubic spline interpolation,
allowing for local adaptation of the functions around their regions of use. The
resulting algorithm is relatively cheap to implement, and overfitting is
counterbalanced by the inclusion of a novel damping criterion, which penalizes
unwanted oscillations from a predefined shape. Experimental results validate
the proposal over two well-known benchmarks.","['Simone Scardapane', 'Michele Scarpiniti', 'Danilo Comminiello', 'Aurelio Uncini']","['stat.ML', 'cs.LG', 'cs.NE']",2016-05-18 10:46:01+00:00
