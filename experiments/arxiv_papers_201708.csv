id,title,abstract,authors,categories,date
http://arxiv.org/abs/1709.05707v2,Nonparametric Shape-restricted Regression,"We consider the problem of nonparametric regression under shape constraints.
The main examples include isotonic regression (with respect to any partial
order), unimodal/convex regression, additive shape-restricted regression, and
constrained single index model. We review some of the theoretical properties of
the least squares estimator (LSE) in these problems, emphasizing on the
adaptive nature of the LSE. In particular, we study the behavior of the risk of
the LSE, and its pointwise limiting distribution theory, with special emphasis
to isotonic regression. We survey various methods for constructing pointwise
confidence intervals around these shape-restricted functions. We also briefly
discuss the computation of the LSE and indicate some open research problems and
future directions.","['Adityanand Guntuboyina', 'Bodhisattva Sen']","['math.ST', 'stat.ML', 'stat.TH']",2017-09-17 19:13:59+00:00
http://arxiv.org/abs/1709.05684v1,A Categorical Approach for Recognizing Emotional Effects of Music,"Recently, digital music libraries have been developed and can be plainly
accessed. Latest research showed that current organization and retrieval of
music tracks based on album information are inefficient. Moreover, they
demonstrated that people use emotion tags for music tracks in order to search
and retrieve them. In this paper, we discuss separability of a set of emotional
labels, proposed in the categorical emotion expression, using Fisher's
separation theorem. We determine a set of adjectives to tag music parts: happy,
sad, relaxing, exciting, epic and thriller. Temporal, frequency and energy
features have been extracted from the music parts. It could be seen that the
maximum separability within the extracted features occurs between relaxing and
epic music parts. Finally, we have trained a classifier using Support Vector
Machines to automatically recognize and generate emotional labels for a music
part. Accuracy for recognizing each label has been calculated; where the
results show that epic music can be recognized more accurately (77.4%),
comparing to the other types of music.","['Mohsen Sahraei Ardakani', 'Ehsan Arbabi']","['cs.AI', 'cs.SD', 'stat.ML']",2017-09-17 16:04:41+00:00
http://arxiv.org/abs/1709.05673v2,Semi-supervised learning,"Semi-supervised learning deals with the problem of how, if possible, to take
advantage of a huge amount of not classified data, to perform classification,
in situations when, typically, the labelled data are few. Even though this is
not always possible (it depends on how useful is to know the distribution of
the unlabelled data in the inference of the labels), several algorithm have
been proposed recently. A new algorithm is proposed, that under almost
neccesary conditions, attains asymptotically the performance of the best
theoretical rule, when the size of unlabeled data tends to infinity. The set of
necessary assumptions, although reasonables, show that semi-parametric
classification only works for very well conditioned problems.","['Alejandro Cholaquidis', 'Ricardo Fraiman', 'Mariela Sued']","['math.ST', 'stat.ML', 'stat.TH']",2017-09-17 14:45:42+00:00
http://arxiv.org/abs/1709.05667v1,Bayesian nonparametric Principal Component Analysis,"Principal component analysis (PCA) is very popular to perform dimension
reduction. The selection of the number of significant components is essential
but often based on some practical heuristics depending on the application. Only
few works have proposed a probabilistic approach able to infer the number of
significant components. To this purpose, this paper introduces a Bayesian
nonparametric principal component analysis (BNP-PCA). The proposed model
projects observations onto a random orthogonal basis which is assigned a prior
distribution defined on the Stiefel manifold. The prior on factor scores
involves an Indian buffet process to model the uncertainty related to the
number of components. The parameters of interest as well as the nuisance
parameters are finally inferred within a fully Bayesian framework via Monte
Carlo sampling. A study of the (in-)consistence of the marginal maximum a
posteriori estimator of the latent dimension is carried out. A new estimator of
the subspace dimension is proposed. Moreover, for sake of statistical
significance, a Kolmogorov-Smirnov test based on the posterior distribution of
the principal components is used to refine this estimate.
  The behaviour of the algorithm is first studied on various synthetic
examples. Finally, the proposed BNP dimension reduction approach is shown to be
easily yet efficiently coupled with clustering or latent factor models within a
unique framework.","['Clément Elvira', 'Pierre Chainais', 'Nicolas Dobigeon']",['stat.ML'],2017-09-17 14:24:05+00:00
http://arxiv.org/abs/1709.05666v1,On Inductive Abilities of Latent Factor Models for Relational Learning,"Latent factor models are increasingly popular for modeling multi-relational
knowledge graphs. By their vectorial nature, it is not only hard to interpret
why this class of models works so well, but also to understand where they fail
and how they might be improved. We conduct an experimental survey of
state-of-the-art models, not towards a purely comparative end, but as a means
to get insight about their inductive abilities. To assess the strengths and
weaknesses of each model, we create simple tasks that exhibit first, atomic
properties of binary relations, and then, common inter-relational inference
through synthetic genealogies. Based on these experimental results, we propose
new research directions to improve on existing models.","['Théo Trouillon', 'Éric Gaussier', 'Christopher R. Dance', 'Guillaume Bouchard']","['cs.LG', 'cs.AI', 'stat.ML']",2017-09-17 14:20:05+00:00
http://arxiv.org/abs/1709.05612v1,Multi-Entity Dependence Learning with Rich Context via Conditional Variational Auto-encoder,"Multi-Entity Dependence Learning (MEDL) explores conditional correlations
among multiple entities. The availability of rich contextual information
requires a nimble learning scheme that tightly integrates with deep neural
networks and has the ability to capture correlation structures among
exponentially many outcomes. We propose MEDL_CVAE, which encodes a conditional
multivariate distribution as a generating process. As a result, the variational
lower bound of the joint likelihood can be optimized via a conditional
variational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE was
motivated by two real-world applications in computational sustainability: one
studies the spatial correlation among multiple bird species using the eBird
data and the other models multi-dimensional landscape composition and human
footprint in the Amazon rainforest with satellite images. We show that
MEDL_CVAE captures rich dependency structures, scales better than previous
methods, and further improves on the joint likelihood taking advantage of very
large datasets that are beyond the capacity of previous methods.","['Luming Tang', 'Yexiang Xue', 'Di Chen', 'Carla P. Gomes']","['cs.LG', 'stat.ML']",2017-09-17 06:21:40+00:00
http://arxiv.org/abs/1709.05602v2,Characterization of Hemodynamic Signal by Learning Multi-View Relationships,"Multi-view data are increasingly prevalent in practice. It is often relevant
to analyze the relationships between pairs of views by multi-view component
analysis techniques such as Canonical Correlation Analysis (CCA). However, data
may easily exhibit nonlinear relations, which CCA cannot reveal. We aim to
investigate the usefulness of nonlinear multi-view relations to characterize
multi-view data in an explainable manner. To address this challenge, we propose
a method to characterize globally nonlinear multi-view relationships as a
mixture of linear relationships. A clustering method, it identifies partitions
of observations that exhibit the same relationships and learns those
relationships simultaneously. It defines cluster variables by multi-view rather
than spatial relationships, unlike almost all other clustering methods.
Furthermore, we introduce a supervised classification method that builds on our
clustering method by employing multi-view relationships as discriminative
factors. The value of these methods resides in their capability to find useful
structure in the data that single-view or current multi-view methods may
struggle to find. We demonstrate the potential utility of the proposed approach
using an application in clinical informatics to detect and characterize slow
bleeding in patients whose central venous pressure (CVP) is monitored at the
bedside. Presently, CVP is considered an insensitive measure of a subject's
intravascular volume status or its change. However, we reason that features of
CVP during inspiration and expiration should be informative in early
identification of emerging changes of patient status. We empirically show how
the proposed method can help discover and analyze multiple-to-multiple
correlations, which could be nonlinear or vary throughout the population, by
finding explainable structure of operational interest to practitioners.","['Eric Lei', 'Kyle Miller', 'Michael R. Pinsky', 'Artur Dubrawski']","['stat.ML', 'cs.LG']",2017-09-17 03:12:27+00:00
http://arxiv.org/abs/1709.05583v4,Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification,"Deep neural networks (DNNs) have transformed several artificial intelligence
research areas including computer vision, speech recognition, and natural
language processing. However, recent studies demonstrated that DNNs are
vulnerable to adversarial manipulations at testing time. Specifically, suppose
we have a testing example, whose label can be correctly predicted by a DNN
classifier. An attacker can add a small carefully crafted noise to the testing
example such that the DNN classifier predicts an incorrect label, where the
crafted testing example is called adversarial example. Such attacks are called
evasion attacks. Evasion attacks are one of the biggest challenges for
deploying DNNs in safety and security critical applications such as
self-driving cars. In this work, we develop new methods to defend against
evasion attacks. Our key observation is that adversarial examples are close to
the classification boundary. Therefore, we propose region-based classification
to be robust to adversarial examples. For a benign/adversarial testing example,
we ensemble information in a hypercube centered at the example to predict its
label. In contrast, traditional classifiers are point-based classification,
i.e., given a testing example, the classifier predicts its label based on the
testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets
demonstrate that our region-based classification can significantly mitigate
evasion attacks without sacrificing classification accuracy on benign examples.
Specifically, our region-based classification achieves the same classification
accuracy on testing benign examples as point-based classification, but our
region-based classification is significantly more robust than point-based
classification to various evasion attacks.","['Xiaoyu Cao', 'Neil Zhenqiang Gong']","['cs.CR', 'cs.LG', 'stat.ML']",2017-09-17 00:18:42+00:00
http://arxiv.org/abs/1709.05554v2,Deep Automated Multi-task Learning,"Multi-task learning (MTL) has recently contributed to learning better
representations in service of various NLP tasks. MTL aims at improving the
performance of a primary task, by jointly training on a secondary task. This
paper introduces automated tasks, which exploit the sequential nature of the
input data, as secondary tasks in an MTL model. We explore next word
prediction, next character prediction, and missing word completion as potential
automated tasks. Our results show that training on a primary task in parallel
with a secondary automated task improves both the convergence speed and
accuracy for the primary task. We suggest two methods for augmenting an
existing network with automated tasks and establish better performance in topic
prediction, sentiment analysis, and hashtag recommendation. Finally, we show
that the MTL models can perform well on datasets that are small and colloquial
by nature.","['Davis Liang', 'Yan Shu']","['cs.LG', 'stat.ML']",2017-09-16 19:04:54+00:00
http://arxiv.org/abs/1709.05552v1,Multivariate Gaussian Network Structure Learning,"We consider a graphical model where a multivariate normal vector is
associated with each node of the underlying graph and estimate the graphical
structure. We minimize a loss function obtained by regressing the vector at
each node on those at the remaining ones under a group penalty. We show that
the proposed estimator can be computed by a fast convex optimization algorithm.
We show that as the sample size increases, the estimated regression
coefficients and the correct graphical structure are correctly estimated with
probability tending to one. By extensive simulations, we show the superiority
of the proposed method over comparable procedures. We apply the technique on
two real datasets. The first one is to identify gene and protein networks
showing up in cancer cell lines, and the second one is to reveal the
connections among different industries in the US.","['Xingqi Du', 'Subhashis Ghosal']",['stat.ML'],2017-09-16 18:58:33+00:00
http://arxiv.org/abs/1709.05548v1,Forecasting of commercial sales with large scale Gaussian Processes,"This paper argues that there has not been enough discussion in the field of
applications of Gaussian Process for the fast moving consumer goods industry.
Yet, this technique can be important as it e.g., can provide automatic feature
relevance determination and the posterior mean can unlock insights on the data.
Significant challenges are the large size and high dimensionality of commercial
data at a point of sale. The study reviews approaches in the Gaussian Processes
modeling for large data sets, evaluates their performance on commercial sales
and shows value of this type of models as a decision-making tool for
management.","['Rodrigo Rivera', 'Evgeny Burnaev']","['stat.AP', 'stat.ML']",2017-09-16 18:51:58+00:00
http://arxiv.org/abs/1709.05545v4,Generating Compact Tree Ensembles via Annealing,"Tree ensembles are flexible predictive models that can capture relevant
variables and to some extent their interactions in a compact and interpretable
manner. Most algorithms for obtaining tree ensembles are based on versions of
boosting or Random Forest. Previous work showed that boosting algorithms
exhibit a cyclic behavior of selecting the same tree again and again due to the
way the loss is optimized. At the same time, Random Forest is not based on loss
optimization and obtains a more complex and less interpretable model. In this
paper we present a novel method for obtaining compact tree ensembles by growing
a large pool of trees in parallel with many independent boosting threads and
then selecting a small subset and updating their leaf weights by loss
optimization. We allow for the trees in the initial pool to have different
depths which further helps with generalization. Experiments on real datasets
show that the obtained model has usually a smaller loss than boosting, which is
also reflected in a lower misclassification error on the test set.","['Gitesh Dawer', 'Yangzi Guo', 'Adrian Barbu']","['stat.ML', 'cs.LG']",2017-09-16 18:26:18+00:00
http://arxiv.org/abs/1709.05515v2,Some variations on Ensembled Random Survival Forest with application to Cancer Research,"In this paper we describe a novel implementation of adaboost for prediction
of survival function. We take different variations of the algorithm and compare
the algorithms based on system run time and root mean square error. Our
construction includes right censoring data and competing risk data too. We take
different data set to illustrate the performance of the algorithms.","['Arabin Kumar Dey', 'Suhas N.', 'Talasila Sai Teja', 'Anshul Juneja']","['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']",2017-09-16 14:12:20+00:00
http://arxiv.org/abs/1709.05510v2,The Geometric Block Model,"To capture the inherent geometric features of many community detection
problems, we propose to use a new random graph model of communities that we
call a Geometric Block Model. The geometric block model generalizes the random
geometric graphs in the same way that the well-studied stochastic block model
generalizes the Erdos-Renyi random graphs. It is also a natural extension of
random community models inspired by the recent theoretical and practical
advancement in community detection. While being a topic of fundamental
theoretical interest, our main contribution is to show that many practical
community structures are better explained by the geometric block model. We also
show that a simple triangle-counting algorithm to detect communities in the
geometric block model is near-optimal. Indeed, even in the regime where the
average degree of the graph grows only logarithmically with the number of
vertices (sparse-graph), we show that this algorithm performs extremely well,
both theoretically and practically. In contrast, the triangle-counting
algorithm is far from being optimum for the stochastic block model. We simulate
our results on both real and synthetic datasets to show superior performance of
both the new model as well as our algorithm.","['Sainyam Galhotra', 'Arya Mazumdar', 'Soumyabrata Pal', 'Barna Saha']","['cs.SI', 'cs.DS', 'stat.ML', 'E.1']",2017-09-16 13:38:03+00:00
http://arxiv.org/abs/1709.05506v5,A statistical interpretation of spectral embedding: the generalised random dot product graph,"Spectral embedding is a procedure which can be used to obtain vector
representations of the nodes of a graph. This paper proposes a generalisation
of the latent position network model known as the random dot product graph, to
allow interpretation of those vector representations as latent position
estimates. The generalisation is needed to model heterophilic connectivity
(e.g., `opposites attract') and to cope with negative eigenvalues more
generally. We show that, whether the adjacency or normalised Laplacian matrix
is used, spectral embedding produces uniformly consistent latent position
estimates with asymptotically Gaussian error (up to identifiability). The
standard and mixed membership stochastic block models are special cases in
which the latent positions take only $K$ distinct vector values, representing
communities, or live in the $(K-1)$-simplex with those vertices, respectively.
Under the stochastic block model, our theory suggests spectral clustering using
a Gaussian mixture model (rather than $K$-means) and, under mixed membership,
fitting the minimum volume enclosing simplex, existing recommendations
previously only supported under non-negative-definite assumptions. Empirical
improvements in link prediction (over the random dot product graph), and the
potential to uncover richer latent structure (than posited under the standard
or mixed membership stochastic block models) are demonstrated in a
cyber-security example.","['Patrick Rubin-Delanchy', 'Joshua Cape', 'Minh Tang', 'Carey E. Priebe']","['stat.ML', 'cs.LG', '62H30, 62H12, 62E20,']",2017-09-16 12:30:40+00:00
http://arxiv.org/abs/1709.05501v6,Constrained Bayesian Optimization for Automatic Chemical Design,"Automatic Chemical Design is a framework for generating novel molecules with
optimized properties. The original scheme, featuring Bayesian optimization over
the latent space of a variational autoencoder, suffers from the pathology that
it tends to produce invalid molecular structures. First, we demonstrate
empirically that this pathology arises when the Bayesian optimization scheme
queries latent points far away from the data on which the variational
autoencoder has been trained. Secondly, by reformulating the search procedure
as a constrained Bayesian optimization problem, we show that the effects of
this pathology can be mitigated, yielding marked improvements in the validity
of the generated molecules. We posit that constrained Bayesian optimization is
a good approach for solving this class of training set mismatch in many
generative tasks involving Bayesian optimization over the latent space of a
variational autoencoder.","['Ryan-Rhys Griffiths', 'José Miguel Hernández-Lobato']",['stat.ML'],2017-09-16 11:38:35+00:00
http://arxiv.org/abs/1709.05480v1,Subset Labeled LDA for Large-Scale Multi-Label Classification,"Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard
unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address
multi-label learning tasks. Previous work has shown it to perform in par with
other state-of-the-art multi-label methods. Nonetheless, with increasing label
sets sizes LLDA encounters scalability issues. In this work, we introduce
Subset LLDA, a simple variant of the standard LLDA algorithm, that not only can
effectively scale up to problems with hundreds of thousands of labels but also
improves over the LLDA state-of-the-art. We conduct extensive experiments on
eight data sets, with label sets sizes ranging from hundreds to hundreds of
thousands, comparing our proposed algorithm with the previously proposed LLDA
algorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme
multi-label classification. The results show a steady advantage of our method
over the other LLDA algorithms and competitive results compared to the extreme
multi-label classification algorithms.","['Yannis Papanikolaou', 'Grigorios Tsoumakas']","['stat.ML', 'cs.LG']",2017-09-16 08:35:12+00:00
http://arxiv.org/abs/1709.05454v1,Statistical inference on random dot product graphs: a survey,"The random dot product graph (RDPG) is an independent-edge random graph that
is analytically tractable and, simultaneously, either encompasses or can
successfully approximate a wide range of random graphs, from relatively simple
stochastic block models to complex latent position graphs. In this survey
paper, we describe a comprehensive paradigm for statistical inference on random
dot product graphs, a paradigm centered on spectral embeddings of adjacency and
Laplacian matrices. We examine the analogues, in graph inference, of several
canonical tenets of classical Euclidean inference: in particular, we summarize
a body of existing results on the consistency and asymptotic normality of the
adjacency and Laplacian spectral embeddings, and the role these spectral
embeddings can play in the construction of single- and multi-sample hypothesis
tests for graph data. We investigate several real-world applications, including
community detection and classification in large social networks and the
determination of functional and biologically relevant network properties from
an exploratory data analysis of the Drosophila connectome. We outline requisite
background and current open problems in spectral graph inference.","['Avanti Athreya', 'Donniell E. Fishkind', 'Keith Levin', 'Vince Lyzinski', 'Youngser Park', 'Yichen Qin', 'Daniel L. Sussman', 'Minh Tang', 'Joshua T. Vogelstein', 'Carey E. Priebe']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH', '62FXX, 62GXX, 62HXX, 05CXX']",2017-09-16 04:22:57+00:00
http://arxiv.org/abs/1709.05418v1,Deep Scattering: Rendering Atmospheric Clouds with Radiance-Predicting Neural Networks,"We present a technique for efficiently synthesizing images of atmospheric
clouds using a combination of Monte Carlo integration and neural networks. The
intricacies of Lorenz-Mie scattering and the high albedo of cloud-forming
aerosols make rendering of clouds---e.g. the characteristic silverlining and
the ""whiteness"" of the inner body---challenging for methods based solely on
Monte Carlo integration or diffusion theory. We approach the problem
differently. Instead of simulating all light transport during rendering, we
pre-learn the spatial and directional distribution of radiant flux from tens of
cloud exemplars. To render a new scene, we sample visible points of the cloud
and, for each, extract a hierarchical 3D descriptor of the cloud geometry with
respect to the shading location and the light source. The descriptor is input
to a deep neural network that predicts the radiance function for each shading
configuration. We make the key observation that progressively feeding the
hierarchical descriptor into the network enhances the network's ability to
learn faster and predict with high accuracy while using few coefficients. We
also employ a block design with residual connections to further improve
performance. A GPU implementation of our method synthesizes images of clouds
that are nearly indistinguishable from the reference solution within seconds
interactively. Our method thus represents a viable solution for applications
such as cloud design and, thanks to its temporal stability, also for
high-quality production of animated content.","['Simon Kallweit', 'Thomas Müller', 'Brian McWilliams', 'Markus Gross', 'Jan Novák']","['cs.LG', 'cs.GR', 'stat.ML']",2017-09-15 21:40:02+00:00
http://arxiv.org/abs/1709.05409v2,Gaussian Process Latent Force Models for Learning and Stochastic Control of Physical Systems,"This article is concerned with learning and stochastic control in physical
systems which contain unknown input signals. These unknown signals are modeled
as Gaussian processes (GP) with certain parametrized covariance structures. The
resulting latent force models (LFMs) can be seen as hybrid models that contain
a first-principles physical model part and a non-parametric GP model part. We
briefly review the statistical inference and learning methods for this kind of
models, introduce stochastic control methodology for the models, and provide
new theoretical observability and controllability results for them.","['Simo Särkkä', 'Mauricio A. Álvarez', 'Neil D. Lawrence']","['cs.SY', 'math.DS', 'stat.ME', 'stat.ML']",2017-09-15 21:07:46+00:00
http://arxiv.org/abs/1709.05380v4,The Uncertainty Bellman Equation and Exploration,"We consider the exploration/exploitation problem in reinforcement learning.
For exploitation, it is well known that the Bellman equation connects the value
at any time-step to the expected value at subsequent time-steps. In this paper
we consider a similar \textit{uncertainty} Bellman equation (UBE), which
connects the uncertainty at any time-step to the expected uncertainties at
subsequent time-steps, thereby extending the potential exploratory benefit of a
policy beyond individual time-steps. We prove that the unique fixed point of
the UBE yields an upper bound on the variance of the posterior distribution of
the Q-values induced by any policy. This bound can be much tighter than
traditional count-based bonuses that compound standard deviation rather than
variance. Importantly, and unlike several existing approaches to optimism, this
method scales naturally to large systems with complex generalization.
Substituting our UBE-exploration strategy for $\epsilon$-greedy improves DQN
performance on 51 out of 57 games in the Atari suite.","[""Brendan O'Donoghue"", 'Ian Osband', 'Remi Munos', 'Volodymyr Mnih']","['cs.AI', 'cs.LG', 'math.OC', 'stat.ML']",2017-09-15 19:55:58+00:00
http://arxiv.org/abs/1709.05379v1,Road Friction Estimation for Connected Vehicles using Supervised Machine Learning,"In this paper, the problem of road friction prediction from a fleet of
connected vehicles is investigated. A framework is proposed to predict the road
friction level using both historical friction data from the connected cars and
data from weather stations, and comparative results from different methods are
presented. The problem is formulated as a classification task where the
available data is used to train three machine learning models including
logistic regression, support vector machine, and neural networks to predict the
friction class (slippery or non-slippery) in the future for specific road
segments. In addition to the friction values, which are measured by moving
vehicles, additional parameters such as humidity, temperature, and rainfall are
used to obtain a set of descriptive feature vectors as input to the
classification methods. The proposed prediction models are evaluated for
different prediction horizons (0 to 120 minutes in the future) where the
evaluation shows that the neural networks method leads to more stable results
in different conditions.","['Ghazaleh Panahandeh', 'Erik Ek', 'Nasser Mohammadiha']","['cs.LG', 'stat.ML']",2017-09-15 19:52:18+00:00
http://arxiv.org/abs/1709.05328v1,Granger Mediation Analysis of Multiple Time Series with an Application to fMRI,"It becomes increasingly popular to perform mediation analysis for complex
data from sophisticated experimental studies. In this paper, we present Granger
Mediation Analysis (GMA), a new framework for causal mediation analysis of
multiple time series. This framework is motivated by a functional magnetic
resonance imaging (fMRI) experiment where we are interested in estimating the
mediation effects between a randomized stimulus time series and brain activity
time series from two brain regions. The stable unit treatment assumption for
causal mediation analysis is thus unrealistic for this type of time series
data. To address this challenge, our framework integrates two types of models:
causal mediation analysis across the variables and vector autoregressive models
across the temporal observations. We further extend this framework to handle
multilevel data to address individual variability and correlated errors between
the mediator and the outcome variables. These models not only provide valid
causal mediation for time series data but also model the causal dynamics across
time. We show that the modeling parameters in our models are identifiable, and
we develop computationally efficient methods to maximize the likelihood-based
optimization criteria. Simulation studies show that our method reduces the
estimation bias and improve statistical power, compared to existing approaches.
On a real fMRI data set, our approach not only infers the causal effects of
brain pathways but accurately captures the feedback effect of the outcome
region on the mediator region.","['Yi Zhao', 'Xi Luo']","['stat.ME', 'stat.AP', 'stat.ML', '62J99, 62H12']",2017-09-15 17:47:51+00:00
http://arxiv.org/abs/1709.05321v3,Learning Functional Causal Models with Generative Neural Networks,"We introduce a new approach to functional causal modeling from observational
data, called Causal Generative Neural Networks (CGNN). CGNN leverages the power
of neural networks to learn a generative model of the joint distribution of the
observed variables, by minimizing the Maximum Mean Discrepancy between
generated and observed data. An approximate learning criterion is proposed to
scale the computational cost of the approach to linear complexity in the number
of observations. The performance of CGNN is studied throughout three
experiments. Firstly, CGNN is applied to cause-effect inference, where the task
is to identify the best causal hypothesis out of $X\rightarrow Y$ and
$Y\rightarrow X$. Secondly, CGNN is applied to the problem of identifying
v-structures and conditional independences. Thirdly, CGNN is applied to
multivariate functional causal modeling: given a skeleton describing the direct
dependences in a set of random variables $\textbf{X} = [X_1, \ldots, X_d]$,
CGNN orients the edges in the skeleton to uncover the directed acyclic causal
graph describing the causal structure of the random variables. On all three
tasks, CGNN is extensively assessed on both artificial and real-world data,
comparing favorably to the state-of-the-art. Finally, CGNN is extended to
handle the case of confounders, where latent variables are involved in the
overall causal model.","['Olivier Goudet', 'Diviyan Kalainathan', 'Philippe Caillou', 'Isabelle Guyon', 'David Lopez-Paz', 'Michèle Sebag']",['stat.ML'],2017-09-15 17:16:21+00:00
http://arxiv.org/abs/1709.05289v4,Optimal approximation of piecewise smooth functions using deep ReLU neural networks,"We study the necessary and sufficient complexity of ReLU neural networks---in
terms of depth and number of weights---which is required for approximating
classifier functions in $L^2$. As a model class, we consider the set
$\mathcal{E}^\beta (\mathbb R^d)$ of possibly discontinuous piecewise $C^\beta$
functions $f : [-1/2, 1/2]^d \to \mathbb R$, where the different smooth regions
of $f$ are separated by $C^\beta$ hypersurfaces. For dimension $d \geq 2$,
regularity $\beta > 0$, and accuracy $\varepsilon > 0$, we construct artificial
neural networks with ReLU activation function that approximate functions from
$\mathcal{E}^\beta(\mathbb R^d)$ up to $L^2$ error of $\varepsilon$. The
constructed networks have a fixed number of layers, depending only on $d$ and
$\beta$, and they have $O(\varepsilon^{-2(d-1)/\beta})$ many nonzero weights,
which we prove to be optimal. In addition to the optimality in terms of the
number of weights, we show that in order to achieve the optimal approximation
rate, one needs ReLU networks of a certain depth. Precisely, for piecewise
$C^\beta(\mathbb R^d)$ functions, this minimal depth is given---up to a
multiplicative constant---by $\beta/d$. Up to a log factor, our constructed
networks match this bound. This partly explains the benefits of depth for ReLU
networks by showing that deep networks are necessary to achieve efficient
approximation of (piecewise) smooth functions. Finally, we analyze
approximation in high-dimensional spaces where the function $f$ to be
approximated can be factorized into a smooth dimension reducing feature map
$\tau$ and classifier function $g$---defined on a low-dimensional feature
space---as $f = g \circ \tau$. We show that in this case the approximation rate
depends only on the dimension of the feature space and not the input dimension.","['Philipp Petersen', 'Felix Voigtlaender']","['math.FA', 'cs.LG', 'stat.ML', '41A25, 41A10, 82C32, 41A46, 68T05']",2017-09-15 16:14:39+00:00
http://arxiv.org/abs/1709.05276v1,Mixtures and products in two graphical models,"We compare two statistical models of three binary random variables. One is a
mixture model and the other is a product of mixtures model called a restricted
Boltzmann machine. Although the two models we study look different from their
parametrizations, we show that they represent the same set of distributions on
the interior of the probability simplex, and are equal up to closure. We give a
semi-algebraic description of the model in terms of six binomial inequalities
and obtain closed form expressions for the maximum likelihood estimates. We
briefly discuss extensions to larger models.","['Anna Seigal', 'Guido Montufar']","['stat.ML', '62E10, 14M07, 14M25, 51M20']",2017-09-15 15:42:04+00:00
http://arxiv.org/abs/1709.05231v1,A Spectral Method for Activity Shaping in Continuous-Time Information Cascades,"Information Cascades Model captures dynamical properties of user activity in
a social network. In this work, we develop a novel framework for activity
shaping under the Continuous-Time Information Cascades Model which allows the
administrator for local control actions by allocating targeted resources that
can alter the spread of the process. Our framework employs the optimization of
the spectral radius of the Hazard matrix, a quantity that has been shown to
drive the maximum influence in a network, while enjoying a simple convex
relaxation when used to minimize the influence of the cascade. In addition,
use-cases such as quarantine and node immunization are discussed to highlight
the generality of the proposed activity shaping framework. Finally, we present
the NetShape influence minimization method which is compared favorably to
baseline and state-of-the-art approaches through simulations on real social
networks.","['Kevin Scaman', 'Argyris Kalogeratos', 'Luca Corinzia', 'Nicolas Vayatis']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.SI', 'math.OC', '93E20, 91D30', 'I.2.6']",2017-09-15 14:32:36+00:00
http://arxiv.org/abs/1709.05119v1,Dependence Modeling in Ultra High Dimensions with Vine Copulas and the Graphical Lasso,"To model high dimensional data, Gaussian methods are widely used since they
remain tractable and yield parsimonious models by imposing strong assumptions
on the data. Vine copulas are more flexible by combining arbitrary marginal
distributions and (conditional) bivariate copulas. Yet, this adaptability is
accompanied by sharply increasing computational effort as the dimension
increases. The approach proposed in this paper overcomes this burden and makes
the first step into ultra high dimensional non-Gaussian dependence modeling by
using a divide-and-conquer approach. First, we apply Gaussian methods to split
datasets into feasibly small subsets and second, apply parsimonious and
flexible vine copulas thereon. Finally, we reconcile them into one joint model.
We provide numerical results demonstrating the feasibility of our approach in
moderate dimensions and showcase its ability to estimate ultra high dimensional
non-Gaussian dependence models in thousands of dimensions.","['Dominik Müller', 'Claudia Czado']",['stat.ML'],2017-09-15 09:13:58+00:00
http://arxiv.org/abs/1709.05006v3,Two-sample Statistics Based on Anisotropic Kernels,"The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD)
statistic for measuring the distance between two distributions given
finitely-many multivariate samples. When the distributions are locally
low-dimensional, the proposed test can be made more powerful to distinguish
certain alternatives by incorporating local covariance matrices and
constructing an anisotropic kernel. The kernel matrix is asymmetric; it
computes the affinity between $n$ data points and a set of $n_R$ reference
points, where $n_R$ can be drastically smaller than $n$. While the proposed
statistic can be viewed as a special class of Reproducing Kernel Hilbert Space
MMD, the consistency of the test is proved, under mild assumptions of the
kernel, as long as $\|p-q\| \sqrt{n} \to \infty $, and a finite-sample lower
bound of the testing power is obtained. Applications to flow cytometry and
diffusion MRI datasets are demonstrated, which motivate the proposed approach
to compare distributions.","['Xiuyuan Cheng', 'Alexander Cloninger', 'Ronald R. Coifman']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.CO']",2017-09-14 23:06:19+00:00
http://arxiv.org/abs/1709.04875v4,Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting,"Timely accurate traffic forecast is crucial for urban traffic control and
guidance. Due to the high nonlinearity and complexity of traffic flow,
traditional methods cannot satisfy the requirements of mid-and-long term
prediction tasks and often neglect spatial and temporal dependencies. In this
paper, we propose a novel deep learning framework, Spatio-Temporal Graph
Convolutional Networks (STGCN), to tackle the time series prediction problem in
traffic domain. Instead of applying regular convolutional and recurrent units,
we formulate the problem on graphs and build the model with complete
convolutional structures, which enable much faster training speed with fewer
parameters. Experiments show that our model STGCN effectively captures
comprehensive spatio-temporal correlations through modeling multi-scale traffic
networks and consistently outperforms state-of-the-art baselines on various
real-world traffic datasets.","['Bing Yu', 'Haoteng Yin', 'Zhanxing Zhu']","['cs.LG', 'stat.ML']",2017-09-14 16:54:41+00:00
http://arxiv.org/abs/1709.04862v1,Random Forests of Interaction Trees for Estimating Individualized Treatment Effects in Randomized Trials,"Assessing heterogeneous treatment effects has become a growing interest in
advancing precision medicine. Individualized treatment effects (ITE) play a
critical role in such an endeavor. Concerning experimental data collected from
randomized trials, we put forward a method, termed random forests of
interaction trees (RFIT), for estimating ITE on the basis of interaction trees
(Su et al., 2009). To this end, we first propose a smooth sigmoid surrogate
(SSS) method, as an alternative to greedy search, to speed up tree
construction. RFIT outperforms the traditional `separate regression' approach
in estimating ITE. Furthermore, standard errors for the estimated ITE via RFIT
can be obtained with the infinitesimal jackknife method. We assess and
illustrate the use of RFIT via both simulation and the analysis of data from an
acupuncture headache trial.","['Xiaogang Su', 'Annette T. Peña', 'Lei Liu', 'Richard A. Levine']","['stat.ML', '62G08', 'G.3']",2017-09-14 16:34:02+00:00
http://arxiv.org/abs/1709.04836v1,Informed Non-convex Robust Principal Component Analysis with Features,"We revisit the problem of robust principal component analysis with features
acting as prior side information. To this aim, a novel, elegant, non-convex
optimization approach is proposed to decompose a given observation matrix into
a low-rank core and the corresponding sparse residual. Rigorous theoretical
analysis of the proposed algorithm results in exact recovery guarantees with
low computational complexity. Aptly designed synthetic experiments demonstrate
that our method is the first to wholly harness the power of non-convexity over
convexity in terms of both recoverability and speed. That is, the proposed
non-convex approach is more accurate and faster compared to the best available
algorithms for the problem under study. Two real-world applications, namely
image classification and face denoising further exemplify the practical
superiority of the proposed method.","['Niannan Xue', 'Jiankang Deng', 'Yannis Panagakis', 'Stefanos Zafeiriou']","['stat.ML', 'cs.CV', 'cs.LG']",2017-09-14 15:06:21+00:00
http://arxiv.org/abs/1709.05262v2,Supervising Unsupervised Learning,"We introduce a framework to leverage knowledge acquired from a repository of
(heterogeneous) supervised datasets to new unsupervised datasets. Our
perspective avoids the subjectivity inherent in unsupervised learning by
reducing it to supervised learning, and provides a principled way to evaluate
unsupervised algorithms. We demonstrate the versatility of our framework via
simple agnostic bounds on unsupervised problems. In the context of clustering,
our approach helps choose the number of clusters and the clustering algorithm,
remove the outliers, and provably circumvent the Kleinberg's impossibility
result. Experimental results across hundreds of problems demonstrate improved
performance on unsupervised data with simple algorithms, despite the fact that
our problems come from heterogeneous domains. Additionally, our framework lets
us leverage deep networks to learn common features from many such small
datasets, and perform zero shot learning.","['Vikas K. Garg', 'Adam Kalai']","['cs.AI', 'cs.LG', 'stat.ML']",2017-09-14 14:42:41+00:00
http://arxiv.org/abs/1709.04764v1,Interpretable Graph-Based Semi-Supervised Learning via Flows,"In this paper, we consider the interpretability of the foundational
Laplacian-based semi-supervised learning approaches on graphs. We introduce a
novel flow-based learning framework that subsumes the foundational approaches
and additionally provides a detailed, transparent, and easily understood
expression of the learning process in terms of graph flows. As a result, one
can visualize and interactively explore the precise subgraph along which the
information from labeled nodes flows to an unlabeled node of interest.
Surprisingly, the proposed framework avoids trading accuracy for
interpretability, but in fact leads to improved prediction accuracy, which is
supported both by theoretical considerations and empirical results. The
flow-based framework guarantees the maximum principle by construction and can
handle directed graphs in an out-of-the-box manner.","['Raif M. Rustamov', 'James T. Klosowski']","['stat.ML', 'cs.LG']",2017-09-14 13:13:52+00:00
http://arxiv.org/abs/1709.04744v3,Subspace Clustering using Ensembles of $K$-Subspaces,"Subspace clustering is the unsupervised grouping of points lying near a union
of low-dimensional linear subspaces. Algorithms based directly on geometric
properties of such data tend to either provide poor empirical performance, lack
theoretical guarantees, or depend heavily on their initialization. We present a
novel geometric approach to the subspace clustering problem that leverages
ensembles of the K-subspaces (KSS) algorithm via the evidence accumulation
clustering framework. Our algorithm, referred to as ensemble K-subspaces
(EKSS), forms a co-association matrix whose (i,j)th entry is the number of
times points i and j are clustered together by several runs of KSS with random
initializations. We prove general recovery guarantees for any algorithm that
forms an affinity matrix with entries close to a monotonic transformation of
pairwise absolute inner products. We then show that a specific instance of EKSS
results in an affinity matrix with entries of this form, and hence our proposed
algorithm can provably recover subspaces under similar conditions to
state-of-the-art algorithms. The finding is, to the best of our knowledge, the
first recovery guarantee for evidence accumulation clustering and for KSS
variants. We show on synthetic data that our method performs well in the
traditionally challenging settings of subspaces with large intersection,
subspaces with small principal angles, and noisy data. Finally, we evaluate our
algorithm on six common benchmark datasets and show that unlike existing
methods, EKSS achieves excellent empirical performance when there are both a
small and large number of points per subspace.","['John Lipor', 'David Hong', 'Yan Shuo Tan', 'Laura Balzano']","['cs.CV', 'cs.LG', 'stat.ML']",2017-09-14 12:55:56+00:00
http://arxiv.org/abs/1709.04718v2,The Impact of Local Geometry and Batch Size on Stochastic Gradient Descent for Nonconvex Problems,"In several experimental reports on nonconvex optimization problems in machine
learning, stochastic gradient descent (SGD) was observed to prefer minimizers
with flat basins in comparison to more deterministic methods, yet there is very
little rigorous understanding of this phenomenon. In fact, the lack of such
work has led to an unverified, but widely-accepted stochastic mechanism
describing why SGD prefers flatter minimizers to sharper minimizers. However,
as we demonstrate, the stochastic mechanism fails to explain this phenomenon.
Here, we propose an alternative deterministic mechanism that can accurately
explain why SGD prefers flatter minimizers to sharper minimizers. We derive
this mechanism based on a detailed analysis of a generic stochastic quadratic
problem, which generalizes known results for classical gradient descent.
Finally, we verify the predictions of our deterministic mechanism on two
nonconvex problems.",['Vivak Patel'],"['math.OC', 'stat.CO', 'stat.ML', '90C15, 90C30']",2017-09-14 11:59:10+00:00
http://arxiv.org/abs/1709.04695v1,The Conditional Analogy GAN: Swapping Fashion Articles on People Images,"We present a novel method to solve image analogy problems : it allows to
learn the relation between paired images present in training data, and then
generalize and generate images that correspond to the relation, but were never
seen in the training set. Therefore, we call the method Conditional Analogy
Generative Adversarial Network (CAGAN), as it is based on adversarial training
and employs deep convolutional neural networks. An especially interesting
application of that technique is automatic swapping of clothing on fashion
model photos. Our work has the following contributions. First, the definition
of the end-to-end trainable CAGAN architecture, which implicitly learns
segmentation masks without expensive supervised labeling data. Second,
experimental results show plausible segmentation masks and often convincing
swapped images, given the target article. Finally, we discuss the next steps
for that technique: neural network architecture improvements and more advanced
applications.","['Nikolay Jetchev', 'Urs Bergmann']","['stat.ML', 'cs.AI', 'cs.CV']",2017-09-14 10:39:51+00:00
http://arxiv.org/abs/1709.04673v5,Analyzing Approximate Value Iteration Algorithms,"In this paper, we consider the stochastic iterative counterpart of the value
iteration scheme wherein only noisy and possibly biased approximations of the
Bellman operator are available. We call this counterpart as the approximate
value iteration (AVI) scheme. Neural networks are often used as function
approximators, in order to counter Bellman's curse of dimensionality. In this
paper, they are used to approximate the Bellman operator. Since neural networks
are typically trained using sample data, errors and biases may be introduced.
The design of AVI accounts for implementations with biased approximations of
the Bellman operator and sampling errors. We present verifiable sufficient
conditions under which AVI is stable (almost surely bounded) and converges to a
fixed point of the approximate Bellman operator. To ensure the stability of
AVI, we present three different yet related sets of sufficient conditions that
are based on the existence of an appropriate Lyapunov function. These Lyapunov
function based conditions are easily verifiable and new to the literature. The
verifiability is enhanced by the fact that a recipe for the construction of the
necessary Lyapunov function is also provided. We also show that the stability
analysis of AVI can be readily extended to the general case of set-valued
stochastic approximations. Finally, we show that AVI can also be used in more
general circumstances, i.e., for finding fixed points of contractive set-valued
maps.","['Arunselvan Ramaswamy', 'Shalabh Bhatnagar']","['cs.SY', 'math.DS', 'stat.ML', '62L20, 93E35, 37B25, 34A60, 90C39, 37C25']",2017-09-14 09:04:41+00:00
http://arxiv.org/abs/1709.04596v1,A Framework for Generalizing Graph-based Representation Learning Methods,"Random walks are at the heart of many existing deep learning algorithms for
graph data. However, such algorithms have many limitations that arise from the
use of random walks, e.g., the features resulting from these methods are unable
to transfer to new nodes and graphs as they are tied to node identity. In this
work, we introduce the notion of attributed random walks which serves as a
basis for generalizing existing methods such as DeepWalk, node2vec, and many
others that leverage random walks. Our proposed framework enables these methods
to be more widely applicable for both transductive and inductive learning as
well as for use on graphs with attributes (if available). This is achieved by
learning functions that generalize to new nodes and graphs. We show that our
proposed framework is effective with an average AUC improvement of 16.1% while
requiring on average 853 times less space than existing methods on a variety of
graphs from several domains.","['Nesreen K. Ahmed', 'Ryan A. Rossi', 'Rong Zhou', 'John Boaz Lee', 'Xiangnan Kong', 'Theodore L. Willke', 'Hoda Eldardiry']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.SI']",2017-09-14 02:37:52+00:00
http://arxiv.org/abs/1709.04594v2,Revisiting Spectral Graph Clustering with Generative Community Models,"The methodology of community detection can be divided into two principles:
imposing a network model on a given graph, or optimizing a designed objective
function. The former provides guarantees on theoretical detectability but falls
short when the graph is inconsistent with the underlying model. The latter is
model-free but fails to provide quality assurance for the detected communities.
In this paper, we propose a novel unified framework to combine the advantages
of these two principles. The presented method, SGC-GEN, not only considers the
detection error caused by the corresponding model mismatch to a given graph,
but also yields a theoretical guarantee on community detectability by analyzing
Spectral Graph Clustering (SGC) under GENerative community models (GCMs).
SGC-GEN incorporates the predictability on correct community detection with a
measure of community fitness to GCMs. It resembles the formulation of
supervised learning problems by enabling various community detection loss
functions and model mismatch metrics. We further establish a theoretical
condition for correct community detection using the normalized graph Laplacian
matrix under a GCM, which provides a novel data-driven loss function for
SGC-GEN. In addition, we present an effective algorithm to implement SGC-GEN,
and show that the computational complexity of SGC-GEN is comparable to the
baseline methods. Our experiments on 18 real-world datasets demonstrate that
SGC-GEN possesses superior and robust performance compared to 6 baseline
methods under 7 representative clustering metrics.","['Pin-Yu Chen', 'Lingfei Wu']","['stat.ML', 'cs.SI']",2017-09-14 02:34:30+00:00
http://arxiv.org/abs/1709.04576v1,Catalyst design using actively learned machine with non-ab initio input features towards CO2 reduction reactions,"In conventional chemisorption model, the d-band center theory (augmented
sometimes with the upper edge of d-band for imporved accuarcy) plays a central
role in predicting adsorption energies and catalytic activity as a function of
d-band center of the solid surfaces, but it requires density functional
calculations that can be quite costly for large scale screening purposes of
materials. In this work, we propose to use the d-band width of the muffin-tin
orbital theory (to account for local coordination environment) plus
electronegativity (to account for adsorbate renormalization) as a simple set of
alternative descriptors for chemisorption, which do not demand the ab initio
calculations. This pair of descriptors are then combined with machine learning
methods, namely, artificial neural network (ANN) and kernel ridge regression
(KRR), to allow large scale materials screenings. We show, for a toy set of 263
alloy systems, that the CO adsorption energy can be predicted with a remarkably
small mean absolute deviation error of 0.05 eV, a significantly improved result
as compared to 0.13 eV obtained with descriptors including costly d-band center
calculations in literature. We achieved this high accuracy by utilizing an
active learning algorithm, without which the accuracy was 0.18 eV otherwise. As
a practical application of this machine, we identified Cu3Y@Cu as a highly
active and cost-effective electrochemical CO2 reduction catalyst to produce CO
with the overpotential 0.37 V lower than Au catalyst.","['Juhwan Noh', 'Jaehoon Kim', 'Seoin Back', 'Yousung Jung']","['cond-mat.mtrl-sci', 'physics.chem-ph', 'stat.ML']",2017-09-14 01:35:00+00:00
http://arxiv.org/abs/1709.04574v1,Towards personalized human AI interaction - adapting the behavior of AI agents using neural signatures of subjective interest,"Reinforcement Learning AI commonly uses reward/penalty signals that are
objective and explicit in an environment -- e.g. game score, completion time,
etc. -- in order to learn the optimal strategy for task performance. However,
Human-AI interaction for such AI agents should include additional reinforcement
that is implicit and subjective -- e.g. human preferences for certain AI
behavior -- in order to adapt the AI behavior to idiosyncratic human
preferences. Such adaptations would mirror naturally occurring processes that
increase trust and comfort during social interactions. Here, we show how a
hybrid brain-computer-interface (hBCI), which detects an individual's level of
interest in objects/events in a virtual environment, can be used to adapt the
behavior of a Deep Reinforcement Learning AI agent that is controlling a
virtual autonomous vehicle. Specifically, we show that the AI learns a driving
strategy that maintains a safe distance from a lead vehicle, and most novelly,
preferentially slows the vehicle when the human passengers of the vehicle
encounter objects of interest. This adaptation affords an additional 20\%
viewing time for subjectively interesting objects. This is the first
demonstration of how an hBCI can be used to provide implicit reinforcement to
an AI agent in a way that incorporates user preferences into the control
system.","['Victor Shih', 'David C Jangraw', 'Paul Sajda', 'Sameer Saproo']","['cs.HC', 'cs.AI', 'cs.SY', 'stat.ML']",2017-09-14 01:27:44+00:00
http://arxiv.org/abs/1709.04555v3,Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network,"The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts.","['Wengong Jin', 'Connor W. Coley', 'Regina Barzilay', 'Tommi Jaakkola']","['cs.LG', 'cs.AI', 'stat.ML']",2017-09-13 22:28:46+00:00
http://arxiv.org/abs/1709.05216v1,Optimal Learning for Sequential Decision Making for Expensive Cost Functions with Stochastic Binary Feedbacks,"We consider the problem of sequentially making decisions that are rewarded by
""successes"" and ""failures"" which can be predicted through an unknown
relationship that depends on a partially controllable vector of attributes for
each instance. The learner takes an active role in selecting samples from the
instance pool. The goal is to maximize the probability of success in either
offline (training) or online (testing) phases. Our problem is motivated by
real-world applications where observations are time-consuming and/or expensive.
We develop a knowledge gradient policy using an online Bayesian linear
classifier to guide the experiment by maximizing the expected value of
information of labeling each alternative. We provide a finite-time analysis of
the estimated error and show that the maximum likelihood estimator based
produced by the KG policy is consistent and asymptotically normal. We also show
that the knowledge gradient policy is asymptotically optimal in an offline
setting. This work further extends the knowledge gradient to the setting of
contextual bandits. We report the results of a series of experiments that
demonstrate its efficiency.","['Yingfei Wang', 'Chu Wang', 'Warren Powell']",['stat.ML'],2017-09-13 22:01:20+00:00
http://arxiv.org/abs/1709.04546v2,Normalized Direction-preserving Adam,"Adaptive optimization algorithms, such as Adam and RMSprop, have shown better
optimization performance than stochastic gradient descent (SGD) in some
scenarios. However, recent studies show that they often lead to worse
generalization performance than SGD, especially for training deep neural
networks (DNNs). In this work, we identify the reasons that Adam generalizes
worse than SGD, and develop a variant of Adam to eliminate the generalization
gap. The proposed method, normalized direction-preserving Adam (ND-Adam),
enables more precise control of the direction and step size for updating weight
vectors, leading to significantly improved generalization performance.
Following a similar rationale, we further improve the generalization
performance in classification tasks by regularizing the softmax logits. By
bridging the gap between SGD and Adam, we also hope to shed light on why
certain optimization algorithms generalize better than others.","['Zijun Zhang', 'Lin Ma', 'Zongpeng Li', 'Chuan Wu']","['cs.LG', 'stat.ML']",2017-09-13 21:38:02+00:00
http://arxiv.org/abs/1709.04481v1,Network Classification and Categorization,"To the best of our knowledge, this paper presents the first large-scale study
that tests whether network categories (e.g., social networks vs. web graphs)
are distinguishable from one another (using both categories of real-world
networks and synthetic graphs). A classification accuracy of $94.2\%$ was
achieved using a random forest classifier with both real and synthetic
networks. This work makes two important findings. First, real-world networks
from various domains have distinct structural properties that allow us to
predict with high accuracy the category of an arbitrary network. Second,
classifying synthetic networks is trivial as our models can easily distinguish
between synthetic graphs and the real-world networks they are supposed to
model.","['James P. Canning', 'Emma E. Ingram', 'Sammantha Nowak-Wolff', 'Adriana M. Ortiz', 'Nesreen K. Ahmed', 'Ryan A. Rossi', 'Karl R. B. Schmitt', 'Sucheta Soundarajan']","['cs.SI', 'cs.DL', 'stat.ML']",2017-09-13 18:02:09+00:00
http://arxiv.org/abs/1709.04451v3,Alternating minimization and alternating descent over nonconvex sets,"We analyze the performance of alternating minimization for loss functions
optimized over two variables, where each variable may be restricted to lie in
some potentially nonconvex constraint set. This type of setting arises
naturally in high-dimensional statistics and signal processing, where the
variables often reflect different structures or components within the signals
being considered. Our analysis relies on the notion of local concavity
coefficients, which has been proposed in Barber and Ha to measure and quantify
the concavity of a general nonconvex set. Our results further reveal important
distinctions between alternating and non-alternating methods. Since computing
the alternating minimization steps may not be tractable for some problems, we
also consider an inexact version of the algorithm and provide a set of
sufficient conditions to ensure fast convergence of the inexact algorithms. We
demonstrate our framework on several examples, including low rank + sparse
decomposition and multitask regression, and provide numerical experiments to
validate our theoretical results.","['Wooseok Ha', 'Rina Foygel Barber']","['math.OC', 'stat.ML']",2017-09-13 17:53:44+00:00
http://arxiv.org/abs/1709.04412v2,The Merging Path Plot: adaptive fusing of k-groups with likelihood-based model selection,"There are many statistical tests that verify the null hypothesis: the
variable of interest has the same distribution among k-groups. But once the
null hypothesis is rejected, how to present the structure of dissimilarity
between groups? In this article, we introduce The Merging Path Plot - a
methodology, and factorMerger - an R package, for exploration and visualization
of k-group dissimilarities. Comparison of k-groups is one of the most important
issues in exploratory analyses and it has zillions of applications. The
classical solution is to test a~null hypothesis that observations from all
groups come from the same distribution. If the global null hypothesis is
rejected, a~more detailed analysis of differences among pairs of groups is
performed. The traditional approach is to use pairwise post hoc tests in order
to verify which groups differ significantly. However, this approach fails with
a large number of groups in both interpretation and visualization layer.
The~Merging Path Plot methodology solves this problem by using an
easy-to-understand description of dissimilarity among groups based on
Likelihood Ratio Test (LRT) statistic.","['Agnieszka Sitko', 'Przemyslaw Biecek']","['stat.ML', 'cs.HC', 'stat.ME']",2017-09-13 16:46:11+00:00
http://arxiv.org/abs/1709.04395v3,Tight Semi-Nonnegative Matrix Factorization,"The nonnegative matrix factorization is a widely used, flexible matrix
decomposition, finding applications in biology, image and signal processing and
information retrieval, among other areas. Here we present a related matrix
factorization. A multi-objective optimization problem finds conical
combinations of templates that approximate a given data matrix. The templates
are chosen so that as far as possible only the initial data set can be
represented this way. However, the templates are not required to be nonnegative
nor convex combinations of the original data.",['David W Dreisigmeyer'],"['stat.ML', 'cs.LG']",2017-09-13 16:05:12+00:00
http://arxiv.org/abs/1709.04384v2,Generating Music Medleys via Playing Music Puzzle Games,"Generating music medleys is about finding an optimal permutation of a given
set of music clips. Toward this goal, we propose a self-supervised learning
task, called the music puzzle game, to train neural network models to learn the
sequential patterns in music. In essence, such a game requires machines to
correctly sort a few multisecond music fragments. In the training stage, we
learn the model by sampling multiple non-overlapping fragment pairs from the
same songs and seeking to predict whether a given pair is consecutive and is in
the correct chronological order. For testing, we design a number of puzzle
games with different difficulty levels, the most difficult one being music
medley, which requiring sorting fragments from different songs. On the basis of
state-of-the-art Siamese convolutional network, we propose an improved
architecture that learns to embed frame-level similarity scores computed from
the input fragment pairs to a common space, where fragment pairs in the correct
order can be more easily identified. Our result shows that the resulting model,
dubbed as the similarity embedding network (SEN), performs better than
competing models across different games, including music jigsaw puzzle, music
sequencing, and music medley. Example results can be found at our project
website, https://remyhuang.github.io/DJnet.","['Yu-Siang Huang', 'Szu-Yu Chou', 'Yi-Hsuan Yang']","['stat.ML', 'cs.LG', 'cs.SD']",2017-09-13 15:33:07+00:00
http://arxiv.org/abs/1709.04212v8,Asymptotic Bayesian Generalization Error in Latent Dirichlet Allocation and Stochastic Matrix Factorization,"Latent Dirichlet allocation (LDA) is useful in document analysis, image
processing, and many information systems; however, its generalization
performance has been left unknown because it is a singular learning machine to
which regular statistical theory can not be applied.
  Stochastic matrix factorization (SMF) is a restricted matrix factorization in
which matrix factors are stochastic; the column of the matrix is in a simplex.
SMF is being applied to image recognition and text mining. We can understand
SMF as a statistical model by which a stochastic matrix of given data is
represented by a product of two stochastic matrices, whose generalization
performance has also been left unknown because of non-regularity.
  In this paper, by using an algebraic and geometric method, we show the
analytic equivalence of LDA and SMF, both of which have the same real log
canonical threshold (RLCT), resulting in that they asymptotically have the same
Bayesian generalization error and the same log marginal likelihood. Moreover,
we derive the upper bound of the RLCT and prove that it is smaller than the
dimension of the parameter divided by two, hence the Bayesian generalization
errors of them are smaller than those of regular statistical models.","['Naoki Hayashi', 'Sumio Watanabe']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH', '62F15']",2017-09-13 09:37:03+00:00
http://arxiv.org/abs/1709.04186v1,On labeling Android malware signatures using minhashing and further classification with Structural Equation Models,"Multi-scanner Antivirus systems provide insightful information on the nature
of a suspect application; however there is often a lack of consensus and
consistency between different Anti-Virus engines. In this article, we analyze
more than 250 thousand malware signatures generated by 61 different Anti-Virus
engines after analyzing 82 thousand different Android malware applications. We
identify 41 different malware classes grouped into three major categories,
namely Adware, Harmful Threats and Unknown or Generic signatures. We further
investigate the relationships between such 41 classes using community detection
algorithms from graph theory to identify similarities between them; and we
finally propose a Structure Equation Model to identify which Anti-Virus engines
are more powerful at detecting each macro-category. As an application, we show
how such models can help in identifying whether Unknown malware applications
are more likely to be of Harmful or Adware type.","['Ignacio Martín', 'José Alberto Hernández', 'Sergio de los Santos']","['cs.CR', 'cs.AI', 'stat.ML']",2017-09-13 08:38:36+00:00
http://arxiv.org/abs/1709.04135v2,Weighted Orthogonal Components Regression Analysis,"In the multiple linear regression setting, we propose a general framework,
termed weighted orthogonal components regression (WOCR), which encompasses many
known methods as special cases, including ridge regression and principal
components regression. WOCR makes use of the monotonicity inherent in
orthogonal components to parameterize the weight function. The formulation
allows for efficient determination of tuning parameters and hence is
computationally advantageous. Moreover, WOCR offers insights for deriving new
better variants. Specifically, we advocate weighting components based on their
correlations with the response, which leads to enhanced predictive performance.
Both simulated studies and real data examples are provided to assess and
illustrate the advantages of the proposed methods.","['Xiaogang Su', 'Yaa Wonkye', 'Pei Wang', 'Xiangrong Yin']","['stat.ML', '62J07']",2017-09-13 05:01:35+00:00
http://arxiv.org/abs/1709.04114v3,EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples,"Recent studies have highlighted the vulnerability of deep neural networks
(DNNs) to adversarial examples - a visually indistinguishable adversarial image
can easily be crafted to cause a well-trained model to misclassify. Existing
methods for crafting adversarial examples are based on $L_2$ and $L_\infty$
distortion metrics. However, despite the fact that $L_1$ distortion accounts
for the total variation and encourages sparsity in the perturbation, little has
been developed for crafting $L_1$-based adversarial examples. In this paper, we
formulate the process of attacking DNNs via adversarial examples as an
elastic-net regularized optimization problem. Our elastic-net attacks to DNNs
(EAD) feature $L_1$-oriented adversarial examples and include the
state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial
examples with small $L_1$ distortion and attains similar attack performance to
the state-of-the-art methods in different attack scenarios. More importantly,
EAD leads to improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in
adversarial machine learning and security implications of DNNs.","['Pin-Yu Chen', 'Yash Sharma', 'Huan Zhang', 'Jinfeng Yi', 'Cho-Jui Hsieh']","['stat.ML', 'cs.CR', 'cs.LG']",2017-09-13 02:40:59+00:00
http://arxiv.org/abs/1709.04108v1,Co-training for Demographic Classification Using Deep Learning from Label Proportions,"Deep learning algorithms have recently produced state-of-the-art accuracy in
many classification tasks, but this success is typically dependent on access to
many annotated training examples. For domains without such data, an attractive
alternative is to train models with light, or distant supervision. In this
paper, we introduce a deep neural network for the Learning from Label
Proportion (LLP) setting, in which the training data consist of bags of
unlabeled instances with associated label distributions for each bag. We
introduce a new regularization layer, Batch Averager, that can be appended to
the last layer of any deep neural network to convert it from supervised
learning to LLP. This layer can be implemented readily with existing deep
learning packages. To further support domains in which the data consist of two
conditionally independent feature views (e.g. image and text), we propose a
co-training algorithm that iteratively generates pseudo bags and refits the
deep LLP model to improve classification accuracy. We demonstrate our models on
demographic attribute classification (gender and race/ethnicity), which has
many applications in social media analysis, public health, and marketing. We
conduct experiments to predict demographics of Twitter users based on their
tweets and profile image, without requiring any user-level annotations for
training. We find that the deep LLP approach outperforms baselines for both
text and image features separately. Additionally, we find that co-training
algorithm improves image and text classification by 4% and 8% absolute F1,
respectively. Finally, an ensemble of text and image classifiers further
improves the absolute F1 measure by 4% on average.","['Ehsan Mohammady Ardehaly', 'Aron Culotta']","['cs.CV', 'cs.LG', 'stat.ML']",2017-09-13 02:06:19+00:00
http://arxiv.org/abs/1709.04077v2,Setpoint Tracking with Partially Observed Loads,"We use online convex optimization (OCO) for setpoint tracking with uncertain,
flexible loads. We consider full feedback from the loads, bandit feedback, and
two intermediate types of feedback: partial bandit where a subset of the loads
are individually observed and the rest are observed in aggregate, and Bernoulli
feedback where in each round the aggregator receives either full or bandit
feedback according to a known probability. We give sublinear regret bounds in
all cases. We numerically evaluate our algorithms on examples with
thermostatically controlled loads and electric vehicles.","['Antoine Lesage-Landry', 'Joshua A. Taylor']","['math.OC', 'stat.ML']",2017-09-12 22:48:07+00:00
http://arxiv.org/abs/1709.04073v1,Linear Stochastic Approximation: Constant Step-Size and Iterate Averaging,"We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)
with a constant step-size and the so called Polyak-Ruppert (PR) averaging of
iterates. LSAs are widely applied in machine learning and reinforcement
learning (RL), where the aim is to compute an appropriate $\theta_{*} \in
\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$
updates per iteration. In this paper, we are motivated by the problem (in RL)
of policy evaluation from experience replay using the \emph{temporal
difference} (TD) class of learning algorithms that are also LSAs. For LSAs with
a constant step-size, and PR averaging, we provide bounds for the mean squared
error (MSE) after $t$ iterations. We assume that data is \iid with finite
variance (underlying distribution being $P$) and that the expected dynamics is
Hurwitz. For a given LSA with PR averaging, and data distribution $P$
satisfying the said assumptions, we show that there exists a range of constant
step-sizes such that its MSE decays as $O(\frac{1}{t})$.
  We examine the conditions under which a constant step-size can be chosen
uniformly for a class of data distributions $\mathcal{P}$, and show that not
all data distributions `admit' such a uniform constant step-size. We also
suggest a heuristic step-size tuning algorithm to choose a constant step-size
of a given LSA for a given data distribution $P$. We compare our results with
related work and also discuss the implication of our results in the context of
TD algorithms that are LSAs.","['Chandrashekar Lakshminarayanan', 'Csaba Szepesvári']","['cs.LG', 'cs.SY', 'stat.ML']",2017-09-12 22:34:09+00:00
http://arxiv.org/abs/1709.04072v6,A convergence framework for inexact nonconvex and nonsmooth algorithms and its applications to several iterations,"In this paper, we consider the convergence of an abstract inexact nonconvex
and nonsmooth algorithm. We promise a pseudo sufficient descent condition and a
pseudo relative error condition, which are both related to an auxiliary
sequence, for the algorithm; and a continuity condition is assumed to hold. In
fact, a lot of classical inexact nonconvex and nonsmooth algorithms allow these
three conditions. Under a special kind of summable assumption on the auxiliary
sequence, we prove the sequence generated by the general algorithm converges to
a critical point of the objective function if being assumed Kurdyka-
Lojasiewicz property. The core of the proofs lies in building a new Lyapunov
function, whose successive difference provides a bound for the successive
difference of the points generated by the algorithm. And then, we apply our
findings to several classical nonconvex iterative algorithms and derive the
corresponding convergence results","['Tao Sun', 'Hao Jiang', 'Lizhi Cheng', 'Wei Zhu']","['math.OC', 'math.NA', 'stat.ML']",2017-09-12 22:28:18+00:00
http://arxiv.org/abs/1709.04054v3,Shifting Mean Activation Towards Zero with Bipolar Activation Functions,"We propose a simple extension to the ReLU-family of activation functions that
allows them to shift the mean activation across a layer towards zero. Combined
with proper weight initialization, this alleviates the need for normalization
layers. We explore the training of deep vanilla recurrent neural networks
(RNNs) with up to 144 layers, and show that bipolar activation functions help
learning in this setting. On the Penn Treebank and Text8 language modeling
tasks we obtain competitive results, improving on the best reported results for
non-gated networks. In experiments with convolutional neural networks without
batch normalization, we find that bipolar activations produce a faster drop in
training error, and results in a lower test error on the CIFAR-10
classification task.","['Lars Eidnes', 'Arild Nøkland']","['stat.ML', 'cs.LG', 'cs.NE']",2017-09-12 20:44:15+00:00
http://arxiv.org/abs/1709.04024v3,Discovering Potential Correlations via Hypercontractivity,"Discovering a correlation from one variable to another variable is of
fundamental scientific and practical interest. While existing correlation
measures are suitable for discovering average correlation, they fail to
discover hidden or potential correlations. To bridge this gap, (i) we postulate
a set of natural axioms that we expect a measure of potential correlation to
satisfy; (ii) we show that the rate of information bottleneck, i.e., the
hypercontractivity coefficient, satisfies all the proposed axioms; (iii) we
provide a novel estimator to estimate the hypercontractivity coefficient from
samples; and (iv) we provide numerical experiments demonstrating that this
proposed estimator discovers potential correlations among various indicators of
WHO datasets, is robust in discovering gene interactions from gene expression
time series data, and is statistically more powerful than the estimators for
other correlation measures in binary hypothesis testing of canonical examples
of potential correlations.","['Hyeji Kim', 'Weihao Gao', 'Sreeram Kannan', 'Sewoong Oh', 'Pramod Viswanath']",['stat.ML'],2017-09-12 19:07:32+00:00
http://arxiv.org/abs/1709.04004v2,Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits,"In this paper, we propose and study opportunistic bandits - a new variant of
bandits where the regret of pulling a suboptimal arm varies under different
environmental conditions, such as network load or produce price. When the
load/price is low, so is the cost/regret of pulling a suboptimal arm (e.g.,
trying a suboptimal network configuration). Therefore, intuitively, we could
explore more when the load/price is low and exploit more when the load/price is
high. Inspired by this intuition, we propose an Adaptive Upper-Confidence-Bound
(AdaUCB) algorithm to adaptively balance the exploration-exploitation tradeoff
for opportunistic bandits. We prove that AdaUCB achieves $O(\log T)$ regret
with a smaller coefficient than the traditional UCB algorithm. Furthermore,
AdaUCB achieves $O(1)$ regret with respect to $T$ if the exploration cost is
zero when the load level is below a certain threshold. Last, based on both
synthetic data and real-world traces, experimental results show that AdaUCB
significantly outperforms other bandit algorithms, such as UCB and TS (Thompson
Sampling), under large load/price fluctuations.","['Huasen Wu', 'Xueying Guo', 'Xin Liu']","['cs.LG', 'stat.ML']",2017-09-12 18:18:33+00:00
http://arxiv.org/abs/1709.03907v1,Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information,"We study the misclassification error for community detection in general
heterogeneous stochastic block models (SBM) with noisy or partial label
information. We establish a connection between the misclassification rate and
the notion of minimum energy on the local neighborhood of the SBM. We develop
an optimally weighted message passing algorithm to reconstruct labels for SBM
based on the minimum energy flow and the eigenvectors of a certain Markov
transition matrix. The general SBM considered in this paper allows for
unequal-size communities, degree heterogeneity, and different connection
probabilities among blocks. We focus on how to optimally weigh the message
passing to improve misclassification.","['T. Tony Cai', 'Tengyuan Liang', 'Alexander Rakhlin']","['math.ST', 'stat.ML', 'stat.TH']",2017-09-12 15:22:21+00:00
http://arxiv.org/abs/1709.03891v1,High-Dimensional Dependency Structure Learning for Physical Processes,"In this paper, we consider the use of structure learning methods for
probabilistic graphical models to identify statistical dependencies in
high-dimensional physical processes. Such processes are often synthetically
characterized using PDEs (partial differential equations) and are observed in a
variety of natural phenomena, including geoscience data capturing atmospheric
and hydrological phenomena. Classical structure learning approaches such as the
PC algorithm and variants are challenging to apply due to their high
computational and sample requirements. Modern approaches, often based on sparse
regression and variants, do come with finite sample guarantees, but are usually
highly sensitive to the choice of hyper-parameters, e.g., parameter $\lambda$
for sparsity inducing constraint or regularization. In this paper, we present
ACLIME-ADMM, an efficient two-step algorithm for adaptive structure learning,
which estimates an edge specific parameter $\lambda_{ij}$ in the first step,
and uses these parameters to learn the structure in the second step. Both steps
of our algorithm use (inexact) ADMM to solve suitable linear programs, and all
iterations can be done in closed form in an efficient block parallel manner. We
compare ACLIME-ADMM with baselines on both synthetic data simulated by partial
differential equations (PDEs) that model advection-diffusion processes, and
real data (50 years) of daily global geopotential heights to study information
flow in the atmosphere. ACLIME-ADMM is shown to be efficient, stable, and
competitive, usually better than the baselines especially on difficult
problems. On real data, ACLIME-ADMM recovers the underlying structure of global
atmospheric circulation, including switches in wind directions at the equator
and tropics entirely from the data.","['Jamal Golmohammadi', 'Imme Ebert-Uphoff', 'Sijie He', 'Yi Deng', 'Arindam Banerjee']","['cs.LG', 'stat.ML']",2017-09-12 14:54:57+00:00
http://arxiv.org/abs/1709.03831v1,Dual Discriminator Generative Adversarial Nets,"We propose in this paper a novel approach to tackle the problem of mode
collapse encountered in generative adversarial network (GAN). Our idea is
intuitive but proven to be very effective, especially in addressing some key
limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and
reverse KL divergences into a unified objective function, thus it exploits the
complementary statistical properties from these divergences to effectively
diversify the estimated density in capturing multi-modes. We term our method
dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has
two discriminators; and together with a generator, it also has the analogy of a
minimax game, wherein a discriminator rewards high scores for samples from data
distribution whilst another discriminator, conversely, favoring data from the
generator, and the generator produces data to fool both two discriminators. We
develop theoretical analysis to show that, given the maximal discriminators,
optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL
divergences between data distribution and the distribution induced from the
data generated by the generator, hence effectively avoiding the mode collapsing
problem. We conduct extensive experiments on synthetic and real-world
large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made
our best effort to compare our D2GAN with the latest state-of-the-art GAN's
variants in comprehensive qualitative and quantitative evaluations. The
experimental results demonstrate the competitive and superior performance of
our approach in generating good quality and diverse samples over baselines, and
the capability of our method to scale up to ImageNet database.","['Tu Dinh Nguyen', 'Trung Le', 'Hung Vu', 'Dinh Phung']","['cs.LG', 'stat.ML']",2017-09-12 13:28:48+00:00
http://arxiv.org/abs/1709.03768v3,Learning with Bounded Instance- and Label-dependent Label Noise,"Instance- and Label-dependent label Noise (ILN) widely exists in real-world
datasets but has been rarely studied. In this paper, we focus on Bounded
Instance- and Label-dependent label Noise (BILN), a particular case of ILN
where the label noise rates -- the probabilities that the true labels of
examples flip into the corrupted ones -- have upper bound less than $1$.
Specifically, we introduce the concept of distilled examples, i.e. examples
whose labels are identical with the labels assigned for them by the Bayes
optimal classifier, and prove that under certain conditions classifiers learnt
on distilled examples will converge to the Bayes optimal classifier. Inspired
by the idea of learning with distilled examples, we then propose a learning
algorithm with theoretical guarantees for its robustness to BILN. At last,
empirical evaluations on both synthetic and real-world datasets show
effectiveness of our algorithm in learning with BILN.","['Jiacheng Cheng', 'Tongliang Liu', 'Kotagiri Ramamohanarao', 'Dacheng Tao']","['stat.ML', 'cs.LG']",2017-09-12 09:59:59+00:00
http://arxiv.org/abs/1709.03741v2,Learning Graph-Level Representation for Drug Discovery,"Predicating macroscopic influences of drugs on human body, like efficacy and
toxicity, is a central problem of small-molecule based drug discovery.
Molecules can be represented as an undirected graph, and we can utilize graph
convolution networks to predication molecular properties. However, graph
convolutional networks and other graph neural networks all focus on learning
node-level representation rather than graph-level representation. Previous
works simply sum all feature vectors for all nodes in the graph to obtain the
graph feature vector for drug predication. In this paper, we introduce a dummy
super node that is connected with all nodes in the graph by a directed edge as
the representation of the graph and modify the graph operation to help the
dummy super node learn graph-level feature. Thus, we can handle graph-level
classification and regression in the same way as node-level classification and
regression. In addition, we apply focal loss to address class imbalance in drug
datasets. The experiments on MoleculeNet show that our method can effectively
improve the performance of molecular properties predication.","['Junying Li', 'Deng Cai', 'Xiaofei He']","['cs.LG', 'stat.ML']",2017-09-12 08:41:39+00:00
http://arxiv.org/abs/1709.03698v2,Reversible Architectures for Arbitrarily Deep Residual Neural Networks,"Recently, deep residual networks have been successfully applied in many
computer vision and natural language processing tasks, pushing the
state-of-the-art performance with deeper and wider architectures. In this work,
we interpret deep residual networks as ordinary differential equations (ODEs),
which have long been studied in mathematics and physics with rich theoretical
and empirical success. From this interpretation, we develop a theoretical
framework on stability and reversibility of deep neural networks, and derive
three reversible neural network architectures that can go arbitrarily deep in
theory. The reversibility property allows a memory-efficient implementation,
which does not need to store the activations for most hidden layers. Together
with the stability of our architectures, this enables training deeper networks
using only modest computational resources. We provide both theoretical analyses
and empirical results. Experimental results demonstrate the efficacy of our
architectures against several strong baselines on CIFAR-10, CIFAR-100 and
STL-10 with superior or on-par state-of-the-art performance. Furthermore, we
show our architectures yield superior results when trained using fewer training
data.","['Bo Chang', 'Lili Meng', 'Eldad Haber', 'Lars Ruthotto', 'David Begert', 'Elliot Holtham']","['cs.CV', 'stat.ML']",2017-09-12 05:41:13+00:00
http://arxiv.org/abs/1709.03683v1,A Practically Competitive and Provably Consistent Algorithm for Uplift Modeling,"Randomized experiments have been critical tools of decision making for
decades. However, subjects can show significant heterogeneity in response to
treatments in many important applications. Therefore it is not enough to simply
know which treatment is optimal for the entire population. What we need is a
model that correctly customize treatment assignment base on subject
characteristics. The problem of constructing such models from randomized
experiments data is known as Uplift Modeling in the literature. Many algorithms
have been proposed for uplift modeling and some have generated promising
results on various data sets. Yet little is known about the theoretical
properties of these algorithms. In this paper, we propose a new tree-based
ensemble algorithm for uplift modeling. Experiments show that our algorithm can
achieve competitive results on both synthetic and industry-provided data. In
addition, by properly tuning the ""node size"" parameter, our algorithm is proved
to be consistent under mild regularity conditions. This is the first consistent
algorithm for uplift modeling that we are aware of.","['Yan Zhao', 'Xiao Fang', 'David Simchi-Levi']","['cs.LG', 'cs.AI', 'stat.ML']",2017-09-12 03:49:57+00:00
http://arxiv.org/abs/1709.03670v1,Community Recovery in Hypergraphs,"Community recovery is a central problem that arises in a wide variety of
applications such as network clustering, motion segmentation, face clustering
and protein complex detection. The objective of the problem is to cluster data
points into distinct communities based on a set of measurements, each of which
is associated with the values of a certain number of data points. While most of
the prior works focus on a setting in which the number of data points involved
in a measurement is two, this work explores a generalized setting in which the
number can be more than two. Motivated by applications particularly in machine
learning and channel coding, we consider two types of measurements: (1)
homogeneity measurement which indicates whether or not the associated data
points belong to the same community; (2) parity measurement which denotes the
modulo-2 sum of the values of the data points. Such measurements are possibly
corrupted by Bernoulli noise. We characterize the fundamental limits on the
number of measurements required to reconstruct the communities for the
considered models.","['Kwangjun Ahn', 'Kangwook Lee', 'Changho Suh']","['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2017-09-12 03:08:33+00:00
http://arxiv.org/abs/1709.03658v2,End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks,"Speech enhancement model is used to map a noisy speech to a clean speech. In
the training stage, an objective function is often adopted to optimize the
model parameters. However, in most studies, there is an inconsistency between
the model optimization criterion and the evaluation criterion on the enhanced
speech. For example, in measuring speech intelligibility, most of the
evaluation metric is based on a short-time objective intelligibility (STOI)
measure, while the frame based minimum mean square error (MMSE) between
estimated and clean speech is widely used in optimizing the model. Due to the
inconsistency, there is no guarantee that the trained model can provide optimal
performance in applications. In this study, we propose an end-to-end
utterance-based speech enhancement framework using fully convolutional neural
networks (FCN) to reduce the gap between the model optimization and evaluation
criterion. Because of the utterance-based optimization, temporal correlation
information of long speech segments, or even at the entire utterance level, can
be considered when perception-based objective functions are used for the direct
optimization. As an example, we implement the proposed FCN enhancement
framework to optimize the STOI measure. Experimental results show that the STOI
of test speech is better than conventional MMSE-optimized speech due to the
consistency between the training and evaluation target. Moreover, by
integrating the STOI in model optimization, the intelligibility of human
subjects and automatic speech recognition (ASR) system on the enhanced speech
is also substantially improved compared to those generated by the MMSE
criterion.","['Szu-Wei Fu', 'Tao-Wei Wang', 'Yu Tsao', 'Xugang Lu', 'Hisashi Kawai']","['stat.ML', 'cs.LG', 'cs.SD']",2017-09-12 02:24:50+00:00
http://arxiv.org/abs/1709.03645v1,Identifying Genetic Risk Factors via Sparse Group Lasso with Group Graph Structure,"Genome-wide association studies (GWA studies or GWAS) investigate the
relationships between genetic variants such as single-nucleotide polymorphisms
(SNPs) and individual traits. Recently, incorporating biological priors
together with machine learning methods in GWA studies has attracted increasing
attention. However, in real-world, nucleotide-level bio-priors have not been
well-studied to date. Alternatively, studies at gene-level, for example,
protein--protein interactions and pathways, are more rigorous and legitimate,
and it is potentially beneficial to utilize such gene-level priors in GWAS. In
this paper, we proposed a novel two-level structured sparse model, called
Sparse Group Lasso with Group-level Graph structure (SGLGG), for GWAS. It can
be considered as a sparse group Lasso along with a group-level graph Lasso.
Essentially, SGLGG penalizes the nucleotide-level sparsity as well as takes
advantages of gene-level priors (both gene groups and networks), to identifying
phenotype-associated risk SNPs. We employ the alternating direction method of
multipliers algorithm to optimize the proposed model. Our experiments on the
Alzheimer's Disease Neuroimaging Initiative whole genome sequence data and
neuroimage data demonstrate the effectiveness of SGLGG. As a regression model,
it is competitive to the state-of-the-arts sparse models; as a variable
selection method, SGLGG is promising for identifying Alzheimer's
disease-related risk SNPs.","['Tao Yang', 'Paul Thompson', 'Sihai Zhao', 'Jieping Ye']","['stat.ML', 'cs.LG', 'q-bio.GN']",2017-09-12 01:34:50+00:00
http://arxiv.org/abs/1709.03625v2,Budgeted Experiment Design for Causal Structure Learning,"We study the problem of causal structure learning when the experimenter is
limited to perform at most $k$ non-adaptive experiments of size $1$. We
formulate the problem of finding the best intervention target set as an
optimization problem, which aims to maximize the average number of edges whose
directions are resolved. We prove that the corresponding objective function is
submodular and a greedy algorithm suffices to achieve
$(1-\frac{1}{e})$-approximation of the optimal value. We further present an
accelerated variant of the greedy algorithm, which can lead to orders of
magnitude performance speedup. We validate our proposed approach on synthetic
and real graphs. The results show that compared to the purely observational
setting, our algorithm orients the majority of the edges through a considerably
small number of interventions.","['AmirEmad Ghassami', 'Saber Salehkaleybar', 'Negar Kiyavash', 'Elias Bareinboim']","['cs.LG', 'cs.AI', 'stat.ML']",2017-09-11 23:43:30+00:00
http://arxiv.org/abs/1709.03615v1,Manifold Learning Using Kernel Density Estimation and Local Principal Components Analysis,"We consider the problem of recovering a $d-$dimensional manifold $\mathcal{M}
\subset \mathbb{R}^n$ when provided with noiseless samples from $\mathcal{M}$.
There are many algorithms (e.g., Isomap) that are used in practice to fit
manifolds and thus reduce the dimensionality of a given data set. Ideally, the
estimate $\mathcal{M}_\mathrm{put}$ of $\mathcal{M}$ should be an actual
manifold of a certain smoothness; furthermore, $\mathcal{M}_\mathrm{put}$
should be arbitrarily close to $\mathcal{M}$ in Hausdorff distance given a
large enough sample. Generally speaking, existing manifold learning algorithms
do not meet these criteria. Fefferman, Mitter, and Narayanan (2016) have
developed an algorithm whose output is provably a manifold. The key idea is to
define an approximate squared-distance function (asdf) to $\mathcal{M}$. Then,
$\mathcal{M}_\mathrm{put}$ is given by the set of points where the gradient of
the asdf is orthogonal to the subspace spanned by the largest $n - d$
eigenvectors of the Hessian of the asdf. As long as the asdf meets certain
regularity conditions, $\mathcal{M}_\mathrm{put}$ is a manifold that is
arbitrarily close in Hausdorff distance to $\mathcal{M}$. In this paper, we
define two asdfs that can be calculated from the data and show that they meet
the required regularity conditions. The first asdf is based on kernel density
estimation, and the second is based on estimation of tangent spaces using local
principal components analysis.","['Kitty Mohammed', 'Hariharan Narayanan']","['math.ST', 'stat.ML', 'stat.TH']",2017-09-11 22:45:10+00:00
http://arxiv.org/abs/1709.03528v5,GIANT: Globally Improved Approximate Newton Method for Distributed Optimization,"For distributed computing environment, we consider the empirical risk
minimization problem and propose a distributed and communication-efficient
Newton-type optimization method. At every iteration, each worker locally finds
an Approximate NewTon (ANT) direction, which is sent to the main driver. The
main driver, then, averages all the ANT directions received from workers to
form a {\it Globally Improved ANT} (GIANT) direction. GIANT is highly
communication efficient and naturally exploits the trade-offs between local
computations and global communications in that more local computations result
in fewer overall rounds of communications. Theoretically, we show that GIANT
enjoys an improved convergence rate as compared with first-order methods and
existing distributed Newton-type methods. Further, and in sharp contrast with
many existing distributed Newton-type methods, as well as popular first-order
methods, a highly advantageous practical feature of GIANT is that it only
involves one tuning parameter. We conduct large-scale experiments on a computer
cluster and, empirically, demonstrate the superior performance of GIANT.","['Shusen Wang', 'Farbod Roosta-Khorasani', 'Peng Xu', 'Michael W. Mahoney']","['cs.LG', 'cs.DC', 'math.OC', 'stat.ML']",2017-09-11 18:17:18+00:00
http://arxiv.org/abs/1709.03473v4,Is completeness necessary? Estimation in nonidentified linear models,"We show that estimators based on spectral regularization converge to the best
approximation of a structural parameter in a class of nonidentified linear
ill-posed inverse models. Importantly, this convergence holds in the uniform
and Hilbert space norms. We describe several circumstances when the best
approximation coincides with a structural parameter, or at least reasonably
approximates it, and discuss how our results can be useful in the partial
identification setting. Lastly, we document that identification failures have
important implications for the asymptotic distribution of a linear functional
of regularized estimators, which can have a weighted chi-squared component. The
theory is illustrated for various high-dimensional and nonparametric IV
regressions.","['Andrii Babii', 'Jean-Pierre Florens']","['math.ST', 'econ.EM', 'stat.ML', 'stat.TH']",2017-09-11 16:49:19+00:00
http://arxiv.org/abs/1709.03423v2,Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,"Deep learning has become the state of the art approach in many machine
learning problems such as classification. It has recently been shown that deep
learning is highly vulnerable to adversarial perturbations. Taking the camera
systems of self-driving cars as an example, small adversarial perturbations can
cause the system to make errors in important tasks, such as classifying traffic
signs or detecting pedestrians. Hence, in order to use deep learning without
safety concerns a proper defense strategy is required. We propose to use
ensemble methods as a defense strategy against adversarial perturbations. We
find that an attack leading one model to misclassify does not imply the same
for other networks performing the same task. This makes ensemble methods an
attractive defense strategy against adversarial attacks. We empirically show
for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve
the accuracy of neural networks on test data but also increase their robustness
against adversarial perturbations.","['Thilo Strauss', 'Markus Hanselmann', 'Andrej Junginger', 'Holger Ulmer']","['stat.ML', 'cs.LG']",2017-09-11 15:01:03+00:00
http://arxiv.org/abs/1709.03943v1,Support Spinor Machine,"We generalize a support vector machine to a support spinor machine by using
the mathematical structure of wedge product over vector machine in order to
extend field from vector field to spinor field. The separated hyperplane is
extended to Kolmogorov space in time series data which allow us to extend a
structure of support vector machine to a support tensor machine and a support
tensor machine moduli space. Our performance test on support spinor machine is
done over one class classification of end point in physiology state of time
series data after empirical mode analysis and compared with support vector
machine test. We implement algorithm of support spinor machine by using
Holo-Hilbert amplitude modulation for fully nonlinear and nonstationary time
series data analysis.","['Kabin Kanjamapornkul', 'Richard Pinčák', 'Sanphet Chunithpaisan', 'Erik Bartoš']","['cs.LG', 'eess.SP', 'q-fin.ST', 'stat.ML']",2017-09-11 07:23:57+00:00
http://arxiv.org/abs/1709.03252v2,Evaluation of Classical Features and Classifiers in Brain-Computer Interface Tasks,"Brain-Computer Interface (BCI) uses brain signals in order to provide a new
method for communication between human and outside world. Feature extraction,
selection and classification are among the main matters of concerns in signal
processing stage of BCI. In this article, we present our findings about the
most effective features and classifiers in some brain tasks. Six different
groups of classical features and twelve classifiers have been examined in nine
datasets of brain signal. The results indicate that energy of brain signals in
{\alpha} and \b{eta} frequency bands, together with some statistical parameters
are more effective, comparing to the other types of extracted features. In
addition, Bayesian classifier with Gaussian distribution assumption and also
Support Vector Machine (SVM) show to classify different BCI datasets more
accurately than the other classifiers. We believe that the results can give an
insight about a strategy for blind classification of brain signals in
brain-computer interface.","['Ehsan Arbabi', 'Mohammad Bagher Shamsollahi']","['cs.HC', 'stat.ML']",2017-09-11 05:57:07+00:00
http://arxiv.org/abs/1709.03239v2,On better training the infinite restricted Boltzmann machines,"The infinite restricted Boltzmann machine (iRBM) is an extension of the
classic RBM. It enjoys a good property of automatically deciding the size of
the hidden layer according to specific training data. With sufficient training,
the iRBM can achieve a competitive performance with that of the classic RBM.
However, the convergence of learning the iRBM is slow, due to the fact that the
iRBM is sensitive to the ordering of its hidden units, the learned filters
change slowly from the left-most hidden unit to right. To break this dependency
between neighboring hidden units and speed up the convergence of training, a
novel training strategy is proposed. The key idea of the proposed training
strategy is randomly regrouping the hidden units before each gradient descent
step. Potentially, a mixing of infinite many iRBMs with different permutations
of the hidden units can be achieved by this learning method, which has a
similar effect of preventing the model from over-fitting as the dropout. The
original iRBM is also modified to be capable of carrying out discriminative
training. To evaluate the impact of our method on convergence speed of learning
and the model's generalization ability, several experiments have been performed
on the binarized MNIST and CalTech101 Silhouettes datasets. Experimental
results indicate that the proposed training strategy can greatly accelerate
learning and enhance generalization ability of iRBMs.","['Xuan Peng', 'Xunzhang Gao', 'Xiang Li']","['cs.LG', 'cs.AI', 'stat.ML']",2017-09-11 04:41:06+00:00
http://arxiv.org/abs/1709.03202v1,Semi-Supervised Active Clustering with Weak Oracles,"Semi-supervised active clustering (SSAC) utilizes the knowledge of a domain
expert to cluster data points by interactively making pairwise ""same-cluster""
queries. However, it is impractical to ask human oracles to answer every
pairwise query. In this paper, we study the influence of allowing ""not-sure""
answers from a weak oracle and propose algorithms to efficiently handle
uncertainties. Different types of model assumptions are analyzed to cover
realistic scenarios of oracle abstraction. In the first model, random-weak
oracle, an oracle randomly abstains with a certain probability. We also
proposed two distance-weak oracle models which simulate the case of getting
confused based on the distance between two points in a pairwise query. For each
weak oracle model, we show that a small query complexity is adequate for the
effective $k$ means clustering with high probability. Sufficient conditions for
the guarantee include a $\gamma$-margin property of the data, and an existence
of a point close to each cluster center. Furthermore, we provide a sample
complexity with a reduced effect of the cluster's margin and only a logarithmic
dependency on the data dimension. Our results allow significantly less number
of same-cluster queries if the margin of the clusters is tight, i.e. $\gamma
\approx 1$. Experimental results on synthetic data show the effective
performance of our approach in overcoming uncertainties.","['Taewan Kim', 'Joydeep Ghosh']","['stat.ML', 'cs.LG']",2017-09-11 00:44:17+00:00
http://arxiv.org/abs/1709.03183v1,Rates of Convergence of Spectral Methods for Graphon Estimation,"This paper studies the problem of estimating the grahpon model - the
underlying generating mechanism of a network. Graphon estimation arises in many
applications such as predicting missing links in networks and learning user
preferences in recommender systems. The graphon model deals with a random graph
of $n$ vertices such that each pair of two vertices $i$ and $j$ are connected
independently with probability $\rho \times f(x_i,x_j)$, where $x_i$ is the
unknown $d$-dimensional label of vertex $i$, $f$ is an unknown symmetric
function, and $\rho$ is a scaling parameter characterizing the graph sparsity.
Recent studies have identified the minimax error rate of estimating the graphon
from a single realization of the random graph. However, there exists a wide gap
between the known error rates of computationally efficient estimation
procedures and the minimax optimal error rate.
  Here we analyze a spectral method, namely universal singular value
thresholding (USVT) algorithm, in the relatively sparse regime with the average
vertex degree $n\rho=\Omega(\log n)$. When $f$ belongs to H\""{o}lder or Sobolev
space with smoothness index $\alpha$, we show the error rate of USVT is at most
$(n\rho)^{ -2 \alpha / (2\alpha+d)}$, approaching the minimax optimal error
rate $\log (n\rho)/(n\rho)$ for $d=1$ as $\alpha$ increases. Furthermore, when
$f$ is analytic, we show the error rate of USVT is at most $\log^d
(n\rho)/(n\rho)$. In the special case of stochastic block model with $k$
blocks, the error rate of USVT is at most $k/(n\rho)$, which is larger than the
minimax optimal error rate by at most a multiplicative factor $k/\log k$. This
coincides with the computational gap observed for community detection. A key
step of our analysis is to derive the eigenvalue decaying rate of the edge
probability matrix using piecewise polynomial approximations of the graphon
function $f$.",['Jiaming Xu'],"['stat.ML', 'cs.LG', 'cs.SI', 'math.ST', 'stat.TH']",2017-09-10 21:45:48+00:00
http://arxiv.org/abs/1709.03163v3,Variational inference for the multi-armed contextual bandit,"In many biomedical, science, and engineering problems, one must sequentially
decide which action to take next so as to maximize rewards. One general class
of algorithms for optimizing interactions with the world, while simultaneously
learning how the world operates, is the multi-armed bandit setting and, in
particular, the contextual bandit case. In this setting, for each executed
action, one observes rewards that are dependent on a given 'context', available
at each interaction with the world. The Thompson sampling algorithm has
recently been shown to enjoy provable optimality properties for this set of
problems, and to perform well in real-world settings. It facilitates generative
and interpretable modeling of the problem at hand. Nevertheless, the design and
complexity of the model limit its application, since one must both sample from
the distributions modeled and calculate their expected rewards. We here show
how these limitations can be overcome using variational inference to
approximate complex models, applying to the reinforcement learning case
advances developed for the inference case in the machine learning community
over the past two decades. We consider contextual multi-armed bandit
applications where the true reward distribution is unknown and complex, which
we approximate with a mixture model whose parameters are inferred via
variational inference. We show how the proposed variational Thompson sampling
approach is accurate in approximating the true distribution, and attains
reduced regrets even with complex reward distributions. The proposed algorithm
is valuable for practical scenarios where restrictive modeling assumptions are
undesirable.","['Iñigo Urteaga', 'Chris H. Wiggins']","['stat.ML', 'cs.LG', 'stat.CO', 'I.2.6']",2017-09-10 19:58:44+00:00
http://arxiv.org/abs/1709.03162v2,Bayesian bandits: balancing the exploration-exploitation tradeoff via double sampling,"Reinforcement learning studies how to balance exploration and exploitation in
real-world systems, optimizing interactions with the world while simultaneously
learning how the world operates. One general class of algorithms for such
learning is the multi-armed bandit setting. Randomized probability matching,
based upon the Thompson sampling approach introduced in the 1930s, has recently
been shown to perform well and to enjoy provable optimality properties. It
permits generative, interpretable modeling in a Bayesian setting, where prior
knowledge is incorporated, and the computed posteriors naturally capture the
full state of knowledge. In this work, we harness the information contained in
the Bayesian posterior and estimate its sufficient statistics via sampling. In
several application domains, for example in health and medicine, each
interaction with the world can be expensive and invasive, whereas drawing
samples from the model is relatively inexpensive. Exploiting this viewpoint, we
develop a double sampling technique driven by the uncertainty in the learning
process: it favors exploitation when certain about the properties of each arm,
exploring otherwise. The proposed algorithm does not make any distributional
assumption and it is applicable to complex reward distributions, as long as
Bayesian posterior updates are computable. Utilizing the estimated posterior
sufficient statistics, double sampling autonomously balances the
exploration-exploitation tradeoff to make better informed decisions. We
empirically show its reduced cumulative regret when compared to
state-of-the-art alternatives in representative bandit settings.","['Iñigo Urteaga', 'Chris H. Wiggins']","['stat.ML', 'cs.LG', 'stat.CO', 'I.2.6']",2017-09-10 19:58:34+00:00
http://arxiv.org/abs/1709.03159v1,R2N2: Residual Recurrent Neural Networks for Multivariate Time Series Forecasting,"Multivariate time-series modeling and forecasting is an important problem
with numerous applications. Traditional approaches such as VAR (vector
auto-regressive) models and more recent approaches such as RNNs (recurrent
neural networks) are indispensable tools in modeling time-series data. In many
multivariate time series modeling problems, there is usually a significant
linear dependency component, for which VARs are suitable, and a nonlinear
component, for which RNNs are suitable. Modeling such times series with only
VAR or only RNNs can lead to poor predictive performance or complex models with
large training times. In this work, we propose a hybrid model called R2N2
(Residual RNN), which first models the time series with a simple linear model
(like VAR) and then models its residual errors using RNNs. R2N2s can be trained
using existing algorithms for VARs and RNNs. Through an extensive empirical
evaluation on two real world datasets (aviation and climate domains), we show
that R2N2 is competitive, usually better than VAR or RNN, used alone. We also
show that R2N2 is faster to train as compared to an RNN, while requiring less
number of hidden units.","['Hardik Goel', 'Igor Melnyk', 'Arindam Banerjee']","['cs.LG', 'stat.ML']",2017-09-10 19:29:49+00:00
http://arxiv.org/abs/1709.03082v8,A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data,"Gated Recurrent Unit (GRU) is a recently-developed variation of the long
short-term memory (LSTM) unit, both of which are types of recurrent neural
network (RNN). Through empirical evidence, both models have been proven to be
effective in a wide variety of machine learning tasks such as natural language
processing (Wen et al., 2015), speech recognition (Chorowski et al., 2015), and
text classification (Yang et al., 2016). Conventionally, like most neural
networks, both of the aforementioned RNN variants employ the Softmax function
as its final output layer for its prediction, and the cross-entropy function
for computing its loss. In this paper, we present an amendment to this norm by
introducing linear support vector machine (SVM) as the replacement for Softmax
in the final output layer of a GRU model. Furthermore, the cross-entropy
function shall be replaced with a margin-based function. While there have been
similar studies (Alalshekmubarak & Smith, 2013; Tang, 2013), this proposal is
primarily intended for binary classification on intrusion detection using the
2013 network traffic data from the honeypot systems of Kyoto University.
Results show that the GRU-SVM model performs relatively higher than the
conventional GRU-Softmax model. The proposed model reached a training accuracy
of ~81.54% and a testing accuracy of ~84.15%, while the latter was able to
reach a training accuracy of ~63.07% and a testing accuracy of ~70.75%. In
addition, the juxtaposition of these two final output layers indicate that the
SVM would outperform Softmax in prediction time - a theoretical implication
which was supported by the actual training and testing time in the study.",['Abien Fred Agarap'],"['cs.NE', 'cs.CR', 'cs.LG', 'stat.ML']",2017-09-10 10:43:09+00:00
http://arxiv.org/abs/1709.03019v1,Classifying Unordered Feature Sets with Convolutional Deep Averaging Networks,"Unordered feature sets are a nonstandard data structure that traditional
neural networks are incapable of addressing in a principled manner. Providing a
concatenation of features in an arbitrary order may lead to the learning of
spurious patterns or biases that do not actually exist. Another complication is
introduced if the number of features varies between each set. We propose
convolutional deep averaging networks (CDANs) for classifying and learning
representations of datasets whose instances comprise variable-size, unordered
feature sets. CDANs are efficient, permutation-invariant, and capable of
accepting sets of arbitrary size. We emphasize the importance of nonlinear
feature embeddings for obtaining effective CDAN classifiers and illustrate
their advantages in experiments versus linear embeddings and alternative
permutation-invariant and -equivariant architectures.","['Andrew Gardner', 'Jinko Kanno', 'Christian A. Duncan', 'Rastko R. Selmic']","['cs.LG', 'stat.ML']",2017-09-10 00:03:37+00:00
http://arxiv.org/abs/1709.02956v1,Deep Residual Networks and Weight Initialization,"Residual Network (ResNet) is the state-of-the-art architecture that realizes
successful training of really deep neural network. It is also known that good
weight initialization of neural network avoids problem of vanishing/exploding
gradients. In this paper, simplified models of ResNets are analyzed. We argue
that goodness of ResNet is correlated with the fact that ResNets are relatively
insensitive to choice of initial weights. We also demonstrate how batch
normalization improves backpropagation of deep ResNets without tuning initial
values of weights.",['Masato Taki'],"['cs.LG', 'stat.ML']",2017-09-09 14:23:50+00:00
http://arxiv.org/abs/1709.02925v2,Less Is More: A Comprehensive Framework for the Number of Components of Ensemble Classifiers,"The number of component classifiers chosen for an ensemble greatly impacts
the prediction ability. In this paper, we use a geometric framework for a
priori determining the ensemble size, which is applicable to most of existing
batch and online ensemble classifiers. There are only a limited number of
studies on the ensemble size examining Majority Voting (MV) and Weighted
Majority Voting (WMV). Almost all of them are designed for batch-mode, hardly
addressing online environments. Big data dimensions and resource limitations,
in terms of time and memory, make determination of ensemble size crucial,
especially for online environments. For the MV aggregation rule, our framework
proves that the more strong components we add to the ensemble, the more
accurate predictions we can achieve. For the WMV aggregation rule, our
framework proves the existence of an ideal number of components, which is equal
to the number of class labels, with the premise that components are completely
independent of each other and strong enough. While giving the exact definition
for a strong and independent classifier in the context of an ensemble is a
challenging task, our proposed geometric framework provides a theoretical
explanation of diversity and its impact on the accuracy of predictions. We
conduct a series of experimental evaluations to show the practical value of our
theorems and existing challenges.","['Hamed Bonab', 'Fazli Can']","['cs.LG', 'stat.ML']",2017-09-09 07:52:58+00:00
http://arxiv.org/abs/1709.02909v1,A Simple Analysis for Exp-concave Empirical Minimization with Arbitrary Convex Regularizer,"In this paper, we present a simple analysis of {\bf fast rates} with {\it
high probability} of {\bf empirical minimization} for {\it stochastic composite
optimization} over a finite-dimensional bounded convex set with exponential
concave loss functions and an arbitrary convex regularization. To the best of
our knowledge, this result is the first of its kind. As a byproduct, we can
directly obtain the fast rate with {\it high probability} for exponential
concave empirical risk minimization with and without any convex regularization,
which not only extends existing results of empirical risk minimization but also
provides a unified framework for analyzing exponential concave empirical risk
minimization with and without {\it any} convex regularization. Our proof is
very simple only exploiting the covering number of a finite-dimensional bounded
set and a concentration inequality of random vectors.","['Tianbao Yang', 'Zhe Li', 'Lijun Zhang']","['stat.ML', 'cs.LG', 'math.OC']",2017-09-09 04:44:14+00:00
http://arxiv.org/abs/1709.02896v1,Simultaneously Learning Neighborship and Projection Matrix for Supervised Dimensionality Reduction,"Explicitly or implicitly, most of dimensionality reduction methods need to
determine which samples are neighbors and the similarity between the neighbors
in the original highdimensional space. The projection matrix is then learned on
the assumption that the neighborhood information (e.g., the similarity) is
known and fixed prior to learning. However, it is difficult to precisely
measure the intrinsic similarity of samples in high-dimensional space because
of the curse of dimensionality. Consequently, the neighbors selected according
to such similarity might and the projection matrix obtained according to such
similarity and neighbors are not optimal in the sense of classification and
generalization. To overcome the drawbacks, in this paper we propose to let the
similarity and neighbors be variables and model them in low-dimensional space.
Both the optimal similarity and projection matrix are obtained by minimizing a
unified objective function. Nonnegative and sum-to-one constraints on the
similarity are adopted. Instead of empirically setting the regularization
parameter, we treat it as a variable to be optimized. It is interesting that
the optimal regularization parameter is adaptive to the neighbors in
low-dimensional space and has intuitive meaning. Experimental results on the
YALE B, COIL-100, and MNIST datasets demonstrate the effectiveness of the
proposed method.","['Yanwei Pang', 'Bo Zhou', 'Feiping Nie']","['cs.CV', 'cs.LG', 'stat.ML']",2017-09-09 02:44:18+00:00
http://arxiv.org/abs/1709.02893v5,Convolutional Dictionary Learning: A Comparative Review and New Algorithms,"Convolutional sparse representations are a form of sparse representation with
a dictionary that has a structure that is equivalent to convolution with a set
of linear filters. While effective algorithms have recently been developed for
the convolutional sparse coding problem, the corresponding dictionary learning
problem is substantially more challenging. Furthermore, although a number of
different approaches have been proposed, the absence of thorough comparisons
between them makes it difficult to determine which of them represents the
current state of the art. The present work both addresses this deficiency and
proposes some new approaches that outperform existing ones in certain contexts.
A thorough set of performance comparisons indicates a very wide range of
performance differences among the existing and proposed methods, and clearly
identifies those that are the most effective.","['Cristina Garcia-Cardona', 'Brendt Wohlberg']","['cs.LG', 'eess.IV', 'stat.ML']",2017-09-09 01:45:43+00:00
http://arxiv.org/abs/1709.02855v1,Roll-back Hamiltonian Monte Carlo,"We propose a new framework for Hamiltonian Monte Carlo (HMC) on truncated
probability distributions with smooth underlying density functions. Traditional
HMC requires computing the gradient of potential function associated with the
target distribution, and therefore does not perform its full power on truncated
distributions due to lack of continuity and differentiability. In our
framework, we introduce a sharp sigmoid factor in the density function to
approximate the probability drop at the truncation boundary. The target
potential function is approximated by a new potential which smoothly extends to
the entire sample space. HMC is then performed on the approximate potential.
While our method is easy to implement and applies to a wide range of problems,
it also achieves comparable computational efficiency on various sampling tasks
compared to other baseline methods. RBHMC also gives rise to a new approach for
Bayesian inference on constrained spaces.","['Kexin Yi', 'Finale Doshi-Velez']",['stat.ML'],2017-09-08 20:47:08+00:00
http://arxiv.org/abs/1709.02840v3,A Brief Introduction to Machine Learning for Engineers,"This monograph aims at providing an introduction to key concepts, algorithms,
and theoretical results in machine learning. The treatment concentrates on
probabilistic models for supervised and unsupervised learning problems. It
introduces fundamental concepts and algorithms by building on first principles,
while also exposing the reader to more advanced topics with extensive pointers
to the literature, within a unified notation and mathematical framework. The
material is organized according to clearly defined categories, such as
discriminative and generative models, frequentist and Bayesian approaches,
exact and approximate inference, as well as directed and undirected models.
This monograph is meant as an entry point for researchers with a background in
probability and linear algebra.",['Osvaldo Simeone'],"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2017-09-08 19:21:26+00:00
http://arxiv.org/abs/1709.02739v1,Crowdsourcing Predictors of Residential Electric Energy Usage,"Crowdsourcing has been successfully applied in many domains including
astronomy, cryptography and biology. In order to test its potential for useful
application in a Smart Grid context, this paper investigates the extent to
which a crowd can contribute predictive hypotheses to a model of residential
electric energy consumption. In this experiment, the crowd generated hypotheses
about factors that make one home different from another in terms of monthly
energy usage. To implement this concept, we deployed a web-based system within
which 627 residential electricity customers posed 632 questions that they
thought predictive of energy usage. While this occurred, the same group
provided 110,573 answers to these questions as they accumulated. Thus users
both suggested the hypotheses that drive a predictive model and provided the
data upon which the model is built. We used the resulting question and answer
data to build a predictive model of monthly electric energy consumption, using
random forest regression. Because of the sparse nature of the answer data,
careful statistical work was needed to ensure that these models are valid. The
results indicate that the crowd can generate useful hypotheses, despite the
sparse nature of the dataset.","['Mark D. Wagy', 'Josh C. Bongard', 'James P. Bagrow', 'Paul D. H. Hines']","['cs.HC', 'physics.soc-ph', 'stat.ML']",2017-09-08 15:17:14+00:00
http://arxiv.org/abs/1709.02727v2,Machine learning modeling of superconducting critical temperature,"Superconductivity has been the focus of enormous research effort since its
discovery more than a century ago. Yet, some features of this unique phenomenon
remain poorly understood; prime among these is the connection between
superconductivity and chemical/structural properties of materials. To bridge
the gap, several machine learning schemes are developed herein to model the
critical temperatures ($T_{\mathrm{c}}$) of the 12,000+ known superconductors
available via the SuperCon database. Materials are first divided into two
classes based on their $T_{\mathrm{c}}$ values, above and below 10 K, and a
classification model predicting this label is trained. The model uses
coarse-grained features based only on the chemical compositions. It shows
strong predictive power, with out-of-sample accuracy of about 92%. Separate
regression models are developed to predict the values of $T_{\mathrm{c}}$ for
cuprate, iron-based, and ""low-$T_{\mathrm{c}}$"" compounds. These models also
demonstrate good performance, with learned predictors offering potential
insights into the mechanisms behind superconductivity in different families of
materials. To improve the accuracy and interpretability of these models, new
features are incorporated using materials data from the AFLOW Online
Repositories. Finally, the classification and regression models are combined
into a single integrated pipeline and employed to search the entire Inorganic
Crystallographic Structure Database (ICSD) for potential new superconductors.
We identify more than 30 non-cuprate and non-iron-based oxides as candidate
materials.","['Valentin Stanev', 'Corey Oses', 'A. Gilad Kusne', 'Efrain Rodriguez', 'Johnpierre Paglione', 'Stefano Curtarolo', 'Ichiro Takeuchi']","['cond-mat.supr-con', 'cond-mat.str-el', 'stat.ML']",2017-09-08 14:55:02+00:00
http://arxiv.org/abs/1709.02726v1,"A Modular Analysis of Adaptive (Non-)Convex Optimization: Optimism, Composite Objectives, and Variational Bounds","Recently, much work has been done on extending the scope of online learning
and incremental stochastic optimization algorithms. In this paper we contribute
to this effort in two ways: First, based on a new regret decomposition and a
generalization of Bregman divergences, we provide a self-contained, modular
analysis of the two workhorses of online learning: (general) adaptive versions
of Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms.
The analysis is done with extra care so as not to introduce assumptions not
needed in the proofs and allows to combine, in a straightforward way, different
algorithmic ideas (e.g., adaptivity, optimism, implicit updates) and learning
settings (e.g., strongly convex or composite objectives). This way we are able
to reprove, extend and refine a large body of the literature, while keeping the
proofs concise. The second contribution is a byproduct of this careful
analysis: We present algorithms with improved variational bounds for smooth,
composite objectives, including a new family of optimistic MD algorithms with
only one projection step per round. Furthermore, we provide a simple extension
of adaptive regret bounds to practically relevant non-convex problem settings
with essentially no extra effort.","['Pooria Joulani', 'András György', 'Csaba Szepesvári']","['cs.LG', 'math.OC', 'stat.ML']",2017-09-08 14:54:44+00:00
http://arxiv.org/abs/1709.02702v1,Entropic Determinants,"The ability of many powerful machine learning algorithms to deal with large
data sets without compromise is often hampered by computationally expensive
linear algebra tasks, of which calculating the log determinant is a canonical
example. In this paper we demonstrate the optimality of Maximum Entropy methods
in approximating such calculations. We prove the equivalence between mean value
constraints and sample expectations in the big data limit, that Covariance
matrix eigenvalue distributions can be completely defined by moment information
and that the reduction of the self entropy of a maximum entropy proposal
distribution, achieved by adding more moments reduces the KL divergence between
the proposal and true eigenvalue distribution. We empirically verify our
results on a variety of SparseSuite matrices and establish best practices.","['Diego Granziol', 'Stephen Roberts']",['stat.ML'],2017-09-08 13:41:26+00:00
http://arxiv.org/abs/1709.05206v1,LSTM Fully Convolutional Networks for Time Series Classification,"Fully convolutional neural networks (FCN) have been shown to achieve
state-of-the-art performance on the task of classifying time series sequences.
We propose the augmentation of fully convolutional networks with long short
term memory recurrent neural network (LSTM RNN) sub-modules for time series
classification. Our proposed models significantly enhance the performance of
fully convolutional networks with a nominal increase in model size and require
minimal preprocessing of the dataset. The proposed Long Short Term Memory Fully
Convolutional Network (LSTM-FCN) achieves state-of-the-art performance compared
to others. We also explore the usage of attention mechanism to improve time
series classification with the Attention Long Short Term Memory Fully
Convolutional Network (ALSTM-FCN). Utilization of the attention mechanism
allows one to visualize the decision process of the LSTM cell. Furthermore, we
propose fine-tuning as a method to enhance the performance of trained models.
An overall analysis of the performance of our model is provided and compared to
other techniques.","['Fazle Karim', 'Somshubra Majumdar', 'Houshang Darabi', 'Shun Chen']","['cs.LG', 'stat.ML']",2017-09-08 13:35:36+00:00
http://arxiv.org/abs/1709.02576v3,Deep learning for undersampled MRI reconstruction,"This paper presents a deep learning method for faster magnetic resonance
imaging (MRI) by reducing k-space data with sub-Nyquist sampling strategies and
provides a rationale for why the proposed approach works well. Uniform
subsampling is used in the time-consuming phase-encoding direction to capture
high-resolution image information, while permitting the image-folding problem
dictated by the Poisson summation formula. To deal with the localization
uncertainty due to image folding, very few low-frequency k-space data are
added. Training the deep learning net involves input and output images that are
pairs of Fourier transforms of the subsampled and fully sampled k-space data.
Numerous experiments show the remarkable performance of the proposed method;
only 29% of k-space data can generate images of high quality as effectively as
standard MRI reconstruction with fully sampled data.","['Chang Min Hyun', 'Hwa Pyung Kim', 'Sung Min Lee', 'Sungchul Lee', 'Jin Keun Seo']","['stat.ML', 'cs.LG', 'physics.med-ph']",2017-09-08 07:35:58+00:00
http://arxiv.org/abs/1709.02802v1,Towards Proving the Adversarial Robustness of Deep Neural Networks,"Autonomous vehicles are highly complex systems, required to function reliably
in a wide variety of situations. Manually crafting software controllers for
these vehicles is difficult, but there has been some success in using deep
neural networks generated using machine-learning. However, deep neural networks
are opaque to human engineers, rendering their correctness very difficult to
prove manually; and existing automated techniques, which were not designed to
operate on neural networks, fail to scale to large systems. This paper focuses
on proving the adversarial robustness of deep neural networks, i.e. proving
that small perturbations to a correctly-classified input to the network cannot
cause it to be misclassified. We describe some of our recent and ongoing work
on verifying the adversarial robustness of networks, and discuss some of the
open questions we have encountered and how they might be addressed.","['Guy Katz', 'Clark Barrett', 'David L. Dill', 'Kyle Julian', 'Mykel J. Kochenderfer']","['cs.LG', 'cs.CR', 'cs.LO', 'stat.ML', 'D.2.4; I.2.2']",2017-09-08 06:34:44+00:00
