id,title,abstract,authors,categories,date
http://arxiv.org/abs/1910.03134v4,Partial Separability and Functional Graphical Models for Multivariate Gaussian Processes,"The covariance structure of multivariate functional data can be highly
complex, especially if the multivariate dimension is large, making extensions
of statistical methods for standard multivariate data to the functional data
setting challenging. For example, Gaussian graphical models have recently been
extended to the setting of multivariate functional data by applying
multivariate methods to the coefficients of truncated basis expansions.
However, a key difficulty compared to multivariate data is that the covariance
operator is compact, and thus not invertible. The methodology in this paper
addresses the general problem of covariance modeling for multivariate
functional data, and functional Gaussian graphical models in particular. As a
first step, a new notion of separability for the covariance operator of
multivariate functional data is proposed, termed partial separability, leading
to a novel Karhunen-Lo\`eve-type expansion for such data. Next, the partial
separability structure is shown to be particularly useful in order to provide a
well-defined functional Gaussian graphical model that can be identified with a
sequence of finite-dimensional graphical models, each of identical fixed
dimension. This motivates a simple and efficient estimation procedure through
application of the joint graphical lasso. Empirical performance of the method
for graphical model estimation is assessed through simulation and analysis of
functional brain connectivity during a motor task. %Empirical performance of
the method for graphical model estimation is assessed through simulation and
analysis of functional brain connectivity during a motor task.","['Javier Zapata', 'Sang-Yun Oh', 'Alexander Petersen']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2019-10-07 23:42:21+00:00
http://arxiv.org/abs/1910.03131v2,Generating valid Euclidean distance matrices,"Generating point clouds, e.g., molecular structures, in arbitrary rotations,
translations, and enumerations remains a challenging task. Meanwhile, neural
networks utilizing symmetry invariant layers have been shown to be able to
optimize their training objective in a data-efficient way. In this spirit, we
present an architecture which allows to produce valid Euclidean distance
matrices, which by construction are already invariant under rotation and
translation of the described object. Motivated by the goal to generate
molecular structures in Cartesian space, we use this architecture to construct
a Wasserstein GAN utilizing a permutation invariant critic network. This makes
it possible to generate molecular structures in a one-shot fashion by producing
Euclidean distance matrices which have a three-dimensional embedding.","['Moritz Hoffmann', 'Frank No√©']","['cs.LG', 'stat.ML']",2019-10-07 23:30:05+00:00
http://arxiv.org/abs/1910.03127v1,Evaluating Scalable Uncertainty Estimation Methods for DNN-Based Molecular Property Prediction,"Advances in deep neural network (DNN) based molecular property prediction
have recently led to the development of models of remarkable accuracy and
generalization ability, with graph convolution neural networks (GCNNs)
reporting state-of-the-art performance for this task. However, some challenges
remain and one of the most important that needs to be fully addressed concerns
uncertainty quantification. DNN performance is affected by the volume and the
quality of the training samples. Therefore, establishing when and to what
extent a prediction can be considered reliable is just as important as
outputting accurate predictions, especially when out-of-domain molecules are
targeted. Recently, several methods to account for uncertainty in DNNs have
been proposed, most of which are based on approximate Bayesian inference. Among
these, only a few scale to the large datasets required in applications.
Evaluating and comparing these methods has recently attracted great interest,
but results are generally fragmented and absent for molecular property
prediction. In this paper, we aim to quantitatively compare scalable techniques
for uncertainty estimation in GCNNs. We introduce a set of quantitative
criteria to capture different uncertainty aspects, and then use these criteria
to compare MC-Dropout, deep ensembles, and bootstrapping, both theoretically in
a unified framework that separates aleatoric/epistemic uncertainty and
experimentally on the QM9 dataset. Our experiments quantify the performance of
the different uncertainty estimation methods and their impact on
uncertainty-related error reduction. Our findings indicate that ensembling and
bootstrapping consistently outperform MC-Dropout, with different
context-specific pros and cons. Our analysis also leads to a better
understanding of the role of aleatoric/epistemic uncertainty and highlights the
challenge posed by out-of-domain uncertainty.","['Gabriele Scalia', 'Colin A. Grambow', 'Barbara Pernici', 'Yi-Pei Li', 'William H. Green']","['cs.LG', 'stat.ML']",2019-10-07 23:04:43+00:00
http://arxiv.org/abs/1910.03103v3,Energy-Aware Neural Architecture Optimization with Fast Splitting Steepest Descent,"Designing energy-efficient networks is of critical importance for enabling
state-of-the-art deep learning in mobile and edge settings where the
computation and energy budgets are highly limited. Recently, Liu et al. (2019)
framed the search of efficient neural architectures into a continuous splitting
process: it iteratively splits existing neurons into multiple off-springs to
achieve progressive loss minimization, thus finding novel architectures by
gradually growing the neural network. However, this method was not specifically
tailored for designing energy-efficient networks, and is computationally
expensive on large-scale benchmarks. In this work, we substantially improve Liu
et al. (2019) in two significant ways: 1) we incorporate the energy cost of
splitting different neurons to better guide the splitting process, thereby
discovering more energy-efficient network architectures; 2) we substantially
speed up the splitting process of Liu et al. (2019), which requires expensive
eigen-decomposition, by proposing a highly scalable Rayleigh-quotient
stochastic gradient algorithm. Our fast algorithm allows us to reduce the
computational cost of splitting to the same level of typical back-propagation
updates and enables efficient implementation on GPU. Extensive empirical
results show that our method can train highly accurate and energy-efficient
networks on challenging datasets such as ImageNet, improving a variety of
baselines, including the pruning-based methods and expert-designed
architectures.","['Dilin Wang', 'Meng Li', 'Lemeng Wu', 'Vikas Chandra', 'Qiang Liu']","['cs.LG', 'stat.ML']",2019-10-07 21:45:17+00:00
http://arxiv.org/abs/1910.03094v3,Combining No-regret and Q-learning,"Counterfactual Regret Minimization (CFR) has found success in settings like
poker which have both terminal states and perfect recall. We seek to understand
how to relax these requirements. As a first step, we introduce a simple
algorithm, local no-regret learning (LONR), which uses a Q-learning-like update
rule to allow learning without terminal states or perfect recall. We prove its
convergence for the basic case of MDPs (and limited extensions of them) and
present empirical results showing that it achieves last iterate convergence in
a number of settings, most notably NoSDE games, a class of Markov games
specifically designed to be challenging to learn where no prior algorithm is
known to achieve convergence to a stationary equilibrium even on average.","['Ian A. Kash', 'Michael Sullins', 'Katja Hofmann']","['cs.LG', 'cs.AI', 'cs.GT', 'cs.MA', 'stat.ML']",2019-10-07 21:13:55+00:00
http://arxiv.org/abs/1910.03084v1,CeliacNet: Celiac Disease Severity Diagnosis on Duodenal Histopathological Images Using Deep Residual Networks,"Celiac Disease (CD) is a chronic autoimmune disease that affects the small
intestine in genetically predisposed children and adults. Gluten exposure
triggers an inflammatory cascade which leads to compromised intestinal barrier
function. If this enteropathy is unrecognized, this can lead to anemia,
decreased bone density, and, in longstanding cases, intestinal cancer. The
prevalence of the disorder is 1% in the United States. An intestinal (duodenal)
biopsy is considered the ""gold standard"" for diagnosis. The mild CD might go
unnoticed due to non-specific clinical symptoms or mild histologic features. In
our current work, we trained a model based on deep residual networks to
diagnose CD severity using a histological scoring system called the modified
Marsh score. The proposed model was evaluated using an independent set of 120
whole slide images from 15 CD patients and achieved an AUC greater than 0.96 in
all classes. These results demonstrate the diagnostic power of the proposed
model for CD severity classification using histological images.","['Rasoul Sali', 'Lubaina Ehsan', 'Kamran Kowsari', 'Marium Khan', 'Christopher A. Moskaluk', 'Sana Syed', 'Donald E. Brown']","['eess.IV', 'cs.CV', 'cs.LG', 'q-bio.QM', 'stat.ML']",2019-10-07 21:06:41+00:00
http://arxiv.org/abs/1910.03081v1,On the Interpretability and Evaluation of Graph Representation Learning,"With the rising interest in graph representation learning, a variety of
approaches have been proposed to effectively capture a graph's properties.
While these approaches have improved performance in graph machine learning
tasks compared to traditional graph techniques, they are still perceived as
techniques with limited insight into the information encoded in these
representations. In this work, we explore methods to interpret node embeddings
and propose the creation of a robust evaluation framework for comparing graph
representation learning algorithms and hyperparameters. We test our methods on
graphs with different properties and investigate the relationship between
embedding training parameters and the ability of the produced embedding to
recover the structure of the original graph in a downstream task.","['Antonia Gogoglou', 'C. Bayan Bruss', 'Keegan E. Hines']","['cs.LG', 'stat.ML']",2019-10-07 21:02:13+00:00
http://arxiv.org/abs/1910.03072v1,Sequence embeddings help to identify fraudulent cases in healthcare insurance,"Fraud causes substantial costs and losses for companies and clients in the
finance and insurance industries. Examples are fraudulent credit card
transactions or fraudulent claims. It has been estimated that roughly $10$
percent of the insurance industry's incurred losses and loss adjustment
expenses each year stem from fraudulent claims. The rise and proliferation of
digitization in finance and insurance have lead to big data sets, consisting in
particular of text data, which can be used for fraud detection. In this paper,
we propose architectures for text embeddings via deep learning, which help to
improve the detection of fraudulent claims compared to other machine learning
methods. We illustrate our methods using a data set from a large international
health insurance company. The empirical results show that our approach
outperforms other state-of-the-art methods and can help make the claims
management process more efficient. As (unstructured) text data become
increasingly available to economists and econometricians, our proposed methods
will be valuable for many similar applications, particularly when variables
have a large number of categories as is typical for example of the
International Classification of Disease (ICD) codes in health economics and
health services.","['I. Fursov', 'A. Zaytsev', 'R. Khasyanov', 'M. Spindler', 'E. Burnaev']","['cs.LG', 'cs.CR', 'stat.ML']",2019-10-07 20:37:02+00:00
http://arxiv.org/abs/1910.03053v3,Graph Few-shot Learning via Knowledge Transfer,"Towards the challenging problem of semi-supervised node classification, there
have been extensive studies. As a frontier, Graph Neural Networks (GNNs) have
aroused great interest recently, which update the representation of each node
by aggregating information of its neighbors. However, most GNNs have shallow
layers with a limited receptive field and may not achieve satisfactory
performance especially when the number of labeled nodes is quite small. To
address this challenge, we innovatively propose a graph few-shot learning (GFL)
algorithm that incorporates prior knowledge learned from auxiliary graphs to
improve classification accuracy on the target graph. Specifically, a
transferable metric space characterized by a node embedding and a
graph-specific prototype embedding function is shared between auxiliary graphs
and the target, facilitating the transfer of structural knowledge. Extensive
experiments and ablation studies on four real-world graph datasets demonstrate
the effectiveness of our proposed model.","['Huaxiu Yao', 'Chuxu Zhang', 'Ying Wei', 'Meng Jiang', 'Suhang Wang', 'Junzhou Huang', 'Nitesh V. Chawla', 'Zhenhui Li']","['cs.LG', 'stat.ML']",2019-10-07 19:52:11+00:00
http://arxiv.org/abs/1910.05270v3,Fast and Bayes-consistent nearest neighbors,"Research on nearest-neighbor methods tends to focus somewhat dichotomously
either on the statistical or the computational aspects -- either on, say, Bayes
consistency and rates of convergence or on techniques for speeding up the
proximity search. This paper aims at bridging these realms: to reap the
advantages of fast evaluation time while maintaining Bayes consistency, and
further without sacrificing too much in the risk decay rate. We combine the
locality-sensitive hashing (LSH) technique with a novel missing-mass argument
to obtain a fast and Bayes-consistent classifier. Our algorithm's prediction
runtime compares favorably against state of the art approximate NN methods,
while maintaining Bayes-consistency and attaining rates comparable to minimax.
On samples of size $n$ in $\R^d$, our pre-processing phase has runtime $O(d n
\log n)$, while the evaluation phase has runtime $O(d\log n)$ per query point.","['Klim Efremenko', 'Aryeh Kontorovich', 'Moshe Noivirt']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2019-10-07 19:46:37+00:00
http://arxiv.org/abs/1910.03016v4,Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?,"Modern deep learning methods provide effective means to learn good
representations. However, is a good representation itself sufficient for sample
efficient reinforcement learning? This question has largely been studied only
with respect to (worst-case) approximation error, in the more classical
approximate dynamic programming literature. With regards to the statistical
viewpoint, this question is largely unexplored, and the extant body of
literature mainly focuses on conditions which permit sample efficient
reinforcement learning with little understanding of what are necessary
conditions for efficient reinforcement learning.
  This work shows that, from the statistical viewpoint, the situation is far
subtler than suggested by the more traditional approximation viewpoint, where
the requirements on the representation that suffice for sample efficient RL are
even more stringent. Our main results provide sharp thresholds for
reinforcement learning methods, showing that there are hard limitations on what
constitutes good function approximation (in terms of the dimensionality of the
representation), where we focus on natural representational conditions relevant
to value-based, model-based, and policy-based learning. These lower bounds
highlight that having a good (value-based, model-based, or policy-based)
representation in and of itself is insufficient for efficient reinforcement
learning, unless the quality of this approximation passes certain hard
thresholds. Furthermore, our lower bounds also imply exponential separations on
the sample complexity between 1) value-based learning with perfect
representation and value-based learning with a good-but-not-perfect
representation, 2) value-based learning and policy-based learning, 3)
policy-based learning and supervised learning and 4) reinforcement learning and
imitation learning.","['Simon S. Du', 'Sham M. Kakade', 'Ruosong Wang', 'Lin F. Yang']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2019-10-07 19:04:43+00:00
http://arxiv.org/abs/1910.03003v2,Stochastic Optimal Control as Approximate Input Inference,"Optimal control of stochastic nonlinear dynamical systems is a major
challenge in the domain of robot learning. Given the intractability of the
global control problem, state-of-the-art algorithms focus on approximate
sequential optimization techniques, that heavily rely on heuristics for
regularization in order to achieve stable convergence. By building upon the
duality between inference and control, we develop the view of Optimal Control
as Input Estimation, devising a probabilistic stochastic optimal control
formulation that iteratively infers the optimal input distributions by
minimizing an upper bound of the control cost. Inference is performed through
Expectation Maximization and message passing on a probabilistic graphical model
of the dynamical system, and time-varying linear Gaussian feedback controllers
are extracted from the joint state-action distribution. This perspective
incorporates uncertainty quantification, effective initialization through
priors, and the principled regularization inherent to the Bayesian treatment.
Moreover, it can be shown that for deterministic linearized systems, our
framework derives the maximum entropy linear quadratic optimal control law. We
provide a complete and detailed derivation of our probabilistic approach and
highlight its advantages in comparison to other deterministic and probabilistic
solvers.","['Joe Watson', 'Hany Abdulsamad', 'Jan Peters']","['cs.LG', 'cs.RO', 'cs.SY', 'eess.SY', 'stat.ML']",2019-10-07 18:41:52+00:00
http://arxiv.org/abs/1910.03002v2,High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes,"Predicting the dependencies between observations from multiple time series is
critical for applications such as anomaly detection, financial risk management,
causal analysis, or demand forecasting. However, the computational and
numerical difficulties of estimating time-varying and high-dimensional
covariance matrices often limits existing methods to handling at most a few
hundred dimensions or requires making strong assumptions on the dependence
between series. We propose to combine an RNN-based time series model with a
Gaussian copula process output model with a low-rank covariance structure to
reduce the computational complexity and handle non-Gaussian marginal
distributions. This permits to drastically reduce the number of parameters and
consequently allows the modeling of time-varying correlations of thousands of
time series. We show on several real-world datasets that our method provides
significant accuracy improvements over state-of-the-art baselines and perform
an ablation study analyzing the contributions of the different components of
our model.","['David Salinas', 'Michael Bohlke-Schneider', 'Laurent Callot', 'Roberto Medico', 'Jan Gasthaus']","['cs.LG', 'stat.ML']",2019-10-07 18:41:00+00:00
http://arxiv.org/abs/1910.02935v1,Automated Enriched Medical Concept Generation for Chest X-ray Images,"Decision support tools that rely on supervised learning require large amounts
of expert annotations. Using past radiological reports obtained from hospital
archiving systems has many advantages as training data above manual
single-class labels: they are expert annotations available in large quantities,
covering a population-representative variety of pathologies, and they provide
additional context to pathology diagnoses, such as anatomical location and
severity. Learning to auto-generate such reports from images present many
challenges such as the difficulty in representing and generating long,
unstructured textual information, accounting for spelling errors and
repetition/redundancy, and the inconsistency across different annotators. We
therefore propose to first learn visually-informative medical concepts from raw
reports, and, using the concept predictions as image annotations, learn to
auto-generate structured reports directly from images. We validate our approach
on the OpenI [2] chest x-ray dataset, which consists of frontal and lateral
views of chest x-ray images, their corresponding raw textual reports and manual
medical subject heading (MeSH ) annotations made by radiologists.",['Aydan Gasimova'],"['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']",2019-10-07 17:52:37+00:00
http://arxiv.org/abs/1910.02934v1,Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks,"The skip-connections used in residual networks have become a standard
architecture choice in deep learning due to the increased training stability
and generalization performance with this architecture, although there has been
limited theoretical understanding for this improvement. In this work, we
analyze overparameterized deep residual networks trained by gradient descent
following random initialization, and demonstrate that (i) the class of networks
learned by gradient descent constitutes a small subset of the entire neural
network function class, and (ii) this subclass of networks is sufficiently
large to guarantee small training error. By showing (i) we are able to
demonstrate that deep residual networks trained with gradient descent have a
small generalization gap between training and test error, and together with
(ii) this guarantees that the test error will be small. Our optimization and
generalization guarantees require overparameterization that is only logarithmic
in the depth of the network, while all known generalization bounds for deep
non-residual networks have overparameterization requirements that are at least
polynomial in the depth. This provides an explanation for why residual networks
are preferable to non-residual ones.","['Spencer Frei', 'Yuan Cao', 'Quanquan Gu']","['cs.LG', 'math.OC', 'stat.ML']",2019-10-07 17:52:20+00:00
http://arxiv.org/abs/1910.02919v3,Multi-step Greedy Reinforcement Learning Algorithms,"Multi-step greedy policies have been extensively used in model-based
reinforcement learning (RL), both when a model of the environment is available
(e.g.,~in the game of Go) and when it is learned. In this paper, we explore
their benefits in model-free RL, when employed using multi-step dynamic
programming algorithms: $\kappa$-Policy Iteration ($\kappa$-PI) and
$\kappa$-Value Iteration ($\kappa$-VI). These methods iteratively compute the
next policy ($\kappa$-PI) and value function ($\kappa$-VI) by solving a
surrogate decision problem with a shaped reward and a smaller discount factor.
We derive model-free RL algorithms based on $\kappa$-PI and $\kappa$-VI in
which the surrogate problem can be solved by any discrete or continuous action
RL method, such as DQN and TRPO. We identify the importance of a
hyper-parameter that controls the extent to which the surrogate problem is
solved and suggest a way to set this parameter. When evaluated on a range of
Atari and MuJoCo benchmark tasks, our results indicate that for the right range
of $\kappa$, our algorithms outperform DQN and TRPO. This shows that our
multi-step greedy algorithms are general enough to be applied over any existing
RL algorithm and can significantly improve its performance.","['Manan Tomar', 'Yonathan Efroni', 'Mohammad Ghavamzadeh']","['cs.LG', 'stat.ML']",2019-10-07 17:20:25+00:00
http://arxiv.org/abs/1910.02912v1,Increasing Expressivity of a Hyperspherical VAE,"Learning suitable latent representations for observed, high-dimensional data
is an important research topic underlying many recent advances in machine
learning. While traditionally the Gaussian normal distribution has been the
go-to latent parameterization, recently a variety of works have successfully
proposed the use of manifold-valued latents. In one such work (Davidson et al.,
2018), the authors empirically show the potential benefits of using a
hyperspherical von Mises-Fisher (vMF) distribution in low dimensionality.
However, due to the unique distributional form of the vMF, expressivity in
higher dimensional space is limited as a result of its scalar concentration
parameter leading to a 'hyperspherical bottleneck'. In this work we propose to
extend the usability of hyperspherical parameterizations to higher dimensions
using a product-space instead, showing improved results on a selection of image
datasets.","['Tim R. Davidson', 'Jakub M. Tomczak', 'Efstratios Gavves']","['stat.ML', 'cs.LG']",2019-10-07 16:59:59+00:00
http://arxiv.org/abs/1910.02876v2,Reinforcement Learning with Structured Hierarchical Grammar Representations of Actions,"From a young age humans learn to use grammatical principles to hierarchically
combine words into sentences. Action grammars is the parallel idea, that there
is an underlying set of rules (a ""grammar"") that govern how we hierarchically
combine actions to form new, more complex actions. We introduce the Action
Grammar Reinforcement Learning (AG-RL) framework which leverages the concept of
action grammars to consistently improve the sample efficiency of Reinforcement
Learning agents. AG-RL works by using a grammar inference algorithm to infer
the ""action grammar"" of an agent midway through training. The agent's action
space is then augmented with macro-actions identified by the grammar. We apply
this framework to Double Deep Q-Learning (AG-DDQN) and a discrete action
version of Soft Actor-Critic (AG-SAC) and find that it improves performance in
8 out of 8 tested Atari games (median +31%, max +668%) and 19 out of 20 tested
Atari games (median +96%, maximum +3,756%) respectively without substantive
hyperparameter tuning. We also show that AG-SAC beats the model-free
state-of-the-art for sample efficiency in 17 out of the 20 tested Atari games
(median +62%, maximum +13,140%), again without substantive hyperparameter
tuning.","['Petros Christodoulou', 'Robert Tjarko Lange', 'Ali Shafti', 'A. Aldo Faisal']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-07 15:59:20+00:00
http://arxiv.org/abs/1910.02845v1,Combining docking pose rank and structure with deep learning improves protein-ligand binding mode prediction,"We present a simple, modular graph-based convolutional neural network that
takes structural information from protein-ligand complexes as input to generate
models for activity and binding mode prediction. Complex structures are
generated by a standard docking procedure and fed into a dual-graph
architecture that includes separate sub-networks for the ligand bonded topology
and the ligand-protein contact map. This network division allows contributions
from ligand identity to be distinguished from effects of protein-ligand
interactions on classification. We show, in agreement with recent literature,
that dataset bias drives many of the promising results on virtual screening
that have previously been reported. However, we also show that our neural
network is capable of learning from protein structural information when, as in
the case of binding mode prediction, an unbiased dataset is constructed. We
develop a deep learning model for binding mode prediction that uses docking
ranking as input in combination with docking structures. This strategy mirrors
past consensus models and outperforms the baseline docking program in a variety
of tests, including on cross-docking datasets that mimic real-world docking use
cases. Furthermore, the magnitudes of network predictions serve as reliable
measures of model confidence","['Joseph A. Morrone', 'Jeffrey K. Weber', 'Tien Huynh', 'Heng Luo', 'Wendy D. Cornell']","['q-bio.BM', 'physics.bio-ph', 'stat.ML']",2019-10-07 15:12:32+00:00
http://arxiv.org/abs/1910.02835v1,A Learnable Safety Measure,"Failures are challenging for learning to control physical systems since they
risk damage, time-consuming resets, and often provide little gradient
information. Adding safety constraints to exploration typically requires a lot
of prior knowledge and domain expertise. We present a safety measure which
implicitly captures how the system dynamics relate to a set of failure states.
Not only can this measure be used as a safety function, but also to directly
compute the set of safe state-action pairs. Further, we show a model-free
approach to learn this measure by active sampling using Gaussian processes.
While safety can only be guaranteed after learning the safety measure, we show
that failures can already be greatly reduced by using the estimated measure
during learning.","['Steve Heim', 'Alexander von Rohr', 'Sebastian Trimpe', 'Alexander Badri-Spr√∂witz']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2019-10-07 14:53:15+00:00
http://arxiv.org/abs/1910.04076v4,FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation using Monocular Fisheye Camera for Autonomous Driving,"Fisheye cameras are commonly used in applications like autonomous driving and
surveillance to provide a large field of view ($>180^{\circ}$). However, they
come at the cost of strong non-linear distortions which require more complex
algorithms. In this paper, we explore Euclidean distance estimation on fisheye
cameras for automotive scenes. Obtaining accurate and dense depth supervision
is difficult in practice, but self-supervised learning approaches show
promising results and could potentially overcome the problem. We present a
novel self-supervised scale-aware framework for learning Euclidean distance and
ego-motion from raw monocular fisheye videos without applying rectification.
While it is possible to perform piece-wise linear approximation of fisheye
projection surface and apply standard rectilinear models, it has its own set of
issues like re-sampling distortion and discontinuities in transition regions.
To encourage further research in this area, we will release our dataset as part
of the WoodScape project \cite{yogamani2019woodscape}. We further evaluated the
proposed algorithm on the KITTI dataset and obtained state-of-the-art results
comparable to other self-supervised monocular methods. Qualitative results on
an unseen fisheye video demonstrate impressive performance
https://youtu.be/Sgq1WzoOmXg.","['Varun Ravi Kumar', 'Sandesh Athni Hiremath', 'Stefan Milz', 'Christian Witt', 'Clement Pinnard', 'Senthil Yogamani', 'Patrick Mader']","['cs.CV', 'cs.LG', 'cs.RO', 'eess.IV', 'stat.ML']",2019-10-07 14:51:38+00:00
http://arxiv.org/abs/1910.02830v1,Open Set Medical Diagnosis,"Machine-learned diagnosis models have shown promise as medical aides but are
trained under a closed-set assumption, i.e. that models will only encounter
conditions on which they have been trained. However, it is practically
infeasible to obtain sufficient training data for every human condition, and
once deployed such models will invariably face previously unseen conditions. We
frame machine-learned diagnosis as an open-set learning problem, and study how
state-of-the-art approaches compare. Further, we extend our study to a setting
where training data is distributed across several healthcare sites that do not
allow data pooling, and experiment with different strategies of building
open-set diagnostic ensembles. Across both settings, we observe consistent
gains from explicitly modeling unseen conditions, but find the optimal training
strategy to vary across settings.","['Viraj Prabhu', 'Anitha Kannan', 'Geoffrey J. Tso', 'Namit Katariya', 'Manish Chablani', 'David Sontag', 'Xavier Amatriain']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-07 14:45:47+00:00
http://arxiv.org/abs/1910.02826v1,Self-Paced Contextual Reinforcement Learning,"Generalization and adaptation of learned skills to novel situations is a core
requirement for intelligent autonomous robots. Although contextual
reinforcement learning provides a principled framework for learning and
generalization of behaviors across related tasks, it generally relies on
uninformed sampling of environments from an unknown, uncontrolled context
distribution, thus missing the benefits of structured, sequential learning. We
introduce a novel relative entropy reinforcement learning algorithm that gives
the agent the freedom to control the intermediate task distribution, allowing
for its gradual progression towards the target context distribution. Empirical
evaluation shows that the proposed curriculum learning scheme drastically
improves sample efficiency and enables learning in scenarios with both broad
and sharp target context distributions in which classical approaches perform
sub-optimally.","['Pascal Klink', 'Hany Abdulsamad', 'Boris Belousov', 'Jan Peters']","['cs.LG', 'stat.ML']",2019-10-07 14:40:53+00:00
http://arxiv.org/abs/1910.02822v2,A mathematical theory of cooperative communication,"Cooperative communication plays a central role in theories of human
cognition, language, development, culture, and human-robot interaction. Prior
models of cooperative communication are algorithmic in nature and do not shed
light on why cooperation may yield effective belief transmission and what
limitations may arise due to differences between beliefs of agents. Through a
connection to the theory of optimal transport, we establishing a mathematical
framework for cooperative communication. We derive prior models as special
cases, statistical interpretations of belief transfer plans, and proofs of
robustness and instability. Computational simulations support and elaborate our
theoretical results, and demonstrate fit to human behavior. The results show
that cooperative communication provably enables effective, robust belief
transmission which is required to explain feats of human learning and improve
human-machine interaction.","['Pei Wang', 'Junqi Wang', 'Pushpi Paranamana', 'Patrick Shafto']","['cs.LG', 'cs.MA', 'stat.ML']",2019-10-07 14:35:22+00:00
http://arxiv.org/abs/1910.02806v3,Learning De-biased Representations with Biased Representations,"Many machine learning algorithms are trained and evaluated by splitting data
from a single source into training and test sets. While such focus on
in-distribution learning scenarios has led to interesting advancement, it has
not been able to tell if models are relying on dataset biases as shortcuts for
successful prediction (e.g., using snow cues for recognising snowmobiles),
resulting in biased models that fail to generalise when the bias shifts to a
different class. The cross-bias generalisation problem has been addressed by
de-biasing training data through augmentation or re-sampling, which are often
prohibitive due to the data collection cost (e.g., collecting images of a
snowmobile on a desert) and the difficulty of quantifying or expressing biases
in the first place. In this work, we propose a novel framework to train a
de-biased representation by encouraging it to be different from a set of
representations that are biased by design. This tactic is feasible in many
scenarios where it is much easier to define a set of biased representations
than to define and quantify bias. We demonstrate the efficacy of our method
across a variety of synthetic and real-world biases; our experiments show that
the method discourages models from taking bias shortcuts, resulting in improved
generalisation. Source code is available at https://github.com/clovaai/rebias.","['Hyojin Bahng', 'Sanghyuk Chun', 'Sangdoo Yun', 'Jaegul Choo', 'Seong Joon Oh']","['cs.CV', 'cs.LG', 'stat.ML']",2019-10-07 14:11:13+00:00
http://arxiv.org/abs/1910.02804v1,Semantic Preserving Generative Adversarial Models,"We introduce generative adversarial models in which the discriminator is
replaced by a calibrated (non-differentiable) classifier repeatedly enhanced by
domain relevant features. The role of the classifier is to prove that the
actual and generated data differ over a controlled semantic space. We
demonstrate that such models have the ability to generate objects with strong
guarantees on their properties in a wide range of domains. They require less
data than ordinary GANs, provide natural stopping conditions, uncover important
properties of the data, and enhance transfer learning. Our techniques can be
combined with standard generative models. We demonstrate the usefulness of our
approach by applying it to several unrelated domains: generating good locations
for cellular antennae, molecule generation preserving key chemical properties,
and generating and extrapolating lines from very few data points. Intriguing
open problems are presented as well.","['Shahar Harel', 'Meir Maor', 'Amir Ronen']","['cs.LG', 'stat.ML']",2019-10-07 14:06:38+00:00
http://arxiv.org/abs/1910.02951v1,Joint analysis of clinical risk factors and 4D cardiac motion for survival prediction using a hybrid deep learning network,"In this work, a novel approach is proposed for joint analysis of high
dimensional time-resolved cardiac motion features obtained from segmented
cardiac MRI and low dimensional clinical risk factors to improve survival
prediction in heart failure. Different methods are evaluated to find the
optimal way to insert conventional covariates into deep prediction networks.
Correlation analysis between autoencoder latent codes and covariate features is
used to examine how these predictors interact. We believe that similar
approaches could also be used to introduce knowledge of genetic variants to
such survival networks to improve outcome prediction by jointly analysing
cardiac motion traits with inheritable risk factors.","['Shihao Jin', 'Nicol√≤ Savioli', 'Antonio de Marvao', 'Timothy JW Dawes', 'Axel Gandy', 'Daniel Rueckert', ""Declan P O'Regan""]","['q-bio.QM', 'cs.LG', 'eess.IV', 'stat.ML']",2019-10-07 14:04:17+00:00
http://arxiv.org/abs/1910.02776v1,Biologically-Inspired Spatial Neural Networks,"We introduce bio-inspired artificial neural networks consisting of neurons
that are additionally characterized by spatial positions. To simulate
properties of biological systems we add the costs penalizing long connections
and the proximity of neurons in a two-dimensional space. Our experiments show
that in the case where the network performs two different tasks, the neurons
naturally split into clusters, where each cluster is responsible for processing
a different task. This behavior not only corresponds to the biological systems,
but also allows for further insight into interpretability or continual
learning.","['Maciej Wo≈Çczyk', 'Jacek Tabor', 'Marek ≈ömieja', 'Szymon Maszke']","['cs.NE', 'cs.LG', 'stat.ML']",2019-10-07 13:22:13+00:00
http://arxiv.org/abs/1910.02760v3,Negative Sampling in Variational Autoencoders,"Modern deep artificial neural networks have achieved great success in the
domain of computer vision and beyond. However, their application to many
real-world tasks is undermined by certain limitations, such as overconfident
uncertainty estimates on out-of-distribution data or performance deterioration
under data distribution shifts. Several types of deep learning models used for
density estimation through probabilistic generative modeling have been shown to
fail to detect out-of-distribution samples by assigning higher likelihoods to
anomalous data. We investigate this failure mode in Variational Autoencoder
models, which are also prone to this, and improve upon the out-of-distribution
generalization performance of the model by employing an alternative training
scheme utilizing negative samples. We present a fully unsupervised version:
when the model is trained in an adversarial manner, the generator's own outputs
can be used as negative samples. We demonstrate empirically the effectiveness
of the approach in reducing the overconfident likelihood estimates of
out-of-distribution inputs on image data.","['Adri√°n Csisz√°rik', 'Beatrix Benk≈ë', 'D√°niel Varga']","['cs.LG', 'stat.ML']",2019-10-07 12:57:45+00:00
http://arxiv.org/abs/1910.02758v2,Algorithmic Probability-guided Supervised Machine Learning on Non-differentiable Spaces,"We show how complexity theory can be introduced in machine learning to help
bring together apparently disparate areas of current research. We show that
this new approach requires less training data and is more generalizable as it
shows greater resilience to random attacks. We investigate the shape of the
discrete algorithmic space when performing regression or classification using a
loss function parametrized by algorithmic complexity, demonstrating that the
property of differentiation is not necessary to achieve results similar to
those obtained using differentiable programming approaches such as deep
learning. In doing so we use examples which enable the two approaches to be
compared (small, given the computational power required for estimations of
algorithmic complexity). We find and report that (i) machine learning can
successfully be performed on a non-smooth surface using algorithmic complexity;
(ii) that parameter solutions can be found using an algorithmic-probability
classifier, establishing a bridge between a fundamentally discrete theory of
computability and a fundamentally continuous mathematical theory of
optimization methods; (iii) a formulation of an algorithmically directed search
technique in non-smooth manifolds can be defined and conducted; (iv)
exploitation techniques and numerical methods for algorithmic search to
navigate these discrete non-differentiable spaces can be performed; in
application of the (a) identification of generative rules from data
observations; (b) solutions to image classification problems more resilient
against pixel attacks compared to neural networks; (c) identification of
equation parameters from a small data-set in the presence of noise in
continuous ODE system problem, (d) classification of Boolean NK networks by (1)
network topology, (2) underlying Boolean function, and (3) number of incoming
edges.","['Santiago Hern√°ndez-Orozco', 'Hector Zenil', 'J√ºrgen Riedel', 'Adam Uccello', 'Narsis A. Kiani', 'Jesper Tegn√©r']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-07 12:48:58+00:00
http://arxiv.org/abs/1910.02757v4,Stochastic Bandits with Delay-Dependent Payoffs,"Motivated by recommendation problems in music streaming platforms, we propose
a nonstationary stochastic bandit model in which the expected reward of an arm
depends on the number of rounds that have passed since the arm was last pulled.
After proving that finding an optimal policy is NP-hard even when all model
parameters are known, we introduce a class of ranking policies provably
approximating, to within a constant factor, the expected reward of the optimal
policy. We show an algorithm whose regret with respect to the best ranking
policy is bounded by $\widetilde{\mathcal{O}}\big(\!\sqrt{kT}\big)$, where $k$
is the number of arms and $T$ is time. Our algorithm uses only
$\mathcal{O}\big(k\ln\ln T\big)$ switches, which helps when switching between
policies is costly. As constructing the class of learning policies requires
ordering the arms according to their expectations, we also bound the number of
pulls required to do so. Finally, we run experiments to compare our algorithm
against UCB on different problem instances.","['Leonardo Cella', 'Nicol√≤ Cesa-Bianchi']","['stat.ML', 'cs.LG']",2019-10-07 12:48:39+00:00
http://arxiv.org/abs/1910.02743v3,Neural network integral representations with the ReLU activation function,"In this effort, we derive a formula for the integral representation of a
shallow neural network with the ReLU activation function. We assume that the
outer weighs admit a finite $L_1$-norm with respect to Lebesgue measure on the
sphere. For univariate target functions we further provide a closed-form
formula for all possible representations. Additionally, in this case our
formula allows one to explicitly solve the least $L_1$-norm neural network
representation for a given function.","['Armenak Petrosyan', 'Anton Dereventsov', 'Clayton Webster']","['cs.LG', 'stat.ML']",2019-10-07 12:00:37+00:00
http://arxiv.org/abs/1910.02720v2,Meta-Learning Deep Energy-Based Memory Models,"We study the problem of learning associative memory -- a system which is able
to retrieve a remembered pattern based on its distorted or incomplete version.
Attractor networks provide a sound model of associative memory: patterns are
stored as attractors of the network dynamics and associative retrieval is
performed by running the dynamics starting from a query pattern until it
converges to an attractor. In such models the dynamics are often implemented as
an optimization procedure that minimizes an energy function, such as in the
classical Hopfield network. In general it is difficult to derive a writing rule
for a given dynamics and energy that is both compressive and fast. Thus, most
research in energy-based memory has been limited either to tractable energy
models not expressive enough to handle complex high-dimensional objects such as
natural images, or to models that do not offer fast writing. We present a novel
meta-learning approach to energy-based memory models (EBMM) that allows one to
use an arbitrary neural architecture as an energy model and quickly store
patterns in its weights. We demonstrate experimentally that our EBMM approach
can build compressed memories for synthetic and natural data, and is capable of
associative retrieval that outperforms existing memory systems in terms of the
reconstruction error and compression rate.","['Sergey Bartunov', 'Jack W Rae', 'Simon Osindero', 'Timothy P Lillicrap']","['stat.ML', 'cs.LG', 'cs.NE']",2019-10-07 10:58:08+00:00
http://arxiv.org/abs/1910.02718v2,Continual Learning in Neural Networks,"Artificial neural networks have exceeded human-level performance in
accomplishing several individual tasks (e.g. voice recognition, object
recognition, and video games). However, such success remains modest compared to
human intelligence that can learn and perform an unlimited number of tasks.
Humans' ability of learning and accumulating knowledge over their lifetime is
an essential aspect of their intelligence. Continual machine learning aims at a
higher level of machine intelligence through providing the artificial agents
with the ability to learn online from a non-stationary and never-ending stream
of data. A key component of such a never-ending learning process is to overcome
the catastrophic forgetting of previously seen data, a problem that neural
networks are well known to suffer from. The work described in this thesis has
been dedicated to the investigation of continual learning and solutions to
mitigate the forgetting phenomena in neural networks. To approach the continual
learning problem, we first assume a task incremental setting where tasks are
received one at a time and data from previous tasks are not stored. Since the
task incremental setting can't be assumed in all continual learning scenarios,
we also study the more general online continual setting. We consider an
infinite stream of data drawn from a non-stationary distribution with a
supervisory or self-supervisory training signal. The proposed methods in this
thesis have tackled important aspects of continual learning. They were
evaluated on different benchmarks and over various learning sequences. Advances
in the state of the art of continual learning have been shown and challenges
for bringing continual learning into application were critically identified.",['Rahaf Aljundi'],"['cs.LG', 'cs.CV', 'stat.ML']",2019-10-07 10:52:14+00:00
http://arxiv.org/abs/1910.02717v2,Brain MRI Tumor Segmentation with Adversarial Networks,"Deep Learning is a promising approach to either automate or simplify several
tasks in the healthcare domain. In this work, we introduce SegAN-CAT, an
approach to brain tumor segmentation in Magnetic Resonance Images (MRI), based
on Adversarial Networks. In particular, we extend SegAN, successfully applied
to the same task in a previous work, in two respects: (i) we used a different
model input and (ii) we employed a modified loss function to train the model.
We tested our approach on two large datasets, made available by the Brain Tumor
Image Segmentation Benchmark (BraTS). First, we trained and tested some
segmentation models assuming the availability of all the major MRI contrast
modalities, i.e., T1-weighted, T1 weighted contrast-enhanced, T2-weighted, and
T2-FLAIR. However, as these four modalities are not always all available for
each patient, we also trained and tested four segmentation models that take as
input MRIs acquired only with a single contrast modality. Finally, we proposed
to apply transfer learning across different contrast modalities to improve the
performance of these single-modality models. Our results are promising and show
that not SegAN-CAT is able to outperform SegAN when all the four modalities are
available, but also that transfer learning can actually lead to better
performances when only a single modality is available.","['Edoardo Giacomello', 'Daniele Loiacono', 'Luca Mainardi']","['eess.IV', 'cs.LG', 'stat.ML']",2019-10-07 10:51:56+00:00
http://arxiv.org/abs/1910.02686v1,Irregular Convolutional Auto-Encoder on Point Clouds,"We proposed a novel graph convolutional neural network that could construct a
coarse, sparse latent point cloud from a dense, raw point cloud. With a novel
non-isotropic convolution operation defined on irregular geometries, the model
then can reconstruct the original point cloud from this latent cloud with fine
details. Furthermore, we proposed that it is even possible to perform particle
simulation using the latent cloud encoded from some simulated particle cloud
(e.g. fluids), to accelerate the particle simulation process. Our model has
been tested on ShapeNetCore dataset for Auto-Encoding with a limited latent
dimension and tested on a synthesis dataset for fluids simulation. We also
compare the model with other state-of-the-art models, and several
visualizations were done to intuitively understand the model.","['Zhang Yuhui', 'Greg Gutmann', 'Konagaya Akihiko']","['cs.LG', 'stat.ML']",2019-10-07 09:24:08+00:00
http://arxiv.org/abs/1910.02684v4,Effective Stabilized Self-Training on Few-Labeled Graph Data,"Graph neural networks (GNNs) are designed for semi-supervised node
classification on graphs where only a subset of nodes have class labels.
However, under extreme cases when very few labels are available (e.g., 1
labeled node per class), GNNs suffer from severe performance degradation.
Specifically, we observe that existing GNNs suffer from unstable training
process on few-labeled graphs, resulting to inferior performance on node
classification. Therefore, we propose an effective framework, Stabilized
Self-Training (SST), which is applicable to existing GNNs to handle the
scarcity of labeled data, and consequently, boost classification accuracy. We
conduct thorough empirical and theoretical analysis to support our findings and
motivate the algorithmic designs in SST. We apply SST to two popular GNN models
GCN and DAGNN, to get SSTGCN and SSTDA methods respectively, and evaluate the
two methods against 10 competitors over 5 benchmarking datasets. Extensive
experiments show that the proposed SST framework is highly effective,
especially when few labeled data are available. Our methods achieve superior
performance under almost all settings over all datasets. For instance, on a
Cora dataset with only 1 labeled node per class, the accuracy of SSTGCN is
62.5%, 17.9% higher than GCN, and the accuracy of SSTDA is 66.4%, which
outperforms DAGNN by 6.6%.","['Ziang Zhou', 'Jieming Shi', 'Shengzhong Zhang', 'Zengfeng Huang', 'Qing Li']","['cs.LG', 'stat.ML']",2019-10-07 09:21:49+00:00
http://arxiv.org/abs/1910.02678v1,An Algorithmic Inference Approach to Learn Copulas,"We introduce a new method for estimating the parameter of the bivariate
Clayton copulas within the framework of Algorithmic Inference. The method
consists of a variant of the standard boot-strapping procedure for inferring
random parameters, which we expressly devise to bypass the two pitfalls of this
specific instance: the non independence of the Kendall statistics, customarily
at the basis of this inference task, and the absence of a sufficient statistic
w.r.t. \alpha. The variant is rooted on a numerical procedure in order to find
the \alpha estimate at a fixed point of an iterative routine. Although paired
with the customary complexity of the program which computes them, numerical
results show an outperforming accuracy of the estimates.",['Bruno Apolloni'],"['stat.ML', 'cs.LG']",2019-10-07 09:06:04+00:00
http://arxiv.org/abs/1910.02673v1,Interpretable Disentanglement of Neural Networks by Extracting Class-Specific Subnetwork,"We propose a novel perspective to understand deep neural networks in an
interpretable disentanglement form. For each semantic class, we extract a
class-specific functional subnetwork from the original full model, with
compressed structure while maintaining comparable prediction performance. The
structure representations of extracted subnetworks display a resemblance to
their corresponding class semantic similarities. We also apply extracted
subnetworks in visual explanation and adversarial example detection tasks by
merely replacing the original full model with class-specific subnetworks.
Experiments demonstrate that this intuitive operation can effectively improve
explanation saliency accuracy for gradient-based explanation methods, and
increase the detection rate for confidence score-based adversarial example
detection methods.","['Yulong Wang', 'Xiaolin Hu', 'Hang Su']","['cs.LG', 'stat.ML']",2019-10-07 08:42:45+00:00
http://arxiv.org/abs/1910.02672v2,Multi-label Detection and Classification of Red Blood Cells in Microscopic Images,"Cell detection and cell type classification from biomedical images play an
important role for high-throughput imaging and various clinical application.
While classification of single cell sample can be performed with standard
computer vision and machine learning methods, analysis of multi-label samples
(region containing congregating cells) is more challenging, as separation of
individual cells can be difficult (e.g. touching cells) or even impossible
(e.g. overlapping cells). As multi-instance images are common in analyzing Red
Blood Cell (RBC) for Sickle Cell Disease (SCD) diagnosis, we develop and
implement a multi-instance cell detection and classification framework to
address this challenge. The framework firstly trains a region proposal model
based on Region-based Convolutional Network (RCNN) to obtain bounding-boxes of
regions potentially containing single or multiple cells from input microscopic
images, which are extracted as image patches. High-level image features are
then calculated from image patches through a pre-trained Convolutional Neural
Network (CNN) with ResNet-50 structure. Using these image features inputs, six
networks are then trained to make multi-label prediction of whether a given
patch contains cells belonging to a specific cell type. As the six networks are
trained with image patches consisting of both individual cells and
touching/overlapping cells, they can effectively recognize cell types that are
presented in multi-instance image samples. Finally, for the purpose of SCD
testing, we train another machine learning classifier to predict whether the
given image patch contains abnormal cell type based on outputs from the six
networks. Testing result of the proposed framework shows that it can achieve
good performance in automatic cell detection and classification.","['Wei Qiu', 'Jiaming Guo', 'Xiang Li', 'Mengjia Xu', 'Mo Zhang', 'Ning Guo', 'Quanzheng Li']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2019-10-07 08:40:49+00:00
http://arxiv.org/abs/1910.02660v1,Deep Kernel Learning via Random Fourier Features,"Kernel learning methods are among the most effective learning methods and
have been vigorously studied in the past decades. However, when tackling with
complicated tasks, classical kernel methods are not flexible or ""rich"" enough
to describe the data and hence could not yield satisfactory performance. In
this paper, via Random Fourier Features (RFF), we successfully incorporate the
deep architecture into kernel learning, which significantly boosts the
flexibility and richness of kernel machines while keeps kernels' advantage of
pairwise handling small data. With RFF, we could establish a deep structure and
make every kernel in RFF layers could be trained end-to-end. Since RFF with
different distributions could represent different kernels, our model has the
capability of finding suitable kernels for each layer, which is much more
flexible than traditional kernel-based methods where the kernel is
pre-selected. This fact also helps yield a more sophisticated kernel cascade
connection in the architecture. On small datasets (less than 1000 samples), for
which deep learning is generally not suitable due to overfitting, our method
achieves superior performance compared to advanced kernel methods. On
large-scale datasets, including non-image and image classification tasks, our
method also has competitive performance.","['Jiaxuan Xie', 'Fanghui Liu', 'Kaijie Wang', 'Xiaolin Huang']","['cs.LG', 'stat.ML']",2019-10-07 08:20:07+00:00
http://arxiv.org/abs/1910.02653v3,Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization,"We formalize the problem of trading-off DNN training time and memory
requirements as the tensor rematerialization optimization problem, a
generalization of prior checkpointing strategies. We introduce Checkmate, a
system that solves for optimal rematerialization schedules in reasonable times
(under an hour) using off-the-shelf MILP solvers or near-optimal schedules with
an approximation algorithm, then uses these schedules to accelerate millions of
training iterations. Our method scales to complex, realistic architectures and
is hardware-aware through the use of accelerator-specific, profile-based cost
models. In addition to reducing training cost, Checkmate enables real-world
networks to be trained with up to 5.1x larger input sizes. Checkmate is an
open-source project, available at https://github.com/parasj/checkmate.","['Paras Jain', 'Ajay Jain', 'Aniruddha Nrusimha', 'Amir Gholami', 'Pieter Abbeel', 'Kurt Keutzer', 'Ion Stoica', 'Joseph E. Gonzalez']","['cs.LG', 'cs.CV', 'cs.DC', 'stat.ML']",2019-10-07 07:54:06+00:00
http://arxiv.org/abs/1910.02635v3,A Decentralized Communication Policy for Multi Agent Multi Armed Bandit Problems,"This paper proposes a novel policy for a group of agents to, individually as
well as collectively, solve a multi armed bandit (MAB) problem. The policy
relies solely on the information that an agent has obtained through sampling of
the options on its own and through communication with neighbors. The option
selection policy is based on an Upper Confidence Based (UCB) strategy while the
communication strategy that is proposed forces agents to communicate with other
agents who they believe are most likely to be exploring than exploiting. The
overall strategy is shown to significantly outperform an independent
Erd\H{o}s-R\'{e}nyi (ER) graph based random communication policy. The policy is
shown to be cost effective in terms of communication and thus to be easily
scalable to a large network of agents.","['Pathmanathan Pankayaraj', 'D. H. S. Maithripala']","['cs.LG', 'stat.ML']",2019-10-07 07:05:36+00:00
http://arxiv.org/abs/1910.02633v1,Deep Hyperedges: a Framework for Transductive and Inductive Learning on Hypergraphs,"From social networks to protein complexes to disease genomes to visual data,
hypergraphs are everywhere. However, the scope of research studying deep
learning on hypergraphs is still quite sparse and nascent, as there has not yet
existed an effective, unified framework for using hyperedge and vertex
embeddings jointly in the hypergraph context, despite a large body of prior
work that has shown the utility of deep learning over graphs and sets. Building
upon these recent advances, we propose \textit{Deep Hyperedges} (DHE), a
modular framework that jointly uses contextual and permutation-invariant vertex
membership properties of hyperedges in hypergraphs to perform classification
and regression in transductive and inductive learning settings. In our
experiments, we use a novel random walk procedure and show that our model
achieves and, in most cases, surpasses state-of-the-art performance on
benchmark datasets. Additionally, we study our framework's performance on a
variety of diverse, non-standard hypergraph datasets and propose several
avenues of future work to further enhance DHE.",['Josh Payne'],"['cs.LG', 'cs.SI', 'stat.ML']",2019-10-07 06:56:02+00:00
http://arxiv.org/abs/1910.03498v1,SentiCite: An Approach for Publication Sentiment Analysis,"With the rapid growth in the number of scientific publications, year after
year, it is becoming increasingly difficult to identify quality authoritative
work on a single topic. Though there is an availability of scientometric
measures which promise to offer a solution to this problem, these measures are
mostly quantitative and rely, for instance, only on the number of times an
article is cited. With this approach, it becomes irrelevant if an article is
cited 10 times in a positive, negative or neutral way. In this context, it is
quite important to study the qualitative aspect of a citation to understand its
significance. This paper presents a novel system for sentiment analysis of
citations in scientific documents (SentiCite) and is also capable of detecting
nature of citations by targeting the motivation behind a citation, e.g.,
reference to a dataset, reading reference. Furthermore, the paper also presents
two datasets (SentiCiteDB and IntentCiteDB) containing about 2,600 citations
with their ground truth for sentiment and nature of citation. SentiCite along
with other state-of-the-art methods for sentiment analysis are evaluated on the
presented datasets. Evaluation results reveal that SentiCite outperforms
state-of-the-art methods for sentiment analysis in scientific publications by
achieving a F1-measure of 0.71.","['Dominique Mercier', 'Akansha Bhardwaj', 'Andreas Dengel', 'Sheraz Ahmed']","['cs.CL', 'cs.DB', 'cs.IR', 'cs.LG', 'stat.ML']",2019-10-07 06:49:52+00:00
http://arxiv.org/abs/1910.02629v3,Softmax Is Not an Artificial Trick: An Information-Theoretic View of Softmax in Neural Networks,"Despite great popularity of applying softmax to map the non-normalised
outputs of a neural network to a probability distribution over predicting
classes, this normalised exponential transformation still seems to be
artificial. A theoretic framework that incorporates softmax as an intrinsic
component is still lacking. In this paper, we view neural networks embedding
softmax from an information-theoretic perspective. Under this view, we can
naturally and mathematically derive log-softmax as an inherent component in a
neural network for evaluating the conditional mutual information between
network output vectors and labels given an input datum. We show that training
deterministic neural networks through maximising log-softmax is equivalent to
enlarging the conditional mutual information, i.e., feeding label information
into network outputs. We also generalise our informative-theoretic perspective
to neural networks with stochasticity and derive information upper and lower
bounds of log-softmax. In theory, such an information-theoretic view offers
rationality support for embedding softmax in neural networks; in practice, we
eventually demonstrate a computer vision application example of how to employ
our information-theoretic view to filter out targeted objects on images.","['Zhenyue Qin', 'Dongwoo Kim']","['cs.LG', 'cs.CV', 'stat.ML']",2019-10-07 06:46:06+00:00
http://arxiv.org/abs/1910.03648v1,Meta-Transfer Learning through Hard Tasks,"Meta-learning has been proposed as a framework to address the challenging
few-shot learning setting. The key idea is to leverage a large number of
similar few-shot tasks in order to learn how to adapt a base-learner to a new
task for which only a few labeled samples are available. As deep neural
networks (DNNs) tend to overfit using a few samples only, typical meta-learning
models use shallow neural networks, thus limiting its effectiveness. In order
to achieve top performance, some recent works tried to use the DNNs pre-trained
on large-scale datasets but mostly in straight-forward manners, e.g., (1)
taking their weights as a warm start of meta-training, and (2) freezing their
convolutional layers as the feature extractor of base-learners. In this paper,
we propose a novel approach called meta-transfer learning (MTL) which learns to
transfer the weights of a deep NN for few-shot learning tasks. Specifically,
meta refers to training multiple tasks, and transfer is achieved by learning
scaling and shifting functions of DNN weights for each task. In addition, we
introduce the hard task (HT) meta-batch scheme as an effective learning
curriculum that further boosts the learning efficiency of MTL. We conduct
few-shot learning experiments and report top performance for five-class
few-shot recognition tasks on three challenging benchmarks: miniImageNet,
tieredImageNet and Fewshot-CIFAR100 (FC100). Extensive comparisons to related
works validate that our MTL approach trained with the proposed HT meta-batch
scheme achieves top performance. An ablation study also shows that both
components contribute to fast convergence and high accuracy.","['Qianru Sun', 'Yaoyao Liu', 'Zhaozheng Chen', 'Tat-Seng Chua', 'Bernt Schiele']","['cs.CV', 'cs.LG', 'stat.ML']",2019-10-07 06:05:18+00:00
http://arxiv.org/abs/1910.02600v2,Deep Evidential Regression,"Deterministic neural networks (NNs) are increasingly being deployed in safety
critical domains, where calibrated, robust, and efficient measures of
uncertainty are crucial. In this paper, we propose a novel method for training
non-Bayesian NNs to estimate a continuous target as well as its associated
evidence in order to learn both aleatoric and epistemic uncertainty. We
accomplish this by placing evidential priors over the original Gaussian
likelihood function and training the NN to infer the hyperparameters of the
evidential distribution. We additionally impose priors during training such
that the model is regularized when its predicted evidence is not aligned with
the correct output. Our method does not rely on sampling during inference or on
out-of-distribution (OOD) examples for training, thus enabling efficient and
scalable uncertainty learning. We demonstrate learning well-calibrated measures
of uncertainty on various benchmarks, scaling to complex computer vision tasks,
as well as robustness to adversarial and OOD test samples.","['Alexander Amini', 'Wilko Schwarting', 'Ava Soleimany', 'Daniela Rus']","['cs.LG', 'cs.NE', 'stat.ML']",2019-10-07 04:11:34+00:00
http://arxiv.org/abs/1910.02594v1,Weighted graphlets and deep neural networks for protein structure classification,"As proteins with similar structures often have similar functions, analysis of
protein structures can help predict protein functions and is thus important. We
consider the problem of protein structure classification, which computationally
classifies the structures of proteins into pre-defined groups. We develop a
weighted network that depicts the protein structures, and more importantly, we
propose the first graphlet-based measure that applies to weighted networks.
Further, we develop a deep neural network (DNN) composed of both convolutional
and recurrent layers to use this measure for classification. Put together, our
approach shows dramatic improvements in performance over existing
graphlet-based approaches on 36 real datasets. Even comparing with the
state-of-the-art approach, it almost halves the classification error. In
addition to protein structure networks, our weighted-graphlet measure and DNN
classifier can potentially be applied to classification of other weighted
networks in computational biology as well as in other domains.","['Hongyu Guo', 'Khalique Newaz', 'Scott Emrich', 'Tijana Milenkovic', 'Jun Li']","['stat.ML', 'cs.LG', 'q-bio.BM']",2019-10-07 03:36:25+00:00
http://arxiv.org/abs/1910.03467v2,Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation,"Among the six challenges of neural machine translation (NMT) coined by (Koehn
and Knowles, 2017), rare-word problem is considered the most severe one,
especially in translation of low-resource languages. In this paper, we propose
three solutions to address the rare words in neural machine translation
systems. First, we enhance source context to predict the target words by
connecting directly the source embeddings to the output of the attention
component in NMT. Second, we propose an algorithm to learn morphology of
unknown words for English in supervised way in order to minimize the adverse
effect of rare-word problem. Finally, we exploit synonymous relation from the
WordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our
approaches on two low-resource language pairs: English-Vietnamese and
Japanese-Vietnamese. In our experiments, we have achieved significant
improvements of up to roughly +1.0 BLEU points in both language pairs.","['Thi-Vinh Ngo', 'Thanh-Le Ha', 'Phuong-Thai Nguyen', 'Le-Minh Nguyen']","['cs.CL', 'cs.LG', 'stat.ML']",2019-10-07 03:11:13+00:00
http://arxiv.org/abs/1910.02575v2,PyODDS: An End-to-End Outlier Detection System,"PyODDS is an end-to end Python system for outlier detection with database
support. PyODDS provides outlier detection algorithms which meet the demands
for users in different fields, w/wo data science or machine learning
background. PyODDS gives the ability to execute machine learning algorithms
in-database without moving data out of the database server or over the network.
It also provides access to a wide range of outlier detection algorithms,
including statistical analysis and more recent deep learning based approaches.
PyODDS is released under the MIT open-source license, and currently available
at (https://github.com/datamllab/pyodds) with official documentations at
(https://pyodds.github.io/).","['Yuening Li', 'Daochen Zha', 'Na Zou', 'Xia Hu']","['cs.LG', 'cs.DB', 'stat.ML']",2019-10-07 02:01:34+00:00
http://arxiv.org/abs/1910.02574v1,Representation Learning of EHR Data via Graph-Based Medical Entity Embedding,"Automatic representation learning of key entities in electronic health record
(EHR) data is a critical step for healthcare informatics that turns
heterogeneous medical records into structured and actionable information. Here
we propose ME2Vec, an algorithmic framework for learning low-dimensional
vectors of the most common entities in EHR: medical services, doctors, and
patients. ME2Vec leverages diverse graph embedding techniques to cater for the
unique characteristic of each medical entity. Using real-world clinical data,
we demonstrate the efficacy of ME2Vec over competitive baselines on disease
diagnosis prediction.","['Tong Wu', 'Yunlong Wang', 'Yue Wang', 'Emily Zhao', 'Yilian Yuan', 'Zhi Yang']","['cs.LG', 'cs.IR', 'stat.ML']",2019-10-07 02:01:32+00:00
http://arxiv.org/abs/1910.02566v1,Gaussian Mixture Clustering Using Relative Tests of Fit,"We consider clustering based on significance tests for Gaussian Mixture
Models (GMMs). Our starting point is the SigClust method developed by Liu et
al. (2008), which introduces a test based on the k-means objective (with k = 2)
to decide whether the data should be split into two clusters. When applied
recursively, this test yields a method for hierarchical clustering that is
equipped with a significance guarantee. We study the limiting distribution and
power of this approach in some examples and show that there are large regions
of the parameter space where the power is low. We then introduce a new test
based on the idea of relative fit. Unlike prior work, we test for whether a
mixture of Gaussians provides a better fit relative to a single Gaussian,
without assuming that either model is correct. The proposed test has a simple
critical value and provides provable error control. One version of our test
provides exact, finite sample control of the type I error. We show how our
tests can be used for hierarchical clustering as well as in a sequential manner
for model selection. We conclude with an extensive simulation study and a
cluster analysis of a gene expression dataset.","['Purvasha Chakravarti', 'Sivaraman Balakrishnan', 'Larry Wasserman']","['stat.ME', 'stat.ML']",2019-10-07 00:51:38+00:00
http://arxiv.org/abs/1910.02551v3,Soft-Label Dataset Distillation and Text Dataset Distillation,"Dataset distillation is a method for reducing dataset sizes by learning a
small number of synthetic samples containing all the information of a large
dataset. This has several benefits like speeding up model training, reducing
energy consumption, and reducing required storage space. Currently, each
synthetic sample is assigned a single `hard' label, and also, dataset
distillation can currently only be used with image data.
  We propose to simultaneously distill both images and their labels, thus
assigning each synthetic sample a `soft' label (a distribution of labels). Our
algorithm increases accuracy by 2-4% over the original algorithm for several
image classification tasks. Using `soft' labels also enables distilled datasets
to consist of fewer samples than there are classes as each sample can encode
information for multiple classes. For example, training a LeNet model with 10
distilled images (one per class) results in over 96% accuracy on MNIST, and
almost 92% accuracy when trained on just 5 distilled images.
  We also extend the dataset distillation algorithm to distill sequential
datasets including texts. We demonstrate that text distillation outperforms
other methods across multiple datasets. For example, models attain almost their
original accuracy on the IMDB sentiment analysis task using just 20 distilled
sentences.
  Our code can be found at
$\href{https://github.com/ilia10000/dataset-distillation}{\text{https://github.com/ilia10000/dataset-distillation}}$.","['Ilia Sucholutsky', 'Matthias Schonlau']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-06 23:57:22+00:00
http://arxiv.org/abs/1910.02548v1,Rethinking Kernel Methods for Node Representation Learning on Graphs,"Graph kernels are kernel methods measuring graph similarity and serve as a
standard tool for graph classification. However, the use of kernel methods for
node classification, which is a related problem to graph representation
learning, is still ill-posed and the state-of-the-art methods are heavily based
on heuristics. Here, we present a novel theoretical kernel-based framework for
node classification that can bridge the gap between these two representation
learning problems on graphs. Our approach is motivated by graph kernel
methodology but extended to learn the node representations capturing the
structural information in a graph. We theoretically show that our formulation
is as powerful as any positive semidefinite kernels. To efficiently learn the
kernel, we propose a novel mechanism for node feature aggregation and a
data-driven similarity metric employed during the training phase. More
importantly, our framework is flexible and complementary to other graph-based
deep learning models, e.g., Graph Convolutional Networks (GCNs). We empirically
evaluate our approach on a number of standard node classification benchmarks,
and demonstrate that our model sets the new state of the art.","['Yu Tian', 'Long Zhao', 'Xi Peng', 'Dimitris N. Metaxas']","['cs.LG', 'stat.ML']",2019-10-06 23:06:49+00:00
http://arxiv.org/abs/1910.02545v1,Early Prediction of 30-day ICU Re-admissions Using Natural Language Processing and Machine Learning,"ICU readmission is associated with longer hospitalization, mortality and
adverse outcomes. An early recognition of ICU re-admission can help prevent
patients from worse situation and lower treatment cost. As the abundance of
Electronics Health Records (EHR), it is popular to design clinical decision
tools with machine learning technique manipulating on healthcare large scale
data. We designed data-driven predictive models to estimate the risk of ICU
readmission. The discharge summary of each hospital admission was carefully
represented by natural language processing techniques. Unified Medical Language
System (UMLS) was further used to standardize inconsistency of discharge
summaries. 5 machine learning classifiers were adopted to construct predictive
models. The best configuration yielded a competitive AUC of 0.748. Our work
suggests that natural language processing of discharge summaries is capable to
send clinicians warning of unplanned 30-day readmission upon discharge.","['Zhiheng Li', 'Xinyue Xing', 'Bingzhang Lu', 'Zhixiang Li']","['cs.LG', 'cs.CL', 'stat.ML']",2019-10-06 22:54:00+00:00
http://arxiv.org/abs/1910.02544v1,Using Deep Learning and Machine Learning to Detect Epileptic Seizure with Electroencephalography (EEG) Data,"The prediction of epileptic seizure has always been extremely challenging in
medical domain. However, as the development of computer technology, the
application of machine learning introduced new ideas for seizure forecasting.
Applying machine learning model onto the predication of epileptic seizure could
help us obtain a better result and there have been plenty of scientists who
have been doing such works so that there are sufficient medical data provided
for researchers to do training of machine learning models.","['Haotian Liu', 'Lin Xi', 'Ying Zhao', 'Zhixiang Li']","['cs.LG', 'eess.SP', 'stat.ML']",2019-10-06 22:53:28+00:00
http://arxiv.org/abs/1910.02519v4,FIS-GAN: GAN with Flow-based Importance Sampling,"Generative Adversarial Networks (GAN) training process, in most cases, apply
Uniform or Gaussian sampling methods in the latent space, which probably spends
most of the computation on examples that can be properly handled and easy to
generate. Theoretically, importance sampling speeds up stochastic optimization
in supervised learning by prioritizing training examples. In this paper, we
explore the possibility of adapting importance sampling into adversarial
learning. We use importance sampling to replace Uniform and Gaussian sampling
methods in the latent space and employ normalizing flow to approximate latent
space posterior distribution by density estimation. Empirically, results on
MNIST and Fashion-MNIST demonstrate that our method significantly accelerates
GAN's optimization while retaining visual fidelity in generated samples.","['Shiyu Yi', 'Donglin Zhan', 'Wenqing Zhang', 'Denglin Jiang', 'Kang An', 'Hao Wang']","['cs.LG', 'stat.ML']",2019-10-06 20:34:52+00:00
http://arxiv.org/abs/1910.02505v2,Boosting Local Causal Discovery in High-Dimensional Expression Data,"We study the performance of Local Causal Discovery (LCD), a simple and
efficient constraint-based method for causal discovery, in predicting causal
effects in large-scale gene expression data. We construct practical estimators
specific to the high-dimensional regime. Inspired by the ICP algorithm, we use
an optional preselection method and two different statistical tests.
Empirically, the resulting LCD estimator is seen to closely approach the
accuracy of ICP, the state-of-the-art method, while it is algorithmically
simpler and computationally more efficient.","['Philip Versteeg', 'Joris M. Mooij']","['stat.ML', 'cs.LG', 'stat.ME']",2019-10-06 19:16:23+00:00
http://arxiv.org/abs/1910.02497v5,mfEGRA: Multifidelity Efficient Global Reliability Analysis through Active Learning for Failure Boundary Location,"This paper develops mfEGRA, a multifidelity active learning method using
data-driven adaptively refined surrogates for failure boundary location in
reliability analysis. This work addresses the issue of prohibitive cost of
reliability analysis using Monte Carlo sampling for expensive-to-evaluate
high-fidelity models by using cheaper-to-evaluate approximations of the
high-fidelity model. The method builds on the Efficient Global Reliability
Analysis (EGRA) method, which is a surrogate-based method that uses adaptive
sampling for refining Gaussian process surrogates for failure boundary location
using a single-fidelity model. Our method introduces a two-stage adaptive
sampling criterion that uses a multifidelity Gaussian process surrogate to
leverage multiple information sources with different fidelities. The method
combines expected feasibility criterion from EGRA with one-step lookahead
information gain to refine the surrogate around the failure boundary. The
computational savings from mfEGRA depends on the discrepancy between the
different models, and the relative cost of evaluating the different models as
compared to the high-fidelity model. We show that accurate estimation of
reliability using mfEGRA leads to computational savings of $\sim$46% for an
analytic multimodal test problem and 24% for a three-dimensional acoustic horn
problem, when compared to single-fidelity EGRA. We also show the effect of
using a priori drawn Monte Carlo samples in the implementation for the acoustic
horn problem, where mfEGRA leads to computational savings of 45% for the
three-dimensional case and 48% for a rarer event four-dimensional case as
compared to single-fidelity EGRA.","['Anirban Chaudhuri', 'Alexandre N. Marques', 'Karen E. Willcox']","['stat.ML', 'cs.LG', 'physics.data-an', 'stat.CO', '62K05, 62L05, 60G15, 68M15']",2019-10-06 18:37:12+00:00
http://arxiv.org/abs/1910.02483v2,Auto-Rotating Perceptrons,"This paper proposes an improved design of the perceptron unit to mitigate the
vanishing gradient problem. This nuisance appears when training deep multilayer
perceptron networks with bounded activation functions. The new neuron design,
named auto-rotating perceptron (ARP), has a mechanism to ensure that the node
always operates in the dynamic region of the activation function, by avoiding
saturation of the perceptron. The proposed method does not change the inference
structure learned at each neuron. We test the effect of using ARP units in some
network architectures which use the sigmoid activation function. The results
support our hypothesis that neural networks with ARP units can achieve better
learning performance than equivalent models with classic perceptrons.","['Daniel Saromo', 'Elizabeth Villota', 'Edwin Villanueva']","['cs.LG', 'stat.ML']",2019-10-06 17:38:47+00:00
http://arxiv.org/abs/1910.02450v1,Mobile APP User Attribute Prediction by Heterogeneous Information Network Modeling,"User-based attribute information, such as age and gender, is usually
considered as user privacy information. It is difficult for enterprises to
obtain user-based privacy attribute information. However, user-based privacy
attribute information has a wide range of applications in personalized
services, user behavior analysis and other aspects. this paper advances the
HetPathMine model and puts forward TPathMine model. With applying the number of
clicks of attributes under each node to express the user's emotional preference
information, optimizations of the solution of meta-path weight are also
presented. Based on meta-path in heterogeneous information networks, the new
model integrates all relationships among objects into isomorphic relationships
of classified objects. Matrix is used to realize the knowledge dissemination of
category knowledge among isomorphic objects. The experimental results show
that: (1) the prediction of user attributes based on heterogeneous information
networks can achieve higher accuracy than traditional machine learning
classification methods; (2) TPathMine model based on the number of clicks is
more accurate in classifying users of different age groups, and the weight of
each meta-path is consistent with human intuition or the real world situation.","['Hekai Zhang', 'Jibing Gong', 'Zhiyong Teng', 'Dan Wang', 'Hongfei Wang', 'Linfeng Du', 'Zakirul Alam Bhuiyan']","['cs.LG', 'stat.ML']",2019-10-06 13:41:58+00:00
http://arxiv.org/abs/1910.02433v2,Weighted Clustering Ensemble: A Review,"Clustering ensemble, or consensus clustering, has emerged as a powerful tool
for improving both the robustness and the stability of results from individual
clustering methods. Weighted clustering ensemble arises naturally from
clustering ensemble. One of the arguments for weighted clustering ensemble is
that elements (clusterings or clusters) in a clustering ensemble are of
different quality, or that objects or features are of varying significance.
However, it is not possible to directly apply the weighting mechanisms from
classification (supervised) domain to clustering (unsupervised) domain, also
because clustering is inherently an ill-posed problem. This paper provides an
overview of weighted clustering ensemble by discussing different types of
weights, major approaches to determining weight values, and applications of
weighted clustering ensemble to complex data. The unifying framework presented
in this paper will help clustering practitioners select the most appropriate
weighting mechanisms for their own problems.",['Mimi Zhang'],"['cs.CV', 'stat.ML']",2019-10-06 12:16:29+00:00
http://arxiv.org/abs/1910.02425v2,Structured Object-Aware Physics Prediction for Video Modeling and Planning,"When humans observe a physical system, they can easily locate objects,
understand their interactions, and anticipate future behavior, even in settings
with complicated and previously unseen interactions. For computers, however,
learning such models from videos in an unsupervised fashion is an unsolved
research problem. In this paper, we present STOVE, a novel state-space model
for videos, which explicitly reasons about objects and their positions,
velocities, and interactions. It is constructed by combining an image model and
a dynamics model in compositional manner and improves on previous work by
reusing the dynamics model for inference, accelerating and regularizing
training. STOVE predicts videos with convincing physical behavior over hundreds
of timesteps, outperforms previous unsupervised models, and even approaches the
performance of supervised baselines. We further demonstrate the strength of our
model as a simulator for sample efficient model-based control in a task with
heavily interacting objects.","['Jannik Kossen', 'Karl Stelzner', 'Marcel Hussing', 'Claas Voelcker', 'Kristian Kersting']","['cs.LG', 'cs.CV', 'stat.ML']",2019-10-06 11:48:26+00:00
http://arxiv.org/abs/1910.02423v1,ChaosNet: A Chaos based Artificial Neural Network Architecture for Classification,"Inspired by chaotic firing of neurons in the brain, we propose ChaosNet -- a
novel chaos based artificial neural network architecture for classification
tasks. ChaosNet is built using layers of neurons, each of which is a 1D chaotic
map known as the Generalized Luroth Series (GLS) which has been shown in
earlier works to possess very useful properties for compression, cryptography
and for computing XOR and other logical operations. In this work, we design a
novel learning algorithm on ChaosNet that exploits the topological transitivity
property of the chaotic GLS neurons. The proposed learning algorithm gives
consistently good performance accuracy in a number of classification tasks on
well known publicly available datasets with very limited training samples. Even
with as low as 7 (or fewer) training samples/class (which accounts for less
than 0.05% of the total available data), ChaosNet yields performance accuracies
in the range 73.89 % - 98.33 %. We demonstrate the robustness of ChaosNet to
additive parameter noise and also provide an example implementation of a
2-layer ChaosNet for enhancing classification accuracy. We envisage the
development of several other novel learning algorithms on ChaosNet in the near
future.","['Harikrishnan Nellippallil Balakrishnan', 'Aditi Kathpalia', 'Snehanshu Saha', 'Nithin Nagaraj']","['cs.LG', 'nlin.CD', 'stat.ML']",2019-10-06 11:40:40+00:00
http://arxiv.org/abs/1910.02421v2,On Universal Equivariant Set Networks,"Using deep neural networks that are either invariant or equivariant to
permutations in order to learn functions on unordered sets has become
prevalent. The most popular, basic models are DeepSets [Zaheer et al. 2017] and
PointNet [Qi et al. 2017]. While known to be universal for approximating
invariant functions, DeepSets and PointNet are not known to be universal when
approximating \emph{equivariant} set functions. On the other hand, several
recent equivariant set architectures have been proven equivariant universal
[Sannai et al. 2019], [Keriven et al. 2019], however these models either use
layers that are not permutation equivariant (in the standard sense) and/or use
higher order tensor variables which are less practical.
  There is, therefore, a gap in understanding the universality of popular
equivariant set models versus theoretical ones.
  In this paper we close this gap by proving that: (i) PointNet is not
equivariant universal; and (ii) adding a single linear transmission layer makes
PointNet universal. We call this architecture PointNetST and argue it is the
simplest permutation equivariant universal model known to date. Another
consequence is that DeepSets is universal, and also PointNetSeg, a popular
point cloud segmentation network (used eg, in [Qi et al. 2017]) is universal.
  The key theoretical tool used to prove the above results is an explicit
characterization of all permutation equivariant polynomial layers. Lastly, we
provide numerical experiments validating the theoretical results and comparing
different permutation equivariant models.","['Nimrod Segol', 'Yaron Lipman']","['cs.LG', 'stat.ML']",2019-10-06 11:37:56+00:00
http://arxiv.org/abs/1910.02420v2,Deep learning-based development of personalized human head model with non-uniform conductivity for brain stimulation,"Electromagnetic stimulation of the human brain is a key tool for the
neurophysiological characterization and diagnosis of several neurological
disorders. Transcranial magnetic stimulation (TMS) is one procedure that is
commonly used clinically. However, personalized TMS requires a pipeline for
accurate head model generation to provide target-specific stimulation. This
process includes intensive segmentation of several head tissues based on
magnetic resonance imaging (MRI), which has significant potential for
segmentation error, especially for low-contrast tissues. Additionally, a
uniform electrical conductivity is assigned to each tissue in the model, which
is an unrealistic assumption based on conventional volume conductor modeling.
This paper proposes a novel approach to the automatic estimation of electric
conductivity in the human head for volume conductor models without anatomical
segmentation. A convolutional neural network is designed to estimate
personalized electrical conductivity values based on anatomical information
obtained from T1- and T2-weighted MRI scans. This approach can avoid the
time-consuming process of tissue segmentation and maximize the advantages of
position-dependent conductivity assignment based on water content values
estimated from MRI intensity values. The computational results of the proposed
approach provide similar but smoother electric field results for the brain when
compared to conventional approaches.","['Essam A. Rashed', 'Jose Gomez-Tames', 'Akimasa Hirata']","['cs.LG', 'cs.CV', 'stat.ML']",2019-10-06 11:33:13+00:00
http://arxiv.org/abs/1910.02390v4,Migration through Machine Learning Lens -- Predicting Sexual and Reproductive Health Vulnerability of Young Migrants,"In this paper, we have discussed initial findings and results of our
experiment to predict sexual and reproductive health vulnerabilities of
migrants in a data-constrained environment. Notwithstanding the limited
research and data about migrants and migration cities, we propose a solution
that simultaneously focuses on data gathering from migrants, augmenting
awareness of the migrants to reduce mishaps, and setting up a mechanism to
present insights to the key stakeholders in migration to act upon. We have
designed a webapp for the stakeholders involved in migration: migrants, who
would participate in data gathering process and can also use the app for
getting to know safety and awareness tips based on analysis of the data
received; public health workers, who would have an access to the database of
migrants on the app; policy makers, who would have a greater understanding of
the ground reality, and of the patterns of migration through machine-learned
analysis. Finally, we have experimented with different machine learning models
on an artificially curated dataset. We have shown, through experiments, how
machine learning can assist in predicting the migrants at risk and can also
help in identifying the critical factors that make migration dangerous for
migrants. The results for identifying vulnerable migrants through machine
learning algorithms are statistically significant at an alpha of 0.05.","['Amber Nigam', 'Pragati Jaiswal', 'Uma Girkar', 'Teertha Arora', 'Leo A. Celi']","['cs.LG', 'cs.CY', 'stat.ML']",2019-10-06 07:09:13+00:00
http://arxiv.org/abs/1910.02384v4,SCALOR: Generative World Models with Scalable Object Representations,"Scalability in terms of object density in a scene is a primary challenge in
unsupervised sequential object-oriented representation learning. Most of the
previous models have been shown to work only on scenes with a few objects. In
this paper, we propose SCALOR, a probabilistic generative world model for
learning SCALable Object-oriented Representation of a video. With the proposed
spatially-parallel attention and proposal-rejection mechanisms, SCALOR can deal
with orders of magnitude larger numbers of objects compared to the previous
state-of-the-art models. Additionally, we introduce a background module that
allows SCALOR to model complex dynamic backgrounds as well as many foreground
objects in the scene. We demonstrate that SCALOR can deal with crowded scenes
containing up to a hundred objects while jointly modeling complex dynamic
backgrounds. Importantly, SCALOR is the first unsupervised object
representation model shown to work for natural scenes containing several tens
of moving objects.","['Jindong Jiang', 'Sepehr Janghorbani', 'Gerard de Melo', 'Sungjin Ahn']","['cs.LG', 'stat.ML']",2019-10-06 06:26:31+00:00
http://arxiv.org/abs/1910.02380v1,Patterns of Urban Foot Traffic Dynamics,"Using publicly available traffic camera data in New York City, we quantify
time-dependent patterns in aggregate pedestrian foot traffic. These patterns
exhibit repeatable diurnal behaviors that differ for weekdays and weekends but
are broadly consistent across neighborhoods in the borough of Manhattan.
Weekday patterns contain a characteristic 3-peak structure with increased foot
traffic around 9:00am, 12:00-1:00pm, and 5:00pm aligned with the ""9-to-5"" work
day in which pedestrians are on the street during their morning commute, during
lunch hour, and then during their evening commute. Weekend days do not show a
peaked structure, but rather increase steadily until sunset. Our study period
of June 28, 2017 to September 11, 2017 contains two holidays, the 4th of July
and Labor Day, and their foot traffic patterns are quantitatively similar to
weekend days despite the fact that they fell on weekdays. Projecting all days
in our study period onto the weekday/weekend phase space (by regressing against
the average weekday and weekend day) we find that Friday foot traffic can be
represented as a mixture of both the 3-peak weekday structure and non-peaked
weekend structure. We also show that anomalies in the foot traffic patterns can
be used for detection of events and network-level disruptions. Finally, we show
that clustering of foot traffic time series generates associations between
cameras that are spatially aligned with Manhattan neighborhood boundaries
indicating that foot traffic dynamics encode information about neighborhood
character.","['Gregory Dobler', 'Jordan Vani', 'Trang Tran Linh Dam']","['physics.soc-ph', 'cs.LG', 'eess.SP', 'stat.ML']",2019-10-06 05:39:57+00:00
http://arxiv.org/abs/1910.02373v3,"Ridge Regression: Structure, Cross-Validation, and Sketching","We study the following three fundamental problems about ridge regression: (1)
what is the structure of the estimator? (2) how to correctly use
cross-validation to choose the regularization parameter? and (3) how to
accelerate computation without losing too much accuracy? We consider the three
problems in a unified large-data linear model. We give a precise representation
of ridge regression as a covariance matrix-dependent linear combination of the
true parameter and the noise. We study the bias of $K$-fold cross-validation
for choosing the regularization parameter, and propose a simple
bias-correction. We analyze the accuracy of primal and dual sketching for ridge
regression, showing they are surprisingly accurate. Our results are illustrated
by simulations and by analyzing empirical data.","['Sifan Liu', 'Edgar Dobriban']","['math.ST', 'stat.ML', 'stat.TH']",2019-10-06 05:00:40+00:00
http://arxiv.org/abs/1910.02370v2,GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding,"Graph embedding techniques have been increasingly deployed in a multitude of
different applications that involve learning on non-Euclidean data. However,
existing graph embedding models either fail to incorporate node attribute
information during training or suffer from node attribute noise, which
compromises the accuracy. Moreover, very few of them scale to large graphs due
to their high computational complexity and memory usage. In this paper we
propose GraphZoom, a multi-level framework for improving both accuracy and
scalability of unsupervised graph embedding algorithms. GraphZoom first
performs graph fusion to generate a new graph that effectively encodes the
topology of the original graph and the node attribute information. This fused
graph is then repeatedly coarsened into much smaller graphs by merging nodes
with high spectral similarities. GraphZoom allows any existing embedding
methods to be applied to the coarsened graph, before it progressively refine
the embeddings obtained at the coarsest level to increasingly finer graphs. We
have evaluated our approach on a number of popular graph datasets for both
transductive and inductive tasks. Our experiments show that GraphZoom can
substantially increase the classification accuracy and significantly accelerate
the entire graph embedding process by up to 40.8x, when compared to the
state-of-the-art unsupervised embedding methods.","['Chenhui Deng', 'Zhiqiang Zhao', 'Yongyu Wang', 'Zhiru Zhang', 'Zhuo Feng']","['cs.LG', 'stat.ML']",2019-10-06 04:43:46+00:00
http://arxiv.org/abs/1910.02366v3,Splitting Steepest Descent for Growing Neural Architectures,"We develop a progressive training approach for neural networks which
adaptively grows the network structure by splitting existing neurons to
multiple off-springs. By leveraging a functional steepest descent idea, we
derive a simple criterion for deciding the best subset of neurons to split and
a splitting gradient for optimally updating the off-springs. Theoretically, our
splitting strategy is a second-order functional steepest descent for escaping
saddle points in an $\infty$-Wasserstein metric space, on which the standard
parametric gradient descent is a first-order steepest descent. Our method
provides a new computationally efficient approach for optimizing neural network
structures, especially for learning lightweight neural architectures in
resource-constrained settings.","['Qiang Liu', 'Lemeng Wu', 'Dilin Wang']","['cs.LG', 'cs.NE', 'stat.ML']",2019-10-06 04:15:23+00:00
http://arxiv.org/abs/1910.02352v2,Operational Calibration: Debugging Confidence Errors for DNNs in the Field,"Trained DNN models are increasingly adopted as integral parts of software
systems, but they often perform deficiently in the field. A particularly
damaging problem is that DNN models often give false predictions with high
confidence, due to the unavoidable slight divergences between operation data
and training data. To minimize the loss caused by inaccurate confidence,
operational calibration, i.e., calibrating the confidence function of a DNN
classifier against its operation domain, becomes a necessary debugging step in
the engineering of the whole system.
  Operational calibration is difficult considering the limited budget of
labeling operation data and the weak interpretability of DNN models. We propose
a Bayesian approach to operational calibration that gradually corrects the
confidence given by the model under calibration with a small number of labeled
operation data deliberately selected from a larger set of unlabeled operation
data. The approach is made effective and efficient by leveraging the locality
of the learned representation of the DNN model and modeling the calibration as
Gaussian Process Regression. Comprehensive experiments with various practical
datasets and DNN models show that it significantly outperformed alternative
methods, and in some difficult tasks it eliminated about 71% to 97%
high-confidence (>0.9) errors with only about 10\% of the minimal amount of
labeled operation data needed for practical learning techniques to barely work.","['Zenan Li', 'Xiaoxing Ma', 'Chang Xu', 'Jingwei Xu', 'Chun Cao', 'Jian L√º']","['cs.LG', 'cs.SE', 'stat.ML']",2019-10-06 01:21:14+00:00
http://arxiv.org/abs/1910.02344v2,Neural Multisensory Scene Inference,"For embodied agents to infer representations of the underlying 3D physical
world they inhabit, they should efficiently combine multisensory cues from
numerous trials, e.g., by looking at and touching objects. Despite its
importance, multisensory 3D scene representation learning has received less
attention compared to the unimodal setting. In this paper, we propose the
Generative Multisensory Network (GMN) for learning latent representations of 3D
scenes which are partially observable through multiple sensory modalities. We
also introduce a novel method, called the Amortized Product-of-Experts, to
improve the computational efficiency and the robustness to unseen combinations
of modalities at test time. Experimental results demonstrate that the proposed
model can efficiently infer robust modality-invariant 3D-scene representations
from arbitrary combinations of modalities and perform accurate cross-modal
generation. To perform this exploration, we also develop the Multisensory
Embodied 3D-Scene Environment (MESE).","['Jae Hyun Lim', 'Pedro O. Pinheiro', 'Negar Rostamzadeh', 'Christopher Pal', 'Sungjin Ahn']","['cs.LG', 'stat.ML']",2019-10-06 00:14:38+00:00
http://arxiv.org/abs/1910.02342v1,Clustering Gaussian Graphical Models,"We derive an efficient method to perform clustering of nodes in Gaussian
graphical models directly from sample data. Nodes are clustered based on the
similarity of their network neighborhoods, with edge weights defined by partial
correlations. In the limited-data scenario, where the covariance matrix would
be rank-deficient, we are able to make use of matrix factors, and never need to
estimate the actual covariance or precision matrix. We demonstrate the method
on functional MRI data from the Human Connectome Project. A matlab
implementation of the algorithm is provided.",['Keith Dillon'],"['cs.LG', 'stat.ML']",2019-10-05 23:48:05+00:00
http://arxiv.org/abs/1910.02338v1,An Optimal Transport Formulation of the Ensemble Kalman Filter,"Controlled interacting particle systems such as the ensemble Kalman filter
(EnKF) and the feedback particle filter (FPF) are numerical algorithms to
approximate the solution of the nonlinear filtering problem in continuous time.
The distinguishing feature of these algorithms is that the Bayesian update step
is implemented using a feedback control law. It has been noted in the
literature that the control law is not unique. This is the main problem
addressed in this paper. To obtain a unique control law, the filtering problem
is formulated here as an optimal transportation problem. An explicit formula
for the (mean-field type) optimal control law is derived in the linear Gaussian
setting. Comparisons are made with the control laws for different types of EnKF
algorithms described in the literature. Via empirical approximation of the
mean-field control law, a finite-$N$ controlled interacting particle algorithm
is obtained. For this algorithm, the equations for empirical mean and
covariance are derived and shown to be identical to the Kalman filter. This
allows strong conclusions on convergence and error properties based on the
classical filter stability theory for the Kalman filter. It is shown that,
under certain technical conditions, the mean squared error (m.s.e.) converges
to zero even with a finite number of particles. A detailed propagation of chaos
analysis is carried out for the finite-$N$ algorithm. The analysis is used to
prove weak convergence of the empirical distribution as $N\rightarrow\infty$.
For a certain simplified filtering problem, analytical comparison of the m.s.e.
with the importance sampling-based algorithms is described. The analysis helps
explain the favorable scaling properties of the control-based algorithms
reported in several numerical studies in recent literature.","['Amirhossein Taghvaei', 'Prashant G. Mehta']","['eess.SY', 'cs.SY', 'stat.ML']",2019-10-05 22:55:49+00:00
http://arxiv.org/abs/1910.02333v3,The Role of Neural Network Activation Functions,"A wide variety of activation functions have been proposed for neural
networks. The Rectified Linear Unit (ReLU) is especially popular today. There
are many practical reasons that motivate the use of the ReLU. This paper
provides new theoretical characterizations that support the use of the ReLU,
its variants such as the leaky ReLU, as well as other activation functions in
the case of univariate, single-hidden layer feedforward neural networks. Our
results also explain the importance of commonly used strategies in the design
and training of neural networks such as ""weight decay"" and ""path-norm""
regularization, and provide a new justification for the use of ""skip
connections"" in network architectures. These new insights are obtained through
the lens of spline theory. In particular, we show how neural network training
problems are related to infinite-dimensional optimizations posed over Banach
spaces of functions whose solutions are well-known to be fractional and
polynomial splines, where the particular Banach space (which controls the order
of the spline) depends on the choice of activation function.","['Rahul Parhi', 'Robert D. Nowak']","['stat.ML', 'cs.LG']",2019-10-05 21:57:32+00:00
http://arxiv.org/abs/1910.02332v1,Content-Based Features to Rank Influential Hidden Services of the Tor Darknet,"The unevenness importance of criminal activities in the onion domains of the
Tor Darknet and the different levels of their appeal to the end-user make them
tangled to measure their influence. To this end, this paper presents a novel
content-based ranking framework to detect the most influential onion domains.
Our approach comprises a modeling unit that represents an onion domain using
forty features extracted from five different resources: user-visible text, HTML
markup, Named Entities, network topology, and visual content. And also, a
ranking unit that, using the Learning-to-Rank (LtR) approach, automatically
learns a ranking function by integrating the previously obtained features.
Using a case-study based on drugs-related onion domains, we obtained the
following results. (1) Among the explored LtR schemes, the listwise approach
outperforms the benchmarked methods with an NDCG of 0.95 for the top-10 ranked
domains. (2) We proved quantitatively that our framework surpasses the
link-based ranking techniques. Also, (3) with the selected feature, we observed
that the textual content, composed by text, NER, and HTML features, is the most
balanced approach, in terms of efficiency and score obtained. The proposed
framework might support Law Enforcement Agencies in detecting the most
influential domains related to possible suspicious activities.","['Mhd Wesam Al-Nabki', 'Eduardo Fidalgo', 'Enrique Alegre', 'Deisy Chaves']","['cs.LG', 'cs.IR', 'stat.ML']",2019-10-05 21:39:20+00:00
http://arxiv.org/abs/1910.02330v2,Towards Deployment of Robust AI Agents for Human-Machine Partnerships,"We study the problem of designing AI agents that can robustly cooperate with
people in human-machine partnerships. Our work is inspired by real-life
scenarios in which an AI agent, e.g., a virtual assistant, has to cooperate
with new users after its deployment. We model this problem via a parametric MDP
framework where the parameters correspond to a user's type and characterize her
behavior. In the test phase, the AI agent has to interact with a user of
unknown type. Our approach to designing a robust AI agent relies on observing
the user's actions to make inferences about the user's type and adapting its
policy to facilitate efficient cooperation. We show that without being
adaptive, an AI agent can end up performing arbitrarily bad in the test phase.
We develop two algorithms for computing policies that automatically adapt to
the user in the test phase. We demonstrate the effectiveness of our approach in
solving a two-agent collaborative task.","['Ahana Ghosh', 'Sebastian Tschiatschek', 'Hamed Mahdavi', 'Adish Singla']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-05 21:04:27+00:00
http://arxiv.org/abs/1910.02321v1,The Impact of Data Preparation on the Fairness of Software Systems,"Machine learning models are widely adopted in scenarios that directly affect
people. The development of software systems based on these models raises
societal and legal concerns, as their decisions may lead to the unfair
treatment of individuals based on attributes like race or gender. Data
preparation is key in any machine learning pipeline, but its effect on fairness
is yet to be studied in detail. In this paper, we evaluate how the fairness and
effectiveness of the learned models are affected by the removal of the
sensitive attribute, the encoding of the categorical attributes, and instance
selection methods (including cross-validators and random undersampling). We
used the Adult Income and the German Credit Data datasets, which are widely
studied and known to have fairness concerns. We applied each data preparation
technique individually to analyse the difference in predictive performance and
fairness, using statistical parity difference, disparate impact, and the
normalised prejudice index. The results show that fairness is affected by
transformations made to the training data, particularly in imbalanced datasets.
Removing the sensitive attribute is insufficient to eliminate all the
unfairness in the predictions, as expected, but it is key to achieve fairer
models. Additionally, the standard random undersampling with respect to the
true labels is sometimes more prejudicial than performing no random
undersampling.","['In√™s Valentim', 'Nuno Louren√ßo', 'Nuno Antunes']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-05 19:50:16+00:00
http://arxiv.org/abs/1910.02304v2,Multiplierless and Sparse Machine Learning based on Margin Propagation Networks,"The new generation of machine learning processors have evolved from
multi-core and parallel architectures that were designed to efficiently
implement matrix-vector-multiplications (MVMs). This is because at the
fundamental level, neural network and machine learning operations extensively
use MVM operations and hardware compilers exploit the inherent parallelism in
MVM operations to achieve hardware acceleration on GPUs and FPGAs. However,
many IoT and edge computing platforms require embedded ML devices close to the
network in order to compensate for communication cost and latency. Hence a
natural question to ask is whether MVM operations are even necessary to
implement ML algorithms and whether simpler hardware primitives can be used to
implement an ultra-energy-efficient ML processor/architecture. In this paper we
propose an alternate hardware-software codesign of ML and neural network
architectures where instead of using MVM operations and non-linear activation
functions, the architecture only uses simple addition and thresholding
operations to implement inference and learning. At the core of the proposed
approach is margin-propagation (MP) based computation that maps multiplications
into additions and additions into a dynamic rectifying-linear-unit (ReLU)
operations. This mapping results in significant improvement in computational
and hence energy cost. In this paper, we show how the MP network formulation
can be applied for designing linear classifiers, shallow multi-layer
perceptrons and support vector networks suitable fot IoT platforms and tiny ML
applications. We show that these MP based classifiers give comparable results
to that of their traditional counterparts for benchmark UCI datasets, with the
added advantage of reduction in computational complexity enabling an
improvement in energy efficiency.","['Nazreen P. M.', 'Shantanu Chakrabartty', 'Chetan Singh Thakur']","['cs.LG', 'stat.ML']",2019-10-05 18:09:57+00:00
http://arxiv.org/abs/1910.02301v1,Change Detection in Noisy Dynamic Networks: A Spectral Embedding Approach,"Change detection in dynamic networks is an important problem in many areas,
such as fraud detection, cyber intrusion detection and health care monitoring.
It is a challenging problem because it involves a time sequence of graphs, each
of which is usually very large and sparse with heterogeneous vertex degrees,
resulting in a complex, high dimensional mathematical object. Spectral
embedding methods provide an effective way to transform a graph to a lower
dimensional latent Euclidean space that preserves the underlying structure of
the network. Although change detection methods that use spectral embedding are
available, they do not address sparsity and degree heterogeneity that usually
occur in noisy real-world graphs and a majority of these methods focus on
changes in the behaviour of the overall network.
  In this paper, we adapt previously developed techniques in spectral graph
theory and propose a novel concept of applying Procrustes techniques to
embedded points for vertices in a graph to detect changes in entity behaviour.
Our spectral embedding approach not only addresses sparsity and degree
heterogeneity issues, but also obtains an estimate of the appropriate embedding
dimension. We call this method CDP (change detection using Procrustes
analysis). We demonstrate the performance of CDP through extensive simulation
experiments and a real-world application. CDP successfully detects various
types of vertex-based changes including (i) changes in vertex degree, (ii)
changes in community membership of vertices, and (iii) unusual increase or
decrease in edge weight between vertices. The change detection performance of
CDP is compared with two other baseline methods that employ alternative
spectral embedding approaches. In both cases, CDP generally shows superior
performance.","['Isuru Udayangani Hewapathirana', 'Dominic Lee', 'Elena Moltchanova', 'Jeanette McLeod']","['cs.LG', 'stat.CO', 'stat.ML', 'G.3; I.5.3; I.6.5']",2019-10-05 18:02:18+00:00
http://arxiv.org/abs/1910.02290v1,Few-shot tweet detection in emerging disaster events,"Social media sources can provide crucial information in crisis situations,
but discovering relevant messages is not trivial. Methods have so far focused
on universal detection models for all kinds of crises or for certain crisis
types (e.g. floods). Event-specific models could implement a more focused
search area, but collecting data and training new models for a crisis that is
already in progress is costly and may take too much time for a prompt response.
As a compromise, manually collecting a small amount of example messages is
feasible. Few-shot models can generalize to unseen classes with such a small
handful of examples, and do not need be trained anew for each event. We compare
how few-shot approaches (matching networks and prototypical networks) perform
for this task. Since this is essentially a one-class problem, we also
demonstrate how a modified one-class version of prototypical models can be used
for this application.",['Anna Kruspe'],"['cs.LG', 'cs.CL', 'cs.CY', 'cs.SI', 'stat.ML']",2019-10-05 16:25:56+00:00
http://arxiv.org/abs/1910.02249v1,Characterizing Membership Privacy in Stochastic Gradient Langevin Dynamics,"Bayesian deep learning is recently regarded as an intrinsic way to
characterize the weight uncertainty of deep neural networks~(DNNs). Stochastic
Gradient Langevin Dynamics~(SGLD) is an effective method to enable Bayesian
deep learning on large-scale datasets. Previous theoretical studies have shown
various appealing properties of SGLD, ranging from the convergence properties
to the generalization bounds. In this paper, we study the properties of SGLD
from a novel perspective of membership privacy protection (i.e., preventing the
membership attack). The membership attack, which aims to determine whether a
specific sample is used for training a given DNN model, has emerged as a common
threat against deep learning algorithms. To this end, we build a theoretical
framework to analyze the information leakage (w.r.t. the training dataset) of a
model trained using SGLD. Based on this framework, we demonstrate that SGLD can
prevent the information leakage of the training dataset to a certain extent.
Moreover, our theoretical analysis can be naturally extended to other types of
Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods. Empirical
results on different datasets and models verify our theoretical findings and
suggest that the SGLD algorithm can not only reduce the information leakage but
also improve the generalization ability of the DNN models in real-world
applications.","['Bingzhe Wu', 'Chaochao Chen', 'Shiwan Zhao', 'Cen Chen', 'Yuan Yao', 'Guangyu Sun', 'Li Wang', 'Xiaolu Zhang', 'Jun Zhou']","['cs.LG', 'stat.ML']",2019-10-05 11:26:54+00:00
http://arxiv.org/abs/1910.02217v1,A Novel Graphical Lasso based approach towards Segmentation Analysis in Energy Game-Theoretic Frameworks,"Energy game-theoretic frameworks have emerged to be a successful strategy to
encourage energy efficient behavior in large scale by leveraging
human-in-the-loop strategy. A number of such frameworks have been introduced
over the years which formulate the energy saving process as a competitive game
with appropriate incentives for energy efficient players. However, prior works
involve an incentive design mechanism which is dependent on knowledge of
utility functions for all the players in the game, which is hard to compute
especially when the number of players is high, common in energy game-theoretic
frameworks. Our research proposes that the utilities of players in such a
framework can be grouped together to a relatively small number of clusters, and
the clusters can then be targeted with tailored incentives. The key to above
segmentation analysis is to learn the features leading to human decision making
towards energy usage in competitive environments. We propose a novel graphical
lasso based approach to perform such segmentation, by studying the feature
correlations in a real-world energy social game dataset. To further improve the
explainability of the model, we perform causality study using grangers
causality. Proposed segmentation analysis results in characteristic clusters
demonstrating different energy usage behaviors. We also present avenues to
implement intelligent incentive design using proposed segmentation method.","['Hari Prasanna Das', 'Ioannis C. Konstantakopoulos', 'Aummul Baneen Manasawala', 'Tanya Veeravalli', 'Huihan Liu', 'Costas J. Spanos']","['cs.LG', 'stat.ML']",2019-10-05 06:04:40+00:00
http://arxiv.org/abs/1910.02208v4,Striving for Simplicity and Performance in Off-Policy DRL: Output Normalization and Non-Uniform Sampling,"We aim to develop off-policy DRL algorithms that not only exceed
state-of-the-art performance but are also simple and minimalistic. For standard
continuous control benchmarks, Soft Actor-Critic (SAC), which employs entropy
maximization, currently provides state-of-the-art performance. We first
demonstrate that the entropy term in SAC addresses action saturation due to the
bounded nature of the action spaces, with this insight, we propose a
streamlined algorithm with a simple normalization scheme or with inverted
gradients. We show that both approaches can match SAC's sample efficiency
performance without the need of entropy maximization, we then propose a simple
non-uniform sampling method for selecting transitions from the replay buffer
during training. Extensive experimental results demonstrate that our proposed
sampling scheme leads to state of the art sample efficiency on challenging
continuous control tasks. We combine all of our findings into one simple
algorithm, which we call Streamlined Off Policy with Emphasizing Recent
Experience, for which we provide robust public-domain code.","['Che Wang', 'Yanqiu Wu', 'Quan Vuong', 'Keith Ross']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-05 04:22:35+00:00
http://arxiv.org/abs/1910.06078v2,MUTLA: A Large-Scale Dataset for Multimodal Teaching and Learning Analytics,"Automatic analysis of teacher and student interactions could be very
important to improve the quality of teaching and student engagement. However,
despite some recent progress in utilizing multimodal data for teaching and
learning analytics, a thorough analysis of a rich multimodal dataset coming for
a complex real learning environment has yet to be done. To bridge this gap, we
present a large-scale MUlti-modal Teaching and Learning Analytics (MUTLA)
dataset. This dataset includes time-synchronized multimodal data records of
students (learning logs, videos, EEG brainwaves) as they work in various
subjects from Squirrel AI Learning System (SAIL) to solve problems of varying
difficulty levels. The dataset resources include user records from the learner
records store of SAIL, brainwave data collected by EEG headset devices, and
video data captured by web cameras while students worked in the SAIL products.
Our hope is that by analyzing real-world student learning activities, facial
expressions, and brainwave patterns, researchers can better predict engagement,
which can then be used to improve adaptive learning selection and student
learning outcomes. An additional goal is to provide a dataset gathered from
real-world educational activities versus those from controlled lab environments
to benefit the educational learning community.","['Fangli Xu', 'Lingfei Wu', 'KP Thai', 'Carol Hsu', 'Wei Wang', 'Richard Tong']","['cs.CY', 'stat.ML']",2019-10-05 03:53:49+00:00
http://arxiv.org/abs/1910.02187v3,Dynamic Embedding on Textual Networks via a Gaussian Process,"Textual network embedding aims to learn low-dimensional representations of
text-annotated nodes in a graph. Prior work in this area has typically focused
on fixed graph structures; however, real-world networks are often dynamic. We
address this challenge with a novel end-to-end node-embedding model, called
Dynamic Embedding for Textual Networks with a Gaussian Process (DetGP). After
training, DetGP can be applied efficiently to dynamic graphs without
re-training or backpropagation. The learned representation of each node is a
combination of textual and structural embeddings. Because the structure is
allowed to be dynamic, our method uses the Gaussian process to take advantage
of its non-parametric properties. To use both local and global graph
structures, diffusion is used to model multiple hops between neighbors. The
relative importance of global versus local structure for the embeddings is
learned automatically. With the non-parametric nature of the Gaussian process,
updating the embeddings for a changed graph structure requires only a forward
pass through the learned model. Considering link prediction and node
classification, experiments demonstrate the empirical effectiveness of our
method compared to baseline approaches. We further show that DetGP can be
straightforwardly and efficiently applied to dynamic textual networks.","['Pengyu Cheng', 'Yitong Li', 'Xinyuan Zhang', 'Liqun Cheng', 'David Carlson', 'Lawrence Carin']","['cs.LG', 'cs.CL', 'cs.SI', 'stat.ML']",2019-10-05 01:16:33+00:00
http://arxiv.org/abs/1910.02182v2,On Tractable Computation of Expected Predictions,"Computing expected predictions of discriminative models is a fundamental task
in machine learning that appears in many interesting applications such as
fairness, handling missing values, and data analysis. Unfortunately, computing
expectations of a discriminative model with respect to a probability
distribution defined by an arbitrary generative model has been proven to be
hard in general. In fact, the task is intractable even for simple models such
as logistic regression and a naive Bayes distribution. In this paper, we
identify a pair of generative and discriminative models that enables tractable
computation of expectations, as well as moments of any order, of the latter
with respect to the former in case of regression. Specifically, we consider
expressive probabilistic circuits with certain structural constraints that
support tractable probabilistic inference. Moreover, we exploit the tractable
computation of high-order moments to derive an algorithm to approximate the
expectations for classification scenarios in which exact computations are
intractable. Our framework to compute expected predictions allows for handling
of missing data during prediction time in a principled and accurate way and
enables reasoning about the behavior of discriminative models. We empirically
show our algorithm to consistently outperform standard imputation techniques on
a variety of datasets. Finally, we illustrate how our framework can be used for
exploratory data analysis.","['Pasha Khosravi', 'YooJung Choi', 'Yitao Liang', 'Antonio Vergari', 'Guy Van den Broeck']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-05 00:20:41+00:00
http://arxiv.org/abs/1910.02176v1,Straight-Through Estimator as Projected Wasserstein Gradient Flow,"The Straight-Through (ST) estimator is a widely used technique for
back-propagating gradients through discrete random variables. However, this
effective method lacks theoretical justification. In this paper, we show that
ST can be interpreted as the simulation of the projected Wasserstein gradient
flow (pWGF). Based on this understanding, a theoretical foundation is
established to justify the convergence properties of ST. Further, another pWGF
estimator variant is proposed, which exhibits superior performance on
distributions with infinite support,e.g., Poisson distributions. Empirically,
we show that ST and our proposed estimator, while applied to different types of
discrete structures (including both Bernoulli and Poisson latent variables),
exhibit comparable or even better performances relative to other
state-of-the-art methods. Our results uncover the origin of the widespread
adoption of the ST estimator and represent a helpful step towards exploring
alternative gradient estimators for discrete variables.","['Pengyu Cheng', 'Chang Liu', 'Chunyuan Li', 'Dinghan Shen', 'Ricardo Henao', 'Lawrence Carin']","['cs.LG', 'stat.ML']",2019-10-05 00:06:34+00:00
http://arxiv.org/abs/1910.02175v3,Pi-PE: A Pipeline for Pulmonary Embolism Detection using Sparsely Annotated 3D CT Images,"Pulmonary embolisms (PE) are known to be one of the leading causes for
cardiac-related mortality. Due to inherent variabilities in how PE manifests
and the cumbersome nature of manual diagnosis, there is growing interest in
leveraging AI tools for detecting PE. In this paper, we build a two-stage
detection pipeline that is accurate, computationally efficient, robust to
variations in PE types and kernels used for CT reconstruction, and most
importantly, does not require dense annotations. Given the challenges in
acquiring expert annotations in large-scale datasets, our approach produces
state-of-the-art results with very sparse emboli contours (at 10mm slice
spacing), while using models with significantly lower number of parameters. We
achieve AUC scores of 0.94 on the validation set and 0.85 on the test set of
highly severe PEs. Using a large, real-world dataset characterized by complex
PE types and patients from multiple hospitals, we present an elaborate
empirical study and provide guidelines for designing highly generalizable
pipelines.","['Deepta Rajan', 'David Beymer', 'Shafiqul Abedin', 'Ehsan Dehghan']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2019-10-05 00:01:28+00:00
http://arxiv.org/abs/1910.02170v3,Donor's Deferral and Return Behavior: Partial Identification from a Regression Discontinuity Design with Manipulation,"Volunteer labor can temporarily yield lower benefits to charities than its
costs. In such instances, organizations may wish to defer volunteer donations
to a later date. Exploiting a discontinuity in blood donations' eligibility
criteria, we show that deferring donors reduces their future volunteerism. In
our setting, medical staff manipulates donors' reported hemoglobin levels over
a threshold to facilitate donation. Such manipulation invalidates standard
regression discontinuity design. To circumvent this issue, we propose a
procedure for obtaining partial identification bounds where manipulation is
present. Our procedure is applicable in various regression discontinuity
settings where the running variable is manipulated and discrete.","['Evan Rosenman', 'Karthik Rajkumar', 'Romain Gauriot', 'Robert Slonim']","['stat.ME', 'stat.AP', 'stat.ML']",2019-10-04 23:32:20+00:00
http://arxiv.org/abs/1910.02160v2,Variable Selection with Random Survival Forest and Bayesian Additive Regression Tree for Survival Data,"In this paper we utilize a survival analysis methodology incorporating
Bayesian additive regression trees to account for nonlinear and additive
covariate effects. We compare the performance of Bayesian additive regression
trees, Cox proportional hazards and random survival forests models for censored
survival data, using simulation studies and survival analysis for breast cancer
with U.S. SEER database for the year 2005. In simulation studies, we compare
the three models across varying sample sizes and censoring rates on the basis
of bias and prediction accuracy. In survival analysis for breast cancer, we
retrospectively analyze a subset of 1500 patients having invasive ductal
carcinoma that is a common form of breast cancer mostly affecting older woman.
Predictive potential of the three models are then compared using some widely
used performance assessment measures in survival literature.","['Satabdi Saha', 'Duchwan Ryu', 'Nader Ebrahimi']","['stat.AP', 'stat.ML']",2019-10-04 22:18:17+00:00
http://arxiv.org/abs/1910.02155v1,The Sparse Reverse of Principal Component Analysis for Fast Low-Rank Matrix Completion,"Matrix completion constantly receives tremendous attention from many research
fields. It is commonly applied for recommender systems such as movie ratings,
computer vision such as image reconstruction or completion, multi-task learning
such as collaboratively modeling time-series trends of multiple sensors, and
many other applications. Matrix completion techniques are usually
computationally exhaustive and/or fail to capture the heterogeneity in the
data. For example, images usually contain a heterogeneous set of objects, and
thus it is a challenging task to reconstruct images with high levels of missing
data. In this paper, we propose the sparse reverse of principal component
analysis for matrix completion. The proposed approach maintains smoothness
across the matrix, produces accurate estimates of the missing data, converges
iteratively, and it is computationally tractable with a controllable upper
bound on the number of iterations until convergence. The accuracy of the
proposed technique is validated on natural images, movie ratings, and
multisensor data. It is also compared with common benchmark methods used for
matrix completion.","['Abdallah Chehade', 'Zunya Shi']","['cs.LG', 'eess.SP', 'stat.ML']",2019-10-04 21:44:55+00:00
http://arxiv.org/abs/1910.02150v2,Tensor-based algorithms for image classification,"The interest in machine learning with tensor networks has been growing
rapidly in recent years. We show that tensor-based methods developed for
learning the governing equations of dynamical systems from data can, in the
same way, be used for supervised learning problems and propose two novel
approaches for image classification. One is a kernel-based reformulation of the
previously introduced MANDy (multidimensional approximation of nonlinear
dynamics), the other an alternating ridge regression in the tensor-train
format. We apply both methods to the MNIST and fashion MNIST data set and show
that the approaches are competitive with state-of-the-art neural network-based
classifiers.","['Stefan Klus', 'Patrick Gel√ü']","['cs.LG', 'cs.CV', 'stat.ML']",2019-10-04 21:16:33+00:00
http://arxiv.org/abs/1910.03487v1,Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents,"Data availability is a bottleneck during early stages of development of new
capabilities for intelligent artificial agents. We investigate the use of text
generation techniques to augment the training data of a popular commercial
artificial agent across categories of functionality, with the goal of faster
development of new functionality. We explore a variety of encoder-decoder
generative models for synthetic training data generation and propose using
conditional variational auto-encoders. Our approach requires only direct
optimization, works well with limited data and significantly outperforms the
previous controlled text generation techniques. Further, the generated data are
used as additional training samples in an extrinsic intent classification task,
leading to improved performance by up to 5\% absolute f-score in low-resource
cases, validating the usefulness of our approach.","['Nikolaos Malandrakis', 'Minmin Shen', 'Anuj Goyal', 'Shuyang Gao', 'Abhishek Sethi', 'Angeliki Metallinou']","['cs.CL', 'cs.LG', 'stat.ML']",2019-10-04 20:44:21+00:00
http://arxiv.org/abs/1910.02136v2,Risks of Using Non-verified Open Data: A case study on using Machine Learning techniques for predicting Pregnancy Outcomes in India,"Artificial intelligence (AI) has evolved considerably in the last few years.
While applications of AI is now becoming more common in fields like retail and
marketing, application of AI in solving problems related to developing
countries is still an emerging topic. Specially, AI applications in
resource-poor settings remains relatively nascent. There is a huge scope of AI
being used in such settings. For example, researchers have started exploring AI
applications to reduce poverty and deliver a broad range of critical public
services. However, despite many promising use cases, there are many dataset
related challenges that one has to overcome in such projects. These challenges
often take the form of missing data, incorrectly collected data and improperly
labeled variables, among other factors. As a result, we can often end up using
data that is not representative of the problem we are trying to solve. In this
case study, we explore the challenges of using such an open dataset from India,
to predict an important health outcome. We highlight how the use of AI without
proper understanding of reporting metrics can lead to erroneous conclusions.","['Anusua Trivedi', 'Sumit Mukherjee', 'Edmund Tse', 'Anne Ewing', 'Juan Lavista Ferres']","['cs.LG', 'cs.AI', 'stat.ML']",2019-10-04 20:27:20+00:00
http://arxiv.org/abs/1910.02133v1,A Conditional Generative Model for Predicting Material Microstructures from Processing Methods,"Microstructures of a material form the bridge linking processing conditions -
which can be controlled, to the material property - which is the primary
interest in engineering applications. Thus a critical task in material design
is establishing the processing-structure relationship, which requires domain
expertise and techniques that can model the high-dimensional material
microstructure. This work proposes a deep learning based approach that models
the processing-structure relationship as a conditional image synthesis problem.
In particular, we develop an auxiliary classifier Wasserstein GAN with gradient
penalty (ACWGAN-GP) to synthesize microstructures under a given processing
condition. This approach is free of feature engineering, requires modest domain
knowledge and is applicable to a wide range of material systems. We demonstrate
this approach using the ultra high carbon steel (UHCS) database, where each
microstructure is annotated with a label describing the cooling method it was
subjected to. Our results show that ACWGAN-GP can synthesize high-quality
multiphase microstructures for a given cooling method.","['Akshay Iyer', 'Biswadip Dey', 'Arindam Dasgupta', 'Wei Chen', 'Amit Chakraborty']","['eess.IV', 'cond-mat.mtrl-sci', 'cs.LG', 'stat.ML']",2019-10-04 20:13:11+00:00
http://arxiv.org/abs/1910.02120v7,Distributed Learning of Deep Neural Networks using Independent Subnet Training,"Distributed machine learning (ML) can bring more computational resources to
bear than single-machine learning, thus enabling reductions in training time.
Distributed learning partitions models and data over many machines, allowing
model and dataset sizes beyond the available compute power and memory of a
single machine. In practice though, distributed ML is challenging when
distribution is mandatory, rather than chosen by the practitioner. In such
scenarios, data could unavoidably be separated among workers due to limited
memory capacity per worker or even because of data privacy issues. There,
existing distributed methods will utterly fail due to dominant transfer costs
across workers, or do not even apply.
  We propose a new approach to distributed fully connected neural network
learning, called independent subnet training (IST), to handle these cases. In
IST, the original network is decomposed into a set of narrow subnetworks with
the same depth. These subnetworks are then trained locally before parameters
are exchanged to produce new subnets and the training cycle repeats. Such a
naturally ""model parallel"" approach limits memory usage by storing only a
portion of network parameters on each device. Additionally, no requirements
exist for sharing data between workers (i.e., subnet training is local and
independent) and communication volume and frequency are reduced by decomposing
the original network into independent subnets. These properties of IST can cope
with issues due to distributed data, slow interconnects, or limited device
memory, making IST a suitable approach for cases of mandatory distribution. We
show experimentally that IST results in training times that are much lower than
common distributed learning approaches.","['Binhang Yuan', 'Cameron R. Wolfe', 'Chen Dun', 'Yuxin Tang', 'Anastasios Kyrillidis', 'Christopher M. Jermaine']","['cs.LG', 'stat.ML']",2019-10-04 19:46:16+00:00
