id,title,abstract,authors,categories,date
http://arxiv.org/abs/2411.06276v1,Multi-View Majority Vote Learning Algorithms: Direct Minimization of PAC-Bayesian Bounds,"The PAC-Bayesian framework has significantly advanced our understanding of
statistical learning, particularly in majority voting methods. However, its
application to multi-view learning remains underexplored. In this paper, we
extend PAC-Bayesian theory to the multi-view setting, introducing novel
PAC-Bayesian bounds based on R\'enyi divergence. These bounds improve upon
traditional Kullback-Leibler divergence and offer more refined complexity
measures. We further propose first and second-order oracle PAC-Bayesian bounds,
along with an extension of the C-bound for multi-view learning. To ensure
practical applicability, we develop efficient optimization algorithms with
self-bounding properties.","['Mehdi Hennequin', 'Abdelkrim Zitouni', 'Khalid Benabdeslem', 'Haytham Elghazel', 'Yacine Gaci']","['cs.LG', 'cs.AI', 'stat.ML']",2024-11-09 20:25:47+00:00
http://arxiv.org/abs/2411.06225v1,RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks,"Parallel-in-time (PinT) techniques have been proposed to solve systems of
time-dependent differential equations by parallelizing the temporal domain.
Among them, Parareal computes the solution sequentially using an inaccurate
(fast) solver, and then ""corrects"" it using an accurate (slow) integrator that
runs in parallel across temporal subintervals. This work introduces
RandNet-Parareal, a novel method to learn the discrepancy between the coarse
and fine solutions using random neural networks (RandNets). RandNet-Parareal
achieves speed gains up to x125 and x22 compared to the fine solver run
serially and Parareal, respectively. Beyond theoretical guarantees of RandNets
as universal approximators, these models are quick to train, allowing the PinT
solution of partial differential equations on a spatial mesh of up to $10^5$
points with minimal overhead, dramatically increasing the scalability of
existing PinT approaches. RandNet-Parareal's numerical performance is
illustrated on systems of real-world significance, such as the viscous Burgers'
equation, the Diffusion-Reaction equation, the two- and three-dimensional
Brusselator, and the shallow water equation.","['Guglielmo Gattiglio', 'Lyudmila Grigoryeva', 'Massimiliano Tamborrino']","['stat.CO', 'cs.DC', 'cs.NA', 'math.NA', 'stat.ML', '68T07, 68T09, 65Y05, 65M55, 65M22, 65L05']",2024-11-09 16:10:26+00:00
http://arxiv.org/abs/2411.06200v1,Weak to Strong Learning from Aggregate Labels,"In learning from aggregate labels, the training data consists of sets or
""bags"" of feature-vectors (instances) along with an aggregate label for each
bag derived from the (usually {0,1}-valued) labels of its instances. In
learning from label proportions (LLP), the aggregate label is the average of
the bag's instance labels, whereas in multiple instance learning (MIL) it is
the OR. The goal is to train an instance-level predictor, typically achieved by
fitting a model on the training data, in particular one that maximizes the
accuracy which is the fraction of satisfied bags i.e., those on which the
predicted labels are consistent with the aggregate label. A weak learner has at
a constant accuracy < 1 on the training bags, while a strong learner's accuracy
can be arbitrarily close to 1. We study the problem of using a weak learner on
such training bags with aggregate labels to obtain a strong learner, analogous
to supervised learning for which boosting algorithms are known. Our first
result shows the impossibility of boosting in LLP using weak classifiers of any
accuracy < 1 by constructing a collection of bags for which such weak learners
(for any weight assignment) exist, while not admitting any strong learner. A
variant of this construction also rules out boosting in MIL for a non-trivial
range of weak learner accuracy. In the LLP setting however, we show that a weak
learner (with small accuracy) on large enough bags can in fact be used to
obtain a strong learner for small bags, in polynomial time. We also provide
more efficient, sampling based variant of our procedure with probabilistic
guarantees which are empirically validated on three real and two synthetic
datasets. Our work is the first to theoretically study weak to strong learning
from aggregate labels, with an algorithm to achieve the same for LLP, while
proving the impossibility of boosting for both LLP and MIL.","['Yukti Makhija', 'Rishi Saket']","['cs.LG', 'cs.DS', 'stat.ML']",2024-11-09 14:56:09+00:00
http://arxiv.org/abs/2411.06192v1,Variational Bayes Portfolio Construction,"Portfolio construction is the science of balancing reward and risk; it is at
the core of modern finance. In this paper, we tackle the question of optimal
decision-making within a Bayesian paradigm, starting from a decision-theoretic
formulation. Despite the inherent intractability of the optimal decision in any
interesting scenarios, we manage to rewrite it as a saddle-point problem.
Leveraging the literature on variational Bayes (VB), we propose a relaxation of
the original problem. This novel methodology results in an efficient algorithm
that not only performs well but is also provably convergent. Furthermore, we
provide theoretical results on the statistical consistency of the resulting
decision with the optimal Bayesian decision. Using real data, our proposal
significantly enhances the speed and scalability of portfolio selection
problems. We benchmark our results against state-of-the-art algorithms, as well
as a Monte Carlo algorithm targeting the optimal decision.","['Nicolas Nguyen', 'James Ridgway', 'Claire Vernade']","['stat.AP', 'stat.ML']",2024-11-09 14:23:14+00:00
http://arxiv.org/abs/2411.06184v1,Alleviating Hyperparameter-Tuning Burden in SVM Classifiers for Pulmonary Nodules Diagnosis with Multi-Task Bayesian Optimization,"In the field of non-invasive medical imaging, radiomic features are utilized
to measure tumor characteristics. However, these features can be affected by
the techniques used to discretize the images, ultimately impacting the accuracy
of diagnosis. To investigate the influence of various image discretization
methods on diagnosis, it is common practice to evaluate multiple discretization
strategies individually. This approach often leads to redundant and
time-consuming tasks such as training predictive models and fine-tuning
hyperparameters separately. This study examines the feasibility of employing
multi-task Bayesian optimization to accelerate the hyperparameters search for
classifying benign and malignant pulmonary nodules using RBF SVM. Our findings
suggest that multi-task Bayesian optimization significantly accelerates the
search for hyperparameters in comparison to a single-task approach. To the best
of our knowledge, this is the first investigation to utilize multi-task
Bayesian optimization in a critical medical context.","['Wenhao Chi', 'Haiping Liu', 'Hongqiao Dong', 'Wenhua Liang', 'Bo Liu']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2024-11-09 13:52:06+00:00
http://arxiv.org/abs/2411.06140v1,Deep Nonparametric Conditional Independence Tests for Images,"Conditional independence tests (CITs) test for conditional dependence between
random variables. As existing CITs are limited in their applicability to
complex, high-dimensional variables such as images, we introduce deep
nonparametric CITs (DNCITs). The DNCITs combine embedding maps, which extract
feature representations of high-dimensional variables, with nonparametric CITs
applicable to these feature representations. For the embedding maps, we derive
general properties on their parameter estimators to obtain valid DNCITs and
show that these properties include embedding maps learned through (conditional)
unsupervised or transfer learning. For the nonparametric CITs, appropriate
tests are selected and adapted to be applicable to feature representations.
Through simulations, we investigate the performance of the DNCITs for different
embedding maps and nonparametric CITs under varying confounder dimensions and
confounder relationships. We apply the DNCITs to brain MRI scans and behavioral
traits, given confounders, of healthy individuals from the UK Biobank (UKB),
confirming null results from a number of ambiguous personality neuroscience
studies with a larger data set and with our more powerful tests. In addition,
in a confounder control study, we apply the DNCITs to brain MRI scans and a
confounder set to test for sufficient confounder control, leading to a
potential reduction in the confounder dimension under improved confounder
control compared to existing state-of-the-art confounder control studies for
the UKB. Finally, we provide an R package implementing the DNCITs.","['Marco Simnacher', 'Xiangnan Xu', 'Hani Park', 'Christoph Lippert', 'Sonja Greven']","['stat.ML', 'cs.LG', 'eess.IV', 'math.ST', 'stat.ME', 'stat.TH']",2024-11-09 10:33:04+00:00
http://arxiv.org/abs/2411.06100v1,Mutual-energy inner product optimization method for constructing feature coordinates and image classification in Machine Learning,"As a key task in machine learning, data classification is essentially to find
a suitable coordinate system to represent data features of different classes of
samples. This paper proposes the mutual-energy inner product optimization
method for constructing a feature coordinate system. First, by analyzing the
solution space and eigenfunctions of partial differential equations describing
a non-uniform membrane, the mutual-energy inner product is defined. Second, by
expressing the mutual-energy inner product as a series of eigenfunctions, it
shows a significant advantage of enhancing low-frequency features and
suppressing high-frequency noise, compared with the Euclidean inner product.
And then, a mutual-energy inner product optimization model is built to extract
data features, and convexity and concavity properties of its objective function
are discussed. Next, by combining the finite element method, a stable and
efficient sequential linearization algorithm is constructed to solve the
optimization model. This algorithm only solves equations including positive
definite symmetric matrix and linear programming with a few constraints, and
its vectorized implementation is discussed. Finally, the mutual-energy inner
product optimization method is used to construct feature coordinates, and
multi-class Gaussian classifiers are trained on the MINST training set. Good
prediction results of Gaussian classifiers are achieved on the MINST test set.",['Yuanxiu Wang'],"['cs.LG', 'stat.ML']",2024-11-09 07:26:03+00:00
http://arxiv.org/abs/2411.06069v1,Model Selection for Average Reward RL with Application to Utility Maximization in Repeated Games,"In standard RL, a learner attempts to learn an optimal policy for a Markov
Decision Process whose structure (e.g. state space) is known. In online model
selection, a learner attempts to learn an optimal policy for an MDP knowing
only that it belongs to one of $M >1$ model classes of varying complexity.
Recent results have shown that this can be feasibly accomplished in episodic
online RL. In this work, we propose $\mathsf{MRBEAR}$, an online model
selection algorithm for the average reward RL setting. The regret of the
algorithm is in $\tilde O(M C_{m^*}^2 \mathsf{B}_{m^*}(T,\delta))$ where
$C_{m^*}$ represents the complexity of the simplest well-specified model class
and $\mathsf{B}_{m^*}(T,\delta)$ is its corresponding regret bound. This result
shows that in average reward RL, like the episodic online RL, the additional
cost of model selection scales only linearly in $M$, the number of model
classes. We apply $\mathsf{MRBEAR}$ to the interaction between a learner and an
opponent in a two-player simultaneous general-sum repeated game, where the
opponent follows a fixed unknown limited memory strategy. The learner's goal is
to maximize its utility without knowing the opponent's utility function. The
interaction is over $T$ rounds with no episode or discounting which leads us to
measure the learner's performance by average reward regret. In this
application, our algorithm enjoys an opponent-complexity-dependent regret in
$\tilde O(M(\mathsf{sp}(h^*) B^{m^*} A^{m^*+1})^{\frac{3}{2}} \sqrt{T})$, where
$m^*\le M$ is the unknown memory limit of the opponent, $\mathsf{sp}(h^*)$ is
the unknown span of optimal bias induced by the opponent, and $A$ and $B$ are
the number of actions for the learner and opponent respectively. We also show
that the exponential dependency on $m^*$ is inevitable by proving a lower bound
on the learner's regret.","['Alireza Masoumian', 'James R. Wright']","['cs.LG', 'cs.GT', 'stat.ML']",2024-11-09 05:03:10+00:00
http://arxiv.org/abs/2411.06056v1,Learning Mixtures of Experts with EM,"Mixtures of Experts (MoE) are Machine Learning models that involve
partitioning the input space, with a separate ""expert"" model trained on each
partition. Recently, MoE have become popular as components in today's large
language models as a means to reduce training and inference costs. There, the
partitioning function and the experts are both learnt jointly via gradient
descent on the log-likelihood. In this paper we focus on studying the
efficiency of the Expectation Maximization (EM) algorithm for the training of
MoE models. We first rigorously analyze EM for the cases of linear or logistic
experts, where we show that EM is equivalent to Mirror Descent with unit step
size and a Kullback-Leibler Divergence regularizer. This perspective allows us
to derive new convergence results and identify conditions for local linear
convergence based on the signal-to-noise ratio (SNR). Experiments on synthetic
and (small-scale) real-world data show that EM outperforms the gradient descent
algorithm both in terms of convergence rate and the achieved accuracy.","['Quentin Fruytier', 'Aryan Mokhtari', 'Sujay Sanghavi']","['cs.LG', 'stat.ML']",2024-11-09 03:44:09+00:00
http://arxiv.org/abs/2411.05998v1,Filling in Missing FX Implied Volatilities with Uncertainties: Improving VAE-Based Volatility Imputation,"Missing data is a common problem in finance and often requires methods to
fill in the gaps, or in other words, imputation. In this work, we focused on
the imputation of missing implied volatilities for FX options. Prior work has
used variational autoencoders (VAEs), a neural network-based approach, to solve
this problem; however, using stronger classical baselines such as Heston with
jumps can significantly outperform their results. We show that simple
modifications to the architecture of the VAE lead to significant imputation
performance improvements (e.g., in low missingness regimes, nearly cutting the
error by half), removing the necessity of using $\beta$-VAEs. Further, we
modify the VAE imputation algorithm in order to better handle the uncertainty
in data, as well as to obtain accurate uncertainty estimates around imputed
values.",['Achintya Gopal'],"['q-fin.ST', 'cs.LG', 'stat.ML']",2024-11-08 22:30:28+00:00
http://arxiv.org/abs/2411.05979v1,Variance-Aware Linear UCB with Deep Representation for Neural Contextual Bandits,"By leveraging the representation power of deep neural networks, neural upper
confidence bound (UCB) algorithms have shown success in contextual bandits. To
further balance the exploration and exploitation, we propose
Neural-$\sigma^2$-LinearUCB, a variance-aware algorithm that utilizes
$\sigma^2_t$, i.e., an upper bound of the reward noise variance at round $t$,
to enhance the uncertainty quantification quality of the UCB, resulting in a
regret performance improvement. We provide an oracle version for our algorithm
characterized by an oracle variance upper bound $\sigma^2_t$ and a practical
version with a novel estimation for this variance bound. Theoretically, we
provide rigorous regret analysis for both versions and prove that our oracle
algorithm achieves a better regret guarantee than other neural-UCB algorithms
in the neural contextual bandits setting. Empirically, our practical method
enjoys a similar computational efficiency, while outperforming state-of-the-art
techniques by having a better calibration and lower regret across multiple
standard settings, including on the synthetic, UCI, MNIST, and CIFAR-10
datasets.","['Ha Manh Bui', 'Enrique Mallada', 'Anqi Liu']","['cs.LG', 'stat.ML']",2024-11-08 21:24:14+00:00
http://arxiv.org/abs/2411.05923v1,DNAMite: Interpretable Calibrated Survival Analysis with Discretized Additive Models,"Survival analysis is a classic problem in statistics with important
applications in healthcare. Most machine learning models for survival analysis
are black-box models, limiting their use in healthcare settings where
interpretability is paramount. More recently, glass-box machine learning models
have been introduced for survival analysis, with both strong predictive
performance and interpretability. Still, several gaps remain, as no prior
glass-box survival model can produce calibrated shape functions with enough
flexibility to capture the complex patterns often found in real data. To fill
this gap, we introduce a new glass-box machine learning model for survival
analysis called DNAMite. DNAMite uses feature discretization and kernel
smoothing in its embedding module, making it possible to learn shape functions
with a flexible balance of smoothness and jaggedness. Further, DNAMite produces
calibrated shape functions that can be directly interpreted as contributions to
the cumulative incidence function. Our experiments show that DNAMite generates
shape functions closer to true shape functions on synthetic data, while making
predictions with comparable predictive performance and better calibration than
previous glass-box and black-box models.","['Mike Van Ness', 'Billy Block', 'Madeleine Udell']","['cs.LG', 'stat.ML']",2024-11-08 19:13:30+00:00
http://arxiv.org/abs/2411.05750v1,On Differentially Private String Distances,"Given a database of bit strings $A_1,\ldots,A_m\in \{0,1\}^n$, a fundamental
data structure task is to estimate the distances between a given query $B\in
\{0,1\}^n$ with all the strings in the database. In addition, one might further
want to ensure the integrity of the database by releasing these distance
statistics in a secure manner. In this work, we propose differentially private
(DP) data structures for this type of tasks, with a focus on Hamming and edit
distance. On top of the strong privacy guarantees, our data structures are also
time- and space-efficient. In particular, our data structure is $\epsilon$-DP
against any sequence of queries of arbitrary length, and for any query $B$ such
that the maximum distance to any string in the database is at most $k$, we
output $m$ distance estimates. Moreover,
  - For Hamming distance, our data structure answers any query in $\widetilde
O(mk+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/\log k})$;
  - For edit distance, our data structure answers any query in $\widetilde
O(mk^2+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/(\log k \log n)})$.
  For moderate $k$, both data structures support sublinear query operations. We
obtain these results via a novel adaptation of the randomized response
technique as a bit flipping procedure, applied to the sketched strings.","['Jerry Yao-Chieh Hu', 'Erzhi Liu', 'Han Liu', 'Zhao Song', 'Lichen Zhang']","['cs.DS', 'cs.AI', 'cs.CR', 'cs.LG', 'stat.ML']",2024-11-08 18:10:07+00:00
http://arxiv.org/abs/2411.05735v1,Aioli: A Unified Optimization Framework for Language Model Data Mixing,"Language model performance depends on identifying the optimal mixture of data
groups to train on (e.g., law, code, math). Prior work has proposed a diverse
set of methods to efficiently learn mixture proportions, ranging from fitting
regression models over training runs to dynamically updating proportions
throughout training. Surprisingly, we find that no existing method consistently
outperforms a simple stratified sampling baseline in terms of average test
perplexity per group. In this paper, we study the cause of this inconsistency
by unifying existing methods into a standard optimization framework. We show
that all methods set proportions to minimize total loss, subject to a
method-specific mixing law -- an assumption on how loss is a function of
mixture proportions. We find that existing parameterizations of mixing laws can
express the true loss-proportion relationship empirically, but the methods
themselves often set the mixing law parameters inaccurately, resulting in poor
and inconsistent performance. Finally, we leverage the insights from our
framework to derive a new online method named Aioli, which directly estimates
the mixing law parameters throughout training and uses them to dynamically
adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out
of 6 datasets by an average of 0.28 test perplexity points, whereas existing
methods fail to consistently beat stratified sampling, doing up to 6.9 points
worse. Moreover, in a practical setting where proportions are learned on
shorter runs due to computational constraints, Aioli can dynamically adjust
these proportions over the full training run, consistently improving
performance over existing methods by up to 12.01 test perplexity points.","['Mayee F. Chen', 'Michael Y. Hu', 'Nicholas Lourie', 'Kyunghyun Cho', 'Christopher Ré']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2024-11-08 17:50:24+00:00
http://arxiv.org/abs/2411.05729v1,Graph-Dictionary Signal Model for Sparse Representations of Multivariate Data,"Representing and exploiting multivariate signals require capturing complex
relations between variables. We define a novel Graph-Dictionary signal model,
where a finite set of graphs characterizes relationships in data distribution
through a weighted sum of their Laplacians. We propose a framework to infer the
graph dictionary representation from observed data, along with a bilinear
generalization of the primal-dual splitting algorithm to solve the learning
problem. Our new formulation allows to include a priori knowledge on signal
properties, as well as on underlying graphs and their coefficients. We show the
capability of our method to reconstruct graphs from signals in multiple
synthetic settings, where our model outperforms previous baselines. Then, we
exploit graph-dictionary representations in a motor imagery decoding task on
brain activity data, where we classify imagined motion better than standard
methods relying on many more features.","['William Cappelletti', 'Pascal Frossard']","['cs.LG', 'stat.ML']",2024-11-08 17:40:43+00:00
http://arxiv.org/abs/2411.05661v1,Multi-armed Bandits with Missing Outcome,"While significant progress has been made in designing algorithms that
minimize regret in online decision-making, real-world scenarios often introduce
additional complexities, perhaps the most challenging of which is missing
outcomes. Overlooking this aspect or simply assuming random missingness
invariably leads to biased estimates of the rewards and may result in linear
regret. Despite the practical relevance of this challenge, no rigorous
methodology currently exists for systematically handling missingness,
especially when the missingness mechanism is not random. In this paper, we
address this gap in the context of multi-armed bandits (MAB) with missing
outcomes by analyzing the impact of different missingness mechanisms on
achievable regret bounds. We introduce algorithms that account for missingness
under both missing at random (MAR) and missing not at random (MNAR) models.
Through both analytical and simulation studies, we demonstrate the drastic
improvements in decision-making by accounting for missingness in these
settings.","['Ilia Mahrooghi', 'Mahshad Moradi', 'Sina Akbari', 'Negar Kiyavash']","['stat.ML', 'cs.LG']",2024-11-08 16:02:39+00:00
http://arxiv.org/abs/2411.05625v1,Cross-validating causal discovery via Leave-One-Variable-Out,"We propose a new approach to falsify causal discovery algorithms without
ground truth, which is based on testing the causal model on a pair of variables
that has been dropped when learning the causal model. To this end, we use the
""Leave-One-Variable-Out (LOVO)"" prediction where $Y$ is inferred from $X$
without any joint observations of $X$ and $Y$, given only training data from
$X,Z_1,\dots,Z_k$ and from $Z_1,\dots,Z_k,Y$. We demonstrate that causal models
on the two subsets, in the form of Acyclic Directed Mixed Graphs (ADMGs), often
entail conclusions on the dependencies between $X$ and $Y$, enabling this type
of prediction. The prediction error can then be estimated since the joint
distribution $P(X, Y)$ is assumed to be available, and $X$ and $Y$ have only
been omitted for the purpose of falsification. After presenting this graphical
method, which is applicable to general causal discovery algorithms, we
illustrate how to construct a LOVO predictor tailored towards algorithms
relying on specific a priori assumptions, such as linear additive noise models.
Simulations indicate that the LOVO prediction error is indeed correlated with
the accuracy of the causal outputs, affirming the method's effectiveness.","['Daniela Schkoda', 'Philipp Faller', 'Patrick Blöbaum', 'Dominik Janzing']","['stat.ML', 'cs.LG', 'stat.ME']",2024-11-08 15:15:34+00:00
http://arxiv.org/abs/2411.05591v1,Network EM Algorithm for Gaussian Mixture Model in Decentralized Federated Learning,"We systematically study various network Expectation-Maximization (EM)
algorithms for the Gaussian mixture model within the framework of decentralized
federated learning. Our theoretical investigation reveals that directly
extending the classical decentralized supervised learning method to the EM
algorithm exhibits poor estimation accuracy with heterogeneous data across
clients and struggles to converge numerically when Gaussian components are
poorly-separated. To address these issues, we propose two novel solutions.
First, to handle heterogeneous data, we introduce a momentum network EM (MNEM)
algorithm, which uses a momentum parameter to combine information from both the
current and historical estimators. Second, to tackle the challenge of
poorly-separated Gaussian components, we develop a semi-supervised MNEM
(semi-MNEM) algorithm, which leverages partially labeled data. Rigorous
theoretical analysis demonstrates that MNEM can achieve statistical efficiency
comparable to that of the whole sample estimator when the mixture components
satisfy certain separation conditions, even in heterogeneous scenarios.
Moreover, the semi-MNEM estimator enhances the convergence speed of the MNEM
algorithm, effectively addressing the numerical convergence challenges in
poorly-separated scenarios. Extensive simulation and real data analyses are
conducted to justify our theoretical findings.","['Shuyuan Wu', 'Bin Du', 'Xuetong Li', 'Hansheng Wang']","['stat.ML', 'cs.LG']",2024-11-08 14:25:46+00:00
http://arxiv.org/abs/2411.05453v1,The sampling complexity of learning invertible residual neural networks,"In recent work it has been shown that determining a feedforward ReLU neural
network to within high uniform accuracy from point samples suffers from the
curse of dimensionality in terms of the number of samples needed. As a
consequence, feedforward ReLU neural networks are of limited use for
applications where guaranteed high uniform accuracy is required.
  We consider the question of whether the sampling complexity can be improved
by restricting the specific neural network architecture. To this end, we
investigate invertible residual neural networks which are foundational
architectures in deep learning and are widely employed in models that power
modern generative methods. Our main result shows that the residual neural
network architecture and invertibility do not help overcome the complexity
barriers encountered with simpler feedforward architectures. Specifically, we
demonstrate that the computational complexity of approximating invertible
residual neural networks from point samples in the uniform norm suffers from
the curse of dimensionality. Similar results are established for invertible
convolutional Residual neural networks.","['Yuanyuan Li', 'Philipp Grohs', 'Philipp Petersen']","['stat.ML', 'cs.LG']",2024-11-08 10:00:40+00:00
http://arxiv.org/abs/2411.05443v1,ClusterGraph: a new tool for visualization and compression of multidimensional data,"Understanding the global organization of complicated and high dimensional
data is of primary interest for many branches of applied sciences. It is
typically achieved by applying dimensionality reduction techniques mapping the
considered data into lower dimensional space. This family of methods, while
preserving local structures and features, often misses the global structure of
the dataset. Clustering techniques are another class of methods operating on
the data in the ambient space. They group together points that are similar
according to a fixed similarity criteria, however unlike dimensionality
reduction techniques, they do not provide information about the global
organization of the data. Leveraging ideas from Topological Data Analysis, in
this paper we provide an additional layer on the output of any clustering
algorithm. Such data structure, ClusterGraph, provides information about the
global layout of clusters, obtained from the considered clustering algorithm.
Appropriate measures are provided to assess the quality and usefulness of the
obtained representation. Subsequently the ClusterGraph, possibly with an
appropriate structure--preserving simplification, can be visualized and used in
synergy with state of the art exploratory data analysis techniques.","['Paweł Dłotko', 'Davide Gurnari', 'Mathis Hallier', 'Anna Jurek-Loughrey']","['cs.CG', 'math.AT', 'stat.ML']",2024-11-08 09:40:54+00:00
http://arxiv.org/abs/2411.05399v1,Post-Hoc Robustness Enhancement in Graph Neural Networks with Conditional Random Fields,"Graph Neural Networks (GNNs), which are nowadays the benchmark approach in
graph representation learning, have been shown to be vulnerable to adversarial
attacks, raising concerns about their real-world applicability. While existing
defense techniques primarily concentrate on the training phase of GNNs,
involving adjustments to message passing architectures or pre-processing
methods, there is a noticeable gap in methods focusing on increasing robustness
during inference. In this context, this study introduces RobustCRF, a post-hoc
approach aiming to enhance the robustness of GNNs at the inference stage. Our
proposed method, founded on statistical relational learning using a Conditional
Random Field, is model-agnostic and does not require prior knowledge about the
underlying model architecture. We validate the efficacy of this approach across
various models, leveraging benchmark node classification datasets.","['Yassine Abbahaddou', 'Sofiane Ennadir', 'Johannes F. Lutzeyer', 'Fragkiskos D. Malliaros', 'Michalis Vazirgiannis']","['cs.LG', 'cs.SI', 'stat.AP', 'stat.ML']",2024-11-08 08:26:42+00:00
http://arxiv.org/abs/2411.05331v1,Discovering Latent Structural Causal Models from Spatio-Temporal Data,"Many important phenomena in scientific fields such as climate, neuroscience,
and epidemiology are naturally represented as spatiotemporal gridded data with
complex interactions. For example, in climate science, researchers aim to
uncover how large-scale events, such as the North Atlantic Oscillation (NAO)
and the Antarctic Oscillation (AAO), influence other global processes.
Inferring causal relationships from these data is a challenging problem
compounded by the high dimensionality of such data and the correlations between
spatially proximate points. We present SPACY (SPAtiotemporal Causal discoverY),
a novel framework based on variational inference, designed to explicitly model
latent time-series and their causal relationships from spatially confined modes
in the data. Our method uses an end-to-end training process that maximizes an
evidence-lower bound (ELBO) for the data likelihood. Theoretically, we show
that, under some conditions, the latent variables are identifiable up to
transformation by an invertible matrix. Empirically, we show that SPACY
outperforms state-of-the-art baselines on synthetic data, remains scalable for
large grids, and identifies key known phenomena from real-world climate data.","['Kun Wang', 'Sumanth Varambally', 'Duncan Watson-Parris', 'Yi-An Ma', 'Rose Yu']","['cs.LG', 'stat.ML']",2024-11-08 05:12:16+00:00
http://arxiv.org/abs/2411.05877v1,Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass,"Large language models (LMs) are typically adapted to improve performance on
new contexts (\eg text prompts that define new tasks or domains) through
fine-tuning or prompting. However, there is an accuracy compute tradeoff --
fine-tuning incurs significant training cost and prompting increases inference
overhead. We introduce $GenerativeAdapter$, an effective and efficient
adaptation method that directly maps new contexts to low-rank LM adapters,
thereby significantly reducing inference overhead with no need for finetuning.
The adapter generator is trained via self-supervised learning, and can be used
to adapt a single frozen LM for any new task simply by mapping the associated
task or domain context to a new adapter. We apply $GenerativeAdapter$ to two
pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the
adapted models in three adaption scenarios: knowledge acquisition from
documents, learning from demonstrations, and personalization for users. In
StreamingQA, our approach is effective in injecting knowledge into the LM's
parameters, achieving a 63.5% improvement in F1 score over the model with
supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K
tokens. In the MetaICL in-context learning evaluation, our method achieves an
average accuracy of $44.9$ across 26 tasks, outperforming the base model. On
MSC, our method proves to be highly competitive in memorizing user information
from conversations with a 4x reduction in computation and memory costs compared
to prompting with full conversation history. Together, these results suggest
that $GenerativeAdapter$ should allow for general adaption to a wide range of
different contexts.","['Tong Chen', 'Hao Fang', 'Patrick Xia', 'Xiaodong Liu', 'Benjamin Van Durme', 'Luke Zettlemoyer', 'Jianfeng Gao', 'Hao Cheng']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2024-11-08 00:42:47+00:00
http://arxiv.org/abs/2411.05238v1,Generating Highly Designable Proteins with Geometric Algebra Flow Matching,"We introduce a generative model for protein backbone design utilizing
geometric products and higher order message passing. In particular, we propose
Clifford Frame Attention (CFA), an extension of the invariant point attention
(IPA) architecture from AlphaFold2, in which the backbone residue frames and
geometric features are represented in the projective geometric algebra. This
enables to construct geometrically expressive messages between residues,
including higher order terms, using the bilinear operations of the algebra. We
evaluate our architecture by incorporating it into the framework of FrameFlow,
a state-of-the-art flow matching model for protein backbone generation. The
proposed model achieves high designability, diversity and novelty, while also
sampling protein backbones that follow the statistical distribution of
secondary structure elements found in naturally occurring proteins, a property
so far only insufficiently achieved by many state-of-the-art generative models.","['Simon Wagner', 'Leif Seute', 'Vsevolod Viliuga', 'Nicolas Wolf', 'Frauke Gräter', 'Jan Stühmer']","['cs.LG', 'stat.ML']",2024-11-07 23:21:36+00:00
http://arxiv.org/abs/2411.05237v1,Pruning the Path to Optimal Care: Identifying Systematically Suboptimal Medical Decision-Making with Inverse Reinforcement Learning,"In aims to uncover insights into medical decision-making embedded within
observational data from clinical settings, we present a novel application of
Inverse Reinforcement Learning (IRL) that identifies suboptimal clinician
actions based on the actions of their peers. This approach centers two stages
of IRL with an intermediate step to prune trajectories displaying behavior that
deviates significantly from the consensus. This enables us to effectively
identify clinical priorities and values from ICU data containing both optimal
and suboptimal clinician decisions. We observe that the benefits of removing
suboptimal actions vary by disease and differentially impact certain
demographic groups.","['Inko Bovenzi', 'Adi Carmel', 'Michael Hu', 'Rebecca M. Hurwitz', 'Fiona McBride', 'Leo Benac', 'José Roberto Tello Ayala', 'Finale Doshi-Velez']","['cs.LG', 'q-bio.QM', 'stat.AP', 'stat.CO', 'stat.ML']",2024-11-07 23:16:59+00:00
http://arxiv.org/abs/2411.05198v1,Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry,"In this work, we conduct a systematic study of stochastic saddle point
problems (SSP) and stochastic variational inequalities (SVI) under the
constraint of $(\epsilon,\delta)$-differential privacy (DP) in both Euclidean
and non-Euclidean setups. We first consider Lipschitz convex-concave SSPs in
the $\ell_p/\ell_q$ setup, $p,q\in[1,2]$. Here, we obtain a bound of
$\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$ on the
strong SP-gap, where $n$ is the number of samples and $d$ is the dimension.
This rate is nearly optimal for any $p,q\in[1,2]$. Without additional
assumptions, such as smoothness or linearity requirements, prior work under DP
has only obtained this rate when $p=q=2$ (i.e., only in the Euclidean setup).
Further, existing algorithms have each only been shown to work for specific
settings of $p$ and $q$ and under certain assumptions on the loss and the
feasible set, whereas we provide a general algorithm for DP SSPs whenever
$p,q\in[1,2]$. Our result is obtained via a novel analysis of the recursive
regularization algorithm. In particular, we develop new tools for analyzing
generalization, which may be of independent interest. Next, we turn our
attention towards SVIs with a monotone, bounded and Lipschitz operator and
consider $\ell_p$-setups, $p\in[1,2]$. Here, we provide the first analysis
which obtains a bound on the strong VI-gap of $\tilde{O}\big(\frac{1}{\sqrt{n}}
+ \frac{\sqrt{d}}{n\epsilon}\big)$. For $p-1=\Omega(1)$, this rate is near
optimal due to existing lower bounds. To obtain this result, we develop a
modified version of recursive regularization. Our analysis builds on the
techniques we develop for SSPs as well as employing additional novel components
which handle difficulties arising from adapting the recursive regularization
framework to SVIs.","['Raef Bassily', 'Cristóbal Guzmán', 'Michael Menart']","['cs.LG', 'cs.CR', 'math.OC', 'stat.ML']",2024-11-07 21:45:05+00:00
http://arxiv.org/abs/2411.05174v1,Inverse Transition Learning: Learning Dynamics from Demonstrations,"We consider the problem of estimating the transition dynamics $T^*$ from
near-optimal expert trajectories in the context of offline model-based
reinforcement learning. We develop a novel constraint-based method, Inverse
Transition Learning, that treats the limited coverage of the expert
trajectories as a \emph{feature}: we use the fact that the expert is
near-optimal to inform our estimate of $T^*$. We integrate our constraints into
a Bayesian approach. Across both synthetic environments and real healthcare
scenarios like Intensive Care Unit (ICU) patient management in hypotension, we
demonstrate not only significant improvements in decision-making, but that our
posterior can inform when transfer will be successful.","['Leo Benac', 'Abhishek Sharma', 'Sonali Parbhoo', 'Finale Doshi-Velez']","['cs.LG', 'cs.AI', 'stat.ML']",2024-11-07 20:27:29+00:00
http://arxiv.org/abs/2411.05869v1,Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data,"The Gaussian process (GP) is a widely used probabilistic machine learning
method for stochastic function approximation, stochastic modeling, and
analyzing real-world measurements of nonlinear processes. Unlike many other
machine learning methods, GPs include an implicit characterization of
uncertainty, making them extremely useful across many areas of science,
technology, and engineering. Traditional implementations of GPs involve
stationary kernels (also termed covariance functions) that limit their
flexibility and exact methods for inference that prevent application to data
sets with more than about ten thousand points. Modern approaches to address
stationarity assumptions generally fail to accommodate large data sets, while
all attempts to address scalability focus on approximating the Gaussian
likelihood, which can involve subjectivity and lead to inaccuracies. In this
work, we explicitly derive an alternative kernel that can discover and encode
both sparsity and nonstationarity. We embed the kernel within a fully Bayesian
GP model and leverage high-performance computing resources to enable the
analysis of massive data sets. We demonstrate the favorable performance of our
novel kernel relative to existing exact and approximate GP methods across a
variety of synthetic data examples. Furthermore, we conduct space-time
prediction based on more than one million measurements of daily maximum
temperature and verify that our results outperform state-of-the-art methods in
the Earth sciences. More broadly, having access to exact GPs that use
ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods
to truly compete with a wide variety of machine learning methods.","['Mark D. Risser', 'Marcus M. Noack', 'Hengrui Luo', 'Ronald Pandolfi']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.CO', 'stat.ME']",2024-11-07 20:07:21+00:00
http://arxiv.org/abs/2411.04939v1,Pareto Set Identification With Posterior Sampling,"The problem of identifying the best answer among a collection of items having
real-valued distribution is well-understood.
  Despite its practical relevance for many applications, fewer works have
studied its extension when multiple and potentially conflicting metrics are
available to assess an item's quality.
  Pareto set identification (PSI) aims to identify the set of answers whose
means are not uniformly worse than another.
  This paper studies PSI in the transductive linear setting with potentially
correlated objectives.
  Building on posterior sampling in both the stopping and the sampling rules,
we propose the PSIPS algorithm that deals simultaneously with structure and
correlation without paying the computational cost of existing oracle-based
algorithms.
  Both from a frequentist and Bayesian perspective, PSIPS is asymptotically
optimal.
  We demonstrate its good empirical performance in real-world and synthetic
instances.","['Cyrille Kone', 'Marc Jourdan', 'Emilie Kaufmann']","['stat.ML', 'cs.LG']",2024-11-07 18:15:38+00:00
http://arxiv.org/abs/2411.04852v1,Conformalized Credal Regions for Classification with Ambiguous Ground Truth,"An open question in \emph{Imprecise Probabilistic Machine Learning} is how to
empirically derive a credal region (i.e., a closed and convex family of
probabilities on the output space) from the available data, without any prior
knowledge or assumption. In classification problems, credal regions are a tool
that is able to provide provable guarantees under realistic assumptions by
characterizing the uncertainty about the distribution of the labels. Building
on previous work, we show that credal regions can be directly constructed using
conformal methods. This allows us to provide a novel extension of classical
conformal prediction to problems with ambiguous ground truth, that is, when the
exact labels for given inputs are not exactly known. The resulting construction
enjoys desirable practical and theoretical properties: (i) conformal coverage
guarantees, (ii) smaller prediction sets (compared to classical conformal
prediction regions) and (iii) disentanglement of uncertainty sources
(epistemic, aleatoric). We empirically verify our findings on both synthetic
and real datasets.","['Michele Caprio', 'David Stutz', 'Shuo Li', 'Arnaud Doucet']","['stat.ML', 'cs.LG']",2024-11-07 16:39:29+00:00
http://arxiv.org/abs/2411.04775v1,Learning dynamical systems from data: Gradient-based dictionary optimization,"The Koopman operator plays a crucial role in analyzing the global behavior of
dynamical systems. Existing data-driven methods for approximating the Koopman
operator or discovering the governing equations of the underlying system
typically require a fixed set of basis functions, also called dictionary. The
optimal choice of basis functions is highly problem-dependent and often
requires domain knowledge. We present a novel gradient descent-based
optimization framework for learning suitable and interpretable basis functions
from data and show how it can be used in combination with EDMD, SINDy, and
PDE-FIND. We illustrate the efficacy of the proposed approach with the aid of
various benchmark problems such as the Ornstein-Uhlenbeck process, Chua's
circuit, a nonlinear heat equation, as well as protein-folding data.","['Mohammad Tabish', 'Neil K. Chada', 'Stefan Klus']","['math.DS', 'cs.LG', 'stat.ML']",2024-11-07 15:15:27+00:00
http://arxiv.org/abs/2411.04729v1,Conjugate gradient methods for high-dimensional GLMMs,"Generalized linear mixed models (GLMMs) are a widely used tool in statistical
analysis. The main bottleneck of many computational approaches lies in the
inversion of the high dimensional precision matrices associated with the random
effects. Such matrices are typically sparse; however, the sparsity pattern
resembles a multi partite random graph, which does not lend itself well to
default sparse linear algebra techniques. Notably, we show that, for typical
GLMMs, the Cholesky factor is dense even when the original precision is sparse.
We thus turn to approximate iterative techniques, in particular to the
conjugate gradient (CG) method. We combine a detailed analysis of the spectrum
of said precision matrices with results from random graph theory to show that
CG-based methods applied to high-dimensional GLMMs typically achieve a fixed
approximation error with a total cost that scales linearly with the number of
parameters and observations. Numerical illustrations with both real and
simulated data confirm the theoretical findings, while at the same time
illustrating situations, such as nested structures, where CG-based methods
struggle.","['Andrea Pandolfi', 'Omiros Papaspiliopoulos', 'Giacomo Zanella']","['stat.CO', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2024-11-07 14:09:12+00:00
http://arxiv.org/abs/2411.04655v1,Centrality Graph Shift Operators for Graph Neural Networks,"Graph Shift Operators (GSOs), such as the adjacency and graph Laplacian
matrices, play a fundamental role in graph theory and graph representation
learning. Traditional GSOs are typically constructed by normalizing the
adjacency matrix by the degree matrix, a local centrality metric. In this work,
we instead propose and study Centrality GSOs (CGSOs), which normalize adjacency
matrices by global centrality metrics such as the PageRank, $k$-core or count
of fixed length walks. We study spectral properties of the CGSOs, allowing us
to get an understanding of their action on graph signals. We confirm this
understanding by defining and running the spectral clustering algorithm based
on different CGSOs on several synthetic and real-world datasets. We furthermore
outline how our CGSO can act as the message passing operator in any Graph
Neural Network and in particular demonstrate strong performance of a variant of
the Graph Convolutional Network and Graph Attention Network using our CGSOs on
several real-world benchmark datasets.","['Yassine Abbahaddou', 'Fragkiskos D. Malliaros', 'Johannes F. Lutzeyer', 'Michalis Vazirgiannis']","['cs.LG', 'cs.SI', 'math.SP', 'stat.AP', 'stat.ML']",2024-11-07 12:32:24+00:00
http://arxiv.org/abs/2411.04625v1,Sharp Analysis for KL-Regularized Contextual Bandits and RLHF,"Reverse-Kullback-Leibler (KL) regularization has emerged to be a predominant
technique used to enhance policy optimization in reinforcement learning (RL)
and reinforcement learning from human feedback (RLHF), which forces the learned
policy to stay close to a reference policy. While the effectiveness and
necessity of KL-regularization have been empirically demonstrated in various
practical scenarios, current theoretical analysis of KL-regularized RLHF still
obtains the same $\mathcal{O}(1 / \epsilon^2)$ sample complexity as problems
without KL-regularization. To understand the fundamental distinction between
policy learning objectives with KL-regularization and ones without
KL-regularization, we are the first to theoretically demonstrate the power of
KL-regularization by providing a sharp analysis for KL-regularized contextual
bandits and RLHF, revealing an $\mathcal{O}(1 / \epsilon)$ sample complexity
when $\epsilon$ is sufficiently small.
  We further explore the role of data coverage in contextual bandits and RLHF.
While the coverage assumption is commonly employed in offline RLHF to link the
samples from the reference policy to the optimal policy, often at the cost of a
multiplicative dependence on the coverage coefficient, its impact on the sample
complexity of online RLHF remains unclear. Previous theoretical analyses of
online RLHF typically require explicit exploration and additional structural
assumptions on the reward function class. In contrast, we show that with
sufficient coverage from the reference policy, a simple two-stage mixed
sampling strategy can achieve a sample complexity with only an additive
dependence on the coverage coefficient. Our results provide a comprehensive
understanding of the roles of KL-regularization and data coverage in RLHF,
shedding light on the design of more efficient RLHF algorithms.","['Heyang Zhao', 'Chenlu Ye', 'Quanquan Gu', 'Tong Zhang']","['cs.LG', 'stat.ML']",2024-11-07 11:22:46+00:00
http://arxiv.org/abs/2411.04551v1,Measure-to-measure interpolation using Transformers,"Transformers are deep neural network architectures that underpin the recent
successes of large language models. Unlike more classical architectures that
can be viewed as point-to-point maps, a Transformer acts as a
measure-to-measure map implemented as specific interacting particle system on
the unit sphere: the input is the empirical measure of tokens in a prompt and
its evolution is governed by the continuity equation. In fact, Transformers are
not limited to empirical measures and can in principle process any input
measure. As the nature of data processed by Transformers is expanding rapidly,
it is important to investigate their expressive power as maps from an arbitrary
measure to another arbitrary measure. To that end, we provide an explicit
choice of parameters that allows a single Transformer to match $N$ arbitrary
input measures to $N$ arbitrary target measures, under the minimal assumption
that every pair of input-target measures can be matched by some transport map.","['Borjan Geshkovski', 'Philippe Rigollet', 'Domènec Ruiz-Balet']","['math.OC', 'cs.LG', 'stat.ML']",2024-11-07 09:18:39+00:00
http://arxiv.org/abs/2411.04466v1,Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity,"The wider application of end-to-end learning methods to embodied
decision-making domains remains bottlenecked by their reliance on a
superabundance of training data representative of the target domain.
Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot
generalization--the goal of standard reinforcement learning (RL)--in favor of
few-shot adaptation, and thus hold promise for bridging larger generalization
gaps. While learning this meta-level adaptive behavior still requires
substantial data, efficient environment simulators approaching real-world
complexity are growing in prevalence. Even so, hand-designing sufficiently
diverse and numerous simulated training tasks for these complex domains is
prohibitively labor-intensive. Domain randomization (DR) and procedural
generation (PG), offered as solutions to this problem, require simulators to
possess carefully-defined parameters which directly translate to meaningful
task diversity--a similarly prohibitive assumption. In this work, we present
DIVA, an evolutionary approach for generating diverse training tasks in such
complex, open-ended simulators. Like unsupervised environment design (UED)
methods, DIVA can be applied to arbitrary parameterizations, but can
additionally incorporate realistically-available domain knowledge--thus
inheriting the flexibility and generality of UED, and the supervised structure
embedded in well-designed simulators exploited by DR and PG. Our empirical
results showcase DIVA's unique ability to overcome complex parameterizations
and successfully train adaptive agent behavior, far outperforming competitive
baselines from prior literature. These findings highlight the potential of such
semi-supervised environment design (SSED) approaches, of which DIVA is the
first humble constituent, to enable training in realistic simulated domains,
and produce more robust and capable adaptive agents.","['Robby Costales', 'Stefanos Nikolaidis']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2024-11-07 06:27:12+00:00
http://arxiv.org/abs/2411.04421v2,Variational Low-Rank Adaptation Using IVON,"We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.","['Bai Cong', 'Nico Daheim', 'Yuesong Shen', 'Daniel Cremers', 'Rio Yokota', 'Mohammad Emtiyaz Khan', 'Thomas Möllenhoff']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2024-11-07 04:17:30+00:00
http://arxiv.org/abs/2411.04394v1,Statistical-Computational Trade-offs for Greedy Recursive Partitioning Estimators,"Models based on recursive partitioning such as decision trees and their
ensembles are popular for high-dimensional regression as they can potentially
avoid the curse of dimensionality. Because empirical risk minimization (ERM) is
computationally infeasible, these models are typically trained using greedy
algorithms. Although effective in many cases, these algorithms have been
empirically observed to get stuck at local optima. We explore this phenomenon
in the context of learning sparse regression functions over $d$ binary
features, showing that when the true regression function $f^*$ does not satisfy
the so-called Merged Staircase Property (MSP), greedy training requires
$\exp(\Omega(d))$ to achieve low estimation error. Conversely, when $f^*$ does
satisfy MSP, greedy training can attain small estimation error with only
$O(\log d)$ samples. This performance mirrors that of two-layer neural networks
trained with stochastic gradient descent (SGD) in the mean-field regime,
thereby establishing a head-to-head comparison between SGD-trained neural
networks and greedy recursive partitioning estimators. Furthermore, ERM-trained
recursive partitioning estimators achieve low estimation error with $O(\log d)$
samples irrespective of whether $f^*$ satisfies MSP, thereby demonstrating a
statistical-computational trade-off for greedy training. Our proofs are based
on a novel interpretation of greedy recursive partitioning using stochastic
process theory and a coupling technique that may be of independent interest.","['Yan Shuo Tan', 'Jason M. Klusowski', 'Krishnakumar Balasubramanian']","['stat.ML', 'cs.DS', 'cs.LG', '68Q32, 62G08', 'G.3']",2024-11-07 03:11:53+00:00
http://arxiv.org/abs/2411.05044v1,Deep Heuristic Learning for Real-Time Urban Pathfinding,"This paper introduces a novel approach to urban pathfinding by transforming
traditional heuristic-based algorithms into deep learning models that leverage
real-time contextual data, such as traffic and weather conditions. We propose
two methods: an enhanced A* algorithm that dynamically adjusts routes based on
current environmental conditions, and a neural network model that predicts the
next optimal path segment using historical and live data. An extensive
benchmark was conducted to compare the performance of different deep learning
models, including MLP, GRU, LSTM, Autoencoders, and Transformers. Both methods
were evaluated in a simulated urban environment in Berlin, with the neural
network model outperforming traditional methods, reducing travel times by up to
40%, while the enhanced A* algorithm achieved a 34% improvement. These results
demonstrate the potential of deep learning to optimize urban navigation in real
time, providing more adaptable and efficient routing solutions.","['Mohamed Hussein Abo El-Ela', 'Ali Hamdi Fergany']","['cs.AI', 'cs.LG', 'stat.ML']",2024-11-07 00:22:04+00:00
http://arxiv.org/abs/2411.05853v1,A Fundamental Accuracy--Robustness Trade-off in Regression and Classification,"We derive a fundamental trade-off between standard and adversarial risk in a
rather general situation that formalizes the following simple intuition: ""If no
(nearly) optimal predictor is smooth, adversarial robustness comes at the cost
of accuracy."" As a concrete example, we evaluate the derived trade-off in
regression with polynomial ridge functions under mild regularity conditions.",['Sohail Bahmani'],"['stat.ML', 'cs.LG']",2024-11-06 22:03:53+00:00
http://arxiv.org/abs/2411.04282v1,Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding,"Large language models (LLMs) have shown impressive capabilities, but still
struggle with complex reasoning tasks requiring multiple steps. While
prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at
inference time, optimizing reasoning capabilities during training remains
challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled
framework that formulates reasoning as sampling from a latent distribution and
optimizes it via variational approaches. LaTRO enables LLMs to concurrently
improve both their reasoning process and ability to evaluate reasoning quality,
without requiring external feedback or reward models. We validate LaTRO through
experiments on GSM8K and ARC-Challenge datasets using multiple model
architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of
12.5% over base models and 9.6% over supervised fine-tuning across
Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that
pre-trained LLMs possess latent reasoning capabilities that can be unlocked and
enhanced through our proposed optimization approach in a self-improvement
manner. The code of LaTRO is available at
\url{https://github.com/SalesforceAIResearch/LaTRO}.","['Haolin Chen', 'Yihao Feng', 'Zuxin Liu', 'Weiran Yao', 'Akshara Prabhakar', 'Shelby Heinecke', 'Ricky Ho', 'Phil Mui', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang']","['cs.AI', 'cs.CL', 'cs.LG', 'stat.ML', 'I.2.7']",2024-11-06 22:02:30+00:00
http://arxiv.org/abs/2411.05852v1,$\spadesuit$ SPADE $\spadesuit$ Split Peak Attention DEcomposition,"Demand forecasting faces challenges induced by Peak Events (PEs)
corresponding to special periods such as promotions and holidays. Peak events
create significant spikes in demand followed by demand ramp down periods.
Neural networks like MQCNN and MQT overreact to demand peaks by carrying over
the elevated PE demand into subsequent Post-Peak-Event (PPE) periods, resulting
in significantly over-biased forecasts. To tackle this challenge, we introduce
a neural forecasting model called Split Peak Attention DEcomposition, SPADE.
This model reduces the impact of PEs on subsequent forecasts by modeling
forecasting as consisting of two separate tasks: one for PEs; and the other for
the rest. Its architecture then uses masked convolution filters and a
specialized Peak Attention module. We show SPADE's performance on a worldwide
retail dataset with hundreds of millions of products. Our results reveal a
reduction in PPE degradation by 4.5% and an improvement in PE accuracy by 3.9%,
relative to current production models.","['Malcolm Wolff', 'Kin G. Olivares', 'Boris Oreshkin', 'Sunny Ruan', 'Sitan Yang', 'Abhinav Katoch', 'Shankar Ramasubramanian', 'Youxin Zhang', 'Michael W. Mahoney', 'Dmitry Efimov', 'Vincent Quenneville-Bélair']","['cs.LG', 'stat.ML']",2024-11-06 22:00:07+00:00
http://arxiv.org/abs/2411.04281v1,Generating Synthetic Electronic Health Record (EHR) Data: A Review with Benchmarking,"We conduct a scoping review of existing approaches for synthetic EHR data
generation, and benchmark major methods with proposed open-source software to
offer recommendations for practitioners. We search three academic databases for
our scoping review. Methods are benchmarked on open-source EHR datasets,
MIMIC-III/IV. Seven existing methods covering major categories and two baseline
methods are implemented and compared. Evaluation metrics concern data fidelity,
downstream utility, privacy protection, and computational cost. 42 studies are
identified and classified into five categories. Seven open-source methods
covering all categories are selected, trained on MIMIC-III, and evaluated on
MIMIC-III or MIMIC-IV for transportability considerations. Among them,
GAN-based methods demonstrate competitive performance in fidelity and utility
on MIMIC-III; rule-based methods excel in privacy protection. Similar findings
are observed on MIMIC-IV, except that GAN-based methods further outperform the
baseline methods in preserving fidelity. A Python package, ``SynthEHRella'', is
provided to integrate various choices of approaches and evaluation metrics,
enabling more streamlined exploration and evaluation of multiple methods. We
found that method choice is governed by the relative importance of the
evaluation metrics in downstream use cases. We provide a decision tree to guide
the choice among the benchmarked methods. Based on the decision tree, GAN-based
methods excel when distributional shifts exist between the training and testing
populations. Otherwise, CorGAN and MedGAN are most suitable for association
modeling and predictive modeling, respectively. Future research should
prioritize enhancing fidelity of the synthetic data while controlling privacy
exposure, and comprehensive benchmarking of longitudinal or conditional
generation methods.","['Xingran Chen', 'Zhenke Wu', 'Xu Shi', 'Hyunghoon Cho', 'Bhramar Mukherjee']","['cs.LG', 'cs.AI', 'stat.ML']",2024-11-06 21:59:19+00:00
http://arxiv.org/abs/2411.04280v1,Bayesian Inference in Recurrent Explicit Duration Switching Linear Dynamical Systems,"In this paper, we propose a novel model called Recurrent Explicit Duration
Switching Linear Dynamical Systems (REDSLDS) that incorporates recurrent
explicit duration variables into the rSLDS model. We also propose an inference
and learning scheme that involves the use of P\'olya-gamma augmentation. We
demonstrate the improved segmentation capabilities of our model on three
benchmark datasets, including two quantitative datasets and one qualitative
dataset.","['Mikołaj Słupiński', 'Piotr Lipiński']","['cs.LG', 'cs.AI', 'math.DS', 'stat.ML']",2024-11-06 21:58:24+00:00
http://arxiv.org/abs/2411.04278v1,The Recurrent Sticky Hierarchical Dirichlet Process Hidden Markov Model,"The Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) is a natural
Bayesian nonparametric extension of the classical Hidden Markov Model for
learning from (spatio-)temporal data. A sticky HDP-HMM has been proposed to
strengthen the self-persistence probability in the HDP-HMM. Then, disentangled
sticky HDP-HMM has been proposed to disentangle the strength of the
self-persistence prior and transition prior. However, the sticky HDP-HMM
assumes that the self-persistence probability is stationary, limiting its
expressiveness. Here, we build on previous work on sticky HDP-HMM and
disentangled sticky HDP-HMM, developing a more general model: the recurrent
sticky HDP-HMM (RS-HDP-HMM). We develop a novel Gibbs sampling strategy for
efficient inference in this model. We show that RS-HDP-HMM outperforms
disentangled sticky HDP-HMM, sticky HDP-HMM, and HDP-HMM in both synthetic and
real data segmentation.","['Mikołaj Słupiński', 'Piotr Lipiński']","['cs.LG', 'cs.AI', 'math.DS', 'stat.ML']",2024-11-06 21:49:20+00:00
http://arxiv.org/abs/2411.04265v1,Graph neural networks and non-commuting operators,"Graph neural networks (GNNs) provide state-of-the-art results in a wide
variety of tasks which typically involve predicting features at the vertices of
a graph. They are built from layers of graph convolutions which serve as a
powerful inductive bias for describing the flow of information among the
vertices. Often, more than one data modality is available. This work considers
a setting in which several graphs have the same vertex set and a common
vertex-level learning task. This generalizes standard GNN models to GNNs with
several graph operators that do not commute. We may call this model graph-tuple
neural networks (GtNN).
  In this work, we develop the mathematical theory to address the stability and
transferability of GtNNs using properties of non-commuting non-expansive
operators. We develop a limit theory of graphon-tuple neural networks and use
it to prove a universal transferability theorem that guarantees that all
graph-tuple neural networks are transferable on convergent graph-tuple
sequences. In particular, there is no non-transferable energy under the
convergence we consider here. Our theoretical results extend well-known
transferability theorems for GNNs to the case of several simultaneous graphs
(GtNNs) and provide a strict improvement on what is currently known even in the
GNN case.
  We illustrate our theoretical results with simple experiments on synthetic
and real-world data. To this end, we derive a training procedure that provably
enforces the stability of the resulting model.","['Mauricio Velasco', ""Kaiying O'Hare"", 'Bernardo Rychtenberg', 'Soledad Villar']","['stat.ML', 'cs.AI', 'cs.LG']",2024-11-06 21:17:14+00:00
http://arxiv.org/abs/2411.04243v1,ION-C: Integration of Overlapping Networks via Constraints,"In many causal learning problems, variables of interest are often not all
measured over the same observations, but are instead distributed across
multiple datasets with overlapping variables. Tillman et al. (2008) presented
the first algorithm for enumerating the minimal equivalence class of
ground-truth DAGs consistent with all input graphs by exploiting local
independence relations, called ION. In this paper, this problem is formulated
as a more computationally efficient answer set programming (ASP) problem, which
we call ION-C, and solved with the ASP system clingo. The ION-C algorithm was
run on random synthetic graphs with varying sizes, densities, and degrees of
overlap between subgraphs, with overlap having the largest impact on runtime,
number of solution graphs, and agreement within the output set. To validate
ION-C on real-world data, we ran the algorithm on overlapping graphs learned
from data from two successive iterations of the European Social Survey (ESS),
using a procedure for conducting joint independence tests to prevent
inconsistencies in the input.","['Praveen Nair', 'Payal Bhandari', 'Mohammadsajad Abavisani', 'Sergey Plis', 'David Danks']","['stat.ML', 'cs.LG']",2024-11-06 20:12:47+00:00
http://arxiv.org/abs/2411.04216v1,Debiasing Synthetic Data Generated by Deep Generative Models,"While synthetic data hold great promise for privacy protection, their
statistical analysis poses significant challenges that necessitate innovative
solutions. The use of deep generative models (DGMs) for synthetic data
generation is known to induce considerable bias and imprecision into synthetic
data analyses, compromising their inferential utility as opposed to original
data analyses. This bias and uncertainty can be substantial enough to impede
statistical convergence rates, even in seemingly straightforward analyses like
mean calculation. The standard errors of such estimators then exhibit slower
shrinkage with sample size than the typical 1 over root-$n$ rate. This
complicates fundamental calculations like p-values and confidence intervals,
with no straightforward remedy currently available. In response to these
challenges, we propose a new strategy that targets synthetic data created by
DGMs for specific data analyses. Drawing insights from debiased and targeted
machine learning, our approach accounts for biases, enhances convergence rates,
and facilitates the calculation of estimators with easily approximated large
sample variances. We exemplify our proposal through a simulation study on toy
data and two case studies on real-world data, highlighting the importance of
tailoring DGMs for targeted data analysis. This debiasing strategy contributes
to advancing the reliability and applicability of synthetic data in statistical
inference.","['Alexander Decruyenaere', 'Heidelinde Dehaene', 'Paloma Rabaey', 'Christiaan Polet', 'Johan Decruyenaere', 'Thomas Demeester', 'Stijn Vansteelandt']","['stat.ML', 'cs.LG']",2024-11-06 19:24:34+00:00
http://arxiv.org/abs/2411.04108v1,Weighted Sobolev Approximation Rates for Neural Networks on Unbounded Domains,"In this work, we consider the approximation capabilities of shallow neural
networks in weighted Sobolev spaces for functions in the spectral Barron space.
The existing literature already covers several cases, in which the spectral
Barron space can be approximated well, i.e., without curse of dimensionality,
by shallow networks and several different classes of activation function. The
limitations of the existing results are mostly on the error measures that were
considered, in which the results are restricted to Sobolev spaces over a
bounded domain. We will here treat two cases that extend upon the existing
results. Namely, we treat the case with bounded domain and Muckenhoupt weights
and the case, where the domain is allowed to be unbounded and the weights are
required to decay. We first present embedding results for the more general
weighted Fourier-Lebesgue spaces in the weighted Sobolev spaces and then we
establish asymptotic approximation rates for shallow neural networks that come
without curse of dimensionality.","['Ahmed Abdeljawad', 'Thomas Dittrich']","['cs.LG', 'math.FA', 'stat.ML', '41A25, 41A46, 41A30, 46E35, 62M45, 68T05']",2024-11-06 18:36:21+00:00
http://arxiv.org/abs/2411.05850v1,Are Deep Learning Methods Suitable for Downscaling Global Climate Projections? Review and Intercomparison of Existing Models,"Deep Learning (DL) has shown promise for downscaling global climate change
projections under different approaches, including Perfect Prognosis (PP) and
Regional Climate Model (RCM) emulation. Unlike emulators, PP downscaling models
are trained on observational data, so it remains an open question whether they
can plausibly extrapolate unseen conditions and changes in future emissions
scenarios. Here we focus on this problem as the main drawback for the
operationalization of these methods and present the results of 1) a literature
review to identify state-of-the-art DL models for PP downscaling and 2) an
intercomparison experiment to evaluate the performance of these models and to
assess their extrapolation capability using a common experimental framework,
taking into account the sensitivity of results to different training replicas.
We focus on minimum and maximum temperatures and precipitation over Spain, a
region with a range of climatic conditions with different influential regional
processes. We conclude with a discussion of the findings, limitations of
existing methods, and prospects for future development.","['Jose González-Abad', 'José Manuel Gutiérrez']","['physics.ao-ph', 'cs.LG', 'stat.ML']",2024-11-06 18:05:45+00:00
http://arxiv.org/abs/2411.04054v1,Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits,"Causal knowledge about the relationships among decision variables and a
reward variable in a bandit setting can accelerate the learning of an optimal
decision. Current works often assume the causal graph is known, which may not
always be available a priori. Motivated by this challenge, we focus on the
causal bandit problem in scenarios where the underlying causal graph is unknown
and may include latent confounders. While intervention on the parents of the
reward node is optimal in the absence of latent confounders, this is not
necessarily the case in general. Instead, one must consider a set of possibly
optimal arms/interventions, each being a special subset of the ancestors of the
reward node, making causal discovery beyond the parents of the reward node
essential. For regret minimization, we identify that discovering the full
causal structure is unnecessary; however, no existing work provides the
necessary and sufficient components of the causal graph. We formally
characterize the set of necessary and sufficient latent confounders one needs
to detect or learn to ensure that all possibly optimal arms are identified
correctly. We also propose a randomized algorithm for learning the causal graph
with a limited number of samples, providing a sample complexity guarantee for
any desired confidence level. In the causal bandit setup, we propose a
two-stage approach. In the first stage, we learn the induced subgraph on
ancestors of the reward, along with a necessary and sufficient subset of latent
confounders, to construct the set of possibly optimal arms. The regret incurred
during this phase scales polynomially with respect to the number of nodes in
the causal graph. The second phase involves the application of a standard
bandit algorithm, such as the UCB algorithm. We also establish a regret bound
for our two-phase approach, which is sublinear in the number of rounds.","['Muhammad Qasim Elahi', 'Mahsa Ghasemi', 'Murat Kocaoglu']","['stat.ML', 'cs.LG']",2024-11-06 16:59:11+00:00
http://arxiv.org/abs/2411.03936v1,GUIDE-VAE: Advancing Data Generation with User Information and Pattern Dictionaries,"Generative modelling of multi-user datasets has become prominent in science
and engineering. Generating a data point for a given user requires employing
user information, and conventional generative models, including variational
autoencoders (VAEs), often ignore that. This paper introduces GUIDE-VAE, a
novel conditional generative model that leverages user embeddings to generate
user-guided data. By allowing the model to benefit from shared patterns across
users, GUIDE-VAE enhances performance in multi-user settings, even under
significant data imbalance. In addition to integrating user information,
GUIDE-VAE incorporates a pattern dictionary-based covariance composition (PDCC)
to improve the realism of generated samples by capturing complex feature
dependencies. While user embeddings drive performance gains, PDCC addresses
common issues such as noise and over-smoothing typically seen in VAEs.
  The proposed GUIDE-VAE was evaluated on a multi-user smart meter dataset
characterized by substantial data imbalance across users. Quantitative results
show that GUIDE-VAE performs effectively in both synthetic data generation and
missing record imputation tasks, while qualitative evaluations reveal that
GUIDE-VAE produces more plausible and less noisy data. These results establish
GUIDE-VAE as a promising tool for controlled, realistic data generation in
multi-user datasets, with potential applications across various domains
requiring user-informed modelling.","['Kutay Bölat', 'Simon Tindemans']","['cs.LG', 'stat.ML']",2024-11-06 14:11:46+00:00
http://arxiv.org/abs/2411.03932v1,Improved Regret of Linear Ensemble Sampling,"In this work, we close the fundamental gap of theory and practice by
providing an improved regret bound for linear ensemble sampling. We prove that
with an ensemble size logarithmic in $T$, linear ensemble sampling can achieve
a frequentist regret bound of $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$, matching
state-of-the-art results for randomized linear bandit algorithms, where $d$ and
$T$ are the dimension of the parameter and the time horizon respectively. Our
approach introduces a general regret analysis framework for linear bandit
algorithms. Additionally, we reveal a significant relationship between linear
ensemble sampling and Linear Perturbed-History Exploration (LinPHE), showing
that LinPHE is a special case of linear ensemble sampling when the ensemble
size equals $T$. This insight allows us to derive a new regret bound of
$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ for LinPHE, independent of the number of
arms. Our contributions advance the theoretical foundation of ensemble
sampling, bringing its regret bounds in line with the best known bounds for
other randomized exploration algorithms.","['Harin Lee', 'Min-hwan Oh']","['stat.ML', 'cs.LG']",2024-11-06 14:09:11+00:00
http://arxiv.org/abs/2411.03810v1,Hybrid Transfer Reinforcement Learning: Provable Sample Efficiency from Shifted-Dynamics Data,"Online Reinforcement learning (RL) typically requires high-stakes online
interaction data to learn a policy for a target task. This prompts interest in
leveraging historical data to improve sample efficiency. The historical data
may come from outdated or related source environments with different dynamics.
It remains unclear how to effectively use such data in the target task to
provably enhance learning and sample efficiency. To address this, we propose a
hybrid transfer RL (HTRL) setting, where an agent learns in a target
environment while accessing offline data from a source environment with shifted
dynamics. We show that -- without information on the dynamics shift -- general
shifted-dynamics data, even with subtle shifts, does not reduce sample
complexity in the target environment. However, with prior information on the
degree of the dynamics shift, we design HySRL, a transfer algorithm that
achieves problem-dependent sample complexity and outperforms pure online RL.
Finally, our experimental results demonstrate that HySRL surpasses
state-of-the-art online RL baseline.","['Chengrui Qu', 'Laixi Shi', 'Kishan Panaganti', 'Pengcheng You', 'Adam Wierman']","['cs.LG', 'stat.ML']",2024-11-06 10:14:46+00:00
http://arxiv.org/abs/2411.03779v1,Multilingual hierarchical classification of job advertisements for job vacancy statistics,"The goal of this paper is to develop a multilingual classifier and
conditional probability estimator of occupation codes for online job
advertisements according in accordance with the International Standard
Classification of Occupations (ISCO) extended with the Polish Classification of
Occupations and Specializations (KZiS), which is analogous to the European
Classification of Occupations. In this paper, we utilise a range of data
sources, including a novel one, namely the Central Job Offers Database, which
is a register of all vacancies submitted to Public Employment Offices. Their
staff members code the vacancies according to the ISCO and KZiS. A hierarchical
multi-class classifier has been developed based on the transformer
architecture. The classifier begins by encoding the jobs found in
advertisements to the widest 1-digit occupational group, and then narrows the
assignment to a 6-digit occupation code. We show that incorporation of the
hierarchical structure of occupations improves prediction accuracy by 1-2
percentage points, particularly for the hand-coded online job advertisements.
Finally, a bilingual (Polish and English) and multilingual (24 languages) model
is developed based on data translated using closed and open-source software.
The open-source software is provided for the benefit of the official statistics
community, with a particular focus on international comparability.","['Maciej Beręsewicz', 'Marek Wydmuch', 'Herman Cherniaiev', 'Robert Pater']","['stat.AP', 'econ.GN', 'q-fin.EC', 'stat.ML']",2024-11-06 09:16:15+00:00
http://arxiv.org/abs/2411.05833v1,Fully Automated Correlated Time Series Forecasting in Minutes,"Societal and industrial infrastructures and systems increasingly leverage
sensors that emit correlated time series. Forecasting of future values of such
time series based on recorded historical values has important benefits.
Automatically designed models achieve higher accuracy than manually designed
models. Given a forecasting task, which includes a dataset and a forecasting
horizon, automated design methods automatically search for an optimal
forecasting model for the task in a manually designed search space, and then
train the identified model using the dataset to enable the forecasting.
Existing automated methods face three challenges. First, the search space is
constructed by human experts, rending the methods only semi-automated and
yielding search spaces prone to subjective biases. Second, it is time consuming
to search for an optimal model. Third, training the identified model for a new
task is also costly. These challenges limit the practicability of automated
methods in real-world settings. To contend with the challenges, we propose a
fully automated and highly efficient correlated time series forecasting
framework where the search and training can be done in minutes. The framework
includes a data-driven, iterative strategy to automatically prune a large
search space to obtain a high-quality search space for a new forecasting task.
It includes a zero-shot search strategy to efficiently identify the optimal
model in the customized search space. And it includes a fast parameter
adaptation strategy to accelerate the training of the identified model.
Experiments on seven benchmark datasets offer evidence that the framework is
capable of state-of-the-art accuracy and is much more efficient than existing
methods.","['Xinle Wu', 'Xingjian Wu', 'Dalin Zhang', 'Miao Zhang', 'Chenjuan Guo', 'Bin Yang', 'Christian S. Jensen']","['cs.LG', 'stat.ML']",2024-11-06 09:02:13+00:00
http://arxiv.org/abs/2411.03759v1,Variational Inference on the Boolean Hypercube with the Quantum Entropy,"In this paper, we derive variational inference upper-bounds on the
log-partition function of pairwise Markov random fields on the Boolean
hypercube, based on quantum relaxations of the Kullback-Leibler divergence. We
then propose an efficient algorithm to compute these bounds based on
primal-dual optimization. An improvement of these bounds through the use of
''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and
we present a greedy algorithm to select among these relaxations. We carry
extensive numerical experiments and compare with state-of-the-art methods for
this inference problem.","['Eliot Beyler', 'Francis Bach']","['cs.IT', 'cs.LG', 'math.IT', 'math.OC', 'stat.ML']",2024-11-06 08:42:53+00:00
http://arxiv.org/abs/2411.03746v1,Optimal Defenses Against Gradient Reconstruction Attacks,"Federated Learning (FL) is designed to prevent data leakage through
collaborative model training without centralized data storage. However, it
remains vulnerable to gradient reconstruction attacks that recover original
training data from shared gradients. To optimize the trade-off between data
leakage and utility loss, we first derive a theoretical lower bound of
reconstruction error (among all attackers) for the two standard methods: adding
noise, and gradient pruning. We then customize these two defenses to be
parameter- and model-specific and achieve the optimal trade-off between our
obtained reconstruction lower bound and model utility. Experimental results
validate that our methods outperform Gradient Noise and Gradient Pruning by
protecting the training data better while also achieving better utility.","['Yuxiao Chen', 'Gamze Gürsoy', 'Qi Lei']","['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML']",2024-11-06 08:22:20+00:00
http://arxiv.org/abs/2411.03731v1,"Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model Training Pipelines via Memoization-Awareness","The training or fine-tuning of machine learning, vision, and language models
is often implemented as a pipeline: a sequence of stages encompassing data
preparation, model training and evaluation. In this paper, we exploit pipeline
structures to reduce the cost of hyperparameter tuning for model
training/fine-tuning, which is particularly valuable for language models given
their high costs in GPU-days. We propose a ""memoization-aware"" Bayesian
Optimization (BO) algorithm, EEIPU, that works in tandem with a pipeline
caching system, allowing it to evaluate significantly more hyperparameter
candidates per GPU-day than other tuning algorithms. The result is
better-quality hyperparameters in the same amount of search time, or
equivalently, reduced search time to reach the same hyperparameter quality. In
our benchmarks on machine learning (model ensembles), vision (convolutional
architecture) and language (T5 architecture) pipelines, we compare EEIPU
against recent BO algorithms: EEIPU produces an average of $103\%$ more
hyperparameter candidates (within the same budget), and increases the
validation metric by an average of $108\%$ more than other algorithms (where
the increase is measured starting from the end of warm-up iterations).","['Abdelmajid Essofi', 'Ridwan Salahuddeen', 'Munachiso Nwadike', 'Elnura Zhalieva', 'Kun Zhang', 'Eric Xing', 'Willie Neiswanger', 'Qirong Ho']","['cs.LG', 'stat.ML']",2024-11-06 07:53:04+00:00
http://arxiv.org/abs/2411.03620v1,A Subsampling Based Neural Network for Spatial Data,"The application of deep neural networks in geospatial data has become a
trending research problem in the present day. A significant amount of
statistical research has already been introduced, such as generalized least
square optimization by incorporating spatial variance-covariance matrix,
considering basis functions in the input nodes of the neural networks, and so
on. However, for lattice data, there is no available literature about the
utilization of asymptotic analysis of neural networks in regression for spatial
data. This article proposes a consistent localized two-layer deep neural
network-based regression for spatial data. We have proved the consistency of
this deep neural network for bounded and unbounded spatial domains under a
fixed sampling design of mixed-increasing spatial regions. We have proved that
its asymptotic convergence rate is faster than that of \cite{zhan2024neural}'s
neural network and an improved generalization of \cite{shen2023asymptotic}'s
neural network structure. We empirically observe the rate of convergence of
discrepancy measures between the empirical probability distribution of observed
and predicted data, which will become faster for a less smooth spatial surface.
We have applied our asymptotic analysis of deep neural networks to the
estimation of the monthly average temperature of major cities in the USA from
its satellite image. This application is an effective showcase of non-linear
spatial regression. We demonstrate our methodology with simulated lattice data
in various scenarios.",['Debjoy Thakur'],"['stat.ML', 'cs.LG']",2024-11-06 02:37:43+00:00
http://arxiv.org/abs/2411.03611v1,Designing a Linearized Potential Function in Neural Network Optimization Using Csiszár Type of Tsallis Entropy,"In recent years, learning for neural networks can be viewed as optimization
in the space of probability measures. To obtain the exponential convergence to
the optimizer, the regularizing term based on Shannon entropy plays an
important role. Even though an entropy function heavily affects convergence
results, there is almost no result on its generalization, because of the
following two technical difficulties: one is the lack of sufficient condition
for generalized logarithmic Sobolev inequality, and the other is the
distributional dependence of the potential function within the gradient flow
equation. In this paper, we establish a framework that utilizes a linearized
potential function via Csisz\'{a}r type of Tsallis entropy, which is one of the
generalized entropies. We also show that our new framework enable us to derive
an exponential convergence result.",['Keito Akiyama'],"['stat.ML', 'cs.LG', 'math.AP', '35Q49, 49J20, 82C32']",2024-11-06 02:12:41+00:00
http://arxiv.org/abs/2411.05829v1,Utilizing RNN for Real-time Cryptocurrency Price Prediction and Trading Strategy Optimization,"This study explores the use of Recurrent Neural Networks (RNN) for real-time
cryptocurrency price prediction and optimized trading strategies. Given the
high volatility of the cryptocurrency market, traditional forecasting models
often fall short. By leveraging RNNs' capability to capture long-term patterns
in time-series data, this research aims to improve accuracy in price prediction
and develop effective trading strategies. The project follows a structured
approach involving data collection, preprocessing, and model refinement,
followed by rigorous backtesting for profitability and risk assessment. This
work contributes to both the academic and practical fields by providing a
robust predictive model and optimized trading strategies that address the
challenges of cryptocurrency trading.","['Shamima Nasrin Tumpa', 'Kehelwala Dewage Gayan Maduranga']","['q-fin.ST', 'cs.AI', 'cs.LG', 'stat.ML']",2024-11-05 22:44:52+00:00
http://arxiv.org/abs/2411.03387v1,Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel Orthogonal Learner,"Estimating causal quantities from observational data is crucial for
understanding the safety and effectiveness of medical treatments. However, to
make reliable inferences, medical practitioners require not only estimating
averaged causal quantities, such as the conditional average treatment effect,
but also understanding the randomness of the treatment effect as a random
variable. This randomness is referred to as aleatoric uncertainty and is
necessary for understanding the probability of benefit from treatment or
quantiles of the treatment effect. Yet, the aleatoric uncertainty of the
treatment effect has received surprisingly little attention in the causal
machine learning community. To fill this gap, we aim to quantify the aleatoric
uncertainty of the treatment effect at the covariate-conditional level, namely,
the conditional distribution of the treatment effect (CDTE). Unlike average
causal quantities, the CDTE is not point identifiable without strong additional
assumptions. As a remedy, we employ partial identification to obtain sharp
bounds on the CDTE and thereby quantify the aleatoric uncertainty of the
treatment effect. We then develop a novel, orthogonal learner for the bounds on
the CDTE, which we call AU-learner. We further show that our AU-learner has
several strengths in that it satisfies Neyman-orthogonality and is doubly
robust. Finally, we propose a fully-parametric deep learning instantiation of
our AU-learner.","['Valentyn Melnychuk', 'Stefan Feuerriegel', 'Mihaela van der Schaar']","['cs.LG', 'stat.ML']",2024-11-05 18:14:49+00:00
http://arxiv.org/abs/2411.03384v1,Solving stochastic partial differential equations using neural networks in the Wiener chaos expansion,"In this paper, we solve stochastic partial differential equations (SPDEs)
numerically by using (possibly random) neural networks in the truncated Wiener
chaos expansion of their corresponding solution. Moreover, we provide some
approximation rates for learning the solution of SPDEs with additive and/or
multiplicative noise. Finally, we apply our results in numerical examples to
approximate the solution of three SPDEs: the stochastic heat equation, the
Heath-Jarrow-Morton equation, and the Zakai equation.","['Ariel Neufeld', 'Philipp Schmocker']","['stat.ML', 'cs.LG', 'cs.NA', 'math.NA', 'math.PR']",2024-11-05 18:11:25+00:00
http://arxiv.org/abs/2411.03383v1,Near-Optimal and Tractable Estimation under Shift-Invariance,"How hard is it to estimate a discrete-time signal $(x_{1}, ..., x_{n}) \in
\mathbb{C}^n$ satisfying an unknown linear recurrence relation of order $s$ and
observed in i.i.d. complex Gaussian noise? The class of all such signals is
parametric but extremely rich: it contains all exponential polynomials over
$\mathbb{C}$ with total degree $s$, including harmonic oscillations with $s$
arbitrary frequencies. Geometrically, this class corresponds to the projection
onto $\mathbb{C}^{n}$ of the union of all shift-invariant subspaces of
$\mathbb{C}^\mathbb{Z}$ of dimension $s$. We show that the statistical
complexity of this class, as measured by the squared minimax radius of the
$(1-\delta)$-confidence $\ell_2$-ball, is nearly the same as for the class of
$s$-sparse signals, namely $O\left(s\log(en) + \log(\delta^{-1})\right) \cdot
\log^2(es) \cdot \log(en/s).$ Moreover, the corresponding near-minimax
estimator is tractable, and it can be used to build a test statistic with a
near-minimax detection threshold in the associated detection problem. These
statistical results rest upon an approximation-theoretic one: we show that
finite-dimensional shift-invariant subspaces admit compactly supported
reproducing kernels whose Fourier spectra have nearly the smallest possible
$\ell_p$-norms, for all $p \in [1,+\infty]$ at once.",['Dmitrii M. Ostrovskii'],"['math.ST', 'math.CA', 'stat.ML', 'stat.TH', '62F35, 62G99, 42A05, 42A10, 42A15, 42A85, 30E10, 34L99', 'G.3']",2024-11-05 18:11:23+00:00
http://arxiv.org/abs/2411.03263v1,Proxy-informed Bayesian transfer learning with unknown sources,"Generalization outside the scope of one's training data requires leveraging
prior knowledge about the effects that transfer, and the effects that don't,
between different data sources. Bayesian transfer learning is a principled
paradigm for specifying this knowledge, and refining it on the basis of data
from the source (training) and target (prediction) tasks. We address the
challenging transfer learning setting where the learner (i) cannot fine-tune in
the target task, and (ii) does not know which source data points correspond to
the same task (i.e., the data sources are unknown). We propose a proxy-informed
robust method for probabilistic transfer learning (PROMPT), which provides a
posterior predictive estimate tailored to the structure of the target task,
without requiring the learner have access to any outcome information from the
target task. Instead, PROMPT relies on the availability of proxy information.
PROMPT uses the same proxy information for two purposes: (i) estimation of
effects specific to the target task, and (ii) construction of a robust
reweighting of the source data for estimation of effects that transfer between
tasks. We provide theoretical results on the effect of this reweighting on the
risk of negative transfer, and demonstrate application of PROMPT in two
synthetic settings.","['Sabina J. Sloman', 'Julien Martinelli', 'Samuel Kaski']","['cs.LG', 'stat.ML']",2024-11-05 17:02:29+00:00
http://arxiv.org/abs/2411.03195v1,Online Data Collection for Efficient Semiparametric Inference,"While many works have studied statistical data fusion, they typically assume
that the various datasets are given in advance. However, in practice,
estimation requires difficult data collection decisions like determining the
available data sources, their costs, and how many samples to collect from each
source. Moreover, this process is often sequential because the data collected
at a given time can improve collection decisions in the future. In our setup,
given access to multiple data sources and budget constraints, the agent must
sequentially decide which data source to query to efficiently estimate a target
parameter. We formalize this task using Online Moment Selection, a
semiparametric framework that applies to any parameter identified by a set of
moment conditions. Interestingly, the optimal budget allocation depends on the
(unknown) true parameters. We present two online data collection policies,
Explore-then-Commit and Explore-then-Greedy, that use the parameter estimates
at a given time to optimally allocate the remaining budget in the future steps.
We prove that both policies achieve zero regret (assessed by asymptotic MSE)
relative to an oracle policy. We empirically validate our methods on both
synthetic and real-world causal effect estimation tasks, demonstrating that the
online data collection policies outperform their fixed counterparts.","['Shantanu Gupta', 'Zachary C. Lipton', 'David Childers']","['stat.ML', 'cs.LG']",2024-11-05 15:40:53+00:00
http://arxiv.org/abs/2411.03107v1,Near-Optimal Dynamic Regret for Adversarial Linear Mixture MDPs,"We study episodic linear mixture MDPs with the unknown transition and
adversarial rewards under full-information feedback, employing dynamic regret
as the performance measure. We start with in-depth analyses of the strengths
and limitations of the two most popular methods: occupancy-measure-based and
policy-based methods. We observe that while the occupancy-measure-based method
is effective in addressing non-stationary environments, it encounters
difficulties with the unknown transition. In contrast, the policy-based method
can deal with the unknown transition effectively but faces challenges in
handling non-stationary environments. Building on this, we propose a novel
algorithm that combines the benefits of both methods. Specifically, it employs
(i) an occupancy-measure-based global optimization with a two-layer structure
to handle non-stationary environments; and (ii) a policy-based variance-aware
value-targeted regression to tackle the unknown transition. We bridge these two
parts by a novel conversion. Our algorithm enjoys an $\widetilde{\mathcal{O}}(d
\sqrt{H^3 K} + \sqrt{HK(H + \bar{P}_K)})$ dynamic regret, where $d$ is the
feature dimension, $H$ is the episode length, $K$ is the number of episodes,
$\bar{P}_K$ is the non-stationarity measure. We show it is minimax optimal up
to logarithmic factors by establishing a matching lower bound. To the best of
our knowledge, this is the first work that achieves near-optimal dynamic regret
for adversarial linear mixture MDPs with the unknown transition without prior
knowledge of the non-stationarity measure.","['Long-Fei Li', 'Peng Zhao', 'Zhi-Hua Zhou']","['cs.LG', 'stat.ML']",2024-11-05 13:55:52+00:00
http://arxiv.org/abs/2411.03103v1,Benign landscape for Burer-Monteiro factorizations of MaxCut-type semidefinite programs,"We consider MaxCut-type semidefinite programs (SDP) which admit a low rank
solution. To numerically leverage the low rank hypothesis, a standard
algorithmic approach is the Burer-Monteiro factorization, which allows to
significantly reduce the dimensionality of the problem at the cost of its
convexity. We give a sharp condition on the conditioning of the Laplacian
matrix associated with the SDP under which any second-order critical point of
the non-convex problem is a global minimizer. By applying our theorem, we
improve on recent results about the correctness of the Burer-Monteiro approach
on $\mathbb{Z}_2$-synchronization problems.","['Faniriana Rakoto Endor', 'Irène Waldspurger']","['math.OC', 'cs.CC', 'stat.ML']",2024-11-05 13:47:07+00:00
http://arxiv.org/abs/2411.03097v1,Correlating Variational Autoencoders Natively For Multi-View Imputation,"Multi-view data from the same source often exhibit correlation. This is
mirrored in correlation between the latent spaces of separate variational
autoencoders (VAEs) trained on each data-view. A multi-view VAE approach is
proposed that incorporates a joint prior with a non-zero correlation structure
between the latent spaces of the VAEs. By enforcing such correlation structure,
more strongly correlated latent spaces are uncovered. Using conditional
distributions to move between these latent spaces, missing views can be imputed
and used for downstream analysis. Learning this correlation structure involves
maintaining validity of the prior distribution, as well as a successful
parameterization that allows end-to-end learning.","['Ella S. C. Orme', 'Marina Evangelou', 'Ulrich Paquet']","['stat.ML', 'cs.LG']",2024-11-05 13:43:37+00:00
http://arxiv.org/abs/2411.03028v1,Graph Agnostic Causal Bayesian Optimisation,"We study the problem of globally optimising a target variable of an unknown
causal graph on which a sequence of soft or hard interventions can be
performed. The problem of optimising the target variable associated with a
causal graph is formalised as Causal Bayesian Optimisation (CBO). We study the
CBO problem under the cumulative regret objective with unknown causal graphs
for two settings, namely structural causal models with hard interventions and
function networks with soft interventions. We propose Graph Agnostic Causal
Bayesian Optimisation (GACBO), an algorithm that actively discovers the causal
structure that contributes to achieving optimal rewards. GACBO seeks to balance
exploiting the actions that give the best rewards against exploring the causal
structures and functions. To the best of our knowledge, our work is the first
to study causal Bayesian optimization with cumulative regret objectives in
scenarios where the graph is unknown or partially known. We show our proposed
algorithm outperforms baselines in simulated experiments and real-world
applications.","['Sumantrak Mukherjee', 'Mengyan Zhang', 'Seth Flaxman', 'Sebastian Josef Vollmer']","['cs.LG', 'stat.ML']",2024-11-05 11:49:33+00:00
http://arxiv.org/abs/2411.03021v1,Testing Generalizability in Causal Inference,"Ensuring robust model performance across diverse real-world scenarios
requires addressing both transportability across domains with covariate shifts
and extrapolation beyond observed data ranges. However, there is no formal
procedure for statistically evaluating generalizability in machine learning
algorithms, particularly in causal inference. Existing methods often rely on
arbitrary metrics like AUC or MSE and focus predominantly on toy datasets,
providing limited insights into real-world applicability. To address this gap,
we propose a systematic and quantitative framework for evaluating model
generalizability under covariate distribution shifts, specifically within
causal inference settings. Our approach leverages the frugal parameterization,
allowing for flexible simulations from fully and semi-synthetic benchmarks,
offering comprehensive evaluations for both mean and distributional regression
methods. By basing simulations on real data, our method ensures more realistic
evaluations, which is often missing in current work relying on simplified
datasets. Furthermore, using simulations and statistical testing, our framework
is robust and avoids over-reliance on conventional metrics. Grounded in
real-world data, it provides realistic insights into model performance,
bridging the gap between synthetic evaluations and practical applications.","['Daniel de Vassimon Manela', 'Linying Yang', 'Robin J. Evans']","['cs.LG', 'stat.AP', 'stat.ME', 'stat.ML']",2024-11-05 11:44:00+00:00
http://arxiv.org/abs/2411.03014v1,Your copula is a classifier in disguise: classification-based copula density estimation,"We propose reinterpreting copula density estimation as a discriminative task.
Under this novel estimation scheme, we train a classifier to distinguish
samples from the joint density from those of the product of independent
marginals, recovering the copula density in the process. We derive equivalences
between well-known copula classes and classification problems naturally arising
in our interpretation. Furthermore, we show our estimator achieves theoretical
guarantees akin to maximum likelihood estimation. By identifying a connection
with density ratio estimation, we benefit from the rich literature and models
available for such problems. Empirically, we demonstrate the applicability of
our approach by estimating copulas of real and high-dimensional datasets,
outperforming competing copula estimators in density evaluation as well as
sampling.","['David Huk', 'Mark Steel', 'Ritabrata Dutta']","['stat.ME', 'stat.ML']",2024-11-05 11:25:34+00:00
http://arxiv.org/abs/2411.02904v2,Gradient Descent Finds Over-Parameterized Neural Networks with Sharp Generalization for Nonparametric Regression: A Distribution-Free Analysis,"We study nonparametric regression by an over-parameterized two-layer neural
network trained by gradient descent (GD) in this paper. We show that, if the
neural network is trained by GD with early stopping, then the trained network
renders a sharp rate of the nonparametric regression risk of $\cO(\eps_n^2)$,
which is the same rate as that for the classical kernel regression trained by
GD with early stopping, where $\eps_n$ is the critical population rate of the
Neural Tangent Kernel (NTK) associated with the network and $n$ is the size of
the training data. It is remarked that our result does not require
distributional assumptions on the training data, in a strong contrast with many
existing results which rely on specific distributions such as the spherical
uniform data distribution or distributions satisfying certain restrictive
conditions. The rate $\cO(\eps_n^2)$ is known to be minimax optimal for
specific cases, such as the case that the NTK has a polynomial eigenvalue decay
rate which happens under certain distributional assumptions. Our result
formally fills the gap between training a classical kernel regression model and
training an over-parameterized but finite-width neural network by GD for
nonparametric regression without distributional assumptions. We also provide
confirmative answers to certain open questions or address particular concerns
in the literature of training over-parameterized neural networks by GD with
early stopping for nonparametric regression, including the characterization of
the stopping time, the lower bound for the network width, and the constant
learning rate used in GD.","['Yingzhen Yang', 'Ping Li']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2024-11-05 08:43:54+00:00
http://arxiv.org/abs/2411.02853v1,ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal Rate,"Adam is one of the most popular optimization algorithms in deep learning.
However, it is known that Adam does not converge in theory unless choosing a
hyperparameter, i.e., $\beta_2$, in a problem-dependent manner. There have been
many attempts to fix the non-convergence (e.g., AMSGrad), but they require an
impractical assumption that the gradient noise is uniformly bounded. In this
paper, we propose a new adaptive gradient method named ADOPT, which achieves
the optimal convergence rate of $\mathcal{O} ( 1 / \sqrt{T} )$ with any choice
of $\beta_2$ without depending on the bounded noise assumption. ADOPT addresses
the non-convergence issue of Adam by removing the current gradient from the
second moment estimate and changing the order of the momentum update and the
normalization by the second moment estimate. We also conduct intensive
numerical experiments, and verify that our ADOPT achieves superior results
compared to Adam and its variants across a wide range of tasks, including image
classification, generative modeling, natural language processing, and deep
reinforcement learning. The implementation is available at
https://github.com/iShohei220/adopt.","['Shohei Taniguchi', 'Keno Harada', 'Gouki Minegishi', 'Yuta Oshima', 'Seong Cheol Jeong', 'Go Nagahara', 'Tomoshi Iiyama', 'Masahiro Suzuki', 'Yusuke Iwasawa', 'Yutaka Matsuo']","['cs.LG', 'stat.ML']",2024-11-05 06:57:47+00:00
http://arxiv.org/abs/2411.02835v1,Community detection with the Bethe-Hessian,"The Bethe-Hessian matrix, introduced by Saade, Krzakala, and Zdeborov\'a
(2014), is a Hermitian matrix designed for applying spectral clustering
algorithms to sparse networks. Rather than employing a non-symmetric and
high-dimensional non-backtracking operator, a spectral method based on the
Bethe-Hessian matrix is conjectured to also reach the Kesten-Stigum detection
threshold in the sparse stochastic block model (SBM). We provide the first
rigorous analysis of the Bethe-Hessian spectral method in the SBM under both
the bounded expected degree and the growing degree regimes. Specifically, we
demonstrate that: (i) When the expected degree $d\geq 2$, the number of
negative outliers of the Bethe-Hessian matrix can consistently estimate the
number of blocks above the Kesten-Stigum threshold, thus confirming a
conjecture from Saade, Krzakala, and Zdeborov\'a (2014) for $d\geq 2$. (ii) For
sufficiently large $d$, its eigenvectors can be used to achieve weak recovery.
(iii) As $d\to\infty$, we establish the concentration of the locations of its
negative outlier eigenvalues, and weak consistency can be achieved via a
spectral method based on the Bethe-Hessian matrix.","['Ludovic Stephan', 'Yizhe Zhu']","['math.ST', 'math.CO', 'math.PR', 'stat.ML', 'stat.TH']",2024-11-05 06:18:37+00:00
http://arxiv.org/abs/2411.02791v1,Language Models and Cycle Consistency for Self-Reflective Machine Translation,"This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM.",['Jianqiao Wangni'],"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG', 'cs.NE', 'stat.ML']",2024-11-05 04:01:41+00:00
http://arxiv.org/abs/2411.02784v1,Generalization and Risk Bounds for Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) have achieved great success in the
prediction of sequential data. However, their theoretical studies are still
lagging behind because of their complex interconnected structures. In this
paper, we establish a new generalization error bound for vanilla RNNs, and
provide a unified framework to calculate the Rademacher complexity that can be
applied to a variety of loss functions. When the ramp loss is used, we show
that our bound is tighter than the existing bounds based on the same
assumptions on the Frobenius and spectral norms of the weight matrices and a
few mild conditions. Our numerical results show that our new generalization
bound is the tightest among all existing bounds in three public datasets. Our
bound improves the second tightest one by an average percentage of 13.80% and
3.01% when the $\tanh$ and ReLU activation functions are used, respectively.
Moreover, we derive a sharp estimation error bound for RNN-based estimators
obtained through empirical risk minimization (ERM) in multi-class
classification problems when the loss function satisfies a Bernstein condition.","['Xuewei Cheng', 'Ke Huang', 'Shujie Ma']","['stat.ML', 'cs.LG']",2024-11-05 03:49:06+00:00
http://arxiv.org/abs/2411.02771v1,Automatic doubly robust inference for linear functionals via calibrated debiased machine learning,"In causal inference, many estimands of interest can be expressed as a linear
functional of the outcome regression function; this includes, for example,
average causal effects of static, dynamic and stochastic interventions. For
learning such estimands, in this work, we propose novel debiased machine
learning estimators that are doubly robust asymptotically linear, thus
providing not only doubly robust consistency but also facilitating doubly
robust inference (e.g., confidence intervals and hypothesis tests). To do so,
we first establish a key link between calibration, a machine learning technique
typically used in prediction and classification tasks, and the conditions
needed to achieve doubly robust asymptotic linearity. We then introduce
calibrated debiased machine learning (C-DML), a unified framework for doubly
robust inference, and propose a specific C-DML estimator that integrates
cross-fitting, isotonic calibration, and debiased machine learning estimation.
A C-DML estimator maintains asymptotic linearity when either the outcome
regression or the Riesz representer of the linear functional is estimated
sufficiently well, allowing the other to be estimated at arbitrarily slow rates
or even inconsistently. We propose a simple bootstrap-assisted approach for
constructing doubly robust confidence intervals. Our theoretical and empirical
results support the use of C-DML to mitigate bias arising from the inconsistent
or slow estimation of nuisance functions.","['Lars van der Laan', 'Alex Luedtke', 'Marco Carone']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2024-11-05 03:32:30+00:00
http://arxiv.org/abs/2411.02770v1,New random projections for isotropic kernels using stable spectral distributions,"Rahimi and Recht [31] introduced the idea of decomposing shift-invariant
kernels by randomly sampling from their spectral distribution. This famous
technique, known as Random Fourier Features (RFF), is in principle applicable
to any shift-invariant kernel whose spectral distribution can be identified and
simulated. In practice, however, it is usually applied to the Gaussian kernel
because of its simplicity, since its spectral distribution is also Gaussian.
Clearly, simple spectral sampling formulas would be desirable for broader
classes of kernel functions. In this paper, we propose to decompose spectral
kernel distributions as a scale mixture of $\alpha$-stable random vectors. This
provides a simple and ready-to-use spectral sampling formula for a very large
class of multivariate shift-invariant kernels, including exponential power
kernels, generalized Mat\'ern kernels, generalized Cauchy kernels, as well as
newly introduced kernels such as the Beta, Kummer, and Tricomi kernels. In
particular, we show that the spectral densities of all these kernels are scale
mixtures of the multivariate Gaussian distribution. This provides a very simple
way to modify existing Random Fourier Features software based on Gaussian
kernels to cover a much richer class of multivariate kernels. This result has
broad applications for support vector machines, kernel ridge regression,
Gaussian processes, and other kernel-based machine learning techniques for
which the random Fourier features technique is applicable.","['Nicolas Langrené', 'Xavier Warin', 'Pierre Gruet']","['cs.LG', 'math.PR', 'stat.CO', 'stat.ML', '42B10, 62H05, 65C05, 65D12, 60E10', 'G.3; I.6.1; I.1.0']",2024-11-05 03:28:01+00:00
http://arxiv.org/abs/2411.02767v1,A Convex Relaxation Approach to Generalization Analysis for Parallel Positively Homogeneous Networks,"We propose a general framework for deriving generalization bounds for
parallel positively homogeneous neural networks--a class of neural networks
whose input-output map decomposes as the sum of positively homogeneous maps.
Examples of such networks include matrix factorization and sensing,
single-layer multi-head attention mechanisms, tensor factorization, deep linear
and ReLU networks, and more. Our general framework is based on linking the
non-convex empirical risk minimization (ERM) problem to a closely related
convex optimization problem over prediction functions, which provides a global,
achievable lower-bound to the ERM problem. We exploit this convex lower-bound
to perform generalization analysis in the convex space while controlling the
discrepancy between the convex model and its non-convex counterpart. We apply
our general framework to a wide variety of models ranging from low-rank matrix
sensing, to structured matrix sensing, two-layer linear networks, two-layer
ReLU networks, and single-layer multi-head attention mechanisms, achieving
generalization bounds with a sample complexity that scales almost linearly with
the network width.","['Uday Kiran Reddy Tadipatri', 'Benjamin D. Haeffele', 'Joshua Agterberg', 'René Vidal']","['cs.LG', 'eess.SP', 'stat.ML']",2024-11-05 03:24:34+00:00
http://arxiv.org/abs/2411.02764v1,"Fast, robust approximate message passing","We give a fast, spectral procedure for implementing approximate-message
passing (AMP) algorithms robustly. For any quadratic optimization problem over
symmetric matrices $X$ with independent subgaussian entries, and any separable
AMP algorithm $\mathcal A$, our algorithm performs a spectral pre-processing
step and then mildly modifies the iterates of $\mathcal A$. If given the
perturbed input $X + E \in \mathbb R^{n \times n}$ for any $E$ supported on a
$\varepsilon n \times \varepsilon n$ principal minor, our algorithm outputs a
solution $\hat v$ which is guaranteed to be close to the output of $\mathcal A$
on the uncorrupted $X$, with $\|\mathcal A(X) - \hat v\|_2 \le f(\varepsilon)
\|\mathcal A(X)\|_2$ where $f(\varepsilon) \to 0$ as $\varepsilon \to 0$
depending only on $\varepsilon$.","['Misha Ivkov', 'Tselil Schramm']","['cs.DS', 'cs.LG', 'stat.ML']",2024-11-05 03:20:14+00:00
http://arxiv.org/abs/2411.02726v1,"Elliptical Wishart distributions: information geometry, maximum likelihood estimator, performance analysis and statistical learning","This paper deals with Elliptical Wishart distributions - which generalize the
Wishart distribution - in the context of signal processing and machine
learning. Two algorithms to compute the maximum likelihood estimator (MLE) are
proposed: a fixed point algorithm and a Riemannian optimization method based on
the derived information geometry of Elliptical Wishart distributions. The
existence and uniqueness of the MLE are characterized as well as the
convergence of both estimation algorithms. Statistical properties of the MLE
are also investigated such as consistency, asymptotic normality and an
intrinsic version of Fisher efficiency. On the statistical learning side, novel
classification and clustering methods are designed. For the $t$-Wishart
distribution, the performance of the MLE and statistical learning algorithms
are evaluated on both simulated and real EEG and hyperspectral data, showcasing
the interest of our proposed methods.","['Imen Ayadi', 'Florent Bouchard', 'Frédéric Pascal']","['stat.ML', 'cs.LG', 'stat.ME']",2024-11-05 01:52:27+00:00
http://arxiv.org/abs/2411.02721v1,Differentiability and Approximation of Probability Functions under Gaussian Mixture Models: A Bayesian Approach,"In this work, we study probability functions associated with Gaussian mixture
models. Our primary focus is on extending the use of spherical radial
decomposition for multivariate Gaussian random vectors to the context of
Gaussian mixture models, which are not inherently spherical but only
conditionally so. Specifically, the conditional probability distribution, given
a random parameter of the random vector, follows a Gaussian distribution,
allowing us to apply Bayesian analysis tools to the probability function. This
assumption, together with spherical radial decomposition for Gaussian random
vectors, enables us to represent the probability function as an integral over
the Euclidean sphere. Using this representation, we establish sufficient
conditions to ensure the differentiability of the probability function and
provide and integral representation of its gradient. Furthermore, leveraging
the Bayesian decomposition, we approximate the probability function using
random sampling over the parameter space and the Euclidean sphere. Finally, we
present numerical examples that illustrate the advantages of this approach over
classical approximations based on random vector sampling.","['Gonzalo Contador', 'Pedro Pérez-Aros', 'Emilio Vilches']","['math.OC', 'math.PR', 'stat.ML', '90C15, 65K10']",2024-11-05 01:36:27+00:00
http://arxiv.org/abs/2411.02709v1,Carbon price fluctuation prediction using blockchain information A new hybrid machine learning approach,"In this study, the novel hybrid machine learning approach is proposed in
carbon price fluctuation prediction. Specifically, a research framework
integrating DILATED Convolutional Neural Networks (CNN) and Long Short-Term
Memory (LSTM) neural network algorithm is proposed. The advantage of the
combined framework is that it can make feature extraction more efficient. Then,
based on the DILATED CNN-LSTM framework, the L1 and L2 parameter norm penalty
as regularization method is adopted to predict. Referring to the
characteristics of high correlation between energy indicator price and
blockchain information in previous literature, and we primarily includes
indicators related to blockchain information through regularization process.
Based on the above methods, this paper uses a dataset containing an amount of
data to carry out the carbon price prediction. The experimental results show
that the DILATED CNN-LSTM framework is superior to the traditional CNN-LSTM
architecture. Blockchain information can effectively predict the price. Since
parameter norm penalty as regularization, Ridge Regression (RR) as L2
regularization is better than Smoothly Clipped Absolute Deviation Penalty
(SCAD) as L1 regularization in price forecasting. Thus, the proposed RR-DILATED
CNN-LSTM approach can effectively and accurately predict the fluctuation trend
of the carbon price. Therefore, the new forecasting methods and theoretical
ecology proposed in this study provide a new basis for trend prediction and
evaluating digital assets policy represented by the carbon price for both the
academia and practitioners.","['H. Wang', 'Y. Pang', 'D. Shang']","['cs.LG', 'stat.ML']",2024-11-05 01:12:17+00:00
http://arxiv.org/abs/2411.02694v1,Point processes with event time uncertainty,"Point processes are widely used statistical models for uncovering the
temporal patterns in dependent event data. In many applications, the event time
cannot be observed exactly, calling for the incorporation of time uncertainty
into the modeling of point process data. In this work, we introduce a framework
to model time-uncertain point processes possibly on a network. We start by
deriving the formulation in the continuous-time setting under a few assumptions
motivated by application scenarios. After imposing a time grid, we obtain a
discrete-time model that facilitates inference and can be computed by
first-order optimization methods such as Gradient Descent or Variation
inequality (VI) using batch-based Stochastic Gradient Descent (SGD). The
parameter recovery guarantee is proved for VI inference at an $O(1/k)$
convergence rate using $k$ SGD steps. Our framework handles non-stationary
processes by modeling the inference kernel as a matrix (or tensor on a network)
and it covers the stationary process, such as the classical Hawkes process, as
a special case. We experimentally show that the proposed approach outperforms
previous General Linear model (GLM) baselines on simulated and real data and
reveals meaningful causal relations on a Sepsis-associated Derangements
dataset.","['Xiuyuan Cheng', 'Tingnan Gong', 'Yao Xie']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2024-11-05 00:46:09+00:00
http://arxiv.org/abs/2411.02664v1,Explanations that reveal all through the definition of encoding,"Feature attributions attempt to highlight what inputs drive predictive power.
Good attributions or explanations are thus those that produce inputs that
retain this predictive power; accordingly, evaluations of explanations score
their quality of prediction. However, evaluations produce scores better than
what appears possible from the values in the explanation for a class of
explanations, called encoding explanations. Probing for encoding remains a
challenge because there is no general characterization of what gives the extra
predictive power. We develop a definition of encoding that identifies this
extra predictive power via conditional dependence and show that the definition
fits existing examples of encoding. This definition implies, in contrast to
encoding explanations, that non-encoding explanations contain all the
informative inputs used to produce the explanation, giving them a ""what you see
is what you get"" property, which makes them transparent and simple to use.
Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank
non-encoding explanations above encoding ones, and develop STRIPE-X which ranks
them correctly. After empirically demonstrating the theoretical insights, we
use STRIPE-X to uncover encoding in LLM-generated explanations for predicting
the sentiment in movie reviews.","['Aahlad Puli', 'Nhi Nguyen', 'Rajesh Ranganath']","['cs.LG', 'cs.AI', 'stat.ML']",2024-11-04 23:00:24+00:00
http://arxiv.org/abs/2411.02638v1,Classifier Chain Networks for Multi-Label Classification,"The classifier chain is a widely used method for analyzing multi-labeled data
sets. In this study, we introduce a generalization of the classifier chain: the
classifier chain network. The classifier chain network enables joint estimation
of model parameters, and allows to account for the influence of earlier label
predictions on subsequent classifiers in the chain. Through simulations, we
evaluate the classifier chain network's performance against multiple benchmark
methods, demonstrating competitive results even in scenarios that deviate from
its modeling assumptions. Furthermore, we propose a new measure for detecting
conditional dependencies between labels and illustrate the classifier chain
network's effectiveness using an empirical data set.","['Daniel J. W. Touw', 'Michel van de Velden']","['stat.ML', 'cs.LG']",2024-11-04 21:56:13+00:00
http://arxiv.org/abs/2411.02603v3,FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees,"The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether a LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. Our approach is distribution-free and works for any number of
human-annotated samples. It is model-agnostic and applies to any black-box or
white-box LM. Extensive experiments on question-answering (QA) and
multiple-choice benchmarks demonstrate that FactTest effectively detects
hallucinations and improves the model's ability to abstain from answering
unknown questions, leading to an over 40% accuracy improvement.","['Fan Nie', 'Xiaotian Hou', 'Shuhang Lin', 'James Zou', 'Huaxiu Yao', 'Linjun Zhang']","['cs.CL', 'cs.AI', 'stat.ML']",2024-11-04 20:53:04+00:00
http://arxiv.org/abs/2411.02557v1,A Directional Rockafellar-Uryasev Regression,"Most ost Big Data datasets suffer from selection bias. For example, X
(Twitter) training observations differ largely from the testing offline
observations as individuals on Twitter are generally more educated, democratic
or left-leaning. Therefore, one major obstacle to reliable estimation is the
differences between training and testing data. How can researchers make use of
such data even in the presence of non-ignorable selection mechanisms? A number
of methods have been developed for this issue, such as distributionally robust
optimization (DRO) or learning fairness. A possible avenue to reducing the
effect of bias is meta-information. Researchers, being field exerts, might have
prior information on the form and extent of selection bias affecting their
dataset, and in which direction the selection might cause the estimate to
change, e.g. over or under estimation. At the same time, there is no direct way
to leverage these types of information in learning. I propose a loss function
which takes into account two types of meta data information given by the
researcher: quantity and direction (under or over sampling) of bias in the
training set. Estimation with the proposed loss function is then implemented
through a neural network, the directional Rockafellar-Uryasev (dRU) regression
model. I test the dRU model on a biased training dataset, a Big Data online
drawn electoral poll. I apply the proposed model using meta data information
coherent with the political and sampling information obtained from previous
studies. The results show that including meta information improves the
electoral results predictions compared to a model that does not include them.",['Alberto Arletti'],"['stat.ML', 'cs.LG']",2024-11-04 19:42:05+00:00
http://arxiv.org/abs/2411.02549v1,Distributionally Robust Optimization,"Distributionally robust optimization (DRO) studies decision problems under
uncertainty where the probability distribution governing the uncertain problem
parameters is itself uncertain. A key component of any DRO model is its
ambiguity set, that is, a family of probability distributions consistent with
any available structural or statistical information. DRO seeks decisions that
perform best under the worst distribution in the ambiguity set. This worst case
criterion is supported by findings in psychology and neuroscience, which
indicate that many decision-makers have a low tolerance for distributional
ambiguity. DRO is rooted in statistics, operations research and control theory,
and recent research has uncovered its deep connections to regularization
techniques and adversarial training in machine learning. This survey presents
the key findings of the field in a unified and self-contained manner.","['Daniel Kuhn', 'Soroosh Shafiee', 'Wolfram Wiesemann']","['math.OC', 'cs.LG', 'stat.ML']",2024-11-04 19:32:24+00:00
http://arxiv.org/abs/2411.02544v1,Pretrained transformer efficiently learns low-dimensional target functions in-context,"Transformers can efficiently learn in-context from example demonstrations.
Most existing theoretical analyses studied the in-context learning (ICL)
ability of transformers for linear function classes, where it is typically
shown that the minimizer of the pretraining loss implements one gradient
descent step on the least squares objective. However, this simplified linear
setting arguably does not demonstrate the statistical efficiency of ICL, since
the pretrained transformer does not outperform directly solving linear
regression on the test prompt. In this paper, we study ICL of a nonlinear
function class via transformer with nonlinear MLP layer: given a class of
\textit{single-index} target functions $f_*(\boldsymbol{x}) =
\sigma_*(\langle\boldsymbol{x},\boldsymbol{\beta}\rangle)$, where the index
features $\boldsymbol{\beta}\in\mathbb{R}^d$ are drawn from a $r$-dimensional
subspace, we show that a nonlinear transformer optimized by gradient descent
(with a pretraining sample complexity that depends on the \textit{information
exponent} of the link functions $\sigma_*$) learns $f_*$ in-context with a
prompt length that only depends on the dimension of the distribution of target
functions $r$; in contrast, any algorithm that directly learns $f_*$ on test
prompt yields a statistical complexity that scales with the ambient dimension
$d$. Our result highlights the adaptivity of the pretrained transformer to
low-dimensional structures of the function class, which enables
sample-efficient ICL that outperforms estimators that only have access to the
in-context data.","['Kazusato Oko', 'Yujin Song', 'Taiji Suzuki', 'Denny Wu']","['cs.LG', 'stat.ML']",2024-11-04 19:24:39+00:00
http://arxiv.org/abs/2411.02383v1,Linear Causal Bandits: Unknown Graph and Soft Interventions,"Designing causal bandit algorithms depends on two central categories of
assumptions: (i) the extent of information about the underlying causal graphs
and (ii) the extent of information about interventional statistical models.
There have been extensive recent advances in dispensing with assumptions on
either category. These include assuming known graphs but unknown interventional
distributions, and the converse setting of assuming unknown graphs but access
to restrictive hard/$\operatorname{do}$ interventions, which removes the
stochasticity and ancestral dependencies. Nevertheless, the problem in its
general form, i.e., unknown graph and unknown stochastic intervention models,
remains open. This paper addresses this problem and establishes that in a graph
with $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after
$T$ interaction rounds the regret upper bound scales as
$\tilde{\mathcal{O}}((cd)^{L-\frac{1}{2}}\sqrt{T} + d + RN)$ where $c>1$ is a
constant and $R$ is a measure of intervention power. A universal minimax lower
bound is also established, which scales as $\Omega(d^{L-\frac{3}{2}}\sqrt{T})$.
Importantly, the graph size $N$ has a diminishing effect on the regret as $T$
grows. These bounds have matching behavior in $T$, exponential dependence on
$L$, and polynomial dependence on $d$ (with the gap $d\ $). On the algorithmic
aspect, the paper presents a novel way of designing a computationally efficient
CB algorithm, addressing a challenge that the existing CB algorithms using soft
interventions face.","['Zirui Yan', 'Ali Tajer']","['stat.ML', 'cs.LG']",2024-11-04 18:50:39+00:00
http://arxiv.org/abs/2411.02335v1,Sparsing Law: Towards Large Language Models with Greater Activation Sparsity,"Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.","['Yuqi Luo', 'Chenyang Song', 'Xu Han', 'Yingfa Chen', 'Chaojun Xiao', 'Zhiyuan Liu', 'Maosong Sun']","['cs.LG', 'cs.CL', 'stat.ML', 'I.2.7']",2024-11-04 17:59:04+00:00
http://arxiv.org/abs/2411.02298v1,Sample-Efficient Private Learning of Mixtures of Gaussians,"We study the problem of learning mixtures of Gaussians with approximate
differential privacy. We prove that roughly $kd^2 + k^{1.5} d^{1.75} + k^2 d$
samples suffice to learn a mixture of $k$ arbitrary $d$-dimensional Gaussians
up to low total variation distance, with differential privacy. Our work
improves over the previous best result [AAL24b] (which required roughly $k^2
d^4$ samples) and is provably optimal when $d$ is much larger than $k^2$.
Moreover, we give the first optimal bound for privately learning mixtures of
$k$ univariate (i.e., $1$-dimensional) Gaussians. Importantly, we show that the
sample complexity for privately learning mixtures of univariate Gaussians is
linear in the number of components $k$, whereas the previous best sample
complexity [AAL21] was quadratic in $k$. Our algorithms utilize various
techniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23],
sample compression for distributions [ABDH+20], and methods for bounding
volumes of sumsets.","['Hassan Ashtiani', 'Mahbod Majid', 'Shyam Narayanan']","['cs.LG', 'cs.DS', 'math.ST', 'stat.ML', 'stat.TH']",2024-11-04 17:23:52+00:00
http://arxiv.org/abs/2411.02271v2,On the Utilization of Unique Node Identifiers in Graph Neural Networks,"Graph Neural Networks have inherent representational limitations due to their
message-passing structure. Recent work has suggested that these limitations can
be overcome by using unique node identifiers (UIDs). Here we argue that despite
the advantages of UIDs, one of their disadvantages is that they lose the
desirable property of permutation-equivariance. We thus propose to focus on UID
models that are permutation-equivariant, and present theoretical arguments for
their advantages. Motivated by this, we propose a method to regularize UID
models towards permutation equivariance, via a contrastive loss. We empirically
demonstrate that our approach improves generalization and extrapolation
abilities while providing faster training convergence. On the recent BREC
expressiveness benchmark, our proposed method achieves state-of-the-art
performance compared to other random-based approaches.","['Maya Bechler-Speicher', 'Moshe Eliasof', 'Carola-Bibiane Schönlieb', 'Ran Gilad-Bachrach', 'Amir Globerson']","['cs.LG', 'cs.AI', 'stat.ML']",2024-11-04 17:03:52+00:00
http://arxiv.org/abs/2411.02253v1,Towards safe Bayesian optimization with Wiener kernel regression,"Bayesian Optimization (BO) is a data-driven strategy for
minimizing/maximizing black-box functions based on probabilistic surrogate
models. In the presence of safety constraints, the performance of BO crucially
relies on tight probabilistic error bounds related to the uncertainty
surrounding the surrogate model. For the case of Gaussian Process surrogates
and Gaussian measurement noise, we present a novel error bound based on the
recently proposed Wiener kernel regression. We prove that under rather mild
assumptions, the proposed error bound is tighter than bounds previously
documented in the literature which leads to enlarged safety regions. We draw
upon a numerical example to demonstrate the efficacy of the proposed error
bound in safe BO.","['Oleksii Molodchyk', 'Johannes Teutsch', 'Timm Faulwasser']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY', 'math.OC']",2024-11-04 16:43:16+00:00
http://arxiv.org/abs/2411.02239v1,Powerful batch conformal prediction for classification,"In a supervised classification split conformal/inductive framework with $K$
classes, a calibration sample of $n$ labeled examples is observed for inference
on the label of a new unlabeled example. In this work, we explore the case
where a ""batch"" of $m$ independent such unlabeled examples is given, and a
multivariate prediction set with $1-\alpha$ coverage should be provided for
this batch. Hence, the batch prediction set takes the form of a collection of
label vectors of size $m$, while the calibration sample only contains
univariate labels. Using the Bonferroni correction consists in concatenating
the individual prediction sets at level $1-\alpha/m$ (Vovk 2013). We propose a
uniformly more powerful solution, based on specific combinations of conformal
$p$-values that exploit the Simes inequality (Simes 1986). Intuitively, the
pooled evidence of fairly ""easy"" examples of the batch can help provide
narrower batch prediction sets. We also introduced adaptive versions of the
novel procedure that are particularly effective when the batch prediction set
is expected to be large. The theoretical guarantees are provided when all
examples are iid, as well as more generally when iid is assumed only
conditionally within each class. In particular, our results are also valid
under a label distribution shift since the distribution of the labels need not
be the same in the calibration sample and in the new `batch'. The usefulness of
the method is illustrated on synthetic and real data examples.","['Ulysse Gazin', 'Ruth Heller', 'Etienne Roquain', 'Aldo Solari']","['stat.ME', 'stat.ML']",2024-11-04 16:32:13+00:00
http://arxiv.org/abs/2411.02225v1,Variable Selection in Convex Piecewise Linear Regression,"This paper presents Sparse Gradient Descent as a solution for variable
selection in convex piecewise linear regression where the model is given as
$\mathrm{max}\langle a_j^\star, x \rangle + b_j^\star$ for $j = 1,\dots,k$
where $x \in \mathbb R^d$ is the covariate vector. Here,
$\{a_j^\star\}_{j=1}^k$ and $\{b_j^\star\}_{j=1}^k$ denote the ground-truth
weight vectors and intercepts. A non-asymptotic local convergence analysis is
provided for Sp-GD under sub-Gaussian noise when the covariate distribution
satisfies sub-Gaussianity and anti-concentration property. When the model order
and parameters are fixed, Sp-GD provides an $\epsilon$-accurate estimate given
$\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ observations where
$\sigma_z^2$ denotes the noise variance. This also implies the exact parameter
recovery by Sp-GD from $\mathcal{O}(s\log(d/s))$ noise-free observations. Since
optimizing the squared loss for sparse max-affine is non-convex, an
initialization scheme is proposed to provide a suitable initial estimate within
the basin of attraction for Sp-GD, i.e. sufficiently accurate to invoke the
convergence guarantees. The initialization scheme uses sparse principal
component analysis to estimate the subspace spanned by $\{ a_j^\star\}_{j=1}^k$
then applies an $r$-covering search to estimate the model parameters. A
non-asymptotic analysis is presented for this initialization scheme when the
covariates and noise samples follow Gaussian distributions. When the model
order and parameters are fixed, this initialization scheme provides an
$\epsilon$-accurate estimate given
$\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$
observations. Numerical Monte Carlo results corroborate theoretical findings
for Sp-GD and the initialization scheme.","['Haitham Kanj', 'Seonho Kim', 'Kiryung Lee']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",2024-11-04 16:19:09+00:00
http://arxiv.org/abs/2411.02221v1,Targeted Learning for Variable Importance,"Variable importance is one of the most widely used measures for interpreting
machine learning with significant interest from both statistics and machine
learning communities. Recently, increasing attention has been directed toward
uncertainty quantification in these metrics. Current approaches largely rely on
one-step procedures, which, while asymptotically efficient, can present higher
sensitivity and instability in finite sample settings. To address these
limitations, we propose a novel method by employing the targeted learning (TL)
framework, designed to enhance robustness in inference for variable importance
metrics. Our approach is particularly suited for conditional permutation
variable importance. We show that it (i) retains the asymptotic efficiency of
traditional methods, (ii) maintains comparable computational complexity, and
(iii) delivers improved accuracy, especially in finite sample contexts. We
further support these findings with numerical experiments that illustrate the
practical advantages of our method and validate the theoretical results.","['Xiaohan Wang', 'Yunzhe Zhou', 'Giles Hooker']","['stat.ML', 'cs.LG', 'stat.ME']",2024-11-04 16:14:45+00:00
