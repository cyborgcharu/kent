id,title,abstract,authors,categories,date
http://arxiv.org/abs/2107.13656v1,Characterizing the Generalization Error of Gibbs Algorithm with Symmetrized KL information,"Bounding the generalization error of a supervised learning algorithm is one
of the most important problems in learning theory, and various approaches have
been developed. However, existing bounds are often loose and lack of
guarantees. As a result, they may fail to characterize the exact generalization
ability of a learning algorithm. Our main contribution is an exact
characterization of the expected generalization error of the well-known Gibbs
algorithm in terms of symmetrized KL information between the input training
samples and the output hypothesis. Such a result can be applied to tighten
existing expected generalization error bound. Our analysis provides more
insight on the fundamental role the symmetrized KL information plays in
controlling the generalization error of the Gibbs algorithm.","['Gholamali Aminian', 'Yuheng Bu', 'Laura Toni', 'Miguel R. D. Rodrigues', 'Gregory Wornell']","['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2021-07-28 22:20:34+00:00
http://arxiv.org/abs/2107.13610v2,Large sample spectral analysis of graph-based multi-manifold clustering,"In this work we study statistical properties of graph-based algorithms for
multi-manifold clustering (MMC). In MMC the goal is to retrieve the
multi-manifold structure underlying a given Euclidean data set when this one is
assumed to be obtained by sampling a distribution on a union of manifolds
$\mathcal{M} = \mathcal{M}_1 \cup\dots \cup \mathcal{M}_N$ that may intersect
with each other and that may have different dimensions. We investigate
sufficient conditions that similarity graphs on data sets must satisfy in order
for their corresponding graph Laplacians to capture the right geometric
information to solve the MMC problem. Precisely, we provide high probability
error bounds for the spectral approximation of a tensorized Laplacian on
$\mathcal{M}$ with a suitable graph Laplacian built from the observations; the
recovered tensorized Laplacian contains all geometric information of all the
individual underlying manifolds. We provide an example of a family of
similarity graphs, which we call annular proximity graphs with angle
constraints, satisfying these sufficient conditions. We contrast our family of
graphs with other constructions in the literature based on the alignment of
tangent planes. Extensive numerical experiments expand the insights that our
theory provides on the MMC problem.","['Nicolas Garcia Trillos', 'Pengfei He', 'Chenghui Li']","['cs.LG', 'math.DG', 'math.SP', 'stat.ML']",2021-07-28 19:39:12+00:00
http://arxiv.org/abs/2107.13494v5,Limit Distribution Theory for the Smooth 1-Wasserstein Distance with Applications,"The smooth 1-Wasserstein distance (SWD) $W_1^\sigma$ was recently proposed as
a means to mitigate the curse of dimensionality in empirical approximation
while preserving the Wasserstein structure. Indeed, SWD exhibits parametric
convergence rates and inherits the metric and topological structure of the
classic Wasserstein distance. Motivated by the above, this work conducts a
thorough statistical study of the SWD, including a high-dimensional limit
distribution result for empirical $W_1^\sigma$, bootstrap consistency,
concentration inequalities, and Berry-Esseen type bounds. The derived
nondegenerate limit stands in sharp contrast with the classic empirical $W_1$,
for which a similar result is known only in the one-dimensional case. We also
explore asymptotics and characterize the limit distribution when the smoothing
parameter $\sigma$ is scaled with $n$, converging to $0$ at a sufficiently slow
rate. The dimensionality of the sampled distribution enters empirical SWD
convergence bounds only through the prefactor (i.e., the constant). We provide
a sharp characterization of this prefactor's dependence on the smoothing
parameter and the intrinsic dimension. This result is then used to derive new
empirical convergence rates for classic $W_1$ in terms of the intrinsic
dimension. As applications of the limit distribution theory, we study
two-sample testing and minimum distance estimation (MDE) under $W_1^\sigma$. We
establish asymptotic validity of SWD testing, while for MDE, we prove
measurability, almost sure convergence, and limit distributions for optimal
estimators and their corresponding $W_1^\sigma$ error. Our results suggest that
the SWD is well suited for high-dimensional statistical learning and inference.","['Ritwik Sadhu', 'Ziv Goldfeld', 'Kengo Kato']","['math.ST', 'math.PR', 'stat.ML', 'stat.TH', '62E17, 60F05, 60F17, 62G10, 62F12, 62F40']",2021-07-28 17:02:24+00:00
http://arxiv.org/abs/2107.13304v1,Bayesian Autoencoders: Analysing and Fixing the Bernoulli likelihood for Out-of-Distribution Detection,"After an autoencoder (AE) has learnt to reconstruct one dataset, it might be
expected that the likelihood on an out-of-distribution (OOD) input would be
low. This has been studied as an approach to detect OOD inputs. Recent work
showed this intuitive approach can fail for the dataset pairs FashionMNIST vs
MNIST. This paper suggests this is due to the use of Bernoulli likelihood and
analyses why this is the case, proposing two fixes: 1) Compute the uncertainty
of likelihood estimate by using a Bayesian version of the AE. 2) Use
alternative distributions to model the likelihood.","['Bang Xiang Yong', 'Tim Pearce', 'Alexandra Brintrup']","['cs.LG', 'stat.ML']",2021-07-28 11:51:35+00:00
http://arxiv.org/abs/2107.13163v3,Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers,"A common lens to theoretically study neural net architectures is to analyze
the functions they can approximate. However, constructions from approximation
theory may be unrealistic and therefore less meaningful. For example, a common
unrealistic trick is to encode target function values using infinite precision.
To address these issues, this work proposes a formal definition of
statistically meaningful (SM) approximation which requires the approximating
network to exhibit good statistical learnability. We study SM approximation for
two function classes: boolean circuits and Turing machines. We show that
overparameterized feedforward neural nets can SM approximate boolean circuits
with sample complexity depending only polynomially on the circuit size, not the
size of the network. In addition, we show that transformers can SM approximate
Turing machines with computation time bounded by $T$ with sample complexity
polynomial in the alphabet size, state space size, and $\log (T)$. We also
introduce new tools for analyzing generalization which provide much tighter
sample complexities than the typical VC-dimension or norm-based bounds, which
may be of independent interest.","['Colin Wei', 'Yining Chen', 'Tengyu Ma']","['cs.LG', 'stat.ML']",2021-07-28 04:28:55+00:00
http://arxiv.org/abs/2107.13090v2,Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games,"We consider a general-sum N-player linear-quadratic game with stochastic
dynamics over a finite horizon and prove the global convergence of the natural
policy gradient method to the Nash equilibrium. In order to prove the
convergence of the method, we require a certain amount of noise in the system.
We give a condition, essentially a lower bound on the covariance of the noise
in terms of the model parameters, in order to guarantee convergence. We
illustrate our results with numerical experiments to show that even in
situations where the policy gradient method may not converge in the
deterministic setting, the addition of noise leads to convergence.","['Ben Hambly', 'Renyuan Xu', 'Huining Yang']","['math.OC', 'cs.GT', 'cs.LG', 'stat.ML']",2021-07-27 22:08:41+00:00
http://arxiv.org/abs/2107.13068v3,End-to-End Balancing for Causal Continuous Treatment-Effect Estimation,"We study the problem of observational causal inference with continuous
treatments in the framework of inverse propensity-score weighting. To obtain
stable weights, we design a new algorithm based on entropy balancing that
learns weights to directly maximize causal inference accuracy using end-to-end
optimization. In the process of optimization, these weights are automatically
tuned to the specific dataset and causal inference algorithm being used. We
provide a theoretical analysis demonstrating consistency of our approach. Using
synthetic and real-world data, we show that our algorithm estimates causal
effect more accurately than baseline entropy balancing.","['Mohammad Taha Bahadori', 'Eric Tchetgen Tchetgen', 'David E. Heckerman']","['cs.LG', 'stat.ME', 'stat.ML', '62D20', 'I.2.6']",2021-07-27 20:04:59+00:00
http://arxiv.org/abs/2107.13059v1,Explicit Pairwise Factorized Graph Neural Network for Semi-Supervised Node Classification,"Node features and structural information of a graph are both crucial for
semi-supervised node classification problems. A variety of graph neural network
(GNN) based approaches have been proposed to tackle these problems, which
typically determine output labels through feature aggregation. This can be
problematic, as it implies conditional independence of output nodes given
hidden representations, despite their direct connections in the graph. To learn
the direct influence among output nodes in a graph, we propose the Explicit
Pairwise Factorized Graph Neural Network (EPFGNN), which models the whole graph
as a partially observed Markov Random Field. It contains explicit pairwise
factors to model output-output relations and uses a GNN backbone to model
input-output relations. To balance model complexity and expressivity, the
pairwise factors have a shared component and a separate scaling coefficient for
each edge. We apply the EM algorithm to train our model, and utilize a
star-shaped piecewise likelihood for the tractable surrogate objective. We
conduct experiments on various datasets, which shows that our model can
effectively improve the performance for semi-supervised node classification on
graphs.","['Yu Wang', 'Yuesong Shen', 'Daniel Cremers']","['cs.LG', 'stat.ML']",2021-07-27 19:47:53+00:00
http://arxiv.org/abs/2107.12972v1,Channel-Wise Early Stopping without a Validation Set via NNK Polytope Interpolation,"State-of-the-art neural network architectures continue to scale in size and
deliver impressive generalization results, although this comes at the expense
of limited interpretability. In particular, a key challenge is to determine
when to stop training the model, as this has a significant impact on
generalization. Convolutional neural networks (ConvNets) comprise
high-dimensional feature spaces formed by the aggregation of multiple channels,
where analyzing intermediate data representations and the model's evolution can
be challenging owing to the curse of dimensionality. We present channel-wise
DeepNNK (CW-DeepNNK), a novel channel-wise generalization estimate based on
non-negative kernel regression (NNK) graphs with which we perform local
polytope interpolation on low-dimensional channels. This method leads to
instance-based interpretability of both the learned data representations and
the relationship between channels. Motivated by our observations, we use
CW-DeepNNK to propose a novel early stopping criterion that (i) does not
require a validation set, (ii) is based on a task performance metric, and (iii)
allows stopping to be reached at different points for each channel. Our
experiments demonstrate that our proposed method has advantages as compared to
the standard criterion based on validation set performance.","['David Bonet', 'Antonio Ortega', 'Javier Ruiz-Hidalgo', 'Sarath Shekkizhar']","['cs.LG', 'stat.ML']",2021-07-27 17:33:30+00:00
http://arxiv.org/abs/2107.13430v2,Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary,"This study proposes multivariate kernel density estimation by stagewise
minimization algorithm based on $U$-divergence and a simple dictionary. The
dictionary consists of an appropriate scalar bandwidth matrix and a part of the
original data. The resulting estimator brings us data-adaptive weighting
parameters and bandwidth matrices, and realizes a sparse representation of
kernel density estimation. We develop the non-asymptotic error bound of
estimator obtained via the proposed stagewise minimization algorithm. It is
confirmed from simulation studies that the proposed estimator performs
competitive to or sometime better than other well-known density estimators.","['Kiheiji Nishida', 'Kanta Naito']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2021-07-27 17:05:06+00:00
http://arxiv.org/abs/2107.12940v1,Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm,"Validating the safety of autonomous systems generally requires the use of
high-fidelity simulators that adequately capture the variability of real-world
scenarios. However, it is generally not feasible to exhaustively search the
space of simulation scenarios for failures. Adaptive stress testing (AST) is a
method that uses reinforcement learning to find the most likely failure of a
system. AST with a deep reinforcement learning solver has been shown to be
effective in finding failures across a range of different systems. This
approach generally involves running many simulations, which can be very
expensive when using a high-fidelity simulator. To improve efficiency, we
present a method that first finds failures in a low-fidelity simulator. It then
uses the backward algorithm, which trains a deep neural network policy using a
single expert demonstration, to adapt the low-fidelity failures to
high-fidelity. We have created a series of autonomous vehicle validation case
studies that represent some of the ways low-fidelity and high-fidelity
simulators can differ, such as time discretization. We demonstrate in a variety
of case studies that this new AST approach is able to find failures with
significantly fewer high-fidelity simulation steps than are needed when just
running AST directly in high-fidelity. As a proof of concept, we also
demonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art
high-fidelity simulator for finding failures in autonomous vehicles.","['Mark Koren', 'Ahmed Nassar', 'Mykel J. Kochenderfer']","['cs.LG', 'stat.ML']",2021-07-27 16:54:04+00:00
http://arxiv.org/abs/2107.12890v2,Subset selection for linear mixed models,"Linear mixed models (LMMs) are instrumental for regression analysis with
structured dependence, such as grouped, clustered, or multilevel data. However,
selection among the covariates--while accounting for this structured
dependence--remains a challenge. We introduce a Bayesian decision analysis for
subset selection with LMMs. Using a Mahalanobis loss function that incorporates
the structured dependence, we derive optimal linear coefficients for (i) any
given subset of variables and (ii) all subsets of variables that satisfy a
cardinality constraint. Crucially, these estimates inherit shrinkage or
regularization and uncertainty quantification from the underlying Bayesian
model, and apply for any well-specified Bayesian LMM. More broadly, our
decision analysis strategy deemphasizes the role of a single ""best"" subset,
which is often unstable and limited in its information content, and instead
favors a collection of near-optimal subsets. This collection is summarized by
key member subsets and variable-specific importance metrics. Customized subset
search and out-of-sample approximation algorithms are provided for more
scalable computing. These tools are applied to simulated data and a
longitudinal physical activity dataset, and demonstrate excellent prediction,
estimation, and selection ability.",['Daniel R. Kowal'],"['stat.ME', 'stat.CO', 'stat.ML']",2021-07-27 15:47:44+00:00
http://arxiv.org/abs/2107.12825v1,Individual Survival Curves with Conditional Normalizing Flows,"Survival analysis, or time-to-event modelling, is a classical statistical
problem that has garnered a lot of interest for its practical use in
epidemiology, demographics or actuarial sciences. Recent advances on the
subject from the point of view of machine learning have been concerned with
precise per-individual predictions instead of population studies, driven by the
rise of individualized medicine. We introduce here a conditional normalizing
flow based estimate of the time-to-event density as a way to model highly
flexible and individualized conditional survival distributions. We use a novel
hierarchical formulation of normalizing flows to enable efficient fitting of
flexible conditional distributions without overfitting and show how the
normalizing flow formulation can be efficiently adapted to the censored
setting. We experimentally validate the proposed approach on a synthetic
dataset as well as four open medical datasets and an example of a common
financial problem.","['Guillaume Ausset', 'Tom Ciffreo', 'Francois Portier', 'Stephan Clémençon', 'Timothée Papin']","['cs.LG', 'stat.ML']",2021-07-27 13:45:12+00:00
http://arxiv.org/abs/2107.12783v1,Statistical Guarantees for Fairness Aware Plug-In Algorithms,"A plug-in algorithm to estimate Bayes Optimal Classifiers for fairness-aware
binary classification has been proposed in (Menon & Williamson, 2018). However,
the statistical efficacy of their approach has not been established. We prove
that the plug-in algorithm is statistically consistent. We also derive finite
sample guarantees associated with learning the Bayes Optimal Classifiers via
the plug-in algorithm. Finally, we propose a protocol that modifies the plug-in
approach, so as to simultaneously guarantee fairness and differential privacy
with respect to a binary feature deemed sensitive.","['Drona Khurana', 'Srinivasan Ravichandran', 'Sparsh Jain', 'Narayanan Unny Edakunni']","['stat.ML', 'cs.LG']",2021-07-27 12:51:33+00:00
http://arxiv.org/abs/2107.12723v2,Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel,"We revisit on-average algorithmic stability of GD for training
overparameterised shallow neural networks and prove new generalisation and
excess risk bounds without the NTK or PL assumptions. In particular, we show
oracle type bounds which reveal that the generalisation and excess risk of GD
is controlled by an interpolating network with the shortest GD path from
initialisation (in a sense, an interpolating network with the smallest relative
norm). While this was known for kernelised interpolants, our proof applies
directly to networks trained by GD without intermediate kernelisation. At the
same time, by relaxing oracle inequalities developed here we recover existing
NTK-based risk bounds in a straightforward way, which demonstrates that our
analysis is tighter. Finally, unlike most of the NTK-based analyses we focus on
regression with label noise and show that GD with early stopping is consistent.","['Dominic Richards', 'Ilja Kuzborskij']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2021-07-27 10:53:15+00:00
http://arxiv.org/abs/2107.12685v1,On the Role of Optimization in Double Descent: A Least Squares Study,"Empirically it has been observed that the performance of deep neural networks
steadily improves as we increase model size, contradicting the classical view
on overfitting and generalization. Recently, the double descent phenomena has
been proposed to reconcile this observation with theory, suggesting that the
test error has a second descent when the model becomes sufficiently
overparameterized, as the model size itself acts as an implicit regularizer. In
this paper we add to the growing body of work in this space, providing a
careful study of learning dynamics as a function of model size for the least
squares scenario. We show an excess risk bound for the gradient descent
solution of the least squares objective. The bound depends on the smallest
non-zero eigenvalue of the covariance matrix of the input features, via a
functional form that has the double descent behavior. This gives a new
perspective on the double descent curves reported in the literature. Our
analysis of the excess risk allows to decouple the effect of optimization and
generalization error. In particular, we find that in case of noiseless
regression, double descent is explained solely by optimization-related
quantities, which was missed in studies focusing on the Moore-Penrose
pseudoinverse solution. We believe that our derivation provides an alternative
view compared to existing work, shedding some light on a possible cause of this
phenomena, at least in the considered least squares setting. We empirically
explore if our predictions hold for neural networks, in particular whether the
covariance of intermediary hidden activations has a similar behavior as the one
predicted by our derivations.","['Ilja Kuzborskij', 'Csaba Szepesvári', 'Omar Rivasplata', 'Amal Rannen-Triki', 'Razvan Pascanu']","['cs.LG', 'math.OC', 'stat.ML']",2021-07-27 09:13:11+00:00
http://arxiv.org/abs/2107.12580v2,Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization,"Central to the success of artificial neural networks is their ability to
generalize. But does neural network generalization primarily rely on seeing
highly similar training examples (memorization)? Or are neural networks capable
of human-intelligence styled reasoning, and if so, to what extent? These remain
fundamental open questions on artificial neural networks. In this paper, as
steps towards answering these questions, we introduce a new benchmark, Pointer
Value Retrieval (PVR) to study the limits of neural network reasoning. The PVR
suite of tasks is based on reasoning about indirection, a hallmark of human
intelligence, where a first stage (task) contains instructions for solving a
second stage (task). In PVR, this is done by having one part of the task input
act as a pointer, giving instructions on a different input location, which
forms the output. We show this simple rule can be applied to create a diverse
set of tasks across different input modalities and configurations. Importantly,
this use of indirection enables systematically varying task difficulty through
distribution shifts and increasing functional complexity. We conduct a detailed
empirical study of different PVR tasks, discovering large variations in
performance across dataset sizes, neural network architectures and task
complexity. Further, by incorporating distribution shift and increased
functional complexity, we develop nuanced tests for reasoning, revealing subtle
failures and surprising successes, suggesting many promising directions of
exploration on this benchmark.","['Chiyuan Zhang', 'Maithra Raghu', 'Jon Kleinberg', 'Samy Bengio']","['cs.LG', 'cs.AI', 'stat.ML']",2021-07-27 03:50:31+00:00
http://arxiv.org/abs/2107.12525v2,Proof: Accelerating Approximate Aggregation Queries with Expensive Predicates,"Given a dataset $\mathcal{D}$, we are interested in computing the mean of a
subset of $\mathcal{D}$ which matches a predicate. ABae leverages stratified
sampling and proxy models to efficiently compute this statistic given a
sampling budget $N$. In this document, we theoretically analyze ABae and show
that the MSE of the estimate decays at rate $O(N_1^{-1} + N_2^{-1} +
N_1^{1/2}N_2^{-3/2})$, where $N=K \cdot N_1+N_2$ for some integer constant $K$
and $K \cdot N_1$ and $N_2$ represent the number of samples used in Stage 1 and
Stage 2 of ABae respectively. Hence, if a constant fraction of the total sample
budget $N$ is allocated to each stage, we will achieve a mean squared error of
$O(N^{-1})$ which matches the rate of mean squared error of the optimal
stratified sampling algorithm given a priori knowledge of the predicate
positive rate and standard deviation per stratum.","['Daniel Kang', 'John Guibas', 'Peter Bailis', 'Tatsunori Hashimoto', 'Yi Sun', 'Matei Zaharia']","['math.ST', 'cs.DB', 'cs.LG', 'stat.ML', 'stat.TH']",2021-07-27 00:18:21+00:00
http://arxiv.org/abs/2107.12521v2,Restricted Boltzmann Machine and Deep Belief Network: Tutorial and Survey,"This is a tutorial and survey paper on Boltzmann Machine (BM), Restricted
Boltzmann Machine (RBM), and Deep Belief Network (DBN). We start with the
required background on probabilistic graphical models, Markov random field,
Gibbs sampling, statistical physics, Ising model, and the Hopfield network.
Then, we introduce the structures of BM and RBM. The conditional distributions
of visible and hidden variables, Gibbs sampling in RBM for generating
variables, training BM and RBM by maximum likelihood estimation, and
contrastive divergence are explained. Then, we discuss different possible
discrete and continuous distributions for the variables. We introduce
conditional RBM and how it is trained. Finally, we explain deep belief network
as a stack of RBM models. This paper on Boltzmann machines can be useful in
various fields including data science, statistics, neural computation, and
statistical physics.","['Benyamin Ghojogh', 'Ali Ghodsi', 'Fakhri Karray', 'Mark Crowley']","['cs.LG', 'cs.NE', 'physics.data-an', 'stat.ML']",2021-07-26 23:59:12+00:00
http://arxiv.org/abs/2107.12438v4,"Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization","Motivated by the poor performance of cross-validation in settings where data
are scarce, we propose a novel estimator of the out-of-sample performance of a
policy in data-driven optimization.Our approach exploits the optimization
problem's sensitivity analysis to estimate the gradient of the optimal
objective value with respect to the amount of noise in the data and uses the
estimated gradient to debias the policy's in-sample performance. Unlike
cross-validation techniques, our approach avoids sacrificing data for a test
set, utilizes all data when training and, hence, is well-suited to settings
where data are scarce. We prove bounds on the bias and variance of our
estimator for optimization problems with uncertain linear objectives but known,
potentially non-convex, feasible regions. For more specialized optimization
problems where the feasible region is ""weakly-coupled"" in a certain sense, we
prove stronger results. Specifically, we provide explicit high-probability
bounds on the error of our estimator that hold uniformly over a policy class
and depends on the problem's dimension and policy class's complexity. Our
bounds show that under mild conditions, the error of our estimator vanishes as
the dimension of the optimization problem grows, even if the amount of
available data remains small and constant. Said differently, we prove our
estimator performs well in the small-data, large-scale regime. Finally, we
numerically compare our proposed method to state-of-the-art approaches through
a case-study on dispatching emergency medical response services using real
data. Our method provides more accurate estimates of out-of-sample performance
and learns better-performing policies.","['Vishal Gupta', 'Michael Huang', 'Paat Rusmevichientong']","['math.OC', 'cs.LG', 'stat.ML']",2021-07-26 19:00:51+00:00
http://arxiv.org/abs/2107.12365v2,Inference for Heteroskedastic PCA with Missing Data,"This paper studies how to construct confidence regions for principal
component analysis (PCA) in high dimension, a problem that has been vastly
under-explored. While computing measures of uncertainty for nonlinear/nonconvex
estimators is in general difficult in high dimension, the challenge is further
compounded by the prevalent presence of missing data and heteroskedastic noise.
We propose a novel approach to performing valid inference on the principal
subspace under a spiked covariance model with missing data, on the basis of an
estimator called HeteroPCA (Zhang et al., 2022). We develop non-asymptotic
distributional guarantees for HeteroPCA, and demonstrate how these can be
invoked to compute both confidence regions for the principal subspace and
entrywise confidence intervals for the spiked covariance matrix. Our inference
procedures are fully data-driven and adaptive to heteroskedastic random noise,
without requiring prior knowledge about the noise levels.","['Yuling Yan', 'Yuxin Chen', 'Jianqing Fan']","['math.ST', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']",2021-07-26 17:59:01+00:00
http://arxiv.org/abs/2107.12364v3,Plugin Estimation of Smooth Optimal Transport Maps,"We analyze a number of natural estimators for the optimal transport map
between two distributions and show that they are minimax optimal. We adopt the
plugin approach: our estimators are simply optimal couplings between measures
derived from our observations, appropriately extended so that they define
functions on $\mathbb{R}^d$. When the underlying map is assumed to be
Lipschitz, we show that computing the optimal coupling between the empirical
measures, and extending it using linear smoothers, already gives a minimax
optimal estimator. When the underlying map enjoys higher regularity, we show
that the optimal coupling between appropriate nonparametric density estimates
yields faster rates. Our work also provides new bounds on the risk of
corresponding plugin estimators for the quadratic Wasserstein distance, and we
show how this problem relates to that of estimating optimal transport maps
using stability arguments for smooth and strongly convex Brenier potentials. As
an application of our results, we derive central limit theorems for plugin
estimators of the squared Wasserstein distance, which are centered at their
population counterpart when the underlying distributions have sufficiently
smooth densities. In contrast to known central limit theorems for empirical
estimators, this result easily lends itself to statistical inference for the
quadratic Wasserstein distance.","['Tudor Manole', 'Sivaraman Balakrishnan', 'Jonathan Niles-Weed', 'Larry Wasserman']","['math.ST', 'stat.ML', 'stat.TH']",2021-07-26 17:58:48+00:00
http://arxiv.org/abs/2107.12797v1,Wasserstein-Splitting Gaussian Process Regression for Heterogeneous Online Bayesian Inference,"Gaussian processes (GPs) are a well-known nonparametric Bayesian inference
technique, but they suffer from scalability problems for large sample sizes,
and their performance can degrade for non-stationary or spatially heterogeneous
data. In this work, we seek to overcome these issues through (i) employing
variational free energy approximations of GPs operating in tandem with online
expectation propagation steps; and (ii) introducing a local splitting step
which instantiates a new GP whenever the posterior distribution changes
significantly as quantified by the Wasserstein metric over posterior
distributions. Over time, then, this yields an ensemble of sparse GPs which may
be updated incrementally, and adapts to locality, heterogeneity, and
non-stationarity in training data.","['Michael E. Kepler', 'Alec Koppel', 'Amrit Singh Bedi', 'Daniel J. Stilwell']","['stat.ML', 'cs.LG']",2021-07-26 17:52:46+00:00
http://arxiv.org/abs/2107.12250v1,Uncertainty-Aware Time-to-Event Prediction using Deep Kernel Accelerated Failure Time Models,"Recurrent neural network based solutions are increasingly being used in the
analysis of longitudinal Electronic Health Record data. However, most works
focus on prediction accuracy and neglect prediction uncertainty. We propose
Deep Kernel Accelerated Failure Time models for the time-to-event prediction
task, enabling uncertainty-awareness of the prediction by a pipeline of a
recurrent neural network and a sparse Gaussian Process. Furthermore, a deep
metric learning based pre-training step is adapted to enhance the proposed
model. Our model shows better point estimate performance than recurrent neural
network based baselines in experiments on two real-world datasets. More
importantly, the predictive variance from our model can be used to quantify the
uncertainty estimates of the time-to-event prediction: Our model delivers
better performance when it is more confident in its prediction. Compared to
related methods, such as Monte Carlo Dropout, our model offers better
uncertainty estimates by leveraging an analytical solution and is more
computationally efficient.","['Zhiliang Wu', 'Yinchong Yang', 'Peter A. Fasching', 'Volker Tresp']","['cs.LG', 'stat.ML']",2021-07-26 14:55:02+00:00
http://arxiv.org/abs/2107.12248v1,Are Bayesian neural networks intrinsically good at out-of-distribution detection?,"The need to avoid confident predictions on unfamiliar data has sparked
interest in out-of-distribution (OOD) detection. It is widely assumed that
Bayesian neural networks (BNN) are well suited for this task, as the endowed
epistemic uncertainty should lead to disagreement in predictions on outliers.
In this paper, we question this assumption and provide empirical evidence that
proper Bayesian inference with common neural network architectures does not
necessarily lead to good OOD detection. To circumvent the use of approximate
inference, we start by studying the infinite-width case, where Bayesian
inference can be exact considering the corresponding Gaussian process.
Strikingly, the kernels induced under common architectural choices lead to
uncertainties that do not reflect the underlying data generating process and
are therefore unsuited for OOD detection. Finally, we study finite-width
networks using HMC, and observe OOD behavior that is consistent with the
infinite-width case. Overall, our study discloses fundamental problems when
naively using BNNs for OOD detection and opens interesting avenues for future
research.","['Christian Henning', ""Francesco D'Angelo"", 'Benjamin F. Grewe']","['cs.LG', 'stat.ML']",2021-07-26 14:53:14+00:00
http://arxiv.org/abs/2107.12100v2,Predicting Influential Higher-Order Patterns in Temporal Network Data,"Networks are frequently used to model complex systems comprised of
interacting elements. While edges capture the topology of direct interactions,
the true complexity of many systems originates from higher-order patterns in
paths by which nodes can indirectly influence each other. Path data,
representing ordered sequences of consecutive direct interactions, can be used
to model these patterns. On the one hand, to avoid overfitting, such models
should only consider those higher-order patterns for which the data provide
sufficient statistical evidence. On the other hand, we hypothesise that network
models, which capture only direct interactions, underfit higher-order patterns
present in data. Consequently, both approaches are likely to misidentify
influential nodes in complex networks. We contribute to this issue by proposing
five centrality measures based on MOGen, a multi-order generative model that
accounts for all indirect influences up to a maximum distance but disregards
influences at higher distances. We compare MOGen-based centralities to
equivalent measures for network models and path data in a prediction experiment
where we aim to identify influential nodes in out-of-sample data. Our results
show strong evidence supporting our hypothesis. MOGen consistently outperforms
both the network model and path-based prediction. We further show that the
performance difference between MOGen and the path-based approach disappears if
we have sufficient observations, confirming that the error is due to
overfitting.","['Christoph Gote', 'Vincenzo Perri', 'Ingo Scholtes']","['cs.SI', 'cs.IT', 'cs.LG', 'math.IT', 'physics.data-an', 'stat.ML']",2021-07-26 10:44:46+00:00
http://arxiv.org/abs/2107.12034v1,Workpiece Image-based Tool Wear Classification in Blanking Processes Using Deep Convolutional Neural Networks,"Blanking processes belong to the most widely used manufacturing techniques
due to their economic efficiency. Their economic viability depends to a large
extent on the resulting product quality and the associated customer
satisfaction as well as on possible downtimes. In particular, the occurrence of
increased tool wear reduces the product quality and leads to downtimes, which
is why considerable research has been carried out in recent years with regard
to wear detection. While processes have widely been monitored based on force
and acceleration signals, a new approach is pursued in this paper. Blanked
workpieces manufactured by punches with 16 different wear states are
photographed and then used as inputs for Deep Convolutional Neural Networks to
classify wear states. The results show that wear states can be predicted with
surprisingly high accuracy, opening up new possibilities and research
opportunities for tool wear monitoring of blanking processes.","['Dirk Alexander Molitor', 'Christian Kubik', 'Ruben Helmut Hetfleisch', 'Peter Groche']","['cs.LG', 'eess.IV', 'stat.ML']",2021-07-26 08:49:08+00:00
http://arxiv.org/abs/2107.11892v1,A brief note on understanding neural networks as Gaussian processes,"As a generalization of the work in [Lee et al., 2017], this note briefly
discusses when the prior of a neural network output follows a Gaussian process,
and how a neural-network-induced Gaussian process is formulated. The posterior
mean functions of such a Gaussian process regression lie in the reproducing
kernel Hilbert space defined by the neural-network-induced kernel. In the case
of two-layer neural networks, the induced Gaussian processes provide an
interpretation of the reproducing kernel Hilbert spaces whose union forms a
Barron space.",['Mengwu Guo'],"['cs.LG', 'cs.CE', 'stat.ML']",2021-07-25 21:06:58+00:00
http://arxiv.org/abs/2107.11869v3,Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities,"We introduce two data-driven procedures for optimal estimation and inference
in nonparametric models using instrumental variables. The first is a
data-driven choice of sieve dimension for a popular class of sieve two-stage
least squares estimators. When implemented with this choice, estimators of both
the structural function $h_0$ and its derivatives (such as elasticities)
converge at the fastest possible (i.e., minimax) rates in sup-norm. The second
is for constructing uniform confidence bands (UCBs) for $h_0$ and its
derivatives. Our UCBs guarantee coverage over a generic class of
data-generating processes and contract at the minimax rate, possibly up to a
logarithmic factor. As such, our UCBs are asymptotically more efficient than
UCBs based on the usual approach of undersmoothing. As an application, we
estimate the elasticity of the intensive margin of firm exports in a
monopolistic competition model of international trade. Simulations illustrate
the good performance of our procedures in empirically calibrated designs. Our
results provide evidence against common parameterizations of the distribution
of unobserved firm heterogeneity.","['Xiaohong Chen', 'Timothy Christensen', 'Sid Kankanala']","['econ.EM', 'stat.ME', 'stat.ML']",2021-07-25 18:46:33+00:00
http://arxiv.org/abs/2107.11858v2,Estimation of Stationary Optimal Transport Plans,"We study optimal transport for stationary stochastic processes taking values
in finite spaces. In order to reflect the stationarity of the underlying
processes, we restrict attention to stationary couplings, also known as
joinings. The resulting optimal joining problem captures differences in the
long run average behavior of the processes of interest. We introduce estimators
of both optimal joinings and the optimal joining cost, and we establish
consistency of the estimators under mild conditions. Furthermore, under
stronger mixing assumptions we establish finite-sample error rates for the
estimated optimal joining cost that extend the best known results in the iid
case. Finally, we extend the consistency and rate analysis to an
entropy-penalized version of the optimal joining problem.","[""Kevin O'Connor"", 'Kevin McGoff', 'Andrew B Nobel']","['math.ST', 'math.DS', 'stat.ML', 'stat.TH']",2021-07-25 17:46:21+00:00
http://arxiv.org/abs/2107.11774v4,SGD with a Constant Large Learning Rate Can Converge to Local Maxima,"Previous works on stochastic gradient descent (SGD) often focus on its
success. In this work, we construct worst-case optimization problems
illustrating that, when not in the regimes that the previous works often
assume, SGD can exhibit many strange and potentially undesirable behaviors.
Specifically, we construct landscapes and data distributions such that (1) SGD
converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly,
(3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local
maxima. We also realize results in a minimal neural network-like example. Our
results highlight the importance of simultaneously analyzing the minibatch
sampling, discrete-time updates rules, and realistic landscapes to understand
the role of SGD in deep learning.","['Liu Ziyin', 'Botao Li', 'James B. Simon', 'Masahito Ueda']","['cs.LG', 'math.OC', 'stat.ML']",2021-07-25 10:12:18+00:00
http://arxiv.org/abs/2107.11717v1,Invariance-based Multi-Clustering of Latent Space Embeddings for Equivariant Learning,"Variational Autoencoders (VAEs) have been shown to be remarkably effective in
recovering model latent spaces for several computer vision tasks. However,
currently trained VAEs, for a number of reasons, seem to fall short in learning
invariant and equivariant clusters in latent space. Our work focuses on
providing solutions to this problem and presents an approach to disentangle
equivariance feature maps in a Lie group manifold by enforcing deep,
group-invariant learning. Simultaneously implementing a novel separation of
semantic and equivariant variables of the latent space representation, we
formulate a modified Evidence Lower BOund (ELBO) by using a mixture model pdf
like Gaussian mixtures for invariant cluster embeddings that allows superior
unsupervised variational clustering. Our experiments show that this model
effectively learns to disentangle the invariant and equivariant representations
with significant improvements in the learning rate and an observably superior
image recognition and canonical state reconstruction compared to the currently
best deep learning models.","['Chandrajit Bajaj', 'Avik Roy', 'Haoran Zhang']","['cs.LG', 'stat.ML']",2021-07-25 03:27:47+00:00
http://arxiv.org/abs/2107.11712v2,Efficient inference of interventional distributions,"We consider the problem of efficiently inferring interventional distributions
in a causal Bayesian network from a finite number of observations. Let
$\mathcal{P}$ be a causal model on a set $\mathbf{V}$ of observable variables
on a given causal graph $G$. For sets $\mathbf{X},\mathbf{Y}\subseteq
\mathbf{V}$, and setting ${\bf x}$ to $\mathbf{X}$, let $P_{\bf x}(\mathbf{Y})$
denote the interventional distribution on $\mathbf{Y}$ with respect to an
intervention ${\bf x}$ to variables ${\bf x}$. Shpitser and Pearl (AAAI 2006),
building on the work of Tian and Pearl (AAAI 2001), gave an exact
characterization of the class of causal graphs for which the interventional
distribution $P_{\bf x}({\mathbf{Y}})$ can be uniquely determined. We give the
first efficient version of the Shpitser-Pearl algorithm. In particular, under
natural assumptions, we give a polynomial-time algorithm that on input a causal
graph $G$ on observable variables $\mathbf{V}$, a setting ${\bf x}$ of a set
$\mathbf{X} \subseteq \mathbf{V}$ of bounded size, outputs succinct
descriptions of both an evaluator and a generator for a distribution $\hat{P}$
that is $\varepsilon$-close (in total variation distance) to $P_{\bf
x}({\mathbf{Y}})$ where $Y=\mathbf{V}\setminus \mathbf{X}$, if $P_{\bf
x}(\mathbf{Y})$ is identifiable. We also show that when $\mathbf{Y}$ is an
arbitrary set, there is no efficient algorithm that outputs an evaluator of a
distribution that is $\varepsilon$-close to $P_{\bf x}({\mathbf{Y}})$ unless
all problems that have statistical zero-knowledge proofs, including the Graph
Isomorphism problem, have efficient randomized algorithms.","['Arnab Bhattacharyya', 'Sutanu Gayen', 'Saravanan Kandasamy', 'Vedant Raval', 'N. V. Vinodchandran']","['cs.DS', 'cs.LG', 'stat.ML']",2021-07-25 02:40:01+00:00
http://arxiv.org/abs/2107.11662v1,Inference of collective Gaussian hidden Markov models,"We consider inference problems for a class of continuous state collective
hidden Markov models, where the data is recorded in aggregate (collective) form
generated by a large population of individuals following the same dynamics. We
propose an aggregate inference algorithm called collective Gaussian
forward-backward algorithm, extending recently proposed Sinkhorn belief
propagation algorithm to models characterized by Gaussian densities. Our
algorithm enjoys convergence guarantee. In addition, it reduces to the standard
Kalman filter when the observations are generated by a single individual. The
efficacy of the proposed algorithm is demonstrated through multiple
experiments.","['Rahul Singh', 'Yongxin Chen']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SP', 'eess.SY']",2021-07-24 17:49:01+00:00
http://arxiv.org/abs/2107.11658v2,Tail of Distribution GAN (TailGAN): Generative-Adversarial-Network-Based Boundary Formation,"Generative Adversarial Networks (GAN) are a powerful methodology and can be
used for unsupervised anomaly detection, where current techniques have
limitations such as the accurate detection of anomalies near the tail of a
distribution. GANs generally do not guarantee the existence of a probability
density and are susceptible to mode collapse, while few GANs use likelihood to
reduce mode collapse. In this paper, we create a GAN-based tail formation model
for anomaly detection, the Tail of distribution GAN (TailGAN), to generate
samples on the tail of the data distribution and detect anomalies near the
support boundary. Using TailGAN, we leverage GANs for anomaly detection and use
maximum entropy regularization. Using GANs that learn the probability of the
underlying distribution has advantages in improving the anomaly detection
methodology by allowing us to devise a generator for boundary samples, and use
this model to characterize anomalies. TailGAN addresses supports with disjoint
components and achieves competitive performance on images. We evaluate TailGAN
for identifying Out-of-Distribution (OoD) data and its performance evaluated on
MNIST, CIFAR-10, Baggage X-Ray, and OoD data shows competitiveness compared to
methods from the literature.","['Nikolaos Dionelis', 'Mehrdad Yaghoobi', 'Sotirios A. Tsaftaris']","['cs.LG', 'stat.ML']",2021-07-24 17:29:21+00:00
http://arxiv.org/abs/2107.11630v2,Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them,"Making classifiers robust to adversarial examples is hard. Thus, many
defenses tackle the seemingly easier task of detecting perturbed inputs. We
show a barrier towards this goal. We prove a general hardness reduction between
detection and classification of adversarial examples: given a robust detector
for attacks at distance {\epsilon} (in some metric), we can build a similarly
robust (but inefficient) classifier for attacks at distance {\epsilon}/2. Our
reduction is computationally inefficient, and thus cannot be used to build
practical classifiers. Instead, it is a useful sanity check to test whether
empirical detection results imply something much stronger than the authors
presumably anticipated. To illustrate, we revisit 13 detector defenses. For
11/13 cases, we show that the claimed detection results would imply an
inefficient classifier with robustness far beyond the state-of-the-art.",['Florian Tramèr'],"['cs.LG', 'cs.CR', 'stat.ML']",2021-07-24 15:14:53+00:00
http://arxiv.org/abs/2107.11609v1,A Model-Agnostic Algorithm for Bayes Error Determination in Binary Classification,"This paper presents the intrinsic limit determination algorithm (ILD
Algorithm), a novel technique to determine the best possible performance,
measured in terms of the AUC (area under the ROC curve) and accuracy, that can
be obtained from a specific dataset in a binary classification problem with
categorical features {\sl regardless} of the model used. This limit, namely the
Bayes error, is completely independent of any model used and describes an
intrinsic property of the dataset. The ILD algorithm thus provides important
information regarding the prediction limits of any binary classification
algorithm when applied to the considered dataset. In this paper the algorithm
is described in detail, its entire mathematical framework is presented and the
pseudocode is given to facilitate its implementation. Finally, an example with
a real dataset is given.","['Umberto Michelucci', 'Michela Sperti', 'Dario Piga', 'Francesca Venturini', 'Marco A. Deriu']","['cs.LG', 'cs.AI', 'stat.ML']",2021-07-24 13:55:31+00:00
http://arxiv.org/abs/2107.11533v1,Combining Online Learning and Offline Learning for Contextual Bandits with Deficient Support,"We address policy learning with logged data in contextual bandits. Current
offline-policy learning algorithms are mostly based on inverse propensity score
(IPS) weighting requiring the logging policy to have \emph{full support} i.e. a
non-zero probability for any context/action of the evaluation policy. However,
many real-world systems do not guarantee such logging policies, especially when
the action space is large and many actions have poor or missing rewards. With
such \emph{support deficiency}, the offline learning fails to find optimal
policies. We propose a novel approach that uses a hybrid of offline learning
with online exploration. The online exploration is used to explore unsupported
actions in the logged data whilst offline learning is used to exploit supported
actions from the logged data avoiding unnecessary explorations. Our approach
determines an optimal policy with theoretical guarantees using the minimal
number of online explorations. We demonstrate our algorithms' effectiveness
empirically on a diverse collection of datasets.","['Hung Tran-The', 'Sunil Gupta', 'Thanh Nguyen-Tang', 'Santu Rana', 'Svetha Venkatesh']","['stat.ML', 'cs.LG']",2021-07-24 05:07:43+00:00
http://arxiv.org/abs/2107.11433v5,A general sample complexity analysis of vanilla policy gradient,"We adapt recent tools developed for the analysis of Stochastic Gradient
Descent (SGD) in non-convex optimization to obtain convergence and sample
complexity guarantees for the vanilla policy gradient (PG). Our only
assumptions are that the expected return is smooth w.r.t. the policy
parameters, that its $H$-step truncated gradient is close to the exact
gradient, and a certain ABC assumption. This assumption requires the second
moment of the estimated gradient to be bounded by $A\geq 0$ times the
suboptimality gap, $B \geq 0$ times the norm of the full batch gradient and an
additive constant $C \geq 0$, or any combination of aforementioned. We show
that the ABC assumption is more general than the commonly used assumptions on
the policy space to prove convergence to a stationary point. We provide a
single convergence theorem that recovers the
$\widetilde{\mathcal{O}}(\epsilon^{-4})$ sample complexity of PG to a
stationary point. Our results also affords greater flexibility in the choice of
hyper parameters such as the step size and the batch size $m$, including the
single trajectory case (i.e., $m=1$). When an additional relaxed weak gradient
domination assumption is available, we establish a novel global optimum
convergence theory of PG with $\widetilde{\mathcal{O}}(\epsilon^{-3})$ sample
complexity. We then instantiate our theorems in different settings, where we
both recover existing results and obtain improved sample complexity, e.g.,
$\widetilde{\mathcal{O}}(\epsilon^{-3})$ sample complexity for the convergence
to the global optimum for Fisher-non-degenerated parametrized policies.","['Rui Yuan', 'Robert M. Gower', 'Alessandro Lazaric']","['cs.LG', 'math.OC', 'stat.ML']",2021-07-23 19:38:17+00:00
http://arxiv.org/abs/2107.11419v2,Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits,"We consider nonstationary multi-armed bandit problems where the model
parameters of the arms change over time. We introduce the adaptive resetting
bandit (ADR-bandit), a bandit algorithm class that leverages adaptive windowing
techniques from literature on data streams. We first provide new guarantees on
the quality of estimators resulting from adaptive windowing techniques, which
are of independent interest. Furthermore, we conduct a finite-time analysis of
ADR-bandit in two typical environments: an abrupt environment where changes
occur instantaneously and a gradual environment where changes occur
progressively. We demonstrate that ADR-bandit has nearly optimal performance
when abrupt or gradual changes occur in a coordinated manner that we call
global changes. We demonstrate that forced exploration is unnecessary when we
assume such global changes. Unlike the existing nonstationary bandit
algorithms, ADR-bandit has optimal performance in stationary environments as
well as nonstationary environments with global changes. Our experiments show
that the proposed algorithms outperform the existing approaches in synthetic
and real-world environments.","['Junpei Komiyama', 'Edouard Fouché', 'Junya Honda']","['stat.ML', 'cs.LG']",2021-07-23 19:02:52+00:00
http://arxiv.org/abs/2107.11357v2,Joint Shapley values: a measure of joint feature importance,"The Shapley value is one of the most widely used measures of feature
importance partly as it measures a feature's average effect on a model's
prediction. We introduce joint Shapley values, which directly extend Shapley's
axioms and intuitions: joint Shapley values measure a set of features' average
contribution to a model's prediction. We prove the uniqueness of joint Shapley
values, for any order of explanation. Results for games show that joint Shapley
values present different insights from existing interaction indices, which
assess the effect of a feature within a set of features. The joint Shapley
values provide intuitive results in ML attribution problems. With binary
features, we present a presence-adjusted global value that is more consistent
with local intuitions than the usual approach.","['Chris Harris', 'Richard Pymar', 'Colin Rowat']","['stat.ML', 'cs.AI', 'cs.LG']",2021-07-23 17:22:37+00:00
http://arxiv.org/abs/2107.11253v4,"State, global and local parameter estimation using local ensemble Kalman filters: applications to online machine learning of chaotic dynamics","In a recent methodological paper, we showed how to learn chaotic dynamics
along with the state trajectory from sequentially acquired observations, using
local ensemble Kalman filters. Here, we more systematically investigate the
possibility to use a local ensemble Kalman filter with either covariance
localisation or local domains, in order to retrieve the state and a mix of key
global and local parameters. Global parameters are meant to represent the
surrogate dynamical core, for instance through a neural network, which is
reminiscent of data-driven machine learning of dynamics, while the local
parameters typically stand for the forcings of the model. Aiming at joint state
and parameter estimation, a family of algorithms for covariance and local
domain localisation is proposed. In particular, we show how to rigorously
update global parameters using a local domain ensemble Kalman filter (EnKF)
such as the local ensemble transform Kalman filter (LETKF), an inherently local
method. The approach is tested with success on the 40-variable Lorenz model
using several of the local EnKF flavors. A two-dimensional illustration based
on a multi-layer Lorenz model is finally provided. It uses radiance-like
non-local observations. It features both local domains and covariance
localisation in order to learn the chaotic dynamics and the local forcings.
This paper more generally addresses the key question of online estimation of
both global and local model parameters.","['Quentin Malartic', 'Alban Farchi', 'Marc Bocquet']","['stat.ML', 'cs.LG', 'nlin.CD', 'physics.ao-ph', 'physics.data-an']",2021-07-23 14:12:20+00:00
http://arxiv.org/abs/2107.11231v2,Optimization on manifolds: A symplectic approach,"Optimization tasks are crucial in statistical machine learning. Recently,
there has been great interest in leveraging tools from dynamical systems to
derive accelerated and robust optimization methods via suitable discretizations
of continuous-time systems. However, these ideas have mostly been limited to
Euclidean spaces and unconstrained settings, or to Riemannian gradient flows.
In this work, we propose a dissipative extension of Dirac's theory of
constrained Hamiltonian systems as a general framework for solving optimization
problems over smooth manifolds, including problems with nonlinear constraints.
We develop geometric/symplectic numerical integrators on manifolds that are
""rate-matching,"" i.e., preserve the continuous-time rates of convergence. In
particular, we introduce a dissipative RATTLE integrator able to achieve
optimal convergence rate locally. Our class of (accelerated) algorithms are not
only simple and efficient but also applicable to a broad range of contexts.","['Guilherme França', 'Alessandro Barp', 'Mark Girolami', 'Michael I. Jordan']","['cond-mat.stat-mech', 'math.OC', 'stat.ML']",2021-07-23 13:43:34+00:00
http://arxiv.org/abs/2107.11153v1,Constellation: Learning relational abstractions over objects for compositional imagination,"Learning structured representations of visual scenes is currently a major
bottleneck to bridging perception with reasoning. While there has been exciting
progress with slot-based models, which learn to segment scenes into sets of
objects, learning configurational properties of entire groups of objects is
still under-explored. To address this problem, we introduce Constellation, a
network that learns relational abstractions of static visual scenes, and
generalises these abstractions over sensory particularities, thus offering a
potential basis for abstract relational reasoning. We further show that this
basis, along with language association, provides a means to imagine sensory
content in new ways. This work is a first step in the explicit representation
of visual relationships and using them for complex cognitive procedures.","['James C. R. Whittington', 'Rishabh Kabra', 'Loic Matthey', 'Christopher P. Burgess', 'Alexander Lerchner']","['cs.LG', 'cs.AI', 'stat.ML']",2021-07-23 11:59:40+00:00
http://arxiv.org/abs/2107.11136v2,High Dimensional Differentially Private Stochastic Optimization with Heavy-tailed Data,"As one of the most fundamental problems in machine learning, statistics and
differential privacy, Differentially Private Stochastic Convex Optimization
(DP-SCO) has been extensively studied in recent years. However, most of the
previous work can only handle either regular data distribution or irregular
data in the low dimensional space case. To better understand the challenges
arising from irregular data distribution, in this paper we provide the first
study on the problem of DP-SCO with heavy-tailed data in the high dimensional
space. In the first part we focus on the problem over some polytope constraint
(such as the $\ell_1$-norm ball). We show that if the loss function is smooth
and its gradient has bounded second order moment, it is possible to get a (high
probability) error bound (excess population risk) of $\tilde{O}(\frac{\log
d}{(n\epsilon)^\frac{1}{3}})$ in the $\epsilon$-DP model, where $n$ is the
sample size and $d$ is the dimensionality of the underlying space. Next, for
LASSO, if the data distribution that has bounded fourth-order moments, we
improve the bound to $\tilde{O}(\frac{\log d}{(n\epsilon)^\frac{2}{5}})$ in the
$(\epsilon, \delta)$-DP model. In the second part of the paper, we study sparse
learning with heavy-tailed data. We first revisit the sparse linear model and
propose a truncated DP-IHT method whose output could achieve an error of
$\tilde{O}(\frac{s^{*2}\log d}{n\epsilon})$, where $s^*$ is the sparsity of the
underlying parameter. Then we study a more general problem over the sparsity
({\em i.e.,} $\ell_0$-norm) constraint, and show that it is possible to achieve
an error of $\tilde{O}(\frac{s^{*\frac{3}{2}}\log d}{n\epsilon})$, which is
also near optimal up to a factor of $\tilde{O}{(\sqrt{s^*})}$, if the loss
function is smooth and strongly convex.","['Lijie Hu', 'Shuo Ni', 'Hanshen Xiao', 'Di Wang']","['cs.LG', 'cs.CR', 'stat.ML']",2021-07-23 11:03:21+00:00
http://arxiv.org/abs/2107.11114v2,A comparison of combined data assimilation and machine learning methods for offline and online model error correction,"Recent studies have shown that it is possible to combine machine learning
methods with data assimilation to reconstruct a dynamical system using only
sparse and noisy observations of that system. The same approach can be used to
correct the error of a knowledge-based model. The resulting surrogate model is
hybrid, with a statistical part supplementing a physical part. In practice, the
correction can be added as an integrated term (i.e. in the model resolvent) or
directly inside the tendencies of the physical model. The resolvent correction
is easy to implement. The tendency correction is more technical, in particular
it requires the adjoint of the physical model, but also more flexible. We use
the two-scale Lorenz model to compare the two methods. The accuracy in
long-range forecast experiments is somewhat similar between the surrogate
models using the resolvent correction and the tendency correction. By contrast,
the surrogate models using the tendency correction significantly outperform the
surrogate models using the resolvent correction in data assimilation
experiments. Finally, we show that the tendency correction opens the
possibility to make online model error correction, i.e. improving the model
progressively as new observations become available. The resulting algorithm can
be seen as a new formulation of weak-constraint 4D-Var. We compare online and
offline learning using the same framework with the two-scale Lorenz system, and
show that with online learning, it is possible to extract all the information
from sparse and noisy observations.","['Alban Farchi', 'Marc Bocquet', 'Patrick Laloyaux', 'Massimo Bonavita', 'Quentin Malartic']","['stat.ML', 'cs.LG']",2021-07-23 09:57:45+00:00
http://arxiv.org/abs/2107.11059v1,LocalGLMnet: interpretable deep learning for tabular data,"Deep learning models have gained great popularity in statistical modeling
because they lead to very competitive regression models, often outperforming
classical statistical models such as generalized linear models. The
disadvantage of deep learning models is that their solutions are difficult to
interpret and explain, and variable selection is not easily possible because
deep learning models solve feature engineering and variable selection
internally in a nontransparent way. Inspired by the appealing structure of
generalized linear models, we propose a new network architecture that shares
similar features as generalized linear models, but provides superior predictive
power benefiting from the art of representation learning. This new architecture
allows for variable selection of tabular data and for interpretation of the
calibrated deep learning model, in fact, our approach provides an additive
decomposition in the spirit of Shapley values and integrated gradients.","['Ronald Richman', 'Mario V. Wüthrich']","['cs.LG', 'cs.AI', 'q-fin.ST', 'stat.AP', 'stat.ML', '62, 68']",2021-07-23 07:38:33+00:00
http://arxiv.org/abs/2107.10970v3,The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian,"The null space of the $k$-th order Laplacian $\mathbf{\mathcal L}_k$, known
as the {\em $k$-th homology vector space}, encodes the non-trivial topology of
a manifold or a network. Understanding the structure of the homology embedding
can thus disclose geometric or topological information from the data. The study
of the null space embedding of the graph Laplacian $\mathbf{\mathcal L}_0$ has
spurred new research and applications, such as spectral clustering algorithms
with theoretical guarantees and estimators of the Stochastic Block Model. In
this work, we investigate the geometry of the $k$-th homology embedding and
focus on cases reminiscent of spectral clustering. Namely, we analyze the {\em
connected sum} of manifolds as a perturbation to the direct sum of their
homology embeddings. We propose an algorithm to factorize the homology
embedding into subspaces corresponding to a manifold's simplest topological
components. The proposed framework is applied to the {\em shortest homologous
loop detection} problem, a problem known to be NP-hard in general. Our spectral
loop detection algorithm scales better than existing methods and is effective
on diverse data such as point clouds and images.","['Yu-Chia Chen', 'Marina Meilă']","['stat.ML', 'cs.LG']",2021-07-23 00:40:01+00:00
http://arxiv.org/abs/2107.10960v3,Implicit Rate-Constrained Optimization of Non-decomposable Objectives,"We consider a popular family of constrained optimization problems arising in
machine learning that involve optimizing a non-decomposable evaluation metric
with a certain thresholded form, while constraining another metric of interest.
Examples of such problems include optimizing the false negative rate at a fixed
false positive rate, optimizing precision at a fixed recall, optimizing the
area under the precision-recall or ROC curves, etc. Our key idea is to
formulate a rate-constrained optimization that expresses the threshold
parameter as a function of the model parameters via the Implicit Function
theorem. We show how the resulting optimization problem can be solved using
standard gradient based methods. Experiments on benchmark datasets demonstrate
the effectiveness of our proposed method over existing state-of-the art
approaches for these problems. The code for the proposed method is available at
https://github.com/google-research/google-research/tree/master/implicit_constrained_optimization .","['Abhishek Kumar', 'Harikrishna Narasimhan', 'Andrew Cotter']","['cs.LG', 'stat.ML']",2021-07-23 00:04:39+00:00
http://arxiv.org/abs/2107.10959v1,Inference for High Dimensional Censored Quantile Regression,"With the availability of high dimensional genetic biomarkers, it is of
interest to identify heterogeneous effects of these predictors on patients'
survival, along with proper statistical inference. Censored quantile regression
has emerged as a powerful tool for detecting heterogeneous effects of
covariates on survival outcomes. To our knowledge, there is little work
available to draw inference on the effects of high dimensional predictors for
censored quantile regression. This paper proposes a novel procedure to draw
inference on all predictors within the framework of global censored quantile
regression, which investigates covariate-response associations over an interval
of quantile levels, instead of a few discrete values. The proposed estimator
combines a sequence of low dimensional model estimates that are based on
multi-sample splittings and variable selection. We show that, under some
regularity conditions, the estimator is consistent and asymptotically follows a
Gaussian process indexed by the quantile level. Simulation studies indicate
that our procedure can properly quantify the uncertainty of the estimates in
high dimensional settings. We apply our method to analyze the heterogeneous
effects of SNPs residing in lung cancer pathways on patients' survival, using
the Boston Lung Cancer Survival Cohort, a cancer epidemiology study on the
molecular mechanism of lung cancer.","['Zhe Fei', 'Qi Zheng', 'Hyokyoung G. Hong', 'Yi Li']","['stat.ME', 'stat.ML']",2021-07-22 23:57:06+00:00
http://arxiv.org/abs/2107.10955v4,Learning Linear Polytree Structural Equation Models,"We are interested in the problem of learning the directed acyclic graph (DAG)
when data are generated from a linear structural equation model (SEM) and the
causal structure can be characterized by a polytree. Under the Gaussian
polytree models, we study sufficient conditions on the sample sizes for the
well-known Chow-Liu algorithm to exactly recover both the skeleton and the
equivalence class of the polytree, which is uniquely represented by a CPDAG. On
the other hand, necessary conditions on the required sample sizes for both
skeleton and CPDAG recovery are also derived in terms of information-theoretic
lower bounds, which match the respective sufficient conditions and thereby give
a sharp characterization of the difficulty of these tasks. We also consider the
problem of inverse correlation matrix estimation under the linear polytree
models, and establish the estimation error bound in terms of the dimension and
the total number of v-structures. We also consider an extension of group linear
polytree models, in which each node represents a group of variables. Our
theoretical findings are illustrated by comprehensive numerical simulations,
and experiments on benchmark data also demonstrate the robustness of polytree
learning when the true graphical structures can only be approximated by
polytrees.","['Xingmei Lou', 'Yu Hu', 'Xiaodong Li']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2021-07-22 23:22:20+00:00
http://arxiv.org/abs/2107.10947v2,On Integral Theorems and their Statistical Properties,"We introduce a class of integral theorems based on cyclic functions and
Riemann sums approximating integrals. The Fourier integral theorem, derived as
a combination of a transform and inverse transform, arises as a special case.
The integral theorems provide natural estimators of density functions via Monte
Carlo methods. Assessments of the quality of the density estimators can be used
to obtain optimal cyclic functions, alternatives to the sin function, which
minimize square integrals. Our proof techniques rely on a variational approach
in ordinary differential equations and the Cauchy residue theorem in complex
analysis.","['Nhat Ho', 'Stephen G. Walker']","['stat.CO', 'math.CA', 'stat.ME', 'stat.ML']",2021-07-22 22:25:21+00:00
http://arxiv.org/abs/2107.10884v3,Structured second-order methods via natural gradient descent,"In this paper, we propose new structured second-order methods and structured
adaptive-gradient methods obtained by performing natural-gradient descent on
structured parameter spaces. Natural-gradient descent is an attractive approach
to design new algorithms in many settings such as gradient-free,
adaptive-gradient, and second-order methods. Our structured methods not only
enjoy a structural invariance but also admit a simple expression. Finally, we
test the efficiency of our proposed methods on both deterministic non-convex
problems and deep learning problems.","['Wu Lin', 'Frank Nielsen', 'Mohammad Emtiyaz Khan', 'Mark Schmidt']","['stat.ML', 'cs.LG']",2021-07-22 19:03:53+00:00
http://arxiv.org/abs/2107.10867v3,A local approach to parameter space reduction for regression and classification tasks,"Parameter space reduction has been proved to be a crucial tool to speed-up
the execution of many numerical tasks such as optimization, inverse problems,
sensitivity analysis, and surrogate models' design, especially when in presence
of high-dimensional parametrized systems. In this work we propose a new method
called local active subspaces (LAS), which explores the synergies of active
subspaces with supervised clustering techniques in order to carry out a more
efficient dimension reduction in the parameter space. The clustering is
performed without losing the input-output relations by introducing a distance
metric induced by the global active subspace. We present two possible
clustering algorithms: K-medoids and a hierarchical top-down approach, which is
able to impose a variety of subdivision criteria specifically tailored for
parameter space reduction tasks. This method is particularly useful for the
community working on surrogate modelling. Frequently, the parameter space
presents subdomains where the objective function of interest varies less on
average along different directions. So, it could be approximated more
accurately if restricted to those subdomains and studied separately. We tested
the new method over several numerical experiments of increasing complexity, we
show how to deal with vectorial outputs, and how to classify the different
regions with respect to the local active subspace dimension. Employing this
classification technique as a preprocessing step in the parameter space, or
output space in case of vectorial outputs, brings remarkable results for the
purpose of surrogate modelling.","['Francesco Romor', 'Marco Tezzele', 'Gianluigi Rozza']","['stat.ML', 'cs.NA', 'math.NA']",2021-07-22 18:06:04+00:00
http://arxiv.org/abs/2107.10835v1,Recovering lost and absent information in temporal networks,"The full range of activity in a temporal network is captured in its edge
activity data -- time series encoding the tie strengths or on-off dynamics of
each edge in the network. However, in many practical applications, edge-level
data are unavailable, and the network analyses must rely instead on node
activity data which aggregates the edge-activity data and thus is less
informative. This raises the question: Is it possible to use the static network
to recover the richer edge activities from the node activities? Here we show
that recovery is possible, often with a surprising degree of accuracy given how
much information is lost, and that the recovered data are useful for subsequent
network analysis tasks. Recovery is more difficult when network density
increases, either topologically or dynamically, but exploiting dynamical and
topological sparsity enables effective solutions to the recovery problem. We
formally characterize the difficulty of the recovery problem both theoretically
and empirically, proving the conditions under which recovery errors can be
bounded and showing that, even when these conditions are not met, good quality
solutions can still be derived. Effective recovery carries both promise and
peril, as it enables deeper scientific study of complex systems but in the
context of social systems also raises privacy concerns when social information
can be aggregated across multiple data sources.","['James P. Bagrow', 'Sune Lehmann']","['cs.SI', 'physics.soc-ph', 'stat.AP', 'stat.ML']",2021-07-22 17:49:27+00:00
http://arxiv.org/abs/2107.10731v2,Neural Variational Gradient Descent,"Particle-based approximate Bayesian inference approaches such as Stein
Variational Gradient Descent (SVGD) combine the flexibility and convergence
guarantees of sampling methods with the computational benefits of variational
inference. In practice, SVGD relies on the choice of an appropriate kernel
function, which impacts its ability to model the target distribution -- a
challenging problem with only heuristic solutions. We propose Neural
Variational Gradient Descent (NVGD), which is based on parameterizing the
witness function of the Stein discrepancy by a deep neural network whose
parameters are learned in parallel to the inference, mitigating the necessity
to make any kernel choices whatsoever. We empirically evaluate our method on
popular synthetic inference problems, real-world Bayesian linear regression,
and Bayesian neural network inference.","['Lauro Langosco di Langosco', 'Vincent Fortuin', 'Heiko Strathmann']","['cs.LG', 'stat.CO', 'stat.ML']",2021-07-22 15:10:50+00:00
http://arxiv.org/abs/2107.10703v2,Typing assumptions improve identification in causal discovery,"Causal discovery from observational data is a challenging task that can only
be solved up to a set of equivalent solutions, called an equivalence class.
Such classes, which are often large in size, encode uncertainties about the
orientation of some edges in the causal graph. In this work, we propose a new
set of assumptions that constrain possible causal relationships based on the
nature of variables, thus circumscribing the equivalence class. Namely, we
introduce typed directed acyclic graphs, in which variable types are used to
determine the validity of causal relationships. We demonstrate, both
theoretically and empirically, that the proposed assumptions can result in
significant gains in the identification of the causal graph. We also propose
causal discovery algorithms that make use of these assumptions and demonstrate
their benefits on simulated and pseudo-real data.","['Philippe Brouillard', 'Perouz Taslakian', 'Alexandre Lacoste', 'Sebastien Lachapelle', 'Alexandre Drouin']","['cs.LG', 'cs.AI', 'stat.ML']",2021-07-22 14:23:08+00:00
http://arxiv.org/abs/2107.10567v1,An overcome of far-distance limitation on tunnel CCTV-based accident detection in AI deep-learning frameworks,"Tunnel CCTVs are installed to low height and long-distance interval. However,
because of the limitation of installation height, severe perspective effect in
distance occurs, and it is almost impossible to detect vehicles in far distance
from the CCTV in the existing tunnel CCTV-based accident detection system
(Pflugfelder 2005). To overcome the limitation, a vehicle object is detected
through an object detection algorithm based on an inverse perspective transform
by re-setting the region of interest (ROI). It can detect vehicles that are far
away from the CCTV. To verify this process, this paper creates each dataset
consisting of images and bounding boxes based on the original and warped images
of the CCTV at the same time, and then compares performance of the deep
learning object detection models trained with the two datasets. As a result,
the model that trained the warped image was able to detect vehicle objects more
accurately at the position far from the CCTV compared to the model that trained
the original image.","['Kyu-Beom Lee', 'Hyu-Soung Shin']","['stat.ML', 'cs.LG']",2021-07-22 10:42:25+00:00
http://arxiv.org/abs/2107.10492v3,Bandit Quickest Changepoint Detection,"Many industrial and security applications employ a suite of sensors for
detecting abrupt changes in temporal behavior patterns. These abrupt changes
typically manifest locally, rendering only a small subset of sensors
informative. Continuous monitoring of every sensor can be expensive due to
resource constraints, and serves as a motivation for the bandit quickest
changepoint detection problem, where sensing actions (or sensors) are
sequentially chosen, and only measurements corresponding to chosen actions are
observed. We derive an information-theoretic lower bound on the detection delay
for a general class of finitely parameterized probability distributions. We
then propose a computationally efficient online sensing scheme, which
seamlessly balances the need for exploration of different sensing options with
exploitation of querying informative actions. We derive expected delay bounds
for the proposed scheme and show that these bounds match our
information-theoretic lower bounds at low false alarm rates, establishing
optimality of the proposed method. We then perform a number of experiments on
synthetic and real datasets demonstrating the effectiveness of our proposed
method.","['Aditya Gopalan', 'Venkatesh Saligrama', 'Braghadeesh Lakshminarayanan']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2021-07-22 07:25:35+00:00
http://arxiv.org/abs/2107.10483v3,Efficient Neural Causal Discovery without Acyclicity Constraints,"Learning the structure of a causal graphical model using both observational
and interventional data is a fundamental problem in many scientific fields. A
promising direction is continuous optimization for score-based methods, which,
however, require constrained optimization to enforce acyclicity or lack
convergence guarantees. In this paper, we present ENCO, an efficient structure
learning method for directed, acyclic causal graphs leveraging observational
and interventional data. ENCO formulates the graph search as an optimization of
independent edge likelihoods, with the edge orientation being modeled as a
separate parameter. Consequently, we can provide convergence guarantees of ENCO
under mild conditions without constraining the score function with respect to
acyclicity. In experiments, we show that ENCO can efficiently recover graphs
with hundreds of nodes, an order of magnitude larger than what was previously
possible, while handling deterministic variables and latent confounders.","['Phillip Lippe', 'Taco Cohen', 'Efstratios Gavves']","['cs.LG', 'cs.AI', 'stat.ML']",2021-07-22 07:01:41+00:00
http://arxiv.org/abs/2107.10450v3,Learning Sparse Fixed-Structure Gaussian Bayesian Networks,"Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation
models) are widely used to model causal interactions among continuous
variables. In this work, we study the problem of learning a fixed-structure
Gaussian Bayesian network up to a bounded error in total variation distance. We
analyze the commonly used node-wise least squares regression (LeastSquares) and
prove that it has a near-optimal sample complexity. We also study a couple of
new algorithms for the problem:
  - BatchAvgLeastSquares takes the average of several batches of least squares
solutions at each node, so that one can interpolate between the batch size and
the number of batches. We show that BatchAvgLeastSquares also has near-optimal
sample complexity.
  - CauchyEst takes the median of solutions to several batches of linear
systems at each node. We show that the algorithm specialized to polytrees,
CauchyEstTree, has near-optimal sample complexity.
  Experimentally, we show that for uncontaminated, realizable data, the
LeastSquares algorithm performs best, but in the presence of contamination or
DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares
respectively perform better.","['Arnab Bhattacharyya', 'Davin Choo', 'Rishikesh Gajjala', 'Sutanu Gayen', 'Yuhao Wang']","['cs.DS', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2021-07-22 04:17:46+00:00
http://arxiv.org/abs/2107.10211v2,Differentiable Annealed Importance Sampling and the Perils of Gradient Noise,"Annealed importance sampling (AIS) and related algorithms are highly
effective tools for marginal likelihood estimation, but are not fully
differentiable due to the use of Metropolis-Hastings correction steps.
Differentiability is a desirable property as it would admit the possibility of
optimizing marginal likelihood as an objective using gradient-based methods. To
this end, we propose Differentiable AIS (DAIS), a variant of AIS which ensures
differentiability by abandoning the Metropolis-Hastings corrections. As a
further advantage, DAIS allows for mini-batch gradients. We provide a detailed
convergence analysis for Bayesian linear regression which goes beyond previous
analyses by explicitly accounting for the sampler not having reached
equilibrium. Using this analysis, we prove that DAIS is consistent in the
full-batch setting and provide a sublinear convergence rate. Furthermore,
motivated by the problem of learning from large-scale datasets, we study a
stochastic variant of DAIS that uses mini-batch gradients. Surprisingly,
stochastic DAIS can be arbitrarily bad due to a fundamental incompatibility
between the goals of last-iterate convergence to the posterior and elimination
of the accumulated stochastic error. This is in stark contrast with other
settings such as gradient-based optimization and Langevin dynamics, where the
effect of gradient noise can be washed out by taking smaller steps. This
indicates that annealing-based marginal likelihood estimation with stochastic
gradients may require new ideas.","['Guodong Zhang', 'Kyle Hsu', 'Jianing Li', 'Chelsea Finn', 'Roger Grosse']","['stat.ML', 'cs.LG']",2021-07-21 17:10:14+00:00
http://arxiv.org/abs/2107.10209v2,Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations,"We present polynomial time and sample efficient algorithms for learning an
unknown depth-2 feedforward neural network with general ReLU activations, under
mild non-degeneracy assumptions. In particular, we consider learning an unknown
network of the form $f(x) = {a}^{\mathsf{T}}\sigma({W}^\mathsf{T}x+b)$, where
$x$ is drawn from the Gaussian distribution, and $\sigma(t) := \max(t,0)$ is
the ReLU activation. Prior works for learning networks with ReLU activations
assume that the bias $b$ is zero. In order to deal with the presence of the
bias terms, our proposed algorithm consists of robustly decomposing multiple
higher order tensors arising from the Hermite expansion of the function $f(x)$.
Using these ideas we also establish identifiability of the network parameters
under minimal assumptions.","['Pranjal Awasthi', 'Alex Tang', 'Aravindan Vijayaraghavan']","['cs.LG', 'cs.DS', 'stat.ML']",2021-07-21 17:06:03+00:00
http://arxiv.org/abs/2107.10199v1,Distribution of Classification Margins: Are All Data Equal?,"Recent theoretical results show that gradient descent on deep neural networks
under exponential loss functions locally maximizes classification margin, which
is equivalent to minimizing the norm of the weight matrices under margin
constraints. This property of the solution however does not fully characterize
the generalization performance. We motivate theoretically and show empirically
that the area under the curve of the margin distribution on the training set is
in fact a good measure of generalization. We then show that, after data
separation is achieved, it is possible to dynamically reduce the training set
by more than 99% without significant loss of performance. Interestingly, the
resulting subset of ""high capacity"" features is not consistent across different
training runs, which is consistent with the theoretical claim that all training
points should converge to the same asymptotic margin under SGD and in the
presence of both batch normalization and weight decay.","['Andrzej Banburski', 'Fernanda De La Torre', 'Nishka Pant', 'Ishana Shastri', 'Tomaso Poggio']","['cs.LG', 'cs.AI', 'stat.ML']",2021-07-21 16:41:57+00:00
http://arxiv.org/abs/2107.10143v1,On the Memorization Properties of Contrastive Learning,"Memorization studies of deep neural networks (DNNs) help to understand what
patterns and how do DNNs learn, and motivate improvements to DNN training
approaches. In this work, we investigate the memorization properties of SimCLR,
a widely used contrastive self-supervised learning approach, and compare them
to the memorization of supervised learning and random labels training. We find
that both training objects and augmentations may have different complexity in
the sense of how SimCLR learns them. Moreover, we show that SimCLR is similar
to random labels training in terms of the distribution of training objects
complexity.","['Ildus Sadrtdinov', 'Nadezhda Chirkova', 'Ekaterina Lobacheva']","['cs.LG', 'stat.ML']",2021-07-21 15:21:58+00:00
http://arxiv.org/abs/2107.10127v1,Extracting Governing Laws from Sample Path Data of Non-Gaussian Stochastic Dynamical Systems,"Advances in data science are leading to new progresses in the analysis and
understanding of complex dynamics for systems with experimental and
observational data. With numerous physical phenomena exhibiting bursting,
flights, hopping, and intermittent features, stochastic differential equations
with non-Gaussian L\'evy noise are suitable to model these systems. Thus it is
desirable and essential to infer such equations from available data to
reasonably predict dynamical behaviors. In this work, we consider a data-driven
method to extract stochastic dynamical systems with non-Gaussian asymmetric
(rather than the symmetric) L\'evy process, as well as Gaussian Brownian
motion. We establish a theoretical framework and design a numerical algorithm
to compute the asymmetric L\'evy jump measure, drift and diffusion (i.e.,
nonlocal Kramers-Moyal formulas), hence obtaining the stochastic governing law,
from noisy data. Numerical experiments on several prototypical examples confirm
the efficacy and accuracy of this method. This method will become an effective
tool in discovering the governing laws from available data sets and in
understanding the mechanisms underlying complex random phenomena.","['Yang Li', 'Jinqiao Duan']","['math.ST', 'stat.ML', 'stat.TH']",2021-07-21 14:50:36+00:00
http://arxiv.org/abs/2107.10125v2,A variational approximate posterior for the deep Wishart process,"Recent work introduced deep kernel processes as an entirely kernel-based
alternative to NNs (Aitchison et al. 2020). Deep kernel processes flexibly
learn good top-layer representations by alternately sampling the kernel from a
distribution over positive semi-definite matrices and performing nonlinear
transformations. A particular deep kernel process, the deep Wishart process
(DWP), is of particular interest because its prior can be made equivalent to
deep Gaussian process (DGP) priors for kernels that can be expressed entirely
in terms of Gram matrices. However, inference in DWPs has not yet been possible
due to the lack of sufficiently flexible distributions over positive
semi-definite matrices. Here, we give a novel approach to obtaining flexible
distributions over positive semi-definite matrices by generalising the Bartlett
decomposition of the Wishart probability density. We use this new distribution
to develop an approximate posterior for the DWP that includes dependency across
layers. We develop a doubly-stochastic inducing-point inference scheme for the
DWP and show experimentally that inference in the DWP can improve performance
over doing inference in a DGP with the equivalent prior.","['Sebastian W. Ober', 'Laurence Aitchison']","['stat.ML', 'cs.LG']",2021-07-21 14:48:27+00:00
http://arxiv.org/abs/2107.10663v1,Fed-ensemble: Improving Generalization through Model Ensembling in Federated Learning,"In this paper we propose Fed-ensemble: a simple approach that bringsmodel
ensembling to federated learning (FL). Instead of aggregating localmodels to
update a single global model, Fed-ensemble uses random permutations to update a
group of K models and then obtains predictions through model averaging.
Fed-ensemble can be readily utilized within established FL methods and does not
impose a computational overhead as it only requires one of the K models to be
sent to a client in each communication round. Theoretically, we show that
predictions on newdata from all K models belong to the same predictive
posterior distribution under a neural tangent kernel regime. This result in
turn sheds light onthe generalization advantages of model averaging. We also
illustrate thatFed-ensemble has an elegant Bayesian interpretation. Empirical
results show that our model has superior performance over several FL
algorithms,on a wide range of data sets, and excels in heterogeneous settings
often encountered in FL applications.","['Naichen Shi', 'Fan Lai', 'Raed Al Kontar', 'Mosharaf Chowdhury']","['stat.ML', 'cs.LG']",2021-07-21 14:40:14+00:00
http://arxiv.org/abs/2107.10110v2,On the Convergence of Prior-Guided Zeroth-Order Optimization Algorithms,"Zeroth-order (ZO) optimization is widely used to handle challenging tasks,
such as query-based black-box adversarial attacks and reinforcement learning.
Various attempts have been made to integrate prior information into the
gradient estimation procedure based on finite differences, with promising
empirical results. However, their convergence properties are not well
understood. This paper makes an attempt to fill up this gap by analyzing the
convergence of prior-guided ZO algorithms under a greedy descent framework with
various gradient estimators. We provide a convergence guarantee for the
prior-guided random gradient-free (PRGF) algorithms. Moreover, to further
accelerate over greedy descent methods, we present a new accelerated random
search (ARS) algorithm that incorporates prior information, together with a
convergence analysis. Finally, our theoretical results are confirmed by
experiments on several numerical benchmarks as well as adversarial attacks.","['Shuyu Cheng', 'Guoqiang Wu', 'Jun Zhu']","['stat.ML', 'cs.LG', 'math.OC']",2021-07-21 14:39:40+00:00
http://arxiv.org/abs/2107.10098v3,Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear ICA,"This work introduces a novel principle we call disentanglement via mechanism
sparsity regularization, which can be applied when the latent factors of
interest depend sparsely on past latent factors and/or observed auxiliary
variables. We propose a representation learning method that induces
disentanglement by simultaneously learning the latent factors and the sparse
causal graphical model that relates them. We develop a rigorous identifiability
theory, building on recent nonlinear independent component analysis (ICA)
results, that formalizes this principle and shows how the latent variables can
be recovered up to permutation if one regularizes the latent mechanisms to be
sparse and if some graph connectivity criterion is satisfied by the data
generating process. As a special case of our framework, we show how one can
leverage unknown-target interventions on the latent factors to disentangle
them, thereby drawing further connections between ICA and causality. We propose
a VAE-based method in which the latent mechanisms are learned and regularized
via binary masks, and validate our theory by showing it learns disentangled
representations in simulations.","['Sébastien Lachapelle', 'Pau Rodríguez López', 'Yash Sharma', 'Katie Everett', 'Rémi Le Priol', 'Alexandre Lacoste', 'Simon Lacoste-Julien']","['stat.ML', 'cs.LG', 'I.2.6; I.5.1']",2021-07-21 14:22:14+00:00
http://arxiv.org/abs/2107.10072v1,Interpreting diffusion score matching using normalizing flow,"Scoring matching (SM), and its related counterpart, Stein discrepancy (SD)
have achieved great success in model training and evaluations. However, recent
research shows their limitations when dealing with certain types of
distributions. One possible fix is incorporating the original score matching
(or Stein discrepancy) with a diffusion matrix, which is called diffusion score
matching (DSM) (or diffusion Stein discrepancy (DSD)). However, the lack of
interpretation of the diffusion limits its usage within simple distributions
and manually chosen matrix. In this work, we plan to fill this gap by
interpreting the diffusion matrix using normalizing flows. Specifically, we
theoretically prove that DSM (or DSD) is equivalent to the original score
matching (or Stein discrepancy) evaluated in the transformed space defined by
the normalizing flow, where the diffusion matrix is the inverse of the flow's
Jacobian matrix. In addition, we also build its connection to Riemannian
manifolds and further extend it to continuous flows, where the change of DSM is
characterized by an ODE.","['Wenbo Gong', 'Yingzhen Li']","['cs.LG', 'stat.ML']",2021-07-21 13:27:32+00:00
http://arxiv.org/abs/2107.10066v1,Adaptive Inducing Points Selection For Gaussian Processes,"Gaussian Processes (\textbf{GPs}) are flexible non-parametric models with
strong probabilistic interpretation. While being a standard choice for
performing inference on time series, GPs have few techniques to work in a
streaming setting. \cite{bui2017streaming} developed an efficient variational
approach to train online GPs by using sparsity techniques: The whole set of
observations is approximated by a smaller set of inducing points (\textbf{IPs})
and moved around with new data. Both the number and the locations of the IPs
will affect greatly the performance of the algorithm. In addition to optimizing
their locations, we propose to adaptively add new points, based on the
properties of the GP and the structure of the data.","['Théo Galy-Fajou', 'Manfred Opper']","['stat.ML', 'cs.LG']",2021-07-21 13:22:46+00:00
http://arxiv.org/abs/2107.10043v3,KalmanNet: Neural Network Aided Kalman Filtering for Partially Known Dynamics,"State estimation of dynamical systems in real-time is a fundamental task in
signal processing. For systems that are well-represented by a fully known
linear Gaussian state space (SS) model, the celebrated Kalman filter (KF) is a
low complexity optimal solution. However, both linearity of the underlying SS
model and accurate knowledge of it are often not encountered in practice. Here,
we present KalmanNet, a real-time state estimator that learns from data to
carry out Kalman filtering under non-linear dynamics with partial information.
By incorporating the structural SS model with a dedicated recurrent neural
network module in the flow of the KF, we retain data efficiency and
interpretability of the classic algorithm while implicitly learning complex
dynamics from data. We demonstrate numerically that KalmanNet overcomes
non-linearities and model mismatch, outperforming classic filtering methods
operating with both mismatched and accurate domain knowledge.","['Guy Revach', 'Nir Shlezinger', 'Xiaoyong Ni', 'Adria Lopez Escoriza', 'Ruud J. G. van Sloun', 'Yonina C. Eldar']","['eess.SP', 'cs.LG', 'stat.ML']",2021-07-21 12:26:46+00:00
http://arxiv.org/abs/2107.10030v1,"Differentiable Feature Selection, a Reparameterization Approach","We consider the task of feature selection for reconstruction which consists
in choosing a small subset of features from which whole data instances can be
reconstructed. This is of particular importance in several contexts involving
for example costly physical measurements, sensor placement or information
compression. To break the intrinsic combinatorial nature of this problem, we
formulate the task as optimizing a binary mask distribution enabling an
accurate reconstruction. We then face two main challenges. One concerns
differentiability issues due to the binary distribution. The second one
corresponds to the elimination of redundant information by selecting variables
in a correlated fashion which requires modeling the covariance of the binary
distribution. We address both issues by introducing a relaxation of the problem
via a novel reparameterization of the logitNormal distribution. We demonstrate
that the proposed method provides an effective exploration scheme and leads to
efficient feature selection for reconstruction through evaluation on several
high dimensional image benchmarks. We show that the method leverages the
intrinsic geometry of the data, facilitating reconstruction.","['Jérémie Dona', 'Patrick Gallinari']","['cs.LG', 'cs.NE', 'stat.ML']",2021-07-21 11:52:34+00:00
http://arxiv.org/abs/2107.10014v1,Delving Into Deep Walkers: A Convergence Analysis of Random-Walk-Based Vertex Embeddings,"Graph vertex embeddings based on random walks have become increasingly
influential in recent years, showing good performance in several tasks as they
efficiently transform a graph into a more computationally digestible format
while preserving relevant information. However, the theoretical properties of
such algorithms, in particular the influence of hyperparameters and of the
graph structure on their convergence behaviour, have so far not been
well-understood. In this work, we provide a theoretical analysis for
random-walks based embeddings techniques. Firstly, we prove that, under some
weak assumptions, vertex embeddings derived from random walks do indeed
converge both in the single limit of the number of random walks $N \to \infty$
and in the double limit of both $N$ and the length of each random walk
$L\to\infty$. Secondly, we derive concentration bounds quantifying the converge
rate of the corpora for the single and double limits. Thirdly, we use these
results to derive a heuristic for choosing the hyperparameters $N$ and $L$. We
validate and illustrate the practical importance of our findings with a range
of numerical and visual experiments on several graphs drawn from real-world
applications.","['Dominik Kloepfer', 'Angelica I. Aviles-Rivero', 'Daniel Heydecker']","['stat.ML', 'cs.LG', 'math.PR']",2021-07-21 11:23:04+00:00
http://arxiv.org/abs/2107.10013v1,Optimal Operation of Power Systems with Energy Storage under Uncertainty: A Scenario-based Method with Strategic Sampling,"The multi-period dynamics of energy storage (ES), intermittent renewable
generation and uncontrollable power loads, make the optimization of power
system operation (PSO) challenging. A multi-period optimal PSO under
uncertainty is formulated using the chance-constrained optimization (CCO)
modeling paradigm, where the constraints include the nonlinear energy storage
and AC power flow models. Based on the emerging scenario optimization method
which does not rely on pre-known probability distribution functions, this paper
develops a novel solution method for this challenging CCO problem. The proposed
meth-od is computationally effective for mainly two reasons. First, the
original AC power flow constraints are approximated by a set of
learning-assisted quadratic convex inequalities based on a generalized least
absolute shrinkage and selection operator. Second, considering the physical
patterns of data and motived by learning-based sampling, the strategic sampling
method is developed to significantly reduce the required number of scenarios
through different sampling strategies. The simulation results on IEEE standard
systems indicate that 1) the proposed strategic sampling significantly improves
the computational efficiency of the scenario-based approach for solving the
chance-constrained optimal PSO problem, 2) the data-driven convex approximation
of power flow can be promising alternatives of nonlinear and nonconvex AC power
flow.","['Ren Hu', 'Qifeng Li']","['eess.SY', 'cs.AI', 'cs.SY', 'stat.ML', '68T09']",2021-07-21 11:21:50+00:00
http://arxiv.org/abs/2107.09957v2,Memorization in Deep Neural Networks: Does the Loss Function matter?,"Deep Neural Networks, often owing to the overparameterization, are shown to
be capable of exactly memorizing even randomly labelled data. Empirical studies
have also shown that none of the standard regularization techniques mitigate
such overfitting. We investigate whether the choice of the loss function can
affect this memorization. We empirically show, with benchmark data sets MNIST
and CIFAR-10, that a symmetric loss function, as opposed to either
cross-entropy or squared error loss, results in significant improvement in the
ability of the network to resist such overfitting. We then provide a formal
definition for robustness to memorization and provide a theoretical explanation
as to why the symmetric losses provide this robustness. Our results clearly
bring out the role loss functions alone can play in this phenomenon of
memorization.","['Deep Patel', 'P. S. Sastry']","['cs.LG', 'cs.CV', 'stat.ML']",2021-07-21 09:08:51+00:00
http://arxiv.org/abs/2107.09950v2,Boundary of Distribution Support Generator (BDSG): Sample Generation on the Boundary,"Generative models, such as Generative Adversarial Networks (GANs), have been
used for unsupervised anomaly detection. While performance keeps improving,
several limitations exist particularly attributed to difficulties at capturing
multimodal supports and to the ability to approximate the underlying
distribution closer to the tails, i.e. the boundary of the distribution's
support. This paper proposes an approach that attempts to alleviate such
shortcomings. We propose an invertible-residual-network-based model, the
Boundary of Distribution Support Generator (BDSG). GANs generally do not
guarantee the existence of a probability distribution and here, we use the
recently developed Invertible Residual Network (IResNet) and Residual Flow
(ResFlow), for density estimation. These models have not yet been used for
anomaly detection. We leverage IResNet and ResFlow for Out-of-Distribution
(OoD) sample detection and for sample generation on the boundary using a
compound loss function that forces the samples to lie on the boundary. The BDSG
addresses non-convex support, disjoint components, and multimodal
distributions. Results on synthetic data and data from multimodal
distributions, such as MNIST and CIFAR-10, demonstrate competitive performance
compared to methods from the literature.","['Nikolaos Dionelis', 'Mehrdad Yaghoobi', 'Sotirios A. Tsaftaris']","['cs.LG', 'stat.ML']",2021-07-21 09:00:32+00:00
http://arxiv.org/abs/2107.09949v1,Online structural kernel selection for mobile health,"Motivated by the need for efficient and personalized learning in mobile
health, we investigate the problem of online kernel selection for Gaussian
Process regression in the multi-task setting. We propose a novel generative
process on the kernel composition for this purpose. Our method demonstrates
that trajectories of kernel evolutions can be transferred between users to
improve learning and that the kernels themselves are meaningful for an mHealth
prediction goal.","['Eura Shin', 'Pedja Klasnja', 'Susan Murphy', 'Finale Doshi-Velez']","['cs.LG', 'stat.ML']",2021-07-21 08:58:53+00:00
http://arxiv.org/abs/2107.09912v2,Design of Experiments for Stochastic Contextual Linear Bandits,"In the stochastic linear contextual bandit setting there exist several
minimax procedures for exploration with policies that are reactive to the data
being acquired. In practice, there can be a significant engineering overhead to
deploy these algorithms, especially when the dataset is collected in a
distributed fashion or when a human in the loop is needed to implement a
different policy. Exploring with a single non-reactive policy is beneficial in
such cases. Assuming some batch contexts are available, we design a single
stochastic policy to collect a good dataset from which a near-optimal policy
can be extracted. We present a theoretical analysis as well as numerical
experiments on both synthetic and real-world datasets.","['Andrea Zanette', 'Kefan Dong', 'Jonathan Lee', 'Emma Brunskill']","['cs.LG', 'stat.ML']",2021-07-21 07:25:37+00:00
http://arxiv.org/abs/2107.09853v1,EMG Pattern Recognition via Bayesian Inference with Scale Mixture-Based Stochastic Generative Models,"Electromyogram (EMG) has been utilized to interface signals for prosthetic
hands and information devices owing to its ability to reflect human motion
intentions. Although various EMG classification methods have been introduced
into EMG-based control systems, they do not fully consider the stochastic
characteristics of EMG signals. This paper proposes an EMG pattern
classification method incorporating a scale mixture-based generative model. A
scale mixture model is a stochastic EMG model in which the EMG variance is
considered as a random variable, enabling the representation of uncertainty in
the variance. This model is extended in this study and utilized for EMG pattern
classification. The proposed method is trained by variational Bayesian
learning, thereby allowing the automatic determination of the model complexity.
Furthermore, to optimize the hyperparameters of the proposed method with a
partial discriminative approach, a mutual information-based determination
method is introduced. Simulation and EMG analysis experiments demonstrated the
relationship between the hyperparameters and classification accuracy of the
proposed method as well as the validity of the proposed method. The comparison
using public EMG datasets revealed that the proposed method outperformed the
various conventional classifiers. These results indicated the validity of the
proposed method and its applicability to EMG-based control systems. In EMG
pattern recognition, a classifier based on a generative model that reflects the
stochastic characteristics of EMG signals can outperform the conventional
general-purpose classifier.","['Akira Furui', 'Takuya Igaue', 'Toshio Tsuji']","['eess.SP', 'cs.LG', 'stat.ML']",2021-07-21 02:51:19+00:00
http://arxiv.org/abs/2107.09802v1,Private Alternating Least Squares: Practical Private Matrix Completion with Tighter Rates,"We study the problem of differentially private (DP) matrix completion under
user-level privacy. We design a joint differentially private variant of the
popular Alternating-Least-Squares (ALS) method that achieves: i) (nearly)
optimal sample complexity for matrix completion (in terms of number of items,
users), and ii) the best known privacy/utility trade-off both theoretically, as
well as on benchmark data sets. In particular, we provide the first global
convergence analysis of ALS with noise introduced to ensure DP, and show that,
in comparison to the best known alternative (the Private Frank-Wolfe algorithm
by Jain et al. (2018)), our error bounds scale significantly better with
respect to the number of items and users, which is critical in practical
problems. Extensive validation on standard benchmarks demonstrate that the
algorithm, in combination with carefully designed sampling procedures, is
significantly more accurate than existing techniques, thus promising to be the
first practical DP embedding model.","['Steve Chien', 'Prateek Jain', 'Walid Krichene', 'Steffen Rendle', 'Shuang Song', 'Abhradeep Thakurta', 'Li Zhang']","['cs.LG', 'cs.CR', 'stat.ML']",2021-07-20 23:19:11+00:00
http://arxiv.org/abs/2107.09773v1,Statistical Estimation from Dependent Data,"We consider a general statistical estimation problem wherein binary labels
across different observations are not independent conditioned on their feature
vectors, but dependent, capturing settings where e.g. these observations are
collected on a spatial domain, a temporal domain, or a social network, which
induce dependencies. We model these dependencies in the language of Markov
Random Fields and, importantly, allow these dependencies to be substantial, i.e
do not assume that the Markov Random Field capturing these dependencies is in
high temperature. As our main contribution we provide algorithms and
statistically efficient estimation rates for this model, giving several
instantiations of our bounds in logistic regression, sparse logistic
regression, and neural network settings with dependent data. Our estimation
guarantees follow from novel results for estimating the parameters (i.e.
external fields and interaction strengths) of Ising models from a {\em single}
sample. {We evaluate our estimation approach on real networked data, showing
that it outperforms standard regression approaches that ignore dependencies,
across three text classification datasets: Cora, Citeseer and Pubmed.}","['Yuval Dagan', 'Constantinos Daskalakis', 'Nishanth Dikkala', 'Surbhi Goel', 'Anthimos Vardis Kandiros']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2021-07-20 21:18:06+00:00
http://arxiv.org/abs/2107.09744v2,Turbulent field fluctuations in gyrokinetic and fluid plasmas,"A key uncertainty in the design and development of magnetic confinement
fusion energy reactors is predicting edge plasma turbulence. An essential step
in overcoming this uncertainty is the validation in accuracy of reduced
turbulent transport models. Drift-reduced Braginskii two-fluid theory is one
such set of reduced equations that has for decades simulated boundary plasmas
in experiment, but significant questions exist regarding its predictive
ability. To this end, using a novel physics-informed deep learning framework,
we demonstrate the first ever direct quantitative comparisons of turbulent
field fluctuations between electrostatic two-fluid theory and electromagnetic
gyrokinetic modelling with good overall agreement found in magnetized helical
plasmas at low normalized pressure. This framework is readily adaptable to
experimental and astrophysical environments, and presents a new technique for
the numerical validation and discovery of reduced global plasma turbulence
models.","['Abhilash Mathews', 'Noah Mandell', 'Manaure Francisquez', 'Jerry Hughes', 'Ammar Hakim']","['physics.plasm-ph', 'cs.LG', 'physics.comp-ph', 'stat.ML']",2021-07-20 19:50:30+00:00
http://arxiv.org/abs/2107.10869v3,Filament Plots for Data Visualization,"The efficiency of modern computer graphics allows us to explore collections
of space curves simultaneously with ""drag-to-rotate"" interfaces. This inspires
us to replace ""scatterplots of points"" with ""scatterplots of curves"" to
simultaneously visualize relationships across an entire dataset. Since spaces
of curves are infinite dimensional, scatterplots of curves avoid the ""lossy""
nature of scatterplots of points. In particular, if two points are close in a
scatterplot of points derived from high-dimensional data, it does not generally
follow that the two associated data points are close in the data space.
Standard Andrews plots provide scatterplots of curves that perfectly preserve
Euclidean distances, but simultaneous visualization of these graphs over an
entire dataset produces visual clutter because graphs of functions generally
overlap in 2D. We mitigate this visual clutter issue by constructing
computationally inexpensive 3D extensions of Andrews plots. First, we construct
optimally smooth 3D Andrews plots by considering linear isometries from
Euclidean data spaces to spaces of planar parametric curves. We rigorously
parametrize the linear isometries that produce (on average) optimally smooth
curves over a given dataset. This parameterization of optimal isometries
reveals many degrees of freedom, and (using recent results on generalized Gauss
sums) we identify a particular member of this set which admits an asymptotic
""tour"" property that avoids certain local degeneracies as well. Finally, we
construct unit-length 3D curves (filaments) by numerically solving
Frenet-Serret systems given data from these 3D Andrews plots. We conclude with
examples of filament plots for several standard datasets, illustrating how
filament plots avoid visual clutter. Code and examples available at
https://github.com/n8epi/filaments/ and https://n8epi.github.io/filaments/",['Nate Strawn'],"['cs.HC', 'cs.LG', 'stat.ML', '42A99, 46N10, 53Z50']",2021-07-20 18:20:33+00:00
http://arxiv.org/abs/2107.09660v1,On Estimating Rank-One Spiked Tensors in the Presence of Heavy Tailed Errors,"In this paper, we study the estimation of a rank-one spiked tensor in the
presence of heavy tailed noise. Our results highlight some of the fundamental
similarities and differences in the tradeoff between statistical and
computational efficiencies under heavy tailed and Gaussian noise. In
particular, we show that, for $p$ th order tensors, the tradeoff manifests in
an identical fashion as the Gaussian case when the noise has finite $4(p-1)$ th
moment. The difference in signal strength requirements, with or without
computational constraints, for us to estimate the singular vectors at the
optimal rate, interestingly, narrows for noise with heavier tails and vanishes
when the noise only has finite fourth moment. Moreover, if the noise has less
than fourth moment, tensor SVD, perhaps the most natural approach, is
suboptimal even though it is computationally intractable. Our analysis exploits
a close connection between estimating the rank-one spikes and the spectral norm
of a random tensor with iid entries. In particular, we show that the order of
the spectral norm of a random tensor can be precisely characterized by the
moment of its entries, generalizing classical results for random matrices. In
addition to the theoretical guarantees, we propose estimation procedures for
the heavy tailed regime, which are easy to implement and efficient to run.
Numerical experiments are presented to demonstrate their practical merits.","['Arnab Auddy', 'Ming Yuan']","['math.ST', 'cs.IT', 'cs.NA', 'math.IT', 'math.NA', 'stat.ML', 'stat.TH', '15A18, 15A69, 62H25, 62F35, 62-08']",2021-07-20 17:45:55+00:00
http://arxiv.org/abs/2107.09597v4,Positively Weighted Kernel Quadrature via Subsampling,"We study kernel quadrature rules with convex weights. Our approach combines
the spectral properties of the kernel with recombination results about point
measures. This results in effective algorithms that construct convex quadrature
rules using only access to i.i.d. samples from the underlying measure and
evaluation of the kernel and that result in a small worst-case error. In
addition to our theoretical results and the benefits resulting from convex
weights, our experiments indicate that this construction can compete with the
optimal bounds in well-known examples.","['Satoshi Hayakawa', 'Harald Oberhauser', 'Terry Lyons']","['math.NA', 'cs.LG', 'cs.NA', 'stat.ML']",2021-07-20 16:18:56+00:00
http://arxiv.org/abs/2107.09542v1,Open Problem: Is There an Online Learning Algorithm That Learns Whenever Online Learning Is Possible?,"This open problem asks whether there exists an online learning algorithm for
binary classification that guarantees, for all target concepts, to make a
sublinear number of mistakes, under only the assumption that the (possibly
random) sequence of points X allows that such a learning algorithm can exist
for that sequence. As a secondary problem, it also asks whether a specific
concise condition completely determines whether a given (possibly random)
sequence of points X admits the existence of online learning algorithms
guaranteeing a sublinear number of mistakes for all target concepts.",['Steve Hanneke'],"['cs.LG', 'cs.AI', 'math.PR', 'math.ST', 'stat.ML', 'stat.TH']",2021-07-20 14:57:37+00:00
http://arxiv.org/abs/2107.09532v1,Estimation of a regression function on a manifold by fully connected deep neural networks,"Estimation of a regression function from independent and identically
distributed data is considered. The $L_2$ error with integration with respect
to the distribution of the predictor variable is used as the error criterion.
The rate of convergence of least squares estimates based on fully connected
spaces of deep neural networks with ReLU activation function is analyzed for
smooth regression functions. It is shown that in case that the distribution of
the predictor variable is concentrated on a manifold, these estimates achieve a
rate of convergence which depends on the dimension of the manifold and not on
the number of components of the predictor variable.","['Michael Kohler', 'Sophie Langer', 'Ulrich Reif']","['math.ST', 'stat.ML', 'stat.TH', '62G05 (Primary), 62G20 (Secondary)']",2021-07-20 14:43:59+00:00
http://arxiv.org/abs/2107.09519v1,Canonical Polyadic Decomposition and Deep Learning for Machine Fault Detection,"Acoustic monitoring for machine fault detection is a recent and expanding
research path that has already provided promising results for industries.
However, it is impossible to collect enough data to learn all types of faults
from a machine. Thus, new algorithms, trained using data from healthy
conditions only, were developed to perform unsupervised anomaly detection. A
key issue in the development of these algorithms is the noise in the signals,
as it impacts the anomaly detection performance. In this work, we propose a
powerful data-driven and quasi non-parametric denoising strategy for spectral
data based on a tensor decomposition: the Non-negative Canonical Polyadic (CP)
decomposition. This method is particularly adapted for machine emitting
stationary sound. We demonstrate in a case study, the Malfunctioning Industrial
Machine Investigation and Inspection (MIMII) baseline, how the use of our
denoising strategy leads to a sensible improvement of the unsupervised anomaly
detection. Such approaches are capable to make sound-based monitoring of
industrial processes more reliable.","['Gaetan Frusque', 'Gabriel Michau', 'Olga Fink']","['stat.ML', 'cs.LG', 'eess.AS']",2021-07-20 14:06:50+00:00
http://arxiv.org/abs/2107.09422v1,Large-scale graph representation learning with very deep GNNs and self-supervision,"Effectively and efficiently deploying graph neural networks (GNNs) at scale
remains one of the most challenging aspects of graph representation learning.
Many powerful solutions have only ever been validated on comparatively small
datasets, often with counter-intuitive outcomes -- a barrier which has been
broken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered
the OGB-LSC with two large-scale GNNs: a deep transductive node classifier
powered by bootstrapping, and a very deep (up to 50-layer) inductive graph
regressor regularised by denoising objectives. Our models achieved an
award-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In
doing so, we demonstrate evidence of scalable self-supervised graph
representation learning, and utility of very deep GNNs -- both very important
open issues. Our code is publicly available at:
https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc.","['Ravichandra Addanki', 'Peter W. Battaglia', 'David Budden', 'Andreea Deac', 'Jonathan Godwin', 'Thomas Keck', 'Wai Lok Sibon Li', 'Alvaro Sanchez-Gonzalez', 'Jacklynn Stott', 'Shantanu Thakoor', 'Petar Veličković']","['cs.LG', 'cs.AI', 'cs.SI', 'stat.ML']",2021-07-20 11:35:25+00:00
http://arxiv.org/abs/2107.09384v1,An induction proof of the backpropagation algorithm in matrix notation,"Backpropagation (BP) is a core component of the contemporary deep learning
incarnation of neural networks. Briefly, BP is an algorithm that exploits the
computational architecture of neural networks to efficiently evaluate the
gradient of a cost function during neural network parameter optimization. The
validity of BP rests on the application of a multivariate chain rule to the
computational architecture of neural networks and their associated objective
functions. Introductions to deep learning theory commonly present the
computational architecture of neural networks in matrix form, but eschew a
parallel formulation and justification of BP in the framework of matrix
differential calculus. This entails several drawbacks for the theory and
didactics of deep learning. In this work, we overcome these limitations by
providing a full induction proof of the BP algorithm in matrix notation.
Specifically, we situate the BP algorithm in the framework of matrix
differential calculus, encompass affine-linear potential functions, prove the
validity of the BP algorithm in inductive form, and exemplify the
implementation of the matrix form BP algorithm in computer code.","['Dirk Ostwald', 'Franziska Usée']","['stat.ML', 'cs.LG', 'math.ST', 'q-bio.NC', 'stat.TH']",2021-07-20 10:02:17+00:00
http://arxiv.org/abs/2107.09355v1,Approximation Theory of Convolutional Architectures for Time Series Modelling,"We study the approximation properties of convolutional architectures applied
to time series modelling, which can be formulated mathematically as a
functional approximation problem. In the recurrent setting, recent results
reveal an intricate connection between approximation efficiency and memory
structures in the data generation process. In this paper, we derive parallel
results for convolutional architectures, with WaveNet being a prime example.
Our results reveal that in this new setting, approximation efficiency is not
only characterised by memory, but also additional fine structures in the target
relationship. This leads to a novel definition of spectrum-based regularity
that measures the complexity of temporal relationships under the convolutional
approximation scheme. These analyses provide a foundation to understand the
differences between architectural choices for time series modelling and can
give theoretically grounded guidance for practical applications.","['Haotian Jiang', 'Zhong Li', 'Qianxiao Li']","['cs.LG', 'stat.ML', '68W25, 68T07, 37M10', 'I.2.6']",2021-07-20 09:19:26+00:00
http://arxiv.org/abs/2107.09338v2,Stein Variational Gradient Descent with Multiple Kernel,"Stein variational gradient descent (SVGD) and its variants have shown
promising successes in approximate inference for complex distributions. In
practice, we notice that the kernel used in SVGD-based methods has a decisive
effect on the empirical performance. Radial basis function (RBF) kernel with
median heuristics is a common choice in previous approaches, but unfortunately
this has proven to be sub-optimal. Inspired by the paradigm of Multiple Kernel
Learning (MKL), our solution to this flaw is using a combination of multiple
kernels to approximate the optimal kernel, rather than a single one which may
limit the performance and flexibility. Specifically, we first extend Kernelized
Stein Discrepancy (KSD) to its multiple kernels view called Multiple Kernelized
Stein Discrepancy (MKSD) and then leverage MKSD to construct a general
algorithm Multiple Kernel SVGD (MK-SVGD). Further, MKSVGD can automatically
assign a weight to each kernel without any other parameters, which means that
our method not only gets rid of optimal kernel dependence but also maintains
computational efficiency. Experiments on various tasks and models demonstrate
that our proposed method consistently matches or outperforms the competing
methods.","['Qingzhong Ai', 'Shiyu Liu', 'Lirong He', 'Zenglin Xu']","['cs.LG', 'stat.ML']",2021-07-20 08:48:42+00:00
http://arxiv.org/abs/2107.09301v2,A Bayesian Approach to Invariant Deep Neural Networks,"We propose a novel Bayesian neural network architecture that can learn
invariances from data alone by inferring a posterior distribution over
different weight-sharing schemes. We show that our model outperforms other
non-invariant architectures, when trained on datasets that contain specific
invariances. The same holds true when no data augmentation is performed.","['Nikolaos Mourdoukoutas', 'Marco Federici', 'Georges Pantalos', 'Mark van der Wilk', 'Vincent Fortuin']","['stat.ML', 'cs.LG']",2021-07-20 07:33:58+00:00
http://arxiv.org/abs/2107.09224v3,From Predictions to Decisions: The Importance of Joint Predictive Distributions,"A fundamental challenge for any intelligent system is prediction: given some
inputs, can you predict corresponding outcomes? Most work on supervised
learning has focused on producing accurate marginal predictions for each input.
However, we show that for a broad class of decision problems, accurate joint
predictions are required to deliver good performance. In particular, we
establish several results pertaining to combinatorial decision problems,
sequential predictions, and multi-armed bandits to elucidate the essential role
of joint predictive distributions. Our treatment of multi-armed bandits
introduces an approximate Thompson sampling algorithm and analytic techniques
that lead to a new kind of regret bound.","['Zheng Wen', 'Ian Osband', 'Chao Qin', 'Xiuyuan Lu', 'Morteza Ibrahimi', 'Vikranth Dwaracherla', 'Mohammad Asghari', 'Benjamin Van Roy']","['cs.LG', 'stat.ML']",2021-07-20 01:55:01+00:00
http://arxiv.org/abs/2107.09207v2,Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix Manifold,"We show that on the manifold of fixed-rank and symmetric positive
semi-definite matrices, the Riemannian gradient descent algorithm almost surely
escapes some spurious critical points on the boundary of the manifold. Our
result is the first to partially overcome the incompleteness of the low-rank
matrix manifold without changing the vanilla Riemannian gradient descent
algorithm. The spurious critical points are some rank-deficient matrices that
capture only part of the eigen components of the ground truth. Unlike classical
strict saddle points, they exhibit very singular behavior. We show that using
the dynamical low-rank approximation and a rescaled gradient flow, some of the
spurious critical points can be converted to classical strict saddle points in
the parameterized domain, which leads to the desired result. Numerical
experiments are provided to support our theoretical findings.","['Thomas Y. Hou', 'Zhenzhen Li', 'Ziyun Zhang']","['math.OC', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2021-07-20 00:25:54+00:00
http://arxiv.org/abs/2107.09194v2,Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression,"Models like LASSO and ridge regression are extensively used in practice due
to their interpretability, ease of use, and strong theoretical guarantees.
Cross-validation (CV) is widely used for hyperparameter tuning in these models,
but do practical optimization methods minimize the true out-of-sample loss? A
recent line of research promises to show that the optimum of the CV loss
matches the optimum of the out-of-sample loss (possibly after simple
corrections). It remains to show how tractable it is to minimize the CV loss.
In the present paper, we show that, in the case of ridge regression, the CV
loss may fail to be quasiconvex and thus may have multiple local optima. We can
guarantee that the CV loss is quasiconvex in at least one case: when the
spectrum of the covariate matrix is nearly flat and the noise in the observed
responses is not too high. More generally, we show that quasiconvexity status
is independent of many properties of the observed data (response norm,
covariate-matrix right singular vectors and singular-value scaling) and has a
complex dependence on the few that remain. We empirically confirm our theory
using simulated experiments.","['William T. Stephenson', 'Zachary Frangella', 'Madeleine Udell', 'Tamara Broderick']","['stat.ML', 'cs.LG', 'stat.ME']",2021-07-19 23:22:24+00:00
http://arxiv.org/abs/2107.09158v1,Improving exploration in policy gradient search: Application to symbolic optimization,"Many machine learning strategies designed to automate mathematical tasks
leverage neural networks to search large combinatorial spaces of mathematical
symbols. In contrast to traditional evolutionary approaches, using a neural
network at the core of the search allows learning higher-level symbolic
patterns, providing an informed direction to guide the search. When no labeled
data is available, such networks can still be trained using reinforcement
learning. However, we demonstrate that this approach can suffer from an early
commitment phenomenon and from initialization bias, both of which limit
exploration. We present two exploration methods to tackle these issues,
building upon ideas of entropy regularization and distribution initialization.
We show that these techniques can improve the performance, increase sample
efficiency, and lower the complexity of solutions for the task of symbolic
regression.","['Mikel Landajuela', 'Brenden K. Petersen', 'Soo K. Kim', 'Claudio P. Santiago', 'Ruben Glatt', 'T. Nathan Mundhenk', 'Jacob F. Pettit', 'Daniel M. Faissol']","['cs.LG', 'stat.ML']",2021-07-19 21:11:07+00:00
http://arxiv.org/abs/2107.09150v1,Inference for Change Points in High Dimensional Mean Shift Models,"We consider the problem of constructing confidence intervals for the
locations of change points in a high-dimensional mean shift model. To that end,
we develop a locally refitted least squares estimator and obtain component-wise
and simultaneous rates of estimation of the underlying change points. The
simultaneous rate is the sharpest available in the literature by at least a
factor of $\log p,$ while the component-wise one is optimal. These results
enable existence of limiting distributions. Component-wise distributions are
characterized under both vanishing and non-vanishing jump size regimes, while
joint distributions for any finite subset of change point estimates are
characterized under the latter regime, which also yields asymptotic
independence of these estimates. The combined results are used to construct
asymptotically valid component-wise and simultaneous confidence intervals for
the change point parameters. The results are established under a high
dimensional scaling, allowing for diminishing jump sizes, in the presence of
diverging number of change points and under subexponential errors. They are
illustrated on synthetic data and on sensor measurements from smartphones for
activity recognition.","['Abhishek Kaul', 'George Michailidis']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2021-07-19 20:56:15+00:00
