id,title,abstract,authors,categories,date
http://arxiv.org/abs/1609.07165v1,Robust Confidence Intervals in High-Dimensional Left-Censored Regression,"This paper develops robust confidence intervals in high-dimensional and
left-censored regression. Type-I censored regression models are extremely
common in practice, where a competing event makes the variable of interest
unobservable. However, techniques developed for entirely observed data do not
directly apply to the censored observations. In this paper, we develop smoothed
estimating equations that augment the de-biasing method, such that the
resulting estimator is adaptive to censoring and is more robust to the
misspecification of the error distribution. We propose a unified class of
robust estimators, including Mallow's, Schweppe's and Hill-Ryan's one-step
estimator.
  In the ultra-high-dimensional setting, where the dimensionality can grow
exponentially with the sample size, we show that as long as the preliminary
estimator converges faster than $n^{-1/4}$, the one-step estimator inherits
asymptotic distribution of fully iterated version. Moreover, we show that the
size of the residuals of the Bahadur representation matches those of the simple
linear models, $s^{3/4 } (\log (p \vee n))^{3/4} / n^{1/4}$ -- that is, the
effects of censoring asymptotically disappear. Simulation studies demonstrate
that our method is adaptive to the censoring level and asymmetry in the error
distribution, and does not lose efficiency when the errors are from symmetric
distributions. Finally, we apply the developed method to a real data set from
the MAQC-II repository that is related to the HIV-1 study.","['Jelena Bradic', 'Jiaqi Guo']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2016-09-22 21:09:43+00:00
http://arxiv.org/abs/1609.07093v3,Neural Photo Editing with Introspective Adversarial Networks,"The increasingly photorealistic sample quality of generative image models
suggests their feasibility in applications beyond image generation. We present
the Neural Photo Editor, an interface that leverages the power of generative
neural networks to make large, semantically coherent changes to existing
images. To tackle the challenge of achieving accurate reconstructions without
loss of feature quality, we introduce the Introspective Adversarial Network, a
novel hybridization of the VAE and GAN. Our model efficiently captures
long-range dependencies through use of a computational block based on
weight-shared dilated convolutions, and improves generalization performance
with Orthogonal Regularization, a novel weight regularization method. We
validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples
and reconstructions with high visual fidelity.","['Andrew Brock', 'Theodore Lim', 'J. M. Ritchie', 'Nick Weston']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2016-09-22 18:07:56+00:00
http://arxiv.org/abs/1609.07087v2,(Bandit) Convex Optimization with Biased Noisy Gradient Oracles,"Algorithms for bandit convex optimization and online learning often rely on
constructing noisy gradient estimates, which are then used in appropriately
adjusted first-order algorithms, replacing actual gradients. Depending on the
properties of the function to be optimized and the nature of ``noise'' in the
bandit feedback, the bias and variance of gradient estimates exhibit various
tradeoffs. In this paper we propose a novel framework that replaces the
specific gradient estimation methods with an abstract oracle. With the help of
the new framework we unify previous works, reproducing their results in a clean
and concise fashion, while, perhaps more importantly, the framework also allows
us to formally show that to achieve the optimal root-$n$ rate either the
algorithms that use existing gradient estimators, or the proof techniques used
to analyze them have to go beyond what exists today.","['Xiaowei Hu', 'Prashanth L. A.', 'András György', 'Csaba Szepesvári']","['cs.LG', 'stat.ML']",2016-09-22 17:56:38+00:00
http://arxiv.org/abs/1609.07072v1,The Many-Body Expansion Combined with Neural Networks,"Fragmentation methods such as the many-body expansion (MBE) are a common
strategy to model large systems by partitioning energies into a hierarchy of
decreasingly significant contributions. The number of fragments required for
chemical accuracy is still prohibitively expensive for ab-initio MBE to compete
with force field approximations for applications beyond single-point energies.
Alongside the MBE, empirical models of ab-initio potential energy surfaces have
improved, especially non-linear models based on neural networks (NN) which can
reproduce ab-initio potential energy surfaces rapidly and accurately. Although
they are fast, NNs suffer from their own curse of dimensionality; they must be
trained on a representative sample of chemical space. In this paper we examine
the synergy of the MBE and NN's, and explore their complementarity. The MBE
offers a systematic way to treat systems of arbitrary size and intelligently
sample chemical space. NN's reduce, by a factor in excess of $10^6$ the
computational overhead of the MBE and reproduce the accuracy of ab-initio
calculations without specialized force fields. We show they are remarkably
general, providing comparable accuracy with drastically different chemical
embeddings. To assess this we test a new chemical embedding which can be
inverted to predict molecules with desired properties.","['Kun Yao', 'John E. Herr', 'John Parkhill']","['physics.chem-ph', 'physics.comp-ph', 'stat.ML']",2016-09-22 17:09:47+00:00
http://arxiv.org/abs/1609.07060v1,An equivalence between high dimensional Bayes optimal inference and M-estimation,"When recovering an unknown signal from noisy measurements, the computational
difficulty of performing optimal Bayesian MMSE (minimum mean squared error)
inference often necessitates the use of maximum a posteriori (MAP) inference, a
special case of regularized M-estimation, as a surrogate. However, MAP is
suboptimal in high dimensions, when the number of unknown signal components is
similar to the number of measurements. In this work we demonstrate, when the
signal distribution and the likelihood function associated with the noise are
both log-concave, that optimal MMSE performance is asymptotically achievable
via another M-estimation procedure. This procedure involves minimizing convex
loss and regularizer functions that are nonlinearly smoothed versions of the
widely applied MAP optimization problem. Our findings provide a new heuristic
derivation and interpretation for recent optimal M-estimators found in the
setting of linear measurements and additive noise, and further extend these
results to nonlinear measurements with non-additive noise. We numerically
demonstrate superior performance of our optimal M-estimators relative to MAP.
Overall, at the heart of our work is the revelation of a remarkable equivalence
between two seemingly very different computational problems: namely that of
high dimensional Bayesian integration underlying MMSE inference, and high
dimensional convex optimization underlying M-estimation. In essence we show
that the former difficult integral may be computed by solving the latter,
simpler optimization problem.","['Madhu Advani', 'Surya Ganguli']","['stat.ML', 'cond-mat.dis-nn', 'math.ST', 'q-bio.NC', 'stat.TH']",2016-09-22 16:46:18+00:00
http://arxiv.org/abs/1609.07042v4,Pose-Selective Max Pooling for Measuring Similarity,"In this paper, we deal with two challenges for measuring the similarity of
the subject identities in practical video-based face recognition - the
variation of the head pose in uncontrolled environments and the computational
expense of processing videos. Since the frame-wise feature mean is unable to
characterize the pose diversity among frames, we define and preserve the
overall pose diversity and closeness in a video. Then, identity will be the
only source of variation across videos since the pose varies even within a
single video. Instead of simply using all the frames, we select those faces
whose pose point is closest to the centroid of the K-means cluster containing
that pose point. Then, we represent a video as a bag of frame-wise deep face
features while the number of features has been reduced from hundreds to K.
Since the video representation can well represent the identity, now we measure
the subject similarity between two videos as the max correlation among all
possible pairs in the two bags of features. On the official 5,000 video-pairs
of the YouTube Face dataset for face verification, our algorithm achieves a
comparable performance with VGG-face that averages over deep features of all
frames. Other vision tasks can also benefit from the generic idea of employing
geometric cues to improve the descriptiveness of deep features.","['Xiang Xiang', 'Trac D. Tran']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2016-09-22 15:59:38+00:00
http://arxiv.org/abs/1609.06942v1,Randomized Independent Component Analysis,"Independent component analysis (ICA) is a method for recovering statistically
independent signals from observations of unknown linear combinations of the
sources. Some of the most accurate ICA decomposition methods require searching
for the inverse transformation which minimizes different approximations of the
Mutual Information, a measure of statistical independence of random vectors.
Two such approximations are the Kernel Generalized Variance or the Kernel
Canonical Correlation which has been shown to reach the highest performance of
ICA methods. However, the computational effort necessary just for computing
these measures is cubic in the sample size. Hence, optimizing them becomes even
more computationally demanding, in terms of both space and time. Here, we
propose a couple of alternative novel measures based on randomized features of
the samples - the Randomized Generalized Variance and the Randomized Canonical
Correlation. The computational complexity of calculating the proposed
alternatives is linear in the sample size and provide a controllable
approximation of their Kernel-based non-random versions. We also show that
optimization of the proposed statistical properties yields a comparable
separation error at an order of magnitude faster compared to Kernel-based
measures.","['Matan Sela', 'Ron Kimmel']","['stat.ML', 'cs.LG', 'cs.SY', 'math.PR', 'math.ST', 'stat.TH']",2016-09-22 12:40:58+00:00
http://arxiv.org/abs/1610.01989v1,Regularized Dynamic Boltzmann Machine with Delay Pruning for Unsupervised Learning of Temporal Sequences,"We introduce Delay Pruning, a simple yet powerful technique to regularize
dynamic Boltzmann machines (DyBM). The recently introduced DyBM provides a
particularly structured Boltzmann machine, as a generative model of a
multi-dimensional time-series. This Boltzmann machine can have infinitely many
layers of units but allows exact inference and learning based on its
biologically motivated structure. DyBM uses the idea of conduction delays in
the form of fixed length first-in first-out (FIFO) queues, with a neuron
connected to another via this FIFO queue, and spikes from a pre-synaptic neuron
travel along the queue to the post-synaptic neuron with a constant period of
delay. Here, we present Delay Pruning as a mechanism to prune the lengths of
the FIFO queues (making them zero) by setting some delay lengths to one with a
fixed probability, and finally selecting the best performing model with fixed
delays. The uniqueness of structure and a non-sampling based learning rule in
DyBM, make the application of previously proposed regularization techniques
like Dropout or DropConnect difficult, leading to poor generalization. First,
we evaluate the performance of Delay Pruning to let DyBM learn a
multidimensional temporal sequence generated by a Markov chain. Finally, we
show the effectiveness of delay pruning in learning high dimensional sequences
using the moving MNIST dataset, and compare it with Dropout and DropConnect
methods.","['Sakyasingha Dasgupta', 'Takayuki Yoshizumi', 'Takayuki Osogami']","['cs.LG', 'cs.NE', 'stat.ML']",2016-09-22 10:04:59+00:00
http://arxiv.org/abs/1609.06864v2,A probabilistic network for the diagnosis of acute cardiopulmonary diseases,"In this paper, the development of a probabilistic network for the diagnosis
of acute cardiopulmonary diseases is presented. This paper is a draft version
of the article published after peer review in 2018
(https://doi.org/10.1002/bimj.201600206). A panel of expert physicians
collaborated to specify the qualitative part, that is a directed acyclic graph
defining a factorization of the joint probability distribution of domain
variables. The quantitative part, that is the set of all conditional
probability distributions defined by each factor, was estimated in the Bayesian
paradigm: we applied a special formal representation, characterized by a low
number of parameters and a parameterization intelligible for physicians,
elicited the joint prior distribution of parameters from medical experts, and
updated it by conditioning on a dataset of hospital patient records using
Markov Chain Monte Carlo simulation. Refinement was cyclically performed until
the probabilistic network provided satisfactory Concordance Index values for a
selection of acute diseases and reasonable inference on six fictitious patient
cases. The probabilistic network can be employed to perform medical diagnosis
on a total of 63 diseases (38 acute and 25 chronic) on the basis of up to 167
patient findings.","['Alessandro Magrini', 'Davide Luciani', 'Federico Mattia Stefanini']","['stat.ML', 'stat.AP', '62F15, 62P10']",2016-09-22 08:28:38+00:00
http://arxiv.org/abs/1609.06840v2,Exact Sampling from Determinantal Point Processes,"Determinantal point processes (DPPs) are an important concept in random
matrix theory and combinatorics. They have also recently attracted interest in
the study of numerical methods for machine learning, as they offer an elegant
""missing link"" between independent Monte Carlo sampling and deterministic
evaluation on regular grids, applicable to a general set of spaces. This is
helpful whenever an algorithm explores to reduce uncertainty, such as in active
learning, Bayesian optimization, reinforcement learning, and marginalization in
graphical models. To draw samples from a DPP in practice, existing literature
focuses on approximate schemes of low cost, or comparably inefficient exact
algorithms like rejection sampling. We point out that, for many settings of
relevance to machine learning, it is also possible to draw exact samples from
DPPs on continuous domains. We start from an intuitive example on the real
line, which is then generalized to multivariate real vector spaces. We also
compare to previously studied approximations, showing that exact sampling,
despite higher cost, can be preferable where precision is needed.","['Philipp Hennig', 'Roman Garnett']","['cs.LG', 'math.PR', 'stat.ML']",2016-09-22 07:06:28+00:00
http://arxiv.org/abs/1609.06831v1,Hawkes Processes with Stochastic Excitations,"We propose an extension to Hawkes processes by treating the levels of
self-excitation as a stochastic differential equation. Our new point process
allows better approximation in application domains where events and intensities
accelerate each other with correlated levels of contagion. We generalize a
recent algorithm for simulating draws from Hawkes processes whose levels of
excitation are stochastic processes, and propose a hybrid Markov chain Monte
Carlo approach for model fitting. Our sampling procedure scales linearly with
the number of required events and does not require stationarity of the point
process. A modular inference procedure consisting of a combination between
Gibbs and Metropolis Hastings steps is put forward. We recover expectation
maximization as a special case. Our general approach is illustrated for
contagion following geometric Brownian motion and exponential Langevin
dynamics.","['Young Lee', 'Kar Wai Lim', 'Cheng Soon Ong']","['cs.LG', 'stat.ML']",2016-09-22 06:18:20+00:00
http://arxiv.org/abs/1609.06826v1,Bibliographic Analysis with the Citation Network Topic Model,"Bibliographic analysis considers author's research areas, the citation
network and paper content among other things. In this paper, we combine these
three in a topic model that produces a bibliographic model of authors, topics
and documents using a non-parametric extension of a combination of the Poisson
mixed-topic link model and the author-topic model. We propose a novel and
efficient inference algorithm for the model to explore subsets of research
publications from CiteSeerX. Our model demonstrates improved performance in
both model fitting and a clustering task compared to several baselines.","['Kar Wai Lim', 'Wray Buntine']","['cs.DL', 'cs.LG', 'stat.ML']",2016-09-22 05:46:46+00:00
http://arxiv.org/abs/1609.06783v1,Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor Processes,"The Dirichlet process and its extension, the Pitman-Yor process, are
stochastic processes that take probability distributions as a parameter. These
processes can be stacked up to form a hierarchical nonparametric Bayesian
model. In this article, we present efficient methods for the use of these
processes in this hierarchical context, and apply them to latent variable
models for text analytics. In particular, we propose a general framework for
designing these Bayesian models, which are called topic models in the computer
science community. We then propose a specific nonparametric Bayesian topic
model for modelling text from social media. We focus on tweets (posts on
Twitter) in this article due to their ease of access. We find that our
nonparametric model performs better than existing parametric models in both
goodness of fit and real world applications.","['Kar Wai Lim', 'Wray Buntine', 'Changyou Chen', 'Lan Du']","['stat.ML', 'cs.CL', 'cs.LG']",2016-09-22 00:10:16+00:00
http://arxiv.org/abs/1609.06764v3,Saturating Splines and Feature Selection,"We extend the adaptive regression spline model by incorporating saturation,
the natural requirement that a function extend as a constant outside a certain
range. We fit saturating splines to data using a convex optimization problem
over a space of measures, which we solve using an efficient algorithm based on
the conditional gradient method. Unlike many existing approaches, our algorithm
solves the original infinite-dimensional (for splines of degree at least two)
optimization problem without pre-specified knot locations. We then adapt our
algorithm to fit generalized additive models with saturating splines as
coordinate functions and show that the saturation requirement allows our model
to simultaneously perform feature selection and nonlinear function fitting.
Finally, we briefly sketch how the method can be extended to higher order
splines and to different requirements on the extension outside the data range.","['Nicholas Boyd', 'Trevor Hastie', 'Stephen Boyd', 'Benjamin Recht', 'Michael Jordan']",['stat.ML'],2016-09-21 21:57:25+00:00
http://arxiv.org/abs/1609.06575v2,Theoretical Evaluation of Feature Selection Methods based on Mutual Information,"Feature selection methods are usually evaluated by wrapping specific
classifiers and datasets in the evaluation process, resulting very often in
unfair comparisons between methods. In this work, we develop a theoretical
framework that allows obtaining the true feature ordering of two-dimensional
sequential forward feature selection methods based on mutual information, which
is independent of entropy or mutual information estimation methods,
classifiers, or datasets, and leads to an undoubtful comparison of the methods.
Moreover, the theoretical framework unveils problems intrinsic to some methods
that are otherwise difficult to detect, namely inconsistencies in the
construction of the objective function used to select the candidate features,
due to various types of indeterminations and to the possibility of the entropy
of continuous random variables taking null and negative values.","['Cláudia Pascoal', 'M. Rosário Oliveira', 'António Pacheco', 'Rui Valadas']","['stat.ML', 'cs.LG']",2016-09-21 14:23:15+00:00
http://arxiv.org/abs/1609.06533v1,On Data-Independent Properties for Density-Based Dissimilarity Measures in Hybrid Clustering,"Hybrid clustering combines partitional and hierarchical clustering for
computational effectiveness and versatility in cluster shape. In such
clustering, a dissimilarity measure plays a crucial role in the hierarchical
merging. The dissimilarity measure has great impact on the final clustering,
and data-independent properties are needed to choose the right dissimilarity
measure for the problem at hand. Properties for distance-based dissimilarity
measures have been studied for decades, but properties for density-based
dissimilarity measures have so far received little attention. Here, we propose
six data-independent properties to evaluate density-based dissimilarity
measures associated with hybrid clustering, regarding equality, orthogonality,
symmetry, outlier and noise observations, and light-tailed models for
heavy-tailed clusters. The significance of the properties is investigated, and
we study some well-known dissimilarity measures based on Shannon entropy,
misclassification rate, Bhattacharyya distance and Kullback-Leibler divergence
with respect to the proposed properties. As none of them satisfy all the
proposed properties, we introduce a new dissimilarity measure based on the
Kullback-Leibler information and show that it satisfies all proposed
properties. The effect of the proposed properties is also illustrated on
several real and simulated data sets.","['Kajsa Møllersen', 'Subhra S. Dhar', 'Fred Godtliebsen']","['stat.ML', 'cs.LG']",2016-09-21 12:46:09+00:00
http://arxiv.org/abs/1609.06532v1,"Bibliographic Analysis on Research Publications using Authors, Categorical Labels and the Citation Network","Bibliographic analysis considers the author's research areas, the citation
network and the paper content among other things. In this paper, we combine
these three in a topic model that produces a bibliographic model of authors,
topics and documents, using a nonparametric extension of a combination of the
Poisson mixed-topic link model and the author-topic model. This gives rise to
the Citation Network Topic Model (CNTM). We propose a novel and efficient
inference algorithm for the CNTM to explore subsets of research publications
from CiteSeerX. The publication datasets are organised into three corpora,
totalling to about 168k publications with about 62k authors. The queried
datasets are made available online. In three publicly available corpora in
addition to the queried datasets, our proposed model demonstrates an improved
performance in both model fitting and document clustering, compared to several
baselines. Moreover, our model allows extraction of additional useful knowledge
from the corpora, such as the visualisation of the author-topics network.
Additionally, we propose a simple method to incorporate supervision into topic
modelling to achieve further improvement on the clustering task.","['Kar Wai Lim', 'Wray Buntine']","['cs.DL', 'cs.LG', 'stat.ML']",2016-09-21 12:44:37+00:00
http://arxiv.org/abs/1609.06480v1,Network-regularized Sparse Logistic Regression Models for Clinical Risk Prediction and Biomarker Discovery,"Molecular profiling data (e.g., gene expression) has been used for clinical
risk prediction and biomarker discovery. However, it is necessary to integrate
other prior knowledge like biological pathways or gene interaction networks to
improve the predictive ability and biological interpretability of biomarkers.
Here, we first introduce a general regularized Logistic Regression (LR)
framework with regularized term $\lambda \|\bm{w}\|_1 +
\eta\bm{w}^T\bm{M}\bm{w}$, which can reduce to different penalties, including
Lasso, elastic net, and network-regularized terms with different $\bm{M}$. This
framework can be easily solved in a unified manner by a cyclic coordinate
descent algorithm which can avoid inverse matrix operation and accelerate the
computing speed. However, if those estimated $\bm{w}_i$ and $\bm{w}_j$ have
opposite signs, then the traditional network-regularized penalty may not
perform well. To address it, we introduce a novel network-regularized sparse LR
model with a new penalty $\lambda \|\bm{w}\|_1 + \eta|\bm{w}|^T\bm{M}|\bm{w}|$
to consider the difference between the absolute values of the coefficients. And
we develop two efficient algorithms to solve it. Finally, we test our methods
and compare them with the related ones using simulated and real data to show
their efficiency.","['Wenwen Min', 'Juan Liu', 'Shihua Zhang']","['q-bio.GN', 'cs.LG', 'stat.ML', 'J.3; H.2.8; G.1.6; I.5']",2016-09-21 09:47:32+00:00
http://arxiv.org/abs/1609.06957v1,Early Warning System for Seismic Events in Coal Mines Using Machine Learning,"This document describes an approach to the problem of predicting dangerous
seismic events in active coal mines up to 8 hours in advance. It was developed
as a part of the AAIA'16 Data Mining Challenge: Predicting Dangerous Seismic
Events in Active Coal Mines. The solutions presented consist of ensembles of
various predictive models trained on different sets of features. The best one
achieved a winning score of 0.939 AUC.","['Robert Bogucki', 'Jan Lasek', 'Jan Kanty Milczek', 'Michal Tadeusiak']","['cs.LG', 'stat.ML']",2016-09-21 09:35:56+00:00
http://arxiv.org/abs/1609.06457v1,AMOS: An Automated Model Order Selection Algorithm for Spectral Graph Clustering,"One of the longstanding problems in spectral graph clustering (SGC) is the
so-called model order selection problem: automated selection of the correct
number of clusters. This is equivalent to the problem of finding the number of
connected components or communities in an undirected graph. In this paper, we
propose AMOS, an automated model order selection algorithm for SGC. Based on a
recent analysis of clustering reliability for SGC under the random
interconnection model, AMOS works by incrementally increasing the number of
clusters, estimating the quality of identified clusters, and providing a series
of clustering reliability tests. Consequently, AMOS outputs clusters of minimal
model order with statistical clustering reliability guarantees. Comparing to
three other automated graph clustering methods on real-world datasets, AMOS
shows superior performance in terms of multiple external and internal
clustering metrics.","['Pin-Yu Chen', 'Thibaut Gensollen', 'Alfred O. Hero III']","['cs.SI', 'cs.LG', 'stat.ML']",2016-09-21 08:14:12+00:00
http://arxiv.org/abs/1609.06390v1,Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices,"Recently, there has been a surge of interest in using spectral methods for
estimating latent variable models. However, it is usually assumed that the
distribution of the observations conditioned on the latent variables is either
discrete or belongs to a parametric family. In this paper, we study the
estimation of an $m$-state hidden Markov model (HMM) with only smoothness
assumptions, such as H\""olderian conditions, on the emission densities. By
leveraging some recent advances in continuous linear algebra and numerical
analysis, we develop a computationally efficient spectral algorithm for
learning nonparametric HMMs. Our technique is based on computing an SVD on
nonparametric estimates of density functions by viewing them as
\emph{continuous matrices}. We derive sample complexity bounds via
concentration results for nonparametric density estimation and novel
perturbation theory results for continuous matrices. We implement our method
using Chebyshev polynomial approximations. Our method is competitive with other
baselines on synthetic and real problems and is also very computationally
efficient.","['Kirthevasan Kandasamy', 'Maruan Al-Shedivat', 'Eric P. Xing']","['stat.ML', 'cs.LG']",2016-09-21 00:15:44+00:00
http://arxiv.org/abs/1609.06385v1,Multiclass Classification Calibration Functions,"In this paper we refine the process of computing calibration functions for a
number of multiclass classification surrogate losses. Calibration functions are
a powerful tool for easily converting bounds for the surrogate risk (which can
be computed through well-known methods) into bounds for the true risk, the
probability of making a mistake. They are particularly suitable in
non-parametric settings, where the approximation error can be controlled, and
provide tighter bounds than the common technique of upper-bounding the 0-1 loss
by the surrogate loss.
  The abstract nature of the more sophisticated existing calibration function
results requires calibration functions to be explicitly derived on a
case-by-case basis, requiring repeated efforts whenever bounds for a new
surrogate loss are required. We devise a streamlined analysis that simplifies
the process of deriving calibration functions for a large number of surrogate
losses that have been proposed in the literature. The effort of deriving
calibration functions is then surmised in verifying, for a chosen surrogate
loss, a small number of conditions that we introduce.
  As case studies, we recover existing calibration functions for the well-known
loss of Lee et al. (2004), and also provide novel calibration functions for
well-known losses, including the one-versus-all loss and the logistic
regression loss, plus a number of other losses that have been shown to be
classification-calibrated in the past, but for which no calibration function
had been derived.","['Bernardo Ávila Pires', 'Csaba Szepesvári']","['stat.ML', 'cs.LG']",2016-09-20 23:41:55+00:00
http://arxiv.org/abs/1609.06369v2,Generalized Kalman Smoothing: Modeling and Algorithms,"State-space smoothing has found many applications in science and engineering.
Under linear and Gaussian assumptions, smoothed estimates can be obtained using
efficient recursions, for example Rauch-Tung-Striebel and Mayne-Fraser
algorithms. Such schemes are equivalent to linear algebraic techniques that
minimize a convex quadratic objective function with structure induced by the
dynamic model.
  These classical formulations fall short in many important circumstances. For
instance, smoothers obtained using quadratic penalties can fail when outliers
are present in the data, and cannot track impulsive inputs and abrupt state
changes. Motivated by these shortcomings, generalized Kalman smoothing
formulations have been proposed in the last few years, replacing quadratic
models with more suitable, often nonsmooth, convex functions. In contrast to
classical models, these general estimators require use of iterated algorithms,
and these have received increased attention from control, signal processing,
machine learning, and optimization communities.
  In this survey we show that the optimization viewpoint provides the control
and signal processing community great freedom in the development of novel
modeling and inference frameworks for dynamical systems. We discuss general
statistical models for dynamic systems, making full use of nonsmooth convex
penalties and constraints, and providing links to important models in signal
processing and machine learning. We also survey optimization techniques for
these formulations, paying close attention to dynamic problem structure.
Modeling concepts and algorithms are illustrated with numerical examples.","['A. Y. Aravkin', 'J. V. Burke', 'L. Ljung', 'A. Lozano', 'G. Pillonetto']","['math.OC', 'stat.ML', '62F35, 65K10, 49M15']",2016-09-20 21:58:06+00:00
http://arxiv.org/abs/1609.07480v1,Predictive modelling of football injuries,"The goal of this thesis is to investigate the potential of predictive
modelling for football injuries. This work was conducted in close collaboration
with Tottenham Hotspurs FC (THFC), the PGA European tour and the participation
of Wolverhampton Wanderers (WW).
  Three investigations were conducted:
  1. Predicting the recovery time of football injuries using the UEFA injury
recordings: The UEFA recordings is a common standard for recording injuries in
professional football. For this investigation, three datasets of UEFA injury
recordings were available. Different machine learning algorithms were used in
order to build a predictive model. The performance of the machine learning
models is then improved by using feature selection conducted through
correlation-based subset feature selection and random forests.
  2. Predicting injuries in professional football using exposure records: The
relationship between exposure (in training hours and match hours) in
professional football athletes and injury incidence was studied. A common
problem in football is understanding how the training schedule of an athlete
can affect the chance of him getting injured. The task was to predict the
number of days a player can train before he gets injured.
  3. Predicting intrinsic injury incidence using in-training GPS measurements:
A significant percentage of football injuries can be attributed to overtraining
and fatigue. GPS data collected during training sessions might provide
indicators of fatigue, or might be used to detect very intense training
sessions which can lead to overtraining. This research used GPS data gathered
during training sessions of the first team of THFC, in order to predict whether
an injury would take place during a week.",['Stylianos Kampakis'],"['stat.AP', 'cs.AI', 'stat.ML']",2016-09-20 11:58:42+00:00
http://arxiv.org/abs/1609.06100v4,Distributed Adaptive Learning of Graph Signals,"The aim of this paper is to propose distributed strategies for adaptive
learning of signals defined over graphs. Assuming the graph signal to be
bandlimited, the method enables distributed reconstruction, with guaranteed
performance in terms of mean-square error, and tracking from a limited number
of sampled observations taken from a subset of vertices. A detailed mean square
analysis is carried out and illustrates the role played by the sampling
strategy on the performance of the proposed method. Finally, some useful
strategies for distributed selection of the sampling set are provided. Several
numerical results validate our theoretical findings, and illustrate the
performance of the proposed method for distributed adaptive learning of signals
defined over graphs.","['P. Di Lorenzo', 'P. Banelli', 'S. Barbarossa', 'S. Sardellitti']","['cs.LG', 'stat.ML']",2016-09-20 11:12:04+00:00
http://arxiv.org/abs/1609.06070v2,Boosting Factor-Specific Functional Historical Models for the Detection of Synchronisation in Bioelectrical Signals,"The link between different psychophysiological measures during emotion
episodes is not well understood. To analyse the functional relationship between
electroencephalography (EEG) and facial electromyography (EMG), we apply
historical function-on-function regression models to EEG and EMG data that were
simultaneously recorded from 24 participants while they were playing a
computerised gambling task. Given the complexity of the data structure for this
application, we extend simple functional historical models to models including
random historical effects, factor-specific historical effects, and
factor-specific random historical effects. Estimation is conducted by a
component-wise gradient boosting algorithm, which scales well to large data
sets and complex models.","['David Rügamer', 'Sarah Brockhaus', 'Kornelia Gentsch', 'Klaus Scherer', 'Sonja Greven']","['stat.AP', 'stat.ME', 'stat.ML']",2016-09-20 09:42:32+00:00
http://arxiv.org/abs/1609.05959v1,Conformalized Kernel Ridge Regression,"General predictive models do not provide a measure of confidence in
predictions without Bayesian assumptions. A way to circumvent potential
restrictions is to use conformal methods for constructing non-parametric
confidence regions, that offer guarantees regarding validity. In this paper we
provide a detailed description of a computationally efficient conformal
procedure for Kernel Ridge Regression (KRR), and conduct a comparative
numerical study to see how well conformal regions perform against the Bayesian
confidence sets. The results suggest that conformalized KRR can yield
predictive confidence regions with specified coverage rate, which is essential
in constructing anomaly detection systems based on predictive models.","['Evgeny Burnaev', 'Ivan Nazarov']","['stat.ML', 'cs.LG', 'stat.AP']",2016-09-19 22:30:36+00:00
http://arxiv.org/abs/1609.05881v1,Online and Distributed learning of Gaussian mixture models by Bayesian Moment Matching,"The Gaussian mixture model is a classic technique for clustering and data
modeling that is used in numerous applications. With the rise of big data,
there is a need for parameter estimation techniques that can handle streaming
data and distribute the computation over several processors. While online
variants of the Expectation Maximization (EM) algorithm exist, their data
efficiency is reduced by a stochastic approximation of the E-step and it is not
clear how to distribute the computation over multiple processors. We propose a
Bayesian learning technique that lends itself naturally to online and
distributed computation. Since the Bayesian posterior is not tractable, we
project it onto a family of tractable distributions after each observation by
matching a set of sufficient moments. This Bayesian moment matching technique
compares favorably to online EM in terms of time and accuracy on a set of data
modeling benchmarks.","['Priyank Jaini', 'Pascal Poupart']","['cs.AI', 'cs.LG', 'stat.ML']",2016-09-19 19:44:06+00:00
http://arxiv.org/abs/1609.05877v1,Geometrically Convergent Distributed Optimization with Uncoordinated Step-Sizes,"A recent algorithmic family for distributed optimization, DIGing's, have been
shown to have geometric convergence over time-varying undirected/directed
graphs. Nevertheless, an identical step-size for all agents is needed. In this
paper, we study the convergence rates of the Adapt-Then-Combine (ATC) variation
of the DIGing algorithm under uncoordinated step-sizes. We show that the ATC
variation of DIGing algorithm converges geometrically fast even if the
step-sizes are different among the agents. In addition, our analysis implies
that the ATC structure can accelerate convergence compared to the distributed
gradient descent (DGD) structure which has been used in the original DIGing
algorithm.","['Angelia Nedić', 'Alex Olshevsky', 'Wei Shi', 'César A. Uribe']","['math.OC', 'cs.SY', 'stat.ML']",2016-09-19 19:22:10+00:00
http://arxiv.org/abs/1609.05866v1,A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size Representations,"The softmax content-based attention mechanism has proven to be very
beneficial in many applications of recurrent neural networks. Nevertheless it
suffers from two major computational limitations. First, its computations for
an attention lookup scale linearly in the size of the attended sequence.
Second, it does not encode the sequence into a fixed-size representation but
instead requires to memorize all the hidden states. These two limitations
restrict the use of the softmax attention mechanism to relatively small-scale
applications with short sequences and few lookups per sequence. In this work we
introduce a family of linear attention mechanisms designed to overcome the two
limitations listed above. We show that removing the softmax non-linearity from
the traditional attention formulation yields constant-time attention lookups
and fixed-size representations of the attended sequences. These properties make
these linear attention mechanisms particularly suitable for large-scale
applications with extreme query loads, real-time requirements and memory
constraints. Early experiments on a question answering task show that these
linear mechanisms yield significantly better accuracy results than no
attention, but obviously worse than their softmax alternative.","['Alexandre de Brébisson', 'Pascal Vincent']","['cs.LG', 'cs.IR', 'cs.NE', 'stat.ML']",2016-09-19 18:55:18+00:00
http://arxiv.org/abs/1609.05820v3,The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences,"Various applications involve assigning discrete label values to a collection
of objects based on some pairwise noisy data. Due to the discrete---and hence
nonconvex---structure of the problem, computing the optimal assignment
(e.g.~maximum likelihood assignment) becomes intractable at first sight. This
paper makes progress towards efficient computation by focusing on a concrete
joint alignment problem---that is, the problem of recovering $n$ discrete
variables $x_i \in \{1,\cdots, m\}$, $1\leq i\leq n$ given noisy observations
of their modulo differences $\{x_i - x_j~\mathsf{mod}~m\}$. We propose a
low-complexity and model-free procedure, which operates in a lifted space by
representing distinct label values in orthogonal directions, and which attempts
to optimize quadratic functions over hypercubes. Starting with a first guess
computed via a spectral method, the algorithm successively refines the iterates
via projected power iterations. We prove that for a broad class of statistical
models, the proposed projected power method makes no error---and hence
converges to the maximum likelihood estimate---in a suitable regime. Numerical
experiments have been carried out on both synthetic and real data to
demonstrate the practicality of our algorithm. We expect this algorithmic
framework to be effective for a broad range of discrete assignment problems.","['Yuxin Chen', 'Emmanuel Candes']","['cs.IT', 'cs.CV', 'cs.LG', 'math.IT', 'math.OC', 'stat.ML']",2016-09-19 16:29:46+00:00
http://arxiv.org/abs/1609.05807v2,Inherent Trade-Offs in the Fair Determination of Risk Scores,"Recent discussion in the public sphere about algorithmic classification has
involved tension between competing notions of what it means for a probabilistic
classification to be fair to different groups. We formalize three fairness
conditions that lie at the heart of these debates, and we prove that except in
highly constrained special cases, there is no method that can satisfy these
three conditions simultaneously. Moreover, even satisfying all three conditions
approximately requires that the data lie in an approximate version of one of
the constrained special cases identified by our theorem. These results suggest
some of the ways in which key notions of fairness are incompatible with each
other, and hence provide a framework for thinking about the trade-offs between
them.","['Jon Kleinberg', 'Sendhil Mullainathan', 'Manish Raghavan']","['cs.LG', 'cs.CY', 'stat.ML']",2016-09-19 16:08:51+00:00
http://arxiv.org/abs/1609.05796v2,Enabling Dark Energy Science with Deep Generative Models of Galaxy Images,"Understanding the nature of dark energy, the mysterious force driving the
accelerated expansion of the Universe, is a major challenge of modern
cosmology. The next generation of cosmological surveys, specifically designed
to address this issue, rely on accurate measurements of the apparent shapes of
distant galaxies. However, shape measurement methods suffer from various
unavoidable biases and therefore will rely on a precise calibration to meet the
accuracy requirements of the science analysis. This calibration process remains
an open challenge as it requires large sets of high quality galaxy images. To
this end, we study the application of deep conditional generative models in
generating realistic galaxy images. In particular we consider variations on
conditional variational autoencoder and introduce a new adversarial objective
for training of conditional generative networks. Our results suggest a reliable
alternative to the acquisition of expensive high quality observations for
generating the calibration data needed by the next generation of cosmological
surveys.","['Siamak Ravanbakhsh', 'Francois Lanusse', 'Rachel Mandelbaum', 'Jeff Schneider', 'Barnabas Poczos']","['astro-ph.IM', 'astro-ph.CO', 'cs.AI', 'stat.ML']",2016-09-19 15:48:03+00:00
http://arxiv.org/abs/1609.05772v1,Stochastic Matrix Factorization,"This paper considers a restriction to non-negative matrix factorization in
which at least one matrix factor is stochastic. That is, the elements of the
matrix factors are non-negative and the columns of one matrix factor sum to 1.
This restriction includes topic models, a popular method for analyzing
unstructured data. It also includes a method for storing and finding pictures.
The paper presents necessary and sufficient conditions on the observed data
such that the factorization is unique. In addition, the paper characterizes
natural bounds on the parameters for any observed data and presents a
consistent least squares estimator. The results are illustrated using a topic
model analysis of PhD abstracts in economics and the problem of storing and
retrieving a set of pictures of faces.",['Christopher Adams'],"['stat.ML', 'cs.LG']",2016-09-19 15:19:44+00:00
http://arxiv.org/abs/1609.05573v2,Optimality and Sub-optimality of PCA for Spiked Random Matrices and Synchronization,"A central problem of random matrix theory is to understand the eigenvalues of
spiked random matrix models, in which a prominent eigenvector is planted into a
random matrix. These distributions form natural statistical models for
principal component analysis (PCA) problems throughout the sciences. Baik, Ben
Arous and P\'ech\'e showed that the spiked Wishart ensemble exhibits a sharp
phase transition asymptotically: when the signal strength is above a critical
threshold, it is possible to detect the presence of a spike based on the top
eigenvalue, and below the threshold the top eigenvalue provides no information.
Such results form the basis of our understanding of when PCA can detect a
low-rank signal in the presence of noise.
  However, not all the information about the spike is necessarily contained in
the spectrum. We study the fundamental limitations of statistical methods,
including non-spectral ones. Our results include:
  I) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal
detection threshold for a variety of benign priors for the spike. We extend
previous work on the spherically symmetric and i.i.d. Rademacher priors through
an elementary, unified analysis.
  II) For any non-Gaussian Wigner ensemble, we show that PCA is always
suboptimal for detection. However, a variant of PCA achieves the optimal
threshold (for benign priors) by pre-transforming the matrix entries according
to a carefully designed function. This approach has been stated before, and we
give a rigorous and general analysis.
  III) For both the Gaussian Wishart ensemble and various synchronization
problems over groups, we show that inefficient procedures can work below the
threshold where PCA succeeds, whereas no known efficient algorithm achieves
this. This conjectural gap between what is statistically possible and what can
be done efficiently remains open.","['Amelia Perry', 'Alexander S. Wein', 'Afonso S. Bandeira', 'Ankur Moitra']","['math.ST', 'cs.DS', 'cs.IT', 'math.IT', 'math.PR', 'stat.ML', 'stat.TH', '62H15, 62B15']",2016-09-19 00:25:43+00:00
http://arxiv.org/abs/1609.05539v2,On Randomized Distributed Coordinate Descent with Quantized Updates,"In this paper, we study the randomized distributed coordinate descent
algorithm with quantized updates. In the literature, the iteration complexity
of the randomized distributed coordinate descent algorithm has been
characterized under the assumption that machines can exchange updates with an
infinite precision. We consider a practical scenario in which the messages
exchange occurs over channels with finite capacity, and hence the updates have
to be quantized. We derive sufficient conditions on the quantization error such
that the algorithm with quantized update still converge. We further verify our
theoretical results by running an experiment, where we apply the algorithm with
quantized updates to solve a linear regression problem.","['Mostafa El Gamal', 'Lifeng Lai']","['stat.ML', 'cs.LG']",2016-09-18 20:17:00+00:00
http://arxiv.org/abs/1609.05536v1,Learning Personalized Optimal Control for Repeatedly Operated Systems,"We consider the problem of online learning of optimal control for repeatedly
operated systems in the presence of parametric uncertainty. During each round
of operation, environment selects system parameters according to a fixed but
unknown probability distribution. These parameters govern the dynamics of a
plant. An agent chooses a control input to the plant and is then revealed the
cost of the choice. In this setting, we design an agent that personalizes the
control input to this plant taking into account the stochasticity involved. We
demonstrate the effectiveness of our approach on a simulated system.",['Theja Tulabandhula'],"['cs.LG', 'stat.ML']",2016-09-18 19:58:48+00:00
http://arxiv.org/abs/1609.05528v1,Sequential Ensemble Learning for Outlier Detection: A Bias-Variance Perspective,"Ensemble methods for classification and clustering have been effectively used
for decades, while ensemble learning for outlier detection has only been
studied recently. In this work, we design a new ensemble approach for outlier
detection in multi-dimensional point data, which provides improved accuracy by
reducing error through both bias and variance. Although classification and
outlier detection appear as different problems, their theoretical underpinnings
are quite similar in terms of the bias-variance trade-off [1], where outlier
detection is considered as a binary classification task with unobserved labels
but a similar bias-variance decomposition of error.
  In this paper, we propose a sequential ensemble approach called CARE that
employs a two-phase aggregation of the intermediate results in each iteration
to reach the final outcome. Unlike existing outlier ensembles which solely
incorporate a parallel framework by aggregating the outcomes of independent
base detectors to reduce variance, our ensemble incorporates both the parallel
and sequential building blocks to reduce bias as well as variance by ($i$)
successively eliminating outliers from the original dataset to build a better
data model on which outlierness is estimated (sequentially), and ($ii$)
combining the results from individual base detectors and across iterations
(parallelly). Through extensive experiments on sixteen real-world datasets
mainly from the UCI machine learning repository [2], we show that CARE performs
significantly better than or at least similar to the individual baselines. We
also compare CARE with the state-of-the-art outlier ensembles where it also
provides significant improvement when it is the winner and remains close
otherwise.","['Shebuti Rayana', 'Wen Zhong', 'Leman Akoglu']","['cs.LG', 'stat.ML']",2016-09-18 18:59:42+00:00
http://arxiv.org/abs/1609.05524v3,Principled Option Learning in Markov Decision Processes,"It is well known that options can make planning more efficient, among their
many benefits. Thus far, algorithms for autonomously discovering a set of
useful options were heuristic. Naturally, a principled way of finding a set of
useful options may be more promising and insightful. In this paper we suggest a
mathematical characterization of good sets of options using tools from
information theory. This characterization enables us to find conditions for a
set of options to be optimal and an algorithm that outputs a useful set of
options and illustrate the proposed algorithm in simulation.","['Roy Fox', 'Michal Moshkovitz', 'Naftali Tishby']","['cs.LG', 'stat.ML']",2016-09-18 18:19:02+00:00
http://arxiv.org/abs/1609.05486v3,Probabilistic Feature Selection and Classification Vector Machine,"Sparse Bayesian learning is a state-of-the-art supervised learning algorithm
that can choose a subset of relevant samples from the input data and make
reliable probabilistic predictions. However, in the presence of
high-dimensional data with irrelevant features, traditional sparse Bayesian
classifiers suffer from performance degradation and low efficiency by failing
to eliminate irrelevant features. To tackle this problem, we propose a novel
sparse Bayesian embedded feature selection method that adopts truncated
Gaussian distributions as both sample and feature priors. The proposed method,
called probabilistic feature selection and classification vector machine
(PFCVMLP ), is able to simultaneously select relevant features and samples for
classification tasks. In order to derive the analytical solutions, Laplace
approximation is applied to compute approximate posteriors and marginal
likelihoods. Finally, parameters and hyperparameters are optimized by the
type-II maximum likelihood method. Experiments on three datasets validate the
performance of PFCVMLP along two dimensions: classification performance and
effectiveness for feature selection. Finally, we analyze the generalization
performance and derive a generalization error bound for PFCVMLP . By tightening
the bound, the importance of feature selection is demonstrated.","['Bingbing Jiang', 'Chang Li', 'Maarten de Rijke', 'Xin Yao', 'Huanhuan Chen']","['cs.LG', 'stat.ML']",2016-09-18 14:01:04+00:00
http://arxiv.org/abs/1609.05388v1,ADAGIO: Fast Data-aware Near-Isometric Linear Embeddings,"Many important applications, including signal reconstruction, parameter
estimation, and signal processing in a compressed domain, rely on a
low-dimensional representation of the dataset that preserves {\em all} pairwise
distances between the data points and leverages the inherent geometric
structure that is typically present. Recently Hedge, Sankaranarayanan, Yin and
Baraniuk \cite{hedge2015} proposed the first data-aware near-isometric linear
embedding which achieves the best of both worlds. However, their method NuMax
does not scale to large-scale datasets.
  Our main contribution is a simple, data-aware, near-isometric linear
dimensionality reduction method which significantly outperforms a
state-of-the-art method \cite{hedge2015} with respect to scalability while
achieving high quality near-isometries. Furthermore, our method comes with
strong worst-case theoretical guarantees that allow us to guarantee the quality
of the obtained near-isometry. We verify experimentally the efficiency of our
method on numerous real-world datasets, where we find that our method ($<$10
secs) is more than 3\,000$\times$ faster than the state-of-the-art method
\cite{hedge2015} ($>$9 hours) on medium scale datasets with 60\,000 data points
in 784 dimensions. Finally, we use our method as a preprocessing step to
increase the computational efficiency of a classification application and for
speeding up approximate nearest neighbor queries.","['Jarosław Błasiok', 'Charalampos E. Tsourakakis']","['stat.ML', 'cs.LG']",2016-09-17 21:01:19+00:00
http://arxiv.org/abs/1609.05342v1,Fast and Effective Algorithms for Symmetric Nonnegative Matrix Factorization,"Symmetric Nonnegative Matrix Factorization (SNMF) models arise naturally as
simple reformulations of many standard clustering algorithms including the
popular spectral clustering method. Recent work has demonstrated that an
elementary instance of SNMF provides superior clustering quality compared to
many classic clustering algorithms on a variety of synthetic and real world
data sets. In this work, we present novel reformulations of this instance of
SNMF based on the notion of variable splitting and produce two fast and
effective algorithms for its optimization using i) the provably convergent
Accelerated Proximal Gradient (APG) procedure and ii) a heuristic version of
the Alternating Direction Method of Multipliers (ADMM) framework. Our two
algorithms present an interesting tradeoff between computational speed and
mathematical convergence guarantee: while the former method is provably
convergent it is considerably slower than the latter approach, for which we
also provide significant but less stringent mathematical proof regarding its
convergence. Through extensive experiments we show not only that the efficacy
of these approaches is equal to that of the state of the art SNMF algorithm,
but also that the latter of our algorithms is extremely fast being one to two
orders of magnitude faster in terms of total computation time than the state of
the art approach, outperforming even spectral clustering in terms of
computation time on large data sets.","['Reza Borhani', 'Jeremy Watt', 'Aggelos Katsaggelos']","['cs.CV', 'cs.LG', 'stat.ML']",2016-09-17 14:41:32+00:00
http://arxiv.org/abs/1609.05191v2,Gradient Descent Learns Linear Dynamical Systems,"We prove that stochastic gradient descent efficiently converges to the global
optimizer of the maximum likelihood objective of an unknown linear
time-invariant dynamical system from a sequence of noisy observations generated
by the system. Even though the objective function is non-convex, we provide
polynomial running time and sample complexity bounds under strong but natural
assumptions. Linear systems identification has been studied for many decades,
yet, to the best of our knowledge, these are the first polynomial guarantees
for the problem we consider.","['Moritz Hardt', 'Tengyu Ma', 'Benjamin Recht']","['cs.LG', 'cs.DS', 'math.OC', 'stat.ML']",2016-09-16 19:42:34+00:00
http://arxiv.org/abs/1609.05158v2,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,"Recently, several models based on deep neural networks have achieved great
success in terms of both reconstruction accuracy and computational performance
for single image super-resolution. In these methods, the low resolution (LR)
input image is upscaled to the high resolution (HR) space using a single
filter, commonly bicubic interpolation, before reconstruction. This means that
the super-resolution (SR) operation is performed in HR space. We demonstrate
that this is sub-optimal and adds computational complexity. In this paper, we
present the first convolutional neural network (CNN) capable of real-time SR of
1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN
architecture where the feature maps are extracted in the LR space. In addition,
we introduce an efficient sub-pixel convolution layer which learns an array of
upscaling filters to upscale the final LR feature maps into the HR output. By
doing so, we effectively replace the handcrafted bicubic filter in the SR
pipeline with more complex upscaling filters specifically trained for each
feature map, whilst also reducing the computational complexity of the overall
SR operation. We evaluate the proposed approach using images and videos from
publicly available datasets and show that it performs significantly better
(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster
than previous CNN-based methods.","['Wenzhe Shi', 'Jose Caballero', 'Ferenc Huszár', 'Johannes Totz', 'Andrew P. Aitken', 'Rob Bishop', 'Daniel Rueckert', 'Zehan Wang']","['cs.CV', 'stat.ML']",2016-09-16 17:58:14+00:00
http://arxiv.org/abs/1609.05148v8,Discovering and Deciphering Relationships Across Disparate Data Modalities,"Understanding the relationships between different properties of data, such as
whether a connectome or genome has information about disease status, is
becoming increasingly important in modern biological datasets. While existing
approaches can test whether two properties are related, they often require
unfeasibly large sample sizes in real data scenarios, and do not provide any
insight into how or why the procedure reached its decision. Our approach,
""Multiscale Graph Correlation"" (MGC), is a dependence test that juxtaposes
previously disparate data science techniques, including k-nearest neighbors,
kernel methods (such as support vector machines), and multiscale analysis (such
as wavelets). Other methods typically require double or triple the number
samples to achieve the same statistical power as MGC in a benchmark suite
including high-dimensional and nonlinear relationships - spanning polynomial
(linear, quadratic, cubic), trigonometric (sinusoidal, circular, ellipsoidal,
spiral), geometric (square, diamond, W-shape), and other functions, with
dimensionality ranging from 1 to 1000. Moreover, MGC uniquely provides a simple
and elegant characterization of the potentially complex latent geometry
underlying the relationship, providing insight while maintaining computational
efficiency. In several real data applications, including brain imaging and
cancer genetics, MGC is the only method that can both detect the presence of a
dependency and provide specific guidance for the next experiment and/or
analysis to conduct.","['Joshua T. Vogelstein', 'Eric Bridgeford', 'Qing Wang', 'Carey E. Priebe', 'Mauro Maggioni', 'Cencheng Shen']",['stat.ML'],2016-09-16 17:29:01+00:00
http://arxiv.org/abs/1609.05057v2,Unbiased Sparse Subspace Clustering By Selective Pursuit,"Sparse subspace clustering (SSC) is an elegant approach for unsupervised
segmentation if the data points of each cluster are located in linear
subspaces. This model applies, for instance, in motion segmentation if some
restrictions on the camera model hold. SSC requires that problems based on the
$l_1$-norm are solved to infer which points belong to the same subspace. If
these unknown subspaces are well-separated this algorithm is guaranteed to
succeed. The algorithm rests upon the assumption that points on the same
subspace are well spread. The question what happens if this condition is
violated has not yet been investigated. In this work, the effect of particular
distributions on the same subspace will be analyzed. It will be shown that SSC
fails to infer correct labels if points on the same subspace fall into more
than one cluster.","['Hanno Ackermann', 'Michael Ying Yang', 'Bodo Rosenhahn']",['stat.ML'],2016-09-16 13:59:01+00:00
http://arxiv.org/abs/1609.04849v5,Predicting Shot Making in Basketball Learnt from Adversarial Multiagent Trajectories,"In this paper, we predict the likelihood of a player making a shot in
basketball from multiagent trajectories. Previous approaches to similar
problems center on hand-crafting features to capture domain specific knowledge.
Although intuitive, recent work in deep learning has shown this approach is
prone to missing important predictive features. To circumvent this issue, we
present a convolutional neural network (CNN) approach where we initially
represent the multiagent behavior as an image. To encode the adversarial nature
of basketball, we use a multi-channel image which we then feed into a CNN.
Additionally, to capture the temporal aspect of the trajectories we ""fade"" the
player trajectories. We find that this approach is superior to a traditional
FFN model. By using gradient ascent to create images using an already trained
CNN, we discover what features the CNN filters learn. Last, we find that a
combined CNN+FFN is the best performing network with an error rate of 39%.","['Mark Harmon', 'Abdolghani Ebrahimi', 'Patrick Lucey', 'Diego Klabjan']","['stat.ML', 'cs.LG']",2016-09-15 20:27:26+00:00
http://arxiv.org/abs/1609.04802v5,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,"Despite the breakthroughs in accuracy and speed of single image
super-resolution using faster and deeper convolutional neural networks, one
central problem remains largely unsolved: how do we recover the finer texture
details when we super-resolve at large upscaling factors? The behavior of
optimization-based super-resolution methods is principally driven by the choice
of the objective function. Recent work has largely focused on minimizing the
mean squared reconstruction error. The resulting estimates have high peak
signal-to-noise ratios, but they are often lacking high-frequency details and
are perceptually unsatisfying in the sense that they fail to match the fidelity
expected at the higher resolution. In this paper, we present SRGAN, a
generative adversarial network (GAN) for image super-resolution (SR). To our
knowledge, it is the first framework capable of inferring photo-realistic
natural images for 4x upscaling factors. To achieve this, we propose a
perceptual loss function which consists of an adversarial loss and a content
loss. The adversarial loss pushes our solution to the natural image manifold
using a discriminator network that is trained to differentiate between the
super-resolved images and original photo-realistic images. In addition, we use
a content loss motivated by perceptual similarity instead of similarity in
pixel space. Our deep residual network is able to recover photo-realistic
textures from heavily downsampled images on public benchmarks. An extensive
mean-opinion-score (MOS) test shows hugely significant gains in perceptual
quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of
the original high-resolution images than to those obtained with any
state-of-the-art method.","['Christian Ledig', 'Lucas Theis', 'Ferenc Huszar', 'Jose Caballero', 'Andrew Cunningham', 'Alejandro Acosta', 'Andrew Aitken', 'Alykhan Tejani', 'Johannes Totz', 'Zehan Wang', 'Wenzhe Shi']","['cs.CV', 'stat.ML']",2016-09-15 19:53:07+00:00
http://arxiv.org/abs/1609.04789v3,"Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis","This paper presents a remarkably simple, yet powerful, algorithm termed
Coherence Pursuit (CoP) to robust Principal Component Analysis (PCA). As
inliers lie in a low dimensional subspace and are mostly correlated, an inlier
is likely to have strong mutual coherence with a large number of data points.
By contrast, outliers either do not admit low dimensional structures or form
small clusters. In either case, an outlier is unlikely to bear strong
resemblance to a large number of data points. Given that, CoP sets an outlier
apart from an inlier by comparing their coherence with the rest of the data
points. The mutual coherences are computed by forming the Gram matrix of the
normalized data points. Subsequently, the sought subspace is recovered from the
span of the subset of the data points that exhibit strong coherence with the
rest of the data. As CoP only involves one simple matrix multiplication, it is
significantly faster than the state-of-the-art robust PCA algorithms. We derive
analytical performance guarantees for CoP under different models for the
distributions of inliers and outliers in both noise-free and noisy settings.
CoP is the first robust PCA algorithm that is simultaneously non-iterative,
provably robust to both unstructured and structured outliers, and can tolerate
a large number of unstructured outliers.","['Mostafa Rahmani', 'George Atia']","['cs.LG', 'stat.ML']",2016-09-15 19:25:55+00:00
http://arxiv.org/abs/1609.04721v1,Mixture model modal clustering,"The two most extended density-based approaches to clustering are surely
mixture model clustering and modal clustering. In the mixture model approach,
the density is represented as a mixture and clusters are associated to the
different mixture components. In modal clustering, clusters are understood as
regions of high density separated from each other by zones of lower density, so
that they are closely related to certain regions around the density modes. If
the true density is indeed in the assumed class of mixture densities, then
mixture model clustering allows to scrutinize more subtle situations than modal
clustering. However, when mixture modeling is used in a nonparametric way,
taking advantage of the denseness of the sieve of mixture densities to
approximate any density, then the correspondence between clusters and mixture
components may become questionable. In this paper we introduce two methods to
adopt a modal clustering point of view after a mixture model fit. Numerous
examples are provided to illustrate that mixture modeling can also be used for
clustering in a nonparametric sense, as long as clusters are understood as the
domains of attraction of the density modes.",['José E. Chacón'],"['stat.ML', 'stat.CO']",2016-09-15 16:24:43+00:00
http://arxiv.org/abs/1609.04699v1,Learning Schizophrenia Imaging Genetics Data Via Multiple Kernel Canonical Correlation Analysis,"Kernel and Multiple Kernel Canonical Correlation Analysis (CCA) are employed
to classify schizophrenic and healthy patients based on their SNPs, DNA
Methylation and fMRI data. Kernel and Multiple Kernel CCA are popular methods
for finding nonlinear correlations between high-dimensional datasets. Data was
gathered from 183 patients, 79 with schizophrenia and 104 healthy controls.
Kernel and Multiple Kernel CCA represent new avenues for studying
schizophrenia, because, to our knowledge, these methods have not been used on
these data before. Classification is performed via k-means clustering on the
kernel matrix outputs of the Kernel and Multiple Kernel CCA algorithm.
Accuracies of the Kernel and Multiple Kernel CCA classification are compared to
that of the regularized linear CCA algorithm classification, and are found to
be significantly more accurate. Both algorithms demonstrate maximal accuracies
when the combination of DNA methylation and fMRI data are used, and experience
lower accuracies when the SNP data are incorporated.","['Owen Richfield', 'Md. Ashad Alam', 'Vince Calhoun', 'Yu-Ping Wang']","['q-bio.QM', 'stat.ML']",2016-09-15 15:26:37+00:00
http://arxiv.org/abs/1609.04608v2,Recursive nearest agglomeration (ReNA): fast clustering for approximation of structured signals,"In this work, we revisit fast dimension reduction approaches, as with random
projections and random sampling. Our goal is to summarize the data to decrease
computational costs and memory footprint of subsequent analysis. Such dimension
reduction can be very efficient when the signals of interest have a strong
structure, such as with images. We focus on this setting and investigate
feature clustering schemes for data reductions that capture this structure. An
impediment to fast dimension reduction is that good clustering comes with large
algorithmic costs. We address it by contributing a linear-time agglomerative
clustering scheme, Recursive Nearest Agglomeration (ReNA). Unlike existing fast
agglomerative schemes, it avoids the creation of giant clusters. We empirically
validate that it approximates the data as well as traditional
variance-minimizing clustering schemes that have a quadratic complexity. In
addition, we analyze signal approximation with feature clustering and show that
it can remove noise, improving subsequent analysis steps. As a consequence,
data reduction by clustering features with ReNA yields very fast and accurate
models, enabling to process large datasets on budget. Our theoretical analysis
is backed by extensive experiments on publicly-available data that illustrate
the computation efficiency and the denoising properties of the resulting
dimension reduction scheme.","['Andrés Hoyos-Idrobo', 'Gaël Varoquaux', 'Jonas Kahn', 'Bertrand Thirion']","['stat.ML', 'cs.LG']",2016-09-15 12:46:52+00:00
http://arxiv.org/abs/1609.06144v1,Multilevel Monte Carlo for Scalable Bayesian Computations,"Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in Bayesian
computations. However, they need to access the full data set in order to
evaluate the posterior density at every step of the algorithm. This results in
a great computational burden in big data applications. In contrast to MCMC
methods, Stochastic Gradient MCMC (SGMCMC) algorithms such as the Stochastic
Gradient Langevin Dynamics (SGLD) only require access to a batch of the data
set at every step. This drastically improves the computational performance and
scales well to large data sets. However, the difficulty with SGMCMC algorithms
comes from the sensitivity to its parameters which are notoriously difficult to
tune. Moreover, the Root Mean Square Error (RMSE) scales as
$\mathcal{O}(c^{-\frac{1}{3}})$ as opposed to standard MCMC
$\mathcal{O}(c^{-\frac{1}{2}})$ where $c$ is the computational cost.
  We introduce a new class of Multilevel Stochastic Gradient Markov chain Monte
Carlo algorithms that are able to mitigate the problem of tuning the step size
and more importantly of recovering the $\mathcal{O}(c^{-\frac{1}{2}})$
convergence of standard Markov Chain Monte Carlo methods without the need to
introduce Metropolis-Hasting steps. A further advantage of this new class of
algorithms is that it can easily be parallelised over a heterogeneous computer
architecture. We illustrate our methodology using Bayesian logistic regression
and provide numerical evidence that for a prescribed relative RMSE the
computational cost is sublinear in the number of data items.","['Mike Giles', 'Tigran Nagapetyan', 'Lukasz Szpruch', 'Sebastian Vollmer', 'Konstantinos Zygalakis']","['stat.ML', 'math.PR']",2016-09-15 10:36:36+00:00
http://arxiv.org/abs/1609.04541v1,Matrix Product State for Higher-Order Tensor Compression and Classification,"This paper introduces matrix product state (MPS) decomposition as a new and
systematic method to compress multidimensional data represented by higher-order
tensors. It solves two major bottlenecks in tensor compression: computation and
compression quality. Regardless of tensor order, MPS compresses tensors to
matrices of moderate dimension which can be used for classification. Mainly
based on a successive sequence of singular value decompositions (SVD), MPS is
quite simple to implement and arrives at the global optimal matrix, bypassing
local alternating optimization, which is not only computationally expensive but
cannot yield the global solution. Benchmark results show that MPS can achieve
better classification performance with favorable computation cost compared to
other tensor compression methods.","['Johann A. Bengua', 'Ho N. Phien', 'Hoang D. Tuan', 'Minh N. Do']","['stat.ML', 'cs.CV', 'cs.DS']",2016-09-15 09:04:25+00:00
http://arxiv.org/abs/1609.04523v3,STORE: Sparse Tensor Response Regression and Neuroimaging Analysis,"Motivated by applications in neuroimaging analysis, we propose a new
regression model, Sparse TensOr REsponse regression (STORE), with a tensor
response and a vector predictor. STORE embeds two key sparse structures:
element-wise sparsity and low-rankness. It can handle both a non-symmetric and
a symmetric tensor response, and thus is applicable to both structural and
functional neuroimaging data. We formulate the parameter estimation as a
non-convex optimization problem, and develop an efficient alternating updating
algorithm. We establish a non-asymptotic estimation error bound for the actual
estimator obtained from the proposed algorithm. This error bound reveals an
interesting interaction between the computational efficiency and the
statistical rate of convergence. When the distribution of the error tensor is
Gaussian, we further obtain a fast estimation error rate which allows the
tensor dimension to grow exponentially with the sample size. We illustrate the
efficacy of our model through intensive simulations and an analysis of the
Autism spectrum disorder neuroimaging data.","['Will Wei Sun', 'Lexin Li']","['stat.ML', 'stat.AP', 'stat.ME']",2016-09-15 06:51:51+00:00
http://arxiv.org/abs/1609.04522v2,Tensor Graphical Model: Non-convex Optimization and Statistical Inference,"We consider the estimation and inference of graphical models that
characterize the dependency structure of high-dimensional tensor-valued data.
To facilitate the estimation of the precision matrix corresponding to each way
of the tensor, we assume the data follow a tensor normal distribution whose
covariance has a Kronecker product structure. A critical challenge in the
estimation and inference of this model is the fact that its penalized maximum
likelihood estimation involves minimizing a non-convex objective function. To
address it, this paper makes two contributions: (i) In spite of the
non-convexity of this estimation problem, we prove that an alternating
minimization algorithm, which iteratively estimates each sparse precision
matrix while fixing the others, attains an estimator with an optimal
statistical rate of convergence. (ii) We propose a de-biased statistical
inference procedure for testing hypotheses on the true support of the sparse
precision matrices, and employ it for testing a growing number of hypothesis
with false discovery rate (FDR) control. The asymptotic normality of our test
statistic and the consistency of FDR control procedure are established. Our
theoretical results are backed up by thorough numerical studies and our real
applications on neuroimaging studies of Autism spectrum disorder and users'
advertising click analysis bring new scientific findings and business insights.
The proposed methods are encoded into a publicly available R package Tlasso.","['Xiang Lyu', 'Will Wei Sun', 'Zhaoran Wang', 'Han Liu', 'Jian Yang', 'Guang Cheng']","['stat.ML', 'stat.ME']",2016-09-15 06:41:11+00:00
http://arxiv.org/abs/1609.04508v2,Column Networks for Collective Classification,"Relational learning deals with data that are characterized by relational
structures. An important task is collective classification, which is to jointly
classify networked objects. While it holds a great promise to produce a better
accuracy than non-collective classifiers, collective classification is
computational challenging and has not leveraged on the recent breakthroughs of
deep learning. We present Column Network (CLN), a novel deep learning model for
collective classification in multi-relational domains. CLN has many desirable
theoretical properties: (i) it encodes multi-relations between any two
instances; (ii) it is deep and compact, allowing complex functions to be
approximated at the network level with a small set of free parameters; (iii)
local and relational features are learned simultaneously; (iv) long-range,
higher-order dependencies between instances are supported naturally; and (v)
crucially, learning and inference are efficient, linear in the size of the
network and the number of relations. We evaluate CLN on multiple real-world
applications: (a) delay prediction in software projects, (b) PubMed Diabetes
publication classification and (c) film genre classification. In all
applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.","['Trang Pham', 'Truyen Tran', 'Dinh Phung', 'Svetha Venkatesh']","['cs.LG', 'cs.AI', 'stat.ML']",2016-09-15 04:45:11+00:00
http://arxiv.org/abs/1609.04468v3,Sampling Generative Networks,"We introduce several techniques for sampling and visualizing the latent
spaces of generative models. Replacing linear interpolation with spherical
linear interpolation prevents diverging from a model's prior distribution and
produces sharper samples. J-Diagrams and MINE grids are introduced as
visualizations of manifolds created by analogies and nearest neighbors. We
demonstrate two new techniques for deriving attribute vectors: bias-corrected
vectors with data replication and synthetic vectors with data augmentation.
Binary classification using attribute vectors is presented as a technique
supporting quantitative analysis of the latent space. Most techniques are
intended to be independent of model type and examples are shown on both
Variational Autoencoders and Generative Adversarial Networks.",['Tom White'],"['cs.NE', 'cs.LG', 'stat.ML']",2016-09-14 22:42:23+00:00
http://arxiv.org/abs/1609.04436v1,Bayesian Reinforcement Learning: A Survey,"Bayesian methods for machine learning have been widely investigated, yielding
principled methods for incorporating prior information into inference
algorithms. In this survey, we provide an in-depth review of the role of
Bayesian methods for the reinforcement learning (RL) paradigm. The major
incentives for incorporating Bayesian reasoning in RL are: 1) it provides an
elegant approach to action-selection (exploration/exploitation) as a function
of the uncertainty in learning; and 2) it provides a machinery to incorporate
prior knowledge into the algorithms. We first discuss models and methods for
Bayesian inference in the simple single-step Bandit model. We then review the
extensive recent literature on Bayesian methods for model-based RL, where prior
information can be expressed on the parameters of the Markov model. We also
present Bayesian methods for model-free RL, where priors are expressed over the
value function or policy class. The objective of the paper is to provide a
comprehensive survey on Bayesian RL algorithms and their theoretical and
empirical properties.","['Mohammad Ghavamzadeh', 'Shie Mannor', 'Joelle Pineau', 'Aviv Tamar']","['cs.AI', 'cs.LG', 'stat.ML']",2016-09-14 20:34:26+00:00
http://arxiv.org/abs/1609.04388v1,Relativistic Monte Carlo,"Hamiltonian Monte Carlo (HMC) is a popular Markov chain Monte Carlo (MCMC)
algorithm that generates proposals for a Metropolis-Hastings algorithm by
simulating the dynamics of a Hamiltonian system. However, HMC is sensitive to
large time discretizations and performs poorly if there is a mismatch between
the spatial geometry of the target distribution and the scales of the momentum
distribution. In particular the mass matrix of HMC is hard to tune well. In
order to alleviate these problems we propose relativistic Hamiltonian Monte
Carlo, a version of HMC based on relativistic dynamics that introduce a maximum
velocity on particles. We also derive stochastic gradient versions of the
algorithm and show that the resulting algorithms bear interesting relationships
to gradient clipping, RMSprop, Adagrad and Adam, popular optimisation methods
in deep learning. Based on this, we develop relativistic stochastic gradient
descent by taking the zero-temperature limit of relativistic stochastic
gradient Hamiltonian Monte Carlo. In experiments we show that the relativistic
algorithms perform better than classical Newtonian variants and Adam.","['Xiaoyu Lu', 'Valerio Perrone', 'Leonard Hasenclever', 'Yee Whye Teh', 'Sebastian J. Vollmer']",['stat.ML'],2016-09-14 19:48:34+00:00
http://arxiv.org/abs/1609.04321v1,Very Simple Classifier: a Concept Binary Classifier toInvestigate Features Based on Subsampling and Localility,"We propose Very Simple Classifier (VSC) a novel method designed to
incorporate the concepts of subsampling and locality in the definition of
features to be used as the input of a perceptron. The rationale is that
locality theoretically guarantees a bound on the generalization error. Each
feature in VSC is a max-margin classifier built on randomly-selected pairs of
samples. The locality in VSC is achieved by multiplying the value of the
feature by a confidence measure that can be characterized in terms of the
Chebichev inequality. The output of the layer is then fed in a output layer of
neurons. The weights of the output layer are then determined by a regularized
pseudoinverse. Extensive comparison of VSC against 9 competitors in the task of
binary classification is carried out. Results on 22 benchmark datasets with
fixed parameters show that VSC is competitive with the Multi Layer Perceptron
(MLP) and outperforms the other competitors. An exploration of the parameter
space shows VSC can outperform MLP.","['Luca Masera', 'Enrico Blanzieri']","['cs.LG', 'stat.ML']",2016-09-14 15:51:46+00:00
http://arxiv.org/abs/1609.04301v3,TristouNet: Triplet Loss for Speaker Turn Embedding,"TristouNet is a neural network architecture based on Long Short-Term Memory
recurrent networks, meant to project speech sequences into a fixed-dimensional
euclidean space. Thanks to the triplet loss paradigm used for training, the
resulting sequence embeddings can be compared directly with the euclidean
distance, for speaker comparison purposes. Experiments on short (between 500ms
and 5s) speech turn comparison and speaker change detection show that
TristouNet brings significant improvements over the current state-of-the-art
techniques for both tasks.",['Hervé Bredin'],"['cs.SD', 'stat.ML']",2016-09-14 14:39:36+00:00
http://arxiv.org/abs/1609.04289v1,Gray-box inference for structured Gaussian process models,"We develop an automated variational inference method for Bayesian structured
prediction problems with Gaussian process (GP) priors and linear-chain
likelihoods. Our approach does not need to know the details of the structured
likelihood model and can scale up to a large number of observations.
Furthermore, we show that the required expected likelihood term and its
gradients in the variational objective (ELBO) can be estimated efficiently by
using expectations over very low-dimensional Gaussian distributions.
Optimization of the ELBO is fully parallelizable over sequences and amenable to
stochastic optimization, which we use along with control variate techniques and
state-of-the-art incremental optimization to make our framework useful in
practice. Results on a set of natural language processing tasks show that our
method can be as good as (and sometimes better than) hard-coded approaches
including SVM-struct and CRFs, and overcomes the scalability limitations of
previous inference algorithms based on sampling. Overall, this is a fundamental
step to developing automated inference methods for Bayesian structured
prediction.","['Pietro Galliani', 'Amir Dezfouli', 'Edwin V. Bonilla', 'Novi Quadrianto']",['stat.ML'],2016-09-14 14:27:32+00:00
http://arxiv.org/abs/1609.04228v2,Stochastic Heavy Ball,"This paper deals with a natural stochastic optimization procedure derived
from the so-called Heavy-ball method differential equation, which was
introduced by Polyak in the 1960s with his seminal contribution [Pol64]. The
Heavy-ball method is a second-order dynamics that was investigated to minimize
convex functions f . The family of second-order methods recently received a
large amount of attention, until the famous contribution of Nesterov [Nes83],
leading to the explosion of large-scale optimization problems. This work
provides an in-depth description of the stochastic heavy-ball method, which is
an adaptation of the deterministic one when only unbiased evalutions of the
gradient are available and used throughout the iterations of the algorithm. We
first describe some almost sure convergence results in the case of general
non-convex coercive functions f . We then examine the situation of convex and
strongly convex potentials and derive some non-asymptotic results about the
stochastic heavy-ball method. We end our study with limit theorems on several
rescaled algorithms.","['Sébastien Gadat', 'Fabien Panloup', 'Sofiane Saadane']","['math.ST', 'math.PR', 'stat.ML', 'stat.TH']",2016-09-14 11:54:04+00:00
http://arxiv.org/abs/1609.04623v1,Distributed Estimation of the Operating State of a Single-Bus DC MicroGrid without an External Communication Interface,"We propose a decentralized Maximum Likelihood solution for estimating the
stochastic renewable power generation and demand in single bus Direct Current
(DC) MicroGrids (MGs), with high penetration of droop controlled power
electronic converters. The solution relies on the fact that the primary control
parameters are set in accordance with the local power generation status of the
generators. Therefore, the steady state voltage is inherently dependent on the
generation capacities and the load, through a non-linear parametric model,
which can be estimated. To have a well conditioned estimation problem, our
solution avoids the use of an external communication interface and utilizes
controlled voltage disturbances to perform distributed training. Using this
tool, we develop an efficient, decentralized Maximum Likelihood Estimator (MLE)
and formulate the sufficient condition for the existence of the globally
optimal solution. The numerical results illustrate the promising performance of
our MLE algorithm.","['Marko Angjelichinoski', 'Anna Scaglione', 'Petar Popovski', 'Cedomir Stefanovic']","['stat.ML', 'cs.SY']",2016-09-14 11:22:32+00:00
http://arxiv.org/abs/1609.04120v3,Private Topic Modeling,"We develop a privatised stochastic variational inference method for Latent
Dirichlet Allocation (LDA). The iterative nature of stochastic variational
inference presents challenges: multiple iterations are required to obtain
accurate posterior distributions, yet each iteration increases the amount of
noise that must be added to achieve a reasonable degree of privacy. We propose
a practical algorithm that overcomes this challenge by combining: (1) an
improved composition method for differential privacy, called the moments
accountant, which provides a tight bound on the privacy cost of multiple
variational inference iterations and thus significantly decreases the amount of
additive noise; and (2) privacy amplification resulting from subsampling of
large-scale data. Focusing on conjugate exponential family models, in our
private variational inference, all the posterior distributions will be
privatised by simply perturbing expected sufficient statistics. Using Wikipedia
data, we illustrate the effectiveness of our algorithm for large-scale data.","['Mijung Park', 'James Foulds', 'Kamalika Chaudhuri', 'Max Welling']","['stat.ML', 'cs.CR']",2016-09-14 03:18:36+00:00
http://arxiv.org/abs/1609.03960v1,Self-Sustaining Iterated Learning,"An important result from psycholinguistics (Griffiths & Kalish, 2005) states
that no language can be learned iteratively by rational agents in a
self-sustaining manner. We show how to modify the learning process slightly in
order to achieve self-sustainability. Our work is in two parts. First, we
characterize iterated learnability in geometric terms and show how a slight,
steady increase in the lengths of the training sessions ensures
self-sustainability for any discrete language class. In the second part, we
tackle the nondiscrete case and investigate self-sustainability for iterated
linear regression. We discuss the implications of our findings to issues of
non-equilibrium dynamics in natural algorithms.","['Bernard Chazelle', 'Chu Wang']","['math.OC', 'cs.LG', 'stat.ML']",2016-09-13 18:18:38+00:00
http://arxiv.org/abs/1609.03958v1,Noisy Inductive Matrix Completion Under Sparse Factor Models,"Inductive Matrix Completion (IMC) is an important class of matrix completion
problems that allows direct inclusion of available features to enhance
estimation capabilities. These models have found applications in personalized
recommendation systems, multilabel learning, dictionary learning, etc. This
paper examines a general class of noisy matrix completion tasks where the
underlying matrix is following an IMC model i.e., it is formed by a mixing
matrix (a priori unknown) sandwiched between two known feature matrices. The
mixing matrix here is assumed to be well approximated by the product of two
sparse matrices---referred here to as ""sparse factor models."" We leverage the
main theorem of Soni:2016:NMC and extend it to provide theoretical error bounds
for the sparsity-regularized maximum likelihood estimators for the class of
problems discussed in this paper. The main result is general in the sense that
it can be used to derive error bounds for various noise models. In this paper,
we instantiate our main result for the case of Gaussian noise and provide
corresponding error bounds in terms of squared loss.","['Akshay Soni', 'Troy Chevalier', 'Swayambhoo Jain']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2016-09-13 18:08:06+00:00
http://arxiv.org/abs/1609.03932v1,Mapping the Similarities of Spectra: Global and Locally-biased Approaches to SDSS Galaxy Data,"We apply a novel spectral graph technique, that of locally-biased
semi-supervised eigenvectors, to study the diversity of galaxies. This
technique permits us to characterize empirically the natural variations in
observed spectra data, and we illustrate how this approach can be used in an
exploratory manner to highlight both large-scale global as well as small-scale
local structure in Sloan Digital Sky Survey (SDSS) data. We use this method in
a way that simultaneously takes into account the measurements of spectral lines
as well as the continuum shape. Unlike Principal Component Analysis, this
method does not assume that the Euclidean distance between galaxy spectra is a
good global measure of similarity between all spectra, but instead it only
assumes that local difference information between similar spectra is reliable.
Moreover, unlike other nonlinear dimensionality methods, this method can be
used to characterize very finely both small-scale local as well as large-scale
global properties of realistic noisy data. The power of the method is
demonstrated on the SDSS Main Galaxy Sample by illustrating that the derived
embeddings of spectra carry an unprecedented amount of information. By using a
straightforward global or unsupervised variant, we observe that the main
features correlate strongly with star formation rate and that they clearly
separate active galactic nuclei. Computed parameters of the method can be used
to describe line strengths and their interdependencies. By using a
locally-biased or semi-supervised variant, we are able to focus on typical
variations around specific objects of astronomical interest. We present several
examples illustrating that this approach can enable new discoveries in the data
as well as a detailed understanding of very fine local structure that would
otherwise be overwhelmed by large-scale noise and global trends in the data.","['David Lawlor', 'Tamás Budavári', 'Michael W. Mahoney']","['astro-ph.IM', 'astro-ph.CO', 'cs.DS', 'stat.ML']",2016-09-13 16:46:51+00:00
http://arxiv.org/abs/1609.03912v1,Information Theoretic Structure Learning with Confidence,"Information theoretic measures (e.g. the Kullback Liebler divergence and
Shannon mutual information) have been used for exploring possibly nonlinear
multivariate dependencies in high dimension. If these dependencies are assumed
to follow a Markov factor graph model, this exploration process is called
structure discovery. For discrete-valued samples, estimates of the information
divergence over the parametric class of multinomial models lead to structure
discovery methods whose mean squared error achieves parametric convergence
rates as the sample size grows. However, a naive application of this method to
continuous nonparametric multivariate models converges much more slowly. In
this paper we introduce a new method for nonparametric structure discovery that
uses weighted ensemble divergence estimators that achieve parametric
convergence rates and obey an asymptotic central limit theorem that facilitates
hypothesis testing and other types of statistical validation.","['Kevin R. Moon', 'Morteza Noshad', 'Salimeh Yasaei Sekeh', 'Alfred O. Hero III']","['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2016-09-13 16:20:02+00:00
http://arxiv.org/abs/1609.03772v1,Learning conditional independence structure for high-dimensional uncorrelated vector processes,"We formulate and analyze a graphical model selection method for inferring the
conditional independence graph of a high-dimensional nonstationary Gaussian
random process (time series) from a finite-length observation. The observed
process samples are assumed uncorrelated over time and having a time-varying
marginal distribution. The selection method is based on testing conditional
variances obtained for small subsets of process components. This allows to cope
with the high-dimensional regime, where the sample size can be (drastically)
smaller than the process dimension. We characterize the required sample size
such that the proposed selection method is successful with high probability.","['Nguyen Tran Quang', 'Alexander Jung']","['stat.ML', 'cs.LG']",2016-09-13 11:35:12+00:00
http://arxiv.org/abs/1609.03769v1,Analysis of Kelner and Levin graph sparsification algorithm for a streaming setting,"We derive a new proof to show that the incremental resparsification algorithm
proposed by Kelner and Levin (2013) produces a spectral sparsifier in high
probability. We rigorously take into account the dependencies across subsequent
resparsifications using martingale inequalities, fixing a flaw in the original
analysis.","['Daniele Calandriello', 'Alessandro Lazaric', 'Michal Valko']","['stat.ML', 'cs.DS', 'cs.LG']",2016-09-13 11:18:03+00:00
http://arxiv.org/abs/1609.03683v2,Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach,"We present a theoretically grounded approach to train deep neural networks,
including recurrent networks, subject to class-dependent label noise. We
propose two procedures for loss correction that are agnostic to both
application domain and network architecture. They simply amount to at most a
matrix inversion and multiplication, provided that we know the probability of
each class being corrupted into another. We further show how one can estimate
these probabilities, adapting a recent technique for noise estimation to the
multi-class setting, and thus providing an end-to-end framework. Extensive
experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of
clothing images employing a diversity of architectures --- stacking dense,
convolutional, pooling, dropout, batch normalization, word embedding, LSTM and
residual layers --- demonstrate the noise robustness of our proposals.
Incidentally, we also prove that, when ReLU is the only non-linearity, the loss
curvature is immune to class-dependent label noise.","['Giorgio Patrini', 'Alessandro Rozza', 'Aditya Menon', 'Richard Nock', 'Lizhen Qu']","['stat.ML', 'cs.LG']",2016-09-13 05:23:29+00:00
http://arxiv.org/abs/1609.03677v3,Unsupervised Monocular Depth Estimation with Left-Right Consistency,"Learning based methods have shown very promising results for the task of
depth estimation in single images. However, most existing approaches treat
depth prediction as a supervised regression problem and as a result, require
vast quantities of corresponding ground truth depth data for training. Just
recording quality depth data in a range of environments is a challenging
problem. In this paper, we innovate beyond existing approaches, replacing the
use of explicit depth data during training with easier-to-obtain binocular
stereo footage.
  We propose a novel training objective that enables our convolutional neural
network to learn to perform single image depth estimation, despite the absence
of ground truth depth data. Exploiting epipolar geometry constraints, we
generate disparity images by training our network with an image reconstruction
loss. We show that solving for image reconstruction alone results in poor
quality depth images. To overcome this problem, we propose a novel training
loss that enforces consistency between the disparities produced relative to
both the left and right images, leading to improved performance and robustness
compared to existing approaches. Our method produces state of the art results
for monocular depth estimation on the KITTI driving dataset, even outperforming
supervised methods that have been trained with ground truth depth.","['Clément Godard', 'Oisin Mac Aodha', 'Gabriel J. Brostow']","['cs.CV', 'cs.LG', 'stat.ML']",2016-09-13 04:48:31+00:00
http://arxiv.org/abs/1609.03666v1,A Greedy Algorithm to Cluster Specialists,"Several recent deep neural networks experiments leverage the
generalist-specialist paradigm for classification. However, no formal study
compared the performance of different clustering algorithms for class
assignment. In this paper we perform such a study, suggest slight modifications
to the clustering procedures, and propose a novel algorithm designed to
optimize the performance of of the specialist-generalist classification system.
Our experiments on the CIFAR-10 and CIFAR-100 datasets allow us to investigate
situations for varying number of classes on similar data. We find that our
\emph{greedy pairs} clustering algorithm consistently outperforms other
alternatives, while the choice of the confusion matrix has little impact on the
final performance.",['Sébastien Arnold'],"['cs.LG', 'stat.ML']",2016-09-13 03:26:42+00:00
http://arxiv.org/abs/1609.03544v1,Online Data Thinning via Multi-Subspace Tracking,"In an era of ubiquitous large-scale streaming data, the availability of data
far exceeds the capacity of expert human analysts. In many settings, such data
is either discarded or stored unprocessed in datacenters. This paper proposes a
method of online data thinning, in which large-scale streaming datasets are
winnowed to preserve unique, anomalous, or salient elements for timely expert
analysis. At the heart of this proposed approach is an online anomaly detection
method based on dynamic, low-rank Gaussian mixture models. Specifically, the
high-dimensional covariances matrices associated with the Gaussian components
are associated with low-rank models. According to this model, most observations
lie near a union of subspaces. The low-rank modeling mitigates the curse of
dimensionality associated with anomaly detection for high-dimensional data, and
recent advances in subspace clustering and subspace tracking allow the proposed
method to adapt to dynamic environments. Furthermore, the proposed method
allows subsampling, is robust to missing data, and uses a mini-batch online
optimization approach. The resulting algorithms are scalable, efficient, and
are capable of operating in real time. Experiments on wide-area motion imagery
and e-mail databases illustrate the efficacy of the proposed approach.","['Xin Jiang', 'Rebecca Willett']","['stat.ML', 'cs.LG']",2016-09-12 19:34:02+00:00
http://arxiv.org/abs/1609.03541v1,"Comment on ""Why does deep and cheap learning work so well?"" [arXiv:1608.08225]","In a recent paper, ""Why does deep and cheap learning work so well?"", Lin and
Tegmark claim to show that the mapping between deep belief networks and the
variational renormalization group derived in [arXiv:1410.3831] is invalid, and
present a ""counterexample"" that claims to show that this mapping does not hold.
In this comment, we show that these claims are incorrect and stem from a
misunderstanding of the variational RG procedure proposed by Kadanoff. We also
explain why the ""counterexample"" of Lin and Tegmark is compatible with the
mapping proposed in [arXiv:1410.3831].","['David J. Schwab', 'Pankaj Mehta']","['cond-mat.dis-nn', 'cs.LG', 'stat.ML']",2016-09-12 19:25:27+00:00
http://arxiv.org/abs/1609.03519v1,Optimal Encoding and Decoding for Point Process Observations: an Approximate Closed-Form Filter,"The process of dynamic state estimation (filtering) based on point process
observations is in general intractable. Numerical sampling techniques are often
practically useful, but lead to limited conceptual insight about optimal
encoding/decoding strategies, which are of significant relevance to
Computational Neuroscience. We develop an analytically tractable Bayesian
approximation to optimal filtering based on point process observations, which
allows us to introduce distributional assumptions about sensor properties, that
greatly facilitate the analysis of optimal encoding in situations deviating
from common assumptions of uniform coding. Numerical comparison with particle
filtering demonstrate the quality of the approximation. The analytic framework
leads to insights which are difficult to obtain from numerical algorithms, and
is consistent with biological observations about the distribution of sensory
cells' tuning curve centers.","['Yuval Harel', 'Ron Meir', 'Manfred Opper']","['stat.ML', 'q-bio.NC']",2016-09-12 18:33:50+00:00
http://arxiv.org/abs/1610.08495v1,Adaptive matching pursuit for sparse signal recovery,"Spike and Slab priors have been of much recent interest in signal processing
as a means of inducing sparsity in Bayesian inference. Applications domains
that benefit from the use of these priors include sparse recovery, regression
and classification. It is well-known that solving for the sparse coefficient
vector to maximize these priors results in a hard non-convex and mixed integer
programming problem. Most existing solutions to this optimization problem
either involve simplifying assumptions/relaxations or are computationally
expensive. We propose a new greedy and adaptive matching pursuit (AMP)
algorithm to directly solve this hard problem. Essentially, in each step of the
algorithm, the set of active elements would be updated by either adding or
removing one index, whichever results in better improvement. In addition, the
intermediate steps of the algorithm are calculated via an inexpensive Cholesky
decomposition which makes the algorithm much faster. Results on simulated data
sets as well as real-world image recovery challenges confirm the benefits of
the proposed AMP, particularly in providing a superior cost-quality trade-off
over existing alternatives.","['Tiep H. Vu', 'Hojjat S. Mousavi', 'Vishal Monga']","['cs.LG', 'stat.ML']",2016-09-12 17:48:38+00:00
http://arxiv.org/abs/1609.03344v2,Finite-sample and asymptotic analysis of generalization ability with an application to penalized regression,"In this paper, we study the performance of extremum estimators from the
perspective of generalization ability (GA): the ability of a model to predict
outcomes in new samples from the same population. By adapting the classical
concentration inequalities, we derive upper bounds on the empirical
out-of-sample prediction errors as a function of the in-sample errors,
in-sample data size, heaviness in the tails of the error distribution, and
model complexity. We show that the error bounds may be used for tuning key
estimation hyper-parameters, such as the number of folds $K$ in
cross-validation. We also show how $K$ affects the bias-variance trade-off for
cross-validation. We demonstrate that the $\mathcal{L}_2$-norm difference
between penalized and the corresponding un-penalized regression estimates is
directly explained by the GA of the estimates and the GA of empirical moment
conditions. Lastly, we prove that all penalized regression estimates are
$L_2$-consistent for both the $n \geqslant p$ and the $n < p$ cases.
Simulations are used to demonstrate key results.
  Keywords: generalization ability, upper bound of generalization error,
penalized regression, cross-validation, bias-variance trade-off,
$\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso,
high-dimensional data.","['Ning Xu', 'Jian Hong', 'Timothy C. G. Fisher']","['stat.ML', 'cs.LG', 'math.ST', 'q-fin.EC', 'stat.CO', 'stat.TH']",2016-09-12 11:09:50+00:00
http://arxiv.org/abs/1609.03333v1,On Generation of Time-based Label Refinements,"Process mining is a research field focused on the analysis of event data with
the aim of extracting insights in processes. Applying process mining techniques
on data from smart home environments has the potential to provide valuable
insights in (un)healthy habits and to contribute to ambient assisted living
solutions. Finding the right event labels to enable application of process
mining techniques is however far from trivial, as simply using the triggering
sensor as the label for sensor events results in uninformative models that
allow for too much behavior (overgeneralizing). Refinements of sensor level
event labels suggested by domain experts have shown to enable discovery of more
precise and insightful process models. However, there exist no automated
approach to generate refinements of event labels in the context of process
mining. In this paper we propose a framework for automated generation of label
refinements based on the time attribute of events. We show on a case study with
real life smart home event data that behaviorally more specific, and therefore
more insightful, process models can be found by using automatically generated
refined labels in process discovery.","['Niek Tax', 'Emin Alasgarov', 'Natalia Sidorova', 'Reinder Haakma']","['stat.ME', 'cs.AI', 'stat.ML']",2016-09-12 10:25:29+00:00
http://arxiv.org/abs/1609.03319v2,"CompAdaGrad: A Compressed, Complementary, Computationally-Efficient Adaptive Gradient Method","The adaptive gradient online learning method known as AdaGrad has seen
widespread use in the machine learning community in stochastic and adversarial
online learning problems and more recently in deep learning methods. The
method's full-matrix incarnation offers much better theoretical guarantees and
potentially better empirical performance than its diagonal version; however,
this version is computationally prohibitive and so the simpler diagonal version
often is used in practice. We introduce a new method, CompAdaGrad, that
navigates the space between these two schemes and show that this method can
yield results much better than diagonal AdaGrad while avoiding the (effectively
intractable) $O(n^3)$ computational complexity of full-matrix AdaGrad for
dimension $n$. CompAdaGrad essentially performs full-matrix regularization in a
low-dimensional subspace while performing diagonal regularization in the
complementary subspace. We derive CompAdaGrad's updates for composite mirror
descent in case of the squared $\ell_2$ norm and the $\ell_1$ norm, demonstrate
that its complexity per iteration is linear in the dimension, and establish
guarantees for the method independent of the choice of composite regularizer.
Finally, we show preliminary results on several datasets.","['Nishant A. Mehta', 'Alistair Rendell', 'Anish Varghese', 'Christfried Webers']","['cs.LG', 'stat.ML']",2016-09-12 09:06:44+00:00
http://arxiv.org/abs/1609.03261v3,Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method,"We develop and analyze a procedure for gradient-based optimization that we
refer to as stochastically controlled stochastic gradient (SCSG). As a member
of the SVRG family of algorithms, SCSG makes use of gradient estimates at two
scales, with the number of updates at the faster scale being governed by a
geometric random variable. Unlike most existing algorithms in this family, both
the computation cost and the communication cost of SCSG do not necessarily
scale linearly with the sample size $n$; indeed, these costs are independent of
$n$ when the target accuracy is low. An experimental evaluation on real
datasets confirms the effectiveness of SCSG.","['Lihua Lei', 'Michael I. Jordan']","['math.OC', 'cs.DS', 'cs.LG', 'stat.ML']",2016-09-12 03:35:29+00:00
http://arxiv.org/abs/1609.03240v2,Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach,"We consider the non-square matrix sensing problem, under restricted isometry
property (RIP) assumptions. We focus on the non-convex formulation, where any
rank-$r$ matrix $X \in \mathbb{R}^{m \times n}$ is represented as $UV^\top$,
where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n \times r}$. In
this paper, we complement recent findings on the non-convex geometry of the
analogous PSD setting [5], and show that matrix factorization does not
introduce any spurious local minima, under RIP.","['Dohyung Park', 'Anastasios Kyrillidis', 'Constantine Caramanis', 'Sujay Sanghavi']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.NA', 'math.OC']",2016-09-12 01:20:45+00:00
http://arxiv.org/abs/1609.03228v2,Supervised multiway factorization,"We describe a probabilistic PARAFAC/CANDECOMP (CP) factorization for multiway
(i.e., tensor) data that incorporates auxiliary covariates, SupCP. SupCP
generalizes the supervised singular value decomposition (SupSVD) for
vector-valued observations, to allow for observations that have the form of a
matrix or higher-order array. Such data are increasingly encountered in
biomedical research and other fields. We describe a likelihood-based latent
variable representation of the CP factorization, in which the latent variables
are informed by additional covariates. We give conditions for identifiability,
and develop an EM algorithm for simultaneous estimation of all model
parameters. SupCP can be used for dimension reduction, capturing latent
structures that are more accurate and interpretable due to covariate
supervision. Moreover, SupCP specifies a full probability distribution for a
multiway data observation with given covariate values, which can be used for
predictive modeling. We conduct comprehensive simulations to evaluate the SupCP
algorithm. We apply it to a facial image database with facial descriptors
(e.g., smiling / not smiling) as covariates, and to a study of amino acid
fluorescence. Software is available at https://github.com/lockEF/SupCP .","['Eric F. Lock', 'Gen Li']","['stat.ME', 'stat.ML']",2016-09-11 23:12:54+00:00
http://arxiv.org/abs/1609.03219v3,Sharing Hash Codes for Multiple Purposes,"Locality sensitive hashing (LSH) is a powerful tool for sublinear-time
approximate nearest neighbor search, and a variety of hashing schemes have been
proposed for different dissimilarity measures. However, hash codes
significantly depend on the dissimilarity, which prohibits users from adjusting
the dissimilarity at query time. In this paper, we propose {multiple purpose
LSH (mp-LSH) which shares the hash codes for different dissimilarities. mp-LSH
supports L2, cosine, and inner product dissimilarities, and their corresponding
weighted sums, where the weights can be adjusted at query time. It also allows
us to modify the importance of pre-defined groups of features. Thus, mp-LSH
enables us, for example, to retrieve similar items to a query with the user
preference taken into account, to find a similar material to a query with some
properties (stability, utility, etc.) optimized, and to turn on or off a part
of multi-modal information (brightness, color, audio, text, etc.) in
image/video retrieval. We theoretically and empirically analyze the performance
of three variants of mp-LSH, and demonstrate their usefulness on real-world
data sets.","['Wikor Pronobis', 'Danny Panknin', 'Johannes Kirschnick', 'Vignesh Srinivasan', 'Wojciech Samek', 'Volker Markl', 'Manohar Kaul', 'Klaus-Robert Mueller', 'Shinichi Nakajima']","['stat.ML', 'cs.LG']",2016-09-11 21:55:07+00:00
http://arxiv.org/abs/1609.03164v1,On the Relationship between Online Gaussian Process Regression and Kernel Least Mean Squares Algorithms,"We study the relationship between online Gaussian process (GP) regression and
kernel least mean squares (KLMS) algorithms. While the latter have no capacity
of storing the entire posterior distribution during online learning, we
discover that their operation corresponds to the assumption of a fixed
posterior covariance that follows a simple parametric model. Interestingly,
several well-known KLMS algorithms correspond to specific cases of this model.
The probabilistic perspective allows us to understand how each of them handles
uncertainty, which could explain some of their performance differences.","['Steven Van Vaerenbergh', 'Jesus Fernandez-Bes', 'Víctor Elvira']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2016-09-11 14:17:33+00:00
http://arxiv.org/abs/1609.03126v4,Energy-based Generative Adversarial Network,"We introduce the ""Energy-based Generative Adversarial Network"" model (EBGAN)
which views the discriminator as an energy function that attributes low
energies to the regions near the data manifold and higher energies to other
regions. Similar to the probabilistic GANs, a generator is seen as being
trained to produce contrastive samples with minimal energies, while the
discriminator is trained to assign high energies to these generated samples.
Viewing the discriminator as an energy function allows to use a wide variety of
architectures and loss functionals in addition to the usual binary classifier
with logistic output. Among them, we show one instantiation of EBGAN framework
as using an auto-encoder architecture, with the energy being the reconstruction
error, in place of the discriminator. We show that this form of EBGAN exhibits
more stable behavior than regular GANs during training. We also show that a
single-scale architecture can be trained to generate high-resolution images.","['Junbo Zhao', 'Michael Mathieu', 'Yann LeCun']","['cs.LG', 'stat.ML']",2016-09-11 07:11:13+00:00
http://arxiv.org/abs/1609.02997v2,Iteratively Reweighted Least Squares Algorithms for L1-Norm Principal Component Analysis,"Principal component analysis (PCA) is often used to reduce the dimension of
data by selecting a few orthonormal vectors that explain most of the variance
structure of the data. L1 PCA uses the L1 norm to measure error, whereas the
conventional PCA uses the L2 norm. For the L1 PCA problem minimizing the
fitting error of the reconstructed data, we propose an exact reweighted and an
approximate algorithm based on iteratively reweighted least squares. We provide
convergence analyses, and compare their performance against benchmark
algorithms in the literature. The computational experiment shows that the
proposed algorithms consistently perform best.","['Young Woong Park', 'Diego Klabjan']","['stat.ML', 'cs.LG']",2016-09-10 04:06:07+00:00
http://arxiv.org/abs/1609.02943v2,Stealing Machine Learning Models via Prediction APIs,"Machine learning (ML) models may be deemed confidential due to their
sensitive training data, commercial value, or use in security applications.
Increasingly often, confidential ML models are being deployed with publicly
accessible query interfaces. ML-as-a-service (""predictive analytics"") systems
are an example: Some allow users to train models on potentially sensitive data
and charge others for access on a pay-per-query basis.
  The tension between model confidentiality and public access motivates our
investigation of model extraction attacks. In such attacks, an adversary with
black-box access, but no prior knowledge of an ML model's parameters or
training data, aims to duplicate the functionality of (i.e., ""steal"") the
model. Unlike in classical learning theory settings, ML-as-a-service offerings
may accept partial feature vectors as inputs and include confidence values with
predictions. Given these practices, we show simple, efficient attacks that
extract target ML models with near-perfect fidelity for popular model classes
including logistic regression, neural networks, and decision trees. We
demonstrate these attacks against the online services of BigML and Amazon
Machine Learning. We further show that the natural countermeasure of omitting
confidence values from model outputs still admits potentially harmful model
extraction attacks. Our results highlight the need for careful ML model
deployment and new model extraction countermeasures.","['Florian Tramèr', 'Fan Zhang', 'Ari Juels', 'Michael K. Reiter', 'Thomas Ristenpart']","['cs.CR', 'cs.LG', 'stat.ML']",2016-09-09 20:39:20+00:00
http://arxiv.org/abs/1609.02938v1,Extract fetal ECG from single-lead abdominal ECG by de-shape short time Fourier transform and nonlocal median,"The multiple fundamental frequency detection problem and the source
separation problem from a single-channel signal containing multiple oscillatory
components and a nonstationary noise are both challenging tasks. To extract the
fetal electrocardiogram (ECG) from a single-lead maternal abdominal ECG, we
face both challenges. In this paper, we propose a novel method to extract the
fetal ECG signal from the single channel maternal abdominal ECG signal, without
any additional measurement. The algorithm is composed of three main
ingredients. First, the maternal and fetal heart rates are estimated by the
de-shape short time Fourier transform, which is a recently proposed nonlinear
time-frequency analysis technique; second, the beat tracking technique is
applied to accurately obtain the maternal and fetal R peaks; third, the
maternal and fetal ECG waveforms are established by the nonlocal median. The
algorithm is evaluated on a simulated fetal ECG signal database ({\em fecgsyn}
database), and tested on two real databases with the annotation provided by
experts ({\em adfecgdb} database and {\em CinC2013} database). In general, the
algorithm could be applied to solve other detection and source separation
problems, and reconstruct the time-varying wave-shape function of each
oscillatory component.","['Su Li', 'Hau-tieng Wu']","['q-bio.QM', 'physics.data-an', 'stat.AP', 'stat.ME', 'stat.ML']",2016-09-09 20:26:31+00:00
http://arxiv.org/abs/1609.02907v4,Semi-Supervised Classification with Graph Convolutional Networks,"We present a scalable approach for semi-supervised learning on
graph-structured data that is based on an efficient variant of convolutional
neural networks which operate directly on graphs. We motivate the choice of our
convolutional architecture via a localized first-order approximation of
spectral graph convolutions. Our model scales linearly in the number of graph
edges and learns hidden layer representations that encode both local graph
structure and features of nodes. In a number of experiments on citation
networks and on a knowledge graph dataset we demonstrate that our approach
outperforms related methods by a significant margin.","['Thomas N. Kipf', 'Max Welling']","['cs.LG', 'stat.ML']",2016-09-09 19:48:41+00:00
http://arxiv.org/abs/1609.02906v1,Robust Spectral Detection of Global Structures in the Data by Learning a Regularization,"Spectral methods are popular in detecting global structures in the given data
that can be represented as a matrix. However when the data matrix is sparse or
noisy, classic spectral methods usually fail to work, due to localization of
eigenvectors (or singular vectors) induced by the sparsity or noise. In this
work, we propose a general method to solve the localization problem by learning
a regularization matrix from the localized eigenvectors. Using matrix
perturbation analysis, we demonstrate that the learned regularizations suppress
down the eigenvalues associated with localized eigenvectors and enable us to
recover the informative eigenvectors representing the global structure. We show
applications of our method in several inference problems: community detection
in networks, clustering from pairwise similarities, rank estimation and matrix
completion problems. Using extensive experiments, we illustrate that our method
solves the localization problem and works down to the theoretical detectability
limits in different kinds of synthetic data. This is in contrast with existing
spectral algorithms based on data matrix, non-backtracking matrix, Laplacians
and those with rank-one regularizations, which perform poorly in the sparse
case with noise.",['Pan Zhang'],"['stat.ML', 'cs.LG', 'cs.SI', 'physics.soc-ph']",2016-09-09 19:48:29+00:00
http://arxiv.org/abs/1609.02845v1,Distributed Online Optimization in Dynamic Environments Using Mirror Descent,"This work addresses decentralized online optimization in non-stationary
environments. A network of agents aim to track the minimizer of a global
time-varying convex function. The minimizer evolves according to a known
dynamics corrupted by an unknown, unstructured noise. At each time, the global
function can be cast as a sum of a finite number of local functions, each of
which is assigned to one agent in the network. Moreover, the local functions
become available to agents sequentially, and agents do not have a prior
knowledge of the future cost functions. Therefore, agents must communicate with
each other to build an online approximation of the global function. We propose
a decentralized variation of the celebrated Mirror Descent, developed by
Nemirovksi and Yudin. Using the notion of Bregman divergence in lieu of
Euclidean distance for projection, Mirror Descent has been shown to be a
powerful tool in large-scale optimization. Our algorithm builds on Mirror
Descent, while ensuring that agents perform a consensus step to follow the
global function and take into account the dynamics of the global minimizer. To
measure the performance of the proposed online algorithm, we compare it to its
offline counterpart, where the global functions are available a priori. The gap
between the two is called dynamic regret. We establish a regret bound that
scales inversely in the spectral gap of the network, and more notably it
represents the deviation of minimizer sequence with respect to the given
dynamics. We then show that our results subsume a number of results in
distributed optimization. We demonstrate the application of our method to
decentralized tracking of dynamic parameters and verify the results via
numerical experiments.","['Shahin Shahrampour', 'Ali Jadbabaie']","['math.OC', 'cs.DC', 'cs.LG', 'stat.ML']",2016-09-09 16:00:04+00:00
http://arxiv.org/abs/1609.02815v3,By-passing the Kohn-Sham equations with machine learning,"Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of
density functional theory to solve electronic structure problems in a wide
variety of scientific fields, ranging from materials science to biochemistry to
astrophysics. Machine learning holds the promise of learning the kinetic energy
functional via examples, by-passing the need to solve the Kohn-Sham equations.
This should yield substantial savings in computer time, allowing either larger
systems or longer time-scales to be tackled, but attempts to machine-learn this
functional have been limited by the need to find its derivative. The present
work overcomes this difficulty by directly learning the density-potential and
energy-density maps for test systems and various molecules. Both improved
accuracy and lower computational cost with this method are demonstrated by
reproducing DFT energies for a range of molecular geometries generated during
molecular dynamics simulations. Moreover, the methodology could be applied
directly to quantum chemical calculations, allowing construction of density
functionals of quantum-chemical accuracy.","['Felix Brockherde', 'Leslie Vogt', 'Li Li', 'Mark E. Tuckerman', 'Kieron Burke', 'Klaus-Robert Müller']","['physics.comp-ph', 'cs.LG', 'physics.chem-ph', 'stat.ML']",2016-09-09 14:45:48+00:00
http://arxiv.org/abs/1609.02700v1,Efficient batch-sequential Bayesian optimization with moments of truncated Gaussian vectors,"We deal with the efficient parallelization of Bayesian global optimization
algorithms, and more specifically of those based on the expected improvement
criterion and its variants. A closed form formula relying on multivariate
Gaussian cumulative distribution functions is established for a generalized
version of the multipoint expected improvement criterion. In turn, the latter
relies on intermediate results that could be of independent interest concerning
moments of truncated Gaussian vectors. The obtained expansion of the criterion
enables studying its differentiability with respect to point batches and
calculating the corresponding gradient in closed form. Furthermore , we derive
fast numerical approximations of this gradient and propose efficient batch
optimization strategies. Numerical experiments illustrate that the proposed
approaches enable computational savings of between one and two order of
magnitudes, hence enabling derivative-based batch-sequential acquisition
function maximization to become a practically implementable and efficient
standard.","['Sébastien Marmin', 'Clément Chevalier', 'David Ginsbourger']","['stat.ML', 'stat.AP', 'stat.ME']",2016-09-09 08:42:12+00:00
http://arxiv.org/abs/1609.02686v2,Boosting Joint Models for Longitudinal and Time-to-Event Data,"Joint Models for longitudinal and time-to-event data have gained a lot of
attention in the last few years as they are a helpful technique to approach
common a data structure in clinical studies where longitudinal outcomes are
recorded alongside event times. Those two processes are often linked and the
two outcomes should thus be modeled jointly in order to prevent the potential
bias introduced by independent modelling. Commonly, joint models are estimated
in likelihood based expectation maximization or Bayesian approaches using
frameworks where variable selection is problematic and which do not immediately
work for high-dimensional data. In this paper, we propose a boosting algorithm
tackling these challenges by being able to simultaneously estimate predictors
for joint models and automatically select the most influential variables even
in high-dimensional data situations. We analyse the performance of the new
algorithm in a simulation study and apply it to the Danish cystic fibrosis
registry which collects longitudinal lung function data on patients with cystic
fibrosis together with data regarding the onset of pulmonary infections. This
is the first approach to combine state-of-the art algorithms from the field of
machine-learning with the model class of joint models, providing a fully
data-driven mechanism to select variables and predictor effects in a unified
framework of boosting joint models.","['Elisabeth Waldmann', 'David Taylor-Robinson', 'Nadja Klein', 'Thomas Kneib', 'Tania Pressler', 'Matthias Schmid', 'Andreas Mayr']","['stat.ML', 'stat.ME']",2016-09-09 08:17:54+00:00
http://arxiv.org/abs/1609.02655v4,Singularity structures and impacts on parameter estimation in finite mixtures of distributions,"Singularities of a statistical model are the elements of the model's
parameter space which make the corresponding Fisher information matrix
degenerate. These are the points for which estimation techniques such as the
maximum likelihood estimator and standard Bayesian procedures do not admit the
root-$n$ parametric rate of convergence. We propose a general framework for the
identification of singularity structures of the parameter space of finite
mixtures, and study the impacts of the singularity structures on minimax lower
bounds and rates of convergence for the maximum likelihood estimator over a
compact parameter space. Our study makes explicit the deep links between model
singularities, parameter estimation convergence rates and minimax lower bounds,
and the algebraic geometry of the parameter space for mixtures of continuous
distributions. The theory is applied to establish concrete convergence rates of
parameter estimation for finite mixture of skew-normal distributions. This rich
and increasingly popular mixture model is shown to exhibit a remarkably complex
range of asymptotic behaviors which have not been hitherto reported in the
literature.","['Nhat Ho', 'XuanLong Nguyen']","['math.ST', 'stat.ML', 'stat.TH']",2016-09-09 04:22:03+00:00
http://arxiv.org/abs/1609.02631v1,Distributed Processing of Biosignal-Database for Emotion Recognition with Mahout,"This paper investigates the use of distributed processing on the problem of
emotion recognition from physiological sensors using a popular machine learning
library on distributed mode. Specifically, we run a random forests classifier
on the biosignal-data, which have been pre-processed to form exclusive groups
in an unsupervised fashion, on a Cloudera cluster using Mahout. The use of
distributed processing significantly reduces the time required for the offline
training of the classifier, enabling processing of large physiological datasets
through many iterations.","['Varvara Kollia', 'Oguz H. Elibol']","['stat.ML', 'cs.LG']",2016-09-09 01:13:20+00:00
http://arxiv.org/abs/1609.02613v3,Why is Differential Evolution Better than Grid Search for Tuning Defect Predictors?,"Context: One of the black arts of data mining is learning the magic
parameters which control the learners. In software analytics, at least for
defect prediction, several methods, like grid search and differential evolution
(DE), have been proposed to learn these parameters, which has been proved to be
able to improve the performance scores of learners.
  Objective: We want to evaluate which method can find better parameters in
terms of performance score and runtime cost.
  Methods: This paper compares grid search to differential evolution, which is
an evolutionary algorithm that makes extensive use of stochastic jumps around
the search space.
  Results: We find that the seemingly complete approach of grid search does no
better, and sometimes worse, than the stochastic search. When repeated 20 times
to check for conclusion validity, DE was over 210 times faster than grid search
to tune Random Forests on 17 testing data sets with F-Measure
  Conclusions: These results are puzzling: why does a quick partial search be
just as effective as a much slower, and much more, extensive search? To answer
that question, we turned to the theoretical optimization literature. Bergstra
and Bengio conjecture that grid search is not more effective than more
randomized searchers if the underlying search space is inherently low
dimensional. This is significant since recent results show that defect
prediction exhibits very low intrinsic dimensionality-- an observation that
explains why a fast method like DE may work as well as a seemingly more
thorough grid search. This suggests, as a future research direction, that it
might be possible to peek at data sets before doing any optimization in order
to match the optimization algorithm to the problem at hand.","['Wei Fu', 'Vivek Nair', 'Tim Menzies']","['cs.SE', 'cs.LG', 'stat.ML']",2016-09-08 22:32:44+00:00
