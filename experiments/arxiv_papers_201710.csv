id,title,abstract,authors,categories,date
http://arxiv.org/abs/1711.06350v1,Towards Deep Learning Models for Psychological State Prediction using Smartphone Data: Challenges and Opportunities,"There is an increasing interest in exploiting mobile sensing technologies and
machine learning techniques for mental health monitoring and intervention.
Researchers have effectively used contextual information, such as mobility,
communication and mobile phone usage patterns for quantifying individuals' mood
and wellbeing. In this paper, we investigate the effectiveness of neural
network models for predicting users' level of stress by using the location
information collected by smartphones. We characterize the mobility patterns of
individuals using the GPS metrics presented in the literature and employ these
metrics as input to the network. We evaluate our approach on the open-source
StudentLife dataset. Moreover, we discuss the challenges and trade-offs
involved in building machine learning models for digital mental health and
highlight potential future work in this direction.","['Gatis Mikelsons', 'Matthew Smith', 'Abhinav Mehrotra', 'Mirco Musolesi']","['cs.LG', 'cs.AI', 'stat.ML']",2017-11-16 23:18:03+00:00
http://arxiv.org/abs/1711.06346v3,Mosquito detection with low-cost smartphones: data acquisition for malaria research,"Mosquitoes are a major vector for malaria, causing hundreds of thousands of
deaths in the developing world each year. Not only is the prevention of
mosquito bites of paramount importance to the reduction of malaria transmission
cases, but understanding in more forensic detail the interplay between malaria,
mosquito vectors, vegetation, standing water and human populations is crucial
to the deployment of more effective interventions. Typically the presence and
detection of malaria-vectoring mosquitoes is only quantified by hand-operated
insect traps or signified by the diagnosis of malaria. If we are to gather
timely, large-scale data to improve this situation, we need to automate the
process of mosquito detection and classification as much as possible. In this
paper, we present a candidate mobile sensing system that acts as both a
portable early warning device and an automatic acoustic data acquisition
pipeline to help fuel scientific inquiry and policy. The machine learning
algorithm that powers the mobile system achieves excellent off-line
multi-species detection performance while remaining computationally efficient.
Further, we have conducted preliminary live mosquito detection tests using
low-cost mobile phones and achieved promising results. The deployment of this
system for field usage in Southeast Asia and Africa is planned in the near
future. In order to accelerate processing of field recordings and labelling of
collected data, we employ a citizen science platform in conjunction with
automated methods, the former implemented using the Zooniverse platform,
allowing crowdsourcing on a grand scale.","['Yunpeng Li', 'Davide Zilli', 'Henry Chan', 'Ivan Kiskin', 'Marianne Sinka', 'Stephen Roberts', 'Kathy Willis']","['stat.ML', 'cs.CY']",2017-11-16 22:58:29+00:00
http://arxiv.org/abs/1711.06323v1,"Poverty Mapping Using Convolutional Neural Networks Trained on High and Medium Resolution Satellite Images, With an Application in Mexico","Mapping the spatial distribution of poverty in developing countries remains
an important and costly challenge. These ""poverty maps"" are key inputs for
poverty targeting, public goods provision, political accountability, and impact
evaluation, that are all the more important given the geographic dispersion of
the remaining bottom billion severely poor individuals. In this paper we train
Convolutional Neural Networks (CNNs) to estimate poverty directly from high and
medium resolution satellite images. We use both Planet and Digital Globe
imagery with spatial resolutions of 3-5 sq. m. and 50 sq. cm. respectively,
covering all 2 million sq. km. of Mexico. Benchmark poverty estimates come from
the 2014 MCS-ENIGH combined with the 2015 Intercensus and are used to estimate
poverty rates for 2,456 Mexican municipalities. CNNs are trained using the 896
municipalities in the 2014 MCS-ENIGH. We experiment with several architectures
(GoogleNet, VGG) and use GoogleNet as a final architecture where weights are
fine-tuned from ImageNet. We find that 1) the best models, which incorporate
satellite-estimated land use as a predictor, explain approximately 57% of the
variation in poverty in a validation sample of 10 percent of MCS-ENIGH
municipalities; 2) Across all MCS-ENIGH municipalities explanatory power
reduces to 44% in a CNN prediction and landcover model; 3) Predicted poverty
from the CNN predictions alone explains 47% of the variation in poverty in the
validation sample, and 37% over all MCS-ENIGH municipalities; 4) In urban areas
we see slight improvements from using Digital Globe versus Planet imagery,
which explain 61% and 54% of poverty variation respectively. We conclude that
CNNs can be trained end-to-end on satellite imagery to estimate poverty,
although there is much work to be done to understand how the training process
influences out of sample validation.","['Boris Babenko', 'Jonathan Hersh', 'David Newhouse', 'Anusha Ramakrishnan', 'Tom Swartz']","['stat.ML', 'cs.CY', 'stat.AP']",2017-11-16 21:27:33+00:00
http://arxiv.org/abs/1711.06221v1,A Forward-Backward Approach for Visualizing Information Flow in Deep Networks,"We introduce a new, systematic framework for visualizing information flow in
deep networks. Specifically, given any trained deep convolutional network model
and a given test image, our method produces a compact support in the image
domain that corresponds to a (high-resolution) feature that contributes to the
given explanation. Our method is both computationally efficient as well as
numerically robust. We present several preliminary numerical results that
support the benefits of our framework over existing methods.","['Aditya Balu', 'Thanh V. Nguyen', 'Apurva Kokate', 'Chinmay Hegde', 'Soumik Sarkar']","['stat.ML', 'cs.CV', 'cs.LG']",2017-11-16 18:00:24+00:00
http://arxiv.org/abs/1711.06195v2,Neurology-as-a-Service for the Developing World,"Electroencephalography (EEG) is an extensively-used and well-studied
technique in the field of medical diagnostics and treatment for brain
disorders, including epilepsy, migraines, and tumors. The analysis and
interpretation of EEGs require physicians to have specialized training, which
is not common even among most doctors in the developed world, let alone the
developing world where physician shortages plague society. This problem can be
addressed by teleEEG that uses remote EEG analysis by experts or by local
computer processing of EEGs. However, both of these options are prohibitively
expensive and the second option requires abundant computing resources and
infrastructure, which is another concern in developing countries where there
are resource constraints on capital and computing infrastructure. In this work,
we present a cloud-based deep neural network approach to provide decision
support for non-specialist physicians in EEG analysis and interpretation. Named
`neurology-as-a-service,' the approach requires almost no manual intervention
in feature engineering and in the selection of an optimal architecture and
hyperparameters of the neural network. In this study, we deploy a pipeline that
includes moving EEG data to the cloud and getting optimal models for various
classification tasks. Our initial prototype has been tested only in developed
world environments to-date, but our intention is to test it in developing world
environments in future work. We demonstrate the performance of our proposed
approach using the BCI2000 EEG MMI dataset, on which our service attains 63.4%
accuracy for the task of classifying real vs. imaginary activity performed by
the subject, which is significantly higher than what is obtained with a shallow
approach such as support vector machines.","['Tejas Dharamsi', 'Payel Das', 'Tejaswini Pedapati', 'Gregory Bramble', 'Vinod Muthusamy', 'Horst Samulowitz', 'Kush R. Varshney', 'Yuvaraj Rajamanickam', 'John Thomas', 'Justin Dauwels']","['stat.ML', 'cs.LG']",2017-11-16 16:58:53+00:00
http://arxiv.org/abs/1711.06178v1,Beyond Sparsity: Tree Regularization of Deep Models for Interpretability,"The lack of interpretability remains a key barrier to the adoption of deep
models in many applications. In this work, we explicitly regularize deep models
so human users might step through the process behind their predictions in
little time. Specifically, we train deep time-series models so their
class-probability predictions have high accuracy while being closely modeled by
decision trees with few nodes. Using intuitive toy examples as well as medical
tasks for treating sepsis and HIV, we demonstrate that this new tree
regularization yields models that are easier for humans to simulate than
simpler L1 or L2 penalties without sacrificing predictive power.","['Mike Wu', 'Michael C. Hughes', 'Sonali Parbhoo', 'Maurizio Zazzi', 'Volker Roth', 'Finale Doshi-Velez']","['stat.ML', 'cs.LG']",2017-11-16 16:35:24+00:00
http://arxiv.org/abs/1711.06114v4,Robust Unsupervised Domain Adaptation for Neural Networks via Moment Alignment,"A novel approach for unsupervised domain adaptation for neural networks is
proposed. It relies on metric-based regularization of the learning process. The
metric-based regularization aims at domain-invariant latent feature
representations by means of maximizing the similarity between domain-specific
activation distributions. The proposed metric results from modifying an
integral probability metric such that it becomes less translation-sensitive on
a polynomial function space. The metric has an intuitive interpretation in the
dual space as the sum of differences of higher order central moments of the
corresponding activation distributions. Under appropriate assumptions on the
input distributions, error minimization is proven for the continuous case. As
demonstrated by an analysis of standard benchmark experiments for sentiment
analysis, object recognition and digit recognition, the outlined approach is
robust regarding parameter changes and achieves higher classification
accuracies than comparable approaches. The source code is available at
https://github.com/wzell/mann.","['Werner Zellinger', 'Bernhard A. Moser', 'Thomas Grubinger', 'Edwin Lughofer', 'Thomas Natschläger', 'Susanne Saminger-Platz']","['stat.ML', 'cs.LG']",2017-11-16 14:45:05+00:00
http://arxiv.org/abs/1711.06104v4,Towards better understanding of gradient-based attribution methods for Deep Neural Networks,"Understanding the flow of information in Deep Neural Networks (DNNs) is a
challenging problem that has gain increasing attention over the last few years.
While several methods have been proposed to explain network predictions, there
have been only a few attempts to compare them from a theoretical perspective.
What is more, no exhaustive empirical comparison has been performed in the
past. In this work, we analyze four gradient-based attribution methods and
formally prove conditions of equivalence and approximation between them. By
reformulating two of these methods, we construct a unified framework which
enables a direct comparison, as well as an easier implementation. Finally, we
propose a novel evaluation metric, called Sensitivity-n and test the
gradient-based attribution methods alongside with a simple perturbation-based
attribution method on several datasets in the domains of image and text
classification, using various network architectures.","['Marco Ancona', 'Enea Ceolini', 'Cengiz Öztireli', 'Markus Gross']","['cs.LG', 'stat.ML']",2017-11-16 14:19:29+00:00
http://arxiv.org/abs/1711.06100v2,"Sequences, Items And Latent Links: Recommendation With Consumed Item Packs","Recommenders personalize the web content by typically using collaborative
filtering to relate users (or items) based on explicit feedback, e.g., ratings.
The difficulty of collecting this feedback has recently motivated to consider
implicit feedback (e.g., item consumption along with the corresponding time).
  In this paper, we introduce the notion of consumed item pack (CIP) which
enables to link users (or items) based on their implicit analogous consumption
behavior. Our proposal is generic, and we show that it captures three novel
implicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word
embedding-based (DEEPCIP), as well as a state-of-the-art technique using
implicit feedback (FISM). We show that our recommenders handle incremental
updates incorporating freshly consumed items. We demonstrate that all three
recommenders provide a recommendation quality that is competitive with
state-of-the-art ones, including one incorporating both explicit and implicit
feedback.","['Rachid Guerraoui', 'Erwan Le Merrer', 'Rhicheek Patra', 'Jean-Ronan Vigouroux']","['cs.IR', 'cs.SI', 'stat.ML']",2017-11-16 14:11:53+00:00
http://arxiv.org/abs/1711.06064v1,Gaussian Process Decentralized Data Fusion Meets Transfer Learning in Large-Scale Distributed Cooperative Perception,"This paper presents novel Gaussian process decentralized data fusion
algorithms exploiting the notion of agent-centric support sets for distributed
cooperative perception of large-scale environmental phenomena. To overcome the
limitations of scale in existing works, our proposed algorithms allow every
mobile sensing agent to choose a different support set and dynamically switch
to another during execution for encapsulating its own data into a local summary
that, perhaps surprisingly, can still be assimilated with the other agents'
local summaries (i.e., based on their current choices of support sets) into a
globally consistent summary to be used for predicting the phenomenon. To
achieve this, we propose a novel transfer learning mechanism for a team of
agents capable of sharing and transferring information encapsulated in a
summary based on a support set to that utilizing a different support set with
some loss that can be theoretically bounded and analyzed. To alleviate the
issue of information loss accumulating over multiple instances of transfer
learning, we propose a new information sharing mechanism to be incorporated
into our algorithms in order to achieve memory-efficient lazy transfer
learning. Empirical evaluation on real-world datasets show that our algorithms
outperform the state-of-the-art methods.","['Ruofei Ouyang', 'Kian Hsiang Low']","['stat.ML', 'cs.LG', 'cs.MA', 'cs.RO']",2017-11-16 12:41:33+00:00
http://arxiv.org/abs/1711.06047v1,Deep Matching Autoencoders,"Increasingly many real world tasks involve data in multiple modalities or
views. This has motivated the development of many effective algorithms for
learning a common latent space to relate multiple domains. However, most
existing cross-view learning algorithms assume access to paired data for
training. Their applicability is thus limited as the paired data assumption is
often violated in practice: many tasks have only a small subset of data
available with pairing annotation, or even no paired data at all. In this paper
we introduce Deep Matching Autoencoders (DMAE), which learn a common latent
space and pairing from unpaired multi-modal data. Specifically we formulate
this as a cross-domain representation learning and object matching problem. We
simultaneously optimise parameters of representation learning auto-encoders and
the pairing of unpaired multi-modal data. This framework elegantly spans the
full regime from fully supervised, semi-supervised, and unsupervised (no paired
data) multi-modal learning. We show promising results in image captioning, and
on a new task that is uniquely enabled by our methodology: unsupervised
classifier learning.","['Tanmoy Mukherjee', 'Makoto Yamada', 'Timothy M. Hospedales']","['cs.CV', 'stat.ML']",2017-11-16 11:50:41+00:00
http://arxiv.org/abs/1711.05957v1,HodgeRank with Information Maximization for Crowdsourced Pairwise Ranking Aggregation,"Recently, crowdsourcing has emerged as an effective paradigm for
human-powered large scale problem solving in various domains. However, task
requester usually has a limited amount of budget, thus it is desirable to have
a policy to wisely allocate the budget to achieve better quality. In this
paper, we study the principle of information maximization for active sampling
strategies in the framework of HodgeRank, an approach based on Hodge
Decomposition of pairwise ranking data with multiple workers. The principle
exhibits two scenarios of active sampling: Fisher information maximization that
leads to unsupervised sampling based on a sequential maximization of graph
algebraic connectivity without considering labels; and Bayesian information
maximization that selects samples with the largest information gain from prior
to posterior, which gives a supervised sampling involving the labels collected.
Experiments show that the proposed methods boost the sampling efficiency as
compared to traditional sampling schemes and are thus valuable to practical
crowdsourcing experiments.","['Qianqian Xu', 'Jiechao Xiong', 'Xi Chen', 'Qingming Huang', 'Yuan Yao']",['stat.ML'],2017-11-16 06:58:23+00:00
http://arxiv.org/abs/1711.05928v1,Budget-Constrained Multi-Armed Bandits with Multiple Plays,"We study the multi-armed bandit problem with multiple plays and a budget
constraint for both the stochastic and the adversarial setting. At each round,
exactly $K$ out of $N$ possible arms have to be played (with $1\leq K \leq N$).
In addition to observing the individual rewards for each arm played, the player
also learns a vector of costs which has to be covered with an a-priori defined
budget $B$. The game ends when the sum of current costs associated with the
played arms exceeds the remaining budget.
  Firstly, we analyze this setting for the stochastic case, for which we assume
each arm to have an underlying cost and reward distribution with support
$[c_{\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound
(UCB) algorithm which achieves $O(NK^4 \log B)$ regret.
  Secondly, for the adversarial case in which the entire sequence of rewards
and costs is fixed in advance, we derive an upper bound on the regret of order
$O(\sqrt{NB\log(N/K)})$ utilizing an extension of the well-known
$\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high
probability and a lower bound of order $\Omega((1 - K/N)^2 \sqrt{NB/K})$.","['Datong P. Zhou', 'Claire J. Tomlin']","['cs.LG', 'cs.AI', 'stat.ML']",2017-11-16 05:07:34+00:00
http://arxiv.org/abs/1711.05869v2,"Predictive Independence Testing, Predictive Conditional Independence Testing, and Predictive Graphical Modelling","Testing (conditional) independence of multivariate random variables is a task
central to statistical inference and modelling in general - though
unfortunately one for which to date there does not exist a practicable
workflow. State-of-art workflows suffer from the need for heuristic or
subjective manual choices, high computational complexity, or strong parametric
assumptions.
  We address these problems by establishing a theoretical link between
multivariate/conditional independence testing, and model comparison in the
multivariate predictive modelling aka supervised learning task. This link
allows advances in the extensively studied supervised learning workflow to be
directly transferred to independence testing workflows - including automated
tuning of machine learning type which addresses the need for a heuristic
choice, the ability to quantitatively trade-off computational demand with
accuracy, and the modern black-box philosophy for checking and interfacing.
  As a practical implementation of this link between the two workflows, we
present a python package 'pcit', which implements our novel multivariate and
conditional independence tests, interfacing the supervised learning API of the
scikit-learn package. Theory and package also allow for straightforward
independence test based learning of graphical model structure.
  We empirically show that our proposed predictive independence test outperform
or are on par to current practice, and the derived graphical model structure
learning algorithms asymptotically recover the 'true' graph. This paper, and
the 'pcit' package accompanying it, thus provide powerful, scalable,
generalizable, and easy-to-use methods for multivariate and conditional
independence testing, as well as for graphical model structure learning.","['Samuel Burkart', 'Franz J Király']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2017-11-16 00:37:34+00:00
http://arxiv.org/abs/1711.05828v2,BoostJet: Towards Combining Statistical Aggregates with Neural Embeddings for Recommendations,"Recommenders have become widely popular in recent years because of their
broader applicability in many e-commerce applications. These applications rely
on recommenders for generating advertisements for various offers or providing
content recommendations. However, the quality of the generated recommendations
depends on user features (like demography, temporality), offer features (like
popularity, price), and user-offer features (like implicit or explicit
feedback). Current state-of-the-art recommenders do not explore such diverse
features concurrently while generating the recommendations.
  In this paper, we first introduce the notion of Trackers which enables us to
capture the above-mentioned features and thus incorporate users' online
behaviour through statistical aggregates of different features (demography,
temporality, popularity, price). We also show how to capture offer-to-offer
relations, based on their consumption sequence, leveraging neural embeddings
for offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel
recommender which integrates the Trackers along with the neural embeddings
using MatrixNet, an efficient distributed implementation of gradient boosted
decision tree, to improve the recommendation quality significantly. We provide
an in-depth evaluation of BoostJet on Yandex's dataset, collecting online
behaviour from tens of millions of online users, to demonstrate the
practicality of BoostJet in terms of recommendation quality as well as
scalability.","['Rhicheek Patra', 'Egor Samosvat', 'Michael Roizner', 'Andrei Mishchenko']","['cs.IR', 'cs.LG', 'stat.ML']",2017-11-15 22:25:49+00:00
http://arxiv.org/abs/1711.05825v2,Bootstrapped synthetic likelihood,"Approximate Bayesian computation (ABC) and synthetic likelihood (SL)
techniques have enabled the use of Bayesian inference for models that may be
simulated, but for which the likelihood cannot be evaluated pointwise at values
of an unknown parameter $\theta$. The main idea in ABC and SL is to, for
different values of $\theta$ (usually chosen using a Monte Carlo algorithm),
build estimates of the likelihood based on simulations from the model
conditional on $\theta$. The quality of these estimates determines the
efficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to
improve an estimated likelihood at $\theta$ is to simulate more times from the
model conditional on $\theta$, which is infeasible in cases where the simulator
is computationally expensive. In this paper we describe how to use
bootstrapping as a means for improving SL estimates whilst using fewer
simulations from the model, and also investigate its use in ABC. Further, we
investigate the use of the bag of little bootstraps as a means for applying
this approach to large datasets, yielding Monte Carlo algorithms that
accurately approximate posterior distributions whilst only simulating
subsamples of the full data. Examples of the approach applied to i.i.d.,
temporal and spatial data are given.",['Richard G. Everitt'],"['stat.CO', 'cs.AI', 'physics.data-an', 'stat.ME', 'stat.ML']",2017-11-15 22:13:48+00:00
http://arxiv.org/abs/1711.05809v1,Hierarchical Modeling of Seed Variety Yields and Decision Making for Future Planting Plans,"Eradicating hunger and malnutrition is a key development goal of the 21st
century. We address the problem of optimally identifying seed varieties to
reliably increase crop yield within a risk-sensitive decision-making framework.
Specifically, we introduce a novel hierarchical machine learning mechanism for
predicting crop yield (the yield of different seed varieties of the same crop).
We integrate this prediction mechanism with a weather forecasting model, and
propose three different approaches for decision making under uncertainty to
select seed varieties for planting so as to balance yield maximization and
risk.We apply our model to the problem of soybean variety selection given in
the 2016 Syngenta Crop Challenge. Our prediction model achieves a median
absolute error of 3.74 bushels per acre and thus provides good estimates for
input into the decision models.Our decision models identify the selection of
soybean varieties that appropriately balance yield and risk as a function of
the farmer's risk aversion level. More generally, our models support farmers in
decision making about which seed varieties to plant.","['Huaiyang Zhong', 'Xiaocheng Li', 'David Lobell', 'Stefano Ermon', 'Margaret L. Brandeau']","['cs.LG', 'stat.ML']",2017-11-15 21:12:30+00:00
http://arxiv.org/abs/1711.05772v2,Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models,"Deep generative neural networks have proven effective at both conditional and
unconditional modeling of complex data distributions. Conditional generation
enables interactive control, but creating new controls often requires expensive
retraining. In this paper, we develop a method to condition generation without
retraining the model. By post-hoc learning latent constraints, value functions
that identify regions in latent space that generate outputs with desired
attributes, we can conditionally sample from these regions with gradient-based
optimization or amortized actor functions. Combining attribute constraints with
a universal ""realism"" constraint, which enforces similarity to the data
distribution, we generate realistic conditional images from an unconditional
variational autoencoder. Further, using gradient-based optimization, we
demonstrate identity-preserving transformations that make the minimal
adjustment in latent space to modify the attributes of an image. Finally, with
discrete sequences of musical notes, we demonstrate zero-shot conditional
generation, learning latent constraints in the absence of labeled data or a
differentiable reward function. Code with dedicated cloud instance has been
made publicly available (https://goo.gl/STGMGx).","['Jesse Engel', 'Matthew Hoffman', 'Adam Roberts']","['cs.LG', 'cs.NE', 'stat.ML']",2017-11-15 19:45:10+00:00
http://arxiv.org/abs/1711.05762v1,Random gradient extrapolation for distributed and stochastic optimization,"In this paper, we consider a class of finite-sum convex optimization problems
defined over a distributed multiagent network with $m$ agents connected to a
central server. In particular, the objective function consists of the average
of $m$ ($\ge 1$) smooth components associated with each network agent together
with a strongly convex term. Our major contribution is to develop a new
randomized incremental gradient algorithm, namely random gradient extrapolation
method (RGEM), which does not require any exact gradient evaluation even for
the initial point, but can achieve the optimal ${\cal O}(\log(1/\epsilon))$
complexity bound in terms of the total number of gradient evaluations of
component functions to solve the finite-sum problems. Furthermore, we
demonstrate that for stochastic finite-sum optimization problems, RGEM
maintains the optimal ${\cal O}(1/\epsilon)$ complexity (up to a certain
logarithmic factor) in terms of the number of stochastic gradient computations,
but attains an ${\cal O}(\log(1/\epsilon))$ complexity in terms of
communication rounds (each round involves only one agent). It is worth noting
that the former bound is independent of the number of agents $m$, while the
latter one only linearly depends on $m$ or even $\sqrt m$ for ill-conditioned
problems. To the best of our knowledge, this is the first time that these
complexity bounds have been obtained for distributed and stochastic
optimization problems. Moreover, our algorithms were developed based on a novel
dual perspective of Nesterov's accelerated gradient method.","['Guanghui Lan', 'Yi Zhou']","['math.OC', 'cs.CC', 'cs.LG', 'stat.ML']",2017-11-15 19:18:59+00:00
http://arxiv.org/abs/1711.05726v1,Markov Decision Processes with Continuous Side Information,"We consider a reinforcement learning (RL) setting in which the agent
interacts with a sequence of episodic MDPs. At the start of each episode the
agent has access to some side-information or context that determines the
dynamics of the MDP for that episode. Our setting is motivated by applications
in healthcare where baseline measurements of a patient at the start of a
treatment episode form the context that may provide information about how the
patient might respond to treatment decisions. We propose algorithms for
learning in such Contextual Markov Decision Processes (CMDPs) under an
assumption that the unobserved MDP parameters vary smoothly with the observed
context. We also give lower and upper PAC bounds under the smoothness
assumption. Because our lower bound has an exponential dependence on the
dimension, we consider a tractable linear setting where the context is used to
create linear combinations of a finite set of MDPs. For the linear setting, we
give a PAC learning algorithm based on KWIK learning techniques.","['Aditya Modi', 'Nan Jiang', 'Satinder Singh', 'Ambuj Tewari']","['stat.ML', 'cs.AI', 'cs.LG']",2017-11-15 18:49:16+00:00
http://arxiv.org/abs/1711.10396v1,Assessment Formats and Student Learning Performance: What is the Relation?,"Although compelling assessments have been examined in recent years, more
studies are required to yield a better understanding of the several methods
where assessment techniques significantly affect student learning process. Most
of the educational research in this area does not consider demographics data,
differing methodologies, and notable sample size. To address these drawbacks,
the objective of our study is to analyse student learning outcomes of multiple
assessment formats for a web-facilitated in-class section with an asynchronous
online class of a core data communications course in the Undergraduate IT
program of the Information Sciences and Technology (IST) Department at George
Mason University (GMU). In this study, students were evaluated based on course
assessments such as home and lab assignments, skill-based assessments, and
traditional midterm and final exams across all four sections of the course. All
sections have equivalent content, assessments, and teaching methodologies.
Student demographics such as exam type and location preferences are considered
in our study to determine whether they have any impact on their learning
approach. Large amount of data from the learning management system (LMS),
Blackboard (BB) Learn, had to be examined to compare the results of several
assessment outcomes for all students within their respective section and
amongst students of other sections. To investigate the effect of dissimilar
assessment formats on student performance, we had to correlate individual
question formats with the overall course grade. The results show that
collective assessment formats allow students to be effective in demonstrating
their knowledge.","['Khondkar Islam', 'Pouyan Ahmadi', 'Salman Yousaf']","['physics.ed-ph', 'cs.CY', 'stat.AP', 'stat.ML']",2017-11-15 18:35:58+00:00
http://arxiv.org/abs/1711.05717v1,Variational Bi-LSTMs,"Recurrent neural networks like long short-term memory (LSTM) are important
architectures for sequential prediction tasks. LSTMs (and RNNs in general)
model sequences along the forward time direction. Bidirectional LSTMs
(Bi-LSTMs) on the other hand model sequences along both forward and backward
directions and are generally known to perform better at such tasks because they
capture a richer representation of the data. In the training of Bi-LSTMs, the
forward and backward paths are learned independently. We propose a variant of
the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a
channel between the two paths (during training, but which may be omitted during
inference); thus optimizing the two paths jointly. We arrive at this joint
objective for our model by minimizing a variational lower bound of the joint
likelihood of the data sequence. Our model acts as a regularizer and encourages
the two networks to inform each other in making their respective predictions
using distinct information. We perform ablation studies to better understand
the different components of our model and evaluate the method on various
benchmarks, showing state-of-the-art performance.","['Samira Shabanian', 'Devansh Arpit', 'Adam Trischler', 'Yoshua Bengio']","['stat.ML', 'cs.LG']",2017-11-15 18:30:05+00:00
http://arxiv.org/abs/1711.06552v1,Introduction to intelligent computing unit 1,"This brief note highlights some basic concepts required toward understanding
the evolution of machine learning and deep learning models. The note starts
with an overview of artificial intelligence and its relationship to biological
neuron that ultimately led to the evolution of todays intelligent models.",['Isa Inuwa-Dutse'],"['cs.LG', 'stat.ML']",2017-11-15 16:52:48+00:00
http://arxiv.org/abs/1711.05656v2,Learning to Predict with Highly Granular Temporal Data: Estimating individual behavioral profiles with smart meter data,"Big spatio-temporal datasets, available through both open and administrative
data sources, offer significant potential for social science research. The
magnitude of the data allows for increased resolution and analysis at
individual level. While there are recent advances in forecasting techniques for
highly granular temporal data, little attention is given to segmenting the time
series and finding homogeneous patterns. In this paper, it is proposed to
estimate behavioral profiles of individuals' activities over time using
Gaussian Process-based models. In particular, the aim is to investigate how
individuals or groups may be clustered according to the model parameters. Such
a Bayesian non-parametric method is then tested by looking at the
predictability of the segments using a combination of models to fit different
parts of the temporal profiles. Model validity is then tested on a set of
holdout data. The dataset consists of half hourly energy consumption records
from smart meters from more than 100,000 households in the UK and covers the
period from 2015 to 2016. The methodological approach developed in the paper
may be easily applied to datasets of similar structure and granularity, for
example social media data, and may lead to improved accuracy in the prediction
of social dynamics and behavior.","['Anastasia Ushakova', 'Slava J. Mikhaylov']","['stat.AP', 'stat.ML']",2017-11-15 16:36:14+00:00
http://arxiv.org/abs/1711.05615v1,Spatial Mapping with Gaussian Processes and Nonstationary Fourier Features,"The use of covariance kernels is ubiquitous in the field of spatial
statistics. Kernels allow data to be mapped into high-dimensional feature
spaces and can thus extend simple linear additive methods to nonlinear methods
with higher order interactions. However, until recently, there has been a
strong reliance on a limited class of stationary kernels such as the Matern or
squared exponential, limiting the expressiveness of these modelling approaches.
Recent machine learning research has focused on spectral representations to
model arbitrary stationary kernels and introduced more general representations
that include classes of nonstationary kernels. In this paper, we exploit the
connections between Fourier feature representations, Gaussian processes and
neural networks to generalise previous approaches and develop a simple and
efficient framework to learn arbitrarily complex nonstationary kernel functions
directly from the data, while taking care to avoid overfitting using
state-of-the-art methods from deep learning. We highlight the very broad array
of kernel classes that could be created within this framework. We apply this to
a time series dataset and a remote sensing problem involving land surface
temperature in Eastern Africa. We show that without increasing the
computational or storage complexity, nonstationary kernels can be used to
improve generalisation performance and provide more interpretable results.","['Jean-Francois Ton', 'Seth Flaxman', 'Dino Sejdinovic', 'Samir Bhatt']",['stat.ML'],2017-11-15 15:07:11+00:00
http://arxiv.org/abs/1711.05610v4,On consistent vertex nomination schemes,"Given a vertex of interest in a network $G_1$, the vertex nomination problem
seeks to find the corresponding vertex of interest (if it exists) in a second
network $G_2$. A vertex nomination scheme produces a list of the vertices in
$G_2$, ranked according to how likely they are judged to be the corresponding
vertex of interest in $G_2$. The vertex nomination problem and related
information retrieval tasks have attracted much attention in the machine
learning literature, with numerous applications to social and biological
networks. However, the current framework has often been confined to a
comparatively small class of network models, and the concept of statistically
consistent vertex nomination schemes has been only shallowly explored. In this
paper, we extend the vertex nomination problem to a very general statistical
model of graphs. Further, drawing inspiration from the long-established
classification framework in the pattern recognition literature, we provide
definitions for the key notions of Bayes optimality and consistency in our
extended vertex nomination framework, including a derivation of the Bayes
optimal vertex nomination scheme. In addition, we prove that no universally
consistent vertex nomination schemes exist. Illustrative examples are provided
throughout.","['Vince Lyzinski', 'Keith Levin', 'Carey E. Priebe']",['stat.ML'],2017-11-15 15:05:21+00:00
http://arxiv.org/abs/1711.05597v3,Advances in Variational Inference,"Many modern unsupervised or semi-supervised machine learning algorithms rely
on Bayesian probabilistic models. These models are usually intractable and thus
require approximate inference. Variational inference (VI) lets us approximate a
high-dimensional Bayesian posterior with a simpler variational distribution by
solving an optimization problem. This approach has been successfully used in
various models and large-scale applications. In this review, we give an
overview of recent trends in variational inference. We first introduce standard
mean field variational inference, then review recent advances focusing on the
following aspects: (a) scalable VI, which includes stochastic approximations,
(b) generic VI, which extends the applicability of VI to a large class of
otherwise intractable models, such as non-conjugate models, (c) accurate VI,
which includes variational models beyond the mean field approximation or with
atypical divergences, and (d) amortized VI, which implements the inference over
local latent variables with inference networks. Finally, we provide a summary
of promising future research directions.","['Cheng Zhang', 'Judith Butepage', 'Hedvig Kjellstrom', 'Stephan Mandt']","['cs.LG', 'stat.ML']",2017-11-15 14:46:27+00:00
http://arxiv.org/abs/1711.05560v1,Variational Adaptive-Newton Method for Explorative Learning,"We present the Variational Adaptive Newton (VAN) method which is a black-box
optimization method especially suitable for explorative-learning tasks such as
active learning and reinforcement learning. Similar to Bayesian methods, VAN
estimates a distribution that can be used for exploration, but requires
computations that are similar to continuous optimization methods. Our
theoretical contribution reveals that VAN is a second-order method that unifies
existing methods in distinct fields of continuous optimization, variational
inference, and evolution strategies. Our experimental results show that VAN
performs well on a wide-variety of learning tasks. This work presents a
general-purpose explorative-learning method that has the potential to improve
learning in areas such as active learning and reinforcement learning.","['Mohammad Emtiyaz Khan', 'Wu Lin', 'Voot Tangkaratt', 'Zuozhu Liu', 'Didrik Nielsen']","['stat.ML', 'cs.LG']",2017-11-15 13:23:29+00:00
http://arxiv.org/abs/1711.05551v1,Sound Event Detection in Synthetic Audio: Analysis of the DCASE 2016 Task Results,"As part of the 2016 public evaluation challenge on Detection and
Classification of Acoustic Scenes and Events (DCASE 2016), the second task
focused on evaluating sound event detection systems using synthetic mixtures of
office sounds. This task, which follows the `Event Detection - Office
Synthetic' task of DCASE 2013, studies the behaviour of tested algorithms when
facing controlled levels of audio complexity with respect to background noise
and polyphony/density, with the added benefit of a very accurate ground truth.
This paper presents the task formulation, evaluation metrics, submitted
systems, and provides a statistical analysis of the results achieved, with
respect to various aspects of the evaluation dataset.","['Grégoire Lafay', 'Emmanouil Benetos', 'Mathieu Lagrange']","['eess.AS', 'cs.SD', 'stat.ML']",2017-11-15 12:59:13+00:00
http://arxiv.org/abs/1711.05482v1,Efficient Estimation of Generalization Error and Bias-Variance Components of Ensembles,"For many applications, an ensemble of base classifiers is an effective
solution. The tuning of its parameters(number of classes, amount of data on
which each classifier is to be trained on, etc.) requires G, the generalization
error of a given ensemble. The efficient estimation of G is the focus of this
paper. The key idea is to approximate the variance of the class
scores/probabilities of the base classifiers over the randomness imposed by the
training subset by normal/beta distribution at each point x in the input
feature space. We estimate the parameters of the distribution using a small set
of randomly chosen base classifiers and use those parameters to give efficient
estimation schemes for G. We give empirical evidence for the quality of the
various estimators. We also demonstrate their usefulness in making design
choices such as the number of classifiers in the ensemble and the size of a
subset of data used for training that is needed to achieve a certain value of
generalization error. Our approach also has great potential for designing
distributed ensemble classifiers.","['Dhruv Mahajan', 'Vivek Gupta', 'S Sathiya Keerthi', 'Sellamanickam Sundararajan', 'Shravan Narayanamurthy', 'Rahul Kidambi']","['cs.LG', 'stat.ML']",2017-11-15 10:03:01+00:00
http://arxiv.org/abs/1711.05477v2,A Convex Parametrization of a New Class of Universal Kernel Functions,"The accuracy and complexity of kernel learning algorithms is determined by
the set of kernels over which it is able to optimize. An ideal set of kernels
should: admit a linear parameterization (tractability); be dense in the set of
all kernels (accuracy); and every member should be universal so that the
hypothesis space is infinite-dimensional (scalability). Currently, there is no
class of kernel that meets all three criteria - e.g. Gaussians are not
tractable or accurate; polynomials are not scalable. We propose a new class
that meet all three criteria - the Tessellated Kernel (TK) class. Specifically,
the TK class: admits a linear parameterization using positive matrices; is
dense in all kernels; and every element in the class is universal. This implies
that the use of TK kernels for learning the kernel can obviate the need for
selecting candidate kernels in algorithms such as SimpleMKL and parameters such
as the bandwidth. Numerical testing on soft margin Support Vector Machine (SVM)
problems show that algorithms using TK kernels outperform other kernel learning
algorithms and neural networks. Furthermore, our results show that when the
ratio of the number of training data to features is high, the improvement of TK
over MKL increases significantly.","['Brendon K. Colbert', 'Matthew M. Peet']","['stat.ML', 'cs.LG']",2017-11-15 09:44:18+00:00
http://arxiv.org/abs/1711.05448v1,Lattice Rescoring Strategies for Long Short Term Memory Language Models in Speech Recognition,"Recurrent neural network (RNN) language models (LMs) and Long Short Term
Memory (LSTM) LMs, a variant of RNN LMs, have been shown to outperform
traditional N-gram LMs on speech recognition tasks. However, these models are
computationally more expensive than N-gram LMs for decoding, and thus,
challenging to integrate into speech recognizers. Recent research has proposed
the use of lattice-rescoring algorithms using RNNLMs and LSTMLMs as an
efficient strategy to integrate these models into a speech recognition system.
In this paper, we evaluate existing lattice rescoring algorithms along with new
variants on a YouTube speech recognition task. Lattice rescoring using LSTMLMs
reduces the word error rate (WER) for this task by 8\% relative to the WER
obtained using an N-gram LM.","['Shankar Kumar', 'Michael Nirschl', 'Daniel Holtmann-Rice', 'Hank Liao', 'Ananda Theertha Suresh', 'Felix Yu']","['stat.ML', 'cs.CL', 'cs.LG']",2017-11-15 08:30:56+00:00
http://arxiv.org/abs/1711.05424v2,The landscape of the spiked tensor model,"We consider the problem of estimating a large rank-one tensor ${\boldsymbol
u}^{\otimes k}\in({\mathbb R}^{n})^{\otimes k}$, $k\ge 3$ in Gaussian noise.
Earlier work characterized a critical signal-to-noise ratio $\lambda_{Bayes}=
O(1)$ above which an ideal estimator achieves strictly positive correlation
with the unknown vector of interest. Remarkably no polynomial-time algorithm is
known that achieved this goal unless $\lambda\ge C n^{(k-2)/4}$ and even
powerful semidefinite programming relaxations appear to fail for $1\ll
\lambda\ll n^{(k-2)/4}$.
  In order to elucidate this behavior, we consider the maximum likelihood
estimator, which requires maximizing a degree-$k$ homogeneous polynomial over
the unit sphere in $n$ dimensions. We compute the expected number of critical
points and local maxima of this objective function and show that it is
exponential in the dimensions $n$, and give exact formulas for the exponential
growth rate. We show that (for $\lambda$ larger than a constant) critical
points are either very close to the unknown vector ${\boldsymbol u}$, or are
confined in a band of width $\Theta(\lambda^{-1/(k-1)})$ around the maximum
circle that is orthogonal to ${\boldsymbol u}$. For local maxima, this band
shrinks to be of size $\Theta(\lambda^{-1/(k-2)})$. These `uninformative' local
maxima are likely to cause the failure of optimization algorithms.","['Gerard Ben Arous', 'Song Mei', 'Andrea Montanari', 'Mihai Nica']","['math.ST', 'math.PR', 'stat.ML', 'stat.TH']",2017-11-15 06:23:43+00:00
http://arxiv.org/abs/1711.05420v2,Accelerating Cross-Validation in Multinomial Logistic Regression with $\ell_1$-Regularization,"We develop an approximate formula for evaluating a cross-validation estimator
of predictive likelihood for multinomial logistic regression regularized by an
$\ell_1$-norm. This allows us to avoid repeated optimizations required for
literally conducting cross-validation; hence, the computational time can be
significantly reduced. The formula is derived through a perturbative approach
employing the largeness of the data size and the model dimensionality. An
extension to the elastic net regularization is also addressed. The usefulness
of the approximate formula is demonstrated on simulated data and the ISOLET
dataset from the UCI machine learning repository.","['Tomoyuki Obuchi', 'Yoshiyuki Kabashima']","['stat.ML', 'cond-mat.dis-nn']",2017-11-15 06:02:30+00:00
http://arxiv.org/abs/1711.05411v2,Z-Forcing: Training Stochastic Recurrent Networks,"Many efforts have been devoted to training generative latent variable models
with autoregressive decoders, such as recurrent neural networks (RNN).
Stochastic recurrent models have been successful in capturing the variability
observed in natural sequential data such as speech. We unify successful ideas
from recently proposed architectures into a stochastic recurrent model: each
step in the sequence is associated with a latent variable that is used to
condition the recurrent dynamics for future steps. Training is performed with
amortized variational inference where the approximate posterior is augmented
with a RNN that runs backward through the sequence. In addition to maximizing
the variational lower bound, we ease training of the latent variables by adding
an auxiliary cost which forces them to reconstruct the state of the backward
recurrent network. This provides the latent variables with a task-independent
objective that enhances the performance of the overall model. We found this
strategy to perform better than alternative approaches such as KL annealing.
Although being conceptually simple, our model achieves state-of-the-art results
on standard speech benchmarks such as TIMIT and Blizzard and competitive
performance on sequential MNIST. Finally, we apply our model to language
modeling on the IMDB dataset where the auxiliary cost helps in learning
interpretable latent variables. Source Code:
\url{https://github.com/anirudh9119/zforcing_nips17}","['Anirudh Goyal', 'Alessandro Sordoni', 'Marc-Alexandre Côté', 'Nan Rosemary Ke', 'Yoshua Bengio']","['stat.ML', 'cs.LG']",2017-11-15 05:16:49+00:00
http://arxiv.org/abs/1711.05407v4,MARGIN: Uncovering Deep Neural Networks using Graph Signal Analysis,"Interpretability has emerged as a crucial aspect of building trust in machine
learning systems, aimed at providing insights into the working of complex
neural networks that are otherwise opaque to a user. There are a plethora of
existing solutions addressing various aspects of interpretability ranging from
identifying prototypical samples in a dataset to explaining image predictions
or explaining mis-classifications. While all of these diverse techniques
address seemingly different aspects of interpretability, we hypothesize that a
large family of interepretability tasks are variants of the same central
problem which is identifying \emph{relative} change in a model's prediction.
This paper introduces MARGIN, a simple yet general approach to address a large
set of interpretability tasks MARGIN exploits ideas rooted in graph signal
analysis to determine influential nodes in a graph, which are defined as those
nodes that maximally describe a function defined on the graph. By carefully
defining task-specific graphs and functions, we demonstrate that MARGIN
outperforms existing approaches in a number of disparate interpretability
challenges.","['Rushil Anirudh', 'Jayaraman J. Thiagarajan', 'Rahul Sridhar', 'Peer-Timo Bremer']","['stat.ML', 'cs.CV', 'cs.LG']",2017-11-15 04:52:38+00:00
http://arxiv.org/abs/1711.05401v3,Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs,"We address the problem of learning vector representations for entities and
relations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This
problem has received significant attention in the past few years and multiple
methods have been proposed. Most of the existing methods in the literature use
a predefined characteristic scoring function for evaluating the correctness of
KG triples. These scoring functions distinguish correct triples (high score)
from incorrect ones (low score). However, their performance vary across
different datasets. In this work, we demonstrate that a simple neural network
based score function can consistently achieve near start-of-the-art performance
on multiple datasets. We also quantitatively demonstrate biases in standard
benchmark datasets, and highlight the need to perform evaluation spanning
various datasets.","['Srinivas Ravishankar', 'Chandrahas', 'Partha Pratim Talukdar']","['cs.AI', 'cs.LG', 'stat.ML']",2017-11-15 04:12:27+00:00
http://arxiv.org/abs/1711.05391v1,Semiblind subgraph reconstruction in Gaussian graphical models,"Consider a social network where only a few nodes (agents) have meaningful
interactions in the sense that the conditional dependency graph over node
attribute variables (behaviors) is sparse. A company that can only observe the
interactions between its own customers will generally not be able to accurately
estimate its customers' dependency subgraph: it is blinded to any external
interactions of its customers and this blindness creates false edges in its
subgraph. In this paper we address the semiblind scenario where the company has
access to a noisy summary of the complementary subgraph connecting external
agents, e.g., provided by a consolidator. The proposed framework applies to
other applications as well, including field estimation from a network of awake
and sleeping sensors and privacy-constrained information sharing over social
subnetworks. We propose a penalized likelihood approach in the context of a
graph signal obeying a Gaussian graphical models (GGM). We use a convex-concave
iterative optimization algorithm to maximize the penalized likelihood.","['Tianpei Xie', 'Sijia Liu', 'Alfred O. Hero III']","['cs.LG', 'stat.ML']",2017-11-15 03:04:51+00:00
http://arxiv.org/abs/1711.05376v2,Sliced Wasserstein Distance for Learning Gaussian Mixture Models,"Gaussian mixture models (GMM) are powerful parametric tools with many
applications in machine learning and computer vision. Expectation maximization
(EM) is the most popular algorithm for estimating the GMM parameters. However,
EM guarantees only convergence to a stationary point of the log-likelihood
function, which could be arbitrarily worse than the optimal solution. Inspired
by the relationship between the negative log-likelihood function and the
Kullback-Leibler (KL) divergence, we propose an alternative formulation for
estimating the GMM parameters using the sliced Wasserstein distance, which
gives rise to a new algorithm. Specifically, we propose minimizing the
sliced-Wasserstein distance between the mixture model and the data distribution
with respect to the GMM parameters. In contrast to the KL-divergence, the
energy landscape for the sliced-Wasserstein distance is more well-behaved and
therefore more suitable for a stochastic gradient descent scheme to obtain the
optimal GMM parameters. We show that our formulation results in parameter
estimates that are more robust to random initializations and demonstrate that
it can estimate high-dimensional data distributions more faithfully than the EM
algorithm.","['Soheil Kolouri', 'Gustavo K. Rohde', 'Heiko Hoffmann']","['cs.CV', 'cs.LG', 'stat.ML']",2017-11-15 01:33:01+00:00
http://arxiv.org/abs/1711.05374v1,Optimizing Kernel Machines using Deep Learning,"Building highly non-linear and non-parametric models is central to several
state-of-the-art machine learning systems. Kernel methods form an important
class of techniques that induce a reproducing kernel Hilbert space (RKHS) for
inferring non-linear models through the construction of similarity functions
from data. These methods are particularly preferred in cases where the training
data sizes are limited and when prior knowledge of the data similarities is
available. Despite their usefulness, they are limited by the computational
complexity and their inability to support end-to-end learning with a
task-specific objective. On the other hand, deep neural networks have become
the de facto solution for end-to-end inference in several learning paradigms.
In this article, we explore the idea of using deep architectures to perform
kernel machine optimization, for both computational efficiency and end-to-end
inferencing. To this end, we develop the DKMO (Deep Kernel Machine
Optimization) framework, that creates an ensemble of dense embeddings using
Nystrom kernel approximations and utilizes deep learning to generate
task-specific representations through the fusion of the embeddings.
Intuitively, the filters of the network are trained to fuse information from an
ensemble of linear subspaces in the RKHS. Furthermore, we introduce the kernel
dropout regularization to enable improved training convergence. Finally, we
extend this framework to the multiple kernel case, by coupling a global fusion
layer with pre-trained deep kernel machines for each of the constituent
kernels. Using case studies with limited training data, and lack of explicit
feature sources, we demonstrate the effectiveness of our framework over
conventional model inferencing techniques.","['Huan Song', 'Jayaraman J. Thiagarajan', 'Prasanna Sattigeri', 'Andreas Spanias']","['stat.ML', 'cs.LG']",2017-11-15 01:30:58+00:00
http://arxiv.org/abs/1711.05365v1,LIUBoost : Locality Informed Underboosting for Imbalanced Data Classification,"The problem of class imbalance along with class-overlapping has become a
major issue in the domain of supervised learning. Most supervised learning
algorithms assume equal cardinality of the classes under consideration while
optimizing the cost function and this assumption does not hold true for
imbalanced datasets which results in sub-optimal classification. Therefore,
various approaches, such as undersampling, oversampling, cost-sensitive
learning and ensemble based methods have been proposed for dealing with
imbalanced datasets. However, undersampling suffers from information loss,
oversampling suffers from increased runtime and potential overfitting while
cost-sensitive methods suffer due to inadequately defined cost assignment
schemes. In this paper, we propose a novel boosting based method called
LIUBoost. LIUBoost uses under sampling for balancing the datasets in every
boosting iteration like RUSBoost while incorporating a cost term for every
instance based on their hardness into the weight update formula minimizing the
information loss introduced by undersampling. LIUBoost has been extensively
evaluated on 18 imbalanced datasets and the results indicate significant
improvement over existing best performing method RUSBoost.","['Sajid Ahmed', 'Farshid Rayhan', 'Asif Mahbub', 'Md. Rafsan Jani', 'Swakkhar Shatabda', 'Dewan Md. Farid', 'Chowdhury Mofizur Rahman']","['cs.LG', 'stat.ML']",2017-11-15 00:44:41+00:00
http://arxiv.org/abs/1711.05363v2,Kernel Conditional Exponential Family,"A nonparametric family of conditional distributions is introduced, which
generalizes conditional exponential families using functional parameters in a
suitable RKHS. An algorithm is provided for learning the generalized natural
parameter, and consistency of the estimator is established in the well
specified case. In experiments, the new method generally outperforms a
competing approach with consistency guarantees, and is competitive with a deep
conditional density model on datasets that exhibit abrupt transitions and
heteroscedasticity.","['Michael Arbel', 'Arthur Gretton']",['stat.ML'],2017-11-15 00:32:08+00:00
http://arxiv.org/abs/1711.05355v2,Automatic Conflict Detection in Police Body-Worn Audio,"Automatic conflict detection has grown in relevance with the advent of
body-worn technology, but existing metrics such as turn-taking and overlap are
poor indicators of conflict in police-public interactions. Moreover, standard
techniques to compute them fall short when applied to such diversified and
noisy contexts. We develop a pipeline catered to this task combining adaptive
noise removal, non-speech filtering and new measures of conflict based on the
repetition and intensity of phrases in speech. We demonstrate the effectiveness
of our approach on body-worn audio data collected by the Los Angeles Police
Department.","['Alistair Letcher', 'Jelena Trišović', 'Collin Cademartori', 'Xi Chen', 'Jason Xu']","['eess.AS', 'cs.SD', 'stat.ML']",2017-11-14 23:28:05+00:00
http://arxiv.org/abs/1711.05323v1,On Optimal Generalizability in Parametric Learning,"We consider the parametric learning problem, where the objective of the
learner is determined by a parametric loss function. Employing empirical risk
minimization with possibly regularization, the inferred parameter vector will
be biased toward the training samples. Such bias is measured by the cross
validation procedure in practice where the data set is partitioned into a
training set used for training and a validation set, which is not used in
training and is left to measure the out-of-sample performance. A classical
cross validation strategy is the leave-one-out cross validation (LOOCV) where
one sample is left out for validation and training is done on the rest of the
samples that are presented to the learner, and this process is repeated on all
of the samples. LOOCV is rarely used in practice due to the high computational
complexity. In this paper, we first develop a computationally efficient
approximate LOOCV (ALOOCV) and provide theoretical guarantees for its
performance. Then we use ALOOCV to provide an optimization algorithm for
finding the regularizer in the empirical risk minimization framework. In our
numerical experiments, we illustrate the accuracy and efficiency of ALOOCV as
well as our proposed framework for the optimization of the regularizer.","['Ahmad Beirami', 'Meisam Razaviyayn', 'Shahin Shahrampour', 'Vahid Tarokh']","['stat.ML', 'cs.LG']",2017-11-14 21:37:03+00:00
http://arxiv.org/abs/1711.05233v1,A visual search engine for Bangladeshi laws,"Browsing and finding relevant information for Bangladeshi laws is a challenge
faced by all law students and researchers in Bangladesh, and by citizens who
want to learn about any legal procedure. Some law archives in Bangladesh are
digitized, but lack proper tools to organize the data meaningfully. We present
a text visualization tool that utilizes machine learning techniques to make the
searching of laws quicker and easier. Using Doc2Vec to layout law article
nodes, link mining techniques to visualize relevant citation networks, and
named entity recognition to quickly find relevant sections in long law
articles, our tool provides a faster and better search experience to the users.
Qualitative feedback from law researchers, students, and government officials
show promise for visually intuitive search tools in the context of
governmental, legal, and constitutional data in developing countries, where
digitized data does not necessarily pave the way towards an easy access to
information.","['Manash Kumar Mandal', 'Pinku Deb Nath', 'Arpeeta Shams Mizan', 'Nazmus Saquib']","['cs.HC', 'cs.CY', 'stat.ML']",2017-11-14 18:15:11+00:00
http://arxiv.org/abs/1711.05225v3,CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning,"We develop an algorithm that can detect pneumonia from chest X-rays at a
level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer
convolutional neural network trained on ChestX-ray14, currently the largest
publicly available chest X-ray dataset, containing over 100,000 frontal-view
X-ray images with 14 diseases. Four practicing academic radiologists annotate a
test set, on which we compare the performance of CheXNet to that of
radiologists. We find that CheXNet exceeds average radiologist performance on
the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and
achieve state of the art results on all 14 diseases.","['Pranav Rajpurkar', 'Jeremy Irvin', 'Kaylie Zhu', 'Brandon Yang', 'Hershel Mehta', 'Tony Duan', 'Daisy Ding', 'Aarti Bagul', 'Curtis Langlotz', 'Katie Shpanskaya', 'Matthew P. Lungren', 'Andrew Y. Ng']","['cs.CV', 'cs.LG', 'stat.ML']",2017-11-14 17:58:50+00:00
http://arxiv.org/abs/1711.09728v1,Evaluating gender portrayal in Bangladeshi TV,"Computer Vision and machine learning methods were previously used to reveal
screen presence of genders in TV and movies. In this work, using head pose,
gender detection, and skin color estimation techniques, we demonstrate that the
gender disparity in TV in a South Asian country such as Bangladesh exhibits
unique characteristics and is sometimes counter-intuitive to popular
perception. We demonstrate a noticeable discrepancy in female screen presence
in Bangladeshi TV advertisements and political talk shows. Further, contrary to
popular hypotheses, we demonstrate that lighter-toned skin colors are less
prevalent than darker complexions, and additionally, quantifiable body language
markers do not provide conclusive insights about gender dynamics. Overall,
these gender portrayal parameters reveal the different layers of onscreen
gender politics and can help direct incentives to address existing disparities
in a nuanced and targeted manner.","['Md. Naimul Hoque', 'Rawshan E Fatima', 'Manash Kumar Mandal', 'Nazmus Saquib']","['cs.CY', 'stat.ML']",2017-11-14 17:54:13+00:00
http://arxiv.org/abs/1711.05197v1,Joint Gaussian Processes for Biophysical Parameter Retrieval,"Solving inverse problems is central to geosciences and remote sensing.
Radiative transfer models (RTMs) represent mathematically the physical laws
which govern the phenomena in remote sensing applications (forward models). The
numerical inversion of the RTM equations is a challenging and computationally
demanding problem, and for this reason, often the application of a nonlinear
statistical regression is preferred. In general, regression models predict the
biophysical parameter of interest from the corresponding received radiance.
However, this approach does not employ the physical information encoded in the
RTMs. An alternative strategy, which attempts to include the physical
knowledge, consists in learning a regression model trained using data simulated
by an RTM code. In this work, we introduce a nonlinear nonparametric regression
model which combines the benefits of the two aforementioned approaches. The
inversion is performed taking into account jointly both real observations and
RTM-simulated data. The proposed Joint Gaussian Process (JGP) provides a solid
framework for exploiting the regularities between the two types of data. The
JGP automatically detects the relative quality of the simulated and real data,
and combines them accordingly. This occurs by learning an additional
hyper-parameter w.r.t. a standard GP model, and fitting parameters through
maximizing the pseudo-likelihood of the real observations. The resulting scheme
is both simple and robust, i.e., capable of adapting to different scenarios.
The advantages of the JGP method compared to benchmark strategies are shown
considering RTM-simulated and real observations in different experiments.
Specifically, we consider leaf area index (LAI) retrieval from Landsat data
combined with simulated data generated by the PROSAIL model.","['Daniel Heestermans Svendsen', 'Luca Martino', 'Manuel Campos-Taberner', 'Francisco Javier García-Haro', 'Gustau Camps-Valls']","['stat.ML', 'cs.LG']",2017-11-14 17:03:51+00:00
http://arxiv.org/abs/1711.05174v1,Near-Optimal Discrete Optimization for Experimental Design: A Regret Minimization Approach,"The experimental design problem concerns the selection of k points from a
potentially large design pool of p-dimensional vectors, so as to maximize the
statistical efficiency regressed on the selected k design points. Statistical
efficiency is measured by optimality criteria, including A(verage),
D(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the
T-optimality, exact optimization is NP-hard.
  We propose a polynomial-time regret minimization framework to achieve a
$(1+\varepsilon)$ approximation with only $O(p/\varepsilon^2)$ design points,
for all the optimality criteria above.
  In contrast, to the best of our knowledge, before our work, no
polynomial-time algorithm achieves $(1+\varepsilon)$ approximations for
D/E/G-optimality, and the best poly-time algorithm achieving
$(1+\varepsilon)$-approximation for A/V-optimality requires $k =
\Omega(p^2/\varepsilon)$ design points.","['Zeyuan Allen-Zhu', 'Yuanzhi Li', 'Aarti Singh', 'Yining Wang']","['stat.ML', 'cs.LG', 'stat.CO']",2017-11-14 16:21:57+00:00
http://arxiv.org/abs/1711.05170v1,On Extending Neural Networks with Loss Ensembles for Text Classification,"Ensemble techniques are powerful approaches that combine several weak
learners to build a stronger one. As a meta learning framework, ensemble
techniques can easily be applied to many machine learning techniques. In this
paper we propose a neural network extended with an ensemble loss function for
text classification. The weight of each weak loss function is tuned within the
training phase through the gradient propagation optimization method of the
neural network. The approach is evaluated on several text classification
datasets. We also evaluate its performance in various environments with several
degrees of label noise. Experimental results indicate an improvement of the
results and strong resilience against label noise in comparison with other
methods.","['Hamideh Hajiabadi', 'Diego Molla-Aliod', 'Reza Monsefi']","['cs.CL', 'cs.LG', 'stat.ML']",2017-11-14 16:19:34+00:00
http://arxiv.org/abs/1711.05150v1,Fast and reliable inference algorithm for hierarchical stochastic block models,"Network clustering reveals the organization of a network or corresponding
complex system with elements represented as vertices and interactions as edges
in a (directed, weighted) graph. Although the notion of clustering can be
somewhat loose, network clusters or groups are generally considered as nodes
with enriched interactions and edges sharing common patterns. Statistical
inference often treats groups as latent variables, with observed networks
generated from latent group structure, termed a stochastic block model.
Regardless of the definitions, statistical inference can be either translated
to modularity maximization, which is provably an NP-complete problem.
  Here we present scalable and reliable algorithms that recover hierarchical
stochastic block models fast and accurately. Our algorithm scales almost
linearly in number of edges, and inferred models were more accurate that other
scalable methods.","['Yongjin Park', 'Joel S. Bader']",['stat.ML'],2017-11-14 15:41:10+00:00
http://arxiv.org/abs/1711.05136v5,Deep Rewiring: Training very sparse deep networks,"Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.","['Guillaume Bellec', 'David Kappel', 'Wolfgang Maass', 'Robert Legenstein']","['cs.NE', 'cs.AI', 'cs.DC', 'cs.LG', 'stat.ML']",2017-11-14 15:02:47+00:00
http://arxiv.org/abs/1711.05102v1,The Multi-layer Information Bottleneck Problem,"The muti-layer information bottleneck (IB) problem, where information is
propagated (or successively refined) from layer to layer, is considered. Based
on information forwarded by the preceding layer, each stage of the network is
required to preserve a certain level of relevance with regards to a specific
hidden variable, quantified by the mutual information. The hidden variables and
the source can be arbitrarily correlated. The optimal trade-off between rates
of relevance and compression (or complexity) is obtained through a
single-letter characterization, referred to as the rate-relevance region.
Conditions of successive refinabilty are given. Binary source with BSC hidden
variables and binary source with BSC/BEC mixed hidden variables are both proved
to be successively refinable. We further extend our result to Guassian models.
A counterexample of successive refinability is also provided.","['Qianqian Yang', 'Pablo Piantanida', 'Deniz Gündüz']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2017-11-14 14:24:37+00:00
http://arxiv.org/abs/1711.05090v1,Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks,"This article presents the use of Answer Set Programming (ASP) to mine
sequential patterns. ASP is a high-level declarative logic programming paradigm
for high level encoding combinatorial and optimization problem solving as well
as knowledge representation and reasoning. Thus, ASP is a good candidate for
implementing pattern mining with background knowledge, which has been a data
mining issue for a long time. We propose encodings of the classical sequential
pattern mining tasks within two representations of embeddings (fill-gaps vs
skip-gaps) and for various kinds of patterns: frequent, constrained and
condensed. We compare the computational performance of these encodings with
each other to get a good insight into the efficiency of ASP encodings. The
results show that the fill-gaps strategy is better on real problems due to
lower memory consumption. Finally, compared to a constraint programming
approach (CPSM), another declarative programming paradigm, our proposal showed
comparable performance.","['Thomas Guyet', 'Yves Moinard', 'René Quiniou', 'Torsten Schaub']","['cs.AI', 'cs.DB', 'stat.ML']",2017-11-14 14:09:05+00:00
http://arxiv.org/abs/1711.05084v1,TripletGAN: Training Generative Model with Triplet Loss,"As an effective way of metric learning, triplet loss has been widely used in
many deep learning tasks, including face recognition and person-ReID, leading
to many states of the arts. The main innovation of triplet loss is using
feature map to replace softmax in the classification task. Inspired by this
concept, we propose here a new adversarial modeling method by substituting the
classification loss of discriminator with triplet loss. Theoretical proof based
on IPM (Integral probability metric) demonstrates that such setting will help
the generator converge to the given distribution theoretically under some
conditions. Moreover, since triplet loss requires the generator to maximize
distance within a class, we justify tripletGAN is also helpful to prevent mode
collapse through both theory and experiment.","['Gongze Cao', 'Yezhou Yang', 'Jie Lei', 'Cheng Jin', 'Yang Liu', 'Mingli Song']","['cs.LG', 'stat.ML']",2017-11-14 12:45:10+00:00
http://arxiv.org/abs/1711.05068v2,Robust Matrix Elastic Net based Canonical Correlation Analysis: An Effective Algorithm for Multi-View Unsupervised Learning,"This paper presents a robust matrix elastic net based canonical correlation
analysis (RMEN-CCA) for multiple view unsupervised learning problems, which
emphasizes the combination of CCA and the robust matrix elastic net (RMEN) used
as coupled feature selection. The RMEN-CCA leverages the strength of the RMEN
to distill naturally meaningful features without any prior assumption and to
measure effectively correlations between different 'views'. We can further
employ directly the kernel trick to extend the RMEN-CCA to the kernel scenario
with theoretical guarantees, which takes advantage of the kernel trick for
highly complicated nonlinear feature learning. Rather than simply incorporating
existing regularization minimization terms into CCA, this paper provides a new
learning paradigm for CCA and is the first to derive a coupled feature
selection based CCA algorithm that guarantees convergence. More significantly,
for CCA, the newly-derived RMEN-CCA bridges the gap between measurement of
relevance and coupled feature selection. Moreover, it is nontrivial to tackle
directly the RMEN-CCA by previous optimization approaches derived from its
sophisticated model architecture. Therefore, this paper further offers a bridge
between a new optimization problem and an existing efficient iterative
approach. As a consequence, the RMEN-CCA can overcome the limitation of CCA and
address large-scale and streaming data problems. Experimental results on four
popular competing datasets illustrate that the RMEN-CCA performs more
effectively and efficiently than do state-of-the-art approaches.","['Peng-Bo Zhang', 'Zhi-Xin Yang']","['cs.LG', 'stat.ML']",2017-11-14 12:00:53+00:00
http://arxiv.org/abs/1711.04992v2,Feature importance scores and lossless feature pruning using Banzhaf power indices,"Understanding the influence of features in machine learning is crucial to
interpreting models and selecting the best features for classification. In this
work we propose the use of principles from coalitional game theory to reason
about importance of features. In particular, we propose the use of the Banzhaf
power index as a measure of influence of features on the outcome of a
classifier. We show that features having Banzhaf power index of zero can be
losslessly pruned without damage to classifier accuracy. Computing the power
indices does not require having access to data samples. However, if samples are
available, the indices can be empirically estimated. We compute Banzhaf power
indices for a neural network classifier on real-life data, and compare the
results with gradient-based feature saliency, and coefficients of a logistic
regression model with $L_1$ regularization.","['Bogdan Kulynych', 'Carmela Troncoso']","['stat.ML', 'cs.LG']",2017-11-14 08:24:01+00:00
http://arxiv.org/abs/1711.04979v2,Quantum transport senses community structure in networks,"Quantum time evolution exhibits rich physics, attributable to the interplay
between the density and phase of a wave function. However, unlike classical
heat diffusion, the wave nature of quantum mechanics has not yet been
extensively explored in modern data analysis. We propose that the Laplace
transform of quantum transport (QT) can be used to construct an ensemble of
maps from a given complex network to a circle $S^1$, such that closely-related
nodes on the network are grouped into sharply concentrated clusters on $S^1$.
The resulting QT clustering (QTC) algorithm is as powerful as the
state-of-the-art spectral clustering in discerning complex geometric patterns
and more robust when clusters show strong density variations or heterogeneity
in size. The observed phenomenon of QTC can be interpreted as a collective
behavior of the microscopic nodes that evolve as macroscopic cluster orbitals
in an effective tight-binding model recapitulating the network. Python source
code implementing the algorithm and examples are available at
https://github.com/jssong-lab/QTC.","['Chenchao Zhao', 'Jun S. Song']","['quant-ph', 'cond-mat.other', 'cs.DS', 'q-bio.QM', 'stat.ML']",2017-11-14 07:03:05+00:00
http://arxiv.org/abs/1711.04969v2,Straggler Mitigation in Distributed Optimization Through Data Encoding,"Slow running or straggler tasks can significantly reduce computation speed in
distributed computation. Recently, coding-theory-inspired approaches have been
applied to mitigate the effect of straggling, through embedding redundancy in
certain linear computational steps of the optimization algorithm, thus
completing the computation without waiting for the stragglers. In this paper,
we propose an alternate approach where we embed the redundancy directly in the
data itself, and allow the computation to proceed completely oblivious to
encoding. We propose several encoding schemes, and demonstrate that popular
batch algorithms, such as gradient descent and L-BFGS, applied in a
coding-oblivious manner, deterministically achieve sample path linear
convergence to an approximate solution of the original problem, using an
arbitrarily varying subset of the nodes at each iteration. Moreover, this
approximation can be controlled by the amount of redundancy and the number of
nodes used in each iteration. We provide experimental results demonstrating the
advantage of the approach over uncoded and data replication strategies.","['Can Karakus', 'Yifan Sun', 'Suhas Diggavi', 'Wotao Yin']","['stat.ML', 'cs.DC', 'cs.IT', 'cs.LG', 'math.IT']",2017-11-14 06:29:41+00:00
http://arxiv.org/abs/1711.04965v1,Near-optimal sample complexity for convex tensor completion,"We analyze low rank tensor completion (TC) using noisy measurements of a
subset of the tensor. Assuming a rank-$r$, order-$d$, $N \times N \times \cdots
\times N$ tensor where $r=O(1)$, the best sampling complexity that was achieved
is $O(N^{\frac{d}{2}})$, which is obtained by solving a tensor nuclear-norm
minimization problem. However, this bound is significantly larger than the
number of free variables in a low rank tensor which is $O(dN)$. In this paper,
we show that by using an atomic-norm whose atoms are rank-$1$ sign tensors, one
can obtain a sample complexity of $O(dN)$. Moreover, we generalize the matrix
max-norm definition to tensors, which results in a max-quasi-norm (max-qnorm)
whose unit ball has small Rademacher complexity. We prove that solving a
constrained least squares estimation using either the convex atomic-norm or the
nonconvex max-qnorm results in optimal sample complexity for the problem of
low-rank tensor completion. Furthermore, we show that these bounds are nearly
minimax rate-optimal. We also provide promising numerical results for max-qnorm
constrained tensor completion, showing improved recovery results compared to
matricization and alternating least squares.","['Navid Ghadermarzy', 'Yaniv Plan', 'Özgür Yılmaz']","['cs.LG', 'math.OC', 'stat.ML', '62H12, 94A15, 15A69']",2017-11-14 06:20:05+00:00
http://arxiv.org/abs/1711.04955v2,Scalable Peaceman-Rachford Splitting Method with Proximal Terms,"Along with developing of Peaceman-Rachford Splittling Method (PRSM), many
batch algorithms based on it have been studied very deeply. But almost no
algorithm focused on the performance of stochastic version of PRSM. In this
paper, we propose a new stochastic algorithm based on PRSM, prove its
convergence rate in ergodic sense, and test its performance on both artificial
and real data. We show that our proposed algorithm, Stochastic Scalable PRSM
(SS-PRSM), enjoys the $O(1/K)$ convergence rate, which is the same as those
newest stochastic algorithms that based on ADMM but faster than general
Stochastic ADMM (which is $O(1/\sqrt{K})$). Our algorithm also owns wide
flexibility, outperforms many state-of-the-art stochastic algorithms coming
from ADMM, and has low memory cost in large-scale splitting optimization
problems.","['Sen Na', 'Mingyuan Ma', 'Mladen Kolar']","['stat.ML', 'cs.LG']",2017-11-14 05:45:32+00:00
http://arxiv.org/abs/1711.04952v2,Sparse High-Dimensional Linear Regression. Algorithmic Barriers and a Local Search Algorithm,"We consider a sparse high dimensional regression model where the goal is to
recover a $k$-sparse unknown vector $\beta^*$ from $n$ noisy linear
observations of the form $Y=X\beta^*+W \in \mathbb{R}^n$ where $X \in
\mathbb{R}^{n \times p}$ has iid $N(0,1)$ entries and $W \in \mathbb{R}^n$ has
iid $N(0,\sigma^2)$ entries. Under certain assumptions on the parameters, an
intriguing assymptotic gap appears between the minimum value of $n$, call it
$n^*$, for which the recovery is information theoretically possible, and the
minimum value of $n$, call it $n_{\mathrm{alg}}$, for which an efficient
algorithm is known to provably recover $\beta^*$. In \cite{gamarnikzadik} it
was conjectured that the gap is not artificial, in the sense that for sample
sizes $n \in [n^*,n_{\mathrm{alg}}]$ the problem is algorithmically hard.
  We support this conjecture in two ways. Firstly, we show that the optimal
solution of the LASSO provably fails to $\ell_2$-stably recover the unknown
vector $\beta^*$ when $n \in [n^*,c n_{\mathrm{alg}}]$, for some sufficiently
small constant $c>0$. Secondly, we establish that $n_{\mathrm{alg}}$, up to a
multiplicative constant factor, is a phase transition point for the appearance
of a certain Overlap Gap Property (OGP) over the space of $k$-sparse vectors.
The presence of such an Overlap Gap Property phase transition, which originates
in statistical physics, is known to provide evidence of an algorithmic
hardness. Finally we show that if $n>C n_{\mathrm{alg}}$ for some large enough
constant $C>0$, a very simple algorithm based on a local search improvement
rule is able both to $\ell_2$-stably recover the unknown vector $\beta^*$ and
to infer correctly its support, adding it to the list of provably successful
algorithms for the high dimensional linear regression problem.","['David Gamarnik', 'Ilias Zadik']","['math.ST', 'math.PR', 'stat.ML', 'stat.TH']",2017-11-14 05:20:20+00:00
http://arxiv.org/abs/1711.04934v2,Statistically Optimal and Computationally Efficient Low Rank Tensor Completion from Noisy Entries,"In this article, we develop methods for estimating a low rank tensor from
noisy observations on a subset of its entries to achieve both statistical and
computational efficiencies. There have been a lot of recent interests in this
problem of noisy tensor completion. Much of the attention has been focused on
the fundamental computational challenges often associated with problems
involving higher order tensors, yet very little is known about their
statistical performance. To fill in this void, in this article, we characterize
the fundamental statistical limits of noisy tensor completion by establishing
minimax optimal rates of convergence for estimating a $k$th order low rank
tensor under the general $\ell_p$ ($1\le p\le 2$) norm which suggest
significant room for improvement over the existing approaches. Furthermore, we
propose a polynomial-time computable estimating procedure based upon power
iteration and a second-order spectral initialization that achieves the optimal
rates of convergence. Our method is fairly easy to implement and numerical
experiments are presented to further demonstrate the practical merits of our
estimator.","['Dong Xia', 'Ming Yuan', 'Cun-Hui Zhang']","['stat.ML', 'cs.IT', 'math.IT', 'math.ST', 'stat.ME', 'stat.TH']",2017-11-14 03:46:05+00:00
http://arxiv.org/abs/1711.04913v1,pyLEMMINGS: Large Margin Multiple Instance Classification and Ranking for Bioinformatics Applications,"Motivation: A major challenge in the development of machine learning based
methods in computational biology is that data may not be accurately labeled due
to the time and resources required for experimentally annotating properties of
proteins and DNA sequences. Standard supervised learning algorithms assume
accurate instance-level labeling of training data. Multiple instance learning
is a paradigm for handling such labeling ambiguities. However, the widely used
large-margin classification methods for multiple instance learning are
heuristic in nature with high computational requirements. In this paper, we
present stochastic sub-gradient optimization large margin algorithms for
multiple instance classification and ranking, and provide them in a software
suite called pyLEMMINGS.
  Results: We have tested pyLEMMINGS on a number of bioinformatics problems as
well as benchmark datasets. pyLEMMINGS has successfully been able to identify
functionally important segments of proteins: binding sites in Calmodulin
binding proteins, prion forming regions, and amyloid cores. pyLEMMINGS achieves
state-of-the-art performance in all these tasks, demonstrating the value of
multiple instance learning. Furthermore, our method has shown more than
100-fold improvement in terms of running time as compared to heuristic
solutions with improved accuracy over benchmark datasets.
  Availability and Implementation: pyLEMMINGS python package is available for
download at: http://faculty.pieas.edu.pk/fayyaz/software.html#pylemmings.","['Amina Asif', 'Wajid Arshad Abbasi', 'Farzeen Munir', 'Asa Ben-Hur', 'Fayyaz ul Amir Afsar Minhas']","['cs.LG', 'stat.ML']",2017-11-14 02:41:01+00:00
http://arxiv.org/abs/1711.04894v1,Sobolev GAN,"We propose a new Integral Probability Metric (IPM) between distributions: the
Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions
for functions (critic) restricted to a Sobolev ball defined with respect to a
dominant measure $\mu$. We show that the Sobolev IPM compares two distributions
in high dimensions based on weighted conditional Cumulative Distribution
Functions (CDF) of each coordinate on a leave one out basis. The Dominant
measure $\mu$ plays a crucial role as it defines the support on which
conditional CDFs are compared. Sobolev IPM can be seen as an extension of the
one dimensional Von-Mises Cram\'er statistics to high dimensional
distributions. We show how Sobolev IPM can be used to train Generative
Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied
by Sobolev IPM in text generation. Finally we show that a variant of Sobolev
GAN achieves competitive results in semi-supervised learning on CIFAR-10,
thanks to the smoothness enforced on the critic by Sobolev GAN which relates to
Laplacian regularization.","['Youssef Mroueh', 'Chun-Liang Li', 'Tom Sercu', 'Anant Raj', 'Yu Cheng']","['cs.LG', 'stat.ML']",2017-11-14 00:41:09+00:00
http://arxiv.org/abs/1711.04887v1,STARK: Structured Dictionary Learning Through Rank-one Tensor Recovery,"In recent years, a class of dictionaries have been proposed for
multidimensional (tensor) data representation that exploit the structure of
tensor data by imposing a Kronecker structure on the dictionary underlying the
data. In this work, a novel algorithm called ""STARK"" is provided to learn
Kronecker structured dictionaries that can represent tensors of any order. By
establishing that the Kronecker product of any number of matrices can be
rearranged to form a rank-1 tensor, we show that Kronecker structure can be
enforced on the dictionary by solving a rank-1 tensor recovery problem. Because
rank-1 tensor recovery is a challenging nonconvex problem, we resort to solving
a convex relaxation of this problem. Empirical experiments on synthetic and
real data show promising results for our proposed algorithm.","['Mohsen Ghassemi', 'Zahra Shakeri', 'Anand D. Sarwate', 'Waheed U. Bajwa']","['stat.ML', 'cs.LG']",2017-11-13 23:17:51+00:00
http://arxiv.org/abs/1712.01081v1,Determinants of Mobile Money Adoption in Pakistan,"In this work, we analyze the problem of adoption of mobile money in Pakistan
by using the call detail records of a major telecom company as our input. Our
results highlight the fact that different sections of the society have
different patterns of adoption of digital financial services but user mobility
related features are the most important one when it comes to adopting and using
mobile money services.","['Muhammad Raza Khan', 'Joshua Blumenstock']",['stat.ML'],2017-11-13 23:02:03+00:00
http://arxiv.org/abs/1711.04877v3,Estimating prediction error for complex samples,"With a growing interest in using non-representative samples to train
prediction models for numerous outcomes it is necessary to account for the
sampling design that gives rise to the data in order to assess the generalized
predictive utility of a proposed prediction rule. After learning a prediction
rule based on a non-uniform sample, it is of interest to estimate the rule's
error rate when applied to unobserved members of the population. Efron (1986)
proposed a general class of covariance penalty inflated prediction error
estimators that assume the available training data are representative of the
target population for which the prediction rule is to be applied. We extend
Efron's estimator to the complex sample context by incorporating
Horvitz-Thompson sampling weights and show that it is consistent for the true
generalization error rate when applied to the underlying superpopulation. The
resulting Horvitz-Thompson-Efron (HTE) estimator is equivalent to dAIC, a
recent extension of AIC to survey sampling data, but is more widely applicable.
The proposed methodology is assessed with simulations and is applied to models
predicting renal function obtained from the large-scale NHANES survey.","['Andrew Holbrook', 'Thomas Lumley', 'Daniel Gillen']","['stat.ME', 'stat.ML']",2017-11-13 22:30:47+00:00
http://arxiv.org/abs/1711.04855v1,Modeling Human Categorization of Natural Images Using Deep Feature Representations,"Over the last few decades, psychologists have developed sophisticated formal
models of human categorization using simple artificial stimuli. In this paper,
we use modern machine learning methods to extend this work into the realm of
naturalistic stimuli, enabling human categorization to be studied over the
complex visual domain in which it evolved and developed. We show that
representations derived from a convolutional neural network can be used to
model behavior over a database of >300,000 human natural image classifications,
and find that a group of models based on these representations perform well,
near the reliability of human judgments. Interestingly, this group includes
both exemplar and prototype models, contrasting with the dominance of exemplar
models in previous work. We are able to improve the performance of the
remaining models by preprocessing neural network representations to more
closely capture human similarity judgments.","['Ruairidh M. Battleday', 'Joshua C. Peterson', 'Thomas L. Griffiths']","['cs.CV', 'cs.LG', 'stat.ML']",2017-11-13 21:18:29+00:00
http://arxiv.org/abs/1711.04851v3,Learning and Visualizing Localized Geometric Features Using 3D-CNN: An Application to Manufacturability Analysis of Drilled Holes,"3D Convolutional Neural Networks (3D-CNN) have been used for object
recognition based on the voxelized shape of an object. However, interpreting
the decision making process of these 3D-CNNs is still an infeasible task. In
this paper, we present a unique 3D-CNN based Gradient-weighted Class Activation
Mapping method (3D-GradCAM) for visual explanations of the distinct local
geometric features of interest within an object. To enable efficient learning
of 3D geometries, we augment the voxel data with surface normals of the object
boundary. We then train a 3D-CNN with this augmented data and identify the
local features critical for decision-making using 3D GradCAM. An application of
this feature identification framework is to recognize difficult-to-manufacture
drilled hole features in a complex CAD geometry. The framework can be extended
to identify difficult-to-manufacture features at multiple spatial scales
leading to a real-time design for manufacturability decision support system.","['Sambit Ghadai', 'Aditya Balu', 'Adarsh Krishnamurthy', 'Soumik Sarkar']","['stat.ML', 'cs.CV', 'cs.LG']",2017-11-13 21:05:39+00:00
http://arxiv.org/abs/1711.04845v1,Invariances and Data Augmentation for Supervised Music Transcription,"This paper explores a variety of models for frame-based music transcription,
with an emphasis on the methods needed to reach state-of-the-art on human
recordings. The translation-invariant network discussed in this paper, which
combines a traditional filterbank with a convolutional neural network, was the
top-performing model in the 2017 MIREX Multiple Fundamental Frequency
Estimation evaluation. This class of models shares parameters in the
log-frequency domain, which exploits the frequency invariance of music to
reduce the number of model parameters and avoid overfitting to the training
data. All models in this paper were trained with supervision by labeled data
from the MusicNet dataset, augmented by random label-preserving pitch-shift
transformations.","['John Thickstun', 'Zaid Harchaoui', 'Dean Foster', 'Sham M. Kakade']","['stat.ML', 'cs.LG', 'cs.SD', 'eess.AS']",2017-11-13 20:47:57+00:00
http://arxiv.org/abs/1711.04837v2,Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals,"On a periodic basis, publicly traded companies are required to report
fundamentals: financial data such as revenue, operating income, debt, among
others. These data points provide some insight into the financial health of a
company. Academic research has identified some factors, i.e. computed features
of the reported data, that are known through retrospective analysis to
outperform the market average. Two popular factors are the book value
normalized by market capitalization (book-to-market) and the operating income
normalized by the enterprise value (EBIT/EV). In this paper: we first show
through simulation that if we could (clairvoyantly) select stocks using factors
calculated on future fundamentals (via oracle), then our portfolios would far
outperform a standard factor approach. Motivated by this analysis, we train
deep neural networks to forecast future fundamentals based on a trailing
5-years window. Quantitative analysis demonstrates a significant improvement in
MSE over a naive strategy. Moreover, in retrospective analysis using an
industry-grade stock portfolio simulator (backtester), we show an improvement
in compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor
model.","['John Alberg', 'Zachary C. Lipton']","['stat.ML', 'cs.LG', 'cs.NE']",2017-11-13 20:30:02+00:00
http://arxiv.org/abs/1711.04817v2,Sparse quadratic classification rules via linear dimension reduction,"We consider the problem of high-dimensional classification between the two
groups with unequal covariance matrices. Rather than estimating the full
quadratic discriminant rule, we propose to perform simultaneous variable
selection and linear dimension reduction on original data, with the subsequent
application of quadratic discriminant analysis on the reduced space. In
contrast to quadratic discriminant analysis, the proposed framework doesn't
require estimation of precision matrices and scales linearly with the number of
measurements, making it especially attractive for the use on high-dimensional
datasets. We support the methodology with theoretical guarantees on variable
selection consistency, and empirical comparison with competing approaches. We
apply the method to gene expression data of breast cancer patients, and confirm
the crucial importance of ESR1 gene in differentiating estrogen receptor
status.","['Irina Gaynanova', 'Tianying Wang']","['stat.ML', 'stat.ME']",2017-11-13 19:52:06+00:00
http://arxiv.org/abs/1711.04810v2,"""Found in Translation"": Predicting Outcomes of Complex Organic Chemistry Reactions using Neural Sequence-to-Sequence Models","There is an intuitive analogy of an organic chemist's understanding of a
compound and a language speaker's understanding of a word. Consequently, it is
possible to introduce the basic concepts and analyze potential impacts of
linguistic analysis to the world of organic chemistry. In this work, we cast
the reaction prediction task as a translation problem by introducing a
template-free sequence-to-sequence model, trained end-to-end and fully
data-driven. We propose a novel way of tokenization, which is arbitrarily
extensible with reaction information. With this approach, we demonstrate
results superior to the state-of-the-art solution by a significant margin on
the top-1 accuracy. Specifically, our approach achieves an accuracy of 80.1%
without relying on auxiliary knowledge such as reaction templates. Also, 66.4%
accuracy is reached on a larger and noisier dataset.","['Philippe Schwaller', 'Theophile Gaudin', 'David Lanyi', 'Costas Bekas', 'Teodoro Laino']","['cs.LG', 'stat.ML']",2017-11-13 19:38:14+00:00
http://arxiv.org/abs/1711.04755v1,ACtuAL: Actor-Critic Under Adversarial Learning,"Generative Adversarial Networks (GANs) are a powerful framework for deep
generative modeling. Posed as a two-player minimax problem, GANs are typically
trained end-to-end on real-valued data and can be used to train a generator of
high-dimensional and realistic images. However, a major limitation of GANs is
that training relies on passing gradients from the discriminator through the
generator via back-propagation. This makes it fundamentally difficult to train
GANs with discrete data, as generation in this case typically involves a
non-differentiable function. These difficulties extend to the reinforcement
learning setting when the action space is composed of discrete decisions. We
address these issues by reframing the GAN framework so that the generator is no
longer trained using gradients through the discriminator, but is instead
trained using a learned critic in the actor-critic framework with a Temporal
Difference (TD) objective. This is a natural fit for sequence modeling and we
use it to achieve improvements on language modeling tasks over the standard
Teacher-Forcing methods.","['Anirudh Goyal', 'Nan Rosemary Ke', 'Alex Lamb', 'R Devon Hjelm', 'Chris Pal', 'Joelle Pineau', 'Yoshua Bengio']","['stat.ML', 'cs.LG']",2017-11-13 18:49:06+00:00
http://arxiv.org/abs/1711.04735v1,Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice,"It is well known that the initialization of weights in deep neural networks
can have a dramatic impact on learning speed. For example, ensuring the mean
squared singular value of a network's input-output Jacobian is $O(1)$ is
essential for avoiding the exponential vanishing or explosion of gradients. The
stronger condition that all singular values of the Jacobian concentrate near
$1$ is a property known as dynamical isometry. For deep linear networks,
dynamical isometry can be achieved through orthogonal weight initialization and
has been shown to dramatically speed up learning; however, it has remained
unclear how to extend these results to the nonlinear setting. We address this
question by employing powerful tools from free probability theory to compute
analytically the entire singular value distribution of a deep network's
input-output Jacobian. We explore the dependence of the singular value
distribution on the depth of the network, the weight initialization, and the
choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable
of dynamical isometry. On the other hand, sigmoidal networks can achieve
isometry, but only with orthogonal weight initialization. Moreover, we
demonstrate empirically that deep nonlinear networks achieving dynamical
isometry learn orders of magnitude faster than networks that do not. Indeed, we
show that properly-initialized deep sigmoidal networks consistently outperform
deep ReLU networks. Overall, our analysis reveals that controlling the entire
distribution of Jacobian singular values is an important design consideration
in deep learning.","['Jeffrey Pennington', 'Samuel S. Schoenholz', 'Surya Ganguli']","['cs.LG', 'stat.ML']",2017-11-13 18:06:09+00:00
http://arxiv.org/abs/1711.04712v1,"Randomized Near Neighbor Graphs, Giant Components, and Applications in Data Science","If we pick $n$ random points uniformly in $[0,1]^d$ and connect each point to
its $k-$nearest neighbors, then it is well known that there exists a giant
connected component with high probability. We prove that in $[0,1]^d$ it
suffices to connect every point to $ c_{d,1} \log{\log{n}}$ points chosen
randomly among its $ c_{d,2} \log{n}-$nearest neighbors to ensure a giant
component of size $n - o(n)$ with high probability. This construction yields a
much sparser random graph with $\sim n \log\log{n}$ instead of $\sim n \log{n}$
edges that has comparable connectivity properties. This result has nontrivial
implications for problems in data science where an affinity matrix is
constructed: instead of picking the $k-$nearest neighbors, one can often pick
$k' \ll k$ random points out of the $k-$nearest neighbors without sacrificing
efficiency. This can massively simplify and accelerate computation, we
illustrate this with several numerical examples.","['George C. Linderman', 'Gal Mishne', 'Yuval Kluger', 'Stefan Steinerberger']","['math.CO', 'cs.DM', 'cs.DS', 'math.PR', 'stat.ML']",2017-11-13 17:22:00+00:00
http://arxiv.org/abs/1711.04686v1,Weightless: Lossy Weight Encoding For Deep Neural Network Compression,"The large memory requirements of deep neural networks limit their deployment
and adoption on many devices. Model compression methods effectively reduce the
memory requirements of these models, usually through applying transformations
such as weight pruning or quantization. In this paper, we present a novel
scheme for lossy weight encoding which complements conventional compression
techniques. The encoding is based on the Bloomier filter, a probabilistic data
structure that can save space at the cost of introducing random errors.
Leveraging the ability of neural networks to tolerate these imperfections and
by re-training around the errors, the proposed technique, Weightless, can
compress DNN weights by up to 496x with the same model accuracy. This results
in up to a 1.51x improvement over the state-of-the-art.","['Brandon Reagen', 'Udit Gupta', 'Robert Adolf', 'Michael M. Mitzenmacher', 'Alexander M. Rush', 'Gu-Yeon Wei', 'David Brooks']","['cs.LG', 'stat.ML']",2017-11-13 16:28:37+00:00
http://arxiv.org/abs/1711.04683v1,Tensor Decompositions for Modeling Inverse Dynamics,"Modeling inverse dynamics is crucial for accurate feedforward robot control.
The model computes the necessary joint torques, to perform a desired movement.
The highly non-linear inverse function of the dynamical system can be
approximated using regression techniques. We propose as regression method a
tensor decomposition model that exploits the inherent three-way interaction of
positions x velocities x accelerations. Most work in tensor factorization has
addressed the decomposition of dense tensors. In this paper, we build upon the
decomposition of sparse tensors, with only small amounts of nonzero entries.
The decomposition of sparse tensors has successfully been used in relational
learning, e.g., the modeling of large knowledge graphs. Recently, the approach
has been extended to multi-class classification with discrete input variables.
Representing the data in high dimensional sparse tensors enables the
approximation of complex highly non-linear functions. In this paper we show how
the decomposition of sparse tensors can be applied to regression problems.
Furthermore, we extend the method to continuous inputs, by learning a mapping
from the continuous inputs to the latent representations of the tensor
decomposition, using basis functions. We evaluate our proposed model on a
dataset with trajectories from a seven degrees of freedom SARCOS robot arm. Our
experimental results show superior performance of the proposed functional
tensor model, compared to challenging state-of-the art methods.","['Stephan Baier', 'Volker Tresp']","['cs.LG', 'cs.RO', 'cs.SY', 'stat.ML']",2017-11-13 16:26:51+00:00
http://arxiv.org/abs/1711.04679v1,Attention-based Information Fusion using Multi-Encoder-Decoder Recurrent Neural Networks,"With the rising number of interconnected devices and sensors, modeling
distributed sensor networks is of increasing interest. Recurrent neural
networks (RNN) are considered particularly well suited for modeling sensory and
streaming data. When predicting future behavior, incorporating information from
neighboring sensor stations is often beneficial. We propose a new RNN based
architecture for context specific information fusion across multiple spatially
distributed sensor stations. Hereby, latent representations of multiple local
models, each modeling one sensor station, are jointed and weighted, according
to their importance for the prediction. The particular importance is assessed
depending on the current context using a separate attention function. We
demonstrate the effectiveness of our model on three different real-world sensor
network datasets.","['Stephan Baier', 'Sigurd Spieckermann', 'Volker Tresp']","['cs.LG', 'stat.ML']",2017-11-13 16:17:45+00:00
http://arxiv.org/abs/1711.04674v2,Model Criticism in Latent Space,"Model criticism is usually carried out by assessing if replicated data
generated under the fitted model looks similar to the observed data, see e.g.
Gelman, Carlin, Stern, and Rubin [2004, p. 165]. This paper presents a method
for latent variable models by pulling back the data into the space of latent
variables, and carrying out model criticism in that space. Making use of a
model's structure enables a more direct assessment of the assumptions made in
the prior and likelihood. We demonstrate the method with examples of model
criticism in latent space applied to factor analysis, linear dynamical systems
and Gaussian processes.","['Sohan Seth', 'Iain Murray', 'Christopher K. I. Williams']",['stat.ML'],2017-11-13 16:07:14+00:00
http://arxiv.org/abs/1711.04623v3,Three Factors Influencing Minima in SGD,"We investigate the dynamical and convergent properties of stochastic gradient
descent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the
relation between learning rate, batch size and the properties of the final
minima, such as width or generalization, remains an open question. In order to
tackle this problem we investigate the previously proposed approximation of SGD
by a stochastic differential equation (SDE). We theoretically argue that three
factors - learning rate, batch size and gradient covariance - influence the
minima found by SGD. In particular we find that the ratio of learning rate to
batch size is a key determinant of SGD dynamics and of the width of the final
minima, and that higher values of the ratio lead to wider minima and often
better generalization. We confirm these findings experimentally. Further, we
include experiments which show that learning rate schedules can be replaced
with batch size schedules and that the ratio of learning rate to batch size is
an important factor influencing the memorization process.","['Stanisław Jastrzębski', 'Zachary Kenton', 'Devansh Arpit', 'Nicolas Ballas', 'Asja Fischer', 'Yoshua Bengio', 'Amos Storkey']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2017-11-13 15:11:56+00:00
http://arxiv.org/abs/1711.11408v1,The identity of information: how deterministic dependencies constrain information synergy and redundancy,"Understanding how different information sources together transmit information
is crucial in many domains. For example, understanding the neural code requires
characterizing how different neurons contribute unique, redundant, or
synergistic pieces of information about sensory or behavioral variables.
Williams and Beer (2010) proposed a partial information decomposition (PID)
which separates the mutual information that a set of sources contains about a
set of targets into nonnegative terms interpretable as these pieces.
Quantifying redundancy requires assigning an identity to different information
pieces, to assess when information is common across sources. Harder et al.
(2013) proposed an identity axiom stating that there cannot be redundancy
between two independent sources about a copy of themselves. However,
Bertschinger et al. (2012) showed that with a deterministically related
sources-target copy this axiom is incompatible with ensuring PID nonnegativity.
Here we study systematically the effect of deterministic target-sources
dependencies. We introduce two synergy stochasticity axioms that generalize the
identity axiom, and we derive general expressions separating stochastic and
deterministic PID components. Our analysis identifies how negative terms can
originate from deterministic dependencies and shows how different assumptions
on information identity, implicit in the stochasticity and identity axioms,
determine the PID structure. The implications for studying neural coding are
discussed.","['Daniel Chicharro', 'Giuseppe Pica', 'Stefano Panzeri']","['q-bio.NC', 'stat.ML', '94A15, 94A17']",2017-11-13 12:27:42+00:00
http://arxiv.org/abs/1711.04528v1,Simple And Efficient Architecture Search for Convolutional Neural Networks,"Neural networks have recently had a lot of success for many tasks. However,
neural network architectures that perform well are still typically designed
manually by experts in a cumbersome trial-and-error process. We propose a new
method to automatically search for well-performing CNN architectures based on a
simple hill climbing procedure whose operators apply network morphisms,
followed by short optimization runs by cosine annealing. Surprisingly, this
simple method yields competitive results, despite only requiring resources in
the same order of magnitude as training a single network. E.g., on CIFAR-10,
our method designs and trains networks with an error rate below 6% in only 12
hours on a single GPU; training for one day reduces this error further, to
almost 5%.","['Thomas Elsken', 'Jan-Hendrik Metzen', 'Frank Hutter']","['stat.ML', 'cs.AI', 'cs.LG']",2017-11-13 11:23:36+00:00
http://arxiv.org/abs/1711.09715v3,Guided Machine Learning for power grid segmentation,"The segmentation of large scale power grids into zones is crucial for control
room operators when managing the grid complexity near real time. In this paper
we propose a new method in two steps which is able to automatically do this
segmentation, while taking into account the real time context, in order to help
them handle shifting dynamics. Our method relies on a ""guided"" machine learning
approach. As a first step, we define and compute a task specific ""Influence
Graph"" in a guided manner. We indeed simulate on a grid state chosen
interventions, representative of our task of interest (managing active power
flows in our case). For visualization and interpretation, we then build a
higher representation of the grid relevant to this task by applying the graph
community detection algorithm \textit{Infomap} on this Influence Graph. To
illustrate our method and demonstrate its practical interest, we apply it on
commonly used systems, the IEEE-14 and IEEE-118. We show promising and original
interpretable results, especially on the previously well studied RTS-96 system
for grid segmentation. We eventually share initial investigation and results on
a large-scale system, the French power grid, whose segmentation had a
surprising resemblance with RTE's historical partitioning.","['Antoine Marot', 'Sami Tazi', 'Benjamin Donnot', 'Patrick Panciatici']","['stat.AP', 'stat.ML']",2017-11-13 10:44:01+00:00
http://arxiv.org/abs/1711.04489v1,A Parallel Best-Response Algorithm with Exact Line Search for Nonconvex Sparsity-Regularized Rank Minimization,"In this paper, we propose a convergent parallel best-response algorithm with
the exact line search for the nondifferentiable nonconvex sparsity-regularized
rank minimization problem. On the one hand, it exhibits a faster convergence
than subgradient algorithms and block coordinate descent algorithms. On the
other hand, its convergence to a stationary point is guaranteed, while ADMM
algorithms only converge for convex problems. Furthermore, the exact line
search procedure in the proposed algorithm is performed efficiently in
closed-form to avoid the meticulous choice of stepsizes, which is however a
common bottleneck in subgradient algorithms and successive convex approximation
algorithms. Finally, the proposed algorithm is numerically tested.","['Yang Yang', 'Marius Pesavento']","['cs.DC', 'cs.LG', 'stat.ML']",2017-11-13 09:28:43+00:00
http://arxiv.org/abs/1711.04460v3,Blind Source Separation Using Mixtures of Alpha-Stable Distributions,"We propose a new blind source separation algorithm based on mixtures of
alpha-stable distributions. Complex symmetric alpha-stable distributions have
been recently showed to better model audio signals in the time-frequency domain
than classical Gaussian distributions thanks to their larger dynamic range.
However, inference of these models is notoriously hard to perform because their
probability density functions do not have a closed-form expression in general.
Here, we introduce a novel method for estimating mixture of alpha-stable
distributions based on characteristic function matching. We apply this to the
blind estimation of binary masks in individual frequency bands from
multichannel convolutive audio mixes. We show that the proposed method yields
better separation performance than Gaussian-based binary-masking methods.","['Nicolas Keriven', 'Antoine Deleforge', 'Antoine Liutkus']","['stat.ML', 'cs.SD', 'eess.AS']",2017-11-13 07:53:35+00:00
http://arxiv.org/abs/1711.04454v2,Thresholding Bandit for Dose-ranging: The Impact of Monotonicity,"We analyze the sample complexity of the thresholding bandit problem, with and
without the assumption that the mean values of the arms are increasing. In each
case, we provide a lower bound valid for any risk $\delta$ and any
$\delta$-correct algorithm; in addition, we propose an algorithm whose sample
complexity is of the same order of magnitude for small risks. This work is
motivated by phase 1 clinical trials, a practically important setting where the
arm means are increasing by nature, and where no satisfactory solution is
available so far.","['Aurélien Garivier', 'Pierre Ménard', 'Laurent Rossi', 'Pierre Menard']","['math.ST', 'stat.ML', 'stat.TH']",2017-11-13 07:36:01+00:00
http://arxiv.org/abs/1711.04425v3,Message Passing Stein Variational Gradient Descent,"Stein variational gradient descent (SVGD) is a recently proposed
particle-based Bayesian inference method, which has attracted a lot of interest
due to its remarkable approximation ability and particle efficiency compared to
traditional variational inference and Markov Chain Monte Carlo methods.
However, we observed that particles of SVGD tend to collapse to modes of the
target distribution, and this particle degeneracy phenomenon becomes more
severe with higher dimensions. Our theoretical analysis finds out that there
exists a negative correlation between the dimensionality and the repulsive
force of SVGD which should be blamed for this phenomenon. We propose Message
Passing SVGD (MP-SVGD) to solve this problem. By leveraging the conditional
independence structure of probabilistic graphical models (PGMs), MP-SVGD
converts the original high-dimensional global inference problem into a set of
local ones over the Markov blanket with lower dimensions. Experimental results
show its advantages of preventing vanishing repulsive force in high-dimensional
space over SVGD, and its particle efficiency and approximation flexibility over
other inference methods on graphical models.","['Jingwei Zhuo', 'Chang Liu', 'Jiaxin Shi', 'Jun Zhu', 'Ning Chen', 'Bo Zhang']",['stat.ML'],2017-11-13 05:39:25+00:00
http://arxiv.org/abs/1711.04416v1,Variance Reduced methods for Non-convex Composition Optimization,"This paper explores the non-convex composition optimization in the form
including inner and outer finite-sum functions with a large number of component
functions. This problem arises in some important applications such as nonlinear
embedding and reinforcement learning. Although existing approaches such as
stochastic gradient descent (SGD) and stochastic variance reduced gradient
(SVRG) descent can be applied to solve this problem, their query complexity
tends to be high, especially when the number of inner component functions is
large. In this paper, we apply the variance-reduced technique to derive two
variance reduced algorithms that significantly improve the query complexity if
the number of inner component functions is large. To the best of our knowledge,
this is the first work that establishes the query complexity analysis for
non-convex stochastic composition. Experiments validate the proposed algorithms
and theoretical analysis.","['Liu Liu', 'Ji Liu', 'Dacheng Tao']","['stat.ML', 'math.OC']",2017-11-13 04:21:41+00:00
http://arxiv.org/abs/1711.05792v2,Aggregated Wasserstein Metric and State Registration for Hidden Markov Models,"We propose a framework, named Aggregated Wasserstein, for computing a
dissimilarity measure or distance between two Hidden Markov Models with state
conditional distributions being Gaussian. For such HMMs, the marginal
distribution at any time position follows a Gaussian mixture distribution, a
fact exploited to softly match, aka register, the states in two HMMs. We refer
to such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of
states is inspired by the intrinsic relationship of optimal transport and the
Wasserstein metric between distributions. Specifically, the components of the
marginal GMMs are matched by solving an optimal transport problem where the
cost between components is the Wasserstein metric for Gaussian distributions.
The solution of the optimization problem is a fast approximation to the
Wasserstein metric between two GMMs. The new Aggregated Wasserstein distance is
a semi-metric and can be computed without generating Monte Carlo samples. It is
invariant to relabeling or permutation of states. The distance is defined
meaningfully even for two HMMs that are estimated from data of different
dimensionality, a situation that can arise due to missing variables. This
distance quantifies the dissimilarity of GMM-HMMs by measuring both the
difference between the two marginal GMMs and that between the two transition
matrices. Our new distance is tested on tasks of retrieval, classification, and
t-SNE visualization of time series. Experiments on both synthetic and real data
have demonstrated its advantages in terms of accuracy as well as efficiency in
comparison with existing distances based on the Kullback-Leibler divergence.","['Yukun Chen', 'Jianbo Ye', 'Jia Li']","['cs.LG', 'cs.CV', 'stat.ML']",2017-11-12 22:43:22+00:00
http://arxiv.org/abs/1711.04374v1,"Should You Derive, Or Let the Data Drive? An Optimization Framework for Hybrid First-Principles Data-Driven Modeling","Mathematical models are used extensively for diverse tasks including
analysis, optimization, and decision making. Frequently, those models are
principled but imperfect representations of reality. This is either due to
incomplete physical description of the underlying phenomenon (simplified
governing equations, defective boundary conditions, etc.), or due to numerical
approximations (discretization, linearization, round-off error, etc.). Model
misspecification can lead to erroneous model predictions, and respectively
suboptimal decisions associated with the intended end-goal task. To mitigate
this effect, one can amend the available model using limited data produced by
experiments or higher fidelity models. A large body of research has focused on
estimating explicit model parameters. This work takes a different perspective
and targets the construction of a correction model operator with implicit
attributes. We investigate the case where the end-goal is inversion and
illustrate how appropriate choices of properties imposed upon the correction
and corrected operator lead to improved end-goal insights.","['Remi R. Lam', 'Lior Horesh', 'Haim Avron', 'Karen E. Willcox']","['stat.ML', 'math.DS', 'math.OC', 'physics.data-an']",2017-11-12 22:37:46+00:00
http://arxiv.org/abs/1711.04368v3,Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples,"Recently, researchers have discovered that the state-of-the-art object
classifiers can be fooled easily by small perturbations in the input
unnoticeable to human eyes. It is also known that an attacker can generate
strong adversarial examples if she knows the classifier parameters. Conversely,
a defender can robustify the classifier by retraining if she has access to the
adversarial examples. We explain and formulate this adversarial example problem
as a two-player continuous zero-sum game, and demonstrate the fallacy of
evaluating a defense or an attack as a static problem. To find the best
worst-case defense against whitebox attacks, we propose a continuous minimax
optimization algorithm. We demonstrate the minimax defense with two types of
attack classes -- gradient-based and neural network-based attacks. Experiments
with the MNIST and the CIFAR-10 datasets demonstrate that the defense found by
numerical minimax optimization is indeed more robust than non-minimax defenses.
We discuss directions for improving the result toward achieving robustness
against multiple types of attack classes.","['Jihun Hamm', 'Akshay Mehra']","['cs.LG', 'stat.ML']",2017-11-12 22:07:36+00:00
http://arxiv.org/abs/1711.04366v2,A unified framework for hard and soft clustering with regularized optimal transport,"In this paper, we formulate the problem of inferring a Finite Mixture Model
from discrete data as an optimal transport problem with entropic regularization
of parameter $\lambda\geq 0$. Our method unifies hard and soft clustering, the
Expectation-Maximization (EM) algorithm being exactly recovered for
$\lambda=1$. The family of clustering algorithm we propose rely on the
resolution of nonconvex problems using alternating minimization. We study the
convergence property of our generalized $\lambda-$EM algorithms and show that
each step in the minimization process has a closed form solution when inferring
finite mixture models of exponential families. Experiments highlight the
benefits of taking a parameter $\lambda>1$ to improve the inference performance
and $\lambda\to 0$ for classification.","['Jean-Frédéric Diebold', 'Nicolas Papadakis', 'Arnaud Dessein', 'Charles-Alban Deledalle']","['cs.LG', 'stat.ML']",2017-11-12 21:52:54+00:00
http://arxiv.org/abs/1711.04345v1,Alpha-Divergences in Variational Dropout,"We investigate the use of alternative divergences to Kullback-Leibler (KL) in
variational inference(VI), based on the Variational Dropout \cite{kingma2015}.
Stochastic gradient variational Bayes (SGVB) \cite{aevb} is a general framework
for estimating the evidence lower bound (ELBO) in Variational Bayes. In this
work, we extend the SGVB estimator with using Alpha-Divergences, which are
alternative to divergences to VI' KL objective. The Gaussian dropout can be
seen as a local reparametrization trick of the SGVB objective. We extend the
Variational Dropout to use alpha divergences for variational inference. Our
results compare $\alpha$-divergence variational dropout with standard
variational dropout with correlated and uncorrelated weight noise. We show that
the $\alpha$-divergence with $\alpha \rightarrow 1$ (or KL divergence) is still
a good measure for use in variational inference, in spite of the efficient use
of Alpha-divergences for Dropout VI \cite{Li17}. $\alpha \rightarrow 1$ can
yield the lowest training error, and optimizes a good lower bound for the
evidence lower bound (ELBO) among all values of the parameter $\alpha \in
[0,\infty)$.","['Bogdan Mazoure', 'Riashat Islam']","['stat.ML', 'cs.LG']",2017-11-12 19:38:09+00:00
http://arxiv.org/abs/1711.04340v3,Data Augmentation Generative Adversarial Networks,"Effective training of neural networks requires much data. In the low-data
regime, parameters are underdetermined, and learnt networks generalise poorly.
Data Augmentation alleviates this by using existing data more effectively.
However standard data augmentation produces only limited plausible alternative
data. Given there is potential to generate a much broader set of augmentations,
we design and train a generative model to do data augmentation. The model,
based on image conditional Generative Adversarial Networks, takes data from a
source domain and learns to take any data item and generalise it to generate
other within-class data items. As this generative process does not depend on
the classes themselves, it can be applied to novel unseen classes of data. We
show that a Data Augmentation Generative Adversarial Network (DAGAN) augments
standard vanilla classifiers well. We also show a DAGAN can enhance few-shot
learning systems such as Matching Networks. We demonstrate these approaches on
Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In
our experiments we can see over 13% increase in accuracy in the low-data regime
experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face
(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%
(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).","['Antreas Antoniou', 'Amos Storkey', 'Harrison Edwards']","['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']",2017-11-12 19:17:57+00:00
http://arxiv.org/abs/1711.04329v2,Medical Diagnosis From Laboratory Tests by Combining Generative and Discriminative Learning,"A primary goal of computational phenotype research is to conduct medical
diagnosis. In hospital, physicians rely on massive clinical data to make
diagnosis decisions, among which laboratory tests are one of the most important
resources. However, the longitudinal and incomplete nature of laboratory test
data casts a significant challenge on its interpretation and usage, which may
result in harmful decisions by both human physicians and automatic diagnosis
systems. In this work, we take advantage of deep generative models to deal with
the complex laboratory tests. Specifically, we propose an end-to-end
architecture that involves a deep generative variational recurrent neural
networks (VRNN) to learn robust and generalizable features, and a
discriminative neural network (NN) model to learn diagnosis decision making,
and the two models are trained jointly. Our experiments are conducted on a
dataset involving 46,252 patients, and the 50 most frequent tests are used to
predict the 50 most common diagnoses. The results show that our model, VRNN+NN,
significantly (p<0.001) outperforms other baseline models. Moreover, we
demonstrate that the representations learned by the joint training are more
informative than those learned by pure generative models. Finally, we find that
our model offers a surprisingly good imputation for missing values.","['Shiyue Zhang', 'Pengtao Xie', 'Dong Wang', 'Eric P. Xing']","['cs.AI', 'cs.LG', 'stat.ML']",2017-11-12 17:58:42+00:00
http://arxiv.org/abs/1711.04315v1,A machine learning approach for efficient uncertainty quantification using multiscale methods,"Several multiscale methods account for sub-grid scale features using coarse
scale basis functions. For example, in the Multiscale Finite Volume method the
coarse scale basis functions are obtained by solving a set of local problems
over dual-grid cells. We introduce a data-driven approach for the estimation of
these coarse scale basis functions. Specifically, we employ a neural network
predictor fitted using a set of solution samples from which it learns to
generate subsequent basis functions at a lower computational cost than solving
the local problems. The computational advantage of this approach is realized
for uncertainty quantification tasks where a large number of realizations has
to be evaluated. We attribute the ability to learn these basis functions to the
modularity of the local problems and the redundancy of the permeability patches
between samples. The proposed method is evaluated on elliptic problems yielding
very promising results.","['Shing Chan', 'Ahmed H. Elsheikh']","['cs.LG', 'physics.comp-ph', 'stat.ML']",2017-11-12 15:45:34+00:00
http://arxiv.org/abs/1711.04313v1,Semi-Supervised Learning via New Deep Network Inversion,"We exploit a recently derived inversion scheme for arbitrary deep neural
networks to develop a new semi-supervised learning framework that applies to a
wide range of systems and problems. The approach outperforms current
state-of-the-art methods on MNIST reaching $99.14\%$ of test set accuracy while
using $5$ labeled examples per class. Experiments with one-dimensional signals
highlight the generality of the method. Importantly, our approach is simple,
efficient, and requires no change in the deep network architecture.","['Randall Balestriero', 'Vincent Roger', 'Herve G. Glotin', 'Richard G. Baraniuk']","['stat.ML', 'cs.LG']",2017-11-12 15:42:24+00:00
http://arxiv.org/abs/1711.04308v3,Sensor Selection and Random Field Reconstruction for Robust and Cost-effective Heterogeneous Weather Sensor Networks for the Developing World,"We address the two fundamental problems of spatial field reconstruction and
sensor selection in heterogeneous sensor networks: (i) how to efficiently
perform spatial field reconstruction based on measurements obtained
simultaneously from networks with both high and low quality sensors; and (ii)
how to perform query based sensor set selection with predictive MSE performance
guarantee. For the first problem, we developed a low complexity algorithm based
on the spatial best linear unbiased estimator (S-BLUE). Next, building on the
S-BLUE, we address the second problem, and develop an efficient algorithm for
query based sensor set selection with performance guarantee. Our algorithm is
based on the Cross Entropy method which solves the combinatorial optimization
problem in an efficient manner.","['Pengfei Zhang', 'Ido Nevat', 'Gareth W. Peters', 'Wolfgang Fruehwirt', 'Yongchao Huang', 'Ivonne Anders', 'Michael Osborne']","['stat.ML', 'eess.SP']",2017-11-12 15:17:46+00:00
