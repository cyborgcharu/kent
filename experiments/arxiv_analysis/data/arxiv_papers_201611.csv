id,title,abstract,authors,categories,date
http://arxiv.org/abs/1612.07374v1,Detecting Unusual Input-Output Associations in Multivariate Conditional Data,"Despite tremendous progress in outlier detection research in recent years,
the majority of existing methods are designed only to detect unconditional
outliers that correspond to unusual data patterns expressed in the joint space
of all data attributes. Such methods are not applicable when we seek to detect
conditional outliers that reflect unusual responses associated with a given
context or condition. This work focuses on multivariate conditional outlier
detection, a special type of the conditional outlier detection problem, where
data instances consist of multi-dimensional input (context) and output
(responses) pairs. We present a novel outlier detection framework that
identifies abnormal input-output associations in data with the help of a
decomposable conditional probabilistic model that is learned from all data
instances. Since components of this model can vary in their quality, we combine
them with the help of weights reflecting their reliability in assessment of
outliers. We study two ways of calculating the component weights: global that
relies on all data, and local that relies only on instances similar to the
target instance. Experimental results on data from various domains demonstrate
the ability of our framework to successfully identify multivariate conditional
outliers.","['Charmgil Hong', 'Milos Hauskrecht']","['cs.LG', 'stat.ML']",2016-12-21 22:43:08+00:00
http://arxiv.org/abs/1612.07222v1,Bayesian Decision Process for Cost-Efficient Dynamic Ranking via Crowdsourcing,"Rank aggregation based on pairwise comparisons over a set of items has a wide
range of applications. Although considerable research has been devoted to the
development of rank aggregation algorithms, one basic question is how to
efficiently collect a large amount of high-quality pairwise comparisons for the
ranking purpose. Because of the advent of many crowdsourcing services, a crowd
of workers are often hired to conduct pairwise comparisons with a small
monetary reward for each pair they compare. Since different workers have
different levels of reliability and different pairs have different levels of
ambiguity, it is desirable to wisely allocate the limited budget for
comparisons among the pairs of items and workers so that the global ranking can
be accurately inferred from the comparison results. To this end, we model the
active sampling problem in crowdsourced ranking as a Bayesian Markov decision
process, which dynamically selects item pairs and workers to improve the
ranking accuracy under a budget constraint. We further develop a
computationally efficient sampling policy based on knowledge gradient as well
as a moment matching technique for posterior approximation. Experimental
evaluations on both synthetic and real data show that the proposed policy
achieves high ranking accuracy with a lower labeling cost.","['Xi Chen', 'Kevin Jiao', 'Qihang Lin']","['stat.ML', 'cs.LG', 'stat.ME']",2016-12-21 16:24:27+00:00
http://arxiv.org/abs/1612.07019v1,Robust Learning with Kernel Mean p-Power Error Loss,"Correntropy is a second order statistical measure in kernel space, which has
been successfully applied in robust learning and signal processing. In this
paper, we define a nonsecond order statistical measure in kernel space, called
the kernel mean-p power error (KMPE), including the correntropic loss (CLoss)
as a special case. Some basic properties of KMPE are presented. In particular,
we apply the KMPE to extreme learning machine (ELM) and principal component
analysis (PCA), and develop two robust learning algorithms, namely ELM-KMPE and
PCA-KMPE. Experimental results on synthetic and benchmark data show that the
developed algorithms can achieve consistently better performance when compared
with some existing methods.","['Badong Chen', 'Lei Xing', 'Xin Wang', 'Jing Qin', 'Nanning Zheng']","['stat.ML', 'cs.LG']",2016-12-21 09:10:48+00:00
http://arxiv.org/abs/1612.06676v2,Multivariate Industrial Time Series with Cyber-Attack Simulation: Fault Detection Using an LSTM-based Predictive Data Model,"We adopted an approach based on an LSTM neural network to monitor and detect
faults in industrial multivariate time series data. To validate the approach we
created a Modelica model of part of a real gasoil plant. By introducing hacks
into the logic of the Modelica model, we were able to generate both the roots
and causes of fault behavior in the plant. Having a self-consistent data set
with labeled faults, we used an LSTM architecture with a forecasting error
threshold to obtain precision and recall quality metrics. The dependency of the
quality metric on the threshold level is considered. An appropriate mechanism
such as ""one handle"" was introduced for filtering faults that are outside of
the plant operator field of interest.","['Pavel Filonov', 'Andrey Lavrentyev', 'Artem Vorontsov']","['cs.LG', 'stat.ML']",2016-12-20 14:24:49+00:00
http://arxiv.org/abs/1612.06669v1,Enhancing Observability in Distribution Grids using Smart Meter Data,"Due to limited metering infrastructure, distribution grids are currently
challenged by observability issues. On the other hand, smart meter data,
including local voltage magnitudes and power injections, are communicated to
the utility operator from grid buses with renewable generation and
demand-response programs. This work employs grid data from metered buses
towards inferring the underlying grid state. To this end, a coupled formulation
of the power flow problem (CPF) is put forth. Exploiting the high variability
of injections at metered buses, the controllability of solar inverters, and the
relative time-invariance of conventional loads, the idea is to solve the
non-linear power flow equations jointly over consecutive time instants. An
intuitive and easily verifiable rule pertaining to the locations of metered and
non-metered buses on the physical grid is shown to be a necessary and
sufficient criterion for local observability in radial networks. To account for
noisy smart meter readings, a coupled power system state estimation (CPSSE)
problem is further developed. Both CPF and CPSSE tasks are tackled via
augmented semi-definite program relaxations. The observability criterion along
with the CPF and CPSSE solvers are numerically corroborated using synthetic and
actual solar generation and load data on the IEEE 34-bus benchmark feeder.","['Siddharth Bhela', 'Vassilis Kekatos', 'Sriharsha Veeramachaneni']","['math.OC', 'cs.LG', 'stat.ML']",2016-12-20 14:12:58+00:00
http://arxiv.org/abs/1612.06650v1,Partially blind domain adaptation for age prediction from DNA methylation data,"Over the last years, huge resources of biological and medical data have
become available for research. This data offers great chances for machine
learning applications in health care, e.g. for precision medicine, but is also
challenging to analyze. Typical challenges include a large number of possibly
correlated features and heterogeneity in the data. One flourishing field of
biological research in which this is relevant is epigenetics. Here, especially
large amounts of DNA methylation data have emerged. This epigenetic mark has
been used to predict a donor's 'epigenetic age' and increased epigenetic aging
has been linked to lifestyle and disease history. In this paper we propose an
adaptive model which performs feature selection for each test sample
individually based on the distribution of the input data. The method can be
seen as partially blind domain adaptation. We apply the model to the problem of
age prediction based on DNA methylation data from a variety of tissues, and
compare it to a standard model, which does not take heterogeneity into account.
The standard approach has particularly bad performance on one tissue type on
which we show substantial improvement with our new adaptive approach even
though no samples of that tissue were part of the training data.","['Lisa Handl', 'Adrin Jalali', 'Michael Scherer', 'Nico Pfeifer']","['q-bio.QM', 'stat.ML']",2016-12-20 13:26:57+00:00
http://arxiv.org/abs/1612.06598v1,WoCE: a framework for clustering ensemble by exploiting the wisdom of Crowds theory,"The Wisdom of Crowds (WOC), as a theory in the social science, gets a new
paradigm in computer science. The WOC theory explains that the aggregate
decision made by a group is often better than those of its individual members
if specific conditions are satisfied. This paper presents a novel framework for
unsupervised and semi-supervised cluster ensemble by exploiting the WOC theory.
We employ four conditions in the WOC theory, i.e., diversity, independency,
decentralization and aggregation, to guide both the constructing of individual
clustering results and the final combination for clustering ensemble. Firstly,
independency criterion, as a novel mapping system on the raw data set, removes
the correlation between features on our proposed method. Then, decentralization
as a novel mechanism generates high-quality individual clustering results.
Next, uniformity as a new diversity metric evaluates the generated clustering
results. Further, weighted evidence accumulation clustering method is proposed
for the final aggregation without using thresholding procedure. Experimental
study on varied data sets demonstrates that the proposed approach achieves
superior performance to state-of-the-art methods.","['Muhammad Yousefnezhad', 'Sheng-Jun Huang', 'Daoqiang Zhang']","['stat.ML', 'cs.LG']",2016-12-20 10:39:45+00:00
http://arxiv.org/abs/1612.06565v1,RIDS: Robust Identification of Sparse Gene Regulatory Networks from Perturbation Experiments,"Reconstructing the causal network in a complex dynamical system plays a
crucial role in many applications, from sub-cellular biology to economic
systems. Here we focus on inferring gene regulation networks (GRNs) from
perturbation or gene deletion experiments. Despite their scientific merit, such
perturbation experiments are not often used for such inference due to their
costly experimental procedure, requiring significant resources to complete the
measurement of every single experiment. To overcome this challenge, we develop
the Robust IDentification of Sparse networks (RIDS) method that reconstructs
the GRN from a small number of perturbation experiments. Our method uses the
gene expression data observed in each experiment and translates that into a
steady state condition of the system's nonlinear interaction dynamics. Applying
a sparse optimization criterion, we are able to extract the parameters of the
underlying weighted network, even from very few experiments. In fact, we
demonstrate analytically that, under certain conditions, the GRN can be
perfectly reconstructed using $K = \Omega (d_{max})$ perturbation experiments,
where $d_{max}$ is the maximum in-degree of the GRN, a small value for
realistic sparse networks, indicating that RIDS can achieve high performance
with a scalable number of experiments. We test our method on both synthetic and
experimental data extracted from the DREAM5 network inference challenge. We
show that the RIDS achieves superior performance compared to the
state-of-the-art methods, while requiring as few as ~60% less experimental
data. Moreover, as opposed to almost all competing methods, RIDS allows us to
infer the directionality of the GRN links, allowing us to infer empirical GRNs,
without relying on the commonly provided list of transcription factors.","['Hoi-To Wai', 'Anna Scaglione', 'Uzi Harush', 'Baruch Barzel', 'Amir Leshem']","['q-bio.QM', 'cs.IT', 'math.IT', 'q-bio.MN', 'stat.ML']",2016-12-20 09:34:27+00:00
http://arxiv.org/abs/1612.06470v1,Randomized Clustered Nystrom for Large-Scale Kernel Machines,"The Nystrom method has been popular for generating the low-rank approximation
of kernel matrices that arise in many machine learning problems. The
approximation quality of the Nystrom method depends crucially on the number of
selected landmark points and the selection procedure. In this paper, we present
a novel algorithm to compute the optimal Nystrom low-approximation when the
number of landmark points exceed the target rank. Moreover, we introduce a
randomized algorithm for generating landmark points that is scalable to
large-scale data sets. The proposed method performs K-means clustering on
low-dimensional random projections of a data set and, thus, leads to
significant savings for high-dimensional data sets. Our theoretical results
characterize the tradeoffs between the accuracy and efficiency of our proposed
method. Extensive experiments demonstrate the competitive performance as well
as the efficiency of our proposed method.","['Farhad Pourkamali-Anaraki', 'Stephen Becker']","['stat.ML', 'cs.LG']",2016-12-20 01:07:04+00:00
http://arxiv.org/abs/1612.06404v2,Random Walk Models of Network Formation and Sequential Monte Carlo Methods for Graphs,"We introduce a class of generative network models that insert edges by
connecting the starting and terminal vertices of a random walk on the network
graph. Within the taxonomy of statistical network models, this class is
distinguished by permitting the location of a new edge to explicitly depend on
the structure of the graph, but being nonetheless statistically and
computationally tractable. In the limit of infinite walk length, the model
converges to an extension of the preferential attachment model---in this sense,
it can be motivated alternatively by asking what preferential attachment is an
approximation to. Theoretical properties, including the limiting degree
sequence, are studied analytically. If the entire history of the graph is
observed, parameters can be estimated by maximum likelihood. If only the final
graph is available, its history can be imputed using MCMC. We develop a class
of sequential Monte Carlo algorithms that are more generally applicable to
sequential network models, and may be of interest in their own right. The model
parameters can be recovered from a single graph generated by the model.
Applications to data clarify the role of the random walk length as a length
scale of interactions within the graph.","['Benjamin Bloem-Reddy', 'Peter Orbanz']","['stat.ME', 'stat.ML']",2016-12-19 21:01:36+00:00
http://arxiv.org/abs/1612.06370v2,Learning Features by Watching Objects Move,"This paper presents a novel yet intuitive approach to unsupervised feature
learning. Inspired by the human visual system, we explore whether low-level
motion-based grouping cues can be used to learn an effective visual
representation. Specifically, we use unsupervised motion-based segmentation on
videos to obtain segments, which we use as 'pseudo ground truth' to train a
convolutional network to segment objects from a single frame. Given the
extensive evidence that motion plays a key role in the development of the human
visual system, we hope that this straightforward approach to unsupervised
learning will be more effective than cleverly designed 'pretext' tasks studied
in the literature. Indeed, our extensive experiments show that this is the
case. When used for transfer learning on object detection, our representation
significantly outperforms previous unsupervised approaches across multiple
settings, especially when training data for the target task is scarce.","['Deepak Pathak', 'Ross Girshick', 'Piotr Dollár', 'Trevor Darrell', 'Bharath Hariharan']","['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']",2016-12-19 20:56:04+00:00
http://arxiv.org/abs/1612.06340v2,Computing Human-Understandable Strategies,"Algorithms for equilibrium computation generally make no attempt to ensure
that the computed strategies are understandable by humans. For instance the
strategies for the strongest poker agents are represented as massive binary
files. In many situations, we would like to compute strategies that can
actually be implemented by humans, who may have computational limitations and
may only be able to remember a small number of features or components of the
strategies that have been computed. We study poker games where private
information distributions can be arbitrary. We create a large training set of
game instances and solutions, by randomly selecting the information
probabilities, and present algorithms that learn from the training instances in
order to perform well in games with unseen information distributions. We are
able to conclude several new fundamental rules about poker strategy that can be
easily implemented by humans.","['Sam Ganzfried', 'Farzana Yusuf']","['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA', 'stat.ML']",2016-12-19 20:40:19+00:00
http://arxiv.org/abs/1612.06299v1,Simple Black-Box Adversarial Perturbations for Deep Networks,"Deep neural networks are powerful and popular learning models that achieve
state-of-the-art pattern recognition performance on many computer vision,
speech, and language processing tasks. However, these networks have also been
shown susceptible to carefully crafted adversarial perturbations which force
misclassification of the inputs. Adversarial examples enable adversaries to
subvert the expected system behavior leading to undesired consequences and
could pose a security risk when these systems are deployed in the real world.
  In this work, we focus on deep convolutional neural networks and demonstrate
that adversaries can easily craft adversarial examples even without any
internal knowledge of the target network. Our attacks treat the network as an
oracle (black-box) and only assume that the output of the network can be
observed on the probed inputs. Our first attack is based on a simple idea of
adding perturbation to a randomly selected single pixel or a small set of them.
We then improve the effectiveness of this attack by carefully constructing a
small set of pixels to perturb by using the idea of greedy local-search. Our
proposed attacks also naturally extend to a stronger notion of
misclassification. Our extensive experimental results illustrate that even
these elementary attacks can reveal a deep neural network's vulnerabilities.
The simplicity and effectiveness of our proposed schemes mean that they could
serve as a litmus test for designing robust networks.","['Nina Narodytska', 'Shiva Prasad Kasiviswanathan']","['cs.LG', 'cs.CR', 'stat.ML']",2016-12-19 18:12:20+00:00
http://arxiv.org/abs/1612.06246v3,Corralling a Band of Bandit Algorithms,"We study the problem of combining multiple bandit algorithms (that is, online
learning algorithms with partial feedback) with the goal of creating a master
algorithm that performs almost as well as the best base algorithm if it were to
be run on its own. The main challenge is that when run with a master, base
algorithms unavoidably receive much less feedback and it is thus critical that
the master not starve a base algorithm that might perform uncompetitively
initially but would eventually outperform others if given enough feedback. We
address this difficulty by devising a version of Online Mirror Descent with a
special mirror map together with a sophisticated learning rate scheme. We show
that this approach manages to achieve a more delicate balance between
exploiting and exploring base algorithms than previous works yielding superior
regret bounds.
  Our results are applicable to many settings, such as multi-armed bandits,
contextual bandits, and convex bandits. As examples, we present two main
applications. The first is to create an algorithm that enjoys worst-case
robustness while at the same time performing much better when the environment
is relatively easy. The second is to create an algorithm that works
simultaneously under different assumptions of the environment, such as
different priors or different loss structures.","['Alekh Agarwal', 'Haipeng Luo', 'Behnam Neyshabur', 'Robert E. Schapire']","['cs.LG', 'stat.ML']",2016-12-19 16:17:56+00:00
http://arxiv.org/abs/1612.06176v1,An extended Perona-Malik model based on probabilistic models,"The Perona-Malik model has been very successful at restoring images from
noisy input. In this paper, we reinterpret the Perona-Malik model in the
language of Gaussian scale mixtures and derive some extensions of the model.
Specifically, we show that the expectation-maximization (EM) algorithm applied
to Gaussian scale mixtures leads to the lagged-diffusivity algorithm for
computing stationary points of the Perona-Malik diffusion equations. Moreover,
we show how mean field approximations to these Gaussian scale mixtures lead to
a modification of the lagged-diffusivity algorithm that better captures the
uncertainties in the restoration. Since this modification can be hard to
compute in practice we propose relaxations to the mean field objective to make
the algorithm computationally feasible. Our numerical experiments show that
this modified lagged-diffusivity algorithm often performs better at restoring
textured areas and fuzzy edges than the unmodified algorithm. As a second
application of the Gaussian scale mixture framework, we show how an efficient
sampling procedure can be obtained for the probabilistic model, making the
computation of the conditional mean and other expectations algorithmically
feasible. Again, the resulting algorithm has a strong resemblance to the
lagged-diffusivity algorithm. Finally, we show that a probabilistic version of
the Mumford-Shah segementation model can be obtained in the same framework with
a discrete edge-prior.","['Lars M. Mescheder', 'Dirk A. Lorenz']","['cs.CV', 'math.NA', 'stat.ML']",2016-12-19 13:39:45+00:00
http://arxiv.org/abs/1612.06131v1,Monte Carlo sampling for stochastic weight functions,"Conventional Monte Carlo simulations are stochastic in the sense that the
acceptance of a trial move is decided by comparing a computed acceptance
probability with a random number, uniformly distributed between 0 and 1. Here
we consider the case that the weight determining the acceptance probability
itself is fluctuating. This situation is common in many numerical studies. We
show that it is possible to construct a rigorous Monte Carlo algorithm that
visits points in state space with a probability proportional to their average
weight. The same approach has the potential to transform the methodology of a
certain class of high-throughput experiments or the analysis of noisy datasets.","['Daan Frenkel', 'K. Julian Schrenk', 'Stefano Martiniani']","['cond-mat.stat-mech', 'physics.comp-ph', 'stat.ME', 'stat.ML']",2016-12-19 11:32:01+00:00
http://arxiv.org/abs/1612.06083v1,Hierarchical Partitioning of the Output Space in Multi-label Data,"Hierarchy Of Multi-label classifiers (HOMER) is a multi-label learning
algorithm that breaks the initial learning task to several, easier sub-tasks by
first constructing a hierarchy of labels from a given label set and secondly
employing a given base multi-label classifier (MLC) to the resulting
sub-problems. The primary goal is to effectively address class imbalance and
scalability issues that often arise in real-world multi-label classification
problems. In this work, we present the general setup for a HOMER model and a
simple extension of the algorithm that is suited for MLCs that output rankings.
Furthermore, we provide a detailed analysis of the properties of the algorithm,
both from an aspect of effectiveness and computational complexity. A secondary
contribution involves the presentation of a balanced variant of the k means
algorithm, which serves in the first step of the label hierarchy construction.
We conduct extensive experiments on six real-world datasets, studying
empirically HOMER's parameters and providing examples of instantiations of the
algorithm with different clustering approaches and MLCs, The empirical results
demonstrate a significant improvement over the given base MLC.","['Yannis Papanikolaou', 'Ioannis Katakis', 'Grigorios Tsoumakas']","['stat.ML', 'cs.LG']",2016-12-19 09:08:59+00:00
http://arxiv.org/abs/1612.06067v2,A Convex Program for Mixed Linear Regression with a Recovery Guarantee for Well-Separated Data,"We introduce a convex approach for mixed linear regression over $d$ features.
This approach is a second-order cone program, based on L1 minimization, which
assigns an estimate regression coefficient in $\mathbb{R}^{d}$ for each data
point. These estimates can then be clustered using, for example, $k$-means. For
problems with two or more mixture classes, we prove that the convex program
exactly recovers all of the mixture components in the noiseless setting under
technical conditions that include a well-separation assumption on the data.
Under these assumptions, recovery is possible if each class has at least $d$
independent measurements. We also explore an iteratively reweighted least
squares implementation of this method on real and synthetic data.","['Paul Hand', 'Babhru Joshi']","['math.OC', 'stat.ML']",2016-12-19 07:58:00+00:00
http://arxiv.org/abs/1612.06061v1,Mixing Times and Structural Inference for Bernoulli Autoregressive Processes,"We introduce a novel multivariate random process producing Bernoulli outputs
per dimension, that can possibly formalize binary interactions in various
graphical structures and can be used to model opinion dynamics, epidemics,
financial and biological time series data, etc. We call this a Bernoulli
Autoregressive Process (BAR). A BAR process models a discrete-time vector
random sequence of $p$ scalar Bernoulli processes with autoregressive dynamics
and corresponds to a particular Markov Chain. The benefit from the
autoregressive dynamics is the description of a $2^p\times 2^p$ transition
matrix by at most $pd$ effective parameters for some $d\ll p$ or by two sparse
matrices of dimensions $p\times p^2$ and $p\times p$, respectively,
parameterizing the transitions. Additionally, we show that the BAR process
mixes rapidly, by proving that the mixing time is $O(\log p)$. The hidden
constant in the previous mixing time bound depends explicitly on the values of
the chain parameters and implicitly on the maximum allowed in-degree of a node
in the corresponding graph. For a network with $p$ nodes, where each node has
in-degree at most $d$ and corresponds to a scalar Bernoulli process generated
by a BAR, we provide a greedy algorithm that can efficiently learn the
structure of the underlying directed graph with a sample complexity
proportional to the mixing time of the BAR process. The sample complexity of
the proposed algorithm is nearly order-optimal as it is only a $\log p$ factor
away from an information-theoretic lower bound. We present simulation results
illustrating the performance of our algorithm in various setups, including a
model for a biological signaling network.","['Dimitrios Katselis', 'Carolyn L. Beck', 'R. Srikant']",['stat.ML'],2016-12-19 06:59:40+00:00
http://arxiv.org/abs/1612.06007v2,A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference,"Modeling continuous-time physiological processes that manifest a patient's
evolving clinical states is a key step in approaching many problems in
healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model
(HASMM): a versatile probabilistic model that is capable of capturing the
modern electronic health record (EHR) data. Unlike exist- ing models, an HASMM
accommodates irregularly sampled, temporally correlated, and informatively
censored physiological data, and can describe non-stationary clinical state
transitions. Learning an HASMM from the EHR data is achieved via a novel
forward- filtering backward-sampling Monte-Carlo EM algorithm that exploits the
knowledge of the end-point clinical outcomes (informative censoring) in the EHR
data, and implements the E-step by sequentially sampling the patients' clinical
states in the reverse-time direction while conditioning on the future states.
Real-time inferences are drawn via a forward- filtering algorithm that operates
on a virtually constructed discrete-time embedded Markov chain that mirrors the
patient's continuous-time state trajectory. We demonstrate the di- agnostic and
prognostic utility of the HASMM in a critical care prognosis setting using a
real-world dataset for patients admitted to the Ronald Reagan UCLA Medical
Center.","['Ahmed M. Alaa', 'Mihaela van der Schaar']","['cs.AI', 'stat.ML']",2016-12-18 23:02:02+00:00
http://arxiv.org/abs/1612.06003v2,Inexact Proximal Gradient Methods for Non-convex and Non-smooth Optimization,"In machine learning research, the proximal gradient methods are popular for
solving various optimization problems with non-smooth regularization. Inexact
proximal gradient methods are extremely important when exactly solving the
proximal operator is time-consuming, or the proximal operator does not have an
analytic solution. However, existing inexact proximal gradient methods only
consider convex problems. The knowledge of inexact proximal gradient methods in
the non-convex setting is very limited. % Moreover, for some machine learning
models, there is still no proposed solver for exactly solving the proximal
operator. To address this challenge, in this paper, we first propose three
inexact proximal gradient algorithms, including the basic version and
Nesterov's accelerated version. After that, we provide the theoretical analysis
to the basic and Nesterov's accelerated versions. The theoretical results show
that our inexact proximal gradient algorithms can have the same convergence
rates as the ones of exact proximal gradient algorithms in the non-convex
setting.
  Finally, we show the applications of our inexact proximal gradient algorithms
on three representative non-convex learning problems. All experimental results
confirm the superiority of our new inexact proximal gradient algorithms.","['Bin Gu', 'De Wang', 'Zhouyuan Huo', 'Heng Huang']","['cs.LG', 'stat.ML']",2016-12-18 22:14:36+00:00
http://arxiv.org/abs/1612.06000v1,Sample-efficient Deep Reinforcement Learning for Dialog Control,"Representing a dialog policy as a recurrent neural network (RNN) is
attractive because it handles partial observability, infers a latent
representation of state, and can be optimized with supervised learning (SL) or
reinforcement learning (RL). For RL, a policy gradient approach is natural, but
is sample inefficient. In this paper, we present 3 methods for reducing the
number of dialogs required to optimize an RNN-based dialog policy with RL. The
key idea is to maintain a second RNN which predicts the value of the current
policy, and to apply experience replay to both networks. On two tasks, these
methods reduce the number of dialogs/episodes required by about a third, vs.
standard policy gradient methods.","['Kavosh Asadi', 'Jason D. Williams']","['cs.AI', 'cs.LG', 'stat.ML']",2016-12-18 21:51:10+00:00
http://arxiv.org/abs/1612.05907v2,Distributed Generalized Cross-Validation for Divide-and-Conquer Kernel Ridge Regression and its Asymptotic Optimality,"Tuning parameter selection is of critical importance for kernel ridge
regression. To this date, data driven tuning method for divide-and-conquer
kernel ridge regression (d-KRR) has been lacking in the literature, which
limits the applicability of d-KRR for large data sets. In this paper, by
modifying the Generalized Cross-validation (GCV, Wahba, 1990) score, we propose
a distributed Generalized Cross-Validation (dGCV) as a data-driven tool for
selecting the tuning parameters in d-KRR. Not only the proposed dGCV is
computationally scalable for massive data sets, it is also shown, under mild
conditions, to be asymptotically optimal in the sense that minimizing the dGCV
score is equivalent to minimizing the true global conditional empirical loss of
the averaged function estimator, extending the existing optimality results of
GCV to the divide-and-conquer framework.","['Ganggang Xu', 'Zuofeng Shang', 'Guang Cheng']",['stat.ML'],2016-12-18 12:41:08+00:00
http://arxiv.org/abs/1612.05888v2,Building Diversified Multiple Trees for Classification in High Dimensional Noisy Biomedical Data,"It is common that a trained classification model is applied to the operating
data that is deviated from the training data because of noise. This paper
demonstrates that an ensemble classifier, Diversified Multiple Tree (DMT), is
more robust in classifying noisy data than other widely used ensemble methods.
DMT is tested on three real world biomedical data sets from different
laboratories in comparison with four benchmark ensemble classifiers.
Experimental results show that DMT is significantly more accurate than other
benchmark ensemble classifiers on noisy test data. We also discuss a limitation
of DMT and its possible variations.","['Jiuyong Li', 'Lin Liu', 'Jixue Liu', 'Ryan Green']","['cs.LG', 'stat.ML']",2016-12-18 10:21:20+00:00
http://arxiv.org/abs/1612.05846v3,Joint Spatial-Angular Sparse Coding for dMRI with Separable Dictionaries,"Diffusion MRI (dMRI) provides the ability to reconstruct neuronal fibers in
the brain, $\textit{in vivo}$, by measuring water diffusion along angular
gradient directions in q-space. High angular resolution diffusion imaging
(HARDI) can produce better estimates of fiber orientation than the popularly
used diffusion tensor imaging, but the high number of samples needed to
estimate diffusivity requires longer patient scan times. To accelerate dMRI,
compressed sensing (CS) has been utilized by exploiting a sparse dictionary
representation of the data, discovered through sparse coding. The sparser the
representation, the fewer samples are needed to reconstruct a high resolution
signal with limited information loss, and so an important area of research has
focused on finding the sparsest possible representation of dMRI. Current
reconstruction methods however, rely on an angular representation $\textit{per
voxel}$ with added spatial regularization, and so, for non-zero signals, one is
required to have at least one non-zero coefficient per voxel. This means that
the global level of sparsity must be greater than the number of voxels. In
contrast, we propose a joint spatial-angular representation of dMRI that will
allow us to achieve levels of global sparsity that are below the number of
voxels. A major challenge, however, is the computational complexity of solving
a global sparse coding problem over large-scale dMRI. In this work, we present
novel adaptations of popular sparse coding algorithms that become better suited
for solving large-scale problems by exploiting spatial-angular separability.
Our experiments show that our method achieves significantly sparser
representations of HARDI than is possible by the state of the art.","['Evan Schwab', 'René Vidal', 'Nicolas Charon']","['stat.ML', 'cs.CV', 'q-bio.QM']",2016-12-18 02:08:42+00:00
http://arxiv.org/abs/1612.05740v1,"Machine Learning, Linear and Bayesian Models for Logistic Regression in Failure Detection Problems","In this work, we study the use of logistic regression in manufacturing
failures detection. As a data set for the analysis, we used the data from
Kaggle competition Bosch Production Line Performance. We considered the use of
machine learning, linear and Bayesian models. For machine learning approach, we
analyzed XGBoost tree based classifier to obtain high scored classification.
Using the generalized linear model for logistic regression makes it possible to
analyze the influence of the factors under study. The Bayesian approach for
logistic regression gives the statistical distribution for the parameters of
the model. It can be useful in the probabilistic analysis, e.g. risk
assessment.",['B. Pavlyshenko'],"['cs.LG', 'stat.ML']",2016-12-17 11:57:45+00:00
http://arxiv.org/abs/1612.05730v2,Towards Wide Learning: Experiments in Healthcare,"In this paper, a Wide Learning architecture is proposed that attempts to
automate the feature engineering portion of the machine learning (ML) pipeline.
Feature engineering is widely considered as the most time consuming and expert
knowledge demanding portion of any ML task. The proposed feature recommendation
approach is tested on 3 healthcare datasets: a) PhysioNet Challenge 2016
dataset of phonocardiogram (PCG) signals, b) MIMIC II blood pressure
classification dataset of photoplethysmogram (PPG) signals and c) an emotion
classification dataset of PPG signals. While the proposed method beats the
state of the art techniques for 2nd and 3rd dataset, it reaches 94.38% of the
accuracy level of the winner of PhysioNet Challenge 2016. In all cases, the
effort to reach a satisfactory performance was drastically less (a few days)
than manual feature engineering.","['Snehasis Banerjee', 'Tanushyam Chattopadhyay', 'Swagata Biswas', 'Rohan Banerjee', 'Anirban Dutta Choudhury', 'Arpan Pal', 'Utpal Garain']","['stat.ML', 'cs.LG']",2016-12-17 11:00:49+00:00
http://arxiv.org/abs/1612.05708v1,Mutual information for fitting deep nonlinear models,"Deep nonlinear models pose a challenge for fitting parameters due to lack of
knowledge of the hidden layer and the potentially non-affine relation of the
initial and observed layers. In the present work we investigate the use of
information theoretic measures such as mutual information and Kullback-Leibler
(KL) divergence as objective functions for fitting such models without
knowledge of the hidden layer. We investigate one model as a proof of concept
and one application of cogntive performance. We further investigate the use of
optimizers with these methods. Mutual information is largely successful as an
objective, depending on the parameters. KL divergence is found to be similarly
succesful, given some knowledge of the statistics of the hidden layer.","['Jacob S. Hunter', 'Nathan O. Hodas']","['math.OC', 'cs.LG', 'stat.ML']",2016-12-17 05:26:46+00:00
http://arxiv.org/abs/1612.05678v4,Causal Learning via Manifold Regularization,"This paper frames causal structure estimation as a machine learning task. The
idea is to treat indicators of causal relationships between variables as
`labels' and to exploit available data on the variables of interest to provide
features for the labelling task. Background scientific knowledge or any
available interventional data provide labels on some causal relationships and
the remainder are treated as unlabelled. To illustrate the key ideas, we
develop a distance-based approach (based on bivariate histograms) within a
manifold regularization framework. We present empirical results on three
different biological data sets (including examples where causal effects can be
verified by experimental intervention), that together demonstrate the efficacy
and general nature of the approach as well as its simplicity from a user's
point of view.","['Steven M. Hill', 'Chris. J. Oates', 'Duncan A. Blythe', 'Sach Mukherjee']",['stat.ML'],2016-12-16 23:05:31+00:00
http://arxiv.org/abs/1612.05628v5,An Alternative Softmax Operator for Reinforcement Learning,"A softmax operator applied to a set of values acts somewhat like the
maximization function and somewhat like an average. In sequential decision
making, softmax is often used in settings where it is necessary to maximize
utility but also to hedge against problems that arise from putting all of one's
weight behind a single maximum utility decision. The Boltzmann softmax operator
is the most commonly used softmax operator in this setting, but we show that
this operator is prone to misbehavior. In this work, we study a differentiable
softmax operator that, among other properties, is a non-expansion ensuring a
convergent behavior in learning and planning. We introduce a variant of SARSA
algorithm that, by utilizing the new operator, computes a Boltzmann policy with
a state-dependent temperature parameter. We show that the algorithm is
convergent and that it performs favorably in practice.","['Kavosh Asadi', 'Michael L. Littman']","['cs.AI', 'cs.LG', 'stat.ML']",2016-12-16 20:49:35+00:00
http://arxiv.org/abs/1612.05614v2,An MM Algorithm for Split Feasibility Problems,"The classical multi-set split feasibility problem seeks a point in the
intersection of finitely many closed convex domain constraints, whose image
under a linear mapping also lies in the intersection of finitely many closed
convex range constraints. Split feasibility generalizes important inverse
problems including convex feasibility, linear complementarity, and regression
with constraint sets. When a feasible point does not exist, solution methods
that proceed by minimizing a proximity function can be used to obtain optimal
approximate solutions to the problem. We present an extension of the proximity
function approach that generalizes the linear split feasibility problem to
allow for non-linear mappings. Our algorithm is based on the principle of
majorization-minimization, is amenable to quasi-Newton acceleration, and comes
complete with convergence guarantees under mild assumptions. Furthermore, we
show that the Euclidean norm appearing in the proximity function of the
non-linear split feasibility problem can be replaced by arbitrary Bregman
divergences. We explore several examples illustrating the merits of non-linear
formulations over the linear case, with a focus on optimization for
intensity-modulated radiation therapy.","['Jason Xu', 'Eric C. Chi', 'Meng Yang', 'Kenneth Lange']","['math.OC', 'math.NA', 'stat.CO', 'stat.ML']",2016-12-16 20:15:09+00:00
http://arxiv.org/abs/1612.05612v4,Asymptotic Optimality in Stochastic Optimization,"We study local complexity measures for stochastic convex optimization
problems, providing a local minimax theory analogous to that of H\'{a}jek and
Le Cam for classical statistical problems. We give complementary optimality
results, developing fully online methods that adaptively achieve optimal
convergence guarantees. Our results provide function-specific lower bounds and
convergence results that make precise a correspondence between statistical
difficulty and the geometric notion of tilt-stability from optimization. As
part of this development, we show how variants of Nesterov's dual averaging---a
stochastic gradient-based procedure---guarantee finite time identification of
constraints in optimization problems, while stochastic gradient procedures
fail. Additionally, we highlight a gap between problems with linear and
nonlinear constraints: standard stochastic-gradient-based procedures are
suboptimal even for the simplest nonlinear constraints, necessitating the
development of asymptotically optimal Riemannian stochastic gradient methods.","['John Duchi', 'Feng Ruan']","['math.ST', 'math.OC', 'stat.ML', 'stat.TH']",2016-12-16 19:54:22+00:00
http://arxiv.org/abs/1612.05535v2,Supervised Quantum Learning without Measurements,"We propose a quantum machine learning algorithm for efficiently solving a
class of problems encoded in quantum controlled unitary operations. The central
physical mechanism of the protocol is the iteration of a quantum time-delayed
equation that introduces feedback in the dynamics and eliminates the necessity
of intermediate measurements. The performance of the quantum algorithm is
analyzed by comparing the results obtained in numerical simulations with the
outcome of classical machine learning methods for the same problem. The use of
time-delayed equations enhances the toolbox of the field of quantum machine
learning, which may enable unprecedented applications in quantum technologies.","['Unai Alvarez-Rodriguez', 'Lucas Lamata', 'Pablo Escandell-Montero', 'José D. Martín-Guerrero', 'Enrique Solano']","['quant-ph', 'cond-mat.mes-hall', 'cond-mat.supr-con', 'cs.AI', 'stat.ML']",2016-12-16 16:15:45+00:00
http://arxiv.org/abs/1612.05519v2,Edge-exchangeable graphs and sparsity (NIPS 2016),"Many popular network models rely on the assumption of (vertex)
exchangeability, in which the distribution of the graph is invariant to
relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that
these graphs are dense or empty with probability one, whereas many real-world
graphs are sparse. We present an alternative notion of exchangeability for
random graphs, which we call edge exchangeability, in which the distribution of
a graph sequence is invariant to the order of the edges. We demonstrate that
edge-exchangeable models, unlike models that are traditionally vertex
exchangeable, can exhibit sparsity. To do so, we outline a general framework
for graph generative models; by contrast to the pioneering work of Caron and
Fox (2015), models within our framework are stationary across steps of the
graph sequence. In particular, our model grows the graph by instantiating more
latent atoms of a single random measure as the dataset size increases, rather
than adding new atoms to the measure.","['Diana Cai', 'Trevor Campbell', 'Tamara Broderick']",['stat.ML'],2016-12-16 15:57:28+00:00
http://arxiv.org/abs/1612.07640v1,Deep Learning and Its Applications to Machine Health Monitoring: A Survey,"Since 2006, deep learning (DL) has become a rapidly growing research
direction, redefining state-of-the-art performances in a wide range of areas
such as object recognition, image segmentation, speech recognition and machine
translation. In modern manufacturing systems, data-driven machine health
monitoring is gaining in popularity due to the widespread deployment of
low-cost sensors and their connection to the Internet. Meanwhile, deep learning
provides useful tools for processing and analyzing these big machinery data.
The main purpose of this paper is to review and summarize the emerging research
work of deep learning on machine health monitoring. After the brief
introduction of deep learning techniques, the applications of deep learning in
machine health monitoring systems are reviewed mainly from the following
aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and
its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines
(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).
Finally, some new trends of DL-based machine health monitoring methods are
discussed.","['Rui Zhao', 'Ruqiang Yan', 'Zhenghua Chen', 'Kezhi Mao', 'Peng Wang', 'Robert X. Gao']","['cs.LG', 'stat.ML']",2016-12-16 04:56:30+00:00
http://arxiv.org/abs/1612.05356v3,Projected Semi-Stochastic Gradient Descent Method with Mini-Batch Scheme under Weak Strong Convexity Assumption,"We propose a projected semi-stochastic gradient descent method with
mini-batch for improving both the theoretical complexity and practical
performance of the general stochastic gradient descent method (SGD). We are
able to prove linear convergence under weak strong convexity assumption. This
requires no strong convexity assumption for minimizing the sum of smooth convex
functions subject to a compact polyhedral set, which remains popular across
machine learning community. Our PS2GD preserves the low-cost per iteration and
high optimization accuracy via stochastic gradient variance-reduced technique,
and admits a simple parallel implementation with mini-batches. Moreover, PS2GD
is also applicable to dual problem of SVM with hinge loss.","['Jie Liu', 'Martin Takac']","['cs.LG', 'math.OC', 'stat.ML']",2016-12-16 03:54:30+00:00
http://arxiv.org/abs/1612.05276v2,Learning Optimal Control of Synchronization in Networks of Coupled Oscillators using Genetic Programming-based Symbolic Regression,"Networks of coupled dynamical systems provide a powerful way to model systems
with enormously complex dynamics, such as the human brain. Control of
synchronization in such networked systems has far reaching applications in many
domains, including engineering and medicine. In this paper, we formulate the
synchronization control in dynamical systems as an optimization problem and
present a multi-objective genetic programming-based approach to infer optimal
control functions that drive the system from a synchronized to a
non-synchronized state and vice-versa. The genetic programming-based controller
allows learning optimal control functions in an interpretable symbolic form.
The effectiveness of the proposed approach is demonstrated in controlling
synchronization in coupled oscillator systems linked in networks of increasing
order complexity, ranging from a simple coupled oscillator system to a
hierarchical network of coupled oscillators. The results show that the proposed
method can learn highly-effective and interpretable control functions for such
systems.","['Julien Gout', 'Markus Quade', 'Kamran Shafi', 'Robert K. Niven', 'Markus Abel']","['nlin.AO', 'cs.SY', 'stat.ML']",2016-12-15 21:20:27+00:00
http://arxiv.org/abs/1612.05270v1,A Simple Approach to Multilingual Polarity Classification in Twitter,"Recently, sentiment analysis has received a lot of attention due to the
interest in mining opinions of social media users. Sentiment analysis consists
in determining the polarity of a given text, i.e., its degree of positiveness
or negativeness. Traditionally, Sentiment Analysis algorithms have been
tailored to a specific language given the complexity of having a number of
lexical variations and errors introduced by the people generating content. In
this contribution, our aim is to provide a simple to implement and easy to use
multilingual framework, that can serve as a baseline for sentiment analysis
contests, and as starting point to build new sentiment analysis systems. We
compare our approach in eight different languages, three of them have important
international contests, namely, SemEval (English), TASS (Spanish), and
SENTIPOLC (Italian). Within the competitions our approach reaches from medium
to high positions in the rankings; whereas in the remaining languages our
approach outperforms the reported results.","['Eric S. Tellez', 'Sabino Miranda Jiménez', 'Mario Graff', 'Daniela Moctezuma', 'Ranyart R. Suárez', 'Oscar S. Siordia']","['cs.CL', 'cs.LG', 'stat.ML']",2016-12-15 21:07:12+00:00
http://arxiv.org/abs/1612.05251v1,Neural Networks for Joint Sentence Classification in Medical Paper Abstracts,"Existing models based on artificial neural networks (ANNs) for sentence
classification often do not incorporate the context in which sentences appear,
and classify sentences individually. However, traditional sentence
classification approaches have been shown to greatly benefit from jointly
classifying subsequent sentences, such as with conditional random fields. In
this work, we present an ANN architecture that combines the effectiveness of
typical ANN models to classify sentences in isolation, with the strength of
structured prediction. Our model achieves state-of-the-art results on two
different datasets for sequential sentence classification in medical abstracts.","['Franck Dernoncourt', 'Ji Young Lee', 'Peter Szolovits']","['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']",2016-12-15 20:57:56+00:00
http://arxiv.org/abs/1612.05231v3,Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs,"Using unitary (instead of general) matrices in artificial neural networks
(ANNs) is a promising way to solve the gradient explosion/vanishing problem, as
well as to enable ANNs to learn long-term correlations in the data. This
approach appears particularly promising for Recurrent Neural Networks (RNNs).
In this work, we present a new architecture for implementing an Efficient
Unitary Neural Network (EUNNs); its main advantages can be summarized as
follows. Firstly, the representation capacity of the unitary space in an EUNN
is fully tunable, ranging from a subspace of SU(N) to the entire unitary space.
Secondly, the computational complexity for training an EUNN is merely
$\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on
the standard copying task, the pixel-permuted MNIST digit recognition benchmark
as well as the Speech Prediction Test (TIMIT). We find that our architecture
significantly outperforms both other state-of-the-art unitary RNNs and the LSTM
architecture, in terms of the final performance and/or the wall-clock training
speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide
variety of applications.","['Li Jing', 'Yichen Shen', 'Tena Dubček', 'John Peurifoy', 'Scott Skirlo', 'Yann LeCun', 'Max Tegmark', 'Marin Soljačić']","['cs.LG', 'cs.NE', 'stat.ML']",2016-12-15 20:39:15+00:00
http://arxiv.org/abs/1612.05086v2,Coupling Adaptive Batch Sizes with Learning Rates,"Mini-batch stochastic gradient descent and variants thereof have become
standard for large-scale empirical risk minimization like the training of
neural networks. These methods are usually used with a constant batch size
chosen by simple empirical inspection. The batch size significantly influences
the behavior of the stochastic optimization algorithm, though, since it
determines the variance of the gradient estimates. This variance also changes
over the optimization process; when using a constant batch size, stability and
convergence is thus often enforced by means of a (manually tuned) decreasing
learning rate schedule.
  We propose a practical method for dynamic batch size adaptation. It estimates
the variance of the stochastic gradients and adapts the batch size to decrease
the variance proportionally to the value of the objective function, removing
the need for the aforementioned learning rate decrease. In contrast to recent
related work, our algorithm couples the batch size to the learning rate,
directly reflecting the known relationship between the two. On popular image
classification benchmarks, our batch size adaptation yields faster optimization
convergence, while simultaneously simplifying learning rate tuning. A
TensorFlow implementation is available.","['Lukas Balles', 'Javier Romero', 'Philipp Hennig']","['cs.LG', 'cs.CV', 'stat.ML']",2016-12-15 14:42:45+00:00
http://arxiv.org/abs/1612.05053v1,Expectation Propagation performs a smoothed gradient descent,"Bayesian inference is a popular method to build learning algorithms but it is
hampered by the fact that its key object, the posterior probability
distribution, is often uncomputable. Expectation Propagation (EP) (Minka
(2001)) is a popular algorithm that solves this issue by computing a parametric
approximation (e.g: Gaussian) to the density of the posterior. However, while
it is known empirically to quickly compute fine approximations, EP is extremely
poorly understood which prevents it from being adopted by a larger fraction of
the community.
  The object of the present article is to shed intuitive light on EP, by
relating it to other better understood methods. More precisely, we link it to
using gradient descent to compute the Laplace approximation of a target
probability distribution. We show that EP is exactly equivalent to performing
gradient descent on a smoothed energy landscape: i.e: the original energy
landscape convoluted with some smoothing kernel. This also relates EP to
algorithms that compute the Gaussian approximation which minimizes the reverse
KL divergence to the target distribution, a link that has been conjectured
before but has not been proved rigorously yet. These results can help
practitioners to get a better feel for how EP works, as well as lead to other
new results on this important method.",['Guillaume P. Dehaene'],"['stat.ML', 'stat.CO']",2016-12-15 13:16:46+00:00
http://arxiv.org/abs/1612.05048v1,Adversarial Message Passing For Graphical Models,"Bayesian inference on structured models typically relies on the ability to
infer posterior distributions of underlying hidden variables. However,
inference in implicit models or complex posterior distributions is hard. A
popular tool for learning implicit models are generative adversarial networks
(GANs) which learn parameters of generators by fooling discriminators.
Typically, GANs are considered to be models themselves and are not understood
in the context of inference. Current techniques rely on inefficient global
discrimination of joint distributions to perform learning, or only consider
discriminating a single output variable. We overcome these limitations by
treating GANs as a basis for likelihood-free inference in generative models and
generalize them to Bayesian posterior inference over factor graphs. We propose
local learning rules based on message passing minimizing a global divergence
criterion involving cooperating local adversaries used to sidestep explicit
likelihood evaluations. This allows us to compose models and yields a unified
inference and learning framework for adversarial learning. Our framework treats
model specification and inference separately and facilitates richly structured
models within the family of Directed Acyclic Graphs, including components such
as intractable likelihoods, non-differentiable models, simulators and generally
cumbersome models. A key result of our treatment is the insight that Bayesian
inference on structured models can be performed only with sampling and
discrimination when using nonparametric variational families, without access to
explicit distributions. As a side-result, we discuss the link to likelihood
maximization. These approaches hold promise to be useful in the toolbox of
probabilistic modelers and enrich the gamut of current probabilistic
programming applications.",['Theofanis Karaletsos'],"['stat.ML', 'cs.AI']",2016-12-15 12:59:33+00:00
http://arxiv.org/abs/1612.05024v2,Optimal structure and parameter learning of Ising models,"Reconstruction of structure and parameters of an Ising model from binary
samples is a problem of practical importance in a variety of disciplines,
ranging from statistical physics and computational biology to image processing
and machine learning. The focus of the research community shifted towards
developing universal reconstruction algorithms which are both computationally
efficient and require the minimal amount of expensive data. We introduce a new
method, Interaction Screening, which accurately estimates the model parameters
using local optimization problems. The algorithm provably achieves perfect
graph structure recovery with an information-theoretically optimal number of
samples, notably in the low-temperature regime which is known to be the hardest
for learning. The efficacy of Interaction Screening is assessed through
extensive numerical tests on synthetic Ising models of various topologies with
different types of interactions, as well as on a real data produced by a D-Wave
quantum computer. This study shows that the Interaction Screening method is an
exact, tractable and optimal technique universally solving the inverse Ising
problem.","['Andrey Y. Lokhov', 'Marc Vuffray', 'Sidhant Misra', 'Michael Chertkov']","['cond-mat.stat-mech', 'cs.LG', 'physics.data-an', 'stat.ML']",2016-12-15 11:30:40+00:00
http://arxiv.org/abs/1612.05001v1,Graph-based semi-supervised learning for relational networks,"We address the problem of semi-supervised learning in relational networks,
networks in which nodes are entities and links are the relationships or
interactions between them. Typically this problem is confounded with the
problem of graph-based semi-supervised learning (GSSL), because both problems
represent the data as a graph and predict the missing class labels of nodes.
However, not all graphs are created equally. In GSSL a graph is constructed,
often from independent data, based on similarity. As such, edges tend to
connect instances with the same class label. Relational networks, however, can
be more heterogeneous and edges do not always indicate similarity. For
instance, instead of links being more likely to connect nodes with the same
class label, they may occur more frequently between nodes with different class
labels (link-heterogeneity). Or nodes with the same class label do not
necessarily have the same type of connectivity across the whole network
(class-heterogeneity), e.g. in a network of sexual interactions we may observe
links between opposite genders in some parts of the graph and links between the
same genders in others. Performing classification in networks with different
types of heterogeneity is a hard problem that is made harder still when we do
not know a-priori the type or level of heterogeneity. Here we present two
scalable approaches for graph-based semi-supervised learning for the more
general case of relational networks. We demonstrate these approaches on
synthetic and real-world networks that display different link patterns within
and between classes. Compared to state-of-the-art approaches, ours give better
classification performance without prior knowledge of how classes interact. In
particular, our two-step label propagation algorithm gives consistently good
accuracy and runs on networks of over 1.6 million nodes and 30 million edges in
around 12 seconds.",['Leto Peel'],"['cs.SI', 'cs.LG', 'physics.data-an', 'stat.ML']",2016-12-15 10:15:57+00:00
http://arxiv.org/abs/1612.04933v1,Dynamical Kinds and their Discovery,"We demonstrate the possibility of classifying causal systems into kinds that
share a common structure without first constructing an explicit dynamical model
or using prior knowledge of the system dynamics. The algorithmic ability to
determine whether arbitrary systems are governed by causal relations of the
same form offers significant practical applications in the development and
validation of dynamical models. It is also of theoretical interest as an
essential stage in the scientific inference of laws from empirical data. The
algorithm presented is based on the dynamical symmetry approach to dynamical
kinds. A dynamical symmetry with respect to time is an intervention on one or
more variables of a system that commutes with the time evolution of the system.
A dynamical kind is a class of systems sharing a set of dynamical symmetries.
The algorithm presented classifies deterministic, time-dependent causal systems
by directly comparing their exhibited symmetries. Using simulated, noisy data
from a variety of nonlinear systems, we show that this algorithm correctly
sorts systems into dynamical kinds. It is robust under significant sampling
error, is immune to violations of normality in sampling error, and fails
gracefully with increasing dynamical similarity. The algorithm we demonstrate
is the first to address this aspect of automated scientific discovery.",['Benjamin C. Jantzen'],"['stat.ML', 'cs.AI', 'cs.LG', 'cs.SY']",2016-12-15 05:25:41+00:00
http://arxiv.org/abs/1612.04899v2,Semi-Supervised Phone Classification using Deep Neural Networks and Stochastic Graph-Based Entropic Regularization,"We describe a graph-based semi-supervised learning framework in the context
of deep neural networks that uses a graph-based entropic regularizer to favor
smooth solutions over a graph induced by the data. The main contribution of
this work is a computationally efficient, stochastic graph-regularization
technique that uses mini-batches that are consistent with the graph structure,
but also provides enough stochasticity (in terms of mini-batch data diversity)
for convergence of stochastic gradient descent methods to good solutions. For
this work, we focus on results of frame-level phone classification accuracy on
the TIMIT speech corpus but our method is general and scalable to much larger
data sets. Results indicate that our method significantly improves
classification accuracy compared to the fully-supervised case when the fraction
of labeled data is low, and it is competitive with other methods in the fully
labeled case.","['Sunil Thulasidasan', 'Jeffrey Bilmes']","['stat.ML', 'cs.LG']",2016-12-15 01:00:45+00:00
http://arxiv.org/abs/1612.04898v2,Efficient Distributed Semi-Supervised Learning using Stochastic Regularization over Affinity Graphs,"We describe a computationally efficient, stochastic graph-regularization
technique that can be utilized for the semi-supervised training of deep neural
networks in a parallel or distributed setting. We utilize a technique, first
described in [13] for the construction of mini-batches for stochastic gradient
descent (SGD) based on synthesized partitions of an affinity graph that are
consistent with the graph structure, but also preserve enough stochasticity for
convergence of SGD to good local minima. We show how our technique allows a
graph-based semi-supervised loss function to be decomposed into a sum over
objectives, facilitating data parallelism for scalable training of machine
learning models. Empirical results indicate that our method significantly
improves classification accuracy compared to the fully-supervised case when the
fraction of labeled data is low, and in the parallel case, achieves significant
speed-up in terms of wall-clock time to convergence. We show the results for
both sequential and distributed-memory semi-supervised DNN training on a speech
corpus.","['Sunil Thulasidasan', 'Jeffrey Bilmes', 'Garrett Kenyon']","['stat.ML', 'cs.DC', 'cs.LG']",2016-12-15 01:00:23+00:00
http://arxiv.org/abs/1612.04897v1,Learning binary or real-valued time-series via spike-timing dependent plasticity,"A dynamic Boltzmann machine (DyBM) has been proposed as a model of a spiking
neural network, and its learning rule of maximizing the log-likelihood of given
time-series has been shown to exhibit key properties of spike-timing dependent
plasticity (STDP), which had been postulated and experimentally confirmed in
the field of neuroscience as a learning rule that refines the Hebbian rule.
Here, we relax some of the constraints in the DyBM in a way that it becomes
more suitable for computation and learning. We show that learning the DyBM can
be considered as logistic regression for binary-valued time-series. We also
show how the DyBM can learn real-valued data in the form of a Gaussian DyBM and
discuss its relation to the vector autoregressive (VAR) model. The Gaussian
DyBM extends the VAR by using additional explanatory variables, which
correspond to the eligibility traces of the DyBM and capture long term
dependency of the time-series. Numerical experiments show that the Gaussian
DyBM significantly improves the predictive accuracy over VAR.",['Takayuki Osogami'],"['cs.NE', 'stat.ML']",2016-12-15 00:57:51+00:00
http://arxiv.org/abs/1612.04895v1,Projected Regression Methods for Inverting Fredholm Integrals: Formalism and Application to Analytical Continuation,"We present a machine learning approach to the inversion of Fredholm integrals
of the first kind. The approach provides a natural regularization in cases
where the inverse of the Fredholm kernel is ill-conditioned. It also provides
an efficient and stable treatment of constraints. The key observation is that
the stability of the forward problem permits the construction of a large
database of outputs for physically meaningful inputs. We apply machine learning
to this database to generate a regression function of controlled complexity,
which returns approximate solutions for previously unseen inputs; the
approximate solutions are then projected onto the subspace of functions
satisfying relevant constraints. We also derive and present uncertainty
estimates. We illustrate the approach by applying it to the analytical
continuation problem of quantum many-body physics, which involves
reconstructing the frequency dependence of physical excitation spectra from
data obtained at specific points in the complex frequency plane. Under standard
error metrics the method performs as well or better than the Maximum Entropy
method for low input noise and is substantially more robust to increased input
noise. We expect the methodology to be similarly effective for any problem
involving a formally ill-conditioned inversion, provided that the forward
problem can be efficiently solved.","['Louis-Francois Arsenault', 'Richard Neuberg', 'Lauren A. Hannah', 'Andrew J. Millis']","['cond-mat.str-el', 'stat.ML']",2016-12-15 00:51:08+00:00
http://arxiv.org/abs/1612.04891v1,Deep learning is effective for the classification of OCT images of normal versus Age-related Macular Degeneration,"Objective: The advent of Electronic Medical Records (EMR) with large
electronic imaging databases along with advances in deep neural networks with
machine learning has provided a unique opportunity to achieve milestones in
automated image analysis. Optical coherence tomography (OCT) is the most
commonly obtained imaging modality in ophthalmology and represents a dense and
rich dataset when combined with labels derived from the EMR. We sought to
determine if deep learning could be utilized to distinguish normal OCT images
from images from patients with Age-related Macular Degeneration (AMD). Methods:
Automated extraction of an OCT imaging database was performed and linked to
clinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg
Spectralis, and each OCT scan was linked to EMR clinical endpoints extracted
from EPIC. The central 11 images were selected from each OCT scan of two
cohorts of patients: normal and AMD. Cross-validation was performed using a
random subset of patients. Area under receiver operator curves (auROC) were
constructed at an independent image level, macular OCT level, and patient
level. Results: Of an extraction of 2.6 million OCT images linked to clinical
datapoints from the EMR, 52,690 normal and 48,312 AMD macular OCT images were
selected. A deep neural network was trained to categorize images as either
normal or AMD. At the image level, we achieved an auROC of 92.78% with an
accuracy of 87.63%. At the macula level, we achieved an auROC of 93.83% with an
accuracy of 88.98%. At a patient level, we achieved an auROC of 97.45% with an
accuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were
92.64% and 93.69% respectively. Conclusions: Deep learning techniques are
effective for classifying OCT images. These findings have important
implications in utilizing OCT in automated screening and computer aided
diagnosis tools.","['Cecilia S. Lee', 'Doug M. Baughman', 'Aaron Y. Lee']","['stat.ML', 'cs.CV', 'cs.LG']",2016-12-15 00:23:43+00:00
http://arxiv.org/abs/1612.04875v1,Robust Local Scaling using Conditional Quantiles of Graph Similarities,"Spectral analysis of neighborhood graphs is one of the most widely used
techniques for exploratory data analysis, with applications ranging from
machine learning to social sciences. In such applications, it is typical to
first encode relationships between the data samples using an appropriate
similarity function. Popular neighborhood construction techniques such as
k-nearest neighbor (k-NN) graphs are known to be very sensitive to the choice
of parameters, and more importantly susceptible to noise and varying densities.
In this paper, we propose the use of quantile analysis to obtain local scale
estimates for neighborhood graph construction. To this end, we build an
auto-encoding neural network approach for inferring conditional quantiles of a
similarity function, which are subsequently used to obtain robust estimates of
the local scales. In addition to being highly resilient to noise or outlying
data, the proposed approach does not require extensive parameter tuning unlike
several existing methods. Using applications in spectral clustering and
single-example label propagation, we show that the proposed neighborhood graphs
outperform existing locally scaled graph construction approaches.","['Jayaraman J. Thiagarajan', 'Prasanna Sattigeri', 'Karthikeyan Natesan Ramamurthy', 'Bhavya Kailkhura']",['stat.ML'],2016-12-14 23:11:49+00:00
http://arxiv.org/abs/1612.04853v1,Constraint Selection in Metric Learning,"A number of machine learning algorithms are using a metric, or a distance, in
order to compare individuals. The Euclidean distance is usually employed, but
it may be more efficient to learn a parametric distance such as Mahalanobis
metric. Learning such a metric is a hot topic since more than ten years now,
and a number of methods have been proposed to efficiently learn it. However,
the nature of the problem makes it quite difficult for large scale data, as
well as data for which classes overlap. This paper presents a simple way of
improving accuracy and scalability of any iterative metric learning algorithm,
where constraints are obtained prior to the algorithm. The proposed approach
relies on a loss-dependent weighted selection of constraints that are used for
learning the metric. Using the corresponding dedicated loss function, the
method clearly allows to obtain better results than state-of-the-art methods,
both in terms of accuracy and time complexity. Some experimental results on
real world, and potentially large, datasets are demonstrating the effectiveness
of our proposition.",['Hoel Le Capitaine'],"['cs.LG', 'stat.ML']",2016-12-14 21:45:14+00:00
http://arxiv.org/abs/1612.04831v1,Uncovering the Dynamics of Crowdlearning and the Value of Knowledge,"Learning from the crowd has become increasingly popular in the Web and social
media. There is a wide variety of crowdlearning sites in which, on the one
hand, users learn from the knowledge that other users contribute to the site,
and, on the other hand, knowledge is reviewed and curated by the same users
using assessment measures such as upvotes or likes.
  In this paper, we present a probabilistic modeling framework of
crowdlearning, which uncovers the evolution of a user's expertise over time by
leveraging other users' assessments of her contributions. The model allows for
both off-site and on-site learning and captures forgetting of knowledge. We
then develop a scalable estimation method to fit the model parameters from
millions of recorded learning and contributing events. We show the
effectiveness of our model by tracing activity of ~25 thousand users in Stack
Overflow over a 4.5 year period. We find that answers with high knowledge value
are rare. Newbies and experts tend to acquire less knowledge than users in the
middle range. Prolific learners tend to be also proficient contributors that
post answers with high knowledge value.","['Utkarsh Upadhyay', 'Isabel Valera', 'Manuel Gomez-Rodriguez']","['cs.SI', 'cs.LG', 'physics.soc-ph', 'stat.ML', 'H.2.8']",2016-12-14 21:00:11+00:00
http://arxiv.org/abs/1612.04799v2,Deep Function Machines: Generalized Neural Networks for Topological Layer Expression,"In this paper we propose a generalization of deep neural networks called deep
function machines (DFMs). DFMs act on vector spaces of arbitrary (possibly
infinite) dimension and we show that a family of DFMs are invariant to the
dimension of input data; that is, the parameterization of the model does not
directly hinge on the quality of the input (eg. high resolution images). Using
this generalization we provide a new theory of universal approximation of
bounded non-linear operators between function spaces. We then suggest that DFMs
provide an expressive framework for designing new neural network layer types
with topological considerations in mind. Finally, we introduce a novel
architecture, RippLeNet, for resolution invariant computer vision, which
empirically achieves state of the art invariance.",['William H. Guss'],"['stat.ML', 'cs.CV', 'cs.LG']",2016-12-14 20:39:23+00:00
http://arxiv.org/abs/1612.04785v1,Quantum Monte Carlo simulation of a particular class of non-stoquastic Hamiltonians in quantum annealing,"Quantum annealing is a generic solver of the optimization problem that uses
fictitious quantum fluctuation. Its simulation in classical computing is often
performed using the quantum Monte Carlo simulation via the Suzuki--Trotter
decomposition. However, the negative sign problem sometimes emerges in the
simulation of quantum annealing with an elaborate driver Hamiltonian, since it
belongs to a class of non-stoquastic Hamiltonians. In the present study, we
propose an alternative way to avoid the negative sign problem involved in a
particular class of the non-stoquastic Hamiltonians. To check the validity of
the method, we demonstrate our method by applying it to a simple problem that
includes the anti-ferromagnetic XX interaction, which is a typical instance of
the non-stoquastic Hamiltonians.",['Masayuki Ohzeki'],"['quant-ph', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'stat.ML']",2016-12-14 19:59:54+00:00
http://arxiv.org/abs/1612.04759v2,Encapsulating models and approximate inference programs in probabilistic modules,"This paper introduces the probabilistic module interface, which allows
encapsulation of complex probabilistic models with latent variables alongside
custom stochastic approximate inference machinery, and provides a
platform-agnostic abstraction barrier separating the model internals from the
host probabilistic inference system. The interface can be seen as a stochastic
generalization of a standard simulation and density interface for probabilistic
primitives. We show that sound approximate inference algorithms can be
constructed for networks of probabilistic modules, and we demonstrate that the
interface can be implemented using learned stochastic inference networks and
MCMC and SMC approximate inference programs.","['Marco F. Cusumano-Towner', 'Vikash K. Mansinghka']","['cs.AI', 'cs.LG', 'stat.ML']",2016-12-14 18:14:59+00:00
http://arxiv.org/abs/1612.04717v7,Network cross-validation by edge sampling,"While many statistical models and methods are now available for network
analysis, resampling network data remains a challenging problem.
Cross-validation is a useful general tool for model selection and parameter
tuning, but is not directly applicable to networks since splitting network
nodes into groups requires deleting edges and destroys some of the network
structure. Here we propose a new network resampling strategy based on splitting
node pairs rather than nodes applicable to cross-validation for a wide range of
network model selection tasks. We provide a theoretical justification for our
method in a general setting and examples of how our method can be used in
specific network model selection and parameter tuning tasks. Numerical results
on simulated networks and on a citation network of statisticians show that this
cross-validation approach works well for model selection.","['Tianxi Li', 'Elizaveta Levina', 'Ji Zhu']","['stat.ME', 'stat.ML']",2016-12-14 16:36:30+00:00
http://arxiv.org/abs/1612.04679v2,IEDC: An Integrated Approach for Overlapping and Non-overlapping Community Detection,"Community detection is a task of fundamental importance in social network
analysis that can be used in a variety of knowledge-based domains. While there
exist many works on community detection based on connectivity structures, they
suffer from either considering the overlapping or non-overlapping communities.
In this work, we propose a novel approach for general community detection
through an integrated framework to extract the overlapping and non-overlapping
community structures without assuming prior structural connectivity on
networks. Our general framework is based on a primary node based criterion
which consists of the internal association degree along with the external
association degree. The evaluation of the proposed method is investigated
through the extensive simulation experiments and several benchmark real network
datasets. The experimental results show that the proposed method outperforms
the earlier state-of-the-art algorithms based on the well-known evaluation
criteria.","['Mahdi Hajiabadi', 'Hadi Zare', 'Hossein Bobarshad']","['cs.SI', 'stat.ML']",2016-12-14 15:14:45+00:00
http://arxiv.org/abs/1612.04642v2,Harmonic Networks: Deep Translation and Rotation Equivariance,"Translating or rotating an input image should not affect the results of many
computer vision tasks. Convolutional neural networks (CNNs) are already
translation equivariant: input image translations produce proportionate feature
map translations. This is not the case for rotations. Global rotation
equivariance is typically sought through data augmentation, but patch-wise
equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN
exhibiting equivariance to patch-wise translation and 360-rotation. We achieve
this by replacing regular CNN filters with circular harmonics, returning a
maximal response and orientation for every receptive field patch.
  H-Nets use a rich, parameter-efficient and low computational complexity
representation, and we show that deep feature maps within the network encode
complicated rotational invariants. We demonstrate that our layers are general
enough to be used in conjunction with the latest architectures and techniques,
such as deep supervision and batch normalization. We also achieve
state-of-the-art classification on rotated-MNIST, and competitive results on
other benchmark challenges.","['Daniel E. Worrall', 'Stephan J. Garbin', 'Daniyar Turmukhambetov', 'Gabriel J. Brostow']","['cs.CV', 'cs.LG', 'stat.ML']",2016-12-14 14:01:11+00:00
http://arxiv.org/abs/1612.04600v2,Predicting Process Behaviour using Deep Learning,"Predicting business process behaviour is an important aspect of business
process management. Motivated by research in natural language processing, this
paper describes an application of deep learning with recurrent neural networks
to the problem of predicting the next event in a business process. This is both
a novel method in process prediction, which has largely relied on explicit
process models, and also a novel application of deep learning methods. The
approach is evaluated on two real datasets and our results surpass the
state-of-the-art in prediction precision.","['Joerg Evermann', 'Jana-Rebecca Rehse', 'Peter Fettke']","['cs.LG', 'stat.ML']",2016-12-14 12:33:28+00:00
http://arxiv.org/abs/1612.04555v1,Scalable Group Level Probabilistic Sparse Factor Analysis,"Many data-driven approaches exist to extract neural representations of
functional magnetic resonance imaging (fMRI) data, but most of them lack a
proper probabilistic formulation. We propose a group level scalable
probabilistic sparse factor analysis (psFA) allowing spatially sparse maps,
component pruning using automatic relevance determination (ARD) and subject
specific heteroscedastic spatial noise modeling. For task-based and resting
state fMRI, we show that the sparsity constraint gives rise to components
similar to those obtained by group independent component analysis. The noise
modeling shows that noise is reduced in areas typically associated with
activation by the experimental design. The psFA model identifies sparse
components and the probabilistic setting provides a natural way to handle
parameter uncertainties. The variational Bayesian framework easily extends to
more complex noise models than the presently considered.","['Jesper L. Hinrich', 'Søren F. V. Nielsen', 'Nicolai A. B. Riis', 'Casper T. Eriksen', 'Jacob Frøsig', 'Marco D. F. Kristensen', 'Mikkel N. Schmidt', 'Kristoffer H. Madsen', 'Morten Mørup']","['stat.AP', 'stat.ML']",2016-12-14 09:59:51+00:00
http://arxiv.org/abs/1612.04530v1,Permutation-equivariant neural networks applied to dynamics prediction,"The introduction of convolutional layers greatly advanced the performance of
neural networks on image tasks due to innately capturing a way of encoding and
learning translation-invariant operations, matching one of the underlying
symmetries of the image domain. In comparison, there are a number of problems
in which there are a number of different inputs which are all 'of the same
type' --- multiple particles, multiple agents, multiple stock prices, etc. The
corresponding symmetry to this is permutation symmetry, in that the algorithm
should not depend on the specific ordering of the input data. We discuss a
permutation-invariant neural network layer in analogy to convolutional layers,
and show the ability of this architecture to learn to predict the motion of a
variable number of interacting hard discs in 2D. In the same way that
convolutional layers can generalize to different image sizes, the permutation
layer we describe generalizes to different numbers of objects.","['Nicholas Guttenberg', 'Nathaniel Virgo', 'Olaf Witkowski', 'Hidetoshi Aoki', 'Ryota Kanai']","['cs.CV', 'stat.ML']",2016-12-14 08:31:53+00:00
http://arxiv.org/abs/1612.04526v2,Astronomical image reconstruction with convolutional neural networks,"State of the art methods in astronomical image reconstruction rely on the
resolution of a regularized or constrained optimization problem. Solving this
problem can be computationally intensive and usually leads to a quadratic or at
least superlinear complexity w.r.t. the number of pixels in the image. We
investigate in this work the use of convolutional neural networks for image
reconstruction in astronomy. With neural networks, the computationally
intensive tasks is the training step, but the prediction step has a fixed
complexity per pixel, i.e. a linear complexity. Numerical experiments show that
our approach is both computationally efficient and competitive with other state
of the art methods in addition to being interpretable.",['Rémi Flamary'],"['cs.CV', 'astro-ph.IM', 'stat.ML']",2016-12-14 08:17:07+00:00
http://arxiv.org/abs/1612.04468v1,Sparse Factorization Layers for Neural Networks with Limited Supervision,"Whereas CNNs have demonstrated immense progress in many vision problems, they
suffer from a dependence on monumental amounts of labeled training data. On the
other hand, dictionary learning does not scale to the size of problems that
CNNs can handle, despite being very effective at low-level vision tasks such as
denoising and inpainting. Recently, interest has grown in adapting dictionary
learning methods for supervised tasks such as classification and inverse
problems. We propose two new network layers that are based on dictionary
learning: a sparse factorization layer and a convolutional sparse factorization
layer, analogous to fully-connected and convolutional layers, respectively.
Using our derivations, these layers can be dropped in to existing CNNs, trained
together in an end-to-end fashion with back-propagation, and leverage
semisupervision in ways classical CNNs cannot. We experimentally compare
networks with these two new layers against a baseline CNN. Our results
demonstrate that networks with either of the sparse factorization layers are
able to outperform classical CNNs when supervised data are few. They also show
performance improvements in certain tasks when compared to the CNN with no
sparse factorization layers with the same exact number of parameters.","['Parker Koch', 'Jason J. Corso']","['cs.CV', 'cs.AI', 'stat.ML']",2016-12-14 03:13:29+00:00
http://arxiv.org/abs/1701.03436v1,Fast Stability Scanning for Future Grid Scenario Analysis,"Future grid scenario analysis requires a major departure from conventional
power system planning, where only a handful of most critical conditions is
typically analyzed. To capture the inter-seasonal variations in renewable
generation of a future grid scenario necessitates the use of computationally
intensive time-series analysis. In this paper, we propose a planning framework
for fast stability scanning of future grid scenarios using a novel feature
selection algorithm and a novel self-adaptive PSO-k-means clustering algorithm.
To achieve the computational speed-up, the stability analysis is performed only
on small number of representative cluster centroids instead of on the full set
of operating conditions. As a case study, we perform small-signal stability and
steady-state voltage stability scanning of a simplified model of the Australian
National Electricity Market with significant penetration of renewable
generation. The simulation results show the effectiveness of the proposed
approach. Compared to an exhaustive time series scanning, the proposed
framework reduced the computational burden up to ten times, with an acceptable
level of accuracy.","['Ruidong Liu', 'Gregor Verbic', 'Jin Ma']","['cs.CY', 'stat.ML']",2016-12-14 00:27:18+00:00
http://arxiv.org/abs/1612.04440v2,Disentangling Space and Time in Video with Hierarchical Variational Auto-encoders,"There are many forms of feature information present in video data. Principle
among them are object identity information which is largely static across
multiple video frames, and object pose and style information which continuously
transforms from frame to frame. Most existing models confound these two types
of representation by mapping them to a shared feature space. In this paper we
propose a probabilistic approach for learning separable representations of
object identity and pose information using unsupervised video data. Our
approach leverages a deep generative model with a factored prior distribution
that encodes properties of temporal invariances in the hidden feature set.
Learning is achieved via variational inference. We present results of learning
identity and pose information on a dataset of moving characters as well as a
dataset of rotating 3D objects. Our experimental results demonstrate our
model's success in factoring its representation, and demonstrate that the model
achieves improved performance in transfer learning tasks.","['Will Grathwohl', 'Aaron Wilson']","['cs.CV', 'cs.LG', 'stat.ML']",2016-12-14 00:20:46+00:00
http://arxiv.org/abs/1612.04425v2,On the Convergence of Asynchronous Parallel Iteration with Unbounded Delays,"Recent years have witnessed the surge of asynchronous parallel
(async-parallel) iterative algorithms due to problems involving very
large-scale data and a large number of decision variables. Because of
asynchrony, the iterates are computed with outdated information, and the age of
the outdated information, which we call delay, is the number of times it has
been updated since its creation. Almost all recent works prove convergence
under the assumption of a finite maximum delay and set their stepsize
parameters accordingly. However, the maximum delay is practically unknown.
  This paper presents convergence analysis of an async-parallel method from a
probabilistic viewpoint, and it allows for large unbounded delays. An explicit
formula of stepsize that guarantees convergence is given depending on delays'
statistics. With $p+1$ identical processors, we empirically measured that
delays closely follow the Poisson distribution with parameter $p$, matching our
theoretical model, and thus the stepsize can be set accordingly. Simulations on
both convex and nonconvex optimization problems demonstrate the validness of
our analysis and also show that the existing maximum-delay induced stepsize is
too conservative, often slowing down the convergence of the algorithm.","['Zhimin Peng', 'Yangyang Xu', 'Ming Yan', 'Wotao Yin']","['math.OC', 'cs.DC', 'math.NA', 'stat.ML']",2016-12-13 23:02:26+00:00
http://arxiv.org/abs/1612.04423v1,Modeling cognitive deficits following neurodegenerative diseases and traumatic brain injuries with deep convolutional neural networks,"The accurate diagnosis and assessment of neurodegenerative disease and
traumatic brain injuries (TBI) remain open challenges. Both cause cognitive and
functional deficits due to focal axonal swellings (FAS), but it is difficult to
deliver a prognosis due to our limited ability to assess damaged neurons at a
cellular level in vivo. We simulate the effects of neurodegenerative disease
and TBI using convolutional neural networks (CNNs) as our model of cognition.
We utilize biophysically relevant statistical data on FAS to damage the
connections in CNNs in a functionally relevant way. We incorporate energy
constraints on the brain by pruning the CNNs to be less over-engineered.
Qualitatively, we demonstrate that damage leads to human-like mistakes. Our
experiments also provide quantitative assessments of how accuracy is affected
by various types and levels of damage. The deficit resulting from a fixed
amount of damage greatly depends on which connections are randomly injured,
providing intuition for why it is difficult to predict impairments. There is a
large degree of subjectivity when it comes to interpreting cognitive deficits
from complex systems such as the human brain. However, we provide important
insight and a quantitative framework for disorders in which FAS are implicated.","['Bethany Lusch', 'Jake Weholt', 'Pedro D. Maia', 'J. Nathan Kutz']","['q-bio.NC', 'q-bio.QM', 'stat.ML']",2016-12-13 22:50:56+00:00
http://arxiv.org/abs/1612.04418v1,User Model-Based Intent-Aware Metrics for Multilingual Search Evaluation,"Despite the growing importance of multilingual aspect of web search, no
appropriate offline metrics to evaluate its quality are proposed so far. At the
same time, personal language preferences can be regarded as intents of a query.
This approach translates the multilingual search problem into a particular task
of search diversification. Furthermore, the standard intent-aware approach
could be adopted to build a diversified metric for multilingual search on the
basis of a classical IR metric such as ERR. The intent-aware approach estimates
user satisfaction under a user behavior model. We show however that the
underlying user behavior models is not realistic in the multilingual case, and
the produced intent-aware metric do not appropriately estimate the user
satisfaction. We develop a novel approach to build intent-aware user behavior
models, which overcome these limitations and convert to quality metrics that
better correlate with standard online metrics of user satisfaction.","['Alexey Drutsa', 'Andrey Shutovich', 'Philipp Pushnyakov', 'Evgeniy Krokhalyov', 'Gleb Gusev', 'Pavel Serdyukov']","['cs.IR', 'cs.CL', 'cs.HC', 'cs.LG', 'stat.ML', 'H.1.2; H.5.2']",2016-12-13 22:09:24+00:00
http://arxiv.org/abs/1612.04413v1,Inferring object rankings based on noisy pairwise comparisons from multiple annotators,"Ranking a set of objects involves establishing an order allowing for
comparisons between any pair of objects in the set. Oftentimes, due to the
unavailability of a ground truth of ranked orders, researchers resort to
obtaining judgments from multiple annotators followed by inferring the ground
truth based on the collective knowledge of the crowd. However, the aggregation
is often ad-hoc and involves imposing stringent assumptions in inferring the
ground truth (e.g. majority vote). In this work, we propose
Expectation-Maximization (EM) based algorithms that rely on the judgments from
multiple annotators and the object attributes for inferring the latent ground
truth. The algorithm learns the relation between the latent ground truth and
object attributes as well as annotator specific probabilities of flipping, a
metric to assess annotator quality. We further extend the EM algorithm to allow
for a variable probability of flipping based on the pair of objects at hand. We
test our algorithms on two data sets with synthetic annotations and investigate
the impact of annotator quality and quantity on the inferred ground truth. We
also obtain the results on two other data sets with annotations from
machine/human annotators and interpret the output trends based on the data
characteristics.","['Rahul Gupta', 'Shrikanth Narayanan']","['stat.ML', 'cs.LG']",2016-12-13 21:52:12+00:00
http://arxiv.org/abs/1612.04357v4,Stacked Generative Adversarial Networks,"In this paper, we propose a novel generative model named Stacked Generative
Adversarial Networks (SGAN), which is trained to invert the hierarchical
representations of a bottom-up discriminative network. Our model consists of a
top-down stack of GANs, each learned to generate lower-level representations
conditioned on higher-level representations. A representation discriminator is
introduced at each feature hierarchy to encourage the representation manifold
of the generator to align with that of the bottom-up discriminative network,
leveraging the powerful discriminative representations to guide the generative
model. In addition, we introduce a conditional loss that encourages the use of
conditional information from the layer above, and a novel entropy loss that
maximizes a variational lower bound on the conditional entropy of generator
outputs. We first train each stack independently, and then train the whole
model end-to-end. Unlike the original GAN that uses a single noise vector to
represent all the variations, our SGAN decomposes variations into multiple
levels and gradually resolves uncertainties in the top-down generative process.
Based on visual inspection, Inception scores and visual Turing test, we
demonstrate that SGAN is able to generate images of much higher quality than
GANs without stacking.","['Xun Huang', 'Yixuan Li', 'Omid Poursaeed', 'John Hopcroft', 'Serge Belongie']","['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']",2016-12-13 20:48:58+00:00
http://arxiv.org/abs/1612.04340v1,End-to-End Deep Reinforcement Learning for Lane Keeping Assist,"Reinforcement learning is considered to be a strong AI paradigm which can be
used to teach machines through interaction with the environment and learning
from their mistakes, but it has not yet been successfully used for automotive
applications. There has recently been a revival of interest in the topic,
however, driven by the ability of deep learning algorithms to learn good
representations of the environment. Motivated by Google DeepMind's successful
demonstrations of learning for games from Breakout to Go, we will propose
different methods for autonomous driving using deep reinforcement learning.
This is of particular interest as it is difficult to pose autonomous driving as
a supervised learning problem as it has a strong interaction with the
environment including other vehicles, pedestrians and roadworks. As this is a
relatively new area of research for autonomous driving, we will formulate two
main categories of algorithms: 1) Discrete actions category, and 2) Continuous
actions category. For the discrete actions category, we will deal with Deep
Q-Network Algorithm (DQN) while for the continuous actions category, we will
deal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to
that, We will also discover the performance of these two categories on an open
source car simulator for Racing called (TORCS) which stands for The Open Racing
car Simulator. Our simulation results demonstrate learning of autonomous
maneuvering in a scenario of complex road curvatures and simple interaction
with other vehicles. Finally, we explain the effect of some restricted
conditions, put on the car during the learning phase, on the convergence time
for finishing its learning phase.","['Ahmad El Sallab', 'Mohammed Abdou', 'Etienne Perot', 'Senthil Yogamani']","['stat.ML', 'cs.LG', 'cs.RO']",2016-12-13 20:19:42+00:00
http://arxiv.org/abs/1612.04315v1,Towards Adaptive Training of Agent-based Sparring Partners for Fighter Pilots,"A key requirement for the current generation of artificial decision-makers is
that they should adapt well to changes in unexpected situations. This paper
addresses the situation in which an AI for aerial dog fighting, with tunable
parameters that govern its behavior, must optimize behavior with respect to an
objective function that is evaluated and learned through simulations. Bayesian
optimization with a Gaussian Process surrogate is used as the method for
investigating the objective function. One key benefit is that during
optimization, the Gaussian Process learns a global estimate of the true
objective function, with predicted outcomes and a statistical measure of
confidence in areas that haven't been investigated yet. Having a model of the
objective function is important for being able to understand possible outcomes
in the decision space; for example this is crucial for training and providing
feedback to human pilots. However, standard Bayesian optimization does not
perform consistently or provide an accurate Gaussian Process surrogate function
for highly volatile objective functions. We treat these problems by introducing
a novel sampling technique called Hybrid Repeat/Multi-point Sampling. This
technique gives the AI ability to learn optimum behaviors in a highly uncertain
environment. More importantly, it not only improves the reliability of the
optimization, but also creates a better model of the entire objective surface.
With this improved model the agent is equipped to more accurately/efficiently
predict performance in unexplored scenarios.","['Brett W. Israelsen', 'Nisar Ahmed', 'Kenneth Center', 'Roderick Green', 'Winston Bennett Jr']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.RO']",2016-12-13 18:43:59+00:00
http://arxiv.org/abs/1612.04262v3,An equation-of-state-meter of QCD transition from deep learning,"Supervised learning with a deep convolutional neural network is used to
identify the QCD equation of state (EoS) employed in relativistic hydrodynamic
simulations of heavy-ion collisions from the simulated final-state particle
spectra $\rho(p_T,\Phi)$. High-level correlations of $\rho(p_T,\Phi)$ learned
by the neural network act as an effective ""EoS-meter"" in detecting the nature
of the QCD transition. The EoS-meter is model independent and insensitive to
other simulation inputs, especially the initial conditions. Thus it provides a
powerful direct-connection of heavy-ion collision observables with the bulk
properties of QCD.","['Long-Gang Pang', 'Kai Zhou', 'Nan Su', 'Hannah Petersen', 'Horst Stöcker', 'Xin-Nian Wang']","['hep-ph', 'cs.LG', 'hep-th', 'nucl-th', 'stat.ML']",2016-12-13 16:19:00+00:00
http://arxiv.org/abs/1612.04174v2,Models of retrieval in sentence comprehension: A computational evaluation using Bayesian hierarchical modeling,"Research on interference has provided evidence that the formation of
dependencies between non-adjacent words relies on a cue-based retrieval
mechanism. Two different models can account for one of the main predictions of
interference, i.e., a slowdown at a retrieval site, when several items share a
feature associated with a retrieval cue: Lewis and Vasishth's (2005)
activation-based model and McElree's (2000) direct access model. Even though
these two models have been used almost interchangeably, they are based on
different assumptions and predict differences in the relationship between
reading times and response accuracy. The activation-based model follows the
assumptions of ACT-R, and its retrieval process behaves as a lognormal race
between accumulators of evidence with a single variance. Under this model,
accuracy of the retrieval is determined by the winner of the race and retrieval
time by its rate of accumulation. In contrast, the direct access model assumes
a model of memory where only the probability of retrieval varies between items;
in this model, differences in latencies are a by-product of the possibility and
repairing incorrect retrievals. We implemented both models in a Bayesian
hierarchical framework in order to evaluate them and compare them. We show that
some aspects of the data are better fit under the direct access model than
under the activation-based model. We suggest that this finding does not rule
out the possibility that retrieval may be behaving as a race model with
assumptions that follow less closely the ones from the ACT-R framework. We show
that by introducing a modification of the activation model, i.e, by assuming
that the accumulation of evidence for retrieval of incorrect items is not only
slower but noisier (i.e., different variances for the correct and incorrect
items), the model can provide a fit as good as the one of the direct access
model.","['Bruno Nicenboim', 'Shravan Vasishth']","['cs.CL', 'stat.AP', 'stat.ML']",2016-12-13 13:55:39+00:00
http://arxiv.org/abs/1612.04112v5,Upper Bound of Bayesian Generalization Error in Non-negative Matrix Factorization,"Non-negative matrix factorization (NMF) is a new knowledge discovery method
that is used for text mining, signal processing, bioinformatics, and consumer
analysis. However, its basic property as a learning machine is not yet
clarified, as it is not a regular statistical model, resulting that theoretical
optimization method of NMF has not yet established. In this paper, we study the
real log canonical threshold of NMF and give an upper bound of the
generalization error in Bayesian learning. The results show that the
generalization error of the matrix factorization can be made smaller than
regular statistical models if Bayesian learning is applied.","['Naoki Hayashi', 'Sumio Watanabe']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2016-12-13 12:02:24+00:00
http://arxiv.org/abs/1612.04111v1,Parsimonious Online Learning with Kernels via Sparse Projections in Function Space,"Despite their attractiveness, popular perception is that techniques for
nonparametric function approximation do not scale to streaming data due to an
intractable growth in the amount of storage they require. To solve this problem
in a memory-affordable way, we propose an online technique based on functional
stochastic gradient descent in tandem with supervised sparsification based on
greedy function subspace projections. The method, called parsimonious online
learning with kernels (POLK), provides a controllable tradeoff? between its
solution accuracy and the amount of memory it requires. We derive conditions
under which the generated function sequence converges almost surely to the
optimal function, and we establish that the memory requirement remains finite.
We evaluate POLK for kernel multi-class logistic regression and kernel
hinge-loss classification on three canonical data sets: a synthetic Gaussian
mixture model, the MNIST hand-written digits, and the Brodatz texture database.
On all three tasks, we observe a favorable tradeoff of objective function
evaluation, classification performance, and complexity of the nonparametric
regressor extracted the proposed method.","['Alec Koppel', 'Garrett Warnell', 'Ethan Stump', 'Alejandro Ribeiro']","['stat.ML', 'cs.LG']",2016-12-13 12:01:28+00:00
http://arxiv.org/abs/1612.04052v1,Theory and Tools for the Conversion of Analog to Spiking Convolutional Neural Networks,"Deep convolutional neural networks (CNNs) have shown great potential for
numerous real-world machine learning applications, but performing inference in
large CNNs in real-time remains a challenge. We have previously demonstrated
that traditional CNNs can be converted into deep spiking neural networks
(SNNs), which exhibit similar accuracy while reducing both latency and
computational load as a consequence of their data-driven, event-based style of
computing. Here we provide a novel theory that explains why this conversion is
successful, and derive from it several new tools to convert a larger and more
powerful class of deep networks into SNNs. We identify the main sources of
approximation errors in previous conversion methods, and propose simple
mechanisms to fix these issues. Furthermore, we develop spiking implementations
of common CNN operations such as max-pooling, softmax, and batch-normalization,
which allow almost loss-less conversion of arbitrary CNN architectures into the
spiking domain. Empirical evaluation of different network architectures on the
MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.","['Bodo Rueckauer', 'Iulia-Alexandra Lungu', 'Yuhuang Hu', 'Michael Pfeiffer']","['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']",2016-12-13 07:58:34+00:00
http://arxiv.org/abs/1612.04022v3,Distributed Multi-Task Relationship Learning,"Multi-task learning aims to learn multiple tasks jointly by exploiting their
relatedness to improve the generalization performance for each task.
Traditionally, to perform multi-task learning, one needs to centralize data
from all the tasks to a single machine. However, in many real-world
applications, data of different tasks may be geo-distributed over different
local machines. Due to heavy communication caused by transmitting the data and
the issue of data privacy and security, it is impossible to send data of
different task to a master machine to perform multi-task learning. Therefore,
in this paper, we propose a distributed multi-task learning framework that
simultaneously learns predictive models for each task as well as task
relationships between tasks alternatingly in the parameter server paradigm. In
our framework, we first offer a general dual form for a family of regularized
multi-task relationship learning methods. Subsequently, we propose a
communication-efficient primal-dual distributed optimization algorithm to solve
the dual problem by carefully designing local subproblems to make the dual
problem decomposable. Moreover, we provide a theoretical convergence analysis
for the proposed algorithm, which is specific for distributed multi-task
relationship learning. We conduct extensive experiments on both synthetic and
real-world datasets to evaluate our proposed framework in terms of
effectiveness and convergence.","['Sulin Liu', 'Sinno Jialin Pan', 'Qirong Ho']","['cs.LG', 'stat.ML']",2016-12-13 04:22:10+00:00
http://arxiv.org/abs/1612.04021v1,Generative Adversarial Parallelization,"Generative Adversarial Networks have become one of the most studied
frameworks for unsupervised learning due to their intuitive formulation. They
have also been shown to be capable of generating convincing examples in limited
domains, such as low-resolution images. However, they still prove difficult to
train in practice and tend to ignore modes of the data generating distribution.
Quantitatively capturing effects such as mode coverage and more generally the
quality of the generative model still remain elusive. We propose Generative
Adversarial Parallelization, a framework in which many GANs or their variants
are trained simultaneously, exchanging their discriminators. This eliminates
the tight coupling between a generator and discriminator, leading to improved
convergence and improved coverage of modes. We also propose an improved variant
of the recently proposed Generative Adversarial Metric and show how it can
score individual GANs or their collections under the GAP model.","['Daniel Jiwoong Im', 'He Ma', 'Chris Dongjoo Kim', 'Graham Taylor']","['cs.LG', 'stat.ML']",2016-12-13 04:19:04+00:00
http://arxiv.org/abs/1612.03981v1,Hybrid Repeat/Multi-point Sampling for Highly Volatile Objective Functions,"A key drawback of the current generation of artificial decision-makers is
that they do not adapt well to changes in unexpected situations. This paper
addresses the situation in which an AI for aerial dog fighting, with tunable
parameters that govern its behavior, will optimize behavior with respect to an
objective function that must be evaluated and learned through simulations. Once
this objective function has been modeled, the agent can then choose its desired
behavior in different situations. Bayesian optimization with a Gaussian Process
surrogate is used as the method for investigating the objective function. One
key benefit is that during optimization the Gaussian Process learns a global
estimate of the true objective function, with predicted outcomes and a
statistical measure of confidence in areas that haven't been investigated yet.
However, standard Bayesian optimization does not perform consistently or
provide an accurate Gaussian Process surrogate function for highly volatile
objective functions. We treat these problems by introducing a novel sampling
technique called Hybrid Repeat/Multi-point Sampling. This technique gives the
AI ability to learn optimum behaviors in a highly uncertain environment. More
importantly, it not only improves the reliability of the optimization, but also
creates a better model of the entire objective surface. With this improved
model the agent is equipped to better adapt behaviors.","['Brett Israelsen', 'Nisar Ahmed']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.RO']",2016-12-13 00:21:45+00:00
http://arxiv.org/abs/1612.03964v1,Probabilistic Bisection Converges Almost as Quickly as Stochastic Approximation,"The probabilistic bisection algorithm (PBA) solves a class of stochastic
root-finding problems in one dimension by successively updating a prior belief
on the location of the root based on noisy responses to queries at chosen
points. The responses indicate the direction of the root from the queried
point, and are incorrect with a fixed probability. The fixed-probability
assumption is problematic in applications, and so we extend the PBA to apply
when this assumption is relaxed. The extension involves the use of a power-one
test at each queried point. We explore the convergence behavior of the extended
PBA, showing that it converges at a rate arbitrarily close to, but slower than,
the canonical ""square root"" rate of stochastic approximation.","['Peter I. Frazier', 'Shane G. Henderson', 'Rolf Waeber']","['math.PR', 'math.OC', 'stat.ML']",2016-12-12 22:57:05+00:00
http://arxiv.org/abs/1612.03957v3,Monte Carlo Structured SVI for Two-Level Non-Conjugate Models,"The stochastic variational inference (SVI) paradigm, which combines
variational inference, natural gradients, and stochastic updates, was recently
proposed for large-scale data analysis in conjugate Bayesian models and
demonstrated to be effective in several problems. This paper studies a family
of Bayesian latent variable models with two levels of hidden variables but
without any conjugacy requirements, making several contributions in this
context. The first is observing that SVI, with an improved structured
variational approximation, is applicable under more general conditions than
previously thought with the only requirement being that the approximating
variational distribution be in the same family as the prior. The resulting
approach, Monte Carlo Structured SVI (MC-SSVI), significantly extends the scope
of SVI, enabling large-scale learning in non-conjugate models. For models with
latent Gaussian variables we propose a hybrid algorithm, using both standard
and natural gradients, which is shown to improve stability and convergence.
Applications in mixed effects models, sparse Gaussian processes, probabilistic
matrix factorization and correlated topic models demonstrate the generality of
the approach and the advantages of the proposed algorithms.","['Rishit Sheth', 'Roni Khardon']",['stat.ML'],2016-12-12 22:36:04+00:00
http://arxiv.org/abs/1612.03950v2,Nonnegative Matrix Factorization for identification of unknown number of sources emitting delayed signals,"Factor analysis is broadly used as a powerful unsupervised machine learning
tool for reconstruction of hidden features in recorded mixtures of signals. In
the case of a linear approximation, the mixtures can be decomposed by a variety
of model-free Blind Source Separation (BSS) algorithms. Most of the available
BSS algorithms consider an instantaneous mixing of signals, while the case when
the mixtures are linear combinations of signals with delays is less explored.
Especially difficult is the case when the number of sources of the signals with
delays is unknown and has to be determined from the data as well. To address
this problem, in this paper, we present a new method based on Nonnegative
Matrix Factorization (NMF) that is capable of identifying: (a) the unknown
number of the sources, (b) the delays and speed of propagation of the signals,
and (c) the locations of the sources. Our method can be used to decompose
records of mixtures of signals with delays emitted by an unknown number of
sources in a nondispersive medium, based only on recorded data. This is the
case, for example, when electromagnetic signals from multiple antennas are
received asynchronously; or mixtures of acoustic or seismic signals recorded by
sensors located at different positions; or when a shift in frequency is induced
by the Doppler effect. By applying our method to synthetic datasets, we
demonstrate its ability to identify the unknown number of sources as well as
the waveforms, the delays, and the strengths of the signals. Using Bayesian
analysis, we also evaluate estimation uncertainties and identify the region of
likelihood where the positions of the sources can be found.","['Filip L. Iliev', 'Valentin G. Stanev', 'Velimir V. Vesselinov', 'Boian S. Alexandrov']","['cs.LG', 'stat.ML', ']68T10']",2016-12-12 22:21:41+00:00
http://arxiv.org/abs/1612.03948v3,Identification of release sources in advection-diffusion system by machine learning combined with Green function inverse method,"The identification of sources of advection-diffusion transport is based
usually on solving complex ill-posed inverse models against the available
state- variable data records. However, if there are several sources with
different locations and strengths, the data records represent mixtures rather
than the separate influences of the original sources. Importantly, the number
of these original release sources is typically unknown, which hinders
reliability of the classical inverse-model analyses. To address this challenge,
we present here a novel hybrid method for identification of the unknown number
of release sources. Our hybrid method, called HNMF, couples unsupervised
learning based on Nonnegative Matrix Factorization (NMF) and inverse-analysis
Green functions method. HNMF synergistically performs decomposition of the
recorded mixtures, finds the number of the unknown sources and uses the Green
function of advection-diffusion equation to identify their characteristics. In
the paper, we introduce the method and demonstrate that it is capable of
identifying the advection velocity and dispersivity of the medium as well as
the unknown number, locations, and properties of various sets of synthetic
release sources with different space and time dependencies, based only on the
recorded data. HNMF can be applied directly to any problem controlled by a
partial-differential parabolic equation where mixtures of an unknown number of
sources are measured at multiple locations.","['Valentin G. Stanev', 'Filip L. Iliev', 'Scott Hansen', 'Velimir V. Vesselinov', 'Boian S. Alexandrov']","['cs.LG', 'stat.ML', '68T10']",2016-12-12 22:05:10+00:00
http://arxiv.org/abs/1612.03871v3,Knowledge Completion for Generics using Guided Tensor Factorization,"Given a knowledge base or KB containing (noisy) facts about common nouns or
generics, such as ""all trees produce oxygen"" or ""some animals live in forests"",
we consider the problem of inferring additional such facts at a precision
similar to that of the starting KB. Such KBs capture general knowledge about
the world, and are crucial for various applications such as question answering.
Different from commonly studied named entity KBs such as Freebase, generics KBs
involve quantification, have more complex underlying regularities, tend to be
more incomplete, and violate the commonly used locally closed world assumption
(LCWA). We show that existing KB completion methods struggle with this new
task, and present the first approach that is successful. Our results
demonstrate that external information, such as relation schemas and entity
taxonomies, if used appropriately, can be a surprisingly powerful tool in this
setting. First, our simple yet effective knowledge guided tensor factorization
approach achieves state-of-the-art results on two generics KBs (80% precise)
for science, doubling their size at 74%-86% precision. Second, our novel
taxonomy guided, submodular, active learning method for collecting annotations
about rare entities (e.g., oriole, a bird) is 6x more effective at inferring
further new facts about them than multiple active learning baselines.","['Hanie Sedghi', 'Ashish Sabharwal']","['cs.AI', 'cs.LG', 'stat.ML']",2016-12-12 19:53:04+00:00
http://arxiv.org/abs/1612.03839v2,Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD),"Tensor decompositions have rich applications in statistics and machine
learning, and developing efficient, accurate algorithms for the problem has
received much attention recently. Here, we present a new method built on
Kruskal's uniqueness theorem to decompose symmetric, nearly orthogonally
decomposable tensors. Unlike the classical higher-order singular value
decomposition which unfolds a tensor along a single mode, we consider
unfoldings along two modes and use rank-1 constraints to characterize the
underlying components. This tensor decomposition method provably handles a
greater level of noise compared to previous methods and achieves a high
estimation accuracy. Numerical results demonstrate that our algorithm is robust
to various noise distributions and that it performs especially favorably as the
order increases.","['Miaoyan Wang', 'Yun S. Song']","['cs.LG', 'stat.ML']",2016-12-12 18:38:40+00:00
http://arxiv.org/abs/1612.03809v1,Generalizable Features From Unsupervised Learning,"Humans learn a predictive model of the world and use this model to reason
about future events and the consequences of actions. In contrast to most
machine predictors, we exhibit an impressive ability to generalize to unseen
scenarios and reason intelligently in these settings. One important aspect of
this ability is physical intuition(Lake et al., 2016). In this work, we explore
the potential of unsupervised learning to find features that promote better
generalization to settings outside the supervised training distribution. Our
task is predicting the stability of towers of square blocks. We demonstrate
that an unsupervised model, trained to predict future frames of a video
sequence of stable and unstable block configurations, can yield features that
support extrapolating stability prediction to blocks configurations outside the
training set distribution","['Mehdi Mirza', 'Aaron Courville', 'Yoshua Bengio']","['stat.ML', 'cs.CV', 'cs.LG']",2016-12-12 17:45:48+00:00
http://arxiv.org/abs/1612.03770v2,Neurogenesis Deep Learning,"Neural machine learning methods, such as deep neural networks (DNN), have
achieved remarkable success in a number of complex data processing tasks. These
methods have arguably had their strongest impact on tasks such as image and
audio processing - data processing domains in which humans have long held clear
advantages over conventional algorithms. In contrast to biological neural
systems, which are capable of learning continuously, deep artificial networks
have a limited ability for incorporating new information in an already trained
network. As a result, methods for continuous learning are potentially highly
impactful in enabling the application of deep networks to dynamic data sets.
Here, inspired by the process of adult neurogenesis in the hippocampus, we
explore the potential for adding new neurons to deep layers of artificial
neural networks in order to facilitate their acquisition of novel information
while preserving previously trained data representations. Our results on the
MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes
lower and upper case letters and digits, demonstrate that neurogenesis is well
suited for addressing the stability-plasticity dilemma that has long challenged
adaptive machine learning algorithms.","['Timothy J. Draelos', 'Nadine E. Miner', 'Christopher C. Lamb', 'Jonathan A. Cox', 'Craig M. Vineyard', 'Kristofor D. Carlson', 'William M. Severa', 'Conrad D. James', 'James B. Aimone']","['cs.NE', 'cs.LG', 'stat.ML']",2016-12-12 16:25:23+00:00
http://arxiv.org/abs/1612.03689v1,Poincaré inequalities on intervals -- application to sensitivity analysis,"The development of global sensitivity analysis of numerical model outputs has
recently raised new issues on 1-dimensional Poincar\'e inequalities. Typically
two kind of sensitivity indices are linked by a Poincar\'e type inequality,
which provide upper bounds of the most interpretable index by using the other
one, cheaper to compute. This allows performing a low-cost screening of
unessential variables. The efficiency of this screening then highly depends on
the accuracy of the upper bounds in Poincar\'e inequalities. The novelty in the
questions concern the wide range of probability distributions involved, which
are often truncated on intervals. After providing an overview of the existing
knowledge and techniques, we add some theory about Poincar\'e constants on
intervals, with improvements for symmetric intervals. Then we exploit the
spectral interpretation for computing exact value of Poincar\'e constants of
any admissible distribution on a given interval. We give semi-analytical
results for some frequent distributions (truncated exponential, triangular,
truncated normal), and present a numerical method in the general case. Finally,
an application is made to a hydrological problem, showing the benefits of the
new results in Poincar\'e inequalities to sensitivity analysis.","['Olivier Roustant', 'Franck Barthe', 'Bertrand Iooss']","['math.ST', 'math.PR', 'stat.ML', 'stat.TH']",2016-12-12 14:08:54+00:00
http://arxiv.org/abs/1612.03663v1,"Analysis and Optimization of Loss Functions for Multiclass, Top-k, and Multilabel Classification","Top-k error is currently a popular performance measure on large scale image
classification benchmarks such as ImageNet and Places. Despite its wide
acceptance, our understanding of this metric is limited as most of the previous
research is focused on its special case, the top-1 error. In this work, we
explore two directions that shed more light on the top-k error. First, we
provide an in-depth analysis of established and recently proposed single-label
multiclass methods along with a detailed account of efficient optimization
algorithms for them. Our results indicate that the softmax loss and the smooth
multiclass SVM are surprisingly competitive in top-k error uniformly across all
k, which can be explained by our analysis of multiclass top-k calibration.
Further improvements for a specific k are possible with a number of proposed
top-k loss functions. Second, we use the top-k methods to explore the
transition from multiclass to multilabel learning. In particular, we find that
it is possible to obtain effective multilabel classifiers on Pascal VOC using a
single label per image for training, while the gap between multiclass and
multilabel methods on MS COCO is more significant. Finally, our contribution of
efficient algorithms for training with the considered top-k and multilabel loss
functions is of independent interest.","['Maksim Lapin', 'Matthias Hein', 'Bernt Schiele']","['cs.CV', 'cs.LG', 'stat.ML']",2016-12-12 13:20:09+00:00
http://arxiv.org/abs/1612.03615v2,Kernel-based Reconstruction of Space-time Functions on Dynamic Graphs,"Graph-based methods pervade the inference toolkits of numerous disciplines
including sociology, biology, neuroscience, physics, chemistry, and
engineering. A challenging problem encountered in this context pertains to
determining the attributes of a set of vertices given those of another subset
at possibly different time instants. Leveraging spatiotemporal dynamics can
drastically reduce the number of observed vertices, and hence the cost of
sampling. Alleviating the limited flexibility of existing approaches, the
present paper broadens the existing kernel-based graph function reconstruction
framework to accommodate time-evolving functions over possibly time-evolving
topologies. This approach inherits the versatility and generality of
kernel-based methods, for which no knowledge on distributions or second-order
statistics is required. Systematic guidelines are provided to construct two
families of space-time kernels with complementary strengths. The first
facilitates judicious control of regularization on a space-time frequency
plane, whereas the second can afford time-varying topologies. Batch and online
estimators are also put forth, and a novel kernel Kalman filter is developed to
obtain these estimates at affordable computational cost. Numerical tests with
real data sets corroborate the merits of the proposed methods relative to
competing alternatives.","['Daniel Romero', 'Vassilis N. Ioannidis', 'Georgios B. Giannakis']","['cs.LG', 'eess.SP', 'stat.ML']",2016-12-12 11:12:00+00:00
http://arxiv.org/abs/1612.03480v1,Self-calibrating Neural Networks for Dimensionality Reduction,"Recently, a novel family of biologically plausible online algorithms for
reducing the dimensionality of streaming data has been derived from the
similarity matching principle. In these algorithms, the number of output
dimensions can be determined adaptively by thresholding the singular values of
the input data matrix. However, setting such threshold requires knowing the
magnitude of the desired singular values in advance. Here we propose online
algorithms where the threshold is self-calibrating based on the singular values
computed from the existing observations. To derive these algorithms from the
similarity matching cost function we propose novel regularizers. As before,
these online algorithms can be implemented by Hebbian/anti-Hebbian neural
networks in which the learning rule depends on the chosen regularizer. We
demonstrate both mathematically and via simulation the effectiveness of these
online algorithms in various settings.","['Yuansi Chen', 'Cengiz Pehlevan', 'Dmitri B. Chklovskii']","['cs.LG', 'cs.NE', 'q-bio.NC', 'stat.ML']",2016-12-11 21:15:05+00:00
http://arxiv.org/abs/1612.03450v2,Noisy subspace clustering via matching pursuits,"Sparsity-based subspace clustering algorithms have attracted significant
attention thanks to their excellent performance in practical applications. A
prominent example is the sparse subspace clustering (SSC) algorithm by
Elhamifar and Vidal, which performs spectral clustering based on an adjacency
matrix obtained by sparsely representing each data point in terms of all the
other data points via the Lasso. When the number of data points is large or the
dimension of the ambient space is high, the computational complexity of SSC
quickly becomes prohibitive. Dyer et al. observed that SSC-OMP obtained by
replacing the Lasso by the greedy orthogonal matching pursuit (OMP) algorithm
results in significantly lower computational complexity, while often yielding
comparable performance. The central goal of this paper is an analytical
performance characterization of SSC-OMP for noisy data. Moreover, we introduce
and analyze the SSC-MP algorithm, which employs matching pursuit (MP) in lieu
of OMP. Both SSC-OMP and SSC-MP are proven to succeed even when the subspaces
intersect and when the data points are contaminated by severe noise. The
clustering conditions we obtain for SSC-OMP and SSC-MP are similar to those for
SSC and for the thresholding-based subspace clustering (TSC) algorithm due to
Heckel and B\""olcskei. Analytical results in combination with numerical results
indicate that both SSC-OMP and SSC-MP with a data-dependent stopping criterion
automatically detect the dimensions of the subspaces underlying the data.
Moreover, experiments on synthetic and on real data show that SSC-MP compares
very favorably to SSC, SSC-OMP, TSC, and the nearest subspace neighbor
algorithm, both in terms of clustering performance and running time. In
addition, we find that, in contrast to SSC-OMP, the performance of SSC-MP is
very robust with respect to the choice of parameters in the stopping criteria.","['Michael Tschannen', 'Helmut Bölcskei']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2016-12-11 18:33:31+00:00
http://arxiv.org/abs/1612.03441v1,Lock-Free Optimization for Non-Convex Problems,"Stochastic gradient descent~(SGD) and its variants have attracted much
attention in machine learning due to their efficiency and effectiveness for
optimization. To handle large-scale problems, researchers have recently
proposed several lock-free strategy based parallel SGD~(LF-PSGD) methods for
multi-core systems. However, existing works have only proved the convergence of
these LF-PSGD methods for convex problems. To the best of our knowledge, no
work has proved the convergence of the LF-PSGD methods for non-convex problems.
In this paper, we provide the theoretical proof about the convergence of two
representative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems.
Empirical results also show that both Hogwild! and AsySVRG are convergent on
non-convex problems, which successfully verifies our theoretical results.","['Shen-Yi Zhao', 'Gong-Duo Zhang', 'Wu-Jun Li']","['stat.ML', 'cs.LG']",2016-12-11 17:26:43+00:00
http://arxiv.org/abs/1612.03412v2,Non-Redundant Spectral Dimensionality Reduction,"Spectral dimensionality reduction algorithms are widely used in numerous
domains, including for recognition, segmentation, tracking and visualization.
However, despite their popularity, these algorithms suffer from a major
limitation known as the ""repeated Eigen-directions"" phenomenon. That is, many
of the embedding coordinates they produce typically capture the same direction
along the data manifold. This leads to redundant and inefficient
representations that do not reveal the true intrinsic dimensionality of the
data. In this paper, we propose a general method for avoiding redundancy in
spectral algorithms. Our approach relies on replacing the orthogonality
constraints underlying those methods by unpredictability constraints.
Specifically, we require that each embedding coordinate be unpredictable (in
the statistical sense) from all previous ones. We prove that these constraints
necessarily prevent redundancy, and provide a simple technique to incorporate
them into existing methods. As we illustrate on challenging high-dimensional
scenarios, our approach produces significantly more informative and compact
representations, which improve visualization and classification tasks.","['Yochai Blau', 'Tomer Michaeli']","['cs.LG', 'stat.ML']",2016-12-11 14:04:33+00:00
http://arxiv.org/abs/1612.03409v2,A New Spectral Method for Latent Variable Models,"This paper presents an algorithm for the unsupervised learning of latent
variable models from unlabeled sets of data. We base our technique on spectral
decomposition, providing a technique that proves to be robust both in theory
and in practice. We also describe how to use this algorithm to learn the
parameters of two well known text mining models: single topic model and Latent
Dirichlet Allocation, providing in both cases an efficient technique to
retrieve the parameters to feed the algorithm. We compare the results of our
algorithm with those of existing algorithms on synthetic data, and we provide
examples of applications to real world text corpora for both single topic model
and LDA, obtaining meaningful results.","['Matteo Ruffini', 'Marta Casanellas', 'Ricard Gavaldà']",['stat.ML'],2016-12-11 13:31:58+00:00
http://arxiv.org/abs/1612.03364v1,Technical Report: A Generalized Matching Pursuit Approach for Graph-Structured Sparsity,"Sparsity-constrained optimization is an important and challenging problem
that has wide applicability in data mining, machine learning, and statistics.
In this paper, we focus on sparsity-constrained optimization in cases where the
cost function is a general nonlinear function and, in particular, the sparsity
constraint is defined by a graph-structured sparsity model. Existing methods
explore this problem in the context of sparse estimation in linear models. To
the best of our knowledge, this is the first work to present an efficient
approximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp),
to optimize a general nonlinear function subject to graph-structured
constraints. We prove that our algorithm enjoys the strong guarantees analogous
to those designed for linear models in terms of convergence rate and
approximation accuracy. As a case study, we specialize Graph-Mp to optimize a
number of well-known graph scan statistic models for the connected subgraph
detection task, and empirical evidence demonstrates that our general algorithm
performs superior over state-of-the-art methods that are designed specifically
for the task of connected subgraph detection.","['Feng Chen', 'Baojian Zhou']","['cs.LG', 'cs.AI', 'stat.ML']",2016-12-11 01:57:50+00:00
http://arxiv.org/abs/1612.03350v1,Non-negative Factorization of the Occurrence Tensor from Financial Contracts,"We propose an algorithm for the non-negative factorization of an occurrence
tensor built from heterogeneous networks. We use l0 norm to model sparse errors
over discrete values (occurrences), and use decomposed factors to model the
embedded groups of nodes. An efficient splitting method is developed to
optimize the nonconvex and nonsmooth objective. We study both synthetic
problems and a new dataset built from financial documents, resMBS.","['Zheng Xu', 'Furong Huang', 'Louiqa Raschid', 'Tom Goldstein']","['cs.CE', 'cs.LG', 'stat.ML']",2016-12-10 22:26:30+00:00
