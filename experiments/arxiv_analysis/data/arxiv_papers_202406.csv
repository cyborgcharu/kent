id,title,abstract,authors,categories,date
http://arxiv.org/abs/2407.09632v2,Granger Causality in Extremes,"We introduce a rigorous mathematical framework for Granger causality in
extremes, designed to identify causal links from extreme events in time series.
Granger causality plays a pivotal role in uncovering directional relationships
among time-varying variables. While this notion gains heightened importance
during extreme and highly volatile periods, state-of-the-art methods primarily
focus on causality within the body of the distribution, often overlooking
causal mechanisms that manifest only during extreme events. Our framework is
designed to infer causality mainly from extreme events by leveraging the causal
tail coefficient. We establish equivalences between causality in extremes and
other causal concepts, including (classical) Granger causality, Sims causality,
and structural causality. We prove other key properties of Granger causality in
extremes and show that the framework is especially helpful under the presence
of hidden confounders. We also propose a novel inference method for detecting
the presence of Granger causality in extremes from data. Our method is
model-free, can handle non-linear and high-dimensional time series, outperforms
current state-of-the-art methods in all considered setups, both in performance
and speed, and was found to uncover coherent effects when applied to financial
and extreme weather observations.","['Juraj Bodik', 'Olivier C. Pasche']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH', '62M10', 'G.3']",2024-07-12 18:41:07+00:00
http://arxiv.org/abs/2407.09387v1,Meta-Analysis with Untrusted Data,"[See paper for full abstract] Meta-analysis is a crucial tool for answering
scientific questions. It is usually conducted on a relatively small amount of
``trusted'' data -- ideally from randomized, controlled trials -- which allow
causal effects to be reliably estimated with minimal assumptions. We show how
to answer causal questions much more precisely by making two changes. First, we
incorporate untrusted data drawn from large observational databases, related
scientific literature and practical experience -- without sacrificing rigor or
introducing strong assumptions. Second, we train richer models capable of
handling heterogeneous trials, addressing a long-standing challenge in
meta-analysis. Our approach is based on conformal prediction, which
fundamentally produces rigorous prediction intervals, but doesn't handle
indirect observations: in meta-analysis, we observe only noisy effects due to
the limited number of participants in each trial. To handle noise, we develop a
simple, efficient version of fully-conformal kernel ridge regression, based on
a novel condition called idiocentricity. We introduce noise-correcting terms in
the residuals and analyze their interaction with a ``variance shaving''
technique. In multiple experiments on healthcare datasets, our algorithms
deliver tighter, sounder intervals than traditional ones. This paper charts a
new course for meta-analysis and evidence-based medicine, where heterogeneity
and untrusted data are embraced for more nuanced and precise predictions.","['Shiva Kaul', 'Geoffrey J. Gordon']","['stat.ML', 'cs.LG', 'stat.ME']",2024-07-12 16:07:53+00:00
http://arxiv.org/abs/2407.09378v1,Graph Neural Network Causal Explanation via Neural Causal Models,"Graph neural network (GNN) explainers identify the important subgraph that
ensures the prediction for a given graph. Until now, almost all GNN explainers
are based on association, which is prone to spurious correlations. We propose
{\name}, a GNN causal explainer via causal inference. Our explainer is based on
the observation that a graph often consists of a causal underlying subgraph.
{\name} includes three main steps: 1) It builds causal structure and the
corresponding structural causal model (SCM) for a graph, which enables the
cause-effect calculation among nodes. 2) Directly calculating the cause-effect
in real-world graphs is computationally challenging. It is then enlightened by
the recent neural causal model (NCM), a special type of SCM that is trainable,
and design customized NCMs for GNNs. By training these GNN NCMs, the
cause-effect can be easily calculated. 3) It uncovers the subgraph that
causally explains the GNN predictions via the optimized GNN-NCMs. Evaluation
results on multiple synthetic and real-world graphs validate that {\name}
significantly outperforms existing GNN explainers in exact groundtruth
explanation identification","['Arman Behnam', 'Binghui Wang']","['cs.LG', 'cs.AI', 'stat.ML']",2024-07-12 15:56:33+00:00
http://arxiv.org/abs/2407.09375v2,HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context,"This work explores the in-context learning capabilities of State Space Models
(SSMs) and presents, to the best of our knowledge, the first theoretical
explanation of a possible underlying mechanism. We introduce a novel weight
construction for SSMs, enabling them to predict the next state of any dynamical
system after observing previous states without parameter fine-tuning. This is
accomplished by extending the HiPPO framework to demonstrate that continuous
SSMs can approximate the derivative of any input signal. Specifically, we find
an explicit weight construction for continuous SSMs and provide an asymptotic
error bound on the derivative approximation. The discretization of this
continuous SSM subsequently yields a discrete SSM that predicts the next state.
Finally, we demonstrate the effectiveness of our parameterization empirically.
This work should be an initial step toward understanding how sequence models
based on SSMs learn in context.","['Federico Arangath Joseph', 'Kilian Konstantin Haefeli', 'Noah Liniger', 'Caglar Gulcehre']","['cs.LG', 'stat.ML']",2024-07-12 15:56:11+00:00
http://arxiv.org/abs/2407.12864v2,Clustering Time-Evolving Networks Using the Spatio-Temporal Graph Laplacian,"Time-evolving graphs arise frequently when modeling complex dynamical systems
such as social networks, traffic flow, and biological processes. Developing
techniques to identify and analyze communities in these time-varying graph
structures is an important challenge. In this work, we generalize existing
spectral clustering algorithms from static to dynamic graphs using canonical
correlation analysis (CCA) to capture the temporal evolution of clusters. Based
on this extended canonical correlation framework, we define the spatio-temporal
graph Laplacian and investigate its spectral properties. We connect these
concepts to dynamical systems theory via transfer operators, and illustrate the
advantages of our method on benchmark graphs by comparison with existing
methods. We show that the spatio-temporal graph Laplacian allows for a clear
interpretation of cluster structure evolution over time for directed and
undirected graphs.","['Maia Trower', 'Nataša Djurdjevac Conrad', 'Stefan Klus']","['cs.SI', 'cs.LG', 'math.DS', 'stat.ML']",2024-07-12 14:31:54+00:00
http://arxiv.org/abs/2407.09297v1,Learning Distances from Data with Normalizing Flows and Score Matching,"Density-based distances (DBDs) offer an elegant solution to the problem of
metric learning. By defining a Riemannian metric which increases with
decreasing probability density, shortest paths naturally follow the data
manifold and points are clustered according to the modes of the data. We show
that existing methods to estimate Fermat distances, a particular choice of DBD,
suffer from poor convergence in both low and high dimensions due to i)
inaccurate density estimates and ii) reliance on graph-based paths which are
increasingly rough in high dimensions. To address these issues, we propose
learning the densities using a normalizing flow, a generative model with
tractable density estimation, and employing a smooth relaxation method using a
score model initialized from a graph-based proposal. Additionally, we introduce
a dimension-adapted Fermat distance that exhibits more intuitive behavior when
scaled to high dimensions and offers better numerical properties. Our work
paves the way for practical use of density-based distances, especially in
high-dimensional spaces.","['Peter Sorrenson', 'Daniel Behrend-Uriarte', 'Christoph Schnörr', 'Ullrich Köthe']","['cs.LG', 'stat.ML']",2024-07-12 14:30:41+00:00
http://arxiv.org/abs/2407.11069v2,Combining Federated Learning and Control: A Survey,"This survey provides an overview of combining Federated Learning (FL) and
control to enhance adaptability, scalability, generalization, and privacy in
(nonlinear) control applications. Traditional control methods rely on
controller design models, but real-world scenarios often require online model
retuning or learning. FL offers a distributed approach to model training,
enabling collaborative learning across distributed devices while preserving
data privacy. By keeping data localized, FL mitigates concerns regarding
privacy and security while reducing network bandwidth requirements for
communication. This survey summarizes the state-of-the-art concepts and ideas
of combining FL and control. The methodical benefits are further discussed,
culminating in a detailed overview of expected applications, from dynamical
system modeling over controller design, focusing on adaptive control, to
knowledge transfer in multi-agent decision-making systems.","['Jakob Weber', 'Markus Gurtner', 'Amadeus Lobe', 'Adrian Trachte', 'Andreas Kugi']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2024-07-12 14:29:17+00:00
http://arxiv.org/abs/2407.09186v2,Variational Inference via Smoothed Particle Hydrodynamics,"A new variational inference method, SPH-ParVI, based on smoothed particle
hydrodynamics (SPH), is proposed for sampling partially known densities (e.g.
up to a constant) or sampling using gradients. SPH-ParVI simulates the flow of
a fluid under external effects driven by the target density; transient or
steady state of the fluid approximates the target density. The continuum fluid
is modelled as an interacting particle system (IPS) via SPH, where each
particle carries smoothed properties, interacts and evolves as per the
Navier-Stokes equations. This mesh-free, Lagrangian simulation method offers
fast, flexible, scalable and deterministic sampling and inference for a class
of probabilistic models such as those encountered in Bayesian inference and
generative modelling.",['Yongchao Huang'],"['cs.AI', 'cs.LG', 'stat.ML']",2024-07-12 11:38:41+00:00
http://arxiv.org/abs/2407.09055v1,Advanced Graph Clustering Methods: A Comprehensive and In-Depth Analysis,"Graph clustering, which aims to divide a graph into several homogeneous
groups, is a critical area of study with applications that span various fields
such as social network analysis, bioinformatics, and image segmentation. This
paper explores both traditional and more recent approaches to graph clustering.
Firstly, key concepts and definitions in graph theory are introduced. The
background section covers essential topics, including graph Laplacians and the
integration of Deep Learning in graph analysis. The paper then delves into
traditional clustering methods, including Spectral Clustering and the Leiden
algorithm. Following this, state-of-the-art clustering techniques that leverage
deep learning are examined. A comprehensive comparison of these methods is made
through experiments. The paper concludes with a discussion of the practical
applications of graph clustering and potential future research directions.","['Timothé Watteau', 'Aubin Bonnefoy', 'Simon Illouz-Laurent', 'Joaquim Jusseau', 'Serge Iovleff']","['stat.ML', 'cs.LG']",2024-07-12 07:22:45+00:00
http://arxiv.org/abs/2407.08987v1,Parameter inference from a non-stationary unknown process,"Non-stationary systems are found throughout the world, from climate patterns
under the influence of variation in carbon dioxide concentration, to brain
dynamics driven by ascending neuromodulation. Accordingly, there is a need for
methods to analyze non-stationary processes, and yet most time-series analysis
methods that are used in practice, on important problems across science and
industry, make the simplifying assumption of stationarity. One important
problem in the analysis of non-stationary systems is the problem class that we
refer to as Parameter Inference from a Non-stationary Unknown Process (PINUP).
Given an observed time series, this involves inferring the parameters that
drive non-stationarity of the time series, without requiring knowledge or
inference of a mathematical model of the underlying system. Here we review and
unify a diverse literature of algorithms for PINUP. We formulate the problem,
and categorize the various algorithmic contributions. This synthesis will allow
researchers to identify gaps in the literature and will enable systematic
comparisons of different methods. We also demonstrate that the most common
systems that existing methods are tested on - notably the non-stationary Lorenz
process and logistic map - are surprisingly easy to perform well on using
simple statistical features like windowed mean and variance, undermining the
practice of using good performance on these systems as evidence of algorithmic
performance. We then identify more challenging problems that many existing
methods perform poorly on and which can be used to drive methodological
advances in the field. Our results unify disjoint scientific contributions to
analyzing non-stationary systems and suggest new directions for progress on the
PINUP problem and the broader study of non-stationary phenomena.","['Kieran S. Owens', 'Ben D. Fulcher']","['physics.data-an', 'cs.LG', 'nlin.CD', 'stat.ML']",2024-07-12 04:44:29+00:00
http://arxiv.org/abs/2407.08976v1,Computational-Statistical Trade-off in Kernel Two-Sample Testing with Random Fourier Features,"Recent years have seen a surge in methods for two-sample testing, among which
the Maximum Mean Discrepancy (MMD) test has emerged as an effective tool for
handling complex and high-dimensional data. Despite its success and widespread
adoption, the primary limitation of the MMD test has been its quadratic-time
complexity, which poses challenges for large-scale analysis. While various
approaches have been proposed to expedite the procedure, it has been unclear
whether it is possible to attain the same power guarantee as the MMD test at
sub-quadratic time cost. To fill this gap, we revisit the approximated MMD test
using random Fourier features, and investigate its computational-statistical
trade-off. We start by revealing that the approximated MMD test is pointwise
consistent in power only when the number of random features approaches
infinity. We then consider the uniform power of the test and study the
time-power trade-off under the minimax testing framework. Our result shows
that, by carefully choosing the number of random features, it is possible to
attain the same minimax separation rates as the MMD test within sub-quadratic
time. We demonstrate this point under different distributional assumptions such
as densities in a Sobolev ball. Our theoretical findings are corroborated by
simulation studies.","['Ikjun Choi', 'Ilmun Kim']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2024-07-12 04:08:01+00:00
http://arxiv.org/abs/2407.08843v2,Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based Models,"Beyond estimating parameters of interest from data, one of the key goals of
statistical inference is to properly quantify uncertainty in these estimates.
In Bayesian inference, this uncertainty is provided by the posterior
distribution, the computation of which typically involves an intractable
high-dimensional integral. Among available approximation methods,
sampling-based approaches come with strong theoretical guarantees but scale
poorly to large problems, while variational approaches scale well but offer few
theoretical guarantees. In particular, variational methods are known to produce
overconfident estimates of posterior uncertainty and are typically
non-identifiable, with many latent variable configurations generating
equivalent predictions. Here, we address these challenges by showing how
diffusion-based models (DBMs), which have recently produced state-of-the-art
performance in generative modeling tasks, can be repurposed for performing
calibrated, identifiable Bayesian inference. By exploiting a previously
established connection between the stochastic and probability flow ordinary
differential equations (pfODEs) underlying DBMs, we derive a class of models,
inflationary flows, that uniquely and deterministically map high-dimensional
data to a lower-dimensional Gaussian distribution via ODE integration. This map
is both invertible and neighborhood-preserving, with controllable numerical
error, with the result that uncertainties in the data are correctly propagated
to the latent space. We demonstrate how such maps can be learned via standard
DBM training using a novel noise schedule and are effective at both preserving
and reducing intrinsic data dimensionality. The result is a class of highly
expressive generative models, uniquely defined on a low-dimensional latent
space, that afford principled Bayesian inference.","['Daniela de Albuquerque', 'John Pearson']","['cs.LG', 'stat.ML', '68T99 (Primary) 62M45 (Secondary)', 'G.3; I.6.5; I.2']",2024-07-11 19:58:19+00:00
http://arxiv.org/abs/2407.08803v2,PID Accelerated Temporal Difference Algorithms,"Long-horizon tasks, which have a large discount factor, pose a challenge for
most conventional reinforcement learning (RL) algorithms. Algorithms such as
Value Iteration and Temporal Difference (TD) learning have a slow convergence
rate and become inefficient in these tasks. When the transition distributions
are given, PID VI was recently introduced to accelerate the convergence of
Value Iteration using ideas from control theory. Inspired by this, we introduce
PID TD Learning and PID Q-Learning algorithms for the RL setting, in which only
samples from the environment are available. We give a theoretical analysis of
the convergence of PID TD Learning and its acceleration compared to the
conventional TD Learning. We also introduce a method for adapting PID gains in
the presence of noise and empirically verify its effectiveness.","['Mark Bedaywi', 'Amin Rakhsha', 'Amir-massoud Farahmand']","['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'math.OC', 'stat.ML']",2024-07-11 18:23:46+00:00
http://arxiv.org/abs/2407.08678v1,How to beat a Bayesian adversary,"Deep neural networks and other modern machine learning models are often
susceptible to adversarial attacks. Indeed, an adversary may often be able to
change a model's prediction through a small, directed perturbation of the
model's input - an issue in safety-critical applications. Adversarially robust
machine learning is usually based on a minmax optimisation problem that
minimises the machine learning loss under maximisation-based adversarial
attacks.
  In this work, we study adversaries that determine their attack using a
Bayesian statistical approach rather than maximisation. The resulting Bayesian
adversarial robustness problem is a relaxation of the usual minmax problem. To
solve this problem, we propose Abram - a continuous-time particle system that
shall approximate the gradient flow corresponding to the underlying learning
problem. We show that Abram approximates a McKean-Vlasov process and justify
the use of Abram by giving assumptions under which the McKean-Vlasov process
finds the minimiser of the Bayesian adversarial robustness problem. We discuss
two ways to discretise Abram and show its suitability in benchmark adversarial
deep learning experiments.","['Zihan Ding', 'Kexin Jin', 'Jonas Latz', 'Chenguang Liu']","['cs.LG', 'math.OC', 'stat.CO', 'stat.ML', '90C15, 65C35, 68T07']",2024-07-11 17:12:42+00:00
http://arxiv.org/abs/2407.08668v1,Estimation of spatio-temporal extremes via generative neural networks,"Recent methods in modeling spatial extreme events have focused on utilizing
parametric max-stable processes and their underlying dependence structure. In
this work, we provide a unified approach for analyzing spatial extremes with
little available data by estimating the distribution of model parameters or the
spatial dependence directly. By employing recent developments in generative
neural networks we predict a full sample-based distribution, allowing for
direct assessment of uncertainty regarding model parameters or other parameter
dependent functionals. We validate our method by fitting several simulated
max-stable processes, showing a high accuracy of the approach, regarding
parameter estimation, as well as uncertainty quantification. Additional
robustness checks highlight the generalization and extrapolation capabilities
of the model, while an application to precipitation extremes across Western
Germany demonstrates the usability of our approach in real-world scenarios.","['Christopher Bülte', 'Lisa Leimenstoll', 'Melanie Schienle']","['stat.ML', 'cs.LG']",2024-07-11 16:57:17+00:00
http://arxiv.org/abs/2407.08654v1,Adaptive Smooth Non-Stationary Bandits,"We study a $K$-armed non-stationary bandit model where rewards change
smoothly, as captured by H\""{o}lder class assumptions on rewards as functions
of time. Such smooth changes are parametrized by a H\""{o}lder exponent $\beta$
and coefficient $\lambda$. While various sub-cases of this general model have
been studied in isolation, we first establish the minimax dynamic regret rate
generally for all $K,\beta,\lambda$. Next, we show this optimal dynamic regret
can be attained adaptively, without knowledge of $\beta,\lambda$. To contrast,
even with parameter knowledge, upper bounds were only previously known for
limited regimes $\beta\leq 1$ and $\beta=2$ (Slivkins, 2014; Krishnamurthy and
Gopalan, 2021; Manegueu et al., 2021; Jia et al.,2023). Thus, our work resolves
open questions raised by these disparate threads of the literature.
  We also study the problem of attaining faster gap-dependent regret rates in
non-stationary bandits. While such rates are long known to be impossible in
general (Garivier and Moulines, 2011), we show that environments admitting a
safe arm (Suk and Kpotufe, 2022) allow for much faster rates than the
worst-case scaling with $\sqrt{T}$. While previous works in this direction
focused on attaining the usual logarithmic regret bounds, as summed over
stationary periods, our new gap-dependent rates reveal new optimistic regimes
of non-stationarity where even the logarithmic bounds are pessimistic. We show
our new gap-dependent rate is tight and that its achievability (i.e., as made
possible by a safe arm) has a surprisingly simple and clean characterization
within the smooth H\""{o}lder class model.",['Joe Suk'],"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2024-07-11 16:37:15+00:00
http://arxiv.org/abs/2407.08571v2,Multi-Group Proportional Representation in Retrieval,"Image search and retrieval tasks can perpetuate harmful stereotypes, erase
cultural identities, and amplify social disparities. Current approaches to
mitigate these representational harms balance the number of retrieved items
across population groups defined by a small number of (often binary)
attributes. However, most existing methods overlook intersectional groups
determined by combinations of group attributes, such as gender, race, and
ethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel
metric that measures representation across intersectional groups. We develop
practical methods for estimating MPR, provide theoretical guarantees, and
propose optimization algorithms to ensure MPR in retrieval. We demonstrate that
existing methods optimizing for equal and proportional representation metrics
may fail to promote MPR. Crucially, our work shows that optimizing MPR yields
more proportional representation across multiple intersectional groups
specified by a rich function class, often with minimal compromise in retrieval
accuracy.","['Alex Oesterling', 'Claudio Mayrink Verdun', 'Carol Xuan Long', 'Alexander Glynn', 'Lucas Monteiro Paes', 'Sajani Vithana', 'Martina Cardone', 'Flavio P. Calmon']","['cs.AI', 'cs.IR', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2024-07-11 14:59:17+00:00
http://arxiv.org/abs/2407.08560v1,Causal inference through multi-stage learning and doubly robust deep neural networks,"Deep neural networks (DNNs) have demonstrated remarkable empirical
performance in large-scale supervised learning problems, particularly in
scenarios where both the sample size $n$ and the dimension of covariates $p$
are large. This study delves into the application of DNNs across a wide
spectrum of intricate causal inference tasks, where direct estimation falls
short and necessitates multi-stage learning. Examples include estimating the
conditional average treatment effect and dynamic treatment effect. In this
framework, DNNs are constructed sequentially, with subsequent stages building
upon preceding ones. To mitigate the impact of estimation errors from early
stages on subsequent ones, we integrate DNNs in a doubly robust manner. In
contrast to previous research, our study offers theoretical assurances
regarding the effectiveness of DNNs in settings where the dimensionality $p$
expands with the sample size. These findings are significant independently and
extend to degenerate single-stage learning problems.","['Yuqian Zhang', 'Jelena Bradic']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2024-07-11 14:47:44+00:00
http://arxiv.org/abs/2407.08494v1,Multivariate root-n-consistent smoothing parameter free matching estimators and estimators of inverse density weighted expectations,"Expected values weighted by the inverse of a multivariate density or,
equivalently, Lebesgue integrals of regression functions with multivariate
regressors occur in various areas of applications, including estimating average
treatment effects, nonparametric estimators in random coefficient regression
models or deconvolution estimators in Berkson errors-in-variables models. The
frequently used nearest-neighbor and matching estimators suffer from bias
problems in multiple dimensions. By using polynomial least squares fits on each
cell of the $K^{\text{th}}$-order Voronoi tessellation for sufficiently large
$K$, we develop novel modifications of nearest-neighbor and matching estimators
which again converge at the parametric $\sqrt n $-rate under mild smoothness
assumptions on the unknown regression function and without any smoothness
conditions on the unknown density of the covariates. We stress that in contrast
to competing methods for correcting for the bias of matching estimators, our
estimators do not involve nonparametric function estimators and in particular
do not rely on sample-size dependent smoothing parameters. We complement the
upper bounds with appropriate lower bounds derived from information-theoretic
arguments, which show that some smoothness of the regression function is indeed
required to achieve the parametric rate. Simulations illustrate the practical
feasibility of the proposed methods.","['Hajo Holzmann', 'Alexander Meister']","['math.ST', 'stat.ML', 'stat.TH', '62H12, 62G05']",2024-07-11 13:28:34+00:00
http://arxiv.org/abs/2407.18257v1,Estimation of Distribution Algorithms with Matrix Transpose in Bayesian Learning,"Estimation of distribution algorithms (EDAs) constitute a new branch of
evolutionary optimization algorithms, providing effective and efficient
optimization performance in a variety of research areas. Recent studies have
proposed new EDAs that employ mutation operators in standard EDAs to increase
the population diversity. We present a new mutation operator, a matrix
transpose, specifically designed for Bayesian structure learning, and we
evaluate its performance in Bayesian structure learning. The results indicate
that EDAs with transpose mutation give markedly better performance than
conventional EDAs.","['Dae-Won Kim', 'Song Ko', 'Bo-Yeong Kang']","['cs.NE', 'stat.ML']",2024-07-11 12:57:35+00:00
http://arxiv.org/abs/2407.08415v1,Parallelizing Autoregressive Generation with Variational State Space Models,"Attention-based models such as Transformers and recurrent models like state
space models (SSMs) have emerged as successful methods for autoregressive
sequence modeling. Although both enable parallel training, none enable parallel
generation due to their autoregressiveness. We propose the variational SSM
(VSSM), a variational autoencoder (VAE) where both the encoder and decoder are
SSMs. Since sampling the latent variables and decoding them with the SSM can be
parallelized, both training and generation can be conducted in parallel.
Moreover, the decoder recurrence allows generation to be resumed without
reprocessing the whole sequence. Finally, we propose the autoregressive VSSM
that can be conditioned on a partial realization of the sequence, as is common
in language generation tasks. Interestingly, the autoregressive VSSM still
enables parallel generation. We highlight on toy problems (MNIST, CIFAR) the
empirical gains in speed-up and show that it competes with traditional models
in terms of generation quality (Transformer, Mamba SSM).","['Gaspard Lambrechts', 'Yann Claes', 'Pierre Geurts', 'Damien Ernst']","['cs.LG', 'stat.ML']",2024-07-11 11:41:29+00:00
http://arxiv.org/abs/2407.08337v2,FedLog: Personalized Federated Classification with Less Communication and More Flexibility,"Federated representation learning (FRL) aims to learn personalized federated
models with effective feature extraction from local data. FRL algorithms that
share the majority of the model parameters face significant challenges with
huge communication overhead. This overhead stems from the millions of neural
network parameters and slow aggregation progress of the averaging heuristic. To
reduce the overhead, we propose to share sufficient data summaries instead of
raw model parameters. The data summaries encode minimal sufficient statistics
of an exponential family, and Bayesian inference is utilized for global
aggregation. It helps to reduce message sizes and communication frequency. To
further ensure formal privacy guarantee, we extend it with differential privacy
framework. Empirical results demonstrate high learning accuracy with low
communication overhead of our method.","['Haolin Yu', 'Guojun Zhang', 'Pascal Poupart']","['cs.LG', 'cs.DC', 'stat.ML']",2024-07-11 09:40:29+00:00
http://arxiv.org/abs/2407.08271v1,Gaussian process interpolation with conformal prediction: methods and comparative analysis,"This article advocates the use of conformal prediction (CP) methods for
Gaussian process (GP) interpolation to enhance the calibration of prediction
intervals. We begin by illustrating that using a GP model with parameters
selected by maximum likelihood often results in predictions that are not
optimally calibrated. CP methods can adjust the prediction intervals, leading
to better uncertainty quantification while maintaining the accuracy of the
underlying GP model. We compare different CP variants and introduce a novel
variant based on an asymmetric score. Our numerical experiments demonstrate the
effectiveness of CP methods in improving calibration without compromising
accuracy. This work aims to facilitate the adoption of CP methods in the GP
community.","['Aurélien Pion', 'Emmanuel Vazquez']","['cs.LG', 'stat.CO', 'stat.ME', 'stat.ML']",2024-07-11 08:15:57+00:00
http://arxiv.org/abs/2407.08094v2,Density Estimation via Binless Multidimensional Integration,"We introduce the Binless Multidimensional Thermodynamic Integration (BMTI)
method for nonparametric, robust, and data-efficient density estimation. BMTI
estimates the logarithm of the density by initially computing log-density
differences between neighbouring data points. Subsequently, such differences
are integrated, weighted by their associated uncertainties, using a
maximum-likelihood formulation. This procedure can be seen as an extension to a
multidimensional setting of the thermodynamic integration, a technique
developed in statistical physics. The method leverages the manifold hypothesis,
estimating quantities within the intrinsic data manifold without defining an
explicit coordinate map. It does not rely on any binning or space partitioning,
but rather on the construction of a neighbourhood graph based on an adaptive
bandwidth selection procedure. BMTI mitigates the limitations commonly
associated with traditional nonparametric density estimators, effectively
reconstructing smooth profiles even in high-dimensional embedding spaces. The
method is tested on a variety of complex synthetic high-dimensional datasets,
where it is shown to outperform traditional estimators, and is benchmarked on
realistic datasets from the chemical physics literature.","['Matteo Carli', 'Alex Rodriguez', 'Alessandro Laio', 'Aldo Glielmo']","['stat.ML', 'cs.LG', 'physics.chem-ph', 'physics.data-an']",2024-07-10 23:45:20+00:00
http://arxiv.org/abs/2407.08086v1,"The GeometricKernels Package: Heat and Matérn Kernels for Geometric Learning on Manifolds, Meshes, and Graphs","Kernels are a fundamental technical primitive in machine learning. In recent
years, kernel-based methods such as Gaussian processes are becoming
increasingly important in applications where quantifying uncertainty is of key
interest. In settings that involve structured data defined on graphs, meshes,
manifolds, or other related spaces, defining kernels with good
uncertainty-quantification behavior, and computing their value numerically, is
less straightforward than in the Euclidean setting. To address this difficulty,
we present GeometricKernels, a software package which implements the geometric
analogs of classical Euclidean squared exponential - also known as heat - and
Mat\'ern kernels, which are widely-used in settings where uncertainty is of key
interest. As a byproduct, we obtain the ability to compute Fourier-feature-type
expansions, which are widely used in their own right, on a wide set of
geometric spaces. Our implementation supports automatic differentiation in
every major current framework simultaneously via a backend-agnostic design. In
this companion paper to the package and its documentation, we outline the
capabilities of the package and present an illustrated example of its
interface. We also include a brief overview of the theory the package is built
upon and provide some historic context in the appendix.","['Peter Mostowsky', 'Vincent Dutordoir', 'Iskander Azangulov', 'Noémie Jaquier', 'Michael John Hutchinson', 'Aditya Ravuri', 'Leonel Rozo', 'Alexander Terenin', 'Viacheslav Borovitskiy']","['cs.LG', 'stat.CO', 'stat.ML']",2024-07-10 23:09:23+00:00
http://arxiv.org/abs/2407.07998v1,What's the score? Automated Denoising Score Matching for Nonlinear Diffusions,"Reversing a diffusion process by learning its score forms the heart of
diffusion-based generative modeling and for estimating properties of scientific
systems. The diffusion processes that are tractable center on linear processes
with a Gaussian stationary distribution. This limits the kinds of models that
can be built to those that target a Gaussian prior or more generally limits the
kinds of problems that can be generically solved to those that have
conditionally linear score functions. In this work, we introduce a family of
tractable denoising score matching objectives, called local-DSM, built using
local increments of the diffusion process. We show how local-DSM melded with
Taylor expansions enables automated training and score estimation with
nonlinear diffusion processes. To demonstrate these ideas, we use automated-DSM
to train generative models using non-Gaussian priors on challenging low
dimensional distributions and the CIFAR10 image dataset. Additionally, we use
the automated-DSM to learn the scores for nonlinear processes studied in
statistical physics.","['Raghav Singhal', 'Mark Goldstein', 'Rajesh Ranganath']","['cs.LG', 'stat.ML']",2024-07-10 19:02:19+00:00
http://arxiv.org/abs/2407.07873v1,Dynamical Measure Transport and Neural PDE Solvers for Sampling,"The task of sampling from a probability density can be approached as
transporting a tractable density function to the target, known as dynamical
measure transport. In this work, we tackle it through a principled unified
framework using deterministic or stochastic evolutions described by partial
differential equations (PDEs). This framework incorporates prior
trajectory-based sampling methods, such as diffusion models or Schr\""odinger
bridges, without relying on the concept of time-reversals. Moreover, it allows
us to propose novel numerical methods for solving the transport task and thus
sampling from complicated targets without the need for the normalization
constant or data samples. We employ physics-informed neural networks (PINNs) to
approximate the respective PDE solutions, implying both conceptional and
computational advantages. In particular, PINNs allow for simulation- and
discretization-free optimization and can be trained very efficiently, leading
to significantly better mode coverage in the sampling task compared to
alternative methods. Moreover, they can readily be fine-tuned with Gauss-Newton
methods to achieve high accuracy in sampling.","['Jingtong Sun', 'Julius Berner', 'Lorenz Richter', 'Marius Zeinhofer', 'Johannes Müller', 'Kamyar Azizzadenesheli', 'Anima Anandkumar']","['cs.LG', 'math.DS', 'math.OC', 'math.PR', 'stat.ML']",2024-07-10 17:39:50+00:00
http://arxiv.org/abs/2407.07829v2,Disentangled Representation Learning with the Gromov-Monge Gap,"Learning disentangled representations from unlabelled data is a fundamental
challenge in machine learning. Solving it may unlock other problems, such as
generalization, interpretability, or fairness. Although remarkably challenging
to solve in theory, disentanglement is often achieved in practice through prior
matching. Furthermore, recent works have shown that prior matching approaches
can be enhanced by leveraging geometrical considerations, e.g., by learning
representations that preserve geometric features of the data, such as distances
or angles between points. However, matching the prior while preserving
geometric features is challenging, as a mapping that fully preserves these
features while aligning the data distribution with the prior does not exist in
general. To address these challenges, we introduce a novel approach to
disentangled representation learning based on quadratic optimal transport. We
formulate the problem using Gromov-Monge maps that transport one distribution
onto another with minimal distortion of predefined geometric features,
preserving them as much as can be achieved. To compute such maps, we propose
the Gromov-Monge-Gap (GMG), a regularizer quantifying whether a map moves a
reference distribution with minimal geometry distortion. We demonstrate the
effectiveness of our approach for disentanglement across four standard
benchmarks, outperforming other methods leveraging geometric considerations.","['Théo Uscidda', 'Luca Eyring', 'Karsten Roth', 'Fabian Theis', 'Zeynep Akata', 'Marco Cuturi']","['cs.LG', 'cs.CV', 'stat.ML']",2024-07-10 16:51:32+00:00
http://arxiv.org/abs/2407.07821v2,When to Accept Automated Predictions and When to Defer to Human Judgment?,"Ensuring the reliability and safety of automated decision-making is crucial.
It is well-known that data distribution shifts in machine learning can produce
unreliable outcomes. This paper proposes a new approach for measuring the
reliability of predictions under distribution shifts. We analyze how the
outputs of a trained neural network change using clustering to measure
distances between outputs and class centroids. We propose this distance as a
metric to evaluate the confidence of predictions under distribution shifts. We
assign each prediction to a cluster with centroid representing the mean softmax
output for all correct predictions of a given class. We then define a safety
threshold for a class as the smallest distance from an incorrect prediction to
the given class centroid. We evaluate the approach on the MNIST and CIFAR-10
datasets using a Convolutional Neural Network and a Vision Transformer,
respectively. The results show that our approach is consistent across these
data sets and network models, and indicate that the proposed metric can offer
an efficient way of determining when automated predictions are acceptable and
when they should be deferred to human operators given a distribution shift.","['Daniel Sikar', 'Artur Garcez', 'Tillman Weyde', 'Robin Bloomfield', 'Kaleem Peeroo']","['cs.LG', 'stat.ML']",2024-07-10 16:45:52+00:00
http://arxiv.org/abs/2407.07781v1,Sequential Kalman Monte Carlo for gradient-free inference in Bayesian inverse problems,"Ensemble Kalman Inversion (EKI) has been proposed as an efficient method for
solving inverse problems with expensive forward models. However, the method is
based on the assumption that we proceed through a sequence of Gaussian measures
in moving from the prior to the posterior, and that the forward model is
linear. In this work, we introduce Sequential Kalman Monte Carlo (SKMC)
samplers, where we exploit EKI and Flow Annealed Kalman Inversion (FAKI) within
a Sequential Monte Carlo (SMC) sampling scheme to perform efficient
gradient-free inference in Bayesian inverse problems. FAKI employs normalizing
flows (NF) to relax the Gaussian ansatz of the target measures in EKI. NFs are
able to learn invertible maps between a Gaussian latent space and the original
data space, allowing us to perform EKI updates in the Gaussianized NF latent
space. However, FAKI alone is not able to correct for the model linearity
assumptions in EKI. Errors in the particle distribution as we move through the
sequence of target measures can therefore compound to give incorrect posterior
moment estimates. In this work we consider the use of EKI and FAKI to
initialize the particle distribution for each target in an adaptive SMC
annealing scheme, before performing t-preconditioned Crank-Nicolson (tpCN)
updates to distribute particles according to the target. We demonstrate the
performance of these SKMC samplers on three challenging numerical benchmarks,
showing significant improvements in the rate of convergence compared to
standard SMC with importance weighted resampling at each temperature level.
Code implementing the SKMC samplers is available at
https://github.com/RichardGrumitt/KalmanMC.","['Richard D. P. Grumitt', 'Minas Karamanis', 'Uroš Seljak']","['stat.CO', 'stat.ML']",2024-07-10 15:56:30+00:00
http://arxiv.org/abs/2407.07765v2,Ramsey Theorems for Trees and a General 'Private Learning Implies Online Learning' Theorem,"This work continues to investigate the link between differentially private
(DP) and online learning. Alon, Livni, Malliaris, and Moran (2019) showed that
for binary concept classes, DP learnability of a given class implies that it
has a finite Littlestone dimension (equivalently, that it is online learnable).
Their proof relies on a model-theoretic result by Hodges (1997), which
demonstrates that any binary concept class with a large Littlestone dimension
contains a large subclass of thresholds. In a follow-up work, Jung, Kim, and
Tewari (2020) extended this proof to multiclass PAC learning with a bounded
number of labels. Unfortunately, Hodges's result does not apply in other
natural settings such as multiclass PAC learning with an unbounded label space,
and PAC learning of partial concept classes.
  This naturally raises the question of whether DP learnability continues to
imply online learnability in more general scenarios: indeed, Alon, Hanneke,
Holzman, and Moran (2021) explicitly leave it as an open question in the
context of partial concept classes, and the same question is open in the
general multiclass setting. In this work, we give a positive answer to these
questions showing that for general classification tasks, DP learnability
implies online learnability. Our proof reasons directly about Littlestone
trees, without relying on thresholds. We achieve this by establishing several
Ramsey-type theorems for trees, which might be of independent interest.","['Simone Fioravanti', 'Steve Hanneke', 'Shay Moran', 'Hilla Schefler', 'Iska Tsubari']","['cs.LG', 'cs.CR', 'cs.DS', 'math.CO', 'stat.ML']",2024-07-10 15:43:30+00:00
http://arxiv.org/abs/2407.07700v2,Split Conformal Prediction under Data Contamination,"Conformal prediction is a non-parametric technique for constructing
prediction intervals or sets from arbitrary predictive models under the
assumption that the data is exchangeable. It is popular as it comes with
theoretical guarantees on the marginal coverage of the prediction sets and the
split conformal prediction variant has a very low computational cost compared
to model training. We study the robustness of split conformal prediction in a
data contamination setting, where we assume a small fraction of the calibration
scores are drawn from a different distribution than the bulk. We quantify the
impact of the corrupted data on the coverage and efficiency of the constructed
sets when evaluated on ""clean"" test points, and verify our results with
numerical experiments. Moreover, we propose an adjustment in the classification
setting which we call Contamination Robust Conformal Prediction, and verify the
efficacy of our approach using both synthetic and real datasets.","['Jase Clarkson', 'Wenkai Xu', 'Mihai Cucuringu', 'Gesine Reinert']","['stat.ML', 'cs.LG']",2024-07-10 14:33:28+00:00
http://arxiv.org/abs/2407.07670v1,Stochastic Gradient Descent for Two-layer Neural Networks,"This paper presents a comprehensive study on the convergence rates of the
stochastic gradient descent (SGD) algorithm when applied to overparameterized
two-layer neural networks. Our approach combines the Neural Tangent Kernel
(NTK) approximation with convergence analysis in the Reproducing Kernel Hilbert
Space (RKHS) generated by NTK, aiming to provide a deep understanding of the
convergence behavior of SGD in overparameterized two-layer neural networks. Our
research framework enables us to explore the intricate interplay between kernel
methods and optimization processes, shedding light on the optimization dynamics
and convergence properties of neural networks. In this study, we establish
sharp convergence rates for the last iterate of the SGD algorithm in
overparameterized two-layer neural networks. Additionally, we have made
significant advancements in relaxing the constraints on the number of neurons,
which have been reduced from exponential dependence to polynomial dependence on
the sample size or number of iterations. This improvement allows for more
flexibility in the design and scaling of neural networks, and will deepen our
theoretical understanding of neural network models trained with SGD.","['Dinghao Cao', 'Zheng-Chu Guo', 'Lei Shi']","['stat.ML', 'cs.LG']",2024-07-10 13:58:57+00:00
http://arxiv.org/abs/2407.07664v1,A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning Geometry,"Hyperspherical Prototypical Learning (HPL) is a supervised approach to
representation learning that designs class prototypes on the unit hypersphere.
The prototypes bias the representations to class separation in a scale
invariant and known geometry. Previous approaches to HPL have either of the
following shortcomings: (i) they follow an unprincipled optimisation procedure;
or (ii) they are theoretically sound, but are constrained to only one possible
latent dimension. In this paper, we address both shortcomings. To address (i),
we present a principled optimisation procedure whose solution we show is
optimal. To address (ii), we construct well-separated prototypes in a wide
range of dimensions using linear block codes. Additionally, we give a full
characterisation of the optimal prototype placement in terms of achievable and
converse bounds, showing that our proposed methods are near-optimal.","['Martin Lindström', 'Borja Rodríguez-Gálvez', 'Ragnar Thobaben', 'Mikael Skoglund']","['cs.LG', 'cs.AI', 'cs.CV', 'eess.SP', 'stat.ML']",2024-07-10 13:44:19+00:00
http://arxiv.org/abs/2407.07631v1,Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning,"We study risk-sensitive reinforcement learning (RL), a crucial field due to
its ability to enhance decision-making in scenarios where it is essential to
manage uncertainty and minimize potential adverse outcomes. Particularly, our
work focuses on applying the entropic risk measure to RL problems. While
existing literature primarily investigates the online setting, there remains a
large gap in understanding how to efficiently derive a near-optimal policy
based on this risk measure using only a pre-collected dataset. We center on the
linear Markov Decision Process (MDP) setting, a well-regarded theoretical
framework that has yet to be examined from a risk-sensitive standpoint. In
response, we introduce two provably sample-efficient algorithms. We begin by
presenting a risk-sensitive pessimistic value iteration algorithm, offering a
tight analysis by leveraging the structure of the risk-sensitive performance
measure. To further improve the obtained bounds, we propose another pessimistic
algorithm that utilizes variance information and reference-advantage
decomposition, effectively improving both the dependence on the space dimension
$d$ and the risk-sensitivity factor. To the best of our knowledge, we obtain
the first provably efficient risk-sensitive offline RL algorithms.","['Dake Zhang', 'Boxiang Lyu', 'Shuang Qiu', 'Mladen Kolar', 'Tong Zhang']","['cs.LG', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",2024-07-10 13:09:52+00:00
http://arxiv.org/abs/2407.07933v2,Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments,"We consider the challenging problem of estimating causal effects from purely
observational data in the bi-directional Mendelian randomization (MR), where
some invalid instruments, as well as unmeasured confounding, usually exist. To
address this problem, most existing methods attempt to find proper valid
instrumental variables (IVs) for the target causal effect by expert knowledge
or by assuming that the causal model is a one-directional MR model. As such, in
this paper, we first theoretically investigate the identification of the
bi-directional MR from observational data. In particular, we provide necessary
and sufficient conditions under which valid IV sets are correctly identified
such that the bi-directional MR model is identifiable, including the causal
directions of a pair of phenotypes (i.e., the treatment and outcome). Moreover,
based on the identification theory, we develop a cluster fusion-like method to
discover valid IV sets and estimate the causal effects of interest. We
theoretically demonstrate the correctness of the proposed algorithm.
Experimental results show the effectiveness of our method for estimating causal
effects in bi-directional MR.","['Feng Xie', 'Zhen Yao', 'Lin Xie', 'Yan Zeng', 'Zhi Geng']","['stat.ME', 'cs.LG', 'stat.ML']",2024-07-10 12:58:30+00:00
http://arxiv.org/abs/2407.07596v1,Learning treatment effects while treating those in need,"Many social programs attempt to allocate scarce resources to people with the
greatest need. Indeed, public services increasingly use algorithmic risk
assessments motivated by this goal. However, targeting the highest-need
recipients often conflicts with attempting to evaluate the causal effect of the
program as a whole, as the best evaluations would be obtained by randomizing
the allocation. We propose a framework to design randomized allocation rules
which optimally balance targeting high-need individuals with learning treatment
effects, presenting policymakers with a Pareto frontier between the two goals.
We give sample complexity guarantees for the policy learning problem and
provide a computationally efficient strategy to implement it. We then apply our
framework to data from human services in Allegheny County, Pennsylvania.
Optimized policies can substantially mitigate the tradeoff between learning and
targeting. For example, it is often possible to obtain 90% of the optimal
utility in targeting high-need individuals while ensuring that the average
treatment effect can be estimated with less than 2 times the samples that a
randomized controlled trial would require. Mechanisms for targeting public
services often focus on measuring need as accurately as possible. However, our
results suggest that algorithmic systems in public services can be most
impactful if they incorporate program evaluation as an explicit goal alongside
targeting.","['Bryan Wilder', 'Pim Welle']","['cs.LG', 'stat.ME', 'stat.ML']",2024-07-10 12:29:46+00:00
http://arxiv.org/abs/2407.07450v1,Using Low-Discrepancy Points for Data Compression in Machine Learning: An Experimental Comparison,"Low-discrepancy points (also called Quasi-Monte Carlo points) are
deterministically and cleverly chosen point sets in the unit cube, which
provide an approximation of the uniform distribution. We explore two methods
based on such low-discrepancy points to reduce large data sets in order to
train neural networks. The first one is the method of Dick and Feischl [4],
which relies on digital nets and an averaging procedure. Motivated by our
experimental findings, we construct a second method, which again uses digital
nets, but Voronoi clustering instead of averaging. Both methods are compared to
the supercompress approach of [14], which is a variant of the K-means
clustering algorithm. The comparison is done in terms of the compression error
for different objective functions and the accuracy of the training of a neural
network.","['Simone Göttlich', 'Jacob Heieck', 'Andreas Neuenkirch']","['stat.ML', 'cs.LG']",2024-07-10 08:07:55+00:00
http://arxiv.org/abs/2407.07350v1,Long-Term Fairness in Sequential Multi-Agent Selection with Positive Reinforcement,"While much of the rapidly growing literature on fair decision-making focuses
on metrics for one-shot decisions, recent work has raised the intriguing
possibility of designing sequential decision-making to positively impact
long-term social fairness. In selection processes such as college admissions or
hiring, biasing slightly towards applicants from under-represented groups is
hypothesized to provide positive feedback that increases the pool of
under-represented applicants in future selection rounds, thus enhancing
fairness in the long term. In this paper, we examine this hypothesis and its
consequences in a setting in which multiple agents are selecting from a common
pool of applicants. We propose the Multi-agent Fair-Greedy policy, that
balances greedy score maximization and fairness. Under this policy, we prove
that the resource pool and the admissions converge to a long-term fairness
target set by the agents when the score distributions across the groups in the
population are identical. We provide empirical evidence of existence of
equilibria under non-identical score distributions through synthetic and
adapted real-world datasets. We then sound a cautionary note for more complex
applicant pool evolution models, under which uncoordinated behavior by the
agents can cause negative reinforcement, leading to a reduction in the fraction
of under-represented applicants. Our results indicate that, while positive
reinforcement is a promising mechanism for long-term fairness, policies must be
designed carefully to be robust to variations in the evolution model, with a
number of open issues that remain to be explored by algorithm designers, social
scientists, and policymakers.","['Bhagyashree Puranik', 'Ozgur Guldogan', 'Upamanyu Madhow', 'Ramtin Pedarsani']","['stat.ML', 'cs.CY', 'cs.LG']",2024-07-10 04:03:23+00:00
http://arxiv.org/abs/2407.07338v2,Towards Complete Causal Explanation with Expert Knowledge,"We study the problem of restricting a Markov equivalence class of maximal
ancestral graphs (MAGs) to only those MAGs that contain certain edge marks,
which we refer to as expert knowledge. Such a restriction of the Markov
equivalence class can be uniquely represented by a restricted essential
ancestral graph. Our contributions are several-fold. First, we prove certain
properties for the entire Markov equivalence class including a conjecture from
Ali et al. (2009). Second, we present several new sound graphical orientation
rules for adding expert knowledge to an essential ancestral graph. We also show
that some orientation rules of Zhang (2008b) are not needed for restricting the
Markov equivalence class with expert knowledge. Third, we provide an algorithm
for including this expert knowledge and show that in certain settings the
output of our algorithm is a restricted essential ancestral graph. Finally,
outside of the specified settings, we provide an algorithm for checking whether
a graph is a restricted essential graph and discuss its runtime. This work can
be seen as a generalization of Meek (1995) to settings which allow for latent
confounding.","['Aparajithan Venkateswaran', 'Emilija Perković']","['stat.ML', 'cs.DM', 'cs.LG', 'stat.ME']",2024-07-10 03:20:17+00:00
http://arxiv.org/abs/2407.07333v2,Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy,"Reinforcement learning algorithms typically rely on the assumption that the
environment dynamics and value function can be expressed in terms of a
Markovian state representation. However, when state information is only
partially observable, how can an agent learn such a state representation, and
how can it detect when it has found one? We introduce a metric that can
accomplish both objectives, without requiring access to--or knowledge of--an
underlying, unobservable state space. Our metric, the $\lambda$-discrepancy, is
the difference between two distinct temporal difference (TD) value estimates,
each computed using TD($\lambda$) with a different value of $\lambda$. Since
TD($\lambda$=0) makes an implicit Markov assumption and TD($\lambda$=1) does
not, a discrepancy between these estimates is a potential indicator of a
non-Markovian state representation. Indeed, we prove that the
$\lambda$-discrepancy is exactly zero for all Markov decision processes and
almost always non-zero for a broad class of partially observable environments.
We also demonstrate empirically that, once detected, minimizing the
$\lambda$-discrepancy can help with learning a memory function to mitigate the
corresponding partial observability. We then train a reinforcement learning
agent that simultaneously constructs two recurrent value networks with
different $\lambda$ parameters and minimizes the difference between them as an
auxiliary loss. The approach scales to challenging partially observable
domains, where the resulting agent frequently performs significantly better
(and never performs worse) than a baseline recurrent agent with only a single
value network.","['Cameron Allen', 'Aaron Kirtland', 'Ruo Yu Tao', 'Sam Lobel', 'Daniel Scott', 'Nicholas Petrocelli', 'Omer Gottesman', 'Ronald Parr', 'Michael L. Littman', 'George Konidaris']","['cs.LG', 'cs.AI', 'stat.ML']",2024-07-10 03:04:20+00:00
http://arxiv.org/abs/2407.07291v1,Causal Discovery in Semi-Stationary Time Series,"Discovering causal relations from observational time series without making
the stationary assumption is a significant challenge. In practice, this
challenge is common in many areas, such as retail sales, transportation
systems, and medical science. Here, we consider this problem for a class of
non-stationary time series. The structural causal model (SCM) of this type of
time series, called the semi-stationary time series, exhibits that a finite
number of different causal mechanisms occur sequentially and periodically
across time. This model holds considerable practical utility because it can
represent periodicity, including common occurrences such as seasonality and
diurnal variation. We propose a constraint-based, non-parametric algorithm for
discovering causal relations in this setting. The resulting algorithm,
PCMCI$_{\Omega}$, can capture the alternating and recurring changes in the
causal mechanisms and then identify the underlying causal graph with
conditional independence (CI) tests. We show that this algorithm is sound in
identifying causal relations on discrete time series. We validate the algorithm
with extensive experiments on continuous and discrete simulated data. We also
apply our algorithm to a real-world climate dataset.","['Shanyun Gao', 'Raghavendra Addanki', 'Tong Yu', 'Ryan A. Rossi', 'Murat Kocaoglu']","['cs.LG', 'cs.AI', 'stat.ML', 'I.2.6, G.3']",2024-07-10 00:55:38+00:00
http://arxiv.org/abs/2407.07290v1,Causal Discovery-Driven Change Point Detection in Time Series,"Change point detection in time series seeks to identify times when the
probability distribution of time series changes. It is widely applied in many
areas, such as human-activity sensing and medical science. In the context of
multivariate time series, this typically involves examining the joint
distribution of high-dimensional data: If any one variable changes, the whole
time series is assumed to have changed. However, in practical applications, we
may be interested only in certain components of the time series, exploring
abrupt changes in their distributions in the presence of other time series.
Here, assuming an underlying structural causal model that governs the
time-series data generation, we address this problem by proposing a two-stage
non-parametric algorithm that first learns parts of the causal structure
through constraint-based discovery methods. The algorithm then uses conditional
relative Pearson divergence estimation to identify the change points. The
conditional relative Pearson divergence quantifies the distribution disparity
between consecutive segments in the time series, while the causal discovery
method enables a focus on the causal mechanism, facilitating access to
independent and identically distributed (IID) samples. Theoretically, the
typical assumption of samples being IID in conventional change point detection
methods can be relaxed based on the Causal Markov Condition. Through
experiments on both synthetic and real-world datasets, we validate the
correctness and utility of our approach.","['Shanyun Gao', 'Raghavendra Addanki', 'Tong Yu', 'Ryan A. Rossi', 'Murat Kocaoglu']","['cs.LG', 'cs.AI', 'stat.ML', 'I.2.6, G.3']",2024-07-10 00:54:42+00:00
http://arxiv.org/abs/2407.07279v1,Towards a theory of learning dynamics in deep state space models,"State space models (SSMs) have shown remarkable empirical performance on many
long sequence modeling tasks, but a theoretical understanding of these models
is still lacking. In this work, we study the learning dynamics of linear SSMs
to understand how covariance structure in data, latent state size, and
initialization affect the evolution of parameters throughout learning with
gradient descent. We show that focusing on the learning dynamics in the
frequency domain affords analytical solutions under mild assumptions, and we
establish a link between one-dimensional SSMs and the dynamics of deep linear
feed-forward networks. Finally, we analyze how latent state
over-parameterization affects convergence time and describe future work in
extending our results to the study of deep SSMs with nonlinear connections.
This work is a step toward a theory of learning dynamics in deep state space
models.","['Jakub Smékal', 'Jimmy T. H. Smith', 'Michael Kleinman', 'Dan Biderman', 'Scott W. Linderman']","['cs.LG', 'stat.ML']",2024-07-10 00:01:56+00:00
http://arxiv.org/abs/2407.07239v2,RotRNN: Modelling Long Sequences with Rotations,"Linear recurrent neural networks, such as State Space Models (SSMs) and
Linear Recurrent Units (LRUs), have recently shown state-of-the-art performance
on long sequence modelling benchmarks. Despite their success, their empirical
performance is not well understood and they come with a number of drawbacks,
most notably their complex initialisation and normalisation schemes. In this
work, we address some of these issues by proposing RotRNN -- a linear recurrent
model which utilises the convenient properties of rotation matrices. We show
that RotRNN provides a simple and efficient model with a robust normalisation
procedure, and a practical implementation that remains faithful to its
theoretical derivation. RotRNN also achieves competitive performance to
state-of-the-art linear recurrent models on several long sequence modelling
datasets.","['Kai Biegun', 'Rares Dolga', 'Jake Cunningham', 'David Barber']","['cs.LG', 'stat.ML']",2024-07-09 21:37:36+00:00
http://arxiv.org/abs/2407.07222v1,SPINEX-Clustering: Similarity-based Predictions with Explainable Neighbors Exploration for Clustering Problems,"This paper presents a novel clustering algorithm from the SPINEX
(Similarity-based Predictions with Explainable Neighbors Exploration)
algorithmic family. The newly proposed clustering variant leverages the concept
of similarity and higher-order interactions across multiple subspaces to group
data into clusters. To showcase the merit of SPINEX, a thorough set of
benchmarking experiments was carried out against 13 algorithms, namely,
Affinity Propagation, Agglomerative, Birch, DBSCAN, Gaussian Mixture, HDBSCAN,
K-Means, KMedoids, Mean Shift, MiniBatch K-Means, OPTICS, Spectral Clustering,
and Ward Hierarchical. Then, the performance of all algorithms was examined
across 51 synthetic and real datasets from various domains, dimensions, and
complexities. Furthermore, we present a companion complexity analysis to
compare the complexity of SPINEX to that of the aforementioned algorithms. Our
results demonstrate that SPINEX can outperform commonly adopted clustering
algorithms by ranking within the top-5 best performing algorithms and has
moderate complexity. Finally, a demonstration of the explainability
capabilities of SPINEX, along with future research needs, is presented.","['MZ Naser', 'Ahmed Naser']","['cs.LG', 'stat.ML']",2024-07-09 20:38:01+00:00
http://arxiv.org/abs/2407.08814v1,Covariate Assisted Entity Ranking with Sparse Intrinsic Scores,"This paper addresses the item ranking problem with associate covariates,
focusing on scenarios where the preference scores can not be fully explained by
covariates, and the remaining intrinsic scores, are sparse. Specifically, we
extend the pioneering Bradley-Terry-Luce (BTL) model by incorporating covariate
information and considering sparse individual intrinsic scores. Our work
introduces novel model identification conditions and examines the regularized
penalized Maximum Likelihood Estimator (MLE) statistical rates. We then
construct a debiased estimator for the penalized MLE and analyze its
distributional properties. Additionally, we apply our method to the
goodness-of-fit test for models with no latent intrinsic scores, namely, the
covariates fully explaining the preference scores of individual items. We also
offer confidence intervals for ranks. Our numerical studies lend further
support to our theoretical findings, demonstrating validation for our proposed
method","['Jianqing Fan', 'Jikai Hou', 'Mengxin Yu']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2024-07-09 19:58:54+00:00
http://arxiv.org/abs/2407.07140v1,Cardinality-Aware Set Prediction and Top-$k$ Classification,"We present a detailed study of cardinality-aware top-$k$ classification, a
novel approach that aims to learn an accurate top-$k$ set predictor while
maintaining a low cardinality. We introduce a new target loss function tailored
to this setting that accounts for both the classification error and the
cardinality of the set predicted. To optimize this loss function, we propose
two families of surrogate losses: cost-sensitive comp-sum losses and
cost-sensitive constrained losses. Minimizing these loss functions leads to new
cardinality-aware algorithms that we describe in detail in the case of both
top-$k$ and threshold-based classifiers. We establish $H$-consistency bounds
for our cardinality-aware surrogate loss functions, thereby providing a strong
theoretical foundation for our algorithms. We report the results of extensive
experiments on CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets demonstrating
the effectiveness and benefits of our cardinality-aware algorithms.","['Corinna Cortes', 'Anqi Mao', 'Christopher Mohri', 'Mehryar Mohri', 'Yutao Zhong']","['cs.LG', 'stat.ML']",2024-07-09 17:57:07+00:00
http://arxiv.org/abs/2407.07135v1,Improving Out-of-Distribution Detection by Combining Existing Post-hoc Methods,"Since the seminal paper of Hendrycks et al. arXiv:1610.02136, Post-hoc deep
Out-of-Distribution (OOD) detection has expanded rapidly. As a result,
practitioners working on safety-critical applications and seeking to improve
the robustness of a neural network now have a plethora of methods to choose
from. However, no method outperforms every other on every dataset
arXiv:2210.07242, so the current best practice is to test all the methods on
the datasets at hand. This paper shifts focus from developing new methods to
effectively combining existing ones to enhance OOD detection. We propose and
compare four different strategies for integrating multiple detection scores
into a unified OOD detector, based on techniques such as majority vote,
empirical and copulas-based Cumulative Distribution Function modeling, and
multivariate quantiles based on optimal transport. We extend common OOD
evaluation metrics -- like AUROC and FPR at fixed TPR rates -- to these
multi-dimensional OOD detectors, allowing us to evaluate them and compare them
with individual methods on extensive benchmarks. Furthermore, we propose a
series of guidelines to choose what OOD detectors to combine in more realistic
settings, i.e. in the absence of known OOD data, relying on principles drawn
from Outlier Exposure arXiv:1812.04606. The code is available at
https://github.com/paulnovello/multi-ood.","['Paul Novello', 'Yannick Prudent', 'Joseba Dalmau', 'Corentin Friedrich', 'Yann Pequignot']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']",2024-07-09 15:46:39+00:00
http://arxiv.org/abs/2407.06945v2,Adaptively Robust and Sparse K-means Clustering,"While K-means is known to be a standard clustering algorithm, its performance
may be compromised due to the presence of outliers and high-dimensional noisy
variables. This paper proposes adaptively robust and sparse K-means clustering
(ARSK) to address these practical limitations of the standard K-means
algorithm. For robustness, we introduce a redundant error component for each
observation, and this additional parameter is penalized using a group sparse
penalty. To accommodate the impact of high-dimensional noisy variables, the
objective function is modified by incorporating weights and implementing a
penalty to control the sparsity of the weight vector. The tuning parameters to
control the robustness and sparsity are selected by Gap statistics. Through
simulation experiments and real data analysis, we demonstrate the proposed
method's superiority to existing algorithms in identifying clusters without
outliers and informative variables simultaneously.","['Hao Li', 'Shonosuke Sugasawa', 'Shota Katayama']","['stat.CO', 'stat.ML']",2024-07-09 15:20:41+00:00
http://arxiv.org/abs/2407.06935v1,Bayesian Federated Learning with Hamiltonian Monte Carlo: Algorithm and Theory,"This work introduces a novel and efficient Bayesian federated learning
algorithm, namely, the Federated Averaging stochastic Hamiltonian Monte Carlo
(FA-HMC), for parameter estimation and uncertainty quantification. We establish
rigorous convergence guarantees of FA-HMC on non-iid distributed data sets,
under the strong convexity and Hessian smoothness assumptions. Our analysis
investigates the effects of parameter space dimension, noise on gradients and
momentum, and the frequency of communication (between the central node and
local nodes) on the convergence and communication costs of FA-HMC. Beyond that,
we establish the tightness of our analysis by showing that the convergence rate
cannot be improved even for continuous FA-HMC process. Moreover, extensive
empirical studies demonstrate that FA-HMC outperforms the existing Federated
Averaging-Langevin Monte Carlo (FA-LD) algorithm.","['Jiajun Liang', 'Qian Zhang', 'Wei Deng', 'Qifan Song', 'Guang Lin']","['cs.LG', 'stat.CO', 'stat.ML']",2024-07-09 15:10:59+00:00
http://arxiv.org/abs/2407.06867v3,Distributionally robust risk evaluation with an isotonic constraint,"Statistical learning under distribution shift is challenging when neither
prior knowledge nor fully accessible data from the target distribution is
available. Distributionally robust learning (DRL) aims to control the
worst-case statistical performance within an uncertainty set of candidate
distributions, but how to properly specify the set remains challenging. To
enable distributional robustness without being overly conservative, in this
paper, we propose a shape-constrained approach to DRL, which incorporates prior
information about the way in which the unknown target distribution differs from
its estimate. More specifically, we assume the unknown density ratio between
the target distribution and its estimate is isotonic with respect to some
partial order. At the population level, we provide a solution to the
shape-constrained optimization problem that does not involve the isotonic
constraint. At the sample level, we provide consistency results for an
empirical estimator of the target in a range of different settings. Empirical
studies on both synthetic and real data examples demonstrate the improved
accuracy of the proposed shape-constrained approach.","['Yu Gui', 'Rina Foygel Barber', 'Cong Ma']","['stat.ME', 'stat.ML']",2024-07-09 13:56:34+00:00
http://arxiv.org/abs/2407.06797v1,ED-VAE: Entropy Decomposition of ELBO in Variational Autoencoders,"Traditional Variational Autoencoders (VAEs) are constrained by the
limitations of the Evidence Lower Bound (ELBO) formulation, particularly when
utilizing simplistic, non-analytic, or unknown prior distributions. These
limitations inhibit the VAE's ability to generate high-quality samples and
provide clear, interpretable latent representations. This work introduces the
Entropy Decomposed Variational Autoencoder (ED-VAE), a novel re-formulation of
the ELBO that explicitly includes entropy and cross-entropy components. This
reformulation significantly enhances model flexibility, allowing for the
integration of complex and non-standard priors. By providing more detailed
control over the encoding and regularization of latent spaces, ED-VAE not only
improves interpretability but also effectively captures the complex
interactions between latent variables and observed data, thus leading to better
generative performance.","['Fotios Lygerakis', 'Elmar Rueckert']","['cs.LG', 'cs.AI', 'stat.ML']",2024-07-09 12:09:21+00:00
http://arxiv.org/abs/2407.06765v1,A Generalization Bound for Nearly-Linear Networks,"We consider nonlinear networks as perturbations of linear ones. Based on this
approach, we present novel generalization bounds that become non-vacuous for
networks that are close to being linear. The main advantage over the previous
works which propose non-vacuous generalization bounds is that our bounds are
a-priori: performing the actual training is not required for evaluating the
bounds. To the best of our knowledge, they are the first non-vacuous
generalization bounds for neural nets possessing this property.",['Eugene Golikov'],"['cs.LG', 'cs.AI', 'stat.ML']",2024-07-09 11:20:01+00:00
http://arxiv.org/abs/2407.07128v1,Modularity aided consistent attributed graph clustering via coarsening,"Graph clustering is an important unsupervised learning technique for
partitioning graphs with attributes and detecting communities. However, current
methods struggle to accurately capture true community structures and
intra-cluster relations, be computationally efficient, and identify smaller
communities. We address these challenges by integrating coarsening and
modularity maximization, effectively leveraging both adjacency and node
features to enhance clustering accuracy. We propose a loss function
incorporating log-determinant, smoothness, and modularity components using a
block majorization-minimization technique, resulting in superior clustering
outcomes. The method is theoretically consistent under the Degree-Corrected
Stochastic Block Model (DC-SBM), ensuring asymptotic error-free performance and
complete label recovery. Our provably convergent and time-efficient algorithm
seamlessly integrates with graph neural networks (GNNs) and variational graph
autoencoders (VGAEs) to learn enhanced node features and deliver exceptional
clustering performance. Extensive experiments on benchmark datasets demonstrate
its superiority over existing state-of-the-art methods for both attributed and
non-attributed graphs.","['Samarth Bhatia', 'Yukti Makhija', 'Manoj Kumar', 'Sandeep Kumar']","['cs.LG', 'cs.SI', 'stat.ML']",2024-07-09 10:42:19+00:00
http://arxiv.org/abs/2407.06646v1,Variational Learning ISTA,"Compressed sensing combines the power of convex optimization techniques with
a sparsity-inducing prior on the signal space to solve an underdetermined
system of equations. For many problems, the sparsifying dictionary is not
directly given, nor its existence can be assumed. Besides, the sensing matrix
can change across different scenarios. Addressing these issues requires solving
a sparse representation learning problem, namely dictionary learning, taking
into account the epistemic uncertainty of the learned dictionaries and,
finally, jointly learning sparse representations and reconstructions under
varying sensing matrix conditions. We address both concerns by proposing a
variant of the LISTA architecture. First, we introduce Augmented Dictionary
Learning ISTA (A-DLISTA), which incorporates an augmentation module to adapt
parameters to the current measurement setup. Then, we propose to learn a
distribution over dictionaries via a variational approach, dubbed Variational
Learning ISTA (VLISTA). VLISTA exploits A-DLISTA as the likelihood model and
approximates a posterior distribution over the dictionaries as part of an
unfolded LISTA-based recovery algorithm. As a result, VLISTA provides a
probabilistic way to jointly learn the dictionary distribution and the
reconstruction algorithm with varying sensing matrices. We provide theoretical
and experimental support for our architecture and show that our model learns
calibrated uncertainties.","['Fabio Valerio Massoli', 'Christos Louizos', 'Arash Behboodi']","['cs.LG', 'eess.SP', 'stat.ML']",2024-07-09 08:17:06+00:00
http://arxiv.org/abs/2407.06635v1,Ensembled Cold-Diffusion Restorations for Unsupervised Anomaly Detection,"Unsupervised Anomaly Detection (UAD) methods aim to identify anomalies in
test samples comparing them with a normative distribution learned from a
dataset known to be anomaly-free. Approaches based on generative models offer
interpretability by generating anomaly-free versions of test images, but are
typically unable to identify subtle anomalies. Alternatively, approaches using
feature modelling or self-supervised methods, such as the ones relying on
synthetically generated anomalies, do not provide out-of-the-box
interpretability. In this work, we present a novel method that combines the
strengths of both strategies: a generative cold-diffusion pipeline (i.e., a
diffusion-like pipeline which uses corruptions not based on noise) that is
trained with the objective of turning synthetically-corrupted images back to
their normal, original appearance. To support our pipeline we introduce a novel
synthetic anomaly generation procedure, called DAG, and a novel anomaly score
which ensembles restorations conditioned with different degrees of abnormality.
Our method surpasses the prior state-of-the art for unsupervised anomaly
detection in three different Brain MRI datasets.","['Sergio Naval Marimont', 'Vasilis Siomos', 'Matthew Baugh', 'Christos Tzelepis', 'Bernhard Kainz', 'Giacomo Tarroni']","['cs.CV', 'stat.ML']",2024-07-09 08:02:46+00:00
http://arxiv.org/abs/2407.06390v1,JANET: Joint Adaptive predictioN-region Estimation for Time-series,"Conformal prediction provides machine learning models with prediction sets
that offer theoretical guarantees, but the underlying assumption of
exchangeability limits its applicability to time series data. Furthermore,
existing approaches struggle to handle multi-step ahead prediction tasks, where
uncertainty estimates across multiple future time points are crucial. We
propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a
novel framework for constructing conformal prediction regions that are valid
for both univariate and multivariate time series. JANET generalises the
inductive conformal framework and efficiently produces joint prediction regions
with controlled K-familywise error rates, enabling flexible adaptation to
specific application needs. Our empirical evaluation demonstrates JANET's
superior performance in multi-step prediction tasks across diverse time series
datasets, highlighting its potential for reliable and interpretable uncertainty
quantification in sequential data.","['Eshant English', 'Eliot Wong-Toi', 'Matteo Fontana', 'Stephan Mandt', 'Padhraic Smyth', 'Christoph Lippert']","['stat.ML', 'cs.LG', 'stat.ME']",2024-07-08 21:03:15+00:00
http://arxiv.org/abs/2407.06346v1,High-Dimensional Distributed Sparse Classification with Scalable Communication-Efficient Global Updates,"As the size of datasets used in statistical learning continues to grow,
distributed training of models has attracted increasing attention. These
methods partition the data and exploit parallelism to reduce memory and
runtime, but suffer increasingly from communication costs as the data size or
the number of iterations grows. Recent work on linear models has shown that a
surrogate likelihood can be optimized locally to iteratively improve on an
initial solution in a communication-efficient manner. However, existing
versions of these methods experience multiple shortcomings as the data size
becomes massive, including diverging updates and efficiently handling sparsity.
In this work we develop solutions to these problems which enable us to learn a
communication-efficient distributed logistic regression model even beyond
millions of features. In our experiments we demonstrate a large improvement in
accuracy over distributed algorithms with only a few distributed update steps
needed, and similar or faster runtimes. Our code is available at
\url{https://github.com/FutureComputing4AI/ProxCSL}.","['Fred Lu', 'Ryan R. Curtin', 'Edward Raff', 'Francis Ferraro', 'James Holt']","['cs.LG', 'cs.DC', 'stat.ML']",2024-07-08 19:34:39+00:00
http://arxiv.org/abs/2407.06321v1,Open Problem: Tight Bounds for Kernelized Multi-Armed Bandits with Bernoulli Rewards,"We consider Kernelized Bandits (KBs) to optimize a function $f : \mathcal{X}
\rightarrow [0,1]$ belonging to the Reproducing Kernel Hilbert Space (RKHS)
$\mathcal{H}_k$. Mainstream works on kernelized bandits focus on a subgaussian
noise model in which observations of the form $f(\mathbf{x}_t)+\epsilon_t$,
being $\epsilon_t$ a subgaussian noise, are available (Chowdhury and Gopalan,
2017). Differently, we focus on the case in which we observe realizations $y_t
\sim \text{Ber}(f(\mathbf{x}_t))$ sampled from a Bernoulli distribution with
parameter $f(\mathbf{x}_t)$. While the Bernoulli model has been investigated
successfully in multi-armed bandits (Garivier and Capp\'e, 2011), logistic
bandits (Faury et al., 2022), bandits in metric spaces (Magureanu et al.,
2014), it remains an open question whether tight results can be obtained for
KBs. This paper aims to draw the attention of the online learning community to
this open problem.","['Marco Mussi', 'Simone Drago', 'Alberto Maria Metelli']","['stat.ML', 'cs.LG']",2024-07-08 18:38:11+00:00
http://arxiv.org/abs/2407.06120v2,Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning,"We revisit data selection in a modern context of finetuning from a
fundamental perspective. Extending the classical wisdom of variance
minimization in low dimensions to high-dimensional finetuning, our
generalization analysis unveils the importance of additionally reducing bias
induced by low-rank approximation. Inspired by the variance-bias tradeoff in
high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a
scalable data selection scheme with two stages. (i) First, the bias is
controlled using gradient sketching that explores the finetuning parameter
space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the
variance is reduced over $\mathcal{S}$ via moment matching between the original
and selected datasets. Theoretically, we show that gradient sketching is fast
and provably accurate: selecting $n$ samples by reducing variance over
$\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$,
independent of the parameter dimension. Empirically, we concretize the
variance-bias balance via synthetic experiments and demonstrate the
effectiveness of SkMM for finetuning in real vision tasks.","['Yijun Dong', 'Hoang Phan', 'Xiang Pan', 'Qi Lei']","['cs.LG', 'stat.ML']",2024-07-08 16:57:26+00:00
http://arxiv.org/abs/2407.06015v1,Simulation-based Benchmarking for Causal Structure Learning in Gene Perturbation Experiments,"Causal structure learning (CSL) refers to the task of learning causal
relationships from data. Advances in CSL now allow learning of causal graphs in
diverse application domains, which has the potential to facilitate data-driven
causal decision-making. Real-world CSL performance depends on a number of
$\textit{context-specific}$ factors, including context-specific data
distributions and non-linear dependencies, that are important in practical
use-cases. However, our understanding of how to assess and select CSL methods
in specific contexts remains limited. To address this gap, we present
$\textit{CausalRegNet}$, a multiplicative effect structural causal model that
allows for generating observational and interventional data incorporating
context-specific properties, with a focus on the setting of gene perturbation
experiments. Using real-world gene perturbation data, we show that CausalRegNet
generates accurate distributions and scales far better than current simulation
frameworks. We illustrate the use of CausalRegNet in assessing CSL methods in
the context of interventional experiments in biology.","['Luka Kovačević', 'Izzy Newsham', 'Sach Mukherjee', 'John Whittaker']","['stat.ML', 'cs.LG', 'stat.AP']",2024-07-08 15:06:03+00:00
http://arxiv.org/abs/2407.05895v1,Link Representation Learning for Probabilistic Travel Time Estimation,"Travel time estimation is a crucial application in navigation apps and web
mapping services. Current deterministic and probabilistic methods primarily
focus on modeling individual trips, assuming independence among trips. However,
in real-world scenarios, we often observe strong inter-trip correlations due to
factors such as weather conditions, traffic management, and road works. In this
paper, we propose to model trip-level link travel time using a Gaussian
hierarchical model, which can characterize both inter-trip and intra-trip
correlations. The joint distribution of travel time of multiple trips becomes a
multivariate Gaussian parameterized by learnable link representations. To
effectively use the sparse GPS trajectories, we also propose a data
augmentation method based on trip sub-sampling, which allows for fine-grained
gradient backpropagation in learning link representations. During inference, we
estimate the probability distribution of the travel time of a queried trip
conditional on the completed trips that are spatiotemporally adjacent. We refer
to the overall framework as ProbTTE. We evaluate ProbTTE on two real-world GPS
trajectory datasets, and the results demonstrate its superior performance
compared to state-of-the-art deterministic and probabilistic baselines.
Additionally, we find that the learned link representations align well with the
physical geometry of the network, making them suitable as input for other
applications.","['Chen Xu', 'Qiang Wang', 'Lijun Sun']","['cs.LG', 'stat.ML']",2024-07-08 13:01:53+00:00
http://arxiv.org/abs/2407.05790v2,Kinetic Interacting Particle Langevin Monte Carlo,"This paper introduces and analyses interacting underdamped Langevin
algorithms, termed Kinetic Interacting Particle Langevin Monte Carlo (KIPLMC)
methods, for statistical inference in latent variable models. We propose a
diffusion process that evolves jointly in the space of parameters and latent
variables and exploit the fact that the stationary distribution of this
diffusion concentrates around the maximum marginal likelihood estimate of the
parameters. We then provide two explicit discretisations of this diffusion as
practical algorithms to estimate parameters of statistical models. For each
algorithm, we obtain nonasymptotic rates of convergence for the case where the
joint log-likelihood is strongly concave with respect to latent variables and
parameters. In particular, we provide convergence analysis for the diffusion
together with the discretisation error, providing convergence rate estimates
for the algorithms in Wasserstein-2 distance. We achieve accelerated
convergence rates clearly demonstrating improvement in dimension dependence,
similar to the underdamped samplers. To demonstrate the utility of the
introduced methodology, we provide numerical experiments that demonstrate the
effectiveness of the proposed diffusion for statistical inference and the
stability of the numerical integrators utilised for discretisation. Our setting
covers a broad number of applications, including unsupervised learning,
statistical inference, and inverse problems.","['Paul Felix Valsecchi Oliva', 'O. Deniz Akyildiz']","['stat.CO', 'stat.ML']",2024-07-08 09:52:46+00:00
http://arxiv.org/abs/2407.05760v1,Dirichlet process mixture model based on topologically augmented signal representation for clustering infant vocalizations,"Based on audio recordings made once a month during the first 12 months of a
child's life, we propose a new method for clustering this set of vocalizations.
We use a topologically augmented representation of the vocalizations, employing
two persistence diagrams for each vocalization: one computed on the surface of
its spectrogram and one on the Takens' embeddings of the vocalization. A
synthetic persistent variable is derived for each diagram and added to the
MFCCs (Mel-frequency cepstral coefficients). Using this representation, we fit
a non-parametric Bayesian mixture model with a Dirichlet process prior to model
the number of components. This procedure leads to a novel data-driven
categorization of vocal productions. Our findings reveal the presence of 8
clusters of vocalizations, allowing us to compare their temporal distribution
and acoustic profiles in the first 12 months of life.","['Guillem Bonafos', 'Clara Bourot', 'Pierre Pudlo', 'Jean-Marc Freyermuth', 'Laurence Reboul', 'Samuel Tronçon', 'Arnaud Rey']","['stat.AP', 'cs.SD', 'eess.AS', 'stat.ML']",2024-07-08 09:12:52+00:00
http://arxiv.org/abs/2407.05669v1,Fractional Budget Allocation for Influence Maximization under General Marketing Strategies,"We consider the fractional influence maximization problem, i.e., identifying
users on a social network to be incentivized with potentially partial discounts
to maximize the influence on the network. The larger the discount given to a
user, the higher the likelihood of its activation (adopting a new product or
innovation), who then attempts to activate its neighboring users, causing a
cascade effect of influence through the network. Our goal is to devise
efficient algorithms that assign initial discounts to the network's users to
maximize the total number of activated users at the end of the cascade, subject
to a constraint on the total sum of discounts given. In general, the activation
likelihood could be any non-decreasing function of the discount, whereas, our
focus lies on the case when the activation likelihood is an affine function of
the discount, potentially varying across different users. As this problem is
shown to be NP-hard, we propose and analyze an efficient (1-1/e)-approximation
algorithm. Furthermore, we run experiments on real-world social networks to
show the performance and scalability of our method.","['Akhil Bhimaraju', 'Eliot W. Robson', 'Lav R. Varshney', 'Abhishek K. Umrawal']","['cs.SI', 'cs.AI', 'cs.DS', 'stat.ML', '05C85, 60J60, 68R05, 68R10, 68T01, 90C27, 90C35', 'F.2.2; G.1.2; G.1.6; G.2.1; G.2.2; G.3; I.2.0; J.4']",2024-07-08 07:09:11+00:00
http://arxiv.org/abs/2407.05664v1,How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning,"We show that deep neural networks (DNNs) can efficiently learn any
composition of functions with bounded $F_{1}$-norm, which allows DNNs to break
the curse of dimensionality in ways that shallow networks cannot. More
specifically, we derive a generalization bound that combines a covering number
argument for compositionality, and the $F_{1}$-norm (or the related Barron
norm) for large width adaptivity. We show that the global minimizer of the
regularized loss of DNNs can fit for example the composition of two functions
$f^{*}=h\circ g$ from a small number of observations, assuming $g$ is
smooth/regular and reduces the dimensionality (e.g. $g$ could be the modulo map
of the symmetries of $f^{*}$), so that $h$ can be learned in spite of its low
regularity. The measures of regularity we consider is the Sobolev norm with
different levels of differentiability, which is well adapted to the $F_{1}$
norm. We compute scaling laws empirically and observe phase transitions
depending on whether $g$ or $h$ is harder to learn, as predicted by our theory.","['Arthur Jacot', 'Seok Hoan Choi', 'Yuxiao Wen']","['stat.ML', 'cs.AI', 'cs.LG']",2024-07-08 06:59:29+00:00
http://arxiv.org/abs/2407.15857v1,BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-task Large Language Models,"This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel
method for finetuning multi-task Large Language Models (LLMs). Current
finetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally
well in reducing training parameters and memory usage but face limitations when
applied to multiple similar tasks. Practitioners usually have to choose between
training separate models for each task or a single model for all tasks, both of
which come with trade-offs in specialization and data utilization.
  BoRA addresses these trade-offs by leveraging a Bayesian hierarchical model
that allows tasks to share information through global hierarchical priors. This
enables tasks with limited data to benefit from the overall structure derived
from related tasks while allowing tasks with more data to specialize. Our
experimental results show that BoRA outperforms both individual and unified
model approaches, achieving lower perplexity and better generalization across
tasks. This method provides a scalable and efficient solution for multi-task
LLM finetuning, with significant practical implications for diverse
applications.","['Simen Eide', 'Arnoldo Frigessi']","['cs.LG', 'cs.CL', 'stat.ML']",2024-07-08 06:38:50+00:00
http://arxiv.org/abs/2407.05593v3,Unmasking Trees for Tabular Data,"Despite much work on advanced deep learning and generative modeling
techniques for tabular data generation and imputation, traditional methods have
continued to win on imputation benchmarks. We herein present UnmaskingTrees, a
simple method for tabular imputation (and generation) employing
gradient-boosted decision trees which are used to incrementally unmask
individual features. This approach offers state-of-the-art performance on
imputation, and on generation given training data with missingness; and it has
competitive performance on vanilla generation. To solve the conditional
generation subproblem, we propose a tabular probabilistic prediction method,
BaltoBot, which fits a balanced tree of boosted tree classifiers. Unlike older
methods, it requires no parametric assumption on the conditional distribution,
accommodating features with multimodal distributions; unlike newer diffusion
methods, it offers fast sampling, closed-form density estimation, and flexible
handling of discrete variables. We finally consider our two approaches as
meta-algorithms, demonstrating in-context learning-based generative modeling
with TabPFN.",['Calvin McCarter'],"['cs.LG', 'stat.ML']",2024-07-08 04:15:43+00:00
http://arxiv.org/abs/2407.05564v1,A Re-solving Heuristic for Dynamic Assortment Optimization with Knapsack Constraints,"In this paper, we consider a multi-stage dynamic assortment optimization
problem with multi-nomial choice modeling (MNL) under resource knapsack
constraints. Given the current resource inventory levels, the retailer makes an
assortment decision at each period, and the goal of the retailer is to maximize
the total profit from purchases. With the exact optimal dynamic assortment
solution being computationally intractable, a practical strategy is to adopt
the re-solving technique that periodically re-optimizes deterministic linear
programs (LP) arising from fluid approximation. However, the fractional
structure of MNL makes the fluid approximation in assortment optimization
highly non-linear, which brings new technical challenges. To address this
challenge, we propose a new epoch-based re-solving algorithm that effectively
transforms the denominator of the objective into the constraint. Theoretically,
we prove that the regret (i.e., the gap between the resolving policy and the
optimal objective of the fluid approximation) scales logarithmically with the
length of time horizon and resource capacities.","['Xi Chen', 'Mo Liu', 'Yining Wang', 'Yuan Zhou']","['math.OC', 'stat.ML']",2024-07-08 02:40:20+00:00
http://arxiv.org/abs/2407.05526v1,Can Machines Learn the True Probabilities?,"When there exists uncertainty, AI machines are designed to make decisions so
as to reach the best expected outcomes. Expectations are based on true facts
about the objective environment the machines interact with, and those facts can
be encoded into AI models in the form of true objective probability functions.
Accordingly, AI models involve probabilistic machine learning in which the
probabilities should be objectively interpreted. We prove under some basic
assumptions when machines can learn the true objective probabilities, if any,
and when machines cannot learn them.",['Jinsook Kim'],"['cs.LG', 'cs.AI', 'stat.ML']",2024-07-08 00:19:43+00:00
http://arxiv.org/abs/2407.05520v1,A Theory of Machine Learning,"We critically review three major theories of machine learning and provide a
new theory according to which machines learn a function when the machines
successfully compute it. We show that this theory challenges common assumptions
in the statistical and the computational learning theories, for it implies that
learning true probabilities is equivalent neither to obtaining a correct
calculation of the true probabilities nor to obtaining an almost-sure
convergence to them. We also briefly discuss some case studies from natural
language processing and macroeconomics from the perspective of the new theory.","['Jinsook Kim', 'Jinho Kang']","['cs.LG', 'stat.ML']",2024-07-07 23:57:10+00:00
http://arxiv.org/abs/2407.05398v1,A Fair Post-Processing Method based on the MADD Metric for Predictive Student Models,"Predictive student models are increasingly used in learning environments.
However, due to the rising social impact of their usage, it is now all the more
important for these models to be both sufficiently accurate and fair in their
predictions. To evaluate algorithmic fairness, a new metric has been developed
in education, namely the Model Absolute Density Distance (MADD). This metric
enables us to measure how different a predictive model behaves regarding two
groups of students, in order to quantify its algorithmic unfairness. In this
paper, we thus develop a post-processing method based on this metric, that aims
at improving the fairness while preserving the accuracy of relevant predictive
models' results. We experiment with our approach on the task of predicting
student success in an online course, using both simulated and real-world
educational data, and obtain successful results. Our source code and data are
in open access at https://github.com/melinaverger/MADD .","['Mélina Verger', 'Chunyang Fan', 'Sébastien Lallé', 'François Bouchet', 'Vanda Luengo']","['cs.CY', 'cs.AI', 'cs.DM', 'cs.LG', 'stat.ML']",2024-07-07 14:53:41+00:00
http://arxiv.org/abs/2407.05385v1,Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis,"Combining the predictions of multiple trained models through ensembling is
generally a good way to improve accuracy by leveraging the different learned
features of the models, however it comes with high computational and storage
costs. Model fusion, the act of merging multiple models into one by combining
their parameters reduces these costs but doesn't work as well in practice.
Indeed, neural network loss landscapes are high-dimensional and non-convex and
the minima found through learning are typically separated by high loss
barriers. Numerous recent works have been focused on finding permutations
matching one network features to the features of a second one, lowering the
loss barrier on the linear path between them in parameter space. However,
permutations are restrictive since they assume a one-to-one mapping between the
different models' neurons exists. We propose a new model merging algorithm, CCA
Merge, which is based on Canonical Correlation Analysis and aims to maximize
the correlations between linear combinations of the model features. We show
that our alignment method leads to better performances than past methods when
averaging models trained on the same, or differing data splits. We also extend
this analysis into the harder setting where more than 2 models are merged, and
we find that CCA Merge works significantly better than past methods. Our code
is publicly available at https://github.com/shoroi/align-n-merge","['Stefan Horoi', 'Albert Manuel Orozco Camacho', 'Eugene Belilovsky', 'Guy Wolf']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2024-07-07 14:21:04+00:00
http://arxiv.org/abs/2407.05302v1,Mamba Hawkes Process,"Irregular and asynchronous event sequences are prevalent in many domains,
such as social media, finance, and healthcare. Traditional temporal point
processes (TPPs), like Hawkes processes, often struggle to model mutual
inhibition and nonlinearity effectively. While recent neural network models,
including RNNs and Transformers, address some of these issues, they still face
challenges with long-term dependencies and computational efficiency. In this
paper, we introduce the Mamba Hawkes Process (MHP), which leverages the Mamba
state space architecture to capture long-range dependencies and dynamic event
interactions. Our results show that MHP outperforms existing models across
various datasets. Additionally, we propose the Mamba Hawkes Process Extension
(MHP-E), which combines Mamba and Transformer models to enhance predictive
capabilities. We present the novel application of the Mamba architecture to
Hawkes processes, a flexible and extensible model structure, and a theoretical
analysis of the synergy between state space models and Hawkes processes.
Experimental results demonstrate the superior performance of both MHP and
MHP-E, advancing the field of temporal point process modeling.","['Anningzhe Gao', 'Shan Dai', 'Yan Hu']","['cs.LG', 'stat.ML']",2024-07-07 08:37:43+00:00
http://arxiv.org/abs/2407.05287v1,Model-agnostic meta-learners for estimating heterogeneous treatment effects over time,"Estimating heterogeneous treatment effects (HTEs) over time is crucial in
many disciplines such as personalized medicine. For example, electronic health
records are commonly collected over several time periods and then used to
personalize treatment decisions. Existing works for this task have mostly
focused on model-based learners (i.e., learners that adapt specific
machine-learning models). In contrast, model-agnostic learners -- so-called
meta-learners -- are largely unexplored. In our paper, we propose several
meta-learners that are model-agnostic and thus can be used in combination with
arbitrary machine learning models (e.g., transformers) to estimate HTEs over
time. Here, our focus is on learners that can be obtained via weighted
pseudo-outcome regressions, which allows for efficient estimation by targeting
the treatment effect directly. We then provide a comprehensive theoretical
analysis that characterizes the different learners and that allows us to offer
insights into when specific learners are preferable. Finally, we confirm our
theoretical insights through numerical experiments. In sum, while meta-learners
are already state-of-the-art for the static setting, we are the first to
propose a comprehensive set of meta-learners for estimating HTEs in the
time-varying setting.","['Dennis Frauen', 'Konstantin Hess', 'Stefan Feuerriegel']","['cs.LG', 'stat.ML']",2024-07-07 07:07:48+00:00
http://arxiv.org/abs/2407.05261v1,Disciplined Geodesically Convex Programming,"Convex programming plays a fundamental role in machine learning, data
science, and engineering. Testing convexity structure in nonlinear programs
relies on verifying the convexity of objectives and constraints.
\citet{grant2006disciplined} introduced a framework, Disciplined Convex
Programming (DCP), for automating this verification task for a wide range of
convex functions that can be decomposed into basic convex functions (atoms)
using convexity-preserving compositions and transformations (rules). However,
the restriction to Euclidean convexity concepts can limit the applicability of
the framework. For instance, many notable instances of statistical estimators
and matrix-valued (sub)routines in machine learning applications are Euclidean
non-convex, but exhibit geodesic convexity through a more general Riemannian
lens. In this work, we extend disciplined programming to this setting by
introducing Disciplined Geodesically Convex Programming (DGCP). We determine
convexity-preserving compositions and transformations for geodesically convex
functions on general Cartan-Hadamard manifolds, as well as for the special case
of symmetric positive definite matrices, a common setting in matrix-valued
optimization. For the latter, we also define a basic set of atoms. Our paper is
accompanied by a Julia package SymbolicAnalysis.jl, which provides
functionality for testing and certifying DGCP-compliant expressions. Our
library interfaces with manifold optimization software, which allows for
directly solving verified geodesically convex programs.","['Andrew Cheng', 'Vaibhav Dixit', 'Melanie Weber']","['math.OC', 'cs.LG', 'cs.MS', 'stat.ML']",2024-07-07 05:13:51+00:00
http://arxiv.org/abs/2407.05237v2,Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex composite losses,"Differentially-private stochastic gradient descent (DP-SGD) is a family of
iterative machine learning training algorithms that privatize gradients to
generate a sequence of differentially-private (DP) model parameters. It is also
the standard tool used to train DP models in practice, even though most users
are only interested in protecting the privacy of the final model. Tight DP
accounting for the last iterate would minimize the amount of noise required
while maintaining the same privacy guarantee and potentially increasing model
utility. However, last-iterate accounting is challenging, and existing works
require strong assumptions not satisfied by most implementations. These include
assuming (i) the global sensitivity constant is known - to avoid gradient
clipping; (ii) the loss function is Lipschitz or convex; and (iii) input
batches are sampled randomly.
  In this work, we forego any unrealistic assumptions and provide privacy
bounds for the most commonly used variant of DP-SGD, in which data is traversed
cyclically, gradients are clipped, and only the last model is released. More
specifically, we establish new Renyi differential privacy (RDP) upper bounds
for the last iterate under realistic assumptions of small stepsize and
Lipschitz smoothness of the loss function. Our general bounds also recover the
special-case convex bounds when the weak-convexity parameter of the objective
function approaches zero and no clipping is performed. The approach itself
leverages optimal transport techniques for last iterate bounds, which is a
nontrivial task when the data is traversed cyclically and the loss function is
nonconvex.","['Weiwei Kong', 'Mónica Ribero']","['cs.LG', 'cs.CR', 'cs.DS', 'math.OC', 'stat.ML', '65K10 (Primary), 60G15, 68P27', 'G.3; G.1.6']",2024-07-07 02:35:55+00:00
http://arxiv.org/abs/2407.05145v3,On high-dimensional modifications of the nearest neighbor classifier,"Nearest neighbor classifier is arguably the most simple and popular
nonparametric classifier available in the literature. However, due to the
concentration of pairwise distances and the violation of the neighborhood
structure, this classifier often suffers in high-dimension, low-sample size
(HDLSS) situations, especially when the scale difference between the competing
classes dominates their location difference. Several attempts have been made in
the literature to take care of this problem. In this article, we discuss some
of these existing methods and propose some new ones. We carry out some
theoretical investigations in this regard and analyze several simulated and
benchmark datasets to compare the empirical performances of proposed methods
with some of the existing ones.","['Annesha Ghosh', 'Deep Ghoshal', 'Bilol Banerjee', 'Anil K. Ghosh']","['stat.ML', 'cs.LG']",2024-07-06 17:53:53+00:00
http://arxiv.org/abs/2407.05108v1,"The Role of Depth, Width, and Tree Size in Expressiveness of Deep Forest","Random forests are classical ensemble algorithms that construct multiple
randomized decision trees and aggregate their predictions using naive
averaging. \citet{zhou2019deep} further propose a deep forest algorithm with
multi-layer forests, which outperforms random forests in various tasks. The
performance of deep forests is related to three hyperparameters in practice:
depth, width, and tree size, but little has been known about its theoretical
explanation. This work provides the first upper and lower bounds on the
approximation complexity of deep forests concerning the three hyperparameters.
Our results confirm the distinctive role of depth, which can exponentially
enhance the expressiveness of deep forests compared with width and tree size.
Experiments confirm the theoretical findings.","['Shen-Huan Lyu', 'Jin-Hui Wu', 'Qin-Cheng Zheng', 'Baoliu Ye']","['cs.LG', 'stat.ML']",2024-07-06 15:32:54+00:00
http://arxiv.org/abs/2407.05082v1,DMTG: One-Shot Differentiable Multi-Task Grouping,"We aim to address Multi-Task Learning (MTL) with a large number of tasks by
Multi-Task Grouping (MTG). Given N tasks, we propose to simultaneously identify
the best task groups from 2^N candidates and train the model weights
simultaneously in one-shot, with the high-order task-affinity fully exploited.
This is distinct from the pioneering methods which sequentially identify the
groups and train the model weights, where the group identification often relies
on heuristics. As a result, our method not only improves the training
efficiency, but also mitigates the objective bias introduced by the sequential
procedures that potentially lead to a suboptimal solution. Specifically, we
formulate MTG as a fully differentiable pruning problem on an adaptive network
architecture determined by an underlying Categorical distribution. To
categorize N tasks into K groups (represented by K encoder branches), we
initially set up KN task heads, where each branch connects to all N task heads
to exploit the high-order task-affinity. Then, we gradually prune the KN heads
down to N by learning a relaxed differentiable Categorical distribution,
ensuring that each task is exclusively and uniquely categorized into only one
branch. Extensive experiments on CelebA and Taskonomy datasets with detailed
ablations show the promising performance and efficiency of our method. The
codes are available at https://github.com/ethanygao/DMTG.","['Yuan Gao', 'Shuguo Jiang', 'Moran Li', 'Jin-Gang Yu', 'Gui-Song Xia']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2024-07-06 13:54:00+00:00
http://arxiv.org/abs/2407.05050v1,Sparse identification of quasipotentials via a combined data-driven method,"The quasipotential function allows for comprehension and prediction of the
escape mechanisms from metastable states in nonlinear dynamical systems. This
function acts as a natural extension of the potential function for non-gradient
systems and it unveils important properties such as the maximum likelihood
transition paths, transition rates and expected exit times of the system. Here,
we leverage on machine learning via the combination of two data-driven
techniques, namely a neural network and a sparse regression algorithm, to
obtain symbolic expressions of quasipotential functions. The key idea is first
to determine an orthogonal decomposition of the vector field that governs the
underlying dynamics using neural networks, then to interpret symbolically the
downhill and circulatory components of the decomposition. These functions are
regressed simultaneously with the addition of mathematical constraints. We show
that our approach discovers a parsimonious quasipotential equation for an
archetypal model with a known exact quasipotential and for the dynamics of a
nanomechanical resonator. The analytical forms deliver direct access to the
stability of the metastable states and predict rare events with significant
computational advantages. Our data-driven approach is of interest for a wide
range of applications in which to assess the fluctuating dynamics.","['Bo Lin', 'Pierpaolo Belardinelli']","['math.DS', 'nlin.CD', 'physics.comp-ph', 'stat.ML']",2024-07-06 11:27:52+00:00
http://arxiv.org/abs/2407.04970v1,Idiographic Personality Gaussian Process for Psychological Assessment,"We develop a novel measurement framework based on a Gaussian process
coregionalization model to address a long-lasting debate in psychometrics:
whether psychological features like personality share a common structure across
the population, vary uniquely for individuals, or some combination. We propose
the idiographic personality Gaussian process (IPGP) framework, an intermediate
model that accommodates both shared trait structure across a population and
""idiographic"" deviations for individuals. IPGP leverages the Gaussian process
coregionalization model to handle the grouped nature of battery responses, but
adjusted to non-Gaussian ordinal data. We further exploit stochastic
variational inference for efficient latent factor estimation required for
idiographic modeling at scale. Using synthetic and real data, we show that IPGP
improves both prediction of actual responses and estimation of individualized
factor structures relative to existing benchmarks. In a third study, we show
that IPGP also identifies unique clusters of personality taxonomies in
real-world data, displaying great potential in advancing individualized
approaches to psychological diagnosis and treatment.","['Yehu Chen', 'Muchen Xi', 'Jacob Montgomery', 'Joshua Jackson', 'Roman Garnett']","['cs.LG', 'stat.ML']",2024-07-06 06:09:04+00:00
http://arxiv.org/abs/2407.04860v1,Kullback-Leibler Barycentre of Stochastic Processes,"We consider the problem where an agent aims to combine the views and insights
of different experts' models. Specifically, each expert proposes a diffusion
process over a finite time horizon. The agent then combines the experts' models
by minimising the weighted Kullback-Leibler divergence to each of the experts'
models. We show existence and uniqueness of the barycentre model and proof an
explicit representation of the Radon-Nikodym derivative relative to the average
drift model. We further allow the agent to include their own constraints, which
results in an optimal model that can be seen as a distortion of the experts'
barycentre model to incorporate the agent's constraints.
  Two deep learning algorithms are proposed to find the optimal drift of the
combined model, allowing for efficient simulations. The first algorithm aims at
learning the optimal drift by matching the change of measure, whereas the
second algorithm leverages the notion of elicitability to directly estimate the
value function. The paper concludes with a extended application to combine
implied volatility smiles models that were estimated on different datasets.","['Sebastian Jaimungal', 'Silvana M. Pesenti']","['q-fin.MF', 'math.PR', 'q-fin.RM', 'stat.ML']",2024-07-05 20:45:27+00:00
http://arxiv.org/abs/2407.04819v1,"RPN: Reconciled Polynomial Network Towards Unifying PGMs, Kernel SVMs, MLP and KAN","In this paper, we will introduce a novel deep model named Reconciled
Polynomial Network (RPN) for deep function learning. RPN has a very general
architecture and can be used to build models with various complexities,
capacities, and levels of completeness, which all contribute to the correctness
of these models. As indicated in the subtitle, RPN can also serve as the
backbone to unify different base models into one canonical representation. This
includes non-deep models, like probabilistic graphical models (PGMs) - such as
Bayesian network and Markov network - and kernel support vector machines
(kernel SVMs), as well as deep models like the classic multi-layer perceptron
(MLP) and the recent Kolmogorov-Arnold network (KAN).
  Technically, RPN proposes to disentangle the underlying function to be
inferred into the inner product of a data expansion function and a parameter
reconciliation function. Together with the remainder function, RPN accurately
approximates the underlying functions that governs data distributions. The data
expansion functions in RPN project data vectors from the input space to a
high-dimensional intermediate space, specified by the expansion functions in
definition. Meanwhile, RPN also introduces the parameter reconciliation
functions to fabricate a small number of parameters into a higher-order
parameter matrix to address the ``curse of dimensionality'' problem caused by
the data expansions. Moreover, the remainder functions provide RPN with
additional complementary information to reduce potential approximation errors.
We conducted extensive empirical experiments on numerous benchmark datasets
across multiple modalities, including continuous function datasets, discrete
vision and language datasets, and classic tabular datasets, to investigate the
effectiveness of RPN.",['Jiawei Zhang'],"['cs.LG', 'cs.AI', 'cs.CV', 'cs.IT', 'math.IT', 'stat.ML']",2024-07-05 19:00:18+00:00
http://arxiv.org/abs/2407.04806v2,Training Guarantees of Neural Network Classification Two-Sample Tests by Kernel Analysis,"We construct and analyze a neural network two-sample test to determine
whether two datasets came from the same distribution (null hypothesis) or not
(alternative hypothesis). We perform time-analysis on a neural tangent kernel
(NTK) two-sample test. In particular, we derive the theoretical minimum
training time needed to ensure the NTK two-sample test detects a
deviation-level between the datasets. Similarly, we derive the theoretical
maximum training time before the NTK two-sample test detects a deviation-level.
By approximating the neural network dynamics with the NTK dynamics, we extend
this time-analysis to the realistic neural network two-sample test generated
from time-varying training dynamics and finite training samples. A similar
extension is done for the neural network two-sample test generated from
time-varying training dynamics but trained on the population. To give
statistical guarantees, we show that the statistical power associated with the
neural network two-sample test goes to 1 as the neural network training samples
and test evaluation samples go to infinity. Additionally, we prove that the
training times needed to detect the same deviation-level in the null and
alternative hypothesis scenarios are well-separated. Finally, we run some
experiments showcasing a two-layer neural network two-sample test on a hard
two-sample test problem and plot a heatmap of the statistical power of the
two-sample test in relation to training time and network complexity.","['Varun Khurana', 'Xiuyuan Cheng', 'Alexander Cloninger']","['stat.ML', 'cs.LG']",2024-07-05 18:41:16+00:00
http://arxiv.org/abs/2407.04783v2,Agnostic Private Density Estimation for GMMs via List Global Stability,"We consider the problem of private density estimation for mixtures of
unrestricted high dimensional Gaussians in the agnostic setting. We prove the
first upper bound on the sample complexity of this problem. Previously, private
learnability of high dimensional GMMs was only known in the realizable setting
[Afzali et al., 2024].
  To prove our result, we exploit the notion of $\textit{list global
stability}$ [Ghazi et al., 2021b,a] that was originally introduced in the
context of private supervised learning. We define an agnostic variant of this
definition, showing that its existence is sufficient for agnostic private
density estimation. We then construct an agnostic list globally stable learner
for GMMs.","['Mohammad Afzali', 'Hassan Ashtiani', 'Christopher Liaw']","['stat.ML', 'cs.CR', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']",2024-07-05 18:00:22+00:00
http://arxiv.org/abs/2407.04605v1,Linear causal disentanglement via higher-order cumulants,"Linear causal disentanglement is a recent method in causal representation
learning to describe a collection of observed variables via latent variables
with causal dependencies between them. It can be viewed as a generalization of
both independent component analysis and linear structural equation models. We
study the identifiability of linear causal disentanglement, assuming access to
data under multiple contexts, each given by an intervention on a latent
variable. We show that one perfect intervention on each latent variable is
sufficient and in the worst case necessary to recover parameters under perfect
interventions, generalizing previous work to allow more latent than observed
variables. We give a constructive proof that computes parameters via a coupled
tensor decomposition. For soft interventions, we find the equivalence class of
latent graphs and parameters that are consistent with observed data, via the
study of a system of polynomial equations. Our results hold assuming the
existence of non-zero higher-order cumulants, which implies non-Gaussianity of
variables.","['Paula Leyes Carreno', 'Chiara Meroni', 'Anna Seigal']","['stat.ML', 'cs.LG', 'math.AG', 'math.CO', 'math.ST', 'stat.TH', '13P15, 15A69, 62H22, 62R01, 68Q32']",2024-07-05 15:53:16+00:00
http://arxiv.org/abs/2407.04600v1,Understanding the Gains from Repeated Self-Distillation,"Self-Distillation is a special type of knowledge distillation where the
student model has the same architecture as the teacher model. Despite using the
same architecture and the same training data, self-distillation has been
empirically observed to improve performance, especially when applied
repeatedly. For such a process, there is a fundamental question of interest:
How much gain is possible by applying multiple steps of self-distillation? To
investigate this relative gain, we propose studying the simple but canonical
task of linear regression. Our analysis shows that the excess risk achieved by
multi-step self-distillation can significantly improve upon a single step of
self-distillation, reducing the excess risk by a factor as large as $d$, where
$d$ is the input dimension. Empirical results on regression tasks from the UCI
repository show a reduction in the learnt model's risk (MSE) by up to 47%.","['Divyansh Pareek', 'Simon S. Du', 'Sewoong Oh']","['cs.LG', 'stat.ML']",2024-07-05 15:48:34+00:00
http://arxiv.org/abs/2407.04495v3,Speed-accuracy trade-off for the diffusion models: Wisdom from nonequilibrium thermodynamics and optimal transport,"We discuss a connection between a generative model, called the diffusion
model, and nonequilibrium thermodynamics for the Fokker-Planck equation, called
stochastic thermodynamics. Based on the techniques of stochastic
thermodynamics, we derive the speed-accuracy trade-off for the diffusion
models, which is a trade-off relationship between the speed and accuracy of
data generation in diffusion models. Our result implies that the entropy
production rate in the forward process affects the errors in data generation.
From a stochastic thermodynamic perspective, our results provide quantitative
insight into how best to generate data in diffusion models. The optimal
learning protocol is introduced by the conservative force in stochastic
thermodynamics and the geodesic of space by the 2-Wasserstein distance in
optimal transport theory. We numerically illustrate the validity of the
speed-accuracy trade-off for the diffusion models with different noise
schedules such as the cosine schedule, the conditional optimal transport, and
the optimal transport.","['Kotaro Ikeda', 'Tomoya Uda', 'Daisuke Okanohara', 'Sosuke Ito']","['cond-mat.stat-mech', 'cs.LG', 'stat.ML']",2024-07-05 13:35:14+00:00
http://arxiv.org/abs/2407.04248v1,Machine Learning for Complex Systems with Abnormal Pattern by Exception Maximization Outlier Detection Method,"This paper proposes a novel fast online methodology for outlier detection
called the exception maximization outlier detection method(EMODM), which
employs probabilistic models and statistical algorithms to detect abnormal
patterns from the outputs of complex systems. The EMODM is based on a two-state
Gaussian mixture model and demonstrates strong performance in probability
anomaly detection working on real-time raw data rather than using special prior
distribution information. We confirm this using the synthetic data from two
numerical cases. For the real-world data, we have detected the short circuit
pattern of the circuit system using EMODM by the current and voltage output of
a three-phase inverter. The EMODM also found an abnormal period due to COVID-19
in the insured unemployment data of 53 regions in the United States from 2000
to 2024. The application of EMODM to these two real-life datasets demonstrated
the effectiveness and accuracy of our algorithm.","['Zhikun Zhang', 'Yiting Duan', 'Xiangjun Wang', 'Mingyuan Zhang']","['stat.ML', 'cs.LG']",2024-07-05 04:30:41+00:00
http://arxiv.org/abs/2407.11035v1,Optimal estimators of cross-partial derivatives and surrogates of functions,"Computing cross-partial derivatives using fewer model runs is relevant in
modeling, such as stochastic approximation, derivative-based ANOVA, exploring
complex models, and active subspaces. This paper introduces surrogates of all
the cross-partial derivatives of functions by evaluating such functions at $N$
randomized points and using a set of $L$ constraints. Randomized points rely on
independent, central, and symmetric variables. The associated estimators, based
on $NL$ model runs, reach the optimal rates of convergence (i.e.,
$\mathcal{O}(N^{-1})$), and the biases of our approximations do not suffer from
the curse of dimensionality for a wide class of functions. Such results are
used for i) computing the main and upper-bounds of sensitivity indices, and ii)
deriving emulators of simulators or surrogates of functions thanks to the
derivative-based ANOVA. Simulations are presented to show the accuracy of our
emulators and estimators of sensitivity indices. The plug-in estimates of
indices using the U-statistics of one sample are numerically much stable.",['Matieyendou Lamboni'],"['stat.ME', 'math.OC', 'math.PR', 'math.ST', 'stat.CO', 'stat.ML', 'stat.TH']",2024-07-05 03:39:06+00:00
http://arxiv.org/abs/2407.04173v1,Quantifying Prediction Consistency Under Model Multiplicity in Tabular LLMs,"Fine-tuning large language models (LLMs) on limited tabular data for
classification tasks can lead to \textit{fine-tuning multiplicity}, where
equally well-performing models make conflicting predictions on the same inputs
due to variations in the training process (i.e., seed, random weight
initialization, retraining on additional or deleted samples). This raises
critical concerns about the robustness and reliability of Tabular LLMs,
particularly when deployed for high-stakes decision-making, such as finance,
hiring, education, healthcare, etc. This work formalizes the challenge of
fine-tuning multiplicity in Tabular LLMs and proposes a novel metric to
quantify the robustness of individual predictions without expensive model
retraining. Our metric quantifies a prediction's stability by analyzing
(sampling) the model's local behavior around the input in the embedding space.
Interestingly, we show that sampling in the local neighborhood can be leveraged
to provide probabilistic robustness guarantees against a broad class of
fine-tuned models. By leveraging Bernstein's Inequality, we show that
predictions with sufficiently high robustness (as defined by our measure) will
remain consistent with high probability. We also provide empirical evaluation
on real-world datasets to support our theoretical results. Our work highlights
the importance of addressing fine-tuning instabilities to enable trustworthy
deployment of LLMs in high-stakes and safety-critical applications.","['Faisal Hamman', 'Pasan Dissanayake', 'Saumitra Mishra', 'Freddy Lecue', 'Sanghamitra Dutta']","['cs.LG', 'cs.AI', 'cs.CY', 'stat.ML']",2024-07-04 22:22:09+00:00
http://arxiv.org/abs/2407.04117v2,Predictive Coding Networks and Inference Learning: Tutorial and Survey,"Recent years have witnessed a growing call for renewed emphasis on
neuroscience-inspired approaches in artificial intelligence research, under the
banner of NeuroAI. A prime example of this is predictive coding networks
(PCNs), based on the neuroscientific framework of predictive coding. This
framework views the brain as a hierarchical Bayesian inference model that
minimizes prediction errors through feedback connections. Unlike traditional
neural networks trained with backpropagation (BP), PCNs utilize inference
learning (IL), a more biologically plausible algorithm that explains patterns
of neural activity that BP cannot. Historically, IL has been more
computationally intensive, but recent advancements have demonstrated that it
can achieve higher efficiency than BP with sufficient parallelization.
Furthermore, PCNs can be mathematically considered a superset of traditional
feedforward neural networks (FNNs), significantly extending the range of
trainable architectures. As inherently probabilistic (graphical) latent
variable models, PCNs provide a versatile framework for both supervised
learning and unsupervised (generative) modeling that goes beyond traditional
artificial neural networks. This work provides a comprehensive review and
detailed formal specification of PCNs, particularly situating them within the
context of modern ML methods. Additionally, we introduce a Python library
(PRECO) for practical implementation. This positions PC as a promising
framework for future ML innovations.","['Björn van Zwol', 'Ro Jefferson', 'Egon L. van den Broek']","['cs.LG', 'cond-mat.dis-nn', 'cs.AI', 'cs.NE', 'stat.ML']",2024-07-04 18:39:20+00:00
http://arxiv.org/abs/2407.04104v1,Network-based Neighborhood regression,"Given the ubiquity of modularity in biological systems, module-level
regulation analysis is vital for understanding biological systems across
various levels and their dynamics. Current statistical analysis on biological
modules predominantly focuses on either detecting the functional modules in
biological networks or sub-group regression on the biological features without
using the network data. This paper proposes a novel network-based neighborhood
regression framework whose regression functions depend on both the global
community-level information and local connectivity structures among entities.
An efficient community-wise least square optimization approach is developed to
uncover the strength of regulation among the network modules while enabling
asymptotic inference. With random graph theory, we derive non-asymptotic
estimation error bounds for the proposed estimator, achieving exact minimax
optimality. Unlike the root-n consistency typical in canonical linear
regression, our model exhibits linear consistency in the number of nodes n,
highlighting the advantage of incorporating neighborhood information. The
effectiveness of the proposed framework is further supported by extensive
numerical experiments. Application to whole-exome sequencing and RNA-sequencing
Autism datasets demonstrates the usage of the proposed method in identifying
the association between the gene modules of genetic variations and the gene
modules of genomic differential expressions.","['Yaoming Zhen', 'Jin-Hong Du']","['stat.ME', 'cs.LG', 'stat.ML']",2024-07-04 18:08:40+00:00
http://arxiv.org/abs/2407.12844v1,$\texttt{metabench}$ -- A Sparse Benchmark to Measure General Ability in Large Language Models,"Large Language Models (LLMs) vary in their abilities on a range of tasks.
Initiatives such as the $\texttt{Open LLM Leaderboard}$ aim to quantify these
differences with several large benchmarks (sets of test items to which an LLM
can respond either correctly or incorrectly). However, high correlations within
and between benchmark scores suggest that (1) there exists a small set of
common underlying abilities that these benchmarks measure, and (2) items tap
into redundant information and the benchmarks may thus be considerably
compressed. We use data from $n > 5000$ LLMs to identify the most informative
items of six benchmarks, ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande
(with $d=28,632$ items in total). From them we distill a sparse benchmark,
$\texttt{metabench}$, that has less than $3\%$ of the original size of all six
benchmarks combined. This new sparse benchmark goes beyond point scores by
yielding estimators of the underlying benchmark-specific abilities. We show
that these estimators (1) can be used to reconstruct each original
$\textit{individual}$ benchmark score with, on average, $1.5\%$ root mean
square error (RMSE), (2) reconstruct the original $\textit{total}$ score with
$0.8\%$ RMSE, and (3) have a single underlying common factor whose Spearman
correlation with the total score is $r = 0.93$.","['Alex Kipnis', 'Konstantinos Voudouris', 'Luca M. Schulze Buschoff', 'Eric Schulz']","['cs.CL', 'cs.LG', 'stat.ML']",2024-07-04 17:57:38+00:00
http://arxiv.org/abs/2407.03878v2,Geodesic Optimization for Predictive Shift Adaptation on EEG data,"Electroencephalography (EEG) data is often collected from diverse contexts
involving different populations and EEG devices. This variability can induce
distribution shifts in the data $X$ and in the biomedical variables of interest
$y$, thus limiting the application of supervised machine learning (ML)
algorithms. While domain adaptation (DA) methods have been developed to
mitigate the impact of these shifts, such methods struggle when distribution
shifts occur simultaneously in $X$ and $y$. As state-of-the-art ML models for
EEG represent the data by spatial covariance matrices, which lie on the
Riemannian manifold of Symmetric Positive Definite (SPD) matrices, it is
appealing to study DA techniques operating on the SPD manifold. This paper
proposes a novel method termed Geodesic Optimization for Predictive Shift
Adaptation (GOPSA) to address test-time multi-source DA for situations in which
source domains have distinct $y$ distributions. GOPSA exploits the geodesic
structure of the Riemannian manifold to jointly learn a domain-specific
re-centering operator representing site-specific intercepts and the regression
model. We performed empirical benchmarks on the cross-site generalization of
age-prediction models with resting-state EEG data from a large multi-national
dataset (HarMNqEEG), which included $14$ recording sites and more than $1500$
human participants. Compared to state-of-the-art methods, our results showed
that GOPSA achieved significantly higher performance on three regression
metrics ($R^2$, MAE, and Spearman's $\rho$) for several source-target site
combinations, highlighting its effectiveness in tackling multi-source DA with
predictive shifts in EEG data analysis. Our method has the potential to combine
the advantages of mixed-effects modeling with machine learning for biomedical
applications of EEG, such as multicenter clinical trials.","['Apolline Mellot', 'Antoine Collas', 'Sylvain Chevallier', 'Alexandre Gramfort', 'Denis A. Engemann']","['stat.ML', 'cs.LG', 'eess.SP']",2024-07-04 12:15:42+00:00
http://arxiv.org/abs/2407.03792v1,NeuroSteiner: A Graph Transformer for Wirelength Estimation,"A core objective of physical design is to minimize wirelength (WL) when
placing chip components on a canvas. Computing the minimal WL of a placement
requires finding rectilinear Steiner minimum trees (RSMTs), an NP-hard problem.
We propose NeuroSteiner, a neural model that distills GeoSteiner, an optimal
RSMT solver, to navigate the cost--accuracy frontier of WL estimation.
NeuroSteiner is trained on synthesized nets labeled by GeoSteiner, alleviating
the need to train on real chip designs. Moreover, NeuroSteiner's
differentiability allows to place by minimizing WL through gradient descent. On
ISPD 2005 and 2019, NeuroSteiner can obtain 0.3% WL error while being 60%
faster than GeoSteiner, or 0.2% and 30%.","['Sahil Manchanda', 'Dana Kianfar', 'Markus Peschl', 'Romain Lepert', 'Michaël Defferrard']","['cs.LG', 'stat.ML']",2024-07-04 09:55:22+00:00
http://arxiv.org/abs/2407.06212v1,Bias Correction in Machine Learning-based Classification of Rare Events,"Online platform businesses can be identified by using web-scraped texts. This
is a classification problem that combines elements of natural language
processing and rare event detection. Because online platforms are rare,
accurately identifying them with Machine Learning algorithms is challenging.
Here, we describe the development of a Machine Learning-based text
classification approach that reduces the number of false positives as much as
possible. It greatly reduces the bias in the estimates obtained by using
calibrated probabilities and ensembles.","['Luuk Gubbels', 'Marco Puts', 'Piet Daas']","['cs.LG', 'stat.ML']",2024-07-04 08:02:34+00:00
http://arxiv.org/abs/2407.03690v3,Robust CATE Estimation Using Novel Ensemble Methods,"The estimation of Conditional Average Treatment Effects (CATE) is crucial for
understanding the heterogeneity of treatment effects in clinical trials. We
evaluate the performance of common methods, including causal forests and
various meta-learners, across a diverse set of scenarios, revealing that each
of the methods struggles in one or more of the tested scenarios. Given the
inherent uncertainty of the data-generating process in real-life scenarios, the
robustness of a CATE estimator to various scenarios is critical for its
reliability. To address this limitation of existing methods, we propose two new
ensemble methods that integrate multiple estimators to enhance prediction
stability and performance - Stacked X-Learner which uses the X-Learner with
model stacking for estimating the nuisance functions, and Consensus Based
Averaging (CBA), which averages only the models with highest internal
agreement. We show that these models achieve good performance across a wide
range of scenarios varying in complexity, sample size and structure of the
underlying-mechanism, including a biologically driven model for PD-L1
inhibition pathway for cancer treatment. Furthermore, we demonstrate improved
performance by the Stacked X-Learner also when comparing to other ensemble
methods, including R-Stacking, Causal-Stacking and others.","['Oshri Machluf', 'Tzviel Frostig', 'Gal Shoham', 'Tomer Milo', 'Elad Berkman', 'Raviv Pryluk']","['stat.ME', 'stat.ML']",2024-07-04 07:23:02+00:00
