id,title,abstract,authors,categories,date
http://arxiv.org/abs/2108.12515v3,Convergence Rates for Learning Linear Operators from Noisy Data,"This paper studies the learning of linear operators between
infinite-dimensional Hilbert spaces. The training data comprises pairs of
random input vectors in a Hilbert space and their noisy images under an unknown
self-adjoint linear operator. Assuming that the operator is diagonalizable in a
known basis, this work solves the equivalent inverse problem of estimating the
operator's eigenvalues given the data. Adopting a Bayesian approach, the
theoretical analysis establishes posterior contraction rates in the infinite
data limit with Gaussian priors that are not directly linked to the forward map
of the inverse problem. The main results also include learning-theoretic
generalization error guarantees for a wide range of distribution shifts. These
convergence rates quantify the effects of data smoothness and true eigenvalue
decay or growth, for compact or unbounded operators, respectively, on sample
complexity. Numerical evidence supports the theory in diagonal and non-diagonal
settings.","['Maarten V. de Hoop', 'Nikola B. Kovachki', 'Nicholas H. Nelsen', 'Andrew M. Stuart']","['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH', '62G20, 62C10, 68T05, 47A62']",2021-08-27 22:09:53+00:00
http://arxiv.org/abs/2108.12461v2,Approximate Bayesian Optimisation for Neural Networks,"A body of work has been done to automate machine learning algorithm to
highlight the importance of model choice. Automating the process of choosing
the best forecasting model and its corresponding parameters can result to
improve a wide range of real-world applications. Bayesian optimisation (BO)
uses a blackbox optimisation methods to propose solutions according to an
exploration-exploitation trade-off criterion through acquisition functions. BO
framework imposes two key ingredients: a probabilistic surrogate model that
consist of prior belief of the unknown objective function(data-dependant) and
an objective function that describes how optimal is the model-fit. Choosing the
best model and its associated hyperparameters can be very expensive, and is
typically fit using Gaussian processes (GPs) and at some extends applying
approximate inference due its intractability. However, since GPs scale
cubically with the number of observations, it has been challenging to handle
objectives whose optimization requires many evaluations. In addition, most
real-dataset are non-stationary which make idealistic assumptions on surrogate
models. The necessity to solve the analytical tractability and the
computational feasibility in a stochastic fashion enables to ensure the
efficiency and the applicability of Bayesian optimisation. In this paper we
explore the use of neural networks as an alternative to GPs to model
distributions over functions, we provide a link between density-ratio
estimation and class probability estimation based on approximate inference,
this reformulation provides algorithm efficiency and tractability.","['Nadhir Hassen', 'Irina Rish']","['cs.LG', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",2021-08-27 19:03:32+00:00
http://arxiv.org/abs/2108.12445v2,Multimodal Data Fusion in High-Dimensional Heterogeneous Datasets via Generative Models,"The commonly used latent space embedding techniques, such as Principal
Component Analysis, Factor Analysis, and manifold learning techniques, are
typically used for learning effective representations of homogeneous data.
However, they do not readily extend to heterogeneous data that are a
combination of numerical and categorical variables, e.g., arising from linked
GPS and text data. In this paper, we are interested in learning probabilistic
generative models from high-dimensional heterogeneous data in an unsupervised
fashion. The learned generative model provides latent unified representations
that capture the factors common to the multiple dimensions of the data, and
thus enable fusing multimodal data for various machine learning tasks.
Following a Bayesian approach, we propose a general framework that combines
disparate data types through the natural parameterization of the exponential
family of distributions. To scale the model inference to millions of instances
with thousands of features, we use the Laplace-Bernstein approximation for
posterior computations involving nonlinear link functions. The proposed
algorithm is presented in detail for the commonly encountered heterogeneous
datasets with real-valued (Gaussian) and categorical (multinomial) features.
Experiments on two high-dimensional and heterogeneous datasets (NYC Taxi and
MovieLens-10M) demonstrate the scalability and competitive performance of the
proposed algorithm on different machine learning tasks such as anomaly
detection, data imputation, and recommender systems.","['Yasin Yilmaz', 'Mehmet Aktukmak', 'Alfred O. Hero']","['cs.LG', 'stat.ML']",2021-08-27 18:10:31+00:00
http://arxiv.org/abs/2108.12250v2,A comparison of approaches to improve worst-case predictive model performance over patient subpopulations,"Predictive models for clinical outcomes that are accurate on average in a
patient population may underperform drastically for some subpopulations,
potentially introducing or reinforcing inequities in care access and quality.
Model training approaches that aim to maximize worst-case model performance
across subpopulations, such as distributionally robust optimization (DRO),
attempt to address this problem without introducing additional harms. We
conduct a large-scale empirical study of DRO and several variations of standard
learning procedures to identify approaches for model development and selection
that consistently improve disaggregated and worst-case performance over
subpopulations compared to standard approaches for learning predictive models
from electronic health records data. In the course of our evaluation, we
introduce an extension to DRO approaches that allows for specification of the
metric used to assess worst-case performance. We conduct the analysis for
models that predict in-hospital mortality, prolonged length of stay, and 30-day
readmission for inpatient admissions, and predict in-hospital mortality using
intensive care data. We find that, with relatively few exceptions, no approach
performs better, for each patient subpopulation examined, than standard
learning procedures using the entire training dataset. These results imply that
when it is of interest to improve model performance for patient subpopulations
beyond what can be achieved with standard practices, it may be necessary to do
so via data collection techniques that increase the effective sample size or
reduce the level of noise in the prediction problem.","['Stephen R. Pfohl', 'Haoran Zhang', 'Yizhe Xu', 'Agata Foryciarz', 'Marzyeh Ghassemi', 'Nigam H. Shah']","['stat.ML', 'cs.CY', 'cs.LG']",2021-08-27 13:10:00+00:00
http://arxiv.org/abs/2108.12172v1,Quantum Sub-Gaussian Mean Estimator,"We present a new quantum algorithm for estimating the mean of a real-valued
random variable obtained as the output of a quantum computation. Our estimator
achieves a nearly-optimal quadratic speedup over the number of classical i.i.d.
samples needed to estimate the mean of a heavy-tailed distribution with a
sub-Gaussian error rate. This result subsumes (up to logarithmic factors)
earlier works on the mean estimation problem that were not optimal for
heavy-tailed distributions [BHMT02,BDGT11], or that require prior information
on the variance [Hein02,Mon15,HM19]. As an application, we obtain new quantum
algorithms for the $(\epsilon,\delta)$-approximation problem with an optimal
dependence on the coefficient of variation of the input random variable.",['Yassine Hamoudi'],"['quant-ph', 'cs.CC', 'cs.DS', 'math.ST', 'stat.ML', 'stat.TH']",2021-08-27 08:34:26+00:00
http://arxiv.org/abs/2108.12112v1,Targeting Underrepresented Populations in Precision Medicine: A Federated Transfer Learning Approach,"The limited representation of minorities and disadvantaged populations in
large-scale clinical and genomics research has become a barrier to translating
precision medicine research into practice. Due to heterogeneity across
populations, risk prediction models are often found to be underperformed in
these underrepresented populations, and therefore may further exacerbate known
health disparities. In this paper, we propose a two-way data integration
strategy that integrates heterogeneous data from diverse populations and from
multiple healthcare institutions via a federated transfer learning approach.
The proposed method can handle the challenging setting where sample sizes from
different populations are highly unbalanced. With only a small number of
communications across participating sites, the proposed method can achieve
performance comparable to the pooled analysis where individual-level data are
directly pooled together. We show that the proposed method improves the
estimation and prediction accuracy in underrepresented populations, and reduces
the gap of model performance across populations. Our theoretical analysis
reveals how estimation accuracy is influenced by communication budgets, privacy
restrictions, and heterogeneity across populations. We demonstrate the
feasibility and validity of our methods through numerical experiments and a
real application to a multi-center study, in which we construct polygenic risk
prediction models for Type II diabetes in AA population.","['Sai Li', 'Tianxi Cai', 'Rui Duan']","['stat.ML', 'cs.CY', 'cs.LG']",2021-08-27 04:04:34+00:00
http://arxiv.org/abs/2108.12107v1,An Introduction to Hamiltonian Monte Carlo Method for Sampling,"The goal of this article is to introduce the Hamiltonian Monte Carlo (HMC)
method -- a Hamiltonian dynamics-inspired algorithm for sampling from a Gibbs
density $\pi(x) \propto e^{-f(x)}$. We focus on the ""idealized"" case, where one
can compute continuous trajectories exactly. We show that idealized HMC
preserves $\pi$ and we establish its convergence when $f$ is strongly convex
and smooth.",['Nisheeth K. Vishnoi'],"['cs.DS', 'cs.LG', 'math.PR', 'stat.CO', 'stat.ML']",2021-08-27 03:28:20+00:00
http://arxiv.org/abs/2108.11875v1,A spatio-temporal LSTM model to forecast across multiple temporal and spatial scales,"This paper presents a novel spatio-temporal LSTM (SPATIAL) architecture for
time series forecasting applied to environmental datasets. The framework was
evaluated across multiple sensors and for three different oceanic variables:
current speed, temperature, and dissolved oxygen. Network implementation
proceeded in two directions that are nominally separated but connected as part
of a natural environmental system -- across the spatial (between individual
sensors) and temporal components of the sensor data. Data from four sensors
sampling current speed, and eight measuring both temperature and dissolved
oxygen evaluated the framework. Results were compared against RF and XGB
baseline models that learned on the temporal signal of each sensor
independently by extracting the date-time features together with the past
history of data using sliding window matrix. Results demonstrated ability to
accurately replicate complex signals and provide comparable performance to
state-of-the-art benchmarks. Notably, the novel framework provided a simpler
pre-processing and training pipeline that handles missing values via a simple
masking layer. Enabling learning across the spatial and temporal directions,
this paper addresses two fundamental challenges of ML applications to
environmental science: 1) data sparsity and the challenges and costs of
collecting measurements of environmental conditions such as ocean dynamics, and
2) environmental datasets are inherently connected in the spatial and temporal
directions while classical ML approaches only consider one of these directions.
Furthermore, sharing of parameters across all input steps makes SPATIAL a fast,
scalable, and easily-parameterized forecasting framework.","['Yihao Hu', ""Fearghal O'Donncha"", 'Paulito Palmes', 'Meredith Burke', 'Ramon Filgueira', 'Jon Grant']","['stat.ML', 'cs.LG', 'physics.ao-ph']",2021-08-26 16:07:13+00:00
http://arxiv.org/abs/2108.11872v2,Comparing Classes of Estimators: When does Gradient Descent Beat Ridge Regression in Linear Models?,"Methods for learning from data depend on various types of tuning parameters,
such as penalization strength or step size. Since performance can depend
strongly on these parameters, it is important to compare classes of
estimators-by considering prescribed finite sets of tuning parameters-not just
particularly tuned methods. In this work, we investigate classes of methods via
the relative performance of the best method in the class. We consider the
central problem of linear regression-with a random isotropic ground truth-and
investigate the estimation performance of two fundamental methods, gradient
descent and ridge regression. We unveil the following phenomena. (1) For
general designs, constant stepsize gradient descent outperforms ridge
regression when the eigenvalues of the empirical data covariance matrix decay
slowly, as a power law with exponent less than unity. If instead the
eigenvalues decay quickly, as a power law with exponent greater than unity or
exponentially, we show that ridge regression outperforms gradient descent. (2)
For orthogonal designs, we compute the exact minimax optimal class of
estimators (achieving min-max-min optimality), showing it is equivalent to
gradient descent with decaying learning rate. We find the sub-optimality of
ridge regression and gradient descent with constant step size. Our results
highlight that statistical performance can depend strongly on tuning
parameters. In particular, while optimally tuned ridge regression is the best
estimator in our setting, it can be outperformed by gradient descent by an
arbitrary/unbounded amount when both methods are only tuned over finitely many
regularization parameters.","['Dominic Richards', 'Edgar Dobriban', 'Patrick Rebeschini']","['math.ST', 'cs.LG', 'math.OC', 'stat.ML', 'stat.TH']",2021-08-26 16:01:37+00:00
http://arxiv.org/abs/2108.11730v1,Deep learning based dictionary learning and tomographic image reconstruction,"This work presents an approach for image reconstruction in clinical low-dose
tomography that combines principles from sparse signal processing with ideas
from deep learning. First, we describe sparse signal representation in terms of
dictionaries from a statistical perspective and interpret dictionary learning
as a process of aligning distribution that arises from a generative model with
empirical distribution of true signals. As a result we can see that sparse
coding with learned dictionaries resembles a specific variational autoencoder,
where the decoder is a linear function and the encoder is a sparse coding
algorithm. Next, we show that dictionary learning can also benefit from
computational advancements introduced in the context of deep learning, such as
parallelism and as stochastic optimization. Finally, we show that
regularization by dictionaries achieves competitive performance in computed
tomography (CT) reconstruction comparing to state-of-the-art model based and
data driven approaches.","['Jevgenija Rudzusika', 'Thomas Koehler', 'Ozan Öktem']","['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE', 'eess.IV', 'math.OC']",2021-08-26 12:10:17+00:00
http://arxiv.org/abs/2108.11683v1,Estimation of Riemannian distances between covariance operators and Gaussian processes,"In this work we study two Riemannian distances between infinite-dimensional
positive definite Hilbert-Schmidt operators, namely affine-invariant Riemannian
and Log-Hilbert-Schmidt distances, in the context of covariance operators
associated with functional stochastic processes, in particular Gaussian
processes. Our first main results show that both distances converge in the
Hilbert-Schmidt norm. Using concentration results for Hilbert space-valued
random variables, we then show that both distances can be consistently and
efficiently estimated from (i) sample covariance operators, (ii) finite,
normalized covariance matrices, and (iii) finite samples generated by the given
processes, all with dimension-independent convergence. Our theoretical analysis
exploits extensively the methodology of reproducing kernel Hilbert space (RKHS)
covariance and cross-covariance operators. The theoretical formulation is
illustrated with numerical experiments on covariance operators of Gaussian
processes.",['Ha Quang Minh'],"['stat.ML', 'cs.LG']",2021-08-26 09:57:47+00:00
http://arxiv.org/abs/2108.11604v1,"Identification of the Resting Position Based on EGG, ECG, Respiration Rate and SpO2 Using Stacked Ensemble Learning","Rest is essential for a high-level physiological and psychological
performance. It is also necessary for the muscles to repair, rebuild, and
strengthen. There is a significant correlation between the quality of rest and
the resting posture. Therefore, identification of the resting position is of
paramount importance to maintain a healthy life. Resting postures can be
classified into four basic categories: Lying on the back (supine), facing of
the left / right sides and free-fall position. The later position is already
considered to be an unhealthy posture by researchers equivocally and hence can
be eliminated. In this paper, we analyzed the other three states of resting
position based on the data collected from the physiological parameters:
Electrogastrogram (EGG), Electrocardiogram (ECG), Respiration Rate, Heart Rate,
and Oxygen Saturation (SpO2). Based on these parameters, the resting position
is classified using a hybrid stacked ensemble machine learning model designed
using the Decision tree, Random Forest, and Xgboost algorithms. Our study
demonstrates a 100% accurate prediction of the resting position using the
hybrid model. The proposed method of identifying the resting position based on
physiological parameters has the potential to be integrated into wearable
devices. This is a low cost, highly accurate and autonomous technique to
monitor the body posture while maintaining the user privacy by eliminating the
use of RGB camera conventionally used to conduct the polysomnography (sleep
Monitoring) or resting position studies.","['Md. Mohsin Sarker Raihan', 'Muhammad Muinul Islam', 'Fariha Fairoz', 'Abdullah Bin Shams']","['cs.LG', 'cs.AI', 'eess.SP', 'q-bio.NC', 'stat.ML']",2021-08-26 06:58:41+00:00
http://arxiv.org/abs/2108.11579v2,Modeling Item Response Theory with Stochastic Variational Inference,"Item Response Theory (IRT) is a ubiquitous model for understanding human
behaviors and attitudes based on their responses to questions. Large modern
datasets offer opportunities to capture more nuances in human behavior,
potentially improving psychometric modeling leading to improved scientific
understanding and public policy. However, while larger datasets allow for more
flexible approaches, many contemporary algorithms for fitting IRT models may
also have massive computational demands that forbid real-world application. To
address this bottleneck, we introduce a variational Bayesian inference
algorithm for IRT, and show that it is fast and scalable without sacrificing
accuracy. Applying this method to five large-scale item response datasets from
cognitive science and education yields higher log likelihoods and higher
accuracy in imputing missing data than alternative inference algorithms. Using
this new inference approach we then generalize IRT with expressive Bayesian
models of responses, leveraging recent advances in deep learning to capture
nonlinear item characteristic curves (ICC) with neural networks. Using an
eigth-grade mathematics test from TIMSS, we show our nonlinear IRT models can
capture interesting asymmetric ICCs. The algorithm implementation is
open-source, and easily usable.","['Mike Wu', 'Richard L. Davis', 'Benjamin W. Domingue', 'Chris Piech', 'Noah Goodman']","['cs.LG', 'stat.ML']",2021-08-26 05:00:27+00:00
http://arxiv.org/abs/2108.11489v3,The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer Linear Networks,"The recent success of neural network models has shone light on a rather
surprising statistical phenomenon: statistical models that perfectly fit noisy
data can generalize well to unseen test data. Understanding this phenomenon of
$\textit{benign overfitting}$ has attracted intense theoretical and empirical
study. In this paper, we consider interpolating two-layer linear neural
networks trained with gradient flow on the squared loss and derive bounds on
the excess risk when the covariates satisfy sub-Gaussianity and
anti-concentration properties, and the noise is independent and sub-Gaussian.
By leveraging recent results that characterize the implicit bias of this
estimator, our bounds emphasize the role of both the quality of the
initialization as well as the properties of the data covariance matrix in
achieving low excess risk.","['Niladri S. Chatterji', 'Philip M. Long', 'Peter L. Bartlett']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2021-08-25 22:01:01+00:00
http://arxiv.org/abs/2108.11483v2,Heavy-tailed Streaming Statistical Estimation,"We consider the task of heavy-tailed statistical estimation given streaming
$p$-dimensional samples. This could also be viewed as stochastic optimization
under heavy-tailed distributions, with an additional $O(p)$ space complexity
constraint. We design a clipped stochastic gradient descent algorithm and
provide an improved analysis, under a more nuanced condition on the noise of
the stochastic gradients, which we show is critical when analyzing stochastic
optimization problems arising from general statistical estimation problems. Our
results guarantee convergence not just in expectation but with exponential
concentration, and moreover does so using $O(1)$ batch size. We provide
consequences of our results for mean estimation and linear regression. Finally,
we provide empirical corroboration of our results and algorithms via synthetic
experiments for mean estimation and linear regression.","['Che-Ping Tsai', 'Adarsh Prasad', 'Sivaraman Balakrishnan', 'Pradeep Ravikumar']","['cs.LG', 'math.OC', 'stat.ML']",2021-08-25 21:30:27+00:00
http://arxiv.org/abs/2108.11371v1,Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization,"Adaptive gradient methods such as Adam have gained increasing popularity in
deep learning optimization. However, it has been observed that compared with
(stochastic) gradient descent, Adam can converge to a different solution with a
significantly worse test error in many deep learning applications such as image
classification, even with a fine-tuned regularization. In this paper, we
provide a theoretical explanation for this phenomenon: we show that in the
nonconvex setting of learning over-parameterized two-layer convolutional neural
networks starting from the same random initialization, for a class of data
distributions (inspired from image data), Adam and gradient descent (GD) can
converge to different global solutions of the training objective with provably
different generalization errors, even with weight decay regularization. In
contrast, we show that if the training objective is convex, and the weight
decay regularization is employed, any optimization algorithms including Adam
and GD will converge to the same solution if the training is successful. This
suggests that the inferior generalization performance of Adam is fundamentally
tied to the nonconvex landscape of deep learning optimization.","['Difan Zou', 'Yuan Cao', 'Yuanzhi Li', 'Quanquan Gu']","['cs.LG', 'math.OC', 'stat.ML']",2021-08-25 17:58:21+00:00
http://arxiv.org/abs/2108.11345v4,A Unifying Theory of Thompson Sampling for Continuous Risk-Averse Bandits,"This paper unifies the design and the analysis of risk-averse Thompson
sampling algorithms for the multi-armed bandit problem for a class of risk
functionals $\rho$ that are continuous and dominant. We prove generalised
concentration bounds for these continuous and dominant risk functionals and
show that a wide class of popular risk functionals belong to this class. Using
our newly developed analytical toolkits, we analyse the algorithm $\rho$-MTS
(for multinomial distributions) and prove that they admit asymptotically
optimal regret bounds of risk-averse algorithms under CVaR, proportional
hazard, and other ubiquitous risk measures. More generally, we prove the
asymptotic optimality of $\rho$-MTS for Bernoulli distributions for a class of
risk measures known as empirical distribution performance measures (EDPMs);
this includes the well-known mean-variance. Numerical simulations show that the
regret bounds incurred by our algorithms are reasonably tight vis-\`a-vis
algorithm-independent lower bounds.","['Joel Q. L. Chang', 'Vincent Y. F. Tan']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2021-08-25 17:09:01+00:00
http://arxiv.org/abs/2108.11211v3,Clustering acoustic emission data streams with sequentially appearing clusters using mixture models,"The interpretation of unlabeled acoustic emission (AE) data classically
relies on general-purpose clustering methods. While several external criteria
have been used in the past to select the hyperparameters of those algorithms,
few studies have paid attention to the development of dedicated objective
functions in clustering methods able to cope with the specificities of AE data.
We investigate how to explicitly represent clusters onsets in mixture models in
general, and in Gaussian Mixture Models (GMM) in particular. By modifying the
internal criterion of such models, we propose the first clustering method able
to provide, through parameters estimated by an expectation-maximization
procedure, information about when clusters occur (onsets), how they grow
(kinetics) and their level of activation through time. This new objective
function accommodates continuous timestamps of AE signals and, thus, their
order of occurrence. The method, called GMMSEQ, is experimentally validated to
characterize the loosening phenomenon in bolted structure under vibrations. A
comparison with three standard clustering methods on raw streaming data from
five experimental campaigns shows that GMMSEQ not only provides useful
qualitative information about the timeline of clusters, but also shows better
performance in terms of cluster characterization. In view of developing an open
acoustic emission initiative and according to the FAIR principles, the datasets
and the codes are made available to reproduce the research of this paper.","['Emmanuel Ramasso', 'Thierry Denoeux', 'Gael Chevallier']","['stat.ML', 'cs.LG', 'cs.SD', 'stat.AP', 'stat.ME']",2021-08-25 13:01:06+00:00
http://arxiv.org/abs/2108.13195v1,On the approximation of a matrix,"Let $F^{*}$ be an approximation of a given $(a \times b)$ matrix $F$ derived
by methods that are not randomized. We prove that for a given $F$ and $F^{*}$,
$H$ and $T$ can be computed by randomized algorithm such that $(HT)$ is an
approximation of $F$ better than $F^{*}$.",['Samriddha Sanyal'],"['math.NA', 'cs.LG', 'cs.NA', 'stat.ML']",2021-08-25 09:30:51+00:00
http://arxiv.org/abs/2108.11019v2,Vector Transport Free Riemannian LBFGS for Optimization on Symmetric Positive Definite Matrix Manifolds,"This work concentrates on optimization on Riemannian manifolds. The
Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) algorithm is a commonly
used quasi-Newton method for numerical optimization in Euclidean spaces.
Riemannian LBFGS (RLBFGS) is an extension of this method to Riemannian
manifolds. RLBFGS involves computationally expensive vector transports as well
as unfolding recursions using adjoint vector transports. In this article, we
propose two mappings in the tangent space using the inverse second root and
Cholesky decomposition. These mappings make both vector transport and adjoint
vector transport identity and therefore isometric. Identity vector transport
makes RLBFGS less computationally expensive and its isometry is also very
useful in convergence analysis of RLBFGS. Moreover, under the proposed
mappings, the Riemannian metric reduces to Euclidean inner product, which is
much less computationally expensive. We focus on the Symmetric Positive
Definite (SPD) manifolds which are beneficial in various fields such as data
science and statistics. This work opens a research opportunity for extension of
the proposed mappings to other well-known manifolds.","['Reza Godaz', 'Benyamin Ghojogh', 'Reshad Hosseini', 'Reza Monsefi', 'Fakhri Karray', 'Mark Crowley']","['math.OC', 'cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2021-08-25 02:39:56+00:00
http://arxiv.org/abs/2108.11000v2,Layer Adaptive Node Selection in Bayesian Neural Networks: Statistical Guarantees and Implementation Details,"Sparse deep neural networks have proven to be efficient for predictive model
building in large-scale studies. Although several works have studied
theoretical and numerical properties of sparse neural architectures, they have
primarily focused on the edge selection. Sparsity through edge selection might
be intuitively appealing; however, it does not necessarily reduce the
structural complexity of a network. Instead pruning excessive nodes leads to a
structurally sparse network with significant computational speedup during
inference. To this end, we propose a Bayesian sparse solution using
spike-and-slab Gaussian priors to allow for automatic node selection during
training. The use of spike-and-slab prior alleviates the need of an ad-hoc
thresholding rule for pruning. In addition, we adopt a variational Bayes
approach to circumvent the computational challenges of traditional Markov Chain
Monte Carlo (MCMC) implementation. In the context of node selection, we
establish the fundamental result of variational posterior consistency together
with the characterization of prior parameters. In contrast to the previous
works, our theoretical development relaxes the assumptions of the equal number
of nodes and uniform bounds on all network weights, thereby accommodating
sparse networks with layer-dependent node structures or coefficient bounds.
With a layer-wise characterization of prior inclusion probabilities, we discuss
the optimal contraction rates of the variational posterior. We empirically
demonstrate that our proposed approach outperforms the edge selection method in
computational complexity with similar or better predictive performance. Our
experimental evidence further substantiates that our theoretical work
facilitates layer-wise optimal node recovery.","['Sanket Jantre', 'Shrijita Bhattacharya', 'Tapabrata Maiti']","['stat.ML', 'cs.LG']",2021-08-25 00:48:07+00:00
http://arxiv.org/abs/2108.10961v3,Entropic Gromov-Wasserstein between Gaussian Distributions,"We study the entropic Gromov-Wasserstein and its unbalanced version between
(unbalanced) Gaussian distributions with different dimensions. When the metric
is the inner product, which we refer to as inner product Gromov-Wasserstein
(IGW), we demonstrate that the optimal transportation plans of entropic IGW and
its unbalanced variant are (unbalanced) Gaussian distributions. Via an
application of von Neumann's trace inequality, we obtain closed-form
expressions for the entropic IGW between these Gaussian distributions. Finally,
we consider an entropic inner product Gromov-Wasserstein barycenter of multiple
Gaussian distributions. We prove that the barycenter is a Gaussian distribution
when the entropic regularization parameter is small. We further derive a
closed-form expression for the covariance matrix of the barycenter.","['Khang Le', 'Dung Le', 'Huy Nguyen', 'Dat Do', 'Tung Pham', 'Nhat Ho']","['math.ST', 'cs.IT', 'math.IT', 'stat.ML', 'stat.TH']",2021-08-24 21:27:11+00:00
http://arxiv.org/abs/2108.10934v3,Mitigating Statistical Bias within Differentially Private Synthetic Data,"Increasing interest in privacy-preserving machine learning has led to new and
evolved approaches for generating private synthetic data from undisclosed real
data. However, mechanisms of privacy preservation can significantly reduce the
utility of synthetic data, which in turn impacts downstream tasks such as
learning predictive models or inference. We propose several re-weighting
strategies using privatised likelihood ratios that not only mitigate
statistical bias of downstream estimators but also have general applicability
to differentially private generative models. Through large-scale empirical
evaluation, we show that private importance weighting provides simple and
effective privacy-compliant augmentation for general applications of synthetic
data.","['Sahra Ghalebikesabi', 'Harrison Wilde', 'Jack Jewson', 'Arnaud Doucet', 'Sebastian Vollmer', 'Chris Holmes']","['stat.ML', 'cs.CR', 'cs.LG']",2021-08-24 19:56:44+00:00
http://arxiv.org/abs/2108.11328v4,Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions,"In this paper we consider the problem of predicting survey response rates
using a family of flexible and interpretable nonparametric models. The study is
motivated by the US Census Bureau's well-known ROAM application which uses a
linear regression model trained on the US Census Planning Database data to
identify hard-to-survey areas. A crowdsourcing competition (Erdman and Bates,
2016) organized around ten years ago revealed that machine learning methods
based on ensembles of regression trees led to the best performance in
predicting survey response rates; however, the corresponding models could not
be adopted for the intended application due to their black-box nature. We
consider nonparametric additive models with small number of main and pairwise
interaction effects using $\ell_0$-based penalization. From a methodological
viewpoint, we study both computational and statistical aspects of our
estimator; and discuss variants that incorporate strong hierarchical
interactions. Our algorithms (opensourced on github) extend the computational
frontiers of existing algorithms for sparse additive models, to be able to
handle datasets relevant for the application we consider. We discuss and
interpret findings from our model on the US Census Planning Database. In
addition to being useful from an interpretability standpoint, our models lead
to predictions that appear to be better than popular black-box machine learning
methods based on gradient boosting and feedforward neural networks - suggesting
that it is possible to have models that have the best of both worlds: good
model accuracy and interpretability.","['Shibal Ibrahim', 'Peter Radchenko', 'Emanuel Ben-David', 'Rahul Mazumder']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.CO']",2021-08-24 17:49:55+00:00
http://arxiv.org/abs/2108.10828v2,Physics-Informed Deep Learning: A Promising Technique for System Reliability Assessment,"Considerable research has been devoted to deep learning-based predictive
models for system prognostics and health management in the reliability and
safety community. However, there is limited study on the utilization of deep
learning for system reliability assessment. This paper aims to bridge this gap
and explore this new interface between deep learning and system reliability
assessment by exploiting the recent advances of physics-informed deep learning.
Particularly, we present an approach to frame system reliability assessment in
the context of physics-informed deep learning and discuss the potential value
of physics-informed generative adversarial networks for the uncertainty
quantification and measurement data incorporation in system reliability
assessment. The proposed approach is demonstrated by three numerical examples
involving a dual-processor computing system. The results indicate the potential
value of physics-informed deep learning to alleviate computational challenges
and combine measurement data and mathematical models for system reliability
assessment.","['Taotao Zhou', 'Enrique Lopez Droguett', 'Ali Mosleh']","['stat.ML', 'cs.LG']",2021-08-24 16:24:46+00:00
http://arxiv.org/abs/2108.10826v2,"S&P 500 Stock Price Prediction Using Technical, Fundamental and Text Data","We summarized both common and novel predictive models used for stock price
prediction and combined them with technical indices, fundamental
characteristics and text-based sentiment data to predict S&P stock prices. A
66.18% accuracy in S&P 500 index directional prediction and 62.09% accuracy in
individual stock directional prediction was achieved by combining different
machine learning models such as Random Forest and LSTM together into
state-of-the-art ensemble models. The data we use contains weekly historical
prices, finance reports, and text information from news items associated with
518 different common stocks issued by current and former S&P 500 large-cap
companies, from January 1, 2000 to December 31, 2019. Our study's innovation
includes utilizing deep language models to categorize and infer financial news
item sentiment; fusing different models containing different combinations of
variables and stocks to jointly make predictions; and overcoming the
insufficient data problem for machine learning models in time series by using
data across different stocks.","['Shan Zhong', 'David B. Hitchcock']","['stat.ML', 'cs.LG']",2021-08-24 16:18:52+00:00
http://arxiv.org/abs/2108.11320v1,The Effect of Noise Level on Causal Identification with Additive Noise Models,"In recent years a lot of research has been conducted within the area of
causal inference and causal learning. Many methods have been developed to
identify the cause-effect pairs in models and have been successfully applied to
observational real-world data in order to determine the direction of causal
relationships. Many of these methods require simplifying assumptions, such as
absence of confounding, cycles, and selection bias. Yet in bivariate situations
causal discovery problems remain challenging. One class of such methods, that
also allows tackling the bivariate case, is based on Additive Noise Models
(ANMs). Unfortunately, one aspect of these methods has not received much
attention until now: what is the impact of different noise levels on the
ability of these methods to identify the direction of the causal relationship.
This work aims to bridge this gap with the help of an empirical study. For this
work, we considered bivariate cases, which is the most elementary form of a
causal discovery problem where one needs to decide whether X causes Y or Y
causes X, given joint distributions of two variables X, Y. Furthermore, two
specific methods have been selected, \textit{Regression with Subsequent
Independence Test} and \textit{Identification using Conditional Variances},
which have been tested with an exhaustive range of ANMs where the additive
noises' levels gradually change from 1% to 10000% of the causes' noise level
(the latter remains fixed). Additionally, the experiments in this work consider
several different types of distributions as well as linear and non-linear ANMs.
The results of the experiments show that these methods can fail to capture the
true causal direction for some levels of noise.",['Benjamin Kap'],"['stat.ML', 'cs.LG', '62G86', 'G.3.7; G.3.11']",2021-08-24 11:18:41+00:00
http://arxiv.org/abs/2108.10639v1,GrADE: A graph based data-driven solver for time-dependent nonlinear partial differential equations,"The physical world is governed by the laws of physics, often represented in
form of nonlinear partial differential equations (PDEs). Unfortunately,
solution of PDEs is non-trivial and often involves significant computational
time. With recent developments in the field of artificial intelligence and
machine learning, the solution of PDEs using neural network has emerged as a
domain with huge potential. However, most of the developments in this field are
based on either fully connected neural networks (FNN) or convolutional neural
networks (CNN). While FNN is computationally inefficient as the number of
network parameters can be potentially huge, CNN necessitates regular grid and
simpler domain. In this work, we propose a novel framework referred to as the
Graph Attention Differential Equation (GrADE) for solving time dependent
nonlinear PDEs. The proposed approach couples FNN, graph neural network, and
recently developed Neural ODE framework. The primary idea is to use graph
neural network for modeling the spatial domain, and Neural ODE for modeling the
temporal domain. The attention mechanism identifies important inputs/features
and assign more weightage to the same; this enhances the performance of the
proposed framework. Neural ODE, on the other hand, results in constant memory
cost and allows trading of numerical precision for speed. We also propose depth
refinement as an effective technique for training the proposed architecture in
lesser time with better accuracy. The effectiveness of the proposed framework
is illustrated using 1D and 2D Burgers' equations. Results obtained illustrate
the capability of the proposed framework in modeling PDE and its scalability to
larger domains without the need for retraining.","['Yash Kumar', 'Souvik Chakraborty']","['stat.ML', 'cs.LG', 'physics.comp-ph']",2021-08-24 10:49:03+00:00
http://arxiv.org/abs/2108.10629v2,Improving Generalization of Batch Whitening by Convolutional Unit Optimization,"Batch Whitening is a technique that accelerates and stabilizes training by
transforming input features to have a zero mean (Centering) and a unit variance
(Scaling), and by removing linear correlation between channels (Decorrelation).
In commonly used structures, which are empirically optimized with Batch
Normalization, the normalization layer appears between convolution and
activation function. Following Batch Whitening studies have employed the same
structure without further analysis; even Batch Whitening was analyzed on the
premise that the input of a linear layer is whitened. To bridge the gap, we
propose a new Convolutional Unit that is in line with the theory, and our
method generally improves the performance of Batch Whitening. Moreover, we show
the inefficacy of the original Convolutional Unit by investigating rank and
correlation of features. As our method is employable off-the-shelf whitening
modules, we use Iterative Normalization (IterNorm), the state-of-the-art
whitening module, and obtain significantly improved performance on five image
classification datasets: CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and
ImageNet. Notably, we verify that our method improves stability and performance
of whitening when using large learning rate, group size, and iteration number.","['Yooshin Cho', 'Hanbyel Cho', 'Youngsoo Kim', 'Junmo Kim']","['cs.CV', 'cs.AI', 'stat.ML']",2021-08-24 10:27:57+00:00
http://arxiv.org/abs/2108.10573v2,The staircase property: How hierarchical structure can guide deep learning,"This paper identifies a structural property of data distributions that
enables deep neural networks to learn hierarchically. We define the ""staircase""
property for functions over the Boolean hypercube, which posits that high-order
Fourier coefficients are reachable from lower-order Fourier coefficients along
increasing chains. We prove that functions satisfying this property can be
learned in polynomial time using layerwise stochastic coordinate descent on
regular neural networks -- a class of network architectures and initializations
that have homogeneity properties. Our analysis shows that for such staircase
functions and neural networks, the gradient-based algorithm learns high-level
features by greedily combining lower-level features along the depth of the
network. We further back our theoretical results with experiments showing that
staircase functions are also learnable by more standard ResNet architectures
with stochastic gradient descent. Both the theoretical and experimental results
support the fact that staircase properties have a role to play in understanding
the capabilities of gradient-based learning on regular networks, in contrast to
general polynomial-size networks that can emulate any SQ or PAC algorithms as
recently shown.","['Emmanuel Abbe', 'Enric Boix-Adsera', 'Matthew Brennan', 'Guy Bresler', 'Dheeraj Nagaraj']","['cs.LG', 'cs.DS', 'cs.NE', 'stat.ML']",2021-08-24 08:19:05+00:00
http://arxiv.org/abs/2108.10566v3,sigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel Classification,"Multiclass multilabel classification is the task of attributing multiple
labels to examples via predictions. Current models formulate a reduction of the
multilabel setting into either multiple binary classifications or multiclass
classification, allowing for the use of existing loss functions (sigmoid,
cross-entropy, logistic, etc.). Multilabel classification reductions do not
accommodate for the prediction of varying numbers of labels per example and the
underlying losses are distant estimates of the performance metrics. We propose
a loss function, sigmoidF1, which is an approximation of the F1 score that (1)
is smooth and tractable for stochastic gradient descent, (2) naturally
approximates a multilabel metric, and (3) estimates label propensities and
label counts. We show that any confusion matrix metric can be formulated with a
smooth surrogate. We evaluate the proposed loss function on text and image
datasets, and with a variety of metrics, to account for the complexity of
multilabel classification evaluation. sigmoidF1 outperforms other loss
functions on one text and two image datasets and several metrics. These results
show the effectiveness of using inference-time metrics as loss functions for
non-trivial classification problems like multilabel classification.","['Gabriel Bénédict', 'Vincent Koops', 'Daan Odijk', 'Maarten de Rijke']","['cs.LG', 'stat.ML']",2021-08-24 08:11:33+00:00
http://arxiv.org/abs/2108.10453v5,Continuous Treatment Recommendation with Deep Survival Dose Response Function,"We propose a general formulation for continuous treatment recommendation
problems in settings with clinical survival data, which we call the Deep
Survival Dose Response Function (DeepSDRF). That is, we consider the problem of
learning the conditional average dose response (CADR) function solely from
historical data in which observed factors (confounders) affect both observed
treatment and time-to-event outcomes. The estimated treatment effect from
DeepSDRF enables us to develop recommender algorithms with the correction for
selection bias. We compared two recommender approaches based on random search
and reinforcement learning and found similar performance in terms of patient
outcome. We tested the DeepSDRF and the corresponding recommender on extensive
simulation studies and the eICU Research Institute (eRI) database. To the best
of our knowledge, this is the first time that causal models are used to address
the continuous treatment effect with observational data in a medical context.","['Jie Zhu', 'Blanca Gallego']","['stat.ML', 'cs.AI', 'cs.LG', 'econ.EM']",2021-08-24 00:19:04+00:00
http://arxiv.org/abs/2108.10411v2,StreaMRAK a Streaming Multi-Resolution Adaptive Kernel Algorithm,"Kernel ridge regression (KRR) is a popular scheme for non-linear
non-parametric learning. However, existing implementations of KRR require that
all the data is stored in the main memory, which severely limits the use of KRR
in contexts where data size far exceeds the memory size. Such applications are
increasingly common in data mining, bioinformatics, and control. A powerful
paradigm for computing on data sets that are too large for memory is the
streaming model of computation, where we process one data sample at a time,
discarding each sample before moving on to the next one. In this paper, we
propose StreaMRAK - a streaming version of KRR. StreaMRAK improves on existing
KRR schemes by dividing the problem into several levels of resolution, which
allows continual refinement to the predictions. The algorithm reduces the
memory requirement by continuously and efficiently integrating new samples into
the training model. With a novel sub-sampling scheme, StreaMRAK reduces memory
and computational complexities by creating a sketch of the original data, where
the sub-sampling density is adapted to the bandwidth of the kernel and the
local dimensionality of the data. We present a showcase study on two synthetic
problems and the prediction of the trajectory of a double pendulum. The results
show that the proposed algorithm is fast and accurate.","['Andreas Oslandsbotn', 'Zeljko Kereta', 'Valeriya Naumova', 'Yoav Freund', 'Alexander Cloninger']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML', '68Q32, 65D15, 46E22, 68W27']",2021-08-23 21:03:09+00:00
http://arxiv.org/abs/2108.10346v1,Explaining Bayesian Neural Networks,"To make advanced learning machines such as Deep Neural Networks (DNNs) more
transparent in decision making, explainable AI (XAI) aims to provide
interpretations of DNNs' predictions. These interpretations are usually given
in the form of heatmaps, each one illustrating relevant patterns regarding the
prediction for a given instance. Bayesian approaches such as Bayesian Neural
Networks (BNNs) so far have a limited form of transparency (model transparency)
already built-in through their prior weight distribution, but notably, they
lack explanations of their predictions for given instances. In this work, we
bring together these two perspectives of transparency into a holistic
explanation framework for explaining BNNs. Within the Bayesian framework, the
network weights follow a probability distribution. Hence, the standard
(deterministic) prediction strategy of DNNs extends in BNNs to a predictive
distribution, and thus the standard explanation extends to an explanation
distribution. Exploiting this view, we uncover that BNNs implicitly employ
multiple heterogeneous prediction strategies. While some of these are inherited
from standard DNNs, others are revealed to us by considering the inherent
uncertainty in BNNs. Our quantitative and qualitative experiments on
toy/benchmark data and real-world data from pathology show that the proposed
approach of explaining BNNs can lead to more effective and insightful
explanations.","['Kirill Bykov', 'Marina M. -C. Höhne', 'Adelaida Creosteanu', 'Klaus-Robert Müller', 'Frederick Klauschen', 'Shinichi Nakajima', 'Marius Kloft']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2021-08-23 18:09:41+00:00
http://arxiv.org/abs/2108.10298v1,Network control by a constrained external agent as a continuous optimization problem,"Social science studies dealing with control in networks typically resort to
heuristics or describing the static control distribution. Optimal policies,
however, require interventions that optimize control over a socioeconomic
network subject to real-world constraints. We integrate optimisation tools from
deep-learning with network science into a framework that is able to optimize
such interventions in real-world networks. We demonstrate the framework in the
context of corporate control, where it allows to characterize the vulnerability
of strategically important corporate networks to sensitive takeovers, an
important contemporaneous policy challenge. The framework produces insights
that are relevant for governing real-world socioeconomic networks, and opens up
new research avenues for improving our understanding and control of such
complex systems.","['Jannes Nys', 'Milan van den Heuvel', 'Koen Schoors', 'Bruno Merlevede']","['cs.SI', 'stat.ML']",2021-08-23 17:21:23+00:00
http://arxiv.org/abs/2108.10284v2,Exclusive Group Lasso for Structured Variable Selection,"A structured variable selection problem is considered in which the
covariates, divided into predefined groups, activate according to sparse
patterns with few nonzero entries per group. Capitalizing on the concept of
atomic norm, a composite norm can be properly designed to promote such
exclusive group sparsity patterns. The resulting norm lends itself to efficient
and flexible regularized optimization algorithms for support recovery, like the
proximal algorithm. Moreover, an active set algorithm is proposed that builds
the solution by successively including structure atoms into the estimated
support. It is also shown that such an algorithm can be tailored to match more
rigid structures than plain exclusive group sparsity. Asymptotic consistency
analysis (with both the number of parameters as well as the number of groups
growing with the observation size) establishes the effectiveness of the
proposed solution in terms of signed support recovery under conventional
assumptions. Finally, a set of numerical simulations further corroborates the
results.","['David Gregoratti', 'Xavier Mestre', 'Carlos Buelga']","['cs.LG', 'eess.SP', 'stat.ML']",2021-08-23 16:55:13+00:00
http://arxiv.org/abs/2108.10274v2,Towards Explainable Fact Checking,"The past decade has seen a substantial rise in the amount of mis- and
disinformation online, from targeted disinformation campaigns to influence
politics, to the unintentional spreading of misinformation about public health.
This development has spurred research in the area of automatic fact checking,
from approaches to detect check-worthy claims and determining the stance of
tweets towards claims, to methods to determine the veracity of claims given
evidence documents. These automatic methods are often content-based, using
natural language processing methods, which in turn utilise deep neural networks
to learn higher-order features from text in order to make predictions. As deep
neural networks are black-box models, their inner workings cannot be easily
explained. At the same time, it is desirable to explain how they arrive at
certain decisions, especially if they are to be used for decision making. While
this has been known for some time, the issues this raises have been exacerbated
by models increasing in size, and by EU legislation requiring models to be used
for decision making to provide explanations, and, very recently, by legislation
requiring online platforms operating in the EU to provide transparent reporting
on their services. Despite this, current solutions for explainability are still
lacking in the area of fact checking. This thesis presents my research on
automatic fact checking, including claim check-worthiness detection, stance
detection and veracity prediction. Its contributions go beyond fact checking,
with the thesis proposing more general machine learning solutions for natural
language processing in the area of learning with limited labelled data.
Finally, the thesis presents some first solutions for explainable fact
checking.",['Isabelle Augenstein'],"['cs.CL', 'stat.ML']",2021-08-23 16:22:50+00:00
http://arxiv.org/abs/2108.10252v4,Federated Multi-Task Learning under a Mixture of Distributions,"The increasing size of data generated by smartphones and IoT devices
motivated the development of Federated Learning (FL), a framework for on-device
collaborative training of machine learning models. First efforts in FL focused
on learning a single global model with good average performance across clients,
but the global model may be arbitrarily bad for a given client, due to the
inherent heterogeneity of local data distributions. Federated multi-task
learning (MTL) approaches can learn personalized models by formulating an
opportune penalized optimization problem. The penalization term can capture
complex relations among personalized models, but eschews clear statistical
assumptions about local data distributions. In this work, we propose to study
federated MTL under the flexible assumption that each local data distribution
is a mixture of unknown underlying distributions. This assumption encompasses
most of the existing personalized FL approaches and leads to federated EM-like
algorithms for both client-server and fully decentralized settings. Moreover,
it provides a principled way to serve personalized models to clients not seen
at training time. The algorithms' convergence is analyzed through a novel
federated surrogate optimization framework, which can be of general interest.
Experimental results on FL benchmarks show that our approach provides models
with higher accuracy and fairness than state-of-the-art methods.","['Othmane Marfoq', 'Giovanni Neglia', 'Aurélien Bellet', 'Laetitia Kameni', 'Richard Vidal']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2021-08-23 15:47:53+00:00
http://arxiv.org/abs/2108.10249v1,"New Q-Newton's method meets Backtracking line search: good convergence guarantee, saddle points avoidance, quadratic rate of convergence, and easy implementation","In a recent joint work, the author has developed a modification of Newton's
method, named New Q-Newton's method, which can avoid saddle points and has
quadratic rate of convergence. While good theoretical convergence guarantee has
not been established for this method, experiments on small scale problems show
that the method works very competitively against other well known modifications
of Newton's method such as Adaptive Cubic Regularization and BFGS, as well as
first order methods such as Unbounded Two-way Backtracking Gradient Descent.
  In this paper, we resolve the convergence guarantee issue by proposing a
modification of New Q-Newton's method, named New Q-Newton's method
Backtracking, which incorporates a more sophisticated use of hyperparameters
and a Backtracking line search. This new method has very good theoretical
guarantees, which for a {\bf Morse function} yields the following (which is
unknown for New Q-Newton's method):
  {\bf Theorem.} Let $f:\mathbb{R}^m\rightarrow \mathbb{R}$ be a Morse
function, that is all its critical points have invertible Hessian. Then for a
sequence $\{x_n\}$ constructed by New Q-Newton's method Backtracking from a
random initial point $x_0$, we have the following two alternatives:
  i) $\lim_{n\rightarrow\infty}||x_n||=\infty$,
  or
  ii) $\{x_n\}$ converges to a point $x_{\infty}$ which is a {\bf local
minimum} of $f$, and the rate of convergence is {\bf quadratic}.
  Moreover, if $f$ has compact sublevels, then only case ii) happens.
  As far as we know, for Morse functions, this is the best theoretical
guarantee for iterative optimization algorithms so far in the literature. We
have tested in experiments on small scale, with some further simplified
versions of New Q-Newton's method Backtracking, and found that the new method
significantly improve New Q-Newton's method.",['Tuyen Trung Truong'],"['math.OC', 'cs.LG', 'cs.NA', 'math.DS', 'math.NA', 'stat.ML']",2021-08-23 15:39:00+00:00
http://arxiv.org/abs/2108.10129v1,Effective Streaming Low-tubal-rank Tensor Approximation via Frequent Directions,"Low-tubal-rank tensor approximation has been proposed to analyze large-scale
and multi-dimensional data. However, finding such an accurate approximation is
challenging in the streaming setting, due to the limited computational
resources. To alleviate this issue, this paper extends a popular matrix
sketching technique, namely Frequent Directions, for constructing an efficient
and accurate low-tubal-rank tensor approximation from streaming data based on
the tensor Singular Value Decomposition (t-SVD). Specifically, the new
algorithm allows the tensor data to be observed slice by slice, but only needs
to maintain and incrementally update a much smaller sketch which could capture
the principal information of the original tensor. The rigorous theoretical
analysis shows that the approximation error of the new algorithm can be
arbitrarily small when the sketch size grows linearly. Extensive experimental
results on both synthetic and real multi-dimensional data further reveal the
superiority of the proposed algorithm compared with other sketching algorithms
for getting low-tubal-rank approximation, in terms of both efficiency and
accuracy.","['Qianxin Yi', 'Chenhao Wang', 'Kaidong Wang', 'Yao Wang']","['cs.LG', 'stat.ML']",2021-08-23 12:53:44+00:00
http://arxiv.org/abs/2108.10101v1,On the Acceleration of Deep Neural Network Inference using Quantized Compressed Sensing,"Accelerating deep neural network (DNN) inference on resource-limited devices
is one of the most important barriers to ensuring a wider and more inclusive
adoption. To alleviate this, DNN binary quantization for faster convolution and
memory savings is one of the most promising strategies despite its serious drop
in accuracy. The present paper therefore proposes a novel binary quantization
function based on quantized compressed sensing (QCS). Theoretical arguments
conjecture that our proposal preserves the practical benefits of standard
methods, while reducing the quantization error and the resulting drop in
accuracy.",['Meshia Cédric Oveneke'],"['cs.LG', 'cs.NA', 'cs.NE', 'eess.SP', 'math.NA', 'stat.ML']",2021-08-23 12:03:24+00:00
http://arxiv.org/abs/2108.10029v2,Modeling time evolving COVID-19 uncertainties with density dependent asymptomatic infections and social reinforcement,"The COVID-19 pandemic has posed significant challenges in modeling its
complex epidemic transmissions, infection and contagion, which are very
different from known epidemics. The challenges in quantifying COVID-19
complexities include effectively modeling its process and data uncertainties.
The uncertainties are embedded in implicit and high-proportional undocumented
infections, asymptomatic contagion, social reinforcement of infections, and
various quality issues in the reported data. These uncertainties become even
more apparent in the first two months of the COVID-19 pandemic, when the
relevant knowledge, case reporting and testing were all limited. Here we
introduce a novel hybrid approach Susceptible-Undocumented infected-Documented
infected-Recovered (SUDR) model. First, SUDR (1) characterizes and
distinguishes Undocumented (U) and Documented (D) infections commonly seen
during COVID-19 incubation periods and asymptomatic infections. Second, SUDR
characterizes the probabilistic density of infections by capturing exogenous
processes. Lastly, SUDR approximates the density likelihood of COVID-19
prevalence over time by incorporating Bayesian inference into SUDR. Different
from existing COVID-19 models, SUDR characterizes the undocumented infections
during unknown transmission processes. To capture the uncertainties of temporal
transmission and social reinforcement during COVID-19 contagion, the
transmission rate is modeled by a time-varying density function of undocumented
infectious cases. By sampling from the mean-field posterior distribution with
reasonable priors, SUDR handles the randomness, noise and sparsity of COVID-19
observations widely seen in the public COVID-19 case data. The results
demonstrate a deeper quantitative understanding of the above uncertainties, in
comparison with classic SIR, time-dependent SIR, and probabilistic SIR models.","['Qing Liu', 'Longbing Cao']","['stat.ML', 'cs.LG', 'q-bio.PE']",2021-08-23 09:38:54+00:00
http://arxiv.org/abs/2108.10015v2,Semantic-Preserving Adversarial Text Attacks,"Deep neural networks (DNNs) are known to be vulnerable to adversarial images,
while their robustness in text classification is rarely studied. Several lines
of text attack methods have been proposed in the literature, including
character-level, word-level, and sentence-level attacks. However, it is still a
challenge to minimize the number of word changes necessary to induce
misclassification, while simultaneously ensuring lexical correctness, syntactic
soundness, and semantic similarity. In this paper, we propose a Bigram and
Unigram based adaptive Semantic Preservation Optimization (BU-SPO) method to
examine the vulnerability of deep models. Our method has four major merits.
Firstly, we propose to attack text documents not only at the unigram word level
but also at the bigram level which better keeps semantics and avoids producing
meaningless outputs. Secondly, we propose a hybrid method to replace the input
words with options among both their synonyms candidates and sememe candidates,
which greatly enriches the potential substitutions compared to only using
synonyms. Thirdly, we design an optimization algorithm, i.e., Semantic
Preservation Optimization (SPO), to determine the priority of word
replacements, aiming to reduce the modification cost. Finally, we further
improve the SPO with a semantic Filter (named SPOF) to find the adversarial
example with the highest semantic similarity. We evaluate the effectiveness of
our BU-SPO and BU-SPOF on IMDB, AG's News, and Yahoo! Answers text datasets by
attacking four popular DNNs models. Results show that our methods achieve the
highest attack success rates and semantics rates by changing the smallest
number of words compared with existing methods.","['Xinghao Yang', 'Weifeng Liu', 'James Bailey', 'Dacheng Tao', 'Wei Liu']","['cs.CL', 'stat.ML']",2021-08-23 09:05:18+00:00
http://arxiv.org/abs/2108.09859v1,Convex Latent Effect Logit Model via Sparse and Low-rank Decomposition,"In this paper, we propose a convex formulation for learning logistic
regression model (logit) with latent heterogeneous effect on sub-population. In
transportation, logistic regression and its variants are often interpreted as
discrete choice models under utility theory (McFadden, 2001). Two prominent
applications of logit models in the transportation domain are traffic accident
analysis and choice modeling. In these applications, researchers often want to
understand and capture the individual variation under the same accident or
choice scenario. The mixed effect logistic regression (mixed logit) is a
popular model employed by transportation researchers. To estimate the
distribution of mixed logit parameters, a non-convex optimization problem with
nested high-dimensional integrals needs to be solved. Simulation-based
optimization is typically applied to solve the mixed logit parameter estimation
problem. Despite its popularity, the mixed logit approach for learning
individual heterogeneity has several downsides. First, the parametric form of
the distribution requires domain knowledge and assumptions imposed by users,
although this issue can be addressed to some extent by using a non-parametric
approach. Second, the optimization problems arise from parameter estimation for
mixed logit and the non-parametric extensions are non-convex, which leads to
unstable model interpretation. Third, the simulation size in
simulation-assisted estimation lacks finite-sample theoretical guarantees and
is chosen somewhat arbitrarily in practice. To address these issues, we are
motivated to develop a formulation that models the latent individual
heterogeneity while preserving convexity, and avoids the need for
simulation-based approximation. Our setup is based on decomposing the
parameters into a sparse homogeneous component in the population and low-rank
heterogeneous parts for each individual.","['Hongyuan Zhan', 'Kamesh Madduri', 'Venkataraman Shankar']","['cs.LG', 'stat.ME', 'stat.ML']",2021-08-22 22:23:39+00:00
http://arxiv.org/abs/2108.09805v2,Efficient Algorithms for Learning from Coarse Labels,"For many learning problems one may not have access to fine grained label
information; e.g., an image can be labeled as husky, dog, or even animal
depending on the expertise of the annotator. In this work, we formalize these
settings and study the problem of learning from such coarse data. Instead of
observing the actual labels from a set $\mathcal{Z}$, we observe coarse labels
corresponding to a partition of $\mathcal{Z}$ (or a mixture of partitions).
  Our main algorithmic result is that essentially any problem learnable from
fine grained labels can also be learned efficiently when the coarse data are
sufficiently informative. We obtain our result through a generic reduction for
answering Statistical Queries (SQ) over fine grained labels given only coarse
labels. The number of coarse labels required depends polynomially on the
information distortion due to coarsening and the number of fine labels
$|\mathcal{Z}|$.
  We also investigate the case of (infinitely many) real valued labels focusing
on a central problem in censored and truncated statistics: Gaussian mean
estimation from coarse data. We provide an efficient algorithm when the sets in
the partition are convex and establish that the problem is NP-hard even for
very simple non-convex sets.","['Dimitris Fotakis', 'Alkis Kalavasis', 'Vasilis Kontonis', 'Christos Tzamos']","['cs.LG', 'cs.DS', 'stat.ML']",2021-08-22 18:01:39+00:00
http://arxiv.org/abs/2108.09733v2,A universally consistent learning rule with a universally monotone error,"We present a universally consistent learning rule whose expected error is
monotone non-increasing with the sample size under every data distribution. The
question of existence of such rules was brought up in 1996 by Devroye, Gy\""orfi
and Lugosi (who called them ""smart""). Our rule is fully deterministic, a
data-dependent partitioning rule constructed in an arbitrary domain (a standard
Borel space) using a cyclic order. The central idea is to only partition at
each step those cyclic intervals that exhibit a sufficient empirical diversity
of labels, thus avoiding a region where the error function is convex.",['Vladimir Pestov'],"['cs.LG', 'stat.ML']",2021-08-22 14:28:33+00:00
http://arxiv.org/abs/2108.09676v3,Efficient Gaussian Neural Processes for Regression,"Conditional Neural Processes (CNP; Garnelo et al., 2018) are an attractive
family of meta-learning models which produce well-calibrated predictions,
enable fast inference at test time, and are trainable via a simple maximum
likelihood procedure. A limitation of CNPs is their inability to model
dependencies in the outputs. This significantly hurts predictive performance
and renders it impossible to draw coherent function samples, which limits the
applicability of CNPs in down-stream applications and decision making. Neural
Processes (NPs; Garnelo et al., 2018) attempt to alleviate this issue by using
latent variables, relying on these to model output dependencies, but introduces
difficulties stemming from approximate inference. One recent alternative
(Bruinsma et al., 2021), which we refer to as the FullConvGNP, models
dependencies in the predictions while still being trainable via exact
maximum-likelihood. Unfortunately, the FullConvGNP relies on expensive
2D-dimensional convolutions, which limit its applicability to only
one-dimensional data. In this work, we present an alternative way to model
output dependencies which also lends itself maximum likelihood training but,
unlike the FullConvGNP, can be scaled to two- and three-dimensional data. The
proposed models exhibit good performance in synthetic experiments.","['Stratis Markou', 'James Requeima', 'Wessel Bruinsma', 'Richard Turner']","['cs.LG', 'stat.ML']",2021-08-22 09:31:50+00:00
http://arxiv.org/abs/2108.09664v1,New Trends in Quantum Machine Learning,"Here we will give a perspective on new possible interplays between Machine
Learning and Quantum Physics, including also practical cases and applications.
We will explore the ways in which machine learning could benefit from new
quantum technologies and algorithms to find new ways to speed up their
computations by breakthroughs in physical hardware, as well as to improve
existing models or devise new learning schemes in the quantum domain. Moreover,
there are lots of experiments in quantum physics that do generate incredible
amounts of data and machine learning would be a great tool to analyze those and
make predictions, or even control the experiment itself. On top of that, data
visualization techniques and other schemes borrowed from machine learning can
be of great use to theoreticians to have better intuition on the structure of
complex manifolds or to make predictions on theoretical models. This new
research field, named as Quantum Machine Learning, is very rapidly growing
since it is expected to provide huge advantages over its classical counterpart
and deeper investigations are timely needed since they can be already tested on
the already commercially available quantum machines.","['Lorenzo Buffoni', 'Filippo Caruso']","['quant-ph', 'cond-mat.dis-nn', 'cs.LG', 'stat.ML']",2021-08-22 08:23:30+00:00
http://arxiv.org/abs/2108.09645v4,Improving Mini-batch Optimal Transport via Partial Transportation,"Mini-batch optimal transport (m-OT) has been widely used recently to deal
with the memory issue of OT in large-scale applications. Despite their
practicality, m-OT suffers from misspecified mappings, namely, mappings that
are optimal on the mini-batch level but are partially wrong in the comparison
with the optimal transportation plan between the original measures. Motivated
by the misspecified mappings issue, we propose a novel mini-batch method by
using partial optimal transport (POT) between mini-batch empirical measures,
which we refer to as mini-batch partial optimal transport (m-POT). Leveraging
the insight from the partial transportation, we explain the source of
misspecified mappings from the m-OT and motivate why limiting the amount of
transported masses among mini-batches via POT can alleviate the incorrect
mappings. Finally, we carry out extensive experiments on various applications
such as deep domain adaptation, partial domain adaptation, deep generative
model, color transfer, and gradient flow to demonstrate the favorable
performance of m-POT compared to current mini-batch methods.","['Khai Nguyen', 'Dang Nguyen', 'The-Anh Vu-Le', 'Tung Pham', 'Nhat Ho']","['stat.ML', 'cs.LG']",2021-08-22 05:45:48+00:00
http://arxiv.org/abs/2108.09585v1,Sequential Stochastic Optimization in Separable Learning Environments,"We consider a class of sequential decision-making problems under uncertainty
that can encompass various types of supervised learning concepts. These
problems have a completely observed state process and a partially observed
modulation process, where the state process is affected by the modulation
process only through an observation process, the observation process only
observes the modulation process, and the modulation process is exogenous to
control. We model this broad class of problems as a partially observed Markov
decision process (POMDP). The belief function for the modulation process is
control invariant, thus separating the estimation of the modulation process
from the control of the state process. We call this specially structured POMDP
the separable POMDP, or SEP-POMDP, and show it (i) can serve as a model for a
broad class of application areas, e.g., inventory control, finance, healthcare
systems, (ii) inherits value function and optimal policy structure from a set
of completely observed MDPs, (iii) can serve as a bridge between classical
models of sequential decision making under uncertainty having fully specified
model artifacts and such models that are not fully specified and require the
use of predictive methods from statistics and machine learning, and (iv) allows
for specialized approximate solution procedures.","['R. Reid Bishop', 'Chelsea C. White III']","['math.OC', 'cs.LG', 'stat.ML']",2021-08-21 21:29:04+00:00
http://arxiv.org/abs/2108.09507v3,"Shift-Curvature, SGD, and Generalization","A longstanding debate surrounds the related hypotheses that low-curvature
minima generalize better, and that SGD discourages curvature. We offer a more
complete and nuanced view in support of both. First, we show that curvature
harms test performance through two new mechanisms, the shift-curvature and
bias-curvature, in addition to a known parameter-covariance mechanism. The
three curvature-mediated contributions to test performance are
reparametrization-invariant although curvature is not. The shift in the
shift-curvature is the line connecting train and test local minima, which
differ due to dataset sampling or distribution shift. Although the shift is
unknown at training time, the shift-curvature can still be mitigated by
minimizing overall curvature. Second, we derive a new, explicit SGD
steady-state distribution showing that SGD optimizes an effective potential
related to but different from train loss, and that SGD noise mediates a
trade-off between deep versus low-curvature regions of this effective
potential. Third, combining our test performance analysis with the SGD steady
state shows that for small SGD noise, the shift-curvature may be the most
significant of the three mechanisms. Our experiments confirm the impact of
shift-curvature on test loss, and further explore the relationship between SGD
noise and curvature.","['Arwen V. Bradley', 'Carlos Alberto Gomez-Uribe', 'Manish Reddy Vuyyuru']","['stat.ML', 'cs.LG']",2021-08-21 13:18:49+00:00
http://arxiv.org/abs/2108.09501v2,A Stochastic Variance-Reduced Coordinate Descent Algorithm for Learning Sparse Bayesian Network from Discrete High-Dimensional Data,"This paper addresses the problem of learning a sparse structure Bayesian
network from high-dimensional discrete data. Compared to continuous Bayesian
networks, learning a discrete Bayesian network is a challenging problem due to
the large parameter space. Although many approaches have been developed for
learning continuous Bayesian networks, few approaches have been proposed for
the discrete ones. In this paper, we address learning Bayesian networks as an
optimization problem and propose a score function which guarantees the learnt
structure to be a sparse directed acyclic graph. Besides, we implement a
block-wised stochastic coordinate descent algorithm to optimize the score
function. Specifically, we use a variance reducing method in our optimization
algorithm to make the algorithm work efficiently for high-dimensional data. The
proposed approach is applied to synthetic data from well-known benchmark
networks. The quality, scalability, and robustness of the constructed network
are measured. Compared to some competitive approaches, the results reveal that
our algorithm outperforms some of the well-known proposed methods.","['Nazanin Shajoonnezhad', 'Amin Nikanjam']","['cs.LG', 'stat.ML']",2021-08-21 12:21:01+00:00
http://arxiv.org/abs/2108.09420v1,Fast Sketching of Polynomial Kernels of Polynomial Degree,"Kernel methods are fundamental in machine learning, and faster algorithms for
kernel approximation provide direct speedups for many core tasks in machine
learning. The polynomial kernel is especially important as other kernels can
often be approximated by the polynomial kernel via a Taylor series expansion.
Recent techniques in oblivious sketching reduce the dependence in the running
time on the degree $q$ of the polynomial kernel from exponential to polynomial,
which is useful for the Gaussian kernel, for which $q$ can be chosen to be
polylogarithmic. However, for more slowly growing kernels, such as the neural
tangent and arc-cosine kernels, $q$ needs to be polynomial, and previous work
incurs a polynomial factor slowdown in the running time. We give a new
oblivious sketch which greatly improves upon this running time, by removing the
dependence on $q$ in the leading order term. Combined with a novel sampling
scheme, we give the fastest algorithms for approximating a large family of
slow-growing kernels.","['Zhao Song', 'David P. Woodruff', 'Zheng Yu', 'Lichen Zhang']","['cs.DS', 'cs.LG', 'stat.ML']",2021-08-21 02:14:55+00:00
http://arxiv.org/abs/2108.09265v2,Efficient Online Estimation of Causal Effects by Deciding What to Observe,"Researchers often face data fusion problems, where multiple data sources are
available, each capturing a distinct subset of variables. While problem
formulations typically take the data as given, in practice, data acquisition
can be an ongoing process. In this paper, we aim to estimate any functional of
a probabilistic model (e.g., a causal effect) as efficiently as possible, by
deciding, at each time, which data source to query. We propose online moment
selection (OMS), a framework in which structural assumptions are encoded as
moment conditions. The optimal action at each step depends, in part, on the
very moments that identify the functional of interest. Our algorithms balance
exploration with choosing the best action as suggested by current estimates of
the moments. We propose two selection strategies: (1) explore-then-commit
(OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero
asymptotic regret as assessed by MSE. We instantiate our setup for average
treatment effect estimation, where structural assumptions are given by a causal
graph and data sources may include subsets of mediators, confounders, and
instrumental variables.","['Shantanu Gupta', 'Zachary C. Lipton', 'David Childers']","['cs.LG', 'econ.EM', 'stat.ML']",2021-08-20 17:00:56+00:00
http://arxiv.org/abs/2108.09262v1,Optimal Order Simple Regret for Gaussian Process Bandits,"Consider the sequential optimization of a continuous, possibly non-convex,
and expensive to evaluate objective function $f$. The problem can be cast as a
Gaussian Process (GP) bandit where $f$ lives in a reproducing kernel Hilbert
space (RKHS). The state of the art analysis of several learning algorithms
shows a significant gap between the lower and upper bounds on the simple regret
performance. When $N$ is the number of exploration trials and $\gamma_N$ is the
maximal information gain, we prove an $\tilde{\mathcal{O}}(\sqrt{\gamma_N/N})$
bound on the simple regret performance of a pure exploration algorithm that is
significantly tighter than the existing bounds. We show that this bound is
order optimal up to logarithmic factors for the cases where a lower bound on
regret is known. To establish these results, we prove novel and sharp
confidence intervals for GP models applicable to RKHS elements which may be of
broader interest.","['Sattar Vakili', 'Nacime Bouziani', 'Sepehr Jalali', 'Alberto Bernacchia', 'Da-shan Shiu']","['stat.ML', 'cs.LG']",2021-08-20 16:49:32+00:00
http://arxiv.org/abs/2108.11753v1,A survey on Bayesian inference for Gaussian mixture model,"Clustering has become a core technology in machine learning, largely due to
its application in the field of unsupervised learning, clustering,
classification, and density estimation. A frequentist approach exists to hand
clustering based on mixture model which is known as the EM algorithm where the
parameters of the mixture model are usually estimated into a maximum likelihood
estimation framework. Bayesian approach for finite and infinite Gaussian
mixture model generates point estimates for all variables as well as associated
uncertainty in the form of the whole estimates' posterior distribution.
  The sole aim of this survey is to give a self-contained introduction to
concepts and mathematical tools in Bayesian inference for finite and infinite
Gaussian mixture model in order to seamlessly introduce their applications in
subsequent sections. However, we clearly realize our inability to cover all the
useful and interesting results concerning this field and given the paucity of
scope to present this discussion, e.g., the separated analysis of the
generation of Dirichlet samples by stick-breaking and Polya's Urn approaches.
We refer the reader to literature in the field of the Dirichlet process mixture
model for a much detailed introduction to the related fields. Some excellent
examples include (Frigyik et al., 2010; Murphy, 2012; Gelman et al., 2014;
Hoff, 2009).
  This survey is primarily a summary of purpose, significance of important
background and techniques for Gaussian mixture model, e.g., Dirichlet prior,
Chinese restaurant process, and most importantly the origin and complexity of
the methods which shed light on their modern applications. The mathematical
prerequisite is a first course in probability. Other than this modest
background, the development is self-contained, with rigorous proofs provided
throughout.",['Jun Lu'],"['cs.LG', 'cs.AI', 'stat.ML']",2021-08-20 13:23:17+00:00
http://arxiv.org/abs/2108.09160v1,State-Of-The-Art Algorithms For Low-Rank Dynamic Mode Decomposition,"This technical note reviews sate-of-the-art algorithms for linear
approximation of high-dimensional dynamical systems using low-rank dynamic mode
decomposition (DMD). While repeating several parts of our article ""low-rank
dynamic mode decomposition: an exact and tractable solution"", this work
provides additional details useful for building a comprehensive picture of
state-of-the-art methods.","['Patrick Heas', 'Cedric Herzet']","['stat.ML', 'cs.LG']",2021-08-20 13:15:31+00:00
http://arxiv.org/abs/2108.09026v2,Federated Distributionally Robust Optimization for Phase Configuration of RISs,"In this article, we study the problem of robust reconfigurable intelligent
surface (RIS)-aided downlink communication over heterogeneous RIS types in the
supervised learning setting. By modeling downlink communication over
heterogeneous RIS designs as different workers that learn how to optimize phase
configurations in a distributed manner, we solve this distributed learning
problem using a distributionally robust formulation in a
communication-efficient manner, while establishing its rate of convergence. By
doing so, we ensure that the global model performance of the worst-case worker
is close to the performance of other workers. Simulation results show that our
proposed algorithm requires fewer communication rounds (about 50% lesser) to
achieve the same worst-case distribution test accuracy compared to competitive
baselines.","['Chaouki Ben Issaid', 'Sumudu Samarakoon', 'Mehdi Bennis', 'H. Vincent Poor']","['cs.LG', 'cs.NI', 'stat.ML']",2021-08-20 07:07:45+00:00
http://arxiv.org/abs/2108.08993v1,Distributionally Robust Learning,"This monograph develops a comprehensive statistical learning framework that
is robust to (distributional) perturbations in the data using Distributionally
Robust Optimization (DRO) under the Wasserstein metric. Beginning with
fundamental properties of the Wasserstein metric and the DRO formulation, we
explore duality to arrive at tractable formulations and develop finite-sample,
as well as asymptotic, performance guarantees. We consider a series of learning
problems, including (i) distributionally robust linear regression; (ii)
distributionally robust regression with group structure in the predictors;
(iii) distributionally robust multi-output regression and multiclass
classification, (iv) optimal decision making that combines distributionally
robust regression with nearest-neighbor estimation; (v) distributionally robust
semi-supervised learning, and (vi) distributionally robust reinforcement
learning. A tractable DRO relaxation for each problem is being derived,
establishing a connection between robustness and regularization, and obtaining
bounds on the prediction and estimation errors of the solution. Beyond theory,
we include numerical experiments and case studies using synthetic and real
data. The real data experiments are all associated with various health
informatics problems, an application area which provided the initial impetus
for this work.","['Ruidi Chen', 'Ioannis Ch. Paschalidis']","['stat.ML', 'cs.LG']",2021-08-20 04:14:18+00:00
http://arxiv.org/abs/2108.08987v2,"Uniformity Testing in the Shuffle Model: Simpler, Better, Faster","Uniformity testing, or testing whether independent observations are uniformly
distributed, is the prototypical question in distribution testing. Over the
past years, a line of work has been focusing on uniformity testing under
privacy constraints on the data, and obtained private and data-efficient
algorithms under various privacy models such as central differential privacy
(DP), local privacy (LDP), pan-privacy, and, very recently, the shuffle model
of differential privacy.
  In this work, we considerably simplify the analysis of the known uniformity
testing algorithm in the shuffle model, and, using a recent result on ""privacy
amplification via shuffling,"" provide an alternative algorithm attaining the
same guarantees with an elementary and streamlined argument.","['Clément L. Canonne', 'Hongyi Lyu']","['cs.DS', 'cs.CR', 'cs.DM', 'stat.ML']",2021-08-20 03:43:12+00:00
http://arxiv.org/abs/2108.08890v2,Local Latin Hypercube Refinement for Multi-objective Design Uncertainty Optimization,"Optimizing the reliability and the robustness of a design is important but
often unaffordable due to high sample requirements. Surrogate models based on
statistical and machine learning methods are used to increase the sample
efficiency. However, for higher dimensional or multi-modal systems, surrogate
models may also require a large amount of samples to achieve good results. We
propose a sequential sampling strategy for the surrogate based solution of
multi-objective reliability based robust design optimization problems. Proposed
local Latin hypercube refinement (LoLHR) strategy is model-agnostic and can be
combined with any surrogate model because there is no free lunch but possibly a
budget one. The proposed method is compared to stationary sampling as well as
other proposed strategies from the literature. Gaussian process and support
vector regression are both used as surrogate models. Empirical evidence is
presented, showing that LoLHR achieves on average better results compared to
other surrogate based strategies on the tested examples.","['Can Bogoclu', 'Dirk Roos', 'Tamara Nestorović']","['stat.ML', 'cs.LG', 'physics.app-ph']",2021-08-19 19:46:38+00:00
http://arxiv.org/abs/2108.08887v2,Risk Bounds and Calibration for a Smart Predict-then-Optimize Method,"The predict-then-optimize framework is fundamental in practical stochastic
decision-making problems: first predict unknown parameters of an optimization
model, then solve the problem using the predicted values. A natural loss
function in this setting is defined by measuring the decision error induced by
the predicted parameters, which was named the Smart Predict-then-Optimize (SPO)
loss by Elmachtoub and Grigas [arXiv:1710.08005]. Since the SPO loss is
typically nonconvex and possibly discontinuous, Elmachtoub and Grigas
[arXiv:1710.08005] introduced a convex surrogate, called the SPO+ loss, that
importantly accounts for the underlying structure of the optimization model. In
this paper, we greatly expand upon the consistency results for the SPO+ loss
provided by Elmachtoub and Grigas [arXiv:1710.08005]. We develop risk bounds
and uniform calibration results for the SPO+ loss relative to the SPO loss,
which provide a quantitative way to transfer the excess surrogate risk to
excess true risk. By combining our risk bounds with generalization bounds, we
show that the empirical minimizer of the SPO+ loss achieves low excess true
risk with high probability. We first demonstrate these results in the case when
the feasible region of the underlying optimization problem is a polyhedron, and
then we show that the results can be strengthened substantially when the
feasible region is a level set of a strongly convex function. We perform
experiments to empirically demonstrate the strength of the SPO+ surrogate, as
compared to standard $\ell_1$ and squared $\ell_2$ prediction error losses, on
portfolio allocation and cost-sensitive multi-class classification problems.","['Heyuan Liu', 'Paul Grigas']","['cs.LG', 'math.OC', 'stat.ML']",2021-08-19 19:25:46+00:00
http://arxiv.org/abs/2108.08871v4,Structure Learning for Directed Trees,"Knowing the causal structure of a system is of fundamental interest in many
areas of science and can aid the design of prediction algorithms that work well
under manipulations to the system. The causal structure becomes identifiable
from the observational distribution under certain restrictions. To learn the
structure from data, score-based methods evaluate different graphs according to
the quality of their fits. However, for large, continuous, and nonlinear
models, these rely on heuristic optimization approaches with no general
guarantees of recovering the true causal structure. In this paper, we consider
structure learning of directed trees. We propose a fast and scalable method
based on Chu-Liu-Edmonds' algorithm we call causal additive trees (CAT). For
the case of Gaussian errors, we prove consistency in an asymptotic regime with
a vanishing identifiability gap. We also introduce two methods for testing
substructure hypotheses with asymptotic family-wise error rate control that is
valid post-selection and in unidentified settings. Furthermore, we study the
identifiability gap, which quantifies how much better the true causal model
fits the observational distribution, and prove that it is lower bounded by
local properties of the causal model. Simulation studies demonstrate the
favorable performance of CAT compared to competing structure learning methods.","['Martin Emil Jakobsen', 'Rajen D. Shah', 'Peter Bühlmann', 'Jonas Peters']","['stat.ML', 'cs.LG', '62H22, 62D20']",2021-08-19 18:38:30+00:00
http://arxiv.org/abs/2108.08843v2,Learning Equilibria in Matching Markets from Bandit Feedback,"Large-scale, two-sided matching platforms must find market outcomes that
align with user preferences while simultaneously learning these preferences
from data. Classical notions of stability (Gale and Shapley, 1962; Shapley and
Shubik, 1971) are unfortunately of limited value in the learning setting, given
that preferences are inherently uncertain and destabilizing while they are
being learned. To bridge this gap, we develop a framework and algorithms for
learning stable market outcomes under uncertainty. Our primary setting is
matching with transferable utilities, where the platform both matches agents
and sets monetary transfers between them. We design an incentive-aware learning
objective that captures the distance of a market outcome from equilibrium.
Using this objective, we analyze the complexity of learning as a function of
preference structure, casting learning as a stochastic multi-armed bandit
problem. Algorithmically, we show that ""optimism in the face of uncertainty,""
the principle underlying many bandit algorithms, applies to a primal-dual
formulation of matching with transfers and leads to near-optimal regret bounds.
Our work takes a first step toward elucidating when and how stable matchings
arise in large, data-driven marketplaces.","['Meena Jagadeesan', 'Alexander Wei', 'Yixin Wang', 'Michael I. Jordan', 'Jacob Steinhardt']","['cs.LG', 'cs.GT', 'stat.ML']",2021-08-19 17:59:28+00:00
http://arxiv.org/abs/2108.08810v2,Do Vision Transformers See Like Convolutional Neural Networks?,"Convolutional neural networks (CNNs) have so far been the de-facto model for
visual data. Recent work has shown that (Vision) Transformer models (ViT) can
achieve comparable or even superior performance on image classification tasks.
This raises a central question: how are Vision Transformers solving these
tasks? Are they acting like convolutional networks, or learning entirely
different visual representations? Analyzing the internal representation
structure of ViTs and CNNs on image classification benchmarks, we find striking
differences between the two architectures, such as ViT having more uniform
representations across all layers. We explore how these differences arise,
finding crucial roles played by self-attention, which enables early aggregation
of global information, and ViT residual connections, which strongly propagate
features from lower to higher layers. We study the ramifications for spatial
localization, demonstrating ViTs successfully preserve input spatial
information, with noticeable effects from different classification methods.
Finally, we study the effect of (pretraining) dataset scale on intermediate
features and transfer learning, and conclude with a discussion on connections
to new architectures such as the MLP-Mixer.","['Maithra Raghu', 'Thomas Unterthiner', 'Simon Kornblith', 'Chiyuan Zhang', 'Alexey Dosovitskiy']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2021-08-19 17:27:03+00:00
http://arxiv.org/abs/2108.08767v2,Learning General Halfspaces with General Massart Noise under the Gaussian Distribution,"We study the problem of PAC learning halfspaces on $\mathbb{R}^d$ with
Massart noise under the Gaussian distribution. In the Massart model, an
adversary is allowed to flip the label of each point $\mathbf{x}$ with unknown
probability $\eta(\mathbf{x}) \leq \eta$, for some parameter $\eta \in
[0,1/2]$. The goal is to find a hypothesis with misclassification error of
$\mathrm{OPT} + \epsilon$, where $\mathrm{OPT}$ is the error of the target
halfspace. This problem had been previously studied under two assumptions: (i)
the target halfspace is homogeneous (i.e., the separating hyperplane goes
through the origin), and (ii) the parameter $\eta$ is strictly smaller than
$1/2$. Prior to this work, no nontrivial bounds were known when either of these
assumptions is removed. We study the general problem and establish the
following:
  For $\eta <1/2$, we give a learning algorithm for general halfspaces with
sample and computational complexity
$d^{O_{\eta}(\log(1/\gamma))}\mathrm{poly}(1/\epsilon)$, where $\gamma
=\max\{\epsilon, \min\{\mathbf{Pr}[f(\mathbf{x}) = 1],
\mathbf{Pr}[f(\mathbf{x}) = -1]\} \}$ is the bias of the target halfspace $f$.
Prior efficient algorithms could only handle the special case of $\gamma =
1/2$. Interestingly, we establish a qualitatively matching lower bound of
$d^{\Omega(\log(1/\gamma))}$ on the complexity of any Statistical Query (SQ)
algorithm.
  For $\eta = 1/2$, we give a learning algorithm for general halfspaces with
sample and computational complexity $O_\epsilon(1) d^{O(\log(1/\epsilon))}$.
This result is new even for the subclass of homogeneous halfspaces; prior
algorithms for homogeneous Massart halfspaces provide vacuous guarantees for
$\eta=1/2$. We complement our upper bound with a nearly-matching SQ lower bound
of $d^{\Omega(\log(1/\epsilon))}$, which holds even for the special case of
homogeneous halfspaces.","['Ilias Diakonikolas', 'Daniel M. Kane', 'Vasilis Kontonis', 'Christos Tzamos', 'Nikos Zarifis']","['cs.LG', 'cs.DS', 'math.ST', 'stat.ML', 'stat.TH']",2021-08-19 16:16:48+00:00
http://arxiv.org/abs/2108.08765v1,Provably Efficient Generative Adversarial Imitation Learning for Online and Offline Setting with Linear Function Approximation,"In generative adversarial imitation learning (GAIL), the agent aims to learn
a policy from an expert demonstration so that its performance cannot be
discriminated from the expert policy on a certain predefined reward set. In
this paper, we study GAIL in both online and offline settings with linear
function approximation, where both the transition and reward function are
linear in the feature maps. Besides the expert demonstration, in the online
setting the agent can interact with the environment, while in the offline
setting the agent only accesses an additional dataset collected by a prior. For
online GAIL, we propose an optimistic generative adversarial policy
optimization algorithm (OGAP) and prove that OGAP achieves
$\widetilde{\mathcal{O}}(H^2 d^{3/2}K^{1/2}+KH^{3/2}dN_1^{-1/2})$ regret. Here
$N_1$ represents the number of trajectories of the expert demonstration, $d$ is
the feature dimension, and $K$ is the number of episodes.
  For offline GAIL, we propose a pessimistic generative adversarial policy
optimization algorithm (PGAP). For an arbitrary additional dataset, we obtain
the optimality gap of PGAP, achieving the minimax lower bound in the
utilization of the additional dataset. Assuming sufficient coverage on the
additional dataset, we show that PGAP achieves
$\widetilde{\mathcal{O}}(H^{2}dK^{-1/2}
+H^2d^{3/2}N_2^{-1/2}+H^{3/2}dN_1^{-1/2} \ )$ optimality gap. Here $N_2$
represents the number of trajectories of the additional dataset with sufficient
coverage.","['Zhihan Liu', 'Yufeng Zhang', 'Zuyue Fu', 'Zhuoran Yang', 'Zhaoran Wang']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2021-08-19 16:16:00+00:00
http://arxiv.org/abs/2108.08758v1,Parallel Quasi-concave set optimization: A new frontier that scales without needing submodularity,"Classes of set functions along with a choice of ground set are a bedrock to
determine and develop corresponding variants of greedy algorithms to obtain
efficient solutions for combinatorial optimization problems. The class of
approximate constrained submodular optimization has seen huge advances at the
intersection of good computational efficiency, versatility and approximation
guarantees while exact solutions for unconstrained submodular optimization are
NP-hard. What is an alternative to situations when submodularity does not hold?
Can efficient and globally exact solutions be obtained? We introduce one such
new frontier: The class of quasi-concave set functions induced as a dual class
to monotone linkage functions. We provide a parallel algorithm with a time
complexity over $n$ processors of $\mathcal{O}(n^2g)
+\mathcal{O}(\log{\log{n}})$ where $n$ is the cardinality of the ground set and
$g$ is the complexity to compute the monotone linkage function that induces a
corresponding quasi-concave set function via a duality. The complexity reduces
to $\mathcal{O}(gn\log(n))$ on $n^2$ processors and to $\mathcal{O}(gn)$ on
$n^3$ processors. Our algorithm provides a globally optimal solution to a
maxi-min problem as opposed to submodular optimization which is approximate. We
show a potential for widespread applications via an example of diverse feature
subset selection with exact global maxi-min guarantees upon showing that a
statistical dependency measure called distance correlation can be used to
induce a quasi-concave set function.","['Praneeth Vepakomma', 'Yulia Kempner', 'Ramesh Raskar']","['math.OC', 'cs.DC', 'math.CO', 'stat.ML']",2021-08-19 15:50:41+00:00
http://arxiv.org/abs/2108.08752v1,A Framework for an Assessment of the Kernel-target Alignment in Tree Ensemble Kernel Learning,"Kernels ensuing from tree ensembles such as random forest (RF) or gradient
boosted trees (GBT), when used for kernel learning, have been shown to be
competitive to their respective tree ensembles (particularly in higher
dimensional scenarios). On the other hand, it has been also shown that
performance of the kernel algorithms depends on the degree of the kernel-target
alignment. However, the kernel-target alignment for kernel learning based on
the tree ensembles has not been investigated and filling this gap is the main
goal of our work.
  Using the eigenanalysis of the kernel matrix, we demonstrate that for
continuous targets good performance of the tree-based kernel learning is
associated with strong kernel-target alignment. Moreover, we show that well
performing tree ensemble based kernels are characterized by strong target
aligned components that are expressed through scalar products between the
eigenvectors of the kernel matrix and the target. This suggests that when tree
ensemble based kernel learning is successful, relevant information for the
supervised problem is concentrated near lower dimensional manifold spanned by
the target aligned components. Persistence of the strong target aligned
components in tree ensemble based kernels is further supported by sensitivity
analysis via landmark learning. In addition to a comprehensive simulation
study, we also provide experimental results from several real life data sets
that are in line with the simulations.","['Dai Feng', 'Richard Baumgartner']","['stat.ML', 'cs.LG']",2021-08-19 15:37:17+00:00
http://arxiv.org/abs/2108.08734v1,odeN: Simultaneous Approximation of Multiple Motif Counts in Large Temporal Networks,"Counting the number of occurrences of small connected subgraphs, called
temporal motifs, has become a fundamental primitive for the analysis of
temporal networks, whose edges are annotated with the time of the event they
represent. One of the main complications in studying temporal motifs is the
large number of motifs that can be built even with a limited number of vertices
or edges. As a consequence, since in many applications motifs are employed for
exploratory analyses, the user needs to iteratively select and analyze several
motifs that represent different aspects of the network, resulting in an
inefficient, time-consuming process. This problem is exacerbated in large
networks, where the analysis of even a single motif is computationally
demanding. As a solution, in this work we propose and study the problem of
simultaneously counting the number of occurrences of multiple temporal motifs,
all corresponding to the same (static) topology (e.g., a triangle). Given that
for large temporal networks computing the exact counts is unfeasible, we
propose odeN, a sampling-based algorithm that provides an accurate
approximation of all the counts of the motifs. We provide analytical bounds on
the number of samples required by odeN to compute rigorous, probabilistic,
relative approximations. Our extensive experimental evaluation shows that odeN
enables the approximation of the counts of motifs in temporal networks in a
fraction of the time needed by state-of-the-art methods, and that it also
reports more accurate approximations than such methods.","['Ilie Sarpe', 'Fabio Vandin']","['cs.SI', 'cs.DS', 'stat.ML', 'G.3; F.2.m']",2021-08-19 15:06:05+00:00
http://arxiv.org/abs/2108.08712v1,Teaching Uncertainty Quantification in Machine Learning through Use Cases,"Uncertainty in machine learning is not generally taught as general knowledge
in Machine Learning course curricula. In this paper we propose a short
curriculum for a course about uncertainty in machine learning, and complement
the course with a selection of use cases, aimed to trigger discussion and let
students play with the concepts of uncertainty in a programming setting. Our
use cases cover the concept of output uncertainty, Bayesian neural networks and
weight distributions, sources of uncertainty, and out of distribution
detection. We expect that this curriculum and set of use cases motivates the
community to adopt these important concepts into courses for safety in AI.",['Matias Valdenegro-Toro'],"['cs.LG', 'stat.ML']",2021-08-19 14:22:17+00:00
http://arxiv.org/abs/2108.08670v1,On Accelerating Distributed Convex Optimizations,"This paper studies a distributed multi-agent convex optimization problem. The
system comprises multiple agents in this problem, each with a set of local data
points and an associated local cost function. The agents are connected to a
server, and there is no inter-agent communication. The agents' goal is to learn
a parameter vector that optimizes the aggregate of their local costs without
revealing their local data points. In principle, the agents can solve this
problem by collaborating with the server using the traditional distributed
gradient-descent method. However, when the aggregate cost is ill-conditioned,
the gradient-descent method (i) requires a large number of iterations to
converge, and (ii) is highly unstable against process noise. We propose an
iterative pre-conditioning technique to mitigate the deleterious effects of the
cost function's conditioning on the convergence rate of distributed
gradient-descent. Unlike the conventional pre-conditioning techniques, the
pre-conditioner matrix in our proposed technique updates iteratively to
facilitate implementation on the distributed network. In the distributed
setting, we provably show that the proposed algorithm converges linearly with
an improved rate of convergence than the traditional and adaptive
gradient-descent methods. Additionally, for the special case when the minimizer
of the aggregate cost is unique, our algorithm converges superlinearly. We
demonstrate our algorithm's superior performance compared to prominent
distributed algorithms for solving real logistic regression problems and
emulating neural network training via a noisy quadratic model, thereby
signifying the proposed algorithm's efficiency for distributively solving
non-convex optimization. Moreover, we empirically show that the proposed
algorithm results in faster training without compromising the generalization
performance.","['Kushal Chakrabarti', 'Nirupam Gupta', 'Nikhil Chopra']","['math.OC', 'cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2021-08-19 13:19:54+00:00
http://arxiv.org/abs/2108.08655v2,Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning,"Actor-critic algorithms are widely used in reinforcement learning, but are
challenging to mathematically analyse due to the online arrival of non-i.i.d.
data samples. The distribution of the data samples dynamically changes as the
model is updated, introducing a complex feedback loop between the data
distribution and the reinforcement learning algorithm. We prove that, under a
time rescaling, the online actor-critic algorithm with tabular parametrization
converges to an ordinary differential equation (ODE) as the number of updates
becomes large. The proof first establishes the geometric ergodicity of the data
samples under a fixed actor policy. Then, using a Poisson equation, we prove
that the fluctuations of the data samples around a dynamic probability measure,
which is a function of the evolving actor model, vanish as the number of
updates become large. Once the ODE limit has been derived, we study its
convergence properties using a two time-scale analysis which asymptotically
de-couples the critic ODE from the actor ODE. The convergence of the critic to
the solution of the Bellman equation and the actor to the optimal policy are
proven. In addition, a convergence rate to this global minimum is also
established. Our convergence analysis holds under specific choices for the
learning rates and exploration rates in the actor-critic algorithm, which could
provide guidance for the implementation of actor-critic algorithms in practice.","['Ziheng Wang', 'Justin Sirignano']","['cs.LG', 'math.OC', 'stat.ML']",2021-08-19 12:37:58+00:00
http://arxiv.org/abs/2108.08247v2,Geometry-informed irreversible perturbations for accelerated convergence of Langevin dynamics,"We introduce a novel geometry-informed irreversible perturbation that
accelerates convergence of the Langevin algorithm for Bayesian computation. It
is well documented that there exist perturbations to the Langevin dynamics that
preserve its invariant measure while accelerating its convergence. Irreversible
perturbations and reversible perturbations (such as Riemannian manifold
Langevin dynamics (RMLD)) have separately been shown to improve the performance
of Langevin samplers. We consider these two perturbations simultaneously by
presenting a novel form of irreversible perturbation for RMLD that is informed
by the underlying geometry. Through numerical examples, we show that this new
irreversible perturbation can improve estimation performance over irreversible
perturbations that do not take the geometry into account. Moreover we
demonstrate that irreversible perturbations generally can be implemented in
conjunction with the stochastic gradient version of the Langevin algorithm.
Lastly, while continuous-time irreversible perturbations cannot impair the
performance of a Langevin estimator, the situation can sometimes be more
complicated when discretization is considered. To this end, we describe a
discrete-time example in which irreversibility increases both the bias and
variance of the resulting estimator.","['Benjamin J. Zhang', 'Youssef M. Marzouk', 'Konstantinos Spiliopoulos']","['stat.ME', 'math.PR', 'stat.AP', 'stat.CO', 'stat.ML']",2021-08-18 17:11:29+00:00
http://arxiv.org/abs/2108.08129v2,Quantitative Uniform Stability of the Iterative Proportional Fitting Procedure,"We establish the uniform in time stability, w.r.t. the marginals, of the
Iterative Proportional Fitting Procedure, also known as Sinkhorn algorithm,
used to solve entropy-regularised Optimal Transport problems. Our result is
quantitative and stated in terms of the 1-Wasserstein metric. As a corollary we
establish a quantitative stability result for Schr\""odinger bridges.","['George Deligiannidis', 'Valentin De Bortoli', 'Arnaud Doucet']","['stat.ML', 'cs.LG', 'math.OC', 'math.PR']",2021-08-18 13:02:31+00:00
http://arxiv.org/abs/2108.08052v2,Moser Flow: Divergence-based Generative Modeling on Manifolds,"We are interested in learning generative models for complex geometries
described via manifolds, such as spheres, tori, and other implicit surfaces.
Current extensions of existing (Euclidean) generative models are restricted to
specific geometries and typically suffer from high computational costs. We
introduce Moser Flow (MF), a new class of generative models within the family
of continuous normalizing flows (CNF). MF also produces a CNF via a solution to
the change-of-variable formula, however differently from other CNF methods, its
model (learned) density is parameterized as the source (prior) density minus
the divergence of a neural network (NN). The divergence is a local, linear
differential operator, easy to approximate and calculate on manifolds.
Therefore, unlike other CNFs, MF does not require invoking or backpropagating
through an ODE solver during training. Furthermore, representing the model
density explicitly as the divergence of a NN rather than as a solution of an
ODE facilitates learning high fidelity densities. Theoretically, we prove that
MF constitutes a universal density approximator under suitable assumptions.
Empirically, we demonstrate for the first time the use of flow models for
sampling from general curved surfaces and achieve significant improvements in
density estimation, sample quality, and training complexity over existing CNFs
on challenging synthetic geometries and real-world benchmarks from the earth
and climate sciences.","['Noam Rozen', 'Aditya Grover', 'Maximilian Nickel', 'Yaron Lipman']","['stat.ML', 'cs.AI', 'cs.LG']",2021-08-18 09:00:24+00:00
http://arxiv.org/abs/2108.08038v1,Combining K-means type algorithms with Hill Climbing for Joint Stratification and Sample Allocation Designs,"In this paper we combine the k-means and/or k-means type algorithms with a
hill climbing algorithm in stages to solve the joint stratification and sample
allocation problem. This is a combinatorial optimisation problem in which we
search for the optimal stratification from the set of all possible
stratifications of basic strata. Each stratification being a solution the
quality of which is measured by its cost. This problem is intractable for
larger sets. Furthermore evaluating the cost of each solution is expensive. A
number of heuristic algorithms have already been developed to solve this
problem with the aim of finding acceptable solutions in reasonable computation
times. However, the heuristics for these algorithms need to be trained in order
to optimise performance in each instance. We compare the above multi-stage
combination of algorithms with three recent algorithms and report the solution
costs, evaluation times and training times. The multi-stage combinations
generally compare well with the recent algorithms both in the case of atomic
and continuous strata and provide the survey designer with a greater choice of
algorithms to choose from.","[""Mervyn O'Luing"", 'Steven Prestwich', 'S. Armagan Tarim']","['stat.ML', 'cs.LG']",2021-08-18 08:41:58+00:00
http://arxiv.org/abs/2108.08001v1,Nonlinear Autoregression with Convergent Dynamics on Novel Computational Platforms,"Nonlinear stochastic modeling is useful for describing complex engineering
systems. Meanwhile, neuromorphic (brain-inspired) computing paradigms are
developing to tackle tasks that are challenging and resource intensive on
digital computers. An emerging scheme is reservoir computing which exploits
nonlinear dynamical systems for temporal information processing. This paper
introduces reservoir computers with output feedback as stationary and ergodic
infinite-order nonlinear autoregressive models. We highlight the versatility of
this approach by employing classical and quantum reservoir computers to model
synthetic and real data sets, further exploring their potential for control
applications.","['J. Chen', 'H. I. Nurdin']","['eess.SY', 'cs.SY', 'quant-ph', 'stat.ML']",2021-08-18 07:01:16+00:00
http://arxiv.org/abs/2108.07992v2,On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity,"We study the multi-marginal partial optimal transport (POT) problem between
$m$ discrete (unbalanced) measures with at most $n$ supports. We first prove
that we can obtain two equivalence forms of the multimarginal POT problem in
terms of the multimarginal optimal transport problem via novel extensions of
cost tensor. The first equivalence form is derived under the assumptions that
the total masses of each measure are sufficiently close while the second
equivalence form does not require any conditions on these masses but at the
price of more sophisticated extended cost tensor. Our proof techniques for
obtaining these equivalence forms rely on novel procedures of moving mass in
graph theory to push transportation plan into appropriate regions. Finally,
based on the equivalence forms, we develop optimization algorithm, named
ApproxMPOT algorithm, that builds upon the Sinkhorn algorithm for solving the
entropic regularized multimarginal optimal transport. We demonstrate that the
ApproxMPOT algorithm can approximate the optimal value of multimarginal POT
problem with a computational complexity upper bound of the order
$\tilde{\mathcal{O}}(m^3(n+1)^{m}/ \varepsilon^2)$ where $\varepsilon > 0$
stands for the desired tolerance.","['Khang Le', 'Huy Nguyen', 'Tung Pham', 'Nhat Ho']","['stat.ML', 'cs.DS', 'cs.LG', 'math.OC', 'stat.CO']",2021-08-18 06:46:59+00:00
http://arxiv.org/abs/2108.07958v1,Semantic Perturbations with Normalizing Flows for Improved Generalization,"Data augmentation is a widely adopted technique for avoiding overfitting when
training deep neural networks. However, this approach requires domain-specific
knowledge and is often limited to a fixed set of hard-coded transformations.
Recently, several works proposed to use generative models for generating
semantically meaningful perturbations to train a classifier. However, because
accurate encoding and decoding are critical, these methods, which use
architectures that approximate the latent-variable inference, remained limited
to pilot studies on small datasets.
  Exploiting the exactly reversible encoder-decoder structure of normalizing
flows, we perform on-manifold perturbations in the latent space to define fully
unsupervised data augmentations. We demonstrate that such perturbations match
the performance of advanced data augmentation techniques -- reaching 96.6% test
accuracy for CIFAR-10 using ResNet-18 and outperform existing methods,
particularly in low data regimes -- yielding 10--25% relative improvement of
test accuracy from classical training. We find that our latent adversarial
perturbations adaptive to the classifier throughout its training are most
effective, yielding the first test accuracy improvement results on real-world
datasets -- CIFAR-10/100 -- via latent-space perturbations.","['Oguz Kaan Yuksel', 'Sebastian U. Stich', 'Martin Jaggi', 'Tatjana Chavdarova']","['stat.ML', 'cs.LG']",2021-08-18 03:20:00+00:00
http://arxiv.org/abs/2108.08687v2,Clustering dynamics on graphs: from spectral clustering to mean shift through Fokker-Planck interpolation,"In this work we build a unifying framework to interpolate between
density-driven and geometry-based algorithms for data clustering, and
specifically, to connect the mean shift algorithm with spectral clustering at
discrete and continuum levels. We seek this connection through the introduction
of Fokker-Planck equations on data graphs. Besides introducing new forms of
mean shift algorithms on graphs, we provide new theoretical insights on the
behavior of the family of diffusion maps in the large sample limit as well as
provide new connections between diffusion maps and mean shift dynamics on a
fixed graph. Several numerical examples illustrate our theoretical findings and
highlight the benefits of interpolating density-driven and geometry-based
clustering algorithms.","['Katy Craig', 'Nicolás García Trillos', 'Dejan Slepčev']","['stat.ML', 'cs.LG', 'math.AP', '62G20, 62H30, 60J27, 60J25, 35Q84, 58J35, 58J90, 28A33']",2021-08-18 02:00:33+00:00
http://arxiv.org/abs/2108.07872v1,Aggregated Customer Engagement Model,"E-commerce websites use machine learned ranking models to serve shopping
results to customers. Typically, the websites log the customer search events,
which include the query entered and the resulting engagement with the shopping
results, such as clicks and purchases. Each customer search event serves as
input training data for the models, and the individual customer engagement
serves as a signal for customer preference. So a purchased shopping result, for
example, is perceived to be more important than one that is not. However, new
or under-impressed products do not have enough customer engagement signals and
end up at a disadvantage when being ranked alongside popular products. In this
paper, we propose a novel method for data curation that aggregates all customer
engagements within a day for the same query to use as input training data. This
aggregated customer engagement gives the models a complete picture of the
relative importance of shopping results. Training models on this aggregated
data leads to less reliance on behavioral features. This helps mitigate the
cold start problem and boosted relevant new products to top search results. In
this paper, we present the offline and online analysis and results comparing
the individual and aggregated customer engagement models trained on e-commerce
data.","['Priya Gupta', 'Cuize Han']","['stat.ML', 'cs.LG']",2021-08-17 20:58:10+00:00
http://arxiv.org/abs/2108.07636v7,Accounting for shared covariates in semi-parametric Bayesian additive regression trees,"We propose some extensions to semi-parametric models based on Bayesian
additive regression trees (BART). In the semi-parametric BART paradigm, the
response variable is approximated by a linear predictor and a BART model, where
the linear component is responsible for estimating the main effects and BART
accounts for non-specified interactions and non-linearities. Previous
semi-parametric models based on BART have assumed that the set of covariates in
the linear predictor and the BART model are mutually exclusive in an attempt to
avoid poor coverage properties and reduce bias in the estimates of the
parameters in the linear predictor. The main novelty in our approach lies in
the way we change the tree-generation moves in BART to deal with this bias and
resolve non-identifiability issues between the parametric and non-parametric
components, even when they have covariates in common. This allows us to model
complex interactions involving the covariates of primary interest, both among
themselves and with those in the BART component. Our novel method is developed
with a view to analysing data from an international education assessment, where
certain predictors of students' achievements in mathematics are of particular
interpretational interest. Through additional simulation studies and another
application to a well-known benchmark dataset, we also show competitive
performance when compared to regression models, alternative formulations of
semi-parametric BART, and other tree-based methods. The implementation of the
proposed method is available at \url{https://github.com/ebprado/CSP-BART}.","['Estevão B. Prado', 'Andrew C. Parnell', 'Keefe Murphy', 'Nathan McJames', ""Ann O'Shea"", 'Rafael A. Moral']","['stat.ML', 'cs.LG']",2021-08-17 13:58:44+00:00
http://arxiv.org/abs/2108.08709v1,Neural density estimation and uncertainty quantification for laser induced breakdown spectroscopy spectra,"Constructing probability densities for inference in high-dimensional spectral
data is often intractable. In this work, we use normalizing flows on structured
spectral latent spaces to estimate such densities, enabling downstream
inference tasks. In addition, we evaluate a method for uncertainty
quantification when predicting unobserved state vectors associated with each
spectrum. We demonstrate the capability of this approach on laser-induced
breakdown spectroscopy data collected by the ChemCam instrument on the Mars
rover Curiosity. Using our approach, we are able to generate realistic spectral
samples and to accurately predict state vectors with associated well-calibrated
uncertainties. We anticipate that this methodology will enable efficient
probabilistic modeling of spectral data, leading to potential advances in
several areas, including out-of-distribution detection and sensitivity
analysis.","['Katiana Kontolati', 'Natalie Klein', 'Nishant Panda', 'Diane Oyen']","['cs.LG', 'math.SP', 'stat.ML']",2021-08-17 01:10:29+00:00
http://arxiv.org/abs/2108.07380v2,InfoGram and Admissible Machine Learning,"We have entered a new era of machine learning (ML), where the most accurate
algorithm with superior predictive power may not even be deployable, unless it
is admissible under the regulatory constraints. This has led to great interest
in developing fair, transparent and trustworthy ML methods. The purpose of this
article is to introduce a new information-theoretic learning framework
(admissible machine learning) and algorithmic risk-management tools (InfoGram,
L-features, ALFA-testing) that can guide an analyst to redesign off-the-shelf
ML methods to be regulatory compliant, while maintaining good prediction
accuracy. We have illustrated our approach using several real-data examples
from financial sectors, biomedical research, marketing campaigns, and the
criminal justice system.",['Subhadeep Mukhopadhyay'],"['stat.ML', 'cs.AI', 'cs.LG', 'econ.EM']",2021-08-17 00:04:38+00:00
http://arxiv.org/abs/2108.07313v3,Federated Asymptotics: a model to compare federated learning algorithms,"We propose an asymptotic framework to analyze the performance of
(personalized) federated learning algorithms. In this new framework, we
formulate federated learning as a multi-criterion objective, where the goal is
to minimize each client's loss using information from all of the clients. We
analyze a linear regression model where, for a given client, we may
theoretically compare the performance of various algorithms in the
high-dimensional asymptotic limit. This asymptotic multi-criterion approach
naturally models the high-dimensional, many-device nature of federated
learning. These tools make fairly precise predictions about the benefits of
personalization and information sharing in federated scenarios -- at least in
our (stylized) model -- including that Federated Averaging with simple client
fine-tuning achieves the same asymptotic risk as the more intricate
meta-learning and proximal-regularized approaches and outperforming Federated
Averaging without personalization. We evaluate these predictions on federated
versions of the EMNIST, CIFAR-100, Shakespeare, and Stack Overflow datasets,
where the experiments corroborate the theoretical predictions, suggesting such
frameworks may provide a useful guide to practical algorithmic development.","['Gary Cheng', 'Karan Chadha', 'John Duchi']","['cs.LG', 'cs.DC', 'math.OC', 'stat.ML']",2021-08-16 18:59:24+00:00
http://arxiv.org/abs/2108.07208v1,Hierarchical Infinite Relational Model,"This paper describes the hierarchical infinite relational model (HIRM), a new
probabilistic generative model for noisy, sparse, and heterogeneous relational
data. Given a set of relations defined over a collection of domains, the model
first infers multiple non-overlapping clusters of relations using a top-level
Chinese restaurant process. Within each cluster of relations, a Dirichlet
process mixture is then used to partition the domain entities and model the
probability distribution of relation values. The HIRM generalizes the standard
infinite relational model and can be used for a variety of data analysis tasks
including dependence detection, clustering, and density estimation. We present
new algorithms for fully Bayesian posterior inference via Gibbs sampling. We
illustrate the efficacy of the method on a density estimation benchmark of
twenty object-attribute datasets with up to 18 million cells and use it to
discover relational structure in real-world datasets from politics and
genomics.","['Feras A. Saad', 'Vikash K. Mansinghka']","['cs.LG', 'cs.AI', 'stat.ME', 'stat.ML']",2021-08-16 16:32:13+00:00
http://arxiv.org/abs/2108.07186v1,Robust Trimmed k-means,"Clustering is a fundamental tool in unsupervised learning, used to group
objects by distinguishing between similar and dissimilar features of a given
data set. One of the most common clustering algorithms is k-means.
Unfortunately, when dealing with real-world data many traditional clustering
algorithms are compromised by lack of clear separation between groups, noisy
observations, and/or outlying data points. Thus, robust statistical algorithms
are required for successful data analytics. Current methods that robustify
k-means clustering are specialized for either single or multi-membership data,
but do not perform competitively in both cases. We propose an extension of the
k-means algorithm, which we call Robust Trimmed k-means (RTKM) that
simultaneously identifies outliers and clusters points and can be applied to
either single- or multi-membership data. We test RTKM on various real-world
datasets and show that RTKM performs competitively with other methods on single
membership data with outliers and multi-membership data without outliers. We
also show that RTKM leverages its relative advantages to outperform other
methods on multi-membership data containing outliers.","['Olga Dorabiala', 'J. Nathan Kutz', 'Aleksandr Aravkin']","['stat.ML', 'cs.LG', 'math.OC', '90C26, 62F35', 'I.5.3']",2021-08-16 15:49:40+00:00
http://arxiv.org/abs/2108.07118v1,NIST SRE CTS Superset: A large-scale dataset for telephony speaker recognition,"This document provides a brief description of the National Institute of
Standards and Technology (NIST) speaker recognition evaluation (SRE)
conversational telephone speech (CTS) Superset. The CTS Superset has been
created in an attempt to provide the research community with a large-scale
dataset along with uniform metadata that can be used to effectively train and
develop telephony (narrowband) speaker recognition systems. It contains a large
number of telephony speech segments from more than 6800 speakers with speech
durations distributed uniformly in the [10s, 60s] range. The segments have been
extracted from the source corpora used to compile prior SRE datasets
(SRE1996-2012), including the Greybeard corpus as well as the Switchboard and
Mixer series collected by the Linguistic Data Consortium (LDC). In addition to
the brief description, we also report speaker recognition results on the NIST
2020 CTS Speaker Recognition Challenge, obtained using a system trained with
the CTS Superset. The results will serve as a reference baseline for the
challenge.",['Seyed Omid Sadjadi'],"['cs.SD', 'cs.AI', 'eess.AS', 'stat.ML']",2021-08-16 14:39:23+00:00
http://arxiv.org/abs/2108.07028v1,Non-Local Feature Aggregation on Graphs via Latent Fixed Data Structures,"In contrast to image/text data whose order can be used to perform non-local
feature aggregation in a straightforward way using the pooling layers, graphs
lack the tensor representation and mostly the element-wise max/mean function is
utilized to aggregate the locally extracted feature vectors. In this paper, we
present a novel approach for global feature aggregation in Graph Neural
Networks (GNNs) which utilizes a Latent Fixed Data Structure (LFDS) to
aggregate the extracted feature vectors. The locally extracted feature vectors
are sorted/distributed on the LFDS and a latent neural network (CNN/GNN) is
utilized to perform feature aggregation on the LFDS. The proposed approach is
used to design several novel global feature aggregation methods based on the
choice of the LFDS. We introduce multiple LFDSs including loop, 3D tensor
(image), sequence, data driven graphs and an algorithm which sorts/distributes
the extracted local feature vectors on the LFDS. While the computational
complexity of the proposed methods are linear with the order of input graphs,
they achieve competitive or better results.","['Mostafa Rahmani', 'Rasoul Shafipour', 'Ping Li']","['stat.ML', 'cs.LG']",2021-08-16 11:43:04+00:00
http://arxiv.org/abs/2108.06980v3,Task-Sensitive Concept Drift Detector with Constraint Embedding,"Detecting drifts in data is essential for machine learning applications, as
changes in the statistics of processed data typically has a profound influence
on the performance of trained models. Most of the available drift detection
methods are either supervised and require access to the true labels during
inference time, or they are completely unsupervised and aim for changes in
distributions without taking label information into account. We propose a novel
task-sensitive semi-supervised drift detection scheme, which utilizes label
information while training the initial model, but takes into account that
supervised label information is no longer available when using the model during
inference. It utilizes a constrained low-dimensional embedding representation
of the input data. This way, it is best suited for the classification task. It
is able to detect real drift, where the drift affects the classification
performance, while it properly ignores virtual drift, where the classification
performance is not affected by the drift. In the proposed framework, the actual
method to detect a change in the statistics of incoming data samples can be
chosen freely. Experimental evaluation on nine benchmarks datasets, with
different types of drift, demonstrates that the proposed framework can reliably
detect drifts, and outperforms state-of-the-art unsupervised drift detection
approaches.","['Andrea Castellani', 'Sebastian Schmitt', 'Barbara Hammer']","['cs.LG', 'cs.NE', 'stat.ML']",2021-08-16 09:10:52+00:00
http://arxiv.org/abs/2108.06953v1,Uniform Function Estimators in Reproducing Kernel Hilbert Spaces,"This paper addresses the problem of regression to reconstruct functions,
which are observed with superimposed errors at random locations. We address the
problem in reproducing kernel Hilbert spaces. It is demonstrated that the
estimator, which is often derived by employing Gaussian random fields,
converges in the mean norm of the reproducing kernel Hilbert space to the
conditional expectation and this implies local and uniform convergence of this
function estimator. By preselecting the kernel, the problem does not suffer
from the curse of dimensionality.
  The paper analyzes the statistical properties of the estimator. We derive
convergence properties and provide a conservative rate of convergence for
increasing sample sizes.","['Paul Dommel', 'Alois Pichler']","['math.ST', 'stat.ML', 'stat.TH']",2021-08-16 08:13:28+00:00
http://arxiv.org/abs/2108.06847v2,Interpreting and improving deep-learning models with reality checks,"Recent deep-learning models have achieved impressive predictive performance
by learning complex functions of many variables, often at the cost of
interpretability. This chapter covers recent work aiming to interpret models by
attributing importance to features and feature groups for a single prediction.
Importantly, the proposed attributions assign importance to interactions
between features, in addition to features in isolation. These attributions are
shown to yield insights across real-world domains, including bio-imaging,
cosmology image and natural-language processing. We then show how these
attributions can be used to directly improve the generalization of a neural
network or to distill it into a simple model. Throughout the chapter, we
emphasize the use of reality checks to scrutinize the proposed interpretation
techniques.","['Chandan Singh', 'Wooseok Ha', 'Bin Yu']","['stat.ML', 'cs.LG']",2021-08-16 00:58:15+00:00
http://arxiv.org/abs/2108.06721v2,Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time,"In several real world applications, machine learning models are deployed to
make predictions on data whose distribution changes gradually along time,
leading to a drift between the train and test distributions. Such models are
often re-trained on new data periodically, and they hence need to generalize to
data not too far into the future. In this context, there is much prior work on
enhancing temporal generalization, e.g. continuous transportation of past data,
kernel smoothed time-sensitive parameters and more recently, adversarial
learning of time-invariant features. However, these methods share several
limitations, e.g, poor scalability, training instability, and dependence on
unlabeled data from the future. Responding to the above limitations, we propose
a simple method that starts with a model with time-sensitive parameters but
regularizes its temporal complexity using a Gradient Interpolation (GI) loss.
GI allows the decision boundary to change along time and can still prevent
overfitting to the limited training time snapshots by allowing task-specific
control over changes along time. We compare our method to existing baselines on
multiple real-world datasets, which show that GI outperforms more complicated
generative and adversarial approaches on the one hand, and simpler gradient
regularization methods on the other.","['Anshul Nasery', 'Soumyadeep Thakur', 'Vihari Piratla', 'Abir De', 'Sunita Sarawagi']","['cs.LG', 'stat.ML']",2021-08-15 11:20:10+00:00
http://arxiv.org/abs/2108.06717v3,Time delay estimation of traffic congestion propagation due to accidents based on statistical causality,"The accurate estimation of time delays is crucial in traffic congestion
analysis, as this information can be used to address fundamental questions
regarding the origin and propagation of traffic congestion. However, the exact
measurement of time delays during congestion remains a challenge owing to the
complex propagation process between roads and high uncertainty regarding future
behavior. To overcome this challenge, we propose a novel time delay estimation
method for the propagation of traffic congestion due to accidents using
lag-specific transfer entropy (TE). The proposed method adopts Markov bootstrap
techniques to quantify uncertainty in the time delay estimator. To the best of
our knowledge, our proposed method is the first to estimate time delays based
on causal relationships between adjacent roads. We validated the method's
efficacy using simulated data, as well as real user trajectory data obtained
from a major GPS navigation system in South Korea.","['YongKyung Oh', 'JiIn Kwak', 'Sungil Kim']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY']",2021-08-15 10:58:59+00:00
http://arxiv.org/abs/2108.06624v1,Equity-Directed Bootstrapping: Examples and Analysis,"When faced with severely imbalanced binary classification problems, we often
train models on bootstrapped data in which the number of instances of each
class occur in a more favorable ratio, e.g., one. We view algorithmic inequity
through the lens of imbalanced classification: in order to balance the
performance of a classifier across groups, we can bootstrap to achieve training
sets that are balanced with respect to both labels and group identity. For an
example problem with severe class imbalance---prediction of suicide death from
administrative patient records---we illustrate how an equity-directed bootstrap
can bring test set sensitivities and specificities much closer to satisfying
the equal odds criterion. In the context of na\""ive Bayes and logistic
regression, we analyze the equity-directed bootstrap, demonstrating that it
works by bringing odds ratios close to one, and linking it to methods involving
intercept adjustment, thresholding, and weighting.","['Harish S. Bhat', 'Majerle E. Reeves', 'Sidra Goldman-Mellor']","['stat.ML', 'cs.LG', 'stat.CO']",2021-08-14 22:09:27+00:00
http://arxiv.org/abs/2108.06552v3,Continual Semi-Supervised Learning through Contrastive Interpolation Consistency,"Continual Learning (CL) investigates how to train Deep Networks on a stream
of tasks without incurring forgetting. CL settings proposed in literature
assume that every incoming example is paired with ground-truth annotations.
However, this clashes with many real-world applications: gathering labeled
data, which is in itself tedious and expensive, becomes infeasible when data
flow as a stream. This work explores Continual Semi-Supervised Learning (CSSL):
here, only a small fraction of labeled input examples are shown to the learner.
We assess how current CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER)
perform in this novel and challenging scenario, where overfitting entangles
forgetting. Subsequently, we design a novel CSSL method that exploits metric
learning and consistency regularization to leverage unlabeled examples while
learning. We show that our proposal exhibits higher resilience to diminishing
supervision and, even more surprisingly, relying only on 25% supervision
suffices to outperform SOTA methods trained under full supervision.","['Matteo Boschini', 'Pietro Buzzega', 'Lorenzo Bonicelli', 'Angelo Porrello', 'Simone Calderara']","['stat.ML', 'cs.LG']",2021-08-14 14:38:20+00:00
http://arxiv.org/abs/2108.06446v1,A fast asynchronous MCMC sampler for sparse Bayesian inference,"We propose a very fast approximate Markov Chain Monte Carlo (MCMC) sampling
framework that is applicable to a large class of sparse Bayesian inference
problems, where the computational cost per iteration in several models is of
order $O(ns)$, where $n$ is the sample size, and $s$ the underlying sparsity of
the model. This cost can be further reduced by data sub-sampling when
stochastic gradient Langevin dynamics are employed. The algorithm is an
extension of the asynchronous Gibbs sampler of Johnson et al. (2013), but can
be viewed from a statistical perspective as a form of Bayesian iterated sure
independent screening (Fan et al. (2009)). We show that in high-dimensional
linear regression problems, the Markov chain generated by the proposed
algorithm admits an invariant distribution that recovers correctly the main
signal with high probability under some statistical assumptions. Furthermore we
show that its mixing time is at most linear in the number of regressors. We
illustrate the algorithm with several models.","['Yves Atchadé', 'Liwei Wang']","['stat.CO', 'math.ST', 'stat.ML', 'stat.TH']",2021-08-14 02:20:49+00:00
http://arxiv.org/abs/2108.06411v1,Optimal and Efficient Algorithms for General Mixable Losses against Switching Oracles,"We investigate the problem of online learning, which has gained significant
attention in recent years due to its applicability in a wide range of fields
from machine learning to game theory. Specifically, we study the online
optimization of mixable loss functions in a dynamic environment. We introduce
online mixture schemes that asymptotically achieves the performance of the best
dynamic estimation sequence of the switching oracle with optimal regret
redundancies. The best dynamic estimation sequence that we compete against is
selected in hindsight with full observation of the loss functions and is
allowed to select different optimal estimations in different time intervals
(segments). We propose two mixtures in our work. Firstly, we propose a
tractable polynomial time complexity algorithm that can achieve the optimal
redundancy of the intractable brute force approach. Secondly, we propose an
efficient logarithmic time complexity algorithm that can achieve the optimal
redundancy up to a constant multiplicity gap. Our results are guaranteed to
hold in a strong deterministic sense in an individual sequence manner.","['Kaan Gokcesu', 'Hakan Gokcesu']","['cs.LG', 'stat.ML']",2021-08-13 21:48:55+00:00
http://arxiv.org/abs/2108.06283v1,Random Subspace Mixture Models for Interpretable Anomaly Detection,"We present a new subspace-based method to construct probabilistic models for
high-dimensional data and highlight its use in anomaly detection. The approach
is based on a statistical estimation of probability density using densities of
random subspaces combined with geometric averaging. In selecting random
subspaces, equal representation of each attribute is used to ensure correct
statistical limits. Gaussian mixture models (GMMs) are used to create the
probability densities for each subspace with techniques included to mitigate
singularities allowing for the ability to handle both numerical and categorial
attributes. The number of components for each GMM is determined automatically
through Bayesian information criterion to prevent overfitting. The proposed
algorithm attains competitive AUC scores compared with prominent algorithms
against benchmark anomaly detection datasets with the added benefits of being
simple, scalable, and interpretable.","['Cetin Savkli', 'Catherine Schwartz']","['cs.LG', 'cs.AI', 'stat.ML']",2021-08-13 15:12:53+00:00
