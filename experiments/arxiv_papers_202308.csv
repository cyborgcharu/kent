id,title,abstract,authors,categories,date
http://arxiv.org/abs/2309.09111v2,Reducing sequential change detection to sequential estimation,"We consider the problem of sequential change detection, where the goal is to
design a scheme for detecting any changes in a parameter or functional $\theta$
of the data stream distribution that has small detection delay, but guarantees
control on the frequency of false alarms in the absence of changes. In this
paper, we describe a simple reduction from sequential change detection to
sequential estimation using confidence sequences: we begin a new
$(1-\alpha)$-confidence sequence at each time step, and proclaim a change when
the intersection of all active confidence sequences becomes empty. We prove
that the average run length is at least $1/\alpha$, resulting in a change
detection scheme with minimal structural assumptions~(thus allowing for
possibly dependent observations, and nonparametric distribution classes), but
strong guarantees. Our approach bears an interesting parallel with the
reduction from change detection to sequential testing of Lorden (1971) and the
e-detector of Shin et al. (2022).","['Shubhanshu Shekhar', 'Aaditya Ramdas']","['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']",2023-09-16 23:48:47+00:00
http://arxiv.org/abs/2309.09032v2,Solving Quadratic Systems with Full-Rank Matrices Using Sparse or Generative Priors,"The problem of recovering a signal $\boldsymbol x\in \mathbb{R}^n$ from a
quadratic system $\{y_i=\boldsymbol x^\top\boldsymbol A_i\boldsymbol x,\
i=1,\ldots,m\}$ with full-rank matrices $\boldsymbol A_i$ frequently arises in
applications such as unassigned distance geometry and sub-wavelength imaging.
With i.i.d. standard Gaussian matrices $\boldsymbol A_i$, this paper addresses
the high-dimensional case where $m\ll n$ by incorporating prior knowledge of
$\boldsymbol x$. First, we consider a $k$-sparse $\boldsymbol x$ and introduce
the thresholded Wirtinger flow (TWF) algorithm that does not require the
sparsity level $k$. TWF comprises two steps: the spectral initialization that
identifies a point sufficiently close to $\boldsymbol x$ (up to a sign flip)
when $m=O(k^2\log n)$, and the thresholded gradient descent which, when
provided a good initialization, produces a sequence linearly converging to
$\boldsymbol x$ with $m=O(k\log n)$ measurements. Second, we explore the
generative prior, assuming that $x$ lies in the range of an $L$-Lipschitz
continuous generative model with $k$-dimensional inputs in an $\ell_2$-ball of
radius $r$. With an estimate correlated with the signal, we develop the
projected gradient descent (PGD) algorithm that also comprises two steps: the
projected power method that provides an initial vector with
$O\big(\sqrt{\frac{k \log L}{m}}\big)$ $\ell_2$-error given $m=O(k\log(Lnr))$
measurements, and the projected gradient descent that refines the
$\ell_2$-error to $O(\delta)$ at a geometric rate when
$m=O(k\log\frac{Lrn}{\delta^2})$. Experimental results corroborate our
theoretical findings and show that: (i) our approach for the sparse case
notably outperforms the existing provable algorithm sparse power factorization;
(ii) leveraging the generative prior allows for precise image recovery in the
MNIST dataset from a small number of quadratic measurements.","['Junren Chen', 'Michael K. Ng', 'Zhaoqiang Liu']","['cs.IT', 'cs.LG', 'eess.SP', 'math.IT', 'stat.ML']",2023-09-16 16:00:07+00:00
http://arxiv.org/abs/2309.08976v1,Data-driven Reachability using Christoffel Functions and Conformal Prediction,"An important mathematical tool in the analysis of dynamical systems is the
approximation of the reach set, i.e., the set of states reachable after a given
time from a given initial state. This set is difficult to compute for complex
systems even if the system dynamics are known and given by a system of ordinary
differential equations with known coefficients. In practice, parameters are
often unknown and mathematical models difficult to obtain. Data-based
approaches are promised to avoid these difficulties by estimating the reach set
based on a sample of states. If a model is available, this training set can be
obtained through numerical simulation. In the absence of a model, real-life
observations can be used instead. A recently proposed approach for data-based
reach set approximation uses Christoffel functions to approximate the reach
set. Under certain assumptions, the approximation is guaranteed to converge to
the true solution. In this paper, we improve upon these results by notably
improving the sample efficiency and relaxing some of the assumptions by
exploiting statistical guarantees from conformal prediction with training and
calibration sets. In addition, we exploit an incremental way to compute the
Christoffel function to avoid the calibration set while maintaining the
statistical convergence guarantees. Furthermore, our approach is robust to
outliers in the training and calibration set.","['Abdelmouaiz Tebjou', 'Goran Frehse', 'Faïcel Chamroukhi']","['cs.LG', 'cs.AI', 'stat.ME', 'stat.ML']",2023-09-16 12:21:57+00:00
http://arxiv.org/abs/2309.08945v1,Inverse classification with logistic and softmax classifiers: efficient optimization,"In recent years, a certain type of problems have become of interest where one
wants to query a trained classifier. Specifically, one wants to find the
closest instance to a given input instance such that the classifier's predicted
label is changed in a desired way. Examples of these ``inverse classification''
problems are counterfactual explanations, adversarial examples and model
inversion. All of them are fundamentally optimization problems over the input
instance vector involving a fixed classifier, and it is of interest to achieve
a fast solution for interactive or real-time applications. We focus on solving
this problem efficiently for two of the most widely used classifiers: logistic
regression and softmax classifiers. Owing to special properties of these
models, we show that the optimization can be solved in closed form for logistic
regression, and iteratively but extremely fast for the softmax classifier. This
allows us to solve either case exactly (to nearly machine precision) in a
runtime of milliseconds to around a second even for very high-dimensional
instances and many classes.","['Miguel Á. Carreira-Perpiñán', 'Suryabhan Singh Hada']","['cs.LG', 'math.OC', 'stat.ML']",2023-09-16 10:34:40+00:00
http://arxiv.org/abs/2309.08923v1,Fast Approximation of the Shapley Values Based on Order-of-Addition Experimental Designs,"Shapley value is originally a concept in econometrics to fairly distribute
both gains and costs to players in a coalition game. In the recent decades, its
application has been extended to other areas such as marketing, engineering and
machine learning. For example, it produces reasonable solutions for problems in
sensitivity analysis, local model explanation towards the interpretable machine
learning, node importance in social network, attribution models, etc. However,
its heavy computational burden has been long recognized but rarely
investigated. Specifically, in a $d$-player coalition game, calculating a
Shapley value requires the evaluation of $d!$ or $2^d$ marginal contribution
values, depending on whether we are taking the permutation or combination
formulation of the Shapley value. Hence it becomes infeasible to calculate the
Shapley value when $d$ is reasonably large. A common remedy is to take a random
sample of the permutations to surrogate for the complete list of permutations.
We find an advanced sampling scheme can be designed to yield much more accurate
estimation of the Shapley value than the simple random sampling (SRS). Our
sampling scheme is based on combinatorial structures in the field of design of
experiments (DOE), particularly the order-of-addition experimental designs for
the study of how the orderings of components would affect the output. We show
that the obtained estimates are unbiased, and can sometimes deterministically
recover the original Shapley value. Both theoretical and simulations results
show that our DOE-based sampling scheme outperforms SRS in terms of estimation
accuracy. Surprisingly, it is also slightly faster than SRS. Lastly, real data
analysis is conducted for the C. elegans nervous system and the 9/11 terrorist
network.","['Liuqing Yang', 'Yongdao Zhou', 'Haoda Fu', 'Min-Qian Liu', 'Wei Zheng']","['stat.ML', 'cs.LG', 'stat.ME']",2023-09-16 08:28:15+00:00
http://arxiv.org/abs/2309.08911v1,Efficient Methods for Non-stationary Online Learning,"Non-stationary online learning has drawn much attention in recent years. In
particular, dynamic regret and adaptive regret are proposed as two principled
performance measures for online convex optimization in non-stationary
environments. To optimize them, a two-layer online ensemble is usually deployed
due to the inherent uncertainty of the non-stationarity, in which a group of
base-learners are maintained and a meta-algorithm is employed to track the best
one on the fly. However, the two-layer structure raises the concern about the
computational complexity -- those methods typically maintain $\mathcal{O}(\log
T)$ base-learners simultaneously for a $T$-round online game and thus perform
multiple projections onto the feasible domain per round, which becomes the
computational bottleneck when the domain is complicated. In this paper, we
present efficient methods for optimizing dynamic regret and adaptive regret,
which reduce the number of projections per round from $\mathcal{O}(\log T)$ to
$1$. Moreover, our obtained algorithms require only one gradient query and one
function evaluation at each round. Our technique hinges on the reduction
mechanism developed in parameter-free online learning and requires non-trivial
twists on non-stationary online methods. Empirical studies verify our
theoretical findings.","['Peng Zhao', 'Yan-Feng Xie', 'Lijun Zhang', 'Zhi-Hua Zhou']","['cs.LG', 'stat.ML']",2023-09-16 07:30:12+00:00
http://arxiv.org/abs/2309.08805v1,Learning Linearized Models from Nonlinear Systems with Finite Data,"Identifying a linear system model from data has wide applications in control
theory. The existing work on finite sample analysis for linear system
identification typically uses data from a single system trajectory under i.i.d
random inputs, and assumes that the underlying dynamics is truly linear. In
contrast, we consider the problem of identifying a linearized model when the
true underlying dynamics is nonlinear. We provide a multiple trajectories-based
deterministic data acquisition algorithm followed by a regularized least
squares algorithm, and provide a finite sample error bound on the learned
linearized dynamics. Our error bound demonstrates a trade-off between the error
due to nonlinearity and the error due to noise, and shows that one can learn
the linearized dynamics with arbitrarily small error given sufficiently many
samples. We validate our results through experiments, where we also show the
potential insufficiency of linear system identification using a single
trajectory with i.i.d random inputs, when nonlinearity does exist.","['Lei Xin', 'George Chiu', 'Shreyas Sundaram']","['eess.SY', 'cs.SY', 'stat.ML']",2023-09-15 22:58:03+00:00
http://arxiv.org/abs/2309.08783v4,Quantifying predictive uncertainty of aphasia severity in stroke patients with sparse heteroscedastic Bayesian high-dimensional regression,"Sparse linear regression methods for high-dimensional data commonly assume
that residuals have constant variance, which can be violated in practice. For
example, Aphasia Quotient (AQ) is a critical measure of language impairment and
informs treatment decisions, but it is challenging to measure in stroke
patients. It is of interest to use high-resolution T2 neuroimages of brain
damage to predict AQ. However, sparse regression models show marked evidence of
heteroscedastic error even after transformations are applied. This violation of
the homoscedasticity assumption can lead to bias in estimated coefficients,
prediction intervals (PI) with improper length, and increased type I errors.
Bayesian heteroscedastic linear regression models relax the homoscedastic error
assumption but can enforce restrictive prior assumptions on parameters, and
many are computationally infeasible in the high-dimensional setting. This paper
proposes estimating high-dimensional heteroscedastic linear regression models
using a heteroscedastic partitioned empirical Bayes Expectation Conditional
Maximization (H-PROBE) algorithm. H-PROBE is a computationally efficient
maximum a posteriori estimation approach that requires minimal prior
assumptions and can incorporate covariates hypothesized to impact
heterogeneity. We apply this method by using high-dimensional neuroimages to
predict and provide PIs for AQ that accurately quantify predictive uncertainty.
Our analysis demonstrates that H-PROBE can provide narrower PI widths than
standard methods without sacrificing coverage. Narrower PIs are clinically
important for determining the risk of moderate to severe aphasia. Additionally,
through extensive simulation studies, we exhibit that H-PROBE results in
superior prediction, variable selection, and predictive inference compared to
alternative methods.","['Anja Zgodic', 'Ray Bai', 'Jiajia Zhang', 'Yuan Wang', 'Chris Rorden', 'Alexander McLain']","['stat.ME', 'stat.ML']",2023-09-15 22:06:29+00:00
http://arxiv.org/abs/2309.08710v2,Clustered Multi-Agent Linear Bandits,"We address in this paper a particular instance of the multi-agent linear
stochastic bandit problem, called clustered multi-agent linear bandits. In this
setting, we propose a novel algorithm leveraging an efficient collaboration
between the agents in order to accelerate the overall optimization problem. In
this contribution, a network controller is responsible for estimating the
underlying cluster structure of the network and optimizing the experiences
sharing among agents within the same groups. We provide a theoretical analysis
for both the regret minimization problem and the clustering quality. Through
empirical evaluation against state-of-the-art algorithms on both synthetic and
real data, we demonstrate the effectiveness of our approach: our algorithm
significantly improves regret minimization while managing to recover the true
underlying cluster partitioning.","['Hamza Cherkaoui', 'Merwan Barlier', 'Igor Colin']","['cs.LG', 'stat.ML']",2023-09-15 19:01:42+00:00
http://arxiv.org/abs/2309.08709v1,Price of Safety in Linear Best Arm Identification,"We introduce the safe best-arm identification framework with linear feedback,
where the agent is subject to some stage-wise safety constraint that linearly
depends on an unknown parameter vector. The agent must take actions in a
conservative way so as to ensure that the safety constraint is not violated
with high probability at each round. Ways of leveraging the linear structure
for ensuring safety has been studied for regret minimization, but not for
best-arm identification to the best our knowledge. We propose a gap-based
algorithm that achieves meaningful sample complexity while ensuring the
stage-wise safety. We show that we pay an extra term in the sample complexity
due to the forced exploration phase incurred by the additional safety
constraint. Experimental illustrations are provided to justify the design of
our algorithm.","['Xuedong Shang', 'Igor Colin', 'Merwan Barlier', 'Hamza Cherkaoui']","['stat.ML', 'cs.LG']",2023-09-15 19:01:21+00:00
http://arxiv.org/abs/2309.08598v1,Projected Langevin dynamics and a gradient flow for entropic optimal transport,"The classical (overdamped) Langevin dynamics provide a natural algorithm for
sampling from its invariant measure, which uniquely minimizes an energy
functional over the space of probability measures, and which concentrates
around the minimizer(s) of the associated potential when the noise parameter is
small. We introduce analogous diffusion dynamics that sample from an
entropy-regularized optimal transport, which uniquely minimizes the same energy
functional but constrained to the set $\Pi(\mu,\nu)$ of couplings of two given
marginal probability measures $\mu$ and $\nu$ on $\mathbb{R}^d$, and which
concentrates around the optimal transport coupling(s) for small regularization
parameter. More specifically, our process satisfies two key properties: First,
the law of the solution at each time stays in $\Pi(\mu,\nu)$ if it is
initialized there. Second, the long-time limit is the unique solution of an
entropic optimal transport problem. In addition, we show by means of a new
log-Sobolev-type inequality that the convergence holds exponentially fast, for
sufficiently large regularization parameter and for a class of marginals which
strictly includes all strongly log-concave measures. By studying the induced
Wasserstein geometry of the submanifold $\Pi(\mu,\nu)$, we argue that the SDE
can be viewed as a Wasserstein gradient flow on this space of couplings, at
least when $d=1$, and we identify a conjectural gradient flow for $d \ge 2$.
The main technical difficulties stems from the appearance of conditional
expectation terms which serve to constrain the dynamics to $\Pi(\mu,\nu)$.","['Giovanni Conforti', 'Daniel Lacker', 'Soumik Pal']","['math.PR', 'math.AP', 'stat.ML', '49Q22, 60H30']",2023-09-15 17:55:56+00:00
http://arxiv.org/abs/2309.08570v1,"Neural Network Driven, Interactive Design for Nonlinear Optical Molecules Based on Group Contribution Method","A Lewis-mode group contribution method (LGC) -- multi-stage Bayesian neural
network (msBNN) -- evolutionary algorithm (EA) framework is reported for
rational design of D-Pi-A type organic small-molecule nonlinear optical
materials is presented. Upon combination of msBNN and corrected Lewis-mode
group contribution method (cLGC), different optical properties of molecules are
afforded accurately and efficiently - by using only a small data set for
training. Moreover, by employing the EA model designed specifically for LGC,
structural search is well achievable. The logical origins of the well
performance of the framework are discussed in detail. Considering that such a
theory guided, machine learning framework combines chemical principles and
data-driven tools, most likely, it will be proven efficient to solve molecular
design related problems in wider fields.","['Jinming Fan', 'Chao Qian', 'Shaodong Zhou']","['stat.ML', 'cs.LG', 'physics.optics']",2023-09-15 17:36:27+00:00
http://arxiv.org/abs/2309.08489v1,Towards Word-Level End-to-End Neural Speaker Diarization with Auxiliary Network,"While standard speaker diarization attempts to answer the question ""who
spoken when"", most of relevant applications in reality are more interested in
determining ""who spoken what"". Whether it is the conventional modularized
approach or the more recent end-to-end neural diarization (EEND), an additional
automatic speech recognition (ASR) model and an orchestration algorithm are
required to associate the speaker labels with recognized words. In this paper,
we propose Word-level End-to-End Neural Diarization (WEEND) with auxiliary
network, a multi-task learning algorithm that performs end-to-end ASR and
speaker diarization in the same neural architecture. That is, while speech is
being recognized, speaker labels are predicted simultaneously for each
recognized word. Experimental results demonstrate that WEEND outperforms the
turn-based diarization baseline system on all 2-speaker short-form scenarios
and has the capability to generalize to audio lengths of 5 minutes. Although
3+speaker conversations are harder, we find that with enough in-domain training
data, WEEND has the potential to deliver high quality diarized text.","['Yiling Huang', 'Weiran Wang', 'Guanlong Zhao', 'Hank Liao', 'Wei Xia', 'Quan Wang']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2023-09-15 15:48:45+00:00
http://arxiv.org/abs/2309.08473v2,On some limitations of data-driven weather forecasting models,"As in many other areas of engineering and applied science, Machine Learning
(ML) is having a profound impact in the domain of Weather and Climate
Prediction. A very recent development in this area has been the emergence of
fully data-driven ML prediction models which routinely claim superior
performance to that of traditional physics-based models. In this work, we
examine some aspects of the forecasts produced by an exemplar of the current
generation of ML models, Pangu-Weather, with a focus on the fidelity and
physical consistency of those forecasts and how these characteristics relate to
perceived forecast performance. The main conclusion is that Pangu-Weather
forecasts, and possibly those of similar ML models, do not have the fidelity
and physical consistency of physics-based models and their advantage in
accuracy on traditional deterministic metrics of forecast skill can be at least
partly attributed to these peculiarities. Balancing forecast skill and physical
consistency of ML-driven predictions will be an important consideration for
future ML models. However, and similarly to other modern post-processing
technologies, the current ML models appear to be already able to add value to
standard NWP output for specific forecast applications and combined with their
extremely low computational cost during deployment, are set to provide an
additional, useful source of forecast information. .",['Massimo Bonavita'],"['stat.ML', 'cs.LG', 'physics.ao-ph']",2023-09-15 15:21:57+00:00
http://arxiv.org/abs/2309.08436v2,Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition,"We study a streamable attention-based encoder-decoder model in which either
the decoder, or both the encoder and decoder, operate on pre-defined,
fixed-size windows called chunks. A special end-of-chunk (EOC) symbol advances
from one chunk to the next chunk, effectively replacing the conventional
end-of-sequence symbol. This modification, while minor, situates our model as
equivalent to a transducer model that operates on chunks instead of frames,
where EOC corresponds to the blank symbol. We further explore the remaining
differences between a standard transducer and our model. Additionally, we
examine relevant aspects such as long-form speech generalization, beam size,
and length normalization. Through experiments on Librispeech and TED-LIUM-v2,
and by concatenating consecutive sequences for long-form trials, we find that
our streamable model maintains competitive performance compared to the
non-streamable variant and generalizes very well to long-form speech.","['Mohammad Zeineldeen', 'Albert Zeyer', 'Ralf Schlüter', 'Hermann Ney']","['eess.AS', 'cs.SD', 'stat.ML']",2023-09-15 14:36:24+00:00
http://arxiv.org/abs/2309.08429v1,IHT-Inspired Neural Network for Single-Snapshot DOA Estimation with Sparse Linear Arrays,"Single-snapshot direction-of-arrival (DOA) estimation using sparse linear
arrays (SLAs) has gained significant attention in the field of automotive MIMO
radars. This is due to the dynamic nature of automotive settings, where
multiple snapshots aren't accessible, and the importance of minimizing hardware
costs. Low-rank Hankel matrix completion has been proposed to interpolate the
missing elements in SLAs. However, the solvers of matrix completion, such as
iterative hard thresholding (IHT), heavily rely on expert knowledge of
hyperparameter tuning and lack task-specificity. Besides, IHT involves
truncated-singular value decomposition (t-SVD), which has high computational
cost in each iteration. In this paper, we propose an IHT-inspired neural
network for single-snapshot DOA estimation with SLAs, termed IHT-Net. We
utilize a recurrent neural network structure to parameterize the IHT algorithm.
Additionally, we integrate shallow-layer autoencoders to replace t-SVD,
reducing computational overhead while generating a novel optimizer through
supervised learning. IHT-Net maintains strong interpretability as its network
layer operations align with the iterations of the IHT algorithm. The learned
optimizer exhibits fast convergence and higher accuracy in the full array
signal reconstruction followed by single-snapshot DOA estimation. Numerical
results validate the effectiveness of the proposed method.","['Yunqiao Hu', 'Shunqiao Sun']","['eess.SP', 'stat.ML']",2023-09-15 14:30:38+00:00
http://arxiv.org/abs/2309.08406v1,Constraint-Free Structure Learning with Smooth Acyclic Orientations,"The structure learning problem consists of fitting data generated by a
Directed Acyclic Graph (DAG) to correctly reconstruct its arcs. In this
context, differentiable approaches constrain or regularize the optimization
problem using a continuous relaxation of the acyclicity property. The
computational cost of evaluating graph acyclicity is cubic on the number of
nodes and significantly affects scalability. In this paper we introduce COSMO,
a constraint-free continuous optimization scheme for acyclic structure
learning. At the core of our method, we define a differentiable approximation
of an orientation matrix parameterized by a single priority vector. Differently
from previous work, our parameterization fits a smooth orientation matrix and
the resulting acyclic adjacency matrix without evaluating acyclicity at any
step. Despite the absence of explicit constraints, we prove that COSMO always
converges to an acyclic solution. In addition to being asymptotically faster,
our empirical analysis highlights how COSMO performance on graph reconstruction
compares favorably with competing structure learning methods.","['Riccardo Massidda', 'Francesco Landolfi', 'Martina Cinquini', 'Davide Bacciu']","['cs.LG', 'stat.ML']",2023-09-15 14:08:09+00:00
http://arxiv.org/abs/2309.08313v2,Conditional validity of heteroskedastic conformal regression,"Conformal prediction, and split conformal prediction as a specific
implementation, offer a distribution-free approach to estimating prediction
intervals with statistical guarantees. Recent work has shown that split
conformal prediction can produce state-of-the-art prediction intervals when
focusing on marginal coverage, i.e. on a calibration dataset the method
produces on average prediction intervals that contain the ground truth with a
predefined coverage level. However, such intervals are often not adaptive,
which can be problematic for regression problems with heteroskedastic noise.
This paper tries to shed new light on how prediction intervals can be
constructed, using methods such as normalized and Mondrian conformal
prediction, in such a way that they adapt to the heteroskedasticity of the
underlying process. Theoretical and experimental results are presented in which
these methods are compared in a systematic way. In particular, it is shown how
the conditional validity of a chosen conformal predictor can be related to
(implicit) assumptions about the data-generating distribution.","['Nicolas Dewolf', 'Bernard De Baets', 'Willem Waegeman']","['stat.ML', 'cs.LG']",2023-09-15 11:10:46+00:00
http://arxiv.org/abs/2309.08256v1,Sampling-Free Probabilistic Deep State-Space Models,"Many real-world dynamical systems can be described as State-Space Models
(SSMs). In this formulation, each observation is emitted by a latent state,
which follows first-order Markovian dynamics. A Probabilistic Deep SSM
(ProDSSM) generalizes this framework to dynamical systems of unknown parametric
form, where the transition and emission models are described by neural networks
with uncertain weights. In this work, we propose the first deterministic
inference algorithm for models of this type. Our framework allows efficient
approximations for training and testing. We demonstrate in our experiments that
our new method can be employed for a variety of tasks and enjoys a superior
balance between predictive performance and computational budget.","['Andreas Look', 'Melih Kandemir', 'Barbara Rakitsch', 'Jan Peters']","['cs.LG', 'stat.ML']",2023-09-15 09:06:23+00:00
http://arxiv.org/abs/2309.08249v3,Deep Nonnegative Matrix Factorization with Beta Divergences,"Deep Nonnegative Matrix Factorization (deep NMF) has recently emerged as a
valuable technique for extracting multiple layers of features across different
scales. However, all existing deep NMF models and algorithms have primarily
centered their evaluation on the least squares error, which may not be the most
appropriate metric for assessing the quality of approximations on diverse
datasets. For instance, when dealing with data types such as audio signals and
documents, it is widely acknowledged that $\beta$-divergences offer a more
suitable alternative. In this paper, we develop new models and algorithms for
deep NMF using some $\beta$-divergences, with a focus on the Kullback-Leibler
divergence. Subsequently, we apply these techniques to the extraction of facial
features, the identification of topics within document collections, and the
identification of materials within hyperspectral images.","['Valentin Leplat', 'Le Thi Khanh Hien', 'Akwum Onwunta', 'Nicolas Gillis']","['cs.LG', 'cs.NA', 'eess.SP', 'math.NA', 'stat.ML']",2023-09-15 08:46:53+00:00
http://arxiv.org/abs/2309.08241v1,Topological Node2vec: Enhanced Graph Embedding via Persistent Homology,"Node2vec is a graph embedding method that learns a vector representation for
each node of a weighted graph while seeking to preserve relative proximity and
global structure. Numerical experiments suggest Node2vec struggles to recreate
the topology of the input graph. To resolve this we introduce a topological
loss term to be added to the training loss of Node2vec which tries to align the
persistence diagram (PD) of the resulting embedding as closely as possible to
that of the input graph. Following results in computational optimal transport,
we carefully adapt entropic regularization to PD metrics, allowing us to
measure the discrepancy between PDs in a differentiable way. Our modified loss
function can then be minimized through gradient descent to reconstruct both the
geometry and the topology of the input graph. We showcase the benefits of this
approach using demonstrative synthetic examples.","['Yasuaki Hiraoka', 'Yusuke Imoto', 'Killian Meehan', 'Théo Lacombe', 'Toshiaki Yachimura']","['stat.ML', 'cs.LG', 'math.AT', 'math.OC']",2023-09-15 08:31:26+00:00
http://arxiv.org/abs/2309.08044v1,How many Neurons do we need? A refined Analysis for Shallow Networks trained with Gradient Descent,"We analyze the generalization properties of two-layer neural networks in the
neural tangent kernel (NTK) regime, trained with gradient descent (GD). For
early stopped GD we derive fast rates of convergence that are known to be
minimax optimal in the framework of non-parametric regression in reproducing
kernel Hilbert spaces. On our way, we precisely keep track of the number of
hidden neurons required for generalization and improve over existing results.
We further show that the weights during training remain in a vicinity around
initialization, the radius being dependent on structural assumptions such as
degree of smoothness of the regression function and eigenvalue decay of the
integral operator associated to the NTK.","['Mike Nguyen', 'Nicole Mücke']","['stat.ML', 'cs.LG']",2023-09-14 22:10:28+00:00
http://arxiv.org/abs/2309.07982v1,Uncertainty quantification for learned ISTA,"Model-based deep learning solutions to inverse problems have attracted
increasing attention in recent years as they bridge state-of-the-art numerical
performance with interpretability. In addition, the incorporated prior domain
knowledge can make the training more efficient as the smaller number of
parameters allows the training step to be executed with smaller datasets.
Algorithm unrolling schemes stand out among these model-based learning
techniques. Despite their rapid advancement and their close connection to
traditional high-dimensional statistical methods, they lack certainty estimates
and a theory for uncertainty quantification is still elusive. This work
provides a step towards closing this gap proposing a rigorous way to obtain
confidence intervals for the LISTA estimator.","['Frederik Hoppe', 'Claudio Mayrink Verdun', 'Felix Krahmer', 'Hannah Laus', 'Holger Rauhut']","['stat.ML', 'cs.IT', 'cs.LG', 'eess.IV', 'eess.SP', 'math.IT']",2023-09-14 18:39:07+00:00
http://arxiv.org/abs/2309.07893v2,Choosing a Proxy Metric from Past Experiments,"In many randomized experiments, the treatment effect of the long-term metric
(i.e. the primary outcome of interest) is often difficult or infeasible to
measure. Such long-term metrics are often slow to react to changes and
sufficiently noisy they are challenging to faithfully estimate in short-horizon
experiments. A common alternative is to measure several short-term proxy
metrics in the hope they closely track the long-term metric -- so they can be
used to effectively guide decision-making in the near-term. We introduce a new
statistical framework to both define and construct an optimal proxy metric for
use in a homogeneous population of randomized experiments. Our procedure first
reduces the construction of an optimal proxy metric in a given experiment to a
portfolio optimization problem which depends on the true latent treatment
effects and noise level of experiment under consideration. We then denoise the
observed treatment effects of the long-term metric and a set of proxies in a
historical corpus of randomized experiments to extract estimates of the latent
treatment effects for use in the optimization problem. One key insight derived
from our approach is that the optimal proxy metric for a given experiment is
not apriori fixed; rather it should depend on the sample size (or effective
noise level) of the randomized experiment for which it is deployed. To
instantiate and evaluate our framework, we employ our methodology in a large
corpus of randomized experiments from an industrial recommendation system and
construct proxy metrics that perform favorably relative to several baselines.","['Nilesh Tripuraneni', 'Lee Richardson', ""Alexander D'Amour"", 'Jacopo Soriano', 'Steve Yadlowsky']","['stat.ME', 'cs.LG', 'stat.ML']",2023-09-14 17:43:02+00:00
http://arxiv.org/abs/2309.07882v1,Scalable Model-Based Gaussian Process Clustering,"Gaussian process is an indispensable tool in clustering functional data,
owing to it's flexibility and inherent uncertainty quantification. However,
when the functional data is observed over a large grid (say, of length $p$),
Gaussian process clustering quickly renders itself infeasible, incurring
$O(p^2)$ space complexity and $O(p^3)$ time complexity per iteration; and thus
prohibiting it's natural adaptation to large environmental applications. To
ensure scalability of Gaussian process clustering in such applications, we
propose to embed the popular Vecchia approximation for Gaussian processes at
the heart of the clustering task, provide crucial theoretical insights towards
algorithmic design, and finally develop a computationally efficient expectation
maximization (EM) algorithm. Empirical evidence of the utility of our proposal
is provided via simulations and analysis of polar temperature anomaly
(\href{https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/global/time-series}{noaa.gov})
data-sets.","['Anirban Chakraborty', 'Abhisek Chakraborty']","['stat.CO', 'stat.AP', 'stat.ML']",2023-09-14 17:28:49+00:00
http://arxiv.org/abs/2309.07867v4,Beta Diffusion,"We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.","['Mingyuan Zhou', 'Tianqi Chen', 'Zhendong Wang', 'Huangjie Zheng']","['cs.LG', 'cs.AI', 'stat.CO', 'stat.ME', 'stat.ML']",2023-09-14 17:14:26+00:00
http://arxiv.org/abs/2309.07810v5,Spectrum-Aware Debiasing: A Modern Inference Framework with Applications to Principal Components Regression,"Debiasing is a fundamental concept in high-dimensional statistics. While
degrees-of-freedom adjustment is the state-of-the-art technique in
high-dimensional linear regression, it is limited to i.i.d. samples and
sub-Gaussian covariates. These constraints hinder its broader practical use.
Here, we introduce Spectrum-Aware Debiasing--a novel method for
high-dimensional regression. Our approach applies to problems with structured
dependencies, heavy tails, and low-rank structures. Our method achieves
debiasing through a rescaled gradient descent step, deriving the rescaling
factor using spectral information of the sample covariance matrix. The
spectrum-based approach enables accurate debiasing in much broader contexts. We
study the common modern regime where the number of features and samples scale
proportionally. We establish asymptotic normality of our proposed estimator
(suitably centered and scaled) under various convergence notions when the
covariates are right-rotationally invariant. Such designs have garnered recent
attention due to their crucial role in compressed sensing. Furthermore, we
devise a consistent estimator for its asymptotic variance.
  Our work has two notable by-products: first, we use Spectrum-Aware Debiasing
to correct bias in principal components regression (PCR), providing the first
debiased PCR estimator in high dimensions. Second, we introduce a principled
test for checking alignment between the signal and the eigenvectors of the
sample covariance matrix. This test is independently valuable for statistical
methods developed using approximate message passing, leave-one-out, or convex
Gaussian min-max theorems. We demonstrate our method through simulated and real
data experiments. Technically, we connect approximate message passing
algorithms with debiasing and provide the first proof of the Cauchy property of
vector approximate message passing (V-AMP).","['Yufan Li', 'Pragya Sur']","['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']",2023-09-14 15:58:30+00:00
http://arxiv.org/abs/2309.07779v2,Convergence analysis of online algorithms for vector-valued kernel regression,"We consider the problem of approximating the regression function from noisy
vector-valued data by an online learning algorithm using an appropriate
reproducing kernel Hilbert space (RKHS) as prior. In an online algorithm,
i.i.d. samples become available one by one by a random process and are
successively processed to build approximations to the regression function. We
are interested in the asymptotic performance of such online approximation
algorithms and show that the expected squared error in the RKHS norm can be
bounded by $C^2 (m+1)^{-s/(2+s)}$, where $m$ is the current number of processed
data, the parameter $0<s\leq 1$ expresses an additional smoothness assumption
on the regression function and the constant $C$ depends on the variance of the
input noise, the smoothness of the regression function and further parameters
of the algorithm.","['Michael Griebel', 'Peter Oswald']","['stat.ML', 'cs.NA', 'math.NA', '65D15, 65F08, 65F10, 68W27']",2023-09-14 15:10:47+00:00
http://arxiv.org/abs/2309.07703v2,Causal Entropy and Information Gain for Measuring Causal Control,"Artificial intelligence models and methods commonly lack causal
interpretability. Despite the advancements in interpretable machine learning
(IML) methods, they frequently assign importance to features which lack causal
influence on the outcome variable. Selecting causally relevant features among
those identified as relevant by these methods, or even before model training,
would offer a solution. Feature selection methods utilizing information
theoretical quantities have been successful in identifying statistically
relevant features. However, the information theoretical quantities they are
based on do not incorporate causality, rendering them unsuitable for such
scenarios. To address this challenge, this article proposes information
theoretical quantities that incorporate the causal structure of the system,
which can be used to evaluate causal importance of features for some given
outcome variable. Specifically, we introduce causal versions of entropy and
mutual information, termed causal entropy and causal information gain, which
are designed to assess how much control a feature provides over the outcome
variable. These newly defined quantities capture changes in the entropy of a
variable resulting from interventions on other variables. Fundamental results
connecting these quantities to the existence of causal effects are derived. The
use of causal information gain in feature selection is demonstrated,
highlighting its superiority over standard mutual information in revealing
which features provide control over a chosen outcome variable. Our
investigation paves the way for the development of methods with improved
interpretability in domains involving causation.","['Francisco Nunes Ferreira Quialheiro Simoes', 'Mehdi Dastani', 'Thijs van Ommen']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2023-09-14 13:25:42+00:00
http://arxiv.org/abs/2309.07666v1,Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning,"In this paper, we consider the intersection of two problems in machine
learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD).
On the one hand, the first considers adapting multiple heterogeneous labeled
source domains to an unlabeled target domain. On the other hand, the second
attacks the problem of synthesizing a small summary containing all the
information about the datasets. We thus consider a new problem called MSDA-DD.
To solve it, we adapt previous works in the MSDA literature, such as
Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD
method Distribution Matching. We thoroughly experiment with this novel problem
on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous
Stirred Tank Reactor, and Case Western Reserve University), where we show that,
even with as little as 1 sample per class, one achieves state-of-the-art
adaptation performance.","['Eduardo Fernandes Montesuma', 'Fred Ngolè Mboula', 'Antoine Souloumiac']","['cs.LG', 'cs.AI', 'stat.ML']",2023-09-14 12:29:41+00:00
http://arxiv.org/abs/2309.07663v1,Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE,"In the Variational Autoencoder (VAE), the variational posterior often aligns
closely with the prior, which is known as posterior collapse and hinders the
quality of representation learning. To mitigate this problem, an adjustable
hyperparameter beta has been introduced in the VAE. This paper presents a
closed-form expression to assess the relationship between the beta in VAE, the
dataset size, the posterior collapse, and the rate-distortion curve by
analyzing a minimal VAE in a high-dimensional limit. These results clarify that
a long plateau in the generalization error emerges with a relatively larger
beta. As the beta increases, the length of the plateau extends and then becomes
infinite beyond a certain beta threshold. This implies that the choice of beta,
unlike the usual regularization parameters, can induce posterior collapse
regardless of the dataset size. Thus, beta is a risky parameter that requires
careful tuning. Furthermore, considering the dataset-size dependence on the
rate-distortion curve, a relatively large dataset is required to obtain a
rate-distortion curve with high rates. Extensive numerical experiments support
our analysis.","['Yuma Ichikawa', 'Koji Hukushima']","['stat.ML', 'cs.LG']",2023-09-14 12:27:17+00:00
http://arxiv.org/abs/2309.07593v2,Statistically Valid Variable Importance Assessment through Conditional Permutations,"Variable importance assessment has become a crucial step in machine-learning
applications when using complex learners, such as deep neural networks, on
large-scale data. Removal-based importance assessment is currently the
reference approach, particularly when statistical guarantees are sought to
justify variable inclusion. It is often implemented with variable permutation
schemes. On the flip side, these approaches risk misidentifying unimportant
variables as important in the presence of correlations among covariates. Here
we develop a systematic approach for studying Conditional Permutation
Importance (CPI) that is model agnostic and computationally lean, as well as
reusable benchmarks of state-of-the-art variable importance estimators. We show
theoretically and empirically that $\textit{CPI}$ overcomes the limitations of
standard permutation importance by providing accurate type-I error control.
When used with a deep neural network, $\textit{CPI}$ consistently showed top
accuracy across benchmarks. An experiment on real-world data analysis in a
large-scale medical dataset showed that $\textit{CPI}$ provides a more
parsimonious selection of statistically significant variables. Our results
suggest that $\textit{CPI}$ can be readily used as drop-in replacement for
permutation-based methods.","['Ahmad Chamma', 'Denis A. Engemann', 'Bertrand Thirion']","['cs.LG', 'cs.AI', 'stat.ML']",2023-09-14 10:53:36+00:00
http://arxiv.org/abs/2309.07945v1,Masked Generative Modeling with Enhanced Sampling Scheme,"This paper presents a novel sampling scheme for masked non-autoregressive
generative modeling. We identify the limitations of TimeVQVAE, MaskGIT, and
Token-Critic in their sampling processes, and propose Enhanced Sampling Scheme
(ESS) to overcome these limitations. ESS explicitly ensures both sample
diversity and fidelity, and consists of three stages: Naive Iterative Decoding,
Critical Reverse Sampling, and Critical Resampling. ESS starts by sampling a
token set using the naive iterative decoding as proposed in MaskGIT, ensuring
sample diversity. Then, the token set undergoes the critical reverse sampling,
masking tokens leading to unrealistic samples. After that, critical resampling
reconstructs masked tokens until the final sampling step is reached to ensure
high fidelity. Critical resampling uses confidence scores obtained from a
self-Token-Critic to better measure the realism of sampled tokens, while
critical reverse sampling uses the structure of the quantized latent vector
space to discover unrealistic sample paths. We demonstrate significant
performance gains of ESS in both unconditional sampling and class-conditional
sampling using all the 128 datasets in the UCR Time Series archive.","['Daesoo Lee', 'Erlend Aune', 'Sara Malacarne']","['cs.LG', 'cs.AI', 'stat.ML']",2023-09-14 09:42:13+00:00
http://arxiv.org/abs/2309.07453v1,SC-MAD: Mixtures of Higher-order Networks for Data Augmentation,"The myriad complex systems with multiway interactions motivate the extension
of graph-based pairwise connections to higher-order relations. In particular,
the simplicial complex has inspired generalizations of graph neural networks
(GNNs) to simplicial complex-based models. Learning on such systems requires
large amounts of data, which can be expensive or impossible to obtain. We
propose data augmentation of simplicial complexes through both linear and
nonlinear mixup mechanisms that return mixtures of existing labeled samples. In
addition to traditional pairwise mixup, we present a convex clustering mixup
approach for a data-driven relationship among several simplicial complexes. We
theoretically demonstrate that the resultant synthetic simplicial complexes
interpolate among existing data with respect to homomorphism densities. Our
method is demonstrated on both synthetic and real-world datasets for simplicial
complex classification.","['Madeline Navarro', 'Santiago Segarra']","['stat.ML', 'cs.LG']",2023-09-14 06:25:39+00:00
http://arxiv.org/abs/2309.07418v1,"A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time","Large language models (LLMs) have played a pivotal role in revolutionizing
various facets of our daily existence. Solving attention regression is a
fundamental task in optimizing LLMs. In this work, we focus on giving a
provable guarantee for the one-layer attention network objective function
$L(X,Y) = \sum_{j_0 = 1}^n \sum_{i_0 = 1}^d ( \langle \langle \exp(
\mathsf{A}_{j_0} x ) , {\bf 1}_n \rangle^{-1} \exp( \mathsf{A}_{j_0} x ), A_{3}
Y_{*,i_0} \rangle - b_{j_0,i_0} )^2$. Here $\mathsf{A} \in \mathbb{R}^{n^2
\times d^2}$ is Kronecker product between $A_1 \in \mathbb{R}^{n \times d}$ and
$A_2 \in \mathbb{R}^{n \times d}$. $A_3$ is a matrix in $\mathbb{R}^{n \times
d}$, $\mathsf{A}_{j_0} \in \mathbb{R}^{n \times d^2}$ is the $j_0$-th block of
$\mathsf{A}$. The $X, Y \in \mathbb{R}^{d \times d}$ are variables we want to
learn. $B \in \mathbb{R}^{n \times d}$ and $b_{j_0,i_0} \in \mathbb{R}$ is one
entry at $j_0$-th row and $i_0$-th column of $B$, $Y_{*,i_0} \in \mathbb{R}^d$
is the $i_0$-column vector of $Y$, and $x \in \mathbb{R}^{d^2}$ is the
vectorization of $X$.
  In a multi-layer LLM network, the matrix $B \in \mathbb{R}^{n \times d}$ can
be viewed as the output of a layer, and $A_1= A_2 = A_3 \in \mathbb{R}^{n
\times d}$ can be viewed as the input of a layer. The matrix version of $x$ can
be viewed as $QK^\top$ and $Y$ can be viewed as $V$. We provide an iterative
greedy algorithm to train loss function $L(X,Y)$ up $\epsilon$ that runs in
$\widetilde{O}( ({\cal T}_{\mathrm{mat}}(n,n,d) + {\cal
T}_{\mathrm{mat}}(n,d,d) + d^{2\omega}) \log(1/\epsilon) )$ time. Here ${\cal
T}_{\mathrm{mat}}(a,b,c)$ denotes the time of multiplying $a \times b$ matrix
another $b \times c$ matrix, and $\omega\approx 2.37$ denotes the exponent of
matrix multiplication.","['Yeqi Gao', 'Zhao Song', 'Weixin Wang', 'Junze Yin']","['cs.DS', 'cs.LG', 'stat.ML']",2023-09-14 04:23:40+00:00
http://arxiv.org/abs/2309.08477v1,Deep Multi-Agent Reinforcement Learning for Decentralized Active Hypothesis Testing,"We consider a decentralized formulation of the active hypothesis testing
(AHT) problem, where multiple agents gather noisy observations from the
environment with the purpose of identifying the correct hypothesis. At each
time step, agents have the option to select a sampling action. These different
actions result in observations drawn from various distributions, each
associated with a specific hypothesis. The agents collaborate to accomplish the
task, where message exchanges between agents are allowed over a rate-limited
communications channel. The objective is to devise a multi-agent policy that
minimizes the Bayes risk. This risk comprises both the cost of sampling and the
joint terminal cost incurred by the agents upon making a hypothesis
declaration. Deriving optimal structured policies for AHT problems is generally
mathematically intractable, even in the context of a single agent. As a result,
recent efforts have turned to deep learning methodologies to address these
problems, which have exhibited significant success in single-agent learning
scenarios. In this paper, we tackle the multi-agent AHT formulation by
introducing a novel algorithm rooted in the framework of deep multi-agent
reinforcement learning. This algorithm, named Multi-Agent Reinforcement
Learning for AHT (MARLA), operates at each time step by having each agent map
its state to an action (sampling rule or stopping rule) using a trained deep
neural network with the goal of minimizing the Bayes risk. We present a
comprehensive set of experimental results that effectively showcase the agents'
ability to learn collaborative strategies and enhance performance using MARLA.
Furthermore, we demonstrate the superiority of MARLA over single-agent learning
approaches. Finally, we provide an open-source implementation of the MARLA
framework, for the benefit of researchers and developers in related domains.","['Hadar Szostak', 'Kobi Cohen']","['stat.ML', 'cs.LG', 'cs.MA', 'eess.SP']",2023-09-14 01:18:04+00:00
http://arxiv.org/abs/2309.08634v1,Doubly High-Dimensional Contextual Bandits: An Interpretable Model for Joint Assortment-Pricing,"Key challenges in running a retail business include how to select products to
present to consumers (the assortment problem), and how to price products (the
pricing problem) to maximize revenue or profit. Instead of considering these
problems in isolation, we propose a joint approach to assortment-pricing based
on contextual bandits. Our model is doubly high-dimensional, in that both
context vectors and actions are allowed to take values in high-dimensional
spaces. In order to circumvent the curse of dimensionality, we propose a simple
yet flexible model that captures the interactions between covariates and
actions via a (near) low-rank representation matrix. The resulting class of
models is reasonably expressive while remaining interpretable through latent
factors, and includes various structured linear bandit and pricing models as
particular cases. We propose a computationally tractable procedure that
combines an exploration/exploitation protocol with an efficient low-rank matrix
estimator, and we prove bounds on its regret. Simulation results show that this
method has lower regret than state-of-the-art methods applied to various
standard bandit and pricing models. Real-world case studies on the
assortment-pricing problem, from an industry-leading instant noodles company to
an emerging beauty start-up, underscore the gains achievable using our method.
In each case, we show at least three-fold gains in revenue or profit by our
bandit method, as well as the interpretability of the latent factor models that
are learned.","['Junhui Cai', 'Ran Chen', 'Martin J. Wainwright', 'Linda Zhao']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.AP', 'stat.ME']",2023-09-14 00:45:36+00:00
http://arxiv.org/abs/2309.07332v1,Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining,"Accurately labeling biomedical data presents a challenge. Traditional
semi-supervised learning methods often under-utilize available unlabeled data.
To address this, we propose a novel reliability-based training data cleaning
method employing inductive conformal prediction (ICP). This method capitalizes
on a small set of accurately labeled training data and leverages ICP-calculated
reliability metrics to rectify mislabeled data and outliers within vast
quantities of noisy training data. The efficacy of the method is validated
across three classification tasks within distinct modalities: filtering
drug-induced-liver-injury (DILI) literature with title and abstract, predicting
ICU admission of COVID-19 patients through CT radiomics and electronic health
records, and subtyping breast cancer using RNA-sequencing data. Varying levels
of noise to the training labels were introduced through label permutation.
Results show significant enhancements in classification performance: accuracy
enhancement in 86 out of 96 DILI experiments (up to 11.4%), AUROC and AUPRC
enhancements in all 48 COVID-19 experiments (up to 23.8% and 69.8%), and
accuracy and macro-average F1 score improvements in 47 out of 48 RNA-sequencing
experiments (up to 74.6% and 89.0%). Our method offers the potential to
substantially boost classification performance in multi-modal biomedical
machine learning tasks. Importantly, it accomplishes this without necessitating
an excessive volume of meticulously curated training data.","['Xianghao Zhan', 'Qinmei Xu', 'Yuanning Zheng', 'Guangming Lu', 'Olivier Gevaert']","['cs.LG', 'cs.AI', 'cs.CV', 'q-bio.GN', 'q-bio.QM', 'stat.AP', 'stat.ML']",2023-09-13 22:04:50+00:00
http://arxiv.org/abs/2309.07261v4,Simultaneous inference for generalized linear models with unmeasured confounders,"Tens of thousands of simultaneous hypothesis tests are routinely performed in
genomic studies to identify differentially expressed genes. However, due to
unmeasured confounders, many standard statistical approaches may be
substantially biased. This paper investigates the large-scale hypothesis
testing problem for multivariate generalized linear models in the presence of
confounding effects. Under arbitrary confounding mechanisms, we propose a
unified statistical estimation and inference framework that harnesses
orthogonal structures and integrates linear projections into three key stages.
It begins by disentangling marginal and uncorrelated confounding effects to
recover the latent coefficients. Subsequently, latent factors and primary
effects are jointly estimated through lasso-type optimization. Finally, we
incorporate projected and weighted bias-correction steps for hypothesis
testing. Theoretically, we establish the identification conditions of various
effects and non-asymptotic error bounds. We show effective Type-I error control
of asymptotic $z$-tests as sample and response sizes approach infinity.
Numerical experiments demonstrate that the proposed method controls the false
discovery rate by the Benjamini-Hochberg procedure and is more powerful than
alternative methods. By comparing single-cell RNA-seq counts from two groups of
samples, we demonstrate the suitability of adjusting confounding effects when
significant covariates are absent from the model.","['Jin-Hong Du', 'Larry Wasserman', 'Kathryn Roeder']","['stat.ME', 'cs.LG', 'q-bio.GN', 'stat.ML']",2023-09-13 18:53:11+00:00
http://arxiv.org/abs/2309.07250v1,All you need is spin: SU(2) equivariant variational quantum circuits based on spin networks,"Variational algorithms require architectures that naturally constrain the
optimisation space to run efficiently. In geometric quantum machine learning,
one achieves this by encoding group structure into parameterised quantum
circuits to include the symmetries of a problem as an inductive bias. However,
constructing such circuits is challenging as a concrete guiding principle has
yet to emerge. In this paper, we propose the use of spin networks, a form of
directed tensor network invariant under a group transformation, to devise SU(2)
equivariant quantum circuit ans\""atze -- circuits possessing spin rotation
symmetry. By changing to the basis that block diagonalises SU(2) group action,
these networks provide a natural building block for constructing parameterised
equivariant quantum circuits. We prove that our construction is mathematically
equivalent to other known constructions, such as those based on twirling and
generalised permutations, but more direct to implement on quantum hardware. The
efficacy of our constructed circuits is tested by solving the ground state
problem of SU(2) symmetric Heisenberg models on the one-dimensional triangular
lattice and on the Kagome lattice. Our results highlight that our equivariant
circuits boost the performance of quantum variational algorithms, indicating
broader applicability to other real-world problems.","['Richard D. P. East', 'Guillermo Alonso-Linaje', 'Chae-Yeun Park']","['quant-ph', 'cond-mat.stat-mech', 'cs.LG', 'stat.ML']",2023-09-13 18:38:41+00:00
http://arxiv.org/abs/2309.07110v1,Data Augmentation via Subgroup Mixup for Improving Fairness,"In this work, we propose data augmentation via pairwise mixup across
subgroups to improve group fairness. Many real-world applications of machine
learning systems exhibit biases across certain groups due to
under-representation or training data that reflects societal biases. Inspired
by the successes of mixup for improving classification performance, we develop
a pairwise mixup scheme to augment training data and encourage fair and
accurate decision boundaries for all subgroups. Data augmentation for group
fairness allows us to add new samples of underrepresented groups to balance
subpopulations. Furthermore, our method allows us to use the generalization
ability of mixup to improve both fairness and accuracy. We compare our proposed
mixup to existing data augmentation and bias mitigation approaches on both
synthetic simulations and real-world benchmark fair classification data,
demonstrating that we are able to achieve fair outcomes with robust if not
improved accuracy.","['Madeline Navarro', 'Camille Little', 'Genevera I. Allen', 'Santiago Segarra']","['stat.ML', 'cs.LG']",2023-09-13 17:32:21+00:00
http://arxiv.org/abs/2309.07065v2,Physics-informed Bayesian inference of external potentials in classical density-functional theory,"The swift progression of machine learning (ML) has not gone unnoticed in the
realm of statistical mechanics. ML techniques have attracted attention by the
classical density-functional theory (DFT) community, as they enable discovery
of free-energy functionals to determine the equilibrium-density profile of a
many-particle system. Within DFT, the external potential accounts for the
interaction of the many-particle system with an external field, thus, affecting
the density distribution. In this context, we introduce a statistical-learning
framework to infer the external potential exerted on a many-particle system. We
combine a Bayesian inference approach with the classical DFT apparatus to
reconstruct the external potential, yielding a probabilistic description of the
external potential functional form with inherent uncertainty quantification.
Our framework is exemplified with a grand-canonical one-dimensional particle
ensemble with excluded volume interactions in a confined geometry. The required
training dataset is generated using a Monte Carlo (MC) simulation where the
external potential is applied to the grand-canonical ensemble. The resulting
particle coordinates from the MC simulation are fed into the learning framework
to uncover the external potential. This eventually allows us to compute the
equilibrium density profile of the system by using the tools of DFT. Our
approach benchmarks the inferred density against the exact one calculated
through the DFT formulation with the true external potential. The proposed
Bayesian procedure accurately infers the external potential and the density
profile. We also highlight the external-potential uncertainty quantification
conditioned on the amount of available simulated data. The seemingly simple
case study introduced in this work might serve as a prototype for studying a
wide variety of applications, including adsorption and capillarity.","['Antonio Malpica-Morales', 'Peter Yatsyshin', 'Miguel A. Duran-Olivencia', 'Serafim Kalliadasis']","['cond-mat.stat-mech', 'physics.data-an', 'stat.ML']",2023-09-13 16:26:03+00:00
http://arxiv.org/abs/2309.06991v2,Unsupervised Contrast-Consistent Ranking with Language Models,"Language models contain ranking-based knowledge and are powerful solvers of
in-context ranking tasks. For instance, they may have parametric knowledge
about the ordering of countries by size or may be able to rank product reviews
by sentiment. We compare pairwise, pointwise and listwise prompting techniques
to elicit a language model's ranking knowledge. However, we find that even with
careful calibration and constrained decoding, prompting-based techniques may
not always be self-consistent in the rankings they produce. This motivates us
to explore an alternative approach that is inspired by an unsupervised probing
method called Contrast-Consistent Search (CCS). The idea is to train a probe
guided by a logical constraint: a language model's representation of a
statement and its negation must be mapped to contrastive true-false poles
consistently across multiple statements. We hypothesize that similar
constraints apply to ranking tasks where all items are related via consistent,
pairwise or listwise comparisons. To this end, we extend the binary CCS method
to Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such
as the Max-Margin Loss, Triplet Loss and an Ordinal Regression objective.
Across different models and datasets, our results confirm that CCR probing
performs better or, at least, on a par with prompting.","['Niklas Stoehr', 'Pengxiang Cheng', 'Jing Wang', 'Daniel Preotiuc-Pietro', 'Rajarshi Bhowmik']","['cs.LG', 'cs.CL', 'stat.ML']",2023-09-13 14:36:26+00:00
http://arxiv.org/abs/2309.06985v2,CARE: Large Precision Matrix Estimation for Compositional Data,"High-dimensional compositional data are prevalent in many applications. The
simplex constraint poses intrinsic challenges to inferring the conditional
dependence relationships among the components forming a composition, as encoded
by a large precision matrix. We introduce a precise specification of the
compositional precision matrix and relate it to its basis counterpart, which is
shown to be asymptotically identifiable under suitable sparsity assumptions. By
exploiting this connection, we propose a composition adaptive regularized
estimation (CARE) method for estimating the sparse basis precision matrix. We
derive rates of convergence for the estimator and provide theoretical
guarantees on support recovery and data-driven parameter tuning. Our theory
reveals an intriguing trade-off between identification and estimation, thereby
highlighting the blessing of dimensionality in compositional data analysis. In
particular, in sufficiently high dimensions, the CARE estimator achieves
minimax optimality and performs as well as if the basis were observed. We
further discuss how our framework can be extended to handle data containing
zeros, including sampling zeros and structural zeros. The advantages of CARE
over existing methods are illustrated by simulation studies and an application
to inferring microbial ecological networks in the human gut.","['Shucong Zhang', 'Huiyuan Wang', 'Wei Lin']","['stat.ME', 'math.ST', 'stat.AP', 'stat.ML', 'stat.TH']",2023-09-13 14:20:22+00:00
http://arxiv.org/abs/2309.06943v1,Effect of hyperparameters on variable selection in random forests,"Random forests (RFs) are well suited for prediction modeling and variable
selection in high-dimensional omics studies. The effect of hyperparameters of
the RF algorithm on prediction performance and variable importance estimation
have previously been investigated. However, how hyperparameters impact RF-based
variable selection remains unclear. We evaluate the effects on the Vita and the
Boruta variable selection procedures based on two simulation studies utilizing
theoretical distributions and empirical gene expression data. We assess the
ability of the procedures to select important variables (sensitivity) while
controlling the false discovery rate (FDR). Our results show that the
proportion of splitting candidate variables (mtry.prop) and the sample fraction
(sample.fraction) for the training dataset influence the selection procedures
more than the drawing strategy of the training datasets and the minimal
terminal node size. A suitable setting of the RF hyperparameters depends on the
correlation structure in the data. For weakly correlated predictor variables,
the default value of mtry is optimal, but smaller values of sample.fraction
result in larger sensitivity. In contrast, the difference in sensitivity of the
optimal compared to the default value of sample.fraction is negligible for
strongly correlated predictor variables, whereas smaller values than the
default are better in the other settings. In conclusion, the default values of
the hyperparameters will not always be suitable for identifying important
variables. Thus, adequate values differ depending on whether the aim of the
study is optimizing prediction performance or variable selection.","['Cesaire J. K. Fouodo', 'Lea L. Kronziel', 'Inke R. König', 'Silke Szymczak']","['stat.ML', 'cs.LG']",2023-09-13 13:26:10+00:00
http://arxiv.org/abs/2309.06838v2,Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy,"Additive friction stir deposition (AFSD) is a novel solid-state additive
manufacturing technique that circumvents issues of porosity, cracking, and
properties anisotropy that plague traditional powder bed fusion and directed
energy deposition approaches. However, correlations between process parameters,
thermal profiles, and resulting microstructure in AFSD remain poorly
understood. This hinders process optimization for properties. This work employs
a framework combining supervised machine learning (SML) and physics-informed
neural networks (PINNs) to predict peak temperature distribution in AFSD from
process parameters. Eight regression algorithms were implemented for SML
modeling, while four PINNs leveraged governing equations for transport, wave
propagation, heat transfer, and quantum mechanics. Across multiple statistical
measures, ensemble techniques like gradient boosting proved superior for SML,
with lowest MSE of 165.78. The integrated ML approach was also applied to
classify deposition quality from process factors, with logistic regression
delivering robust accuracy. By fusing data-driven learning and fundamental
physics, this dual methodology provides comprehensive insights into tailoring
microstructure through thermal management in AFSD. The work demonstrates the
power of bridging statistical and physics-based modeling for elucidating AM
process-property relationships.",['Akshansh Mishra'],"['cs.LG', 'math.OC', 'stat.ML']",2023-09-13 09:39:42+00:00
http://arxiv.org/abs/2309.06782v6,Improved particle-flow event reconstruction with scalable neural networks for current and future particle detectors,"Efficient and accurate algorithms are necessary to reconstruct particles in
the highly granular detectors anticipated at the High-Luminosity Large Hadron
Collider and the Future Circular Collider. We study scalable machine learning
models for event reconstruction in electron-positron collisions based on a full
detector simulation. Particle-flow reconstruction can be formulated as a
supervised learning task using tracks and calorimeter clusters. We compare a
graph neural network and kernel-based transformer and demonstrate that we can
avoid quadratic operations while achieving realistic reconstruction. We show
that hyperparameter tuning significantly improves the performance of the
models. The best graph neural network model shows improvement in the jet
transverse momentum resolution by up to 50% compared to the rule-based
algorithm. The resulting model is portable across Nvidia, AMD and Habana
hardware. Accurate and fast machine-learning based reconstruction can
significantly improve future measurements at colliders.","['Joosep Pata', 'Eric Wulff', 'Farouk Mokhtar', 'David Southwick', 'Mengke Zhang', 'Maria Girone', 'Javier Duarte']","['physics.data-an', 'cs.LG', 'hep-ex', 'physics.ins-det', 'stat.ML']",2023-09-13 08:16:15+00:00
http://arxiv.org/abs/2309.06724v2,"Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense","We aim to provide a general framework of for computational photography that
recovers the real scene from imperfect images, via the Deep Nonparametric
Convexified Filtering (DNCF). It is consists of a nonparametric deep network to
resemble the physical equations behind the image formation, such as denoising,
super-resolution, inpainting, and flash. DNCF has no parameterization dependent
on training data, therefore has a strong generalization and robustness to
adversarial image manipulation. During inference, we also encourage the network
parameters to be nonnegative and create a bi-convex function on the input and
parameters, and this adapts to second-order optimization algorithms with
insufficient running time, having 10X acceleration over Deep Image Prior. With
these tools, we empirically verify its capability to defend image
classification deep networks against adversary attack algorithms in real-time.",['Jianqiao Wangni'],"['cs.CV', 'cs.LG', 'eess.IV', 'math.OC', 'stat.ML']",2023-09-13 04:57:12+00:00
http://arxiv.org/abs/2309.06641v1,Quantum Data Center: Perspectives,"A quantum version of data centers might be significant in the quantum era. In
this paper, we introduce Quantum Data Center (QDC), a quantum version of
existing classical data centers, with a specific emphasis on combining Quantum
Random Access Memory (QRAM) and quantum networks. We argue that QDC will
provide significant benefits to customers in terms of efficiency, security, and
precision, and will be helpful for quantum computing, communication, and
sensing. We investigate potential scientific and business opportunities along
this novel research direction through hardware realization and possible
specific applications. We show the possible impacts of QDCs in business and
science, especially the machine learning and big data industries.","['Junyu Liu', 'Liang Jiang']","['quant-ph', 'cs.AI', 'cs.ET', 'cs.LG', 'stat.ML']",2023-09-12 23:24:38+00:00
http://arxiv.org/abs/2309.06634v2,$G$-Mapper: Learning a Cover in the Mapper Construction,"The Mapper algorithm is a visualization technique in topological data
analysis (TDA) that outputs a graph reflecting the structure of a given
dataset. However, the Mapper algorithm requires tuning several parameters in
order to generate a ``nice"" Mapper graph. This paper focuses on selecting the
cover parameter. We present an algorithm that optimizes the cover of a Mapper
graph by splitting a cover repeatedly according to a statistical test for
normality. Our algorithm is based on $G$-means clustering which searches for
the optimal number of clusters in $k$-means by iteratively applying the
Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model
to carefully choose the cover according to the distribution of the given data.
Experiments for synthetic and real-world datasets demonstrate that our
algorithm generates covers so that the Mapper graphs retain the essence of the
datasets, while also running significantly fast.","['Enrique Alvarado', 'Robin Belton', 'Emily Fischer', 'Kang-Ju Lee', 'Sourabh Palande', 'Sarah Percival', 'Emilie Purvine']","['cs.LG', 'math.AT', 'stat.ML']",2023-09-12 22:51:16+00:00
http://arxiv.org/abs/2309.06627v2,A Sequentially Fair Mechanism for Multiple Sensitive Attributes,"In the standard use case of Algorithmic Fairness, the goal is to eliminate
the relationship between a sensitive variable and a corresponding score.
Throughout recent years, the scientific community has developed a host of
definitions and tools to solve this task, which work well in many practical
applications. However, the applicability and effectivity of these tools and
definitions becomes less straightfoward in the case of multiple sensitive
attributes. To tackle this issue, we propose a sequential framework, which
allows to progressively achieve fairness across a set of sensitive features. We
accomplish this by leveraging multi-marginal Wasserstein barycenters, which
extends the standard notion of Strong Demographic Parity to the case with
multiple sensitive characteristics. This method also provides a closed-form
solution for the optimal, sequentially fair predictor, permitting a clear
interpretation of inter-sensitive feature correlations. Our approach seamlessly
extends to approximate fairness, enveloping a framework accommodating the
trade-off between risk and unfairness. This extension permits a targeted
prioritization of fairness improvements for a specific attribute within a set
of sensitive attributes, allowing for a case specific adaptation. A data-driven
estimation procedure for the derived solution is developed, and comprehensive
numerical experiments are conducted on both synthetic and real datasets. Our
empirical findings decisively underscore the practical efficacy of our
post-processing approach in fostering fair decision-making.","['François Hu', 'Philipp Ratz', 'Arthur Charpentier']","['stat.ML', 'cs.CY', 'cs.LG']",2023-09-12 22:31:57+00:00
http://arxiv.org/abs/2309.06622v1,On the Contraction Coefficient of the Schrödinger Bridge for Stochastic Linear Systems,"Schr\""{o}dinger bridge is a stochastic optimal control problem to steer a
given initial state density to another, subject to controlled diffusion and
deadline constraints. A popular method to numerically solve the Schr\""{o}dinger
bridge problems, in both classical and in the linear system settings, is via
contractive fixed point recursions. These recursions can be seen as dynamic
versions of the well-known Sinkhorn iterations, and under mild assumptions,
they solve the so-called Schr\""{o}dinger systems with guaranteed linear
convergence. In this work, we study a priori estimates for the contraction
coefficients associated with the convergence of respective Schr\""{o}dinger
systems. We provide new geometric and control-theoretic interpretations for the
same. Building on these newfound interpretations, we point out the possibility
of improved computation for the worst-case contraction coefficients of linear
SBPs by preconditioning the endpoint support sets.","['Alexis M. H. Teter', 'Yongxin Chen', 'Abhishek Halder']","['math.OC', 'cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2023-09-12 22:24:05+00:00
http://arxiv.org/abs/2309.07176v2,Optimal and Fair Encouragement Policy Evaluation and Learning,"In consequential domains, it is often impossible to compel individuals to
take treatment, so that optimal policy rules are merely suggestions in the
presence of human non-adherence to treatment recommendations. In these same
domains, there may be heterogeneity both in who responds in taking-up
treatment, and heterogeneity in treatment efficacy. While optimal treatment
rules can maximize causal outcomes across the population, access parity
constraints or other fairness considerations can be relevant in the case of
encouragement. For example, in social services, a persistent puzzle is the gap
in take-up of beneficial services among those who may benefit from them the
most. When in addition the decision-maker has distributional preferences over
both access and average outcomes, the optimal decision rule changes. We study
causal identification, statistical variance-reduced estimation, and robust
estimation of optimal treatment rules, including under potential violations of
positivity. We consider fairness constraints such as demographic parity in
treatment take-up, and other constraints, via constrained optimization. Our
framework can be extended to handle algorithmic recommendations under an
often-reasonable covariate-conditional exclusion restriction, using our
robustness checks for lack of positivity in the recommendation. We develop a
two-stage algorithm for solving over parametrized policy classes under general
constraints to obtain variance-sensitive regret bounds. We illustrate the
methods in two case studies based on data from randomized encouragement to
enroll in insurance and from pretrial supervised release with electronic
monitoring.",['Angela Zhou'],"['cs.LG', 'stat.ML']",2023-09-12 20:45:30+00:00
http://arxiv.org/abs/2309.06413v1,On Computationally Efficient Learning of Exponential Family Distributions,"We consider the classical problem of learning, with arbitrary accuracy, the
natural parameters of a $k$-parameter truncated \textit{minimal} exponential
family from i.i.d. samples in a computationally and statistically efficient
manner. We focus on the setting where the support as well as the natural
parameters are appropriately bounded. While the traditional maximum likelihood
estimator for this class of exponential family is consistent, asymptotically
normal, and asymptotically efficient, evaluating it is computationally hard. In
this work, we propose a novel loss function and a computationally efficient
estimator that is consistent as well as asymptotically normal under mild
conditions. We show that, at the population level, our method can be viewed as
the maximum likelihood estimation of a re-parameterized distribution belonging
to the same class of exponential family. Further, we show that our estimator
can be interpreted as a solution to minimizing a particular Bregman score as
well as an instance of minimizing the \textit{surrogate} likelihood. We also
provide finite sample guarantees to achieve an error (in $\ell_2$-norm) of
$\alpha$ in the parameter estimation with sample complexity $O({\sf
poly}(k)/\alpha^2)$. Our method achives the order-optimal sample complexity of
$O({\sf log}(k)/\alpha^2)$ when tailored for node-wise-sparse Markov random
fields. Finally, we demonstrate the performance of our estimator via numerical
experiments.","['Abhin Shah', 'Devavrat Shah', 'Gregory W. Wornell']","['cs.LG', 'stat.ML']",2023-09-12 17:25:32+00:00
http://arxiv.org/abs/2309.06349v1,Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors,"Thompson sampling (TS) is one of the most popular and earliest algorithms to
solve stochastic multi-armed bandit problems. We consider a variant of TS,
named $\alpha$-TS, where we use a fractional or $\alpha$-posterior
($\alpha\in(0,1)$) instead of the standard posterior distribution. To compute
an $\alpha$-posterior, the likelihood in the definition of the standard
posterior is tempered with a factor $\alpha$. For $\alpha$-TS we obtain both
instance-dependent $\mathcal{O}\left(\sum_{k \neq i^*}
\Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$
and instance-independent $\mathcal{O}(\sqrt{KT\log K})$ frequentist regret
bounds under very mild conditions on the prior and reward distributions, where
$\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and the
best arms, and $C(\alpha)$ is a known constant. Both the sub-Gaussian and
exponential family models satisfy our general conditions on the reward
distribution. Our conditions on the prior distribution just require its density
to be positive, continuous, and bounded. We also establish another
instance-dependent regret upper bound that matches (up to constants) to that of
improved UCB [Auer and Ortner, 2010]. Our regret analysis carefully combines
recent theoretical developments in the non-asymptotic concentration analysis
and Bernstein-von Mises type results for the $\alpha$-posterior distribution.
Moreover, our analysis does not require additional structural properties such
as closed-form posteriors or conjugate priors.","['Prateek Jaiswal', 'Debdeep Pati', 'Anirban Bhattacharya', 'Bani K. Mallick']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY', 'math.OC', 'math.ST', 'stat.TH']",2023-09-12 16:15:33+00:00
http://arxiv.org/abs/2309.06299v2,Modeling Supply and Demand in Public Transportation Systems,"We propose two neural network based and data-driven supply and demand models
to analyze the efficiency, identify service gaps, and determine the significant
predictors of demand, in the bus system for the Department of Public
Transportation (HDPT) in Harrisonburg City, Virginia, which is the home to
James Madison University (JMU). The supply and demand models, one temporal and
one spatial, take many variables into account, including the demographic data
surrounding the bus stops, the metrics that the HDPT reports to the federal
government, and the drastic change in population between when JMU is on or off
session. These direct and data-driven models to quantify supply and demand and
identify service gaps can generalize to other cities' bus systems.","['Miranda Bihler', 'Hala Nelson', 'Erin Okey', 'Noe Reyes Rivas', 'John Webb', 'Anna White']","['cs.LG', 'stat.AP', 'stat.ML', '00A69, 62-07, 62P30']",2023-09-12 15:05:11+00:00
http://arxiv.org/abs/2309.06240v2,Calibration in Machine Learning Uncertainty Quantification: beyond consistency to target adaptivity,"Reliable uncertainty quantification (UQ) in machine learning (ML) regression
tasks is becoming the focus of many studies in materials and chemical science.
It is now well understood that average calibration is insufficient, and most
studies implement additional methods testing the conditional calibration with
respect to uncertainty, i.e. consistency. Consistency is assessed mostly by
so-called reliability diagrams. There exists however another way beyond average
calibration, which is conditional calibration with respect to input features,
i.e. adaptivity. In practice, adaptivity is the main concern of the final users
of a ML-UQ method, seeking for the reliability of predictions and uncertainties
for any point in features space. This article aims to show that consistency and
adaptivity are complementary validation targets, and that a good consistency
does not imply a good adaptivity. Adapted validation methods are proposed and
illustrated on a representative example.",['Pascal Pernot'],"['stat.ML', 'cs.LG', 'physics.chem-ph', 'physics.data-an']",2023-09-12 13:58:04+00:00
http://arxiv.org/abs/2309.06230v1,A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models,"Analysis of high-dimensional data has led to increased interest in both
single index models (SIMs) and best subset selection. SIMs provide an
interpretable and flexible modeling framework for high-dimensional data, while
best subset selection aims to find a sparse model from a large set of
predictors. However, best subset selection in high-dimensional models is known
to be computationally intractable. Existing methods tend to relax the
selection, but do not yield the best subset solution. In this paper, we
directly tackle the intractability by proposing the first provably scalable
algorithm for best subset selection in high-dimensional SIMs. Our algorithmic
solution enjoys the subset selection consistency and has the oracle property
with a high probability. The algorithm comprises a generalized information
criterion to determine the support size of the regression coefficients,
eliminating the model selection tuning. Moreover, our method does not assume an
error distribution or a specific link function and hence is flexible to apply.
Extensive simulation results demonstrate that our method is not only
computationally efficient but also able to exactly recover the best subset in
various settings (e.g., linear regression, Poisson regression, heteroscedastic
models).","['Borui Tang', 'Jin Zhu', 'Junxian Zhu', 'Xueqin Wang', 'Heping Zhang']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2023-09-12 13:48:06+00:00
http://arxiv.org/abs/2309.06166v1,Certified Robust Models with Slack Control and Large Lipschitz Constants,"Despite recent success, state-of-the-art learning-based models remain highly
vulnerable to input changes such as adversarial examples. In order to obtain
certifiable robustness against such perturbations, recent work considers
Lipschitz-based regularizers or constraints while at the same time increasing
prediction margin. Unfortunately, this comes at the cost of significantly
decreased accuracy. In this paper, we propose a Calibrated Lipschitz-Margin
Loss (CLL) that addresses this issue and improves certified robustness by
tackling two problems: Firstly, commonly used margin losses do not adjust the
penalties to the shrinking output distribution; caused by minimizing the
Lipschitz constant $K$. Secondly, and most importantly, we observe that
minimization of $K$ can lead to overly smooth decision functions. This limits
the model's complexity and thus reduces accuracy. Our CLL addresses these
issues by explicitly calibrating the loss w.r.t. margin and Lipschitz constant,
thereby establishing full control over slack and improving robustness
certificates even with larger Lipschitz constants. On CIFAR-10, CIFAR-100 and
Tiny-ImageNet, our models consistently outperform losses that leave the
constant unattended. On CIFAR-100 and Tiny-ImageNet, CLL improves upon
state-of-the-art deterministic $L_2$ robust accuracies. In contrast to current
trends, we unlock potential of much smaller models without $K=1$ constraints.","['Max Losch', 'David Stutz', 'Bernt Schiele', 'Mario Fritz']","['cs.LG', 'cs.CV', 'stat.ML']",2023-09-12 12:23:49+00:00
http://arxiv.org/abs/2309.05925v2,On Regularized Sparse Logistic Regression,"Sparse logistic regression is for classification and feature selection
simultaneously. Although many studies have been done to solve
$\ell_1$-regularized logistic regression, there is no equivalently abundant
work on solving sparse logistic regression with nonconvex regularization term.
In this paper, we propose a unified framework to solve $\ell_1$-regularized
logistic regression, which can be naturally extended to nonconvex
regularization term, as long as certain requirement is satisfied. In addition,
we also utilize a different line search criteria to guarantee monotone
convergence for various regularization terms. Empirical experiments on binary
classification tasks with real-world datasets demonstrate our proposed
algorithms are capable of performing classification and feature selection
effectively at a lower computational cost.","['Mengyuan Zhang', 'Kai Liu']","['cs.LG', 'cs.AI', 'stat.ML']",2023-09-12 02:52:40+00:00
http://arxiv.org/abs/2309.05878v1,Reaction coordinate flows for model reduction of molecular kinetics,"In this work, we introduce a flow based machine learning approach, called
reaction coordinate (RC) flow, for discovery of low-dimensional kinetic models
of molecular systems. The RC flow utilizes a normalizing flow to design the
coordinate transformation and a Brownian dynamics model to approximate the
kinetics of RC, where all model parameters can be estimated in a data-driven
manner. In contrast to existing model reduction methods for molecular kinetics,
RC flow offers a trainable and tractable model of reduced kinetics in
continuous time and space due to the invertibility of the normalizing flow.
Furthermore, the Brownian dynamics-based reduced kinetic model investigated in
this work yields a readily discernible representation of metastable states
within the phase space of the molecular system. Numerical experiments
demonstrate how effectively the proposed method discovers interpretable and
accurate low-dimensional representations of given full-state kinetics from
simulations.","['Hao Wu', 'Frank Noé']","['cs.LG', 'math.DS', 'physics.chem-ph', 'physics.data-an', 'stat.ML']",2023-09-11 23:59:18+00:00
http://arxiv.org/abs/2309.05862v1,Subgroup detection in linear growth curve models with generalized linear mixed model (GLMM) trees,"Growth curve models are popular tools for studying the development of a
response variable within subjects over time. Heterogeneity between subjects is
common in such models, and researchers are typically interested in explaining
or predicting this heterogeneity. We show how generalized linear mixed effects
model (GLMM) trees can be used to identify subgroups with differently shaped
trajectories in linear growth curve models. Originally developed for clustered
cross-sectional data, GLMM trees are extended here to longitudinal data. The
resulting extended GLMM trees are directly applicable to growth curve models as
an important special case. In simulated and real-world data, we assess the
performance of the extensions and compare against other partitioning methods
for growth curve models. Extended GLMM trees perform more accurately than the
original algorithm and LongCART, and similarly accurate as structural equation
model (SEM) trees. In addition, GLMM trees allow for modeling both discrete and
continuous time series, are less sensitive to (mis-)specification of the
random-effects structure and are much faster to compute.","['Marjolein Fokkema', 'Achim Zeileis']","['stat.ME', 'stat.ML']",2023-09-11 22:53:13+00:00
http://arxiv.org/abs/2309.05838v1,Liu-type Shrinkage Estimators for Mixture of Poisson Regressions with Experts: A Heart Disease Study,"Count data play a critical role in medical research, such as heart disease.
The Poisson regression model is a common technique for evaluating the impact of
a set of covariates on the count responses. The mixture of Poisson regression
models with experts is a practical tool to exploit the covariates, not only to
handle the heterogeneity in the Poisson regressions but also to learn the
mixing structure of the population. Multicollinearity is one of the most common
challenges with regression models, leading to ill-conditioned design matrices
of Poisson regression components and expert classes. The maximum likelihood
method produces unreliable and misleading estimates for the effects of the
covariates in multicollinearity. In this research, we develop Ridge and
Liu-type methods as two shrinkage approaches to cope with the ill-conditioned
design matrices of the mixture of Poisson regression models with experts.
Through various numerical studies, we demonstrate that the shrinkage methods
offer more reliable estimates for the coefficients of the mixture model in
multicollinearity while maintaining the classification performance of the ML
method. The shrinkage methods are finally applied to a heart study to analyze
the heart disease rate stages.","['Elsayed Ghanem', 'Moein Yoosefi', 'Armin Hatefi']","['stat.ME', 'stat.CO', 'stat.ML']",2023-09-11 21:44:43+00:00
http://arxiv.org/abs/2309.05812v1,Interpretable learning of effective dynamics for multiscale systems,"The modeling and simulation of high-dimensional multiscale systems is a
critical challenge across all areas of science and engineering. It is broadly
believed that even with today's computer advances resolving all spatiotemporal
scales described by the governing equations remains a remote target. This
realization has prompted intense efforts to develop model order reduction
techniques. In recent years, techniques based on deep recurrent neural networks
have produced promising results for the modeling and simulation of complex
spatiotemporal systems and offer large flexibility in model development as they
can incorporate experimental and computational data. However, neural networks
lack interpretability, which limits their utility and generalizability across
complex systems. Here we propose a novel framework of Interpretable Learning
Effective Dynamics (iLED) that offers comparable accuracy to state-of-the-art
recurrent neural network-based approaches while providing the added benefit of
interpretability. The iLED framework is motivated by Mori-Zwanzig and Koopman
operator theory, which justifies the choice of the specific architecture. We
demonstrate the effectiveness of the proposed framework in simulations of three
benchmark multiscale systems. Our results show that the iLED framework can
generate accurate predictions and obtain interpretable dynamics, making it a
promising approach for solving high-dimensional multiscale systems.","['Emmanuel Menier', 'Sebastian Kaltenbach', 'Mouadh Yagoubi', 'Marc Schoenauer', 'Petros Koumoutsakos']","['stat.ML', 'cs.LG', 'physics.comp-ph']",2023-09-11 20:29:38+00:00
http://arxiv.org/abs/2309.05795v1,On the Fine-Grained Hardness of Inverting Generative Models,"The objective of generative model inversion is to identify a size-$n$ latent
vector that produces a generative model output that closely matches a given
target. This operation is a core computational primitive in numerous modern
applications involving computer vision and NLP. However, the problem is known
to be computationally challenging and NP-hard in the worst case. This paper
aims to provide a fine-grained view of the landscape of computational hardness
for this problem. We establish several new hardness lower bounds for both exact
and approximate model inversion. In exact inversion, the goal is to determine
whether a target is contained within the range of a given generative model.
Under the strong exponential time hypothesis (SETH), we demonstrate that the
computational complexity of exact inversion is lower bounded by $\Omega(2^n)$
via a reduction from $k$-SAT; this is a strengthening of known results. For the
more practically relevant problem of approximate inversion, the goal is to
determine whether a point in the model range is close to a given target with
respect to the $\ell_p$-norm. When $p$ is a positive odd integer, under SETH,
we provide an $\Omega(2^n)$ complexity lower bound via a reduction from the
closest vectors problem (CVP). Finally, when $p$ is even, under the exponential
time hypothesis (ETH), we provide a lower bound of $2^{\Omega (n)}$ via a
reduction from Half-Clique and Vertex-Cover.","['Feyza Duman Keles', 'Chinmay Hegde']","['stat.ML', 'cs.CC', 'cs.LG']",2023-09-11 20:03:25+00:00
http://arxiv.org/abs/2309.05751v3,Compressive Mahalanobis Metric Learning Adapts to Intrinsic Dimension,"Metric learning aims at finding a suitable distance metric over the input
space, to improve the performance of distance-based learning algorithms. In
high-dimensional settings, it can also serve as dimensionality reduction by
imposing a low-rank restriction to the learnt metric. In this paper, we
consider the problem of learning a Mahalanobis metric, and instead of training
a low-rank metric on high-dimensional data, we use a randomly compressed
version of the data to train a full-rank metric in this reduced feature space.
We give theoretical guarantees on the error for Mahalanobis metric learning,
which depend on the stable dimension of the data support, but not on the
ambient dimension. Our bounds make no assumptions aside from i.i.d. data
sampling from a bounded support, and automatically tighten when benign
geometrical structures are present. An important ingredient is an extension of
Gordon's theorem, which may be of independent interest. We also corroborate our
findings by numerical experiments.","['Efstratios Palias', 'Ata Kabán']","['cs.LG', 'stat.ML']",2023-09-11 18:15:51+00:00
http://arxiv.org/abs/2309.05697v3,21cmEMU: an emulator of 21cmFAST summary observables,"Recent years have witnessed rapid progress in observations of the Epoch of
Reionization (EoR). These have enabled high-dimensional inference of galaxy and
intergalactic medium (IGM) properties during the first billion years of our
Universe. However, even using efficient, semi-numerical simulations,
traditional inference approaches that compute 3D lightcones on-the-fly can take
$10^5$ core hours. Here we present 21cmEMU: an emulator of several summary
observables from the popular 21cmFAST simulation code. 21cmEMU takes as input
nine parameters characterizing EoR galaxies, and outputs the following summary
statistics: (i) the IGM mean neutral fraction; (ii) the 21-cm power spectrum;
(iii) the mean 21-cm spin temperature; (iv) the sky-averaged (global) 21-cm
signal; (v) the ultraviolet (UV) luminosity functions (LFs); and (vi) the
Thomson scattering optical depth to the cosmic microwave background (CMB). All
observables are predicted with sub-percent median accuracy, with a reduction of
the computational cost by a factor of over 10$^4$. After validating inference
results, we showcase a few applications, including: (i) quantifying the
relative constraining power of different observational datasets; (ii) seeing
how recent claims of a late EoR impact previous inferences; and (iii)
forecasting upcoming constraints from the sixth observing season of the
Hydrogen Epoch of Reionization Array (HERA) telescope. 21cmEMU is
publicly-available, and is included as an alternative simulator in the public
21CMMC sampler.","['Daniela Breitman', 'Andrei Mesinger', 'Steven Murray', 'David Prelogovic', 'Yuxiang Qin', 'Roberto Trotta']","['astro-ph.CO', 'astro-ph.GA', 'stat.ML']",2023-09-11 18:00:00+00:00
http://arxiv.org/abs/2309.05657v2,On the quality of randomized approximations of Tukey's depth,"Tukey's depth (or halfspace depth) is a widely used measure of centrality for
multivariate data. However, exact computation of Tukey's depth is known to be a
hard problem in high dimensions. As a remedy, randomized approximations of
Tukey's depth have been proposed. In this paper we explore when such randomized
algorithms return a good approximation of Tukey's depth. We study the case when
the data are sampled from a log-concave isotropic distribution. We prove that,
if one requires that the algorithm runs in polynomial time in the dimension,
the randomized algorithm correctly approximates the maximal depth $1/2$ and
depths close to zero. On the other hand, for any point of intermediate depth,
any good approximation requires exponential complexity.","['Simon Briend', 'Gábor Lugosi', 'Roberto Imbuzeiro Oliveira']","['stat.ML', 'cs.LG', 'math.PR']",2023-09-11 17:52:28+00:00
http://arxiv.org/abs/2309.05630v2,Boundary Peeling: Outlier Detection Method Using One-Class Peeling,"Unsupervised outlier detection constitutes a crucial phase within data
analysis and remains a dynamic realm of research. A good outlier detection
algorithm should be computationally efficient, robust to tuning parameter
selection, and perform consistently well across diverse underlying data
distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier
detection algorithm. One-class Boundary Peeling uses the average signed
distance from iteratively-peeled, flexible boundaries generated by one-class
support vector machines. One-class Boundary Peeling has robust hyperparameter
settings and, for increased flexibility, can be cast as an ensemble method. In
synthetic data simulations One-Class Boundary Peeling outperforms all state of
the art methods when no outliers are present while maintaining comparable or
superior performance in the presence of outliers, as compared to benchmark
methods. One-Class Boundary Peeling performs competitively in terms of correct
classification, AUC, and processing time using common benchmark data sets.","['Sheikh Arafat', 'Na Sun', 'Maria L. Weese', 'Waldyn G. Martinez']","['stat.ML', 'cs.LG', 'stat.ME']",2023-09-11 17:19:07+00:00
http://arxiv.org/abs/2309.05505v1,Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning,"Repeated parameter sharing in federated learning causes significant
information leakage about private data, thus defeating its main purpose: data
privacy. Mitigating the risk of this information leakage, using state of the
art differentially private algorithms, also does not come for free. Randomized
mechanisms can prevent convergence of models on learning even the useful
representation functions, especially if there is more disagreement between
local models on the classification functions (due to data heterogeneity). In
this paper, we consider a representation federated learning objective that
encourages various parties to collaboratively refine the consensus part of the
model, with differential privacy guarantees, while separately allowing
sufficient freedom for local personalization (without releasing it). We prove
that in the linear representation setting, while the objective is non-convex,
our proposed new algorithm \DPFEDREP\ converges to a ball centered around the
\emph{global optimal} solution at a linear rate, and the radius of the ball is
proportional to the reciprocal of the privacy budget. With this novel utility
analysis, we improve the SOTA utility-privacy trade-off for this problem by a
factor of $\sqrt{d}$, where $d$ is the input dimension. We empirically evaluate
our method with the image classification task on CIFAR10, CIFAR100, and EMNIST,
and observe a significant performance improvement over the prior work under the
same small privacy budget. The code can be found in this link:
https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.","['Zebang Shen', 'Jiayuan Ye', 'Anmin Kang', 'Hamed Hassani', 'Reza Shokri']","['cs.LG', 'stat.ML']",2023-09-11 14:46:55+00:00
http://arxiv.org/abs/2309.05306v1,Comprehensive analysis of synthetic learning applied to neonatal brain MRI segmentation,"Brain segmentation from neonatal MRI images is a very challenging task due to
large changes in the shape of cerebral structures and variations in signal
intensities reflecting the gestational process. In this context, there is a
clear need for segmentation techniques that are robust to variations in image
contrast and to the spatial configuration of anatomical structures. In this
work, we evaluate the potential of synthetic learning, a contrast-independent
model trained using synthetic images generated from the ground truth labels of
very few subjects.We base our experiments on the dataset released by the
developmental Human Connectome Project, for which high-quality T1- and
T2-weighted images are available for more than 700 babies aged between 26 and
45 weeks post-conception. First, we confirm the impressive performance of a
standard Unet trained on a few T2-weighted volumes, but also confirm that such
models learn intensity-related features specific to the training domain. We
then evaluate the synthetic learning approach and confirm its robustness to
variations in image contrast by reporting the capacity of such a model to
segment both T1- and T2-weighted images from the same individuals. However, we
observe a clear influence of the age of the baby on the predictions. We improve
the performance of this model by enriching the synthetic training set with
realistic motion artifacts and over-segmentation of the white matter. Based on
extensive visual assessment, we argue that the better performance of the model
trained on real T2w data may be due to systematic errors in the ground truth.
We propose an original experiment combining two definitions of the ground truth
allowing us to show that learning from real data will reproduce any systematic
bias from the training set, while synthetic models can avoid this limitation.
Overall, our experiments confirm that synthetic learning is an effective
solution for segmenting neonatal brain MRI. Our adapted synthetic learning
approach combines key features that will be instrumental for large multi-site
studies and clinical applications.","['R Valabregue', 'F Girka', 'A Pron', 'F Rousseau', 'G Auzias']",['stat.ML'],2023-09-11 08:51:24+00:00
http://arxiv.org/abs/2309.05292v1,The fine print on tempered posteriors,"We conduct a detailed investigation of tempered posteriors and uncover a
number of crucial and previously undiscussed points. Contrary to previous
results, we first show that for realistic models and datasets and the tightly
controlled case of the Laplace approximation to the posterior, stochasticity
does not in general improve test accuracy. The coldest temperature is often
optimal. One might think that Bayesian models with some stochasticity can at
least obtain improvements in terms of calibration. However, we show empirically
that when gains are obtained this comes at the cost of degradation in test
accuracy. We then discuss how targeting Frequentist metrics using Bayesian
models provides a simple explanation of the need for a temperature parameter
$\lambda$ in the optimization objective. Contrary to prior works, we finally
show through a PAC-Bayesian analysis that the temperature $\lambda$ cannot be
seen as simply fixing a misspecified prior or likelihood.","['Konstantinos Pitas', 'Julyan Arbel']","['cs.LG', 'stat.ML']",2023-09-11 08:21:42+00:00
http://arxiv.org/abs/2309.05153v5,Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood,"Training energy-based models (EBMs) on high-dimensional data can be both
challenging and time-consuming, and there exists a noticeable gap in sample
quality between EBMs and other generative frameworks like GANs and diffusion
models. To close this gap, inspired by the recent efforts of learning EBMs by
maximizing diffusion recovery likelihood (DRL), we propose cooperative
diffusion recovery likelihood (CDRL), an effective approach to tractably learn
and sample from a series of EBMs defined on increasingly noisy versions of a
dataset, paired with an initializer model for each EBM. At each noise level,
the two models are jointly estimated within a cooperative training framework:
samples from the initializer serve as starting points that are refined by a few
MCMC sampling steps from the EBM. The EBM is then optimized by maximizing
recovery likelihood, while the initializer model is optimized by learning from
the difference between the refined samples and the initial samples. In
addition, we made several practical designs for EBM training to further improve
the sample quality. Combining these advances, our approach significantly boost
the generation performance compared to existing EBM methods on CIFAR-10 and
ImageNet datasets. We also demonstrate the effectiveness of our models for
several downstream tasks, including classifier-free guided generation,
compositional generation, image inpainting and out-of-distribution detection.","['Yaxuan Zhu', 'Jianwen Xie', 'Yingnian Wu', 'Ruiqi Gao']","['stat.ML', 'cs.LG']",2023-09-10 22:05:24+00:00
http://arxiv.org/abs/2309.05145v1,Outlier Robust Adversarial Training,"Supervised learning models are challenged by the intrinsic complexities of
training data such as outliers and minority subpopulations and intentional
attacks at inference time with adversarial samples. While traditional robust
learning methods and the recent adversarial training approaches are designed to
handle each of the two challenges, to date, no work has been done to develop
models that are robust with regard to the low-quality training data and the
potential adversarial attack at inference time simultaneously. It is for this
reason that we introduce Outlier Robust Adversarial Training (ORAT) in this
work. ORAT is based on a bi-level optimization formulation of adversarial
training with a robust rank-based loss function. Theoretically, we show that
the learning objective of ORAT satisfies the $\mathcal{H}$-consistency in
binary classification, which establishes it as a proper surrogate to
adversarial 0/1 loss. Furthermore, we analyze its generalization ability and
provide uniform convergence rates in high probability. ORAT can be optimized
with a simple algorithm. Experimental evaluations on three benchmark datasets
demonstrate the effectiveness and robustness of ORAT in handling outliers and
adversarial attacks. Our code is available at
https://github.com/discovershu/ORAT.","['Shu Hu', 'Zhenhuan Yang', 'Xin Wang', 'Yiming Ying', 'Siwei Lyu']","['cs.LG', 'cs.AI', 'stat.ML']",2023-09-10 21:36:38+00:00
http://arxiv.org/abs/2309.05132v1,DAD++: Improved Data-free Test Time Adversarial Defense,"With the increasing deployment of deep neural networks in safety-critical
applications such as self-driving cars, medical imaging, anomaly detection,
etc., adversarial robustness has become a crucial concern in the reliability of
these networks in real-world scenarios. A plethora of works based on
adversarial training and regularization-based techniques have been proposed to
make these deep networks robust against adversarial attacks. However, these
methods require either retraining models or training them from scratch, making
them infeasible to defend pre-trained models when access to training data is
restricted. To address this problem, we propose a test time Data-free
Adversarial Defense (DAD) containing detection and correction frameworks.
Moreover, to further improve the efficacy of the correction framework in cases
when the detector is under-confident, we propose a soft-detection scheme
(dubbed as ""DAD++""). We conduct a wide range of experiments and ablations on
several datasets and network architectures to show the efficacy of our proposed
approach. Furthermore, we demonstrate the applicability of our approach in
imparting adversarial defense at test time under data-free (or data-efficient)
applications/setups, such as Data-free Knowledge Distillation and Source-free
Unsupervised Domain Adaptation, as well as Semi-supervised classification
frameworks. We observe that in all the experiments and applications, our DAD++
gives an impressive performance against various adversarial attacks with a
minimal drop in clean accuracy. The source code is available at:
https://github.com/vcl-iisc/Improved-Data-free-Test-Time-Adversarial-Defense","['Gaurav Kumar Nayak', 'Inder Khatri', 'Shubham Randive', 'Ruchit Rawal', 'Anirban Chakraborty']","['cs.CV', 'cs.LG', 'stat.ML']",2023-09-10 20:39:53+00:00
http://arxiv.org/abs/2309.05107v1,Nonlinear Granger Causality using Kernel Ridge Regression,"I introduce a novel algorithm and accompanying Python library, named
mlcausality, designed for the identification of nonlinear Granger causal
relationships. This novel algorithm uses a flexible plug-in architecture that
enables researchers to employ any nonlinear regressor as the base prediction
model. Subsequently, I conduct a comprehensive performance analysis of
mlcausality when the prediction regressor is the kernel ridge regressor with
the radial basis function kernel. The results demonstrate that mlcausality
employing kernel ridge regression achieves competitive AUC scores across a
diverse set of simulated data. Furthermore, mlcausality with kernel ridge
regression yields more finely calibrated $p$-values in comparison to rival
algorithms. This enhancement enables mlcausality to attain superior accuracy
scores when using intuitive $p$-value-based thresholding criteria. Finally,
mlcausality with the kernel ridge regression exhibits significantly reduced
computation times compared to existing nonlinear Granger causality algorithms.
In fact, in numerous instances, this innovative approach achieves superior
solutions within computational timeframes that are an order of magnitude
shorter than those required by competing algorithms.","['Wojciech ""Victor"" Fulmyk']","['stat.ML', 'cs.LG', 'econ.EM', 'stat.ME']",2023-09-10 18:28:48+00:00
http://arxiv.org/abs/2309.05077v3,Generalization error bounds for iterative learning algorithms with bounded updates,"This paper explores the generalization characteristics of iterative learning
algorithms with bounded updates for non-convex loss functions, employing
information-theoretic techniques. Our key contribution is a novel bound for the
generalization error of these algorithms with bounded updates. Our approach
introduces two main novelties: 1) we reformulate the mutual information as the
uncertainty of updates, providing a new perspective, and 2) instead of using
the chaining rule of mutual information, we employ a variance decomposition
technique to decompose information across iterations, allowing for a simpler
surrogate process. We analyze our generalization bound under various settings
and demonstrate improved bounds. To bridge the gap between theory and practice,
we also examine the previously observed scaling behavior in large language
models. Ultimately, our work takes a further step for developing practical
generalization theories.","['Jingwen Fu', 'Nanning Zheng']","['cs.LG', 'stat.ML']",2023-09-10 16:55:59+00:00
http://arxiv.org/abs/2309.05030v3,"Decolonial AI Alignment: Openness, Viśe\d{s}a-Dharma, and Including Excluded Knowledges","Prior work has explicated the coloniality of artificial intelligence (AI)
development and deployment through mechanisms such as extractivism, automation,
sociological essentialism, surveillance, and containment. However, that work
has not engaged much with alignment: teaching behaviors to a large language
model (LLM) in line with desired values, and has not considered a mechanism
that arises within that process: moral absolutism -- a part of the coloniality
of knowledge. Colonialism has a history of altering the beliefs and values of
colonized peoples; in this paper, I argue that this history is recapitulated in
current LLM alignment practices and technologies. Furthermore, I suggest that
AI alignment be decolonialized using three forms of openness: openness of
models, openness to society, and openness to excluded knowledges. This
suggested approach to decolonial AI alignment uses ideas from the argumentative
moral philosophical tradition of Hinduism, which has been described as an
open-source religion. One concept used is vi\'{s}e\d{s}a-dharma, or particular
context-specific notions of right and wrong. At the end of the paper, I provide
a suggested reference architecture to work toward the proposed framework.",['Kush R. Varshney'],"['cs.CY', 'cs.AI', 'stat.ML']",2023-09-10 14:04:21+00:00
http://arxiv.org/abs/2309.05019v2,SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models,"Diffusion Probabilistic Models (DPMs) have achieved considerable success in
generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE
or ODE which is time-consuming, numerous fast sampling methods built upon
improved differential equation solvers are proposed. The majority of such
techniques consider solving the diffusion ODE due to its superior efficiency.
However, stochastic sampling could offer additional advantages in generating
diverse and high-quality data. In this work, we engage in a comprehensive
analysis of stochastic sampling from two aspects: variance-controlled diffusion
SDE and linear multi-step SDE solver. Based on our analysis, we propose
SA-Solver, which is an improved efficient stochastic Adams method for solving
diffusion SDE to generate data with high quality. Our experiments show that
SA-Solver achieves: 1) improved or comparable performance compared with the
existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID
scores on substantial benchmark datasets under a suitable number of function
evaluations (NFEs).","['Shuchen Xue', 'Mingyang Yi', 'Weijian Luo', 'Shifeng Zhang', 'Jiacheng Sun', 'Zhenguo Li', 'Zhi-Ming Ma']","['cs.LG', 'stat.ML']",2023-09-10 12:44:54+00:00
http://arxiv.org/abs/2309.04877v2,A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning,"The rapid progress in machine learning in recent years has been based on a
highly productive connection to gradient-based optimization. Further progress
hinges in part on a shift in focus from pattern recognition to decision-making
and multi-agent problems. In these broader settings, new mathematical
challenges emerge that involve equilibria and game theory instead of optima.
Gradient-based methods remain essential -- given the high dimensionality and
large scale of machine-learning problems -- but simple gradient descent is no
longer the point of departure for algorithm design. We provide a gentle
introduction to a broader framework for gradient-based algorithms in machine
learning, beginning with saddle points and monotone games, and proceeding to
general variational inequalities. While we provide convergence proofs for
several of the algorithms that we present, our main focus is that of providing
motivation and intuition.","['Neha S. Wadia', 'Yatin Dandi', 'Michael I. Jordan']","['cs.LG', 'stat.ML']",2023-09-09 21:36:51+00:00
http://arxiv.org/abs/2309.04860v1,Approximation Results for Gradient Descent trained Neural Networks,"The paper contains approximation guarantees for neural networks that are
trained with gradient flow, with error measured in the continuous
$L_2(\mathbb{S}^{d-1})$-norm on the $d$-dimensional unit sphere and targets
that are Sobolev smooth. The networks are fully connected of constant depth and
increasing width. Although all layers are trained, the gradient flow
convergence is based on a neural tangent kernel (NTK) argument for the
non-convex second but last layer. Unlike standard NTK analysis, the continuous
error norm implies an under-parametrized regime, possible by the natural
smoothness assumption required for approximation. The typical
over-parametrization re-enters the results in form of a loss in approximation
rate relative to established approximation methods for Sobolev smooth
functions.",['G. Welper'],"['cs.LG', 'cs.NA', 'math.NA', 'stat.ML', '41A46, 65K10, 68T07']",2023-09-09 18:47:55+00:00
http://arxiv.org/abs/2309.04821v1,Non-linear dimension reduction in factor-augmented vector autoregressions,"This paper introduces non-linear dimension reduction in factor-augmented
vector autoregressions to analyze the effects of different economic shocks. I
argue that controlling for non-linearities between a large-dimensional dataset
and the latent factors is particularly useful during turbulent times of the
business cycle. In simulations, I show that non-linear dimension reduction
techniques yield good forecasting performance, especially when data is highly
volatile. In an empirical application, I identify a monetary policy as well as
an uncertainty shock excluding and including observations of the COVID-19
pandemic. Those two applications suggest that the non-linear FAVAR approaches
are capable of dealing with the large outliers caused by the COVID-19 pandemic
and yield reliable results in both scenarios.",['Karin Klieber'],"['econ.EM', 'stat.ML']",2023-09-09 15:22:30+00:00
http://arxiv.org/abs/2309.04810v3,Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization,"Recent research indicates that the performance of machine learning models can
be improved by aligning the geometry of the latent space with the underlying
data structure. Rather than relying solely on Euclidean space, researchers have
proposed using hyperbolic and spherical spaces with constant curvature, or
combinations thereof, to better model the latent space and enhance model
performance. However, little attention has been given to the problem of
automatically identifying the optimal latent geometry for the downstream task.
We mathematically define this novel formulation and coin it as neural latent
geometry search (NLGS). More specifically, we introduce an initial attempt to
search for a latent geometry composed of a product of constant curvature model
spaces with a small number of query evaluations, under some simplifying
assumptions. To accomplish this, we propose a novel notion of distance between
candidate latent geometries based on the Gromov-Hausdorff distance from metric
geometry. In order to compute the Gromov-Hausdorff distance, we introduce a
mapping function that enables the comparison of different manifolds by
embedding them in a common high-dimensional ambient space. We then design a
graph search space based on the notion of smoothness between latent geometries
and employ the calculated distances as an additional inductive bias. Finally,
we use Bayesian optimization to search for the optimal latent geometry in a
query-efficient manner. This is a general method which can be applied to search
for the optimal latent geometry for a variety of models and downstream tasks.
We perform experiments on synthetic and real-world datasets to identify the
optimal latent geometry for multiple machine learning problems.","['Haitz Saez de Ocariz Borde', 'Alvaro Arroyo', 'Ismael Morales', 'Ingmar Posner', 'Xiaowen Dong']","['cs.LG', 'stat.ML']",2023-09-09 14:29:22+00:00
http://arxiv.org/abs/2309.04742v2,Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in Neural Networks,"We consider the problem of performing Bayesian inference for logistic
regression using appropriate extensions of the ensemble Kalman filter. Two
interacting particle systems are proposed that sample from an approximate
posterior and prove quantitative convergence rates of these interacting
particle systems to their mean-field limit as the number of particles tends to
infinity. Furthermore, we apply these techniques and examine their
effectiveness as methods of Bayesian approximation for quantifying predictive
uncertainty in neural networks.","['Diksha Bhandari', 'Jakiw Pidstrigach', 'Sebastian Reich']","['stat.ML', 'cs.LG', 'cs.NA', 'math.NA']",2023-09-09 10:01:51+00:00
http://arxiv.org/abs/2309.04627v1,Probabilistic Safety Regions Via Finite Families of Scalable Classifiers,"Supervised classification recognizes patterns in the data to separate classes
of behaviours. Canonical solutions contain misclassification errors that are
intrinsic to the numerical approximating nature of machine learning. The data
analyst may minimize the classification error on a class at the expense of
increasing the error of the other classes. The error control of such a design
phase is often done in a heuristic manner. In this context, it is key to
develop theoretical foundations capable of providing probabilistic
certifications to the obtained classifiers. In this perspective, we introduce
the concept of probabilistic safety region to describe a subset of the input
space in which the number of misclassified instances is probabilistically
controlled. The notion of scalable classifiers is then exploited to link the
tuning of machine learning with error control. Several tests corroborate the
approach. They are provided through synthetic data in order to highlight all
the steps involved, as well as through a smart mobility application.","['Alberto Carlevaro', 'Teodoro Alamo', 'Fabrizio Dabbene', 'Maurizio Mongelli']","['stat.ML', 'cs.LG']",2023-09-08 22:40:19+00:00
http://arxiv.org/abs/2309.04626v1,Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning,"We introduce a new type of query mechanism for collecting human feedback,
called the perceptual adjustment query ( PAQ). Being both informative and
cognitively lightweight, the PAQ adopts an inverted measurement scheme, and
combines advantages from both cardinal and ordinal queries. We showcase the PAQ
in the metric learning problem, where we collect PAQ measurements to learn an
unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank
matrix estimation problem to which standard matrix estimators cannot be
applied. Consequently, we develop a two-stage estimator for metric learning
from PAQs, and provide sample complexity guarantees for this estimator. We
present numerical simulations demonstrating the performance of the estimator
and its notable properties.","['Austin Xu', 'Andrew D. McRae', 'Jingyan Wang', 'Mark A. Davenport', 'Ashwin Pananjady']","['stat.ML', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT']",2023-09-08 22:36:33+00:00
http://arxiv.org/abs/2309.06548v3,Online Infinite-Dimensional Regression: Learning Linear Operators,"We consider the problem of learning linear operators under squared loss
between two infinite-dimensional Hilbert spaces in the online setting. We show
that the class of linear operators with uniformly bounded $p$-Schatten norm is
online learnable for any $p \in [1, \infty)$. On the other hand, we prove an
impossibility result by showing that the class of uniformly bounded linear
operators with respect to the operator norm is \textit{not} online learnable.
Moreover, we show a separation between sequential uniform convergence and
online learnability by identifying a class of bounded linear operators that is
online learnable but uniform convergence does not hold. Finally, we prove that
the impossibility result and the separation between uniform convergence and
learnability also hold in the batch setting.","['Vinod Raman', 'Unique Subedi', 'Ambuj Tewari']","['stat.ML', 'cs.LG']",2023-09-08 21:34:52+00:00
http://arxiv.org/abs/2309.04452v2,Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks,"Statistical postprocessing is used to translate ensembles of raw numerical
weather forecasts into reliable probabilistic forecast distributions. In this
study, we examine the use of permutation-invariant neural networks for this
task. In contrast to previous approaches, which often operate on ensemble
summary statistics and dismiss details of the ensemble distribution, we propose
networks that treat forecast ensembles as a set of unordered member forecasts
and learn link functions that are by design invariant to permutations of the
member ordering. We evaluate the quality of the obtained forecast distributions
in terms of calibration and sharpness and compare the models against classical
and neural network-based benchmark methods. In case studies addressing the
postprocessing of surface temperature and wind gust forecasts, we demonstrate
state-of-the-art prediction quality. To deepen the understanding of the learned
inference process, we further propose a permutation-based importance analysis
for ensemble-valued predictors, which highlights specific aspects of the
ensemble forecast that are considered important by the trained postprocessing
models. Our results suggest that most of the relevant information is contained
in a few ensemble-internal degrees of freedom, which may impact the design of
future ensemble forecasting and postprocessing systems.","['Kevin Höhlein', 'Benedikt Schulz', 'Rüdiger Westermann', 'Sebastian Lerch']","['stat.ML', 'cs.LG', 'physics.ao-ph']",2023-09-08 17:20:51+00:00
http://arxiv.org/abs/2309.04381v2,Generalization Bounds: Perspectives from Information Theory and PAC-Bayes,"A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.","['Fredrik Hellström', 'Giuseppe Durisi', 'Benjamin Guedj', 'Maxim Raginsky']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2023-09-08 15:23:40+00:00
http://arxiv.org/abs/2309.04354v1,Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts,"Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due
to their ability to decouple model size from inference efficiency by only
activating a small subset of the model parameters for any given input token. As
such, sparse MoEs have enabled unprecedented scalability, resulting in
tremendous successes across domains such as natural language processing and
computer vision. In this work, we instead explore the use of sparse MoEs to
scale-down Vision Transformers (ViTs) to make them more attractive for
resource-constrained vision applications. To this end, we propose a simplified
and mobile-friendly MoE design where entire images rather than individual
patches are routed to the experts. We also propose a stable MoE training
procedure that uses super-class information to guide the router. We empirically
show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off
between performance and efficiency than the corresponding dense ViTs. For
example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense
counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only
54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.","['Erik Daxberger', 'Floris Weers', 'Bowen Zhang', 'Tom Gunter', 'Ruoming Pang', 'Marcin Eichner', 'Michael Emmersberger', 'Yinfei Yang', 'Alexander Toshev', 'Xianzhi Du']","['cs.CV', 'cs.LG', 'stat.ML']",2023-09-08 14:24:10+00:00
http://arxiv.org/abs/2309.04317v1,Actor critic learning algorithms for mean-field control with moment neural networks,"We develop a new policy gradient and actor-critic algorithm for solving
mean-field control problems within a continuous time reinforcement learning
setting. Our approach leverages a gradient-based representation of the value
function, employing parametrized randomized policies. The learning for both the
actor (policy) and critic (value function) is facilitated by a class of moment
neural network functions on the Wasserstein space of probability measures, and
the key feature is to sample directly trajectories of distributions. A central
challenge addressed in this study pertains to the computational treatment of an
operator specific to the mean-field framework. To illustrate the effectiveness
of our methods, we provide a comprehensive set of numerical results. These
encompass diverse examples, including multi-dimensional settings and nonlinear
quadratic mean-field control problems with controlled volatility.","['Huyên Pham', 'Xavier Warin']","['stat.ML', 'cs.LG', 'math.OC', '68T07']",2023-09-08 13:29:57+00:00
http://arxiv.org/abs/2309.04268v2,Optimal Rate of Kernel Regression in Large Dimensions,"We perform a study on kernel regression for large-dimensional data (where the
sample size $n$ is polynomially depending on the dimension $d$ of the samples,
i.e., $n\asymp d^{\gamma}$ for some $\gamma >0$ ). We first build a general
tool to characterize the upper bound and the minimax lower bound of kernel
regression for large dimensional data through the Mendelson complexity
$\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$
respectively. When the target function falls into the RKHS associated with a
(general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new
tool to show that the minimax rate of the excess risk of kernel regression is
$n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then
further determine the optimal rate of the excess risk of kernel regression for
all the $\gamma>0$ and find that the curve of optimal rate varying along
$\gamma$ exhibits several new phenomena including the multiple descent behavior
and the periodic plateau behavior. As an application, For the neural tangent
kernel (NTK), we also provide a similar explicit description of the curve of
optimal rate. As a direct corollary, we know these claims hold for wide neural
networks as well.","['Weihao Lu', 'Haobo Zhang', 'Yicheng Li', 'Manyun Xu', 'Qian Lin']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH', '62G08, 46E22, 68T07']",2023-09-08 11:29:05+00:00
http://arxiv.org/abs/2309.04507v1,Generating drawdown-realistic financial price paths using path signatures,"A novel generative machine learning approach for the simulation of sequences
of financial price data with drawdowns quantifiably close to empirical data is
introduced. Applications such as pricing drawdown insurance options or
developing portfolio drawdown control strategies call for a host of
drawdown-realistic paths. Historical scenarios may be insufficient to
effectively train and backtest the strategy, while standard parametric Monte
Carlo does not adequately preserve drawdowns. We advocate a non-parametric
Monte Carlo approach combining a variational autoencoder generative model with
a drawdown reconstruction loss function. To overcome issues of numerical
complexity and non-differentiability, we approximate drawdown as a linear
function of the moments of the path, known in the literature as path
signatures. We prove the required regularity of drawdown function and
consistency of the approximation. Furthermore, we obtain close numerical
approximations using linear regression for fractional Brownian and empirical
data. We argue that linear combinations of the moments of a path yield a
mathematically non-trivial smoothing of the drawdown function, which gives one
leeway to simulate drawdown-realistic price paths by including drawdown
evaluation metrics in the learning objective. We conclude with numerical
experiments on mixed equity, bond, real estate and commodity portfolios and
obtain a host of drawdown-realistic paths.","['Emiel Lemahieu', 'Kris Boudt', 'Maarten Wyns']","['q-fin.CP', 'cs.LG', 'stat.ML', '91B28, 91B84']",2023-09-08 10:06:40+00:00
http://arxiv.org/abs/2309.04236v1,Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos,"Data silos, mainly caused by privacy and interoperability, significantly
constrain collaborations among different organizations with similar data for
the same purpose. Distributed learning based on divide-and-conquer provides a
promising way to settle the data silos, but it suffers from several challenges,
including autonomy, privacy guarantees, and the necessity of collaborations.
This paper focuses on developing an adaptive distributed kernel ridge
regression (AdaDKRR) by taking autonomy in parameter selection, privacy in
communicating non-sensitive information, and the necessity of collaborations in
performance improvement into account. We provide both solid theoretical
verification and comprehensive experiments for AdaDKRR to demonstrate its
feasibility and effectiveness. Theoretically, we prove that under some mild
conditions, AdaDKRR performs similarly to running the optimal learning
algorithms on the whole data, verifying the necessity of collaborations and
showing that no other distributed learning scheme can essentially beat AdaDKRR
under the same conditions. Numerically, we test AdaDKRR on both toy simulations
and two real-world applications to show that AdaDKRR is superior to other
existing distributed learning schemes. All these results show that AdaDKRR is a
feasible scheme to defend against data silos, which are highly desired in
numerous application regions such as intelligent decision-making, pricing
forecasting, and performance prediction for products.","['Di Wang', 'Xiaotong Liu', 'Shao-Bo Lin', 'Ding-Xuan Zhou']","['cs.LG', 'stat.ML']",2023-09-08 09:54:36+00:00
http://arxiv.org/abs/2309.04222v1,Offline Recommender System Evaluation under Unobserved Confounding,"Off-Policy Estimation (OPE) methods allow us to learn and evaluate
decision-making policies from logged data. This makes them an attractive choice
for the offline evaluation of recommender systems, and several recent works
have reported successful adoption of OPE methods to this end. An important
assumption that makes this work is the absence of unobserved confounders:
random variables that influence both actions and rewards at data collection
time. Because the data collection policy is typically under the practitioner's
control, the unconfoundedness assumption is often left implicit, and its
violations are rarely dealt with in the existing literature.
  This work aims to highlight the problems that arise when performing
off-policy estimation in the presence of unobserved confounders, specifically
focusing on a recommendation use-case. We focus on policy-based estimators,
where the logging propensities are learned from logged data. We characterise
the statistical bias that arises due to confounding, and show how existing
diagnostics are unable to uncover such cases. Because the bias depends directly
on the true and unobserved logging propensities, it is non-identifiable. As the
unconfoundedness assumption is famously untestable, this becomes especially
problematic. This paper emphasises this common, yet often overlooked issue.
Through synthetic data, we empirically show how na\""ive propensity estimation
under confounding can lead to severely biased metric estimates that are allowed
to fly under the radar. We aim to cultivate an awareness among researchers and
practitioners of this important problem, and touch upon potential research
directions towards mitigating its effects.","['Olivier Jeunen', 'Ben London']","['cs.LG', 'cs.IR', 'stat.ML']",2023-09-08 09:11:26+00:00
http://arxiv.org/abs/2309.04072v1,Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank,"This paper introduces two explicit schemes to sample matrices from Gibbs
distributions on $\mathcal S^{n,p}_+$, the manifold of real positive
semi-definite (PSD) matrices of size $n\times n$ and rank $p$. Given an energy
function $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ and certain Riemannian
metrics $g$ on $\mathcal S^{n,p}_+$, these schemes rely on an Euler-Maruyama
discretization of the Riemannian Langevin equation (RLE) with Brownian motion
on the manifold. We present numerical schemes for RLE under two fundamental
metrics on $\mathcal S^{n,p}_+$: (a) the metric obtained from the embedding of
$\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $; and (b) the
Bures-Wasserstein metric corresponding to quotient geometry. We also provide
examples of energy functions with explicit Gibbs distributions that allow
numerical validation of these schemes.","['Tianmin Yu', 'Shixin Zheng', 'Jianfeng Lu', 'Govind Menon', 'Xiangxiong Zhang']","['math.NA', 'cs.LG', 'cs.NA', 'stat.ML']",2023-09-08 02:09:40+00:00
http://arxiv.org/abs/2309.04013v1,An Element-wise RSAV Algorithm for Unconstrained Optimization Problems,"We present a novel optimization algorithm, element-wise relaxed scalar
auxiliary variable (E-RSAV), that satisfies an unconditional energy dissipation
law and exhibits improved alignment between the modified and the original
energy. Our algorithm features rigorous proofs of linear convergence in the
convex setting. Furthermore, we present a simple accelerated algorithm that
improves the linear convergence rate to super-linear in the univariate case. We
also propose an adaptive version of E-RSAV with Steffensen step size. We
validate the robustness and fast convergence of our algorithm through ample
numerical experiments.","['Shiheng Zhang', 'Jiahao Zhang', 'Jie Shen', 'Guang Lin']","['math.OC', 'cs.LG', 'stat.ML', '90C26, 68T99, 68W40']",2023-09-07 20:37:23+00:00
http://arxiv.org/abs/2309.03873v2,A Tutorial on the Non-Asymptotic Theory of System Identification,"This tutorial serves as an introduction to recently developed non-asymptotic
methods in the theory of -- mainly linear -- system identification. We
emphasize tools we deem particularly useful for a range of problems in this
domain, such as the covering technique, the Hanson-Wright Inequality and the
method of self-normalized martingales. We then employ these tools to give
streamlined proofs of the performance of various least-squares based estimators
for identifying the parameters in autoregressive models. We conclude by
sketching out how the ideas presented herein can be extended to certain
nonlinear identification problems.","['Ingvar Ziemann', 'Anastasios Tsiamis', 'Bruce Lee', 'Yassir Jedra', 'Nikolai Matni', 'George J. Pappas']","['eess.SY', 'cs.LG', 'cs.SY', 'stat.ML']",2023-09-07 17:33:30+00:00
http://arxiv.org/abs/2309.03847v3,Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples,"We study the problem of estimating mixtures of Gaussians under the constraint
of differential privacy (DP). Our main result is that
$\text{poly}(k,d,1/\alpha,1/\varepsilon,\log(1/\delta))$ samples are sufficient
to estimate a mixture of $k$ Gaussians in $\mathbb{R}^d$ up to total variation
distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the
first finite sample complexity upper bound for the problem that does not make
any structural assumptions on the GMMs.
  To solve the problem, we devise a new framework which may be useful for other
tasks. On a high level, we show that if a class of distributions (such as
Gaussians) is (1) list decodable and (2) admits a ""locally small'' cover (Bun
et al., 2021) with respect to total variation distance, then the class of its
mixtures is privately learnable. The proof circumvents a known barrier
indicating that, unlike Gaussians, GMMs do not admit a locally small cover
(Aden-Ali et al., 2021b).","['Mohammad Afzali', 'Hassan Ashtiani', 'Christopher Liaw']","['stat.ML', 'cs.CR', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']",2023-09-07 17:02:32+00:00
http://arxiv.org/abs/2309.03843v1,Gradient-Based Feature Learning under Structured Data,"Recent works have demonstrated that the sample complexity of gradient-based
learning of single index models, i.e. functions that depend on a 1-dimensional
projection of the input data, is governed by their information exponent.
However, these results are only concerned with isotropic data, while in
practice the input often contains additional structure which can implicitly
guide the algorithm. In this work, we investigate the effect of a spiked
covariance structure and reveal several interesting phenomena. First, we show
that in the anisotropic setting, the commonly used spherical gradient dynamics
may fail to recover the true direction, even when the spike is perfectly
aligned with the target direction. Next, we show that appropriate weight
normalization that is reminiscent of batch normalization can alleviate this
issue. Further, by exploiting the alignment between the (spiked) input
covariance and the target, we obtain improved sample complexity compared to the
isotropic case. In particular, under the spiked model with a suitably large
spike, the sample complexity of gradient-based training can be made independent
of the information exponent while also outperforming lower bounds for
rotationally invariant kernel methods.","['Alireza Mousavi-Hosseini', 'Denny Wu', 'Taiji Suzuki', 'Murat A. Erdogdu']","['stat.ML', 'cs.LG']",2023-09-07 16:55:50+00:00
