id,title,abstract,authors,categories,date
http://arxiv.org/abs/1611.07103v3,Generation of discrete random variables in scalable frameworks,"In this paper, we face the problem of simulating discrete random variables
with general and varying distributions in a scalable framework, where fully
parallelizable operations should be preferred. The new paradigm is inspired by
the context of discrete choice models. Compared to classical algorithms, we add
parallelized randomness, and we leave the final simulation of the random
variable to a single associative operation. We characterize the set of
algorithms that work in this way, and those algorithms that may have an
additive or multiplicative local noise. As a consequence, we could define a
natural way to solve some popular simulation problems.",['Giacomo Aletti'],"['stat.ME', 'math.ST', 'stat.ML', 'stat.TH', '62D05 (Primary) 65C10, 68A55 (Secondary)']",2016-11-21 23:37:31+00:00
http://arxiv.org/abs/1611.07100v2,Interpreting Finite Automata for Sequential Data,"Automaton models are often seen as interpretable models. Interpretability
itself is not well defined: it remains unclear what interpretability means
without first explicitly specifying objectives or desired attributes. In this
paper, we identify the key properties used to interpret automata and propose a
modification of a state-merging approach to learn variants of finite state
automata. We apply the approach to problems beyond typical grammar inference
tasks. Additionally, we cover several use-cases for prediction, classification,
and clustering on sequential data in both supervised and unsupervised scenarios
to show how the identified key properties are applicable in a wide range of
contexts.","['Christian Albert Hammerschmidt', 'Sicco Verwer', 'Qin Lin', 'Radu State']","['stat.ML', 'cs.AI', 'I.2.6']",2016-11-21 23:21:13+00:00
http://arxiv.org/abs/1611.07096v2,Structured Prediction by Conditional Risk Minimization,"We propose a general approach for supervised learning with structured output
spaces, such as combinatorial and polyhedral sets, that is based on minimizing
estimated conditional risk functions. Given a loss function defined over pairs
of output labels, we first estimate the conditional risk function by solving a
(possibly infinite) collection of regularized least squares problems. A
prediction is made by solving an inference problem that minimizes the estimated
conditional risk function over the output space. We show that this approach
enables, in some cases, efficient training and inference without explicitly
introducing a convex surrogate for the original loss function, even when it is
discontinuous. Empirical evaluations on real-world and synthetic data sets
demonstrate the effectiveness of our method in adapting to a variety of loss
functions.","['Chong Yang Goh', 'Patrick Jaillet']","['stat.ML', 'cs.LG']",2016-11-21 23:14:58+00:00
http://arxiv.org/abs/1611.07093v3,Using Empirical Covariance Matrix in Enhancing Prediction Accuracy of Linear Models with Missing Information,"Inference and Estimation in Missing Information (MI) scenarios are important
topics in Statistical Learning Theory and Machine Learning (ML). In ML
literature, attempts have been made to enhance prediction through precise
feature selection methods. In sparse linear models, LASSO is well-known in
extracting the desired support of the signal and resisting against noisy
systems. When sparse models are also suffering from MI, the sparse recovery and
inference of the missing models are taken into account simultaneously. In this
paper, we will introduce an approach which enjoys sparse regression and
covariance matrix estimation to improve matrix completion accuracy, and as a
result enhancing feature selection preciseness which leads to reduction in
prediction Mean Squared Error (MSE). We will compare the effect of employing
covariance matrix in enhancing estimation accuracy to the case it is not used
in feature selection. Simulations show the improvement in the performance as
compared to the case where the covariance matrix estimation is not used.","['Ahmadreza Moradipari', 'Sina Shahsavari', 'Ashkan Esmaeili', 'Farokh Marvasti']","['stat.ML', 'stat.ME']",2016-11-21 23:07:51+00:00
http://arxiv.org/abs/1611.07078v2,A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games,"Reinforcement learning is concerned with identifying reward-maximizing
behaviour policies in environments that are initially unknown. State-of-the-art
reinforcement learning approaches, such as deep Q-networks, are model-free and
learn to act effectively across a wide range of environments such as Atari
games, but require huge amounts of data. Model-based techniques are more
data-efficient, but need to acquire explicit knowledge about the environment.
  In this paper, we take a step towards using model-based techniques in
environments with a high-dimensional visual state space by demonstrating that
it is possible to learn system dynamics and the reward structure jointly. Our
contribution is to extend a recently developed deep neural network for video
frame prediction in Atari games to enable reward prediction as well. To this
end, we phrase a joint optimization problem for minimizing both video frame and
reward reconstruction loss, and adapt network parameters accordingly. Empirical
evaluations on five Atari games demonstrate accurate cumulative reward
prediction of up to 200 frames. We consider these results as opening up
important directions for model-based reinforcement learning in complex,
initially unknown environments.","['Felix Leibfried', 'Nate Kushman', 'Katja Hofmann']","['cs.AI', 'cs.LG', 'stat.ML']",2016-11-21 22:06:23+00:00
http://arxiv.org/abs/1611.07056v2,The Recycling Gibbs Sampler for Efficient Learning,"Monte Carlo methods are essential tools for Bayesian inference. Gibbs
sampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively
used in signal processing, machine learning, and statistics, employed to draw
samples from complicated high-dimensional posterior distributions. The key
point for the successful application of the Gibbs sampler is the ability to
draw efficiently samples from the full-conditional probability density
functions. Since in the general case this is not possible, in order to speed up
the convergence of the chain, it is required to generate auxiliary samples
whose information is eventually disregarded. In this work, we show that these
auxiliary samples can be recycled within the Gibbs estimators, improving their
efficiency with no extra cost. This novel scheme arises naturally after
pointing out the relationship between the standard Gibbs sampler and the chain
rule used for sampling purposes. Numerical simulations involving simple and
real inference problems confirm the excellent performance of the proposed
scheme in terms of accuracy and computational efficiency. In particular we give
empirical evidence of performance in a toy example, inference of Gaussian
processes hyperparameters, and learning dependence graphs through regression.","['Luca Martino', 'Victor Elvira', 'Gustau Camps-Valls']","['stat.CO', 'cs.LG', 'stat.ML']",2016-11-21 21:13:00+00:00
http://arxiv.org/abs/1611.07054v1,An Efficient Training Algorithm for Kernel Survival Support Vector Machines,"Survival analysis is a fundamental tool in medical research to identify
predictors of adverse events and develop systems for clinical decision support.
In order to leverage large amounts of patient data, efficient optimisation
routines are paramount. We propose an efficient training algorithm for the
kernel survival support vector machine (SSVM). We directly optimise the primal
objective function and employ truncated Newton optimisation and order statistic
trees to significantly lower computational costs compared to previous training
algorithms, which require $O(n^4)$ space and $O(p n^6)$ time for datasets with
$n$ samples and $p$ features. Our results demonstrate that our proposed
optimisation scheme allows analysing data of a much larger scale with no loss
in prediction performance. Experiments on synthetic and 5 real-world datasets
show that our technique outperforms existing kernel SSVM formulations if the
amount of right censoring is high ($\geq85\%$), and performs comparably
otherwise.","['Sebastian PÃ¶lsterl', 'Nassir Navab', 'Amin Katouzian']","['cs.LG', 'cs.AI', 'stat.ML', 'G.1.6; I.5.1; J.3']",2016-11-21 21:09:33+00:00
http://arxiv.org/abs/1611.07051v3,Time Series Structure Discovery via Probabilistic Program Synthesis,"There is a widespread need for techniques that can discover structure from
time series data. Recently introduced techniques such as Automatic Bayesian
Covariance Discovery (ABCD) provide a way to find structure within a single
time series by searching through a space of covariance kernels that is
generated using a simple grammar. While ABCD can identify a broad class of
temporal patterns, it is difficult to extend and can be brittle in practice.
This paper shows how to extend ABCD by formulating it in terms of probabilistic
program synthesis. The key technical ideas are to (i) represent models using
abstract syntax trees for a domain-specific probabilistic language, and (ii)
represent the time series model prior, likelihood, and search strategy using
probabilistic programs in a sufficiently expressive language. The final
probabilistic program is written in under 70 lines of probabilistic code in
Venture. The paper demonstrates an application to time series clustering that
involves a non-parametric extension to ABCD, experiments for interpolation and
extrapolation on real-world econometric data, and improvements in accuracy over
both non-parametric and standard regression baselines.","['Ulrich Schaechtle', 'Feras Saad', 'Alexey Radul', 'Vikash Mansinghka']",['stat.ML'],2016-11-21 21:03:01+00:00
http://arxiv.org/abs/1611.07012v3,GRAM: Graph-based Attention Model for Healthcare Representation Learning,"Deep learning methods exhibit promising performance for predictive modeling
in healthcare, but two important challenges remain: -Data insufficiency:Often
in healthcare predictive modeling, the sample size is insufficient for deep
learning methods to achieve satisfactory results. -Interpretation:The
representations learned by deep learning methods should align with medical
knowledge. To address these challenges, we propose a GRaph-based Attention
Model, GRAM that supplements electronic health records (EHR) with hierarchical
information inherent to medical ontologies. Based on the data volume and the
ontology structure, GRAM represents a medical concept as a combination of its
ancestors in the ontology via an attention mechanism. We compared predictive
performance (i.e. accuracy, data needs, interpretability) of GRAM to various
methods including the recurrent neural network (RNN) in two sequential
diagnoses prediction tasks and one heart failure prediction task. Compared to
the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely
observed in the training data and 3% improved area under the ROC curve for
predicting heart failure using an order of magnitude less training data.
Additionally, unlike other methods, the medical concept representations learned
by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits
intuitive attention behaviors by adaptively generalizing to higher level
concepts when facing data insufficiency at the lower level concepts.","['Edward Choi', 'Mohammad Taha Bahadori', 'Le Song', 'Walter F. Stewart', 'Jimeng Sun']","['cs.LG', 'stat.ML']",2016-11-21 20:59:22+00:00
http://arxiv.org/abs/1611.06996v1,Spatial contrasting for deep unsupervised learning,"Convolutional networks have marked their place over the last few years as the
best performing model for various visual tasks. They are, however, most suited
for supervised learning from large amounts of labeled data. Previous attempts
have been made to use unlabeled data to improve model performance by applying
unsupervised techniques. These attempts require different architectures and
training methods. In this work we present a novel approach for unsupervised
training of Convolutional networks that is based on contrasting between spatial
regions within images. This criterion can be employed within conventional
neural networks and trained using standard techniques such as SGD and
back-propagation, thus complementing supervised methods.","['Elad Hoffer', 'Itay Hubara', 'Nir Ailon']","['stat.ML', 'cs.LG']",2016-11-21 20:24:58+00:00
http://arxiv.org/abs/1611.06972v6,Measuring Sample Quality with Diffusions,"Stein's method for measuring convergence to a continuous target distribution
relies on an operator characterizing the target and Stein factor bounds on the
solutions of an associated differential equation. While such operators and
bounds are readily available for a diversity of univariate targets, few
multivariate targets have been analyzed. We introduce a new class of
characterizing operators based on Ito diffusions and develop explicit
multivariate Stein factor bounds for any target with a fast-coupling Ito
diffusion. As example applications, we develop computable and
convergence-determining diffusion Stein discrepancies for log-concave,
heavy-tailed, and multimodal targets and use these quality measures to select
the hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare
random and deterministic quadrature rules, and quantify bias-variance tradeoffs
in approximate MCMC. Our results establish a near-linear relationship between
diffusion Stein discrepancies and Wasserstein distances, improving upon past
work even for strongly log-concave targets. The exposed relationship between
Stein factors and Markov process coupling may be of independent interest.","['Jackson Gorham', 'Andrew B. Duncan', 'Sebastian J. Vollmer', 'Lester Mackey']","['stat.ML', 'cs.LG', 'math.PR', '60J60, 62-04, 62E17, 60E15, 65C60 (Primary) 62-07, 65C05, 68T05\n  (Secondary)']",2016-11-21 19:47:08+00:00
http://arxiv.org/abs/1611.06933v1,Unsupervised Learning for Lexicon-Based Classification,"In lexicon-based classification, documents are assigned labels by comparing
the number of words that appear from two opposed lexicons, such as positive and
negative sentiment. Creating such words lists is often easier than labeling
instances, and they can be debugged by non-experts if classification
performance is unsatisfactory. However, there is little analysis or
justification of this classification heuristic. This paper describes a set of
assumptions that can be used to derive a probabilistic justification for
lexicon-based classification, as well as an analysis of its expected accuracy.
One key assumption behind lexicon-based classification is that all words in
each lexicon are equally predictive. This is rarely true in practice, which is
why lexicon-based approaches are usually outperformed by supervised classifiers
that learn distinct weights on each word from labeled instances. This paper
shows that it is possible to learn such weights without labeled data, by
leveraging co-occurrence statistics across the lexicons. This offers the best
of both worlds: light supervision in the form of lexicons, and data-driven
classification with higher accuracy than traditional word-counting heuristics.",['Jacob Eisenstein'],"['cs.LG', 'cs.CL', 'stat.ML', 'I.2.6; I.2.7']",2016-11-21 18:30:17+00:00
http://arxiv.org/abs/1611.06928v1,Memory Lens: How Much Memory Does an Agent Use?,"We propose a new method to study the internal memory used by reinforcement
learning policies. We estimate the amount of relevant past information by
estimating mutual information between behavior histories and the current action
of an agent. We perform this estimation in the passive setting, that is, we do
not intervene but merely observe the natural behavior of the agent. Moreover,
we provide a theoretical justification for our approach by showing that it
yields an implementation-independent lower bound on the minimal memory capacity
of any agent that implement the observed policy. We demonstrate our approach by
estimating the use of memory of DQN policies on concatenated Atari frames,
demonstrating sharply different use of memory across 49 games. The study of
memory as information that flows from the past to the current action opens
avenues to understand and improve successful reinforcement learning algorithms.","['Christoph Dann', 'Katja Hofmann', 'Sebastian Nowozin']","['cs.AI', 'stat.ML']",2016-11-21 18:22:27+00:00
http://arxiv.org/abs/1611.06882v1,Learning From Graph Neighborhoods Using LSTMs,"Many prediction problems can be phrased as inferences over local
neighborhoods of graphs. The graph represents the interaction between entities,
and the neighborhood of each entity contains information that allows the
inferences or predictions. We present an approach for applying machine learning
directly to such graph neighborhoods, yielding predicitons for graph nodes on
the basis of the structure of their local neighborhood and the features of the
nodes in it. Our approach allows predictions to be learned directly from
examples, bypassing the step of creating and tuning an inference model or
summarizing the neighborhoods via a fixed set of hand-crafted features. The
approach is based on a multi-level architecture built from Long Short-Term
Memory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood
from data. We demonstrate the effectiveness of the proposed technique on a
synthetic example and on real-world data related to crowdsourced grading,
Bitcoin transactions, and Wikipedia edit reversions.","['Rakshit Agrawal', 'Luca de Alfaro', 'Vassilis Polychronopoulos']","['cs.LG', 'cs.AI', 'stat.ML']",2016-11-21 16:25:34+00:00
http://arxiv.org/abs/1611.06863v1,Probabilistic structure discovery in time series data,"Existing methods for structure discovery in time series data construct
interpretable, compositional kernels for Gaussian process regression models.
While the learned Gaussian process model provides posterior mean and variance
estimates, typically the structure is learned via a greedy optimization
procedure. This restricts the space of possible solutions and leads to
over-confident uncertainty estimates. We introduce a fully Bayesian approach,
inferring a full posterior over structures, which more reliably captures the
uncertainty of the model.","['David Janz', 'Brooks Paige', 'Tom Rainforth', 'Jan-Willem van de Meent', 'Frank Wood']","['stat.ML', 'cs.LG']",2016-11-21 16:08:12+00:00
http://arxiv.org/abs/1611.06800v1,MDL-motivated compression of GLM ensembles increases interpretability and retains predictive power,"Over the years, ensemble methods have become a staple of machine learning.
Similarly, generalized linear models (GLMs) have become very popular for a wide
variety of statistical inference tasks. The former have been shown to enhance
out- of-sample predictive power and the latter possess easy interpretability.
Recently, ensembles of GLMs have been proposed as a possibility. On the
downside, this approach loses the interpretability that GLMs possess. We show
that minimum description length (MDL)-motivated compression of the inferred
ensembles can be used to recover interpretability without much, if any,
downside to performance and illustrate on a number of standard classification
data sets.","['Boris Hayete', 'Matthew Valko', 'Alex Greenfield', 'Raymond Yan']",['stat.ML'],2016-11-21 14:20:04+00:00
http://arxiv.org/abs/1611.06759v2,Emergence of Compositional Representations in Restricted Boltzmann Machines,"Extracting automatically the complex set of features composing real
high-dimensional data is crucial for achieving high performance in
machine--learning tasks. Restricted Boltzmann Machines (RBM) are empirically
known to be efficient for this purpose, and to be able to generate distributed
and graded representations of the data. We characterize the structural
conditions (sparsity of the weights, low effective temperature, nonlinearities
in the activation functions of hidden units, and adaptation of fields
maintaining the activity in the visible layer) allowing RBM to operate in such
a compositional phase. Evidence is provided by the replica analysis of an
adequate statistical ensemble of random RBMs and by RBM trained on the
handwritten digits dataset MNIST.","['JÃ©rÃ´me Tubiana', 'RÃ©mi Monasson']","['physics.data-an', 'cond-mat.dis-nn', 'cs.LG', 'stat.ML']",2016-11-21 12:46:25+00:00
http://arxiv.org/abs/1611.06740v2,Variational Fourier features for Gaussian processes,"This work brings together two powerful concepts in Gaussian processes: the
variational approach to sparse approximation and the spectral representation of
Gaussian processes. This gives rise to an approximation that inherits the
benefits of the variational approach but with the representational power and
computational scalability of spectral representations. The work hinges on a key
result that there exist spectral features related to a finite domain of the
Gaussian process which exhibit almost-independent covariances. We derive these
expressions for Matern kernels in one dimension, and generalize to more
dimensions using kernels with specific structures. Under the assumption of
additive Gaussian noise, our method requires only a single pass through the
dataset, making for very fast and accurate computation. We fit a model to 4
million training points in just a few minutes on a standard laptop. With
non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from
O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the
number of data and M is the number of features.","['James Hensman', 'Nicolas Durrande', 'Arno Solin']",['stat.ML'],2016-11-21 11:57:38+00:00
http://arxiv.org/abs/1611.07308v1,Variational Graph Auto-Encoders,"We introduce the variational graph auto-encoder (VGAE), a framework for
unsupervised learning on graph-structured data based on the variational
auto-encoder (VAE). This model makes use of latent variables and is capable of
learning interpretable latent representations for undirected graphs. We
demonstrate this model using a graph convolutional network (GCN) encoder and a
simple inner product decoder. Our model achieves competitive results on a link
prediction task in citation networks. In contrast to most existing models for
unsupervised learning on graph-structured data and link prediction, our model
can naturally incorporate node features, which significantly improves
predictive performance on a number of benchmark datasets.","['Thomas N. Kipf', 'Max Welling']","['stat.ML', 'cs.LG']",2016-11-21 11:37:17+00:00
http://arxiv.org/abs/1611.06686v1,Scalable Approximations for Generalized Linear Problems,"In stochastic optimization, the population risk is generally approximated by
the empirical risk. However, in the large-scale setting, minimization of the
empirical risk may be computationally restrictive. In this paper, we design an
efficient algorithm to approximate the population risk minimizer in generalized
linear problems such as binary classification with surrogate losses and
generalized linear regression models. We focus on large-scale problems, where
the iterative minimization of the empirical risk is computationally
intractable, i.e., the number of observations $n$ is much larger than the
dimension of the parameter $p$, i.e. $n \gg p \gg 1$. We show that under random
sub-Gaussian design, the true minimizer of the population risk is approximately
proportional to the corresponding ordinary least squares (OLS) estimator. Using
this relation, we design an algorithm that achieves the same accuracy as the
empirical risk minimizer through iterations that attain up to a cubic
convergence rate, and that are cheaper than any batch optimization algorithm by
at least a factor of $\mathcal{O}(p)$. We provide theoretical guarantees for
our algorithm, and analyze the convergence behavior in terms of data
dimensions. Finally, we demonstrate the performance of our algorithm on
well-known classification and regression problems, through extensive numerical
studies on large-scale datasets, and show that it achieves the highest
performance compared to several other widely used and specialized optimization
algorithms.","['Murat A. Erdogdu', 'Mohsen Bayati', 'Lee H. Dicker']","['stat.ML', 'stat.CO']",2016-11-21 09:10:05+00:00
http://arxiv.org/abs/1611.06684v1,Probabilistic Duality for Parallel Gibbs Sampling without Graph Coloring,"We present a new notion of probabilistic duality for random variables
involving mixture distributions. Using this notion, we show how to implement a
highly-parallelizable Gibbs sampler for weakly coupled discrete pairwise
graphical models with strictly positive factors that requires almost no
preprocessing and is easy to implement. Moreover, we show how our method can be
combined with blocking to improve mixing. Even though our method leads to
inferior mixing times compared to a sequential Gibbs sampler, we argue that our
method is still very useful for large dynamic networks, where factors are added
and removed on a continuous basis, as it is hard to maintain a graph coloring
in this setup. Similarly, our method is useful for parallelizing Gibbs sampling
in graphical models that do not allow for graph colorings with a small number
of colors such as densely connected graphs.","['Lars Mescheder', 'Sebastian Nowozin', 'Andreas Geiger']","['cs.LG', 'math.PR', 'stat.ML']",2016-11-21 08:57:58+00:00
http://arxiv.org/abs/1611.06652v1,Scalable Adaptive Stochastic Optimization Using Random Projections,"Adaptive stochastic gradient methods such as AdaGrad have gained popularity
in particular for training deep neural networks. The most commonly used and
studied variant maintains a diagonal matrix approximation to second order
information by accumulating past gradients which are used to tune the step size
adaptively. In certain situations the full-matrix variant of AdaGrad is
expected to attain better performance, however in high dimensions it is
computationally impractical. We present Ada-LR and RadaGrad two computationally
efficient approximations to full-matrix AdaGrad based on randomized
dimensionality reduction. They are able to capture dependencies between
features and achieve similar performance to full-matrix AdaGrad but at a much
smaller computational cost. We show that the regret of Ada-LR is close to the
regret of full-matrix AdaGrad which can have an up-to exponentially smaller
dependence on the dimension than the diagonal variant. Empirically, we show
that Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task
of training convolutional neural networks as well as recurrent neural networks,
RadaGrad achieves faster convergence than diagonal AdaGrad.","['Gabriel Krummenacher', 'Brian McWilliams', 'Yannic Kilcher', 'Joachim M. Buhmann', 'Nicolai Meinshausen']","['stat.ML', 'cs.LG']",2016-11-21 05:15:50+00:00
http://arxiv.org/abs/1611.06585v2,Variational Boosting: Iteratively Refining Posterior Approximations,"We propose a black-box variational inference method to approximate
intractable distributions with an increasingly rich approximating class. Our
method, termed variational boosting, iteratively refines an existing
variational approximation by solving a sequence of optimization problems,
allowing the practitioner to trade computation time for accuracy. We show how
to expand the variational approximating class by incorporating additional
covariance structure and by introducing new components to form a mixture. We
apply variational boosting to synthetic and real statistical models, and show
that resulting posterior inferences compare favorably to existing posterior
approximation algorithms in both accuracy and efficiency.","['Andrew C. Miller', 'Nicholas Foti', 'Ryan P. Adams']","['stat.ML', 'cs.LG', 'stat.ME']",2016-11-20 20:25:39+00:00
http://arxiv.org/abs/1611.06534v3,Linear Thompson Sampling Revisited,"We derive an alternative proof for the regret of Thompson sampling (\ts) in
the stochastic linear bandit setting. While we obtain a regret bound of order
$\widetilde{O}(d^{3/2}\sqrt{T})$ as in previous results, the proof sheds new
light on the functioning of the \ts. We leverage on the structure of the
problem to show how the regret is related to the sensitivity (i.e., the
gradient) of the objective function and how selecting optimal arms associated
to \textit{optimistic} parameters does control it. Thus we show that \ts can be
seen as a generic randomized algorithm where the sampling distribution is
designed to have a fixed probability of being optimistic, at the cost of an
additional $\sqrt{d}$ regret factor compared to a UCB-like approach.
Furthermore, we show that our proof can be readily applied to regularized
linear optimization and generalized linear model problems.","['Marc Abeille', 'Alessandro Lazaric']","['stat.ML', 'cs.LG']",2016-11-20 15:52:41+00:00
http://arxiv.org/abs/1611.06475v2,Dealing with Range Anxiety in Mean Estimation via Statistical Queries,"We give algorithms for estimating the expectation of a given real-valued
function $\phi:X\to {\bf R}$ on a sample drawn randomly from some unknown
distribution $D$ over domain $X$, namely ${\bf E}_{{\bf x}\sim D}[\phi({\bf
x})]$. Our algorithms work in two well-studied models of restricted access to
data samples. The first one is the statistical query (SQ) model in which an
algorithm has access to an SQ oracle for the input distribution $D$ over $X$
instead of i.i.d. samples from $D$. Given a query function $\phi:X \to [0,1]$,
the oracle returns an estimate of ${\bf E}_{{\bf x}\sim D}[\phi({\bf x})]$
within some tolerance $\tau$. The second, is a model in which only a single bit
is communicated from each sample. In both of these models the error obtained
using a naive implementation would scale polynomially with the range of the
random variable $\phi({\bf x})$ (which might even be infinite). In contrast,
without restrictions on access to data the expected error scales with the
standard deviation of $\phi({\bf x})$. Here we give a simple algorithm whose
error scales linearly in standard deviation of $\phi({\bf x})$ and
logarithmically with an upper bound on the second moment of $\phi({\bf x})$.
  As corollaries, we obtain algorithms for high dimensional mean estimation and
stochastic convex optimization in these models that work in more general
settings than previously known solutions.",['Vitaly Feldman'],"['cs.LG', 'stat.ML']",2016-11-20 06:12:43+00:00
http://arxiv.org/abs/1611.06455v4,Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline,"We propose a simple but strong baseline for time series classification from
scratch with deep neural networks. Our proposed baseline models are pure
end-to-end without any heavy preprocessing on the raw data or feature crafting.
The proposed Fully Convolutional Network (FCN) achieves premium performance to
other state-of-the-art approaches and our exploration of the very deep neural
networks with the ResNet structure is also competitive. The global average
pooling in our convolutional model enables the exploitation of the Class
Activation Map (CAM) to find out the contributing region in the raw data for
the specific labels. Our models provides a simple choice for the real world
application and a good starting point for the future research. An overall
analysis is provided to discuss the generalization capability of our models,
learned features, network structures and the classification semantics.","['Zhiguang Wang', 'Weizhong Yan', 'Tim Oates']","['cs.LG', 'cs.NE', 'stat.ML']",2016-11-20 00:34:09+00:00
http://arxiv.org/abs/1611.06440v2,Pruning Convolutional Neural Networks for Resource Efficient Inference,"We propose a new formulation for pruning convolutional kernels in neural
networks to enable efficient inference. We interleave greedy criteria-based
pruning with fine-tuning by backpropagation - a computationally efficient
procedure that maintains good generalization in the pruned network. We propose
a new criterion based on Taylor expansion that approximates the change in the
cost function induced by pruning network parameters. We focus on transfer
learning, where large pretrained networks are adapted to specialized tasks. The
proposed criterion demonstrates superior performance compared to other
criteria, e.g. the norm of kernel weights or feature map activation, for
pruning large CNNs after adaptation to fine-grained classification tasks
(Birds-200 and Flowers-102) relaying only on the first order gradient
information. We also show that pruning can lead to more than 10x theoretical
(5x practical) reduction in adapted 3D-convolutional filters with a small drop
in accuracy in a recurrent gesture classifier. Finally, we show results for the
large-scale ImageNet dataset to emphasize the flexibility of our approach.","['Pavlo Molchanov', 'Stephen Tyree', 'Tero Karras', 'Timo Aila', 'Jan Kautz']","['cs.LG', 'stat.ML']",2016-11-19 22:48:30+00:00
http://arxiv.org/abs/1611.06426v2,Conservative Contextual Linear Bandits,"Safety is a desirable property that can immensely increase the applicability
of learning algorithms in real-world decision-making problems. It is much
easier for a company to deploy an algorithm that is safe, i.e., guaranteed to
perform at least as well as a baseline. In this paper, we study the issue of
safety in contextual linear bandits that have application in many different
fields including personalized ad recommendation in online marketing. We
formulate a notion of safety for this class of algorithms. We develop a safe
contextual linear bandit algorithm, called conservative linear UCB (CLUCB),
that simultaneously minimizes its regret and satisfies the safety constraint,
i.e., maintains its performance above a fixed percentage of the performance of
a baseline strategy, uniformly over time. We prove an upper-bound on the regret
of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound
for the regret of the standard linear UCB algorithm that grows with the time
horizon and 2) a constant (does not grow with the time horizon) term that
accounts for the loss of being conservative in order to satisfy the safety
constraint. We empirically show that our algorithm is safe and validate our
theoretical analysis.","['Abbas Kazerouni', 'Mohammad Ghavamzadeh', 'Yasin Abbasi-Yadkori', 'Benjamin Van Roy']","['stat.ML', 'cs.LG']",2016-11-19 20:36:30+00:00
http://arxiv.org/abs/1611.06314v1,Determining the Veracity of Rumours on Twitter,"While social networks can provide an ideal platform for up-to-date
information from individuals across the world, it has also proved to be a place
where rumours fester and accidental or deliberate misinformation often emerges.
In this article, we aim to support the task of making sense from social media
data, and specifically, seek to build an autonomous message-classifier that
filters relevant and trustworthy information from Twitter. For our work, we
collected about 100 million public tweets, including users' past tweets, from
which we identified 72 rumours (41 true, 31 false). We considered over 80
trustworthiness measures including the authors' profile and past behaviour, the
social network connections (graphs), and the content of tweets themselves. We
ran modern machine-learning classifiers over those measures to produce
trustworthiness scores at various time windows from the outbreak of the rumour.
Such time-windows were key as they allowed useful insight into the progression
of the rumours. From our findings, we identified that our model was
significantly more accurate than similar studies in the literature. We also
identified critical attributes of the data that give rise to the
trustworthiness scores assigned. Finally we developed a software demonstration
that provides a visual user interface to allow the user to examine the
analysis.","['Georgios Giasemidis', 'Colin Singleton', 'Ioannis Agrafiotis', 'Jason R. C. Nurse', 'Alan Pilgrim', 'Chris Willis', 'Danica Vukadinovic Greetham']","['cs.SI', 'stat.ML']",2016-11-19 06:22:50+00:00
http://arxiv.org/abs/1611.06310v2,Local minima in training of neural networks,"There has been a lot of recent interest in trying to characterize the error
surface of deep models. This stems from a long standing question. Given that
deep networks are highly nonlinear systems optimized by local gradient methods,
why do they not seem to be affected by bad local minima? It is widely believed
that training of deep models using gradient methods works so well because the
error surface either has no local minima, or if they exist they need to be
close in value to the global minimum. It is known that such results hold under
very strong assumptions which are not satisfied by real models. In this paper
we present examples showing that for such theorem to be true additional
assumptions on the data, initialization schemes and/or the model classes have
to be made. We look at the particular case of finite size datasets. We
demonstrate that in this scenario one can construct counter-examples (datasets
or initialization schemes) when the network does become susceptible to bad
local minima over the weight space.","['Grzegorz Swirszcz', 'Wojciech Marian Czarnecki', 'Razvan Pascanu']","['stat.ML', 'cs.LG', 'cs.NE']",2016-11-19 05:49:22+00:00
http://arxiv.org/abs/1611.06265v2,Deep Clustering and Conventional Networks for Music Separation: Stronger Together,"Deep clustering is the first method to handle general audio separation
scenarios with multiple sources of the same type and an arbitrary number of
sources, performing impressively in speaker-independent speech separation
tasks. However, little is known about its effectiveness in other challenging
situations such as music source separation. Contrary to conventional networks
that directly estimate the source signals, deep clustering generates an
embedding for each time-frequency bin, and separates sources by clustering the
bins in the embedding space. We show that deep clustering outperforms
conventional networks on a singing voice separation task, in both matched and
mismatched conditions, even though conventional networks have the advantage of
end-to-end training for best signal approximation, presumably because its more
flexible objective engenders better regularization. Since the strengths of deep
clustering and conventional network architectures appear complementary, we
explore combining them in a single hybrid network trained via an approach akin
to multi-task learning. Remarkably, the combination significantly outperforms
either of its components.","['Yi Luo', 'Zhuo Chen', 'John R. Hershey', 'Jonathan Le Roux', 'Nima Mesgarani']","['stat.ML', 'cs.LG', 'cs.SD']",2016-11-18 22:33:05+00:00
http://arxiv.org/abs/1611.06245v1,Spikes as regularizers,"We present a confidence-based single-layer feed-forward learning algorithm
SPIRAL (Spike Regularized Adaptive Learning) relying on an encoding of
activation spikes. We adaptively update a weight vector relying on confidence
estimates and activation offsets relative to previous activity. We regularize
updates proportionally to item-level confidence and weight-specific support,
loosely inspired by the observation from neurophysiology that high spike rates
are sometimes accompanied by low temporal precision. Our experiments suggest
that the new learning algorithm SPIRAL is more robust and less prone to
overfitting than both the averaged perceptron and AROW.",['Anders SÃ¸gaard'],"['cs.NE', 'cs.LG', 'stat.ML']",2016-11-18 21:09:16+00:00
http://arxiv.org/abs/1611.06213v2,GaDei: On Scale-up Training As A Service For Deep Learning,"Deep learning (DL) training-as-a-service (TaaS) is an important emerging
industrial workload. The unique challenge of TaaS is that it must satisfy a
wide range of customers who have no experience and resources to tune DL
hyper-parameters, and meticulous tuning for each user's dataset is
prohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with
values that are applicable to all users. IBM Watson Natural Language Classifier
(NLC) service, the most popular IBM cognitive service used by thousands of
enterprise-level clients around the globe, is a typical TaaS service. By
evaluating the NLC workloads, we show that only the conservative
hyper-parameter setup (e.g., small mini-batch size and small learning rate) can
guarantee acceptable model accuracy for a wide range of customers. We further
justify theoretically why such a setup guarantees better model convergence in
general. Unfortunately, the small mini-batch size causes a high volume of
communication traffic in a parameter-server based system. We characterize the
high communication bandwidth requirement of TaaS using representative
industrial deep learning workloads and demonstrate that none of the
state-of-the-art scale-up or scale-out solutions can satisfy such a
requirement. We then present GaDei, an optimized shared-memory based scale-up
parameter server design. We prove that the designed protocol is deadlock-free
and it processes each gradient exactly once. Our implementation is evaluated on
both commercial benchmarks and public benchmarks to demonstrate that it
significantly outperforms the state-of-the-art parameter-server based
implementation while maintaining the required accuracy and our implementation
reaches near the best possible runtime performance, constrained only by the
hardware limitation. Furthermore, to the best of our knowledge, GaDei is the
only scale-up DL system that provides fault-tolerance.","['Wei Zhang', 'Minwei Feng', 'Yunhui Zheng', 'Yufei Ren', 'Yandong Wang', 'Ji Liu', 'Peng Liu', 'Bing Xiang', 'Li Zhang', 'Bowen Zhou', 'Fei Wang']","['stat.ML', 'cs.DC', 'cs.LG', 'C.1.3; D.1.3; I.2.6; I.2.7; H.3.4']",2016-11-18 20:06:27+00:00
http://arxiv.org/abs/1611.06194v2,Expert Gate: Lifelong Learning with a Network of Experts,"In this paper we introduce a model of lifelong learning, based on a Network
of Experts. New tasks / experts are learned and added to the model
sequentially, building on what was learned before. To ensure scalability of
this process,data from previous tasks cannot be stored and hence is not
available when learning a new task. A critical issue in such context, not
addressed in the literature so far, relates to the decision which expert to
deploy at test time. We introduce a set of gating autoencoders that learn a
representation for the task at hand, and, at test time, automatically forward
the test sample to the relevant expert. This also brings memory efficiency as
only one expert network has to be loaded into memory at any given time.
Further, the autoencoders inherently capture the relatedness of one task to
another, based on which the most relevant prior model to be used for training a
new expert, with finetuning or learning without-forgetting, can be selected. We
evaluate our method on image classification and video prediction problems.","['Rahaf Aljundi', 'Punarjay Chakravarty', 'Tinne Tuytelaars']","['cs.CV', 'cs.AI', 'stat.ML']",2016-11-18 18:50:15+00:00
http://arxiv.org/abs/1611.06188v2,Variable Computation in Recurrent Neural Networks,"Recurrent neural networks (RNNs) have been used extensively and with
increasing success to model various types of sequential data. Much of this
progress has been achieved through devising recurrent units and architectures
with the flexibility to capture complex statistics in the data, such as long
range dependency or localized attention phenomena. However, while many
sequential data (such as video, speech or language) can have highly variable
information flow, most recurrent models still consume input features at a
constant rate and perform a constant number of computations per time step,
which can be detrimental to both speed and model capacity. In this paper, we
explore a modification to existing recurrent units which allows them to learn
to vary the amount of computation they perform at each step, without prior
knowledge of the sequence's time structure. We show experimentally that not
only do our models require fewer operations, they also lead to better
performance overall on evaluation tasks.","['Yacine Jernite', 'Edouard Grave', 'Armand Joulin', 'Tomas Mikolov']","['stat.ML', 'cs.AI', 'cs.CL', 'cs.LG']",2016-11-18 18:13:46+00:00
http://arxiv.org/abs/1611.06175v1,Learning Interpretability for Visualizations using Adapted Cox Models through a User Experiment,"In order to be useful, visualizations need to be interpretable. This paper
uses a user-based approach to combine and assess quality measures in order to
better model user preferences. Results show that cluster separability measures
are outperformed by a neighborhood conservation measure, even though the former
are usually considered as intuitively representative of user motives. Moreover,
combining measures, as opposed to using a single measure, further improves
prediction performances.","['Adrien Bibal', 'Benoit FrÃ©nay']","['stat.ML', 'cs.AI', 'cs.HC', 'cs.LG']",2016-11-18 17:52:23+00:00
http://arxiv.org/abs/1611.06172v2,Parallelizing Word2Vec in Multi-Core and Many-Core Architectures,"Word2vec is a widely used algorithm for extracting low-dimensional vector
representations of words. State-of-the-art algorithms including those by
Mikolov et al. have been parallelized for multi-core CPU architectures, but are
based on vector-vector operations with ""Hogwild"" updates that are
memory-bandwidth intensive and do not efficiently use computational resources.
In this paper, we propose ""HogBatch"" by improving reuse of various data
structures in the algorithm through the use of minibatching and negative sample
sharing, hence allowing us to express the problem using matrix multiply
operations. We also explore different techniques to distribute word2vec
computation across nodes in a compute cluster, and demonstrate good strong
scalability up to 32 nodes. The new algorithm is particularly suitable for
modern multi-core/many-core architectures, especially Intel's latest Knights
Landing processors, and allows us to scale up the computation near linearly
across cores and nodes, and process hundreds of millions of words per second,
which is the fastest word2vec implementation to the best of our knowledge.","['Shihao Ji', 'Nadathur Satish', 'Sheng Li', 'Pradeep Dubey']","['cs.DC', 'stat.ML']",2016-11-18 17:47:44+00:00
http://arxiv.org/abs/1611.06148v2,Compacting Neural Network Classifiers via Dropout Training,"We introduce dropout compaction, a novel method for training feed-forward
neural networks which realizes the performance gains of training a large model
with dropout regularization, yet extracts a compact neural network for run-time
efficiency. In the proposed method, we introduce a sparsity-inducing prior on
the per unit dropout retention probability so that the optimizer can
effectively prune hidden units during training. By changing the prior
hyperparameters, we can control the size of the resulting network. We performed
a systematic comparison of dropout compaction and competing methods on several
real-world speech recognition tasks and found that dropout compaction achieved
comparable accuracy with fewer than 50% of the hidden units, translating to a
2.5x speedup in run-time.","['Yotaro Kubo', 'George Tucker', 'Simon Wiesler']","['stat.ML', 'cs.LG', 'cs.NE']",2016-11-18 16:20:41+00:00
http://arxiv.org/abs/1611.06132v1,Faster variational inducing input Gaussian process classification,"Gaussian processes (GP) provide a prior over functions and allow finding
complex regularities in data. Gaussian processes are successfully used for
classification/regression problems and dimensionality reduction. In this work
we consider the classification problem only. The complexity of standard methods
for GP-classification scales cubically with the size of the training dataset.
This complexity makes them inapplicable to big data problems. Therefore, a
variety of methods were introduced to overcome this limitation. In the paper we
focus on methods based on so called inducing inputs. This approach is based on
variational inference and proposes a particular lower bound for marginal
likelihood (evidence). This bound is then maximized w.r.t. parameters of kernel
function of the Gaussian process, thus fitting the model to data. The
computational complexity of this method is $O(nm^2)$, where $m$ is the number
of inducing inputs used by the model and is assumed to be substantially smaller
than the size of the dataset $n$. Recently, a new evidence lower bound for
GP-classification problem was introduced. It allows using stochastic
optimization, which makes it suitable for big data problems. However, the new
lower bound depends on $O(m^2)$ variational parameter, which makes optimization
challenging in case of big m. In this work we develop a new approach for
training inducing input GP models for classification problems. Here we use
quadratic approximation of several terms in the aforementioned evidence lower
bound, obtaining analytical expressions for optimal values of most of the
parameters in the optimization, thus sufficiently reducing the dimension of
optimization space. In our experiments we achieve as well or better results,
compared to the existing method. Moreover, our method doesn't require the user
to manually set the learning rate, making it more practical, than the existing
method.","['Pavel Izmailov', 'Dmitry Kropotov']","['cs.LG', 'cs.AI', 'stat.ML']",2016-11-18 15:53:50+00:00
http://arxiv.org/abs/1611.06094v1,Generalizing diffuse interface methods on graphs: non-smooth potentials and hypergraphs,"Diffuse interface methods have recently been introduced for the task of
semi-supervised learning. The underlying model is well-known in materials
science but was extended to graphs using a Ginzburg--Landau functional and the
graph Laplacian. We here generalize the previously proposed model by a
non-smooth potential function. Additionally, we show that the diffuse interface
method can be used for the segmentation of data coming from hypergraphs. For
this we show that the graph Laplacian in almost all cases is derived from
hypergraph information. Additionally, we show that the formerly introduced
hypergraph Laplacian coming from a relaxed optimization problem is well suited
to be used within the diffuse interface method. We present computational
experiments for graph and hypergraph Laplacians.","['Jessica Bosch', 'Steffen Klamt', 'Martin Stoll']","['stat.ML', 'math.NA']",2016-11-18 14:23:10+00:00
http://arxiv.org/abs/1611.06080v1,A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Process Regression,"While much research effort has been dedicated to scaling up sparse Gaussian
process (GP) models based on inducing variables for big data, little attention
is afforded to the other less explored class of low-rank GP approximations that
exploit the sparse spectral representation of a GP kernel. This paper presents
such an effort to advance the state of the art of sparse spectrum GP models to
achieve competitive predictive performance for massive datasets. Our
generalized framework of stochastic variational Bayesian sparse spectrum GP
(sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment
of the spectral frequencies to avoid overfitting, modeling these frequencies
jointly in its variational distribution to enable their interaction a
posteriori, and exploiting local data for boosting the predictive performance.
However, such structural improvements result in a variational lower bound that
is intractable to be optimized. To resolve this, we exploit a variational
parameterization trick to make it amenable to stochastic optimization.
Interestingly, the resulting stochastic gradient has a linearly decomposable
structure that can be exploited to refine our stochastic optimization method to
incur constant time per iteration while preserving its property of being an
unbiased estimator of the exact gradient of the variational lower bound.
Empirical evaluation on real-world datasets shows that sVBSSGP outperforms
state-of-the-art stochastic implementations of sparse GP models.","['Quang Minh Hoang', 'Trong Nghia Hoang', 'Kian Hsiang Low']","['stat.ML', 'cs.LG']",2016-11-18 14:00:48+00:00
http://arxiv.org/abs/1611.06066v1,Deriving reproducible biomarkers from multi-site resting-state data: An Autism-based example,"Resting-state functional Magnetic Resonance Imaging (R-fMRI) holds the
promise to reveal functional biomarkers of neuropsychiatric disorders. However,
extracting such biomarkers is challenging for complex multi-faceted
neuropatholo-gies, such as autism spectrum disorders. Large multi-site datasets
increase sample sizes to compensate for this complexity, at the cost of
uncontrolled heterogeneity. This heterogeneity raises new challenges, akin to
those face in realistic diagnostic applications. Here, we demonstrate the
feasibility of inter-site classification of neuropsychiatric status, with an
application to the Autism Brain Imaging Data Exchange (ABIDE) database, a large
(N=871) multi-site autism dataset. For this purpose, we investigate pipelines
that extract the most predictive biomarkers from the data. These R-fMRI
pipelines build participant-specific connectomes from functionally-defined
brain areas. Connectomes are then compared across participants to learn
patterns of connectivity that differentiate typical controls from individuals
with autism. We predict this neuropsychiatric status for participants from the
same acquisition sites or different, unseen, ones. Good choices of methods for
the various steps of the pipeline lead to 67% prediction accuracy on the full
ABIDE data, which is significantly better than previously reported results. We
perform extensive validation on multiple subsets of the data defined by
different inclusion criteria. These enables detailed analysis of the factors
contributing to successful connectome-based prediction. First, prediction
accuracy improves as we include more subjects, up to the maximum amount of
subjects available. Second, the definition of functional brain areas is of
paramount importance for biomarker discovery: brain areas extracted from large
R-fMRI datasets outperform reference atlases in the classification tasks.","['Alexandre Abraham', 'Michael Milham', 'Adriana Di Martino', 'R. Cameron Craddock', 'Dimitris Samaras', 'Bertrand Thirion', 'GaÃ«l Varoquaux']","['stat.ML', 'q-bio.NC']",2016-11-18 13:31:47+00:00
http://arxiv.org/abs/1611.05977v1,Robust and Scalable Column/Row Sampling from Corrupted Big Data,"Conventional sampling techniques fall short of drawing descriptive sketches
of the data when the data is grossly corrupted as such corruptions break the
low rank structure required for them to perform satisfactorily. In this paper,
we present new sampling algorithms which can locate the informative columns in
presence of severe data corruptions. In addition, we develop new scalable
randomized designs of the proposed algorithms. The proposed approach is
simultaneously robust to sparse corruption and outliers and substantially
outperforms the state-of-the-art robust sampling algorithms as demonstrated by
experiments conducted using both real and synthetic data.","['Mostafa Rahmani', 'George Atia']","['cs.LG', 'cs.NA', 'stat.AP', 'stat.ML']",2016-11-18 05:07:21+00:00
http://arxiv.org/abs/1611.05940v2,Finding Alternate Features in Lasso,"We propose a method for finding alternate features missing in the Lasso
optimal solution. In ordinary Lasso problem, one global optimum is obtained and
the resulting features are interpreted as task-relevant features. However, this
can overlook possibly relevant features not selected by the Lasso. With the
proposed method, we can provide not only the Lasso optimal solution but also
possible alternate features to the Lasso solution. We show that such alternate
features can be computed efficiently by avoiding redundant computations. We
also demonstrate how the proposed method works in the 20 newsgroup data, which
shows that reasonable features are found as alternate features.","['Satoshi Hara', 'Takanori Maehara']",['stat.ML'],2016-11-18 01:11:34+00:00
http://arxiv.org/abs/1611.05934v1,Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models,"As deep neural networks continue to revolutionize various application
domains, there is increasing interest in making these powerful models more
understandable and interpretable, and narrowing down the causes of good and bad
predictions. We focus on recurrent neural networks, state of the art models in
speech recognition and translation. Our approach to increasing interpretability
is by combining a long short-term memory (LSTM) model with a hidden Markov
model (HMM), a simpler and more transparent model. We add the HMM state
probabilities to the output layer of the LSTM, and then train the HMM and LSTM
either sequentially or jointly. The LSTM can make use of the information from
the HMM, and fill in the gaps when the HMM is not performing well. A small
hybrid model usually performs better than a standalone LSTM of the same size,
especially on smaller data sets. We test the algorithms on text data and
medical time series data, and find that the LSTM and HMM learn complementary
information about the features in the text.","['Viktoriya Krakovna', 'Finale Doshi-Velez']","['stat.ML', 'cs.LG']",2016-11-18 00:13:32+00:00
http://arxiv.org/abs/1611.05923v3,"""Influence Sketching"": Finding Influential Samples In Large-Scale Regressions","There is an especially strong need in modern large-scale data analysis to
prioritize samples for manual inspection. For example, the inspection could
target important mislabeled samples or key vulnerabilities exploitable by an
adversarial attack. In order to solve the ""needle in the haystack"" problem of
which samples to inspect, we develop a new scalable version of Cook's distance,
a classical statistical technique for identifying samples which unusually
strongly impact the fit of a regression model (and its downstream predictions).
In order to scale this technique up to very large and high-dimensional
datasets, we introduce a new algorithm which we call ""influence sketching.""
Influence sketching embeds random projections within the influence computation;
in particular, the influence score is calculated using the randomly projected
pseudo-dataset from the post-convergence Generalized Linear Model (GLM). We
validate that influence sketching can reliably and successfully discover
influential samples by applying the technique to a malware detection dataset of
over 2 million executable files, each represented with almost 100,000 features.
For example, we find that randomly deleting approximately 10% of training
samples reduces predictive accuracy only slightly from 99.47% to 99.45%,
whereas deleting the same number of samples with high influence sketch scores
reduces predictive accuracy all the way down to 90.24%. Moreover, we find that
influential samples are especially likely to be mislabeled. In the case study,
we manually inspect the most influential samples, and find that influence
sketching pointed us to new, previously unidentified pieces of malware.","['Mike Wojnowicz', 'Ben Cruz', 'Xuan Zhao', 'Brian Wallace', 'Matt Wolff', 'Jay Luan', 'Caleb Crable']","['stat.ML', 'cs.LG']",2016-11-17 22:23:08+00:00
http://arxiv.org/abs/1611.05817v1,Nothing Else Matters: Model-Agnostic Explanations By Identifying Prediction Invariance,"At the core of interpretable machine learning is the question of whether
humans are able to make accurate predictions about a model's behavior. Assumed
in this question are three properties of the interpretable output: coverage,
precision, and effort. Coverage refers to how often humans think they can
predict the model's behavior, precision to how accurate humans are in those
predictions, and effort is either the up-front effort required in interpreting
the model, or the effort required to make predictions about a model's behavior.
  In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that
produces high-precision rule-based explanations for which the coverage
boundaries are very clear. We compare aLIME to linear LIME with simulated
experiments, and demonstrate the flexibility of aLIME with qualitative examples
from a variety of domains and tasks.","['Marco Tulio Ribeiro', 'Sameer Singh', 'Carlos Guestrin']","['stat.ML', 'cs.AI', 'cs.LG']",2016-11-17 19:07:00+00:00
http://arxiv.org/abs/1611.05780v4,Gap Safe screening rules for sparsity enforcing penalties,"In high dimensional regression settings, sparsity enforcing penalties have
proved useful to regularize the data-fitting term. A recently introduced
technique called screening rules propose to ignore some variables in the
optimization leveraging the expected sparsity of the solutions and consequently
leading to faster solvers. When the procedure is guaranteed not to discard
variables wrongly the rules are said to be safe. In this work, we propose a
unifying framework for generalized linear models regularized with standard
sparsity enforcing penalties such as $\ell_1$ or $\ell_1/\ell_2$ norms. Our
technique allows to discard safely more variables than previously considered
safe rules, particularly for low regularization parameters. Our proposed Gap
Safe rules (so called because they rely on duality gap computation) can cope
with any iterative solver but are particularly well suited to (block)
coordinate descent methods. Applied to many standard learning tasks, Lasso,
Sparse-Group Lasso, multi-task Lasso, binary and multinomial logistic
regression, etc., we report significant speed-ups compared to previously
proposed safe rules on all tested data sets.","['Eugene Ndiaye', 'Olivier Fercoq', 'Alexandre Gramfort', 'Joseph Salmon']","['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",2016-11-17 16:55:12+00:00
http://arxiv.org/abs/1611.05778v2,Towards the Modeling of Behavioral Trajectories of Users in Online Social Media,"In this paper, we introduce a methodology that allows to model behavioral
trajectories of users in online social media. First, we illustrate how to
leverage the probabilistic framework provided by Hidden Markov Models (HMMs) to
represent users by embedding the temporal sequences of actions they performed
online. We then derive a model-based distance between trained HMMs, and we use
spectral clustering to find homogeneous clusters of users showing similar
behavioral trajectories. To provide platform-agnostic results, we apply the
proposed approach to two different online social media --- i.e. Facebook and
YouTube. We conclude discussing merits and limitations of our approach as well
as future and promising research directions.",['Alessandro Bessi'],"['cs.CY', 'cs.SI', 'physics.soc-ph', 'stat.AP', 'stat.ML']",2016-11-17 16:53:50+00:00
http://arxiv.org/abs/1611.05763v3,Learning to reinforcement learn,"In recent years deep reinforcement learning (RL) systems have attained
superhuman performance in a number of challenging task domains. However, a
major limitation of such applications is their demand for massive amounts of
training data. A critical present objective is thus to develop deep RL methods
that can adapt rapidly to new tasks. In the present work we introduce a novel
approach to this challenge, which we refer to as deep meta-reinforcement
learning. Previous work has shown that recurrent networks can support
meta-learning in a fully supervised context. We extend this approach to the RL
setting. What emerges is a system that is trained using one RL algorithm, but
whose recurrent dynamics implement a second, quite separate RL procedure. This
second, learned RL algorithm can differ from the original one in arbitrary
ways. Importantly, because it is learned, it is configured to exploit structure
in the training domain. We unpack these points in a series of seven
proof-of-concept experiments, each of which examines a key aspect of deep
meta-RL. We consider prospects for extending and scaling up the approach, and
also point out some potentially important implications for neuroscience.","['Jane X Wang', 'Zeb Kurth-Nelson', 'Dhruva Tirumala', 'Hubert Soyer', 'Joel Z Leibo', 'Remi Munos', 'Charles Blundell', 'Dharshan Kumaran', 'Matt Botvinick']","['cs.LG', 'cs.AI', 'stat.ML']",2016-11-17 16:29:11+00:00
http://arxiv.org/abs/1611.05751v1,A Multi-Modal Graph-Based Semi-Supervised Pipeline for Predicting Cancer Survival,"Cancer survival prediction is an active area of research that can help
prevent unnecessary therapies and improve patient's quality of life. Gene
expression profiling is being widely used in cancer studies to discover
informative biomarkers that aid predict different clinical endpoint prediction.
We use multiple modalities of data derived from RNA deep-sequencing (RNA-seq)
to predict survival of cancer patients. Despite the wealth of information
available in expression profiles of cancer tumors, fulfilling the
aforementioned objective remains a big challenge, for the most part, due to the
paucity of data samples compared to the high dimension of the expression
profiles. As such, analysis of transcriptomic data modalities calls for
state-of-the-art big-data analytics techniques that can maximally use all the
available data to discover the relevant information hidden within a significant
amount of noise. In this paper, we propose a pipeline that predicts cancer
patients' survival by exploiting the structure of the input (manifold learning)
and by leveraging the unlabeled samples using Laplacian support vector
machines, a graph-based semi supervised learning (GSSL) paradigm. We show that
under certain circumstances, no single modality per se will result in the best
accuracy and by fusing different models together via a stacked generalization
strategy, we may boost the accuracy synergistically. We apply our approach to
two cancer datasets and present promising results. We maintain that a similar
pipeline can be used for predictive tasks where labeled samples are expensive
to acquire.","['Hamid Reza Hassanzadeh', 'John H. Phan', 'May D. Wang']","['cs.LG', 'stat.ML']",2016-11-17 16:01:36+00:00
http://arxiv.org/abs/1611.05724v2,Unimodal Thompson Sampling for Graph-Structured Arms,"We study, to the best of our knowledge, the first Bayesian algorithm for
unimodal Multi-Armed Bandit (MAB) problems with graph structure. In this
setting, each arm corresponds to a node of a graph and each edge provides a
relationship, unknown to the learner, between two nodes in terms of expected
reward. Furthermore, for any node of the graph there is a path leading to the
unique node providing the maximum expected reward, along which the expected
reward is monotonically increasing. Previous results on this setting describe
the behavior of frequentist MAB algorithms. In our paper, we design a Thompson
Sampling-based algorithm whose asymptotic pseudo-regret matches the lower bound
for the considered setting. We show that -as it happens in a wide number of
scenarios- Bayesian MAB algorithms dramatically outperform frequentist ones. In
particular, we provide a thorough experimental evaluation of the performance of
our and state-of-the-art algorithms as the properties of the graph vary.","['Stefano Paladino', 'Francesco TrovÃ²', 'Marcello Restelli', 'Nicola Gatti']","['cs.LG', 'stat.ML']",2016-11-17 14:59:55+00:00
http://arxiv.org/abs/1611.05722v1,"GENESIM: genetic extraction of a single, interpretable model","Models obtained by decision tree induction techniques excel in being
interpretable.However, they can be prone to overfitting, which results in a low
predictive performance. Ensemble techniques are able to achieve a higher
accuracy. However, this comes at a cost of losing interpretability of the
resulting model. This makes ensemble techniques impractical in applications
where decision support, instead of decision making, is crucial.
  To bridge this gap, we present the GENESIM algorithm that transforms an
ensemble of decision trees to a single decision tree with an enhanced
predictive performance by using a genetic algorithm. We compared GENESIM to
prevalent decision tree induction and ensemble techniques using twelve publicly
available data sets. The results show that GENESIM achieves a better predictive
performance on most of these data sets than decision tree induction techniques
and a predictive performance in the same order of magnitude as the ensemble
techniques. Moreover, the resulting model of GENESIM has a very low complexity,
making it very interpretable, in contrast to ensemble techniques.","['Gilles Vandewiele', 'Olivier Janssens', 'Femke Ongenae', 'Filip De Turck', 'Sofie Van Hoecke']","['stat.ML', 'cs.LG']",2016-11-17 14:58:35+00:00
http://arxiv.org/abs/1611.05559v2,Boosting Variational Inference,"Variational inference (VI) provides fast approximations of a Bayesian
posterior in part because it formulates posterior approximation as an
optimization problem: to find the closest distribution to the exact posterior
over some family of distributions. For practical reasons, the family of
distributions in VI is usually constrained so that it does not include the
exact posterior, even as a limit point. Thus, no matter how long VI is run, the
resulting approximation will not approach the exact posterior. We propose to
instead consider a more flexible approximating family consisting of all
possible finite mixtures of a parametric base distribution (e.g., Gaussian).
For efficient inference, we borrow ideas from gradient boosting to develop an
algorithm we call boosting variational inference (BVI). BVI iteratively
improves the current approximation by mixing it with a new component from the
base distribution family and thereby yields progressively more accurate
posterior approximations as more computing time is spent. Unlike a number of
common VI variants including mean-field VI, BVI is able to capture
multimodality, general posterior covariance, and nonstandard posterior shapes.","['Fangjian Guo', 'Xiangyu Wang', 'Kai Fan', 'Tamara Broderick', 'David B. Dunson']","['stat.ML', 'cs.LG']",2016-11-17 04:19:16+00:00
http://arxiv.org/abs/1611.05545v4,Stochastic Gradient Descent in Continuous Time,"Stochastic gradient descent in continuous time (SGDCT) provides a
computationally efficient method for the statistical learning of
continuous-time models, which are widely used in science, engineering, and
finance. The SGDCT algorithm follows a (noisy) descent direction along a
continuous stream of data. SGDCT performs an online parameter update in
continuous time, with the parameter updates $\theta_t$ satisfying a stochastic
differential equation. We prove that $\lim_{t \rightarrow \infty} \nabla \bar
g(\theta_t) = 0$ where $\bar g$ is a natural objective function for the
estimation of the continuous-time dynamics. The convergence proof leverages
ergodicity by using an appropriate Poisson equation to help describe the
evolution of the parameters for large times. SGDCT can also be used to solve
continuous-time optimization problems, such as American options. For certain
continuous-time problems, SGDCT has some promising advantages compared to a
traditional stochastic gradient descent algorithm. As an example application,
SGDCT is combined with a deep neural network to price high-dimensional American
options (up to 100 dimensions).","['Justin Sirignano', 'Konstantinos Spiliopoulos']","['math.PR', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",2016-11-17 03:02:01+00:00
http://arxiv.org/abs/1611.05527v1,Automatic Node Selection for Deep Neural Networks using Group Lasso Regularization,"We examine the effect of the Group Lasso (gLasso) regularizer in selecting
the salient nodes of Deep Neural Network (DNN) hidden layers by applying a
DNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of
gLasso regularization, one for outgoing weight vectors and another for incoming
weight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096
nodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment
results demonstrate that our DNN training, in which the gLasso regularizer was
embedded, successfully selected the hidden layer nodes that are necessary and
sufficient for achieving high classification power.","['Tsubasa Ochiai', 'Shigeki Matsuda', 'Hideyuki Watanabe', 'Shigeru Katagiri']","['cs.CL', 'cs.LG', 'stat.ML']",2016-11-17 01:43:01+00:00
http://arxiv.org/abs/1611.05487v2,Algebraic multigrid support vector machines,"The support vector machine is a flexible optimization-based technique widely
used for classification problems. In practice, its training part becomes
computationally expensive on large-scale data sets because of such reasons as
the complexity and number of iterations in parameter fitting methods,
underlying optimization solvers, and nonlinearity of kernels. We introduce a
fast multilevel framework for solving support vector machine models that is
inspired by the algebraic multigrid. Significant improvement in the running has
been achieved without any loss in the quality. The proposed technique is highly
beneficial on imbalanced sets. We demonstrate computational results on publicly
available and industrial data sets.","['Ehsan Sadrfaridpour', 'Sandeep Jeereddy', 'Ken Kennedy', 'Andre Luckow', 'Talayeh Razzaghi', 'Ilya Safro']","['stat.ML', 'cs.DS', 'cs.LG', 'stat.CO']",2016-11-16 22:32:50+00:00
http://arxiv.org/abs/1611.05469v1,Embedding Projector: Interactive Visualization and Interpretation of Embeddings,"Embeddings are ubiquitous in machine learning, appearing in recommender
systems, NLP, and many other applications. Researchers and developers often
need to explore the properties of a specific embedding, and one way to analyze
embeddings is to visualize them. We present the Embedding Projector, a tool for
interactive visualization and interpretation of embeddings.","['Daniel Smilkov', 'Nikhil Thorat', 'Charles Nicholson', 'Emily Reif', 'Fernanda B. ViÃ©gas', 'Martin Wattenberg']","['stat.ML', 'cs.HC']",2016-11-16 21:21:11+00:00
http://arxiv.org/abs/1611.05433v1,ROS Regression: Integrating Regularization and Optimal Scaling Regression,"In this paper we combine two important extensions of ordinary least squares
regression: regularization and optimal scaling. Optimal scaling (sometimes also
called optimal scoring) has originally been developed for categorical data, and
the process finds quantifications for the categories that are optimal for the
regression model in the sense that they maximize the multiple correlation.
Although the optimal scaling method was developed initially for variables with
a limited number of categories, optimal transformations of continuous variables
are a special case. We will consider a variety of transformation types;
typically we use step functions for categorical variables, and smooth (spline)
functions for continuous variables. Both types of functions can be restricted
to be monotonic, preserving the ordinal information in the data. In addition to
optimal scaling, three regularization methods will be considered: Ridge
regression, the Lasso, and the Elastic Net. The resulting method will be called
ROS Regression (Regularized Optimal Scaling Regression. We will show that the
basic OS algorithm provides straightforward and efficient estimation of the
regularized regression coefficients, automatically gives the Group Lasso and
Blockwise Sparse Regression, and extends them with monotonicity properties. We
will show that Optimal Scaling linearizes nonlinear relationships between
predictors and outcome, and improves upon the condition of the predictor
correlation matrix, increasing (on average) the conditional independence of the
predictors. Alternative options for regularization of either regression
coefficients or category quantifications are mentioned. Extended examples are
provided.
  Keywords: Categorical Data, Optimal Scaling, Conditional Independence, Step
Functions, Splines, Monotonic Transformations, Regularization, Lasso, Elastic
Net, Group Lasso, Blockwise Sparse Regression.","['Jacqueline J. Meulman', 'Anita J. van der Kooij']",['stat.ML'],2016-11-16 20:40:19+00:00
http://arxiv.org/abs/1611.05425v1,ProjE: Embedding Projection for Knowledge Graph Completion,"With the large volume of new information created every day, determining the
validity of information in a knowledge graph and filling in its missing parts
are crucial tasks for many researchers and practitioners. To address this
challenge, a number of knowledge graph completion methods have been developed
using low-dimensional graph embeddings. Although researchers continue to
improve these models using an increasingly complex feature space, we show that
simple changes in the architecture of the underlying model can outperform
state-of-the-art models without the need for complex feature engineering. In
this work, we present a shared variable neural network model called ProjE that
fills-in missing information in a knowledge graph by learning joint embeddings
of the knowledge graph's entities and edges, and through subtle, but important,
changes to the standard loss function. In doing so, ProjE has a parameter size
that is smaller than 11 out of 15 existing methods while performing $37\%$
better than the current-best method on standard datasets. We also show, via a
new fact checking task, that ProjE is capable of accurately determining the
veracity of many declarative statements.","['Baoxu Shi', 'Tim Weninger']","['cs.AI', 'stat.ML']",2016-11-16 20:09:08+00:00
http://arxiv.org/abs/1611.05407v1,A Semidefinite Program for Structured Blockmodels,"Semidefinite programs have recently been developed for the problem of
community detection, which may be viewed as a special case of the stochastic
blockmodel. Here, we develop a semidefinite program that can be tailored to
other instances of the blockmodel, such as non-assortative networks and
overlapping communities. We establish label recovery in sparse settings, with
conditions that are analogous to recent results for community detection. In
settings where the data is not generated by a blockmodel, we give an oracle
inequality that bounds excess risk relative to the best blockmodel
approximation. Simulations are presented for community detection, for
overlapping communities, and for latent space models.",['David Choi'],"['math.ST', 'stat.ML', 'stat.TH']",2016-11-16 19:00:47+00:00
http://arxiv.org/abs/1611.05402v3,"The ZipML Framework for Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning","Recently there has been significant interest in training machine-learning
models at low precision: by reducing precision, one can reduce computation and
communication by one order of magnitude. We examine training at reduced
precision, both from a theoretical and practical perspective, and ask: is it
possible to train models at end-to-end low precision with provable guarantees?
Can this lead to consistent order-of-magnitude speedups? We present a framework
called ZipML to answer these questions. For linear models, the answer is yes.
We develop a simple framework based on one simple but novel strategy called
double sampling. Our framework is able to execute training at low precision
with no bias, guaranteeing convergence, whereas naive quantization would
introduce significant bias. We validate our framework across a range of
applications, and show that it enables an FPGA prototype that is up to 6.5x
faster than an implementation using full 32-bit precision. We further develop a
variance-optimal stochastic quantization strategy and show that it can make a
significant difference in a variety of settings. When applied to linear models
together with double sampling, we save up to another 1.7x in data movement
compared with uniform quantization. When training deep networks with quantized
models, we achieve higher accuracy than the state-of-the-art XNOR-Net. Finally,
we extend our framework through approximation to non-linear models, such as
SVM. We show that, although using low-precision data induces bias, we can
appropriately bound and control the bias. We find in practice 8-bit precision
is often sufficient to converge to the correct solution. Interestingly,
however, in practice we notice that our framework does not always outperform
the naive rounding approach. We discuss this negative result in detail.","['Hantian Zhang', 'Jerry Li', 'Kaan Kara', 'Dan Alistarh', 'Ji Liu', 'Ce Zhang']","['cs.LG', 'stat.ML']",2016-11-16 18:45:09+00:00
http://arxiv.org/abs/1611.05378v1,Spectral Convolution Networks,"Previous research has shown that computation of convolution in the frequency
domain provides a significant speedup versus traditional convolution network
implementations. However, this performance increase comes at the expense of
repeatedly computing the transform and its inverse in order to apply other
network operations such as activation, pooling, and dropout. We show,
mathematically, how convolution and activation can both be implemented in the
frequency domain using either the Fourier or Laplace transformation. The main
contributions are a description of spectral activation under the Fourier
transform and a further description of an efficient algorithm for computing
both convolution and activation under the Laplace transform. By computing both
the convolution and activation functions in the frequency domain, we can reduce
the number of transforms required, as well as reducing overall complexity. Our
description of a spectral activation function, together with existing spectral
analogs of other network functions may then be used to compose a fully spectral
implementation of a convolution network.","['Maria Francesca', 'Arthur Hughes', 'David Gregg']","['cs.LG', 'stat.ML']",2016-11-16 17:32:09+00:00
http://arxiv.org/abs/1611.05368v1,Neural Style Representations and the Large-Scale Classification of Artistic Style,"The artistic style of a painting is a subtle aesthetic judgment used by art
historians for grouping and classifying artwork. The recently introduced
`neural-style' algorithm substantially succeeds in merging the perceived
artistic style of one image or set of images with the perceived content of
another. In light of this and other recent developments in image analysis via
convolutional neural networks, we investigate the effectiveness of a
`neural-style' representation for classifying the artistic style of paintings.",['Jeremiah Johnson'],"['cs.CV', 'cs.AI', 'cs.IR', 'stat.AP', 'stat.ML']",2016-11-16 17:04:04+00:00
http://arxiv.org/abs/1611.05209v1,Deep Variational Inference Without Pixel-Wise Reconstruction,"Variational autoencoders (VAEs), that are built upon deep neural networks
have emerged as popular generative models in computer vision. Most of the work
towards improving variational autoencoders has focused mainly on making the
approximations to the posterior flexible and accurate, leading to tremendous
progress. However, there have been limited efforts to replace pixel-wise
reconstruction, which have known shortcomings. In this work, we use real-valued
non-volume preserving transformations (real NVP) to exactly compute the
conditional likelihood of the data given the latent distribution. We show that
a simple VAE with this form of reconstruction is competitive with complicated
VAE structures, on image modeling tasks. As part of our model, we develop
powerful conditional coupling layers that enable real NVP to learn with fewer
intermediate layers.","['Siddharth Agrawal', 'Ambedkar Dukkipati']","['stat.ML', 'cs.CV', 'cs.LG']",2016-11-16 10:20:10+00:00
http://arxiv.org/abs/1611.05181v3,Graph Learning from Data under Structural and Laplacian Constraints,"Graphs are fundamental mathematical structures used in various fields to
represent data, signals and processes. In this paper, we propose a novel
framework for learning/estimating graphs from data. The proposed framework
includes (i) formulation of various graph learning problems, (ii) their
probabilistic interpretations and (iii) associated algorithms. Specifically,
graph learning problems are posed as estimation of graph Laplacian matrices
from some observed data under given structural constraints (e.g., graph
connectivity and sparsity level). From a probabilistic perspective, the
problems of interest correspond to maximum a posteriori (MAP) parameter
estimation of Gaussian-Markov random field (GMRF) models, whose precision
(inverse covariance) is a graph Laplacian matrix. For the proposed graph
learning problems, specialized algorithms are developed by incorporating the
graph Laplacian and structural constraints. The experimental results
demonstrate that the proposed algorithms outperform the current
state-of-the-art methods in terms of accuracy and computational efficiency.","['Hilmi E. Egilmez', 'Eduardo Pavez', 'Antonio Ortega']","['cs.LG', 'stat.ML']",2016-11-16 08:11:14+00:00
http://arxiv.org/abs/1611.05162v4,Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee,"We introduce and analyze a new technique for model reduction for deep neural
networks. While large networks are theoretically capable of learning
arbitrarily complex models, overfitting and model redundancy negatively affects
the prediction accuracy and model variance. Our Net-Trim algorithm prunes
(sparsifies) a trained network layer-wise, removing connections at each layer
by solving a convex optimization program. This program seeks a sparse set of
weights at each layer that keeps the layer inputs and outputs consistent with
the originally trained model. The algorithms and associated analysis are
applicable to neural networks operating with the rectified linear unit (ReLU)
as the nonlinear activation. We present both parallel and cascade versions of
the algorithm. While the latter can achieve slightly simpler models with the
same generalization performance, the former can be computed in a distributed
manner. In both cases, Net-Trim significantly reduces the number of connections
in the network, while also providing enough regularization to slightly reduce
the generalization error. We also provide a mathematical analysis of the
consistency between the initial network and the retrained model. To analyze the
model sample complexity, we derive the general sufficient conditions for the
recovery of a sparse transform matrix. For a single layer taking independent
Gaussian random vectors of length $N$ as inputs, we show that if the network
response can be described using a maximum number of $s$ non-zero weights per
node, these weights can be learned from $\mathcal{O}(s\log N)$ samples.","['Alireza Aghasi', 'Afshin Abdi', 'Nam Nguyen', 'Justin Romberg']","['cs.LG', 'stat.ML']",2016-11-16 06:34:41+00:00
http://arxiv.org/abs/1611.05146v1,A Semi-Markov Switching Linear Gaussian Model for Censored Physiological Data,"Critically ill patients in regular wards are vulnerable to unanticipated
clinical dete- rioration which requires timely transfer to the intensive care
unit (ICU). To allow for risk scoring and patient monitoring in such a setting,
we develop a novel Semi- Markov Switching Linear Gaussian Model (SSLGM) for the
inpatients' physiol- ogy. The model captures the patients' latent clinical
states and their corresponding observable lab tests and vital signs. We present
an efficient unsupervised learn- ing algorithm that capitalizes on the
informatively censored data in the electronic health records (EHR) to learn the
parameters of the SSLGM; the learned model is then used to assess the new
inpatients' risk for clinical deterioration in an online fashion, allowing for
timely ICU admission. Experiments conducted on a het- erogeneous cohort of
6,094 patients admitted to a large academic medical center show that the
proposed model significantly outperforms the currently deployed risk scores
such as Rothman index, MEWS, SOFA and APACHE.","['Ahmed M. Alaa', 'Jinsung Yoon', 'Scott Hu', 'Mihaela van der Schaar']","['cs.LG', 'stat.ML']",2016-11-16 05:11:36+00:00
http://arxiv.org/abs/1611.05136v1,Machine Learning Approach for Skill Evaluation in Robotic-Assisted Surgery,"Evaluating surgeon skill has predominantly been a subjective task.
Development of objective methods for surgical skill assessment are of increased
interest. Recently, with technological advances such as robotic-assisted
minimally invasive surgery (RMIS), new opportunities for objective and
automated assessment frameworks have arisen. In this paper, we applied machine
learning methods to automatically evaluate performance of the surgeon in RMIS.
Six important movement features were used in the evaluation including
completion time, path length, depth perception, speed, smoothness and
curvature. Different classification methods applied to discriminate expert and
novice surgeons. We test our method on real surgical data for suturing task and
compare the classification result with the ground truth data (obtained by
manual labeling). The experimental results show that the proposed framework can
classify surgical skill level with relatively high accuracy of 85.7%. This
study demonstrates the ability of machine learning methods to automatically
classify expert and novice surgeons using movement features for different RMIS
tasks. Due to the simplicity and generalizability of the introduced
classification method, it is easy to implement in existing trainers.","['Mahtab J. Fard', 'Sattar Ameri', 'Ratna B. Chinnam', 'Abhilash K. Pandya', 'Michael D. Klein', 'R. Darin Ellis']","['cs.LG', 'stat.ML']",2016-11-16 03:45:12+00:00
http://arxiv.org/abs/1611.05126v2,Localized Coulomb Descriptors for the Gaussian Approximation Potential,"We introduce a novel class of localized atomic environment representations,
based upon the Coulomb matrix. By combining these functions with the Gaussian
approximation potential approach, we present LC-GAP, a new system for
generating atomic potentials through machine learning (ML). Tests on the QM7,
QM7b and GDB9 biomolecular datasets demonstrate that potentials created with
LC-GAP can successfully predict atomization energies for molecules larger than
those used for training to chemical accuracy, and can (in the case of QM7b)
also be used to predict a range of other atomic properties with accuracy in
line with the recent literature. As the best-performing representation has only
linear dimensionality in the number of atoms in a local atomic environment,
this represents an improvement both in prediction accuracy and computational
cost when considered against similar Coulomb matrix-based methods.","['James Barker', 'Johannes Bulin', 'Jan Hamaekers', 'Sonja Mathias']","['stat.ML', 'physics.chem-ph', '92E10']",2016-11-16 02:57:40+00:00
http://arxiv.org/abs/1611.05010v1,Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm,"In topic modeling, many algorithms that guarantee identifiability of the
topics have been developed under the premise that there exist anchor words --
i.e., words that only appear (with positive probability) in one topic.
Follow-up work has resorted to three or higher-order statistics of the data
corpus to relax the anchor word assumption. Reliable estimates of higher-order
statistics are hard to obtain, however, and the identification of topics under
those models hinges on uncorrelatedness of the topics, which can be
unrealistic. This paper revisits topic modeling based on second-order moments,
and proposes an anchor-free topic mining framework. The proposed approach
guarantees the identification of the topics under a much milder condition
compared to the anchor-word assumption, thereby exhibiting much better
robustness in practice. The associated algorithm only involves one
eigen-decomposition and a few small linear programs. This makes it easy to
implement and scale up to very large problem instances. Experiments using the
TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free
approach exhibits very favorable performance (measured using coherence,
similarity count, and clustering accuracy metrics) compared to the prior art.","['Kejun Huang', 'Xiao Fu', 'Nicholas D. Sidiropoulos']","['stat.ML', 'cs.CL', 'cs.IR', 'cs.SI']",2016-11-15 20:06:40+00:00
http://arxiv.org/abs/1611.04982v2,Oracle Complexity of Second-Order Methods for Finite-Sum Problems,"Finite-sum optimization problems are ubiquitous in machine learning, and are
commonly solved using first-order methods which rely on gradient computations.
Recently, there has been growing interest in \emph{second-order} methods, which
rely on both gradients and Hessians. In principle, second-order methods can
require much fewer iterations than first-order methods, and hold the promise
for more efficient algorithms. Although computing and manipulating Hessians is
prohibitive for high-dimensional problems in general, the Hessians of
individual functions in finite-sum problems can often be efficiently computed,
e.g. because they possess a low-rank structure. Can second-order information
indeed be used to solve such problems more efficiently? In this paper, we
provide evidence that the answer -- perhaps surprisingly -- is negative, at
least in terms of worst-case guarantees. However, we also discuss what
additional assumptions and algorithmic approaches might potentially circumvent
this negative result.","['Yossi Arjevani', 'Ohad Shamir']","['math.OC', 'cs.LG', 'stat.ML']",2016-11-15 18:41:55+00:00
http://arxiv.org/abs/1611.04967v1,Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box Models,"Predictive models are increasingly deployed for the purpose of determining
access to services such as credit, insurance, and employment. Despite potential
gains in productivity and efficiency, several potential problems have yet to be
addressed, particularly the potential for unintentional discrimination. We
present an iterative procedure, based on orthogonal projection of input
attributes, for enabling interpretability of black-box predictive models.
Through our iterative procedure, one can quantify the relative dependence of a
black-box model on its input attributes.The relative significance of the inputs
to a predictive model can then be used to assess the fairness (or
discriminatory extent) of such a model.","['Julius Adebayo', 'Lalana Kagal']","['cs.LG', 'stat.ML']",2016-11-15 18:10:24+00:00
http://arxiv.org/abs/1611.04920v2,Unsupervised Learning with Truncated Gaussian Graphical Models,"Gaussian graphical models (GGMs) are widely used for statistical modeling,
because of ease of inference and the ubiquitous use of the normal distribution
in practical approximations. However, they are also known for their limited
modeling abilities, due to the Gaussian assumption. In this paper, we introduce
a novel variant of GGMs, which relaxes the Gaussian restriction and yet admits
efficient inference. Specifically, we impose a bipartite structure on the GGM
and govern the hidden variables by truncated normal distributions. The
nonlinearity of the model is revealed by its connection to rectified linear
unit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and
appealing properties of truncated normals, we are able to train the models
efficiently using contrastive divergence. We consider three output constructs,
accounting for real-valued, binary and count data. We further extend the model
to deep constructions and show that deep models can be used for unsupervised
pre-training of rectifier neural networks. Extensive experimental results are
provided to validate the proposed models and demonstrate their superiority over
competing models.","['Qinliang Su', 'Xuejun Liao', 'Chunyuan Li', 'Zhe Gan', 'Lawrence Carin']","['stat.ML', 'cs.LG']",2016-11-15 16:26:17+00:00
http://arxiv.org/abs/1611.04835v1,Multilinear Low-Rank Tensors on Graphs & Applications,"We propose a new framework for the analysis of low-rank tensors which lies at
the intersection of spectral graph theory and signal processing. As a first
step, we present a new graph based low-rank decomposition which approximates
the classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,
building on this novel decomposition we construct a general class of convex
optimization problems for approximately solving low-rank tensor inverse
problems, such as tensor Robust PCA. The whole framework is named as
'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis
shows: 1) MLRTG stands on the notion of approximate stationarity of
multi-dimensional signals on graphs and 2) the approximation error depends on
the eigen gaps of the graphs. We demonstrate applications for a wide variety of
4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance
videos and hyperspectral images. Generalization of the tensor concepts to
non-euclidean domain, orders of magnitude speed-up, low-memory requirement and
significantly enhanced performance at low SNR are the key aspects of our
framework.","['Nauman Shahid', 'Francesco Grassi', 'Pierre Vandergheynst']","['cs.CV', 'cs.LG', 'stat.ML']",2016-11-15 14:05:43+00:00
http://arxiv.org/abs/1611.04831v1,The Power of Normalization: Faster Evasion of Saddle Points,"A commonly used heuristic in non-convex optimization is Normalized Gradient
Descent (NGD) - a variant of gradient descent in which only the direction of
the gradient is taken into account and its magnitude ignored. We analyze this
heuristic and show that with carefully chosen parameters and noise injection,
this method can provably evade saddle points. We establish the convergence of
NGD to a local minimum, and demonstrate rates which improve upon the fastest
known first order algorithm due to Ge e al. (2015).
  The effectiveness of our method is demonstrated via an application to the
problem of online tensor decomposition; a task for which saddle point evasion
is known to result in convergence to global minima.",['Kfir Y. Levy'],"['cs.LG', 'math.OC', 'stat.ML']",2016-11-15 13:56:24+00:00
http://arxiv.org/abs/1611.04790v1,Improved Particle Filters for Vehicle Localisation,"The ability to track a moving vehicle is of crucial importance in numerous
applications. The task has often been approached by the importance sampling
technique of particle filters due to its ability to model non-linear and
non-Gaussian dynamics, of which a vehicle travelling on a road network is a
good example. Particle filters perform poorly when observations are highly
informative. In this paper, we address this problem by proposing particle
filters that sample around the most recent observation. The proposal leads to
an order of magnitude improvement in accuracy and efficiency over conventional
particle filters, especially when observations are infrequent but low-noise.","['Kira Kempinska', 'John Shawe-Taylor']",['stat.ML'],2016-11-15 11:17:33+00:00
http://arxiv.org/abs/1611.04709v1,Recoverability of Joint Distribution from Missing Data,"A probabilistic query may not be estimable from observed data corrupted by
missing values if the data are not missing at random (MAR). It is therefore of
theoretical interest and practical importance to determine in principle whether
a probabilistic query is estimable from missing data or not when the data are
not MAR. We present an algorithm that systematically determines whether the
joint probability is estimable from observed data with missing values, assuming
that the data-generation model is represented as a Bayesian network containing
unobserved latent variables that not only encodes the dependencies among the
variables but also explicitly portrays the mechanisms responsible for the
missingness process. The result significantly advances the existing work.",['Jin Tian'],"['stat.ML', 'cs.AI']",2016-11-15 06:06:42+00:00
http://arxiv.org/abs/1611.04701v2,Errors-in-variables models with dependent measurements,"Suppose that we observe $y \in \mathbb{R}^n$ and $X \in \mathbb{R}^{n \times
m}$ in the following errors-in-variables model: \begin{eqnarray*} y & = & X_0
\beta^* +\epsilon \\ X & = & X_0 + W, \end{eqnarray*} where $X_0$ is an $n
\times m$ design matrix with independent subgaussian row vectors, $\epsilon \in
\mathbb{R}^n$ is a noise vector and $W$ is a mean zero $n \times m$ random
noise matrix with independent subgaussian column vectors, independent of $X_0$
and $\epsilon$. This model is significantly different from those analyzed in
the literature in the sense that we allow the measurement error for each
covariate to be a dependent vector across its $n$ observations. Such error
structures appear in the science literature when modeling the trial-to-trial
fluctuations in response strength shared across a set of neurons.
  Under sparsity and restrictive eigenvalue type of conditions, we show that
one is able to recover a sparse vector $\beta^* \in \mathbb{R}^m$ from the
model given a single observation matrix $X$ and the response vector $y$. We
establish consistency in estimating $\beta^*$ and obtain the rates of
convergence in the $\ell_q$ norm, where $q = 1, 2$. We show error bounds which
approach that of the regular Lasso and the Dantzig selector in case the errors
in $W$ are tending to 0. We analyze the convergence rates of the gradient
descent methods for solving the nonconvex programs and show that the composite
gradient descent algorithm is guaranteed to converge at a geometric rate to a
neighborhood of the global minimizers: the size of the neighborhood is bounded
by the statistical error in the $\ell_2$ norm. Our analysis reveals interesting
connections between computational and statistical efficiency and the
concentration of measure phenomenon in random matrix theory. We provide
simulation evidence illuminating the theoretical predictions.","['Mark Rudelson', 'Shuheng Zhou']","['stat.ML', 'math.ST', 'stat.TH']",2016-11-15 04:12:08+00:00
http://arxiv.org/abs/1611.04561v1,Splitting matters: how monotone transformation of predictor variables may improve the predictions of decision tree models,"It is widely believed that the prediction accuracy of decision tree models is
invariant under any strictly monotone transformation of the individual
predictor variables. However, this statement may be false when predicting new
observations with values that were not seen in the training-set and are close
to the location of the split point of a tree rule. The sensitivity of the
prediction error to the split point interpolation is high when the split point
of the tree is estimated based on very few observations, reaching 9%
misclassification error when only 10 observations are used for constructing a
split, and shrinking to 1% when relying on 100 observations. This study
compares the performance of alternative methods for split point interpolation
and concludes that the best choice is taking the mid-point between the two
closest points to the split point of the tree. Furthermore, if the (continuous)
distribution of the predictor variable is known, then using its probability
integral for transforming the variable (""quantile transformation"") will reduce
the model's interpolation error by up to about a half on average. Accordingly,
this study provides guidelines for both developers and users of decision tree
models (including bagging and random forest).","['Tal Galili', 'Isaac Meilijson']","['stat.ML', 'cs.LG']",2016-11-14 20:34:29+00:00
http://arxiv.org/abs/1611.04528v1,Benchmarking Quantum Hardware for Training of Fully Visible Boltzmann Machines,"Quantum annealing (QA) is a hardware-based heuristic optimization and
sampling method applicable to discrete undirected graphical models. While
similar to simulated annealing, QA relies on quantum, rather than thermal,
effects to explore complex search spaces. For many classes of problems, QA is
known to offer computational advantages over simulated annealing. Here we
report on the ability of recent QA hardware to accelerate training of fully
visible Boltzmann machines. We characterize the sampling distribution of QA
hardware, and show that in many cases, the quantum distributions differ
significantly from classical Boltzmann distributions. In spite of this
difference, training (which seeks to match data and model statistics) using
standard classical gradient updates is still effective. We investigate the use
of QA for seeding Markov chains as an alternative to contrastive divergence
(CD) and persistent contrastive divergence (PCD). Using $k=50$ Gibbs steps, we
show that for problems with high-energy barriers between modes, QA-based seeds
can improve upon chains with CD and PCD initializations. For these hard
problems, QA gradient estimates are more accurate, and allow for faster
learning. Furthermore, and interestingly, even the case of raw QA samples (that
is, $k=0$) achieved similar improvements. We argue that this relates to the
fact that we are training a quantum rather than classical Boltzmann
distribution in this case. The learned parameters give rise to hardware QA
distributions closely approximating classical Boltzmann distributions that are
hard to train with CD/PCD.","['Dmytro Korenkevych', 'Yanbo Xue', 'Zhengbing Bian', 'Fabian Chudak', 'William G. Macready', 'Jason Rolfe', 'Evgeny Andriyash']","['quant-ph', 'cs.LG', 'stat.ML']",2016-11-14 19:15:57+00:00
http://arxiv.org/abs/1611.04520v2,Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,"Normalization techniques have only recently begun to be exploited in
supervised learning tasks. Batch normalization exploits mini-batch statistics
to normalize the activations. This was shown to speed up training and result in
better models. However its success has been very limited when dealing with
recurrent neural networks. On the other hand, layer normalization normalizes
the activations across all activities within a layer. This was shown to work
well in the recurrent setting. In this paper we propose a unified view of
normalization techniques, as forms of divisive normalization, which includes
layer and batch normalization as special cases. Our second contribution is the
finding that a small modification to these normalization schemes, in
conjunction with a sparse regularizer on the activations, leads to significant
benefits over standard normalization techniques. We demonstrate the
effectiveness of our unified divisive normalization framework in the context of
convolutional neural nets and recurrent neural networks, showing improvements
over baselines in image classification, language modeling as well as
super-resolution.","['Mengye Ren', 'Renjie Liao', 'Raquel Urtasun', 'Fabian H. Sinz', 'Richard S. Zemel']","['cs.LG', 'stat.ML']",2016-11-14 19:04:58+00:00
http://arxiv.org/abs/1611.04500v3,Deep Learning with Sets and Point Clouds,"We introduce a simple permutation equivariant layer for deep learning with
set structure.This type of layer, obtained by parameter-sharing, has a simple
implementation and linear-time complexity in the size of each set. We use deep
permutation-invariant networks to perform point-could classification and
MNIST-digit summation, where in both cases the output is invariant to
permutations of the input. In a semi-supervised setting, where the goal is make
predictions for each instance within a set, we demonstrate the usefulness of
this type of layer in set-outlier detection as well as semi-supervised learning
with clustering side-information.","['Siamak Ravanbakhsh', 'Jeff Schneider', 'Barnabas Poczos']","['stat.ML', 'cs.LG', 'cs.NE']",2016-11-14 17:55:34+00:00
http://arxiv.org/abs/1611.04499v2,Post Training in Deep Learning with Last Kernel,"One of the main challenges of deep learning methods is the choice of an
appropriate training strategy. In particular, additional steps, such as
unsupervised pre-training, have been shown to greatly improve the performances
of deep structures. In this article, we propose an extra training step, called
post-training, which only optimizes the last layer of the network. We show that
this procedure can be analyzed in the context of kernel theory, with the first
layers computing an embedding of the data and the last layer a statistical
model to solve the task based on this embedding. This step makes sure that the
embedding, or representation, of the data is used in the best possible way for
the considered task. This idea is then tested on multiple architectures with
various data sets, showing that it consistently provides a boost in
performance.","['Thomas Moreau', 'Julien Audiffren']","['stat.ML', 'cs.LG']",2016-11-14 17:54:28+00:00
http://arxiv.org/abs/1611.04488v6,Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,"We propose a method to optimize the representation and distinguishability of
samples from two probability distributions, by maximizing the estimated power
of a statistical test based on the maximum mean discrepancy (MMD). This
optimized MMD is applied to the setting of unsupervised learning by generative
adversarial networks (GAN), in which a model attempts to generate realistic
samples, and a discriminator attempts to tell these apart from data samples. In
this context, the MMD may be used in two roles: first, as a discriminator,
either directly on the samples, or on features of the samples. Second, the MMD
can be used to evaluate the performance of a generative model, by testing the
model's samples against a reference data set. In the latter role, the optimized
MMD is particularly helpful, as it gives an interpretable indication of how the
model and data distributions differ, even in cases where individual model
samples are not easily distinguished either by eye or by classifier.","['Danica J. Sutherland', 'Hsiao-Yu Tung', 'Heiko Strathmann', 'Soumyajit De', 'Aaditya Ramdas', 'Alex Smola', 'Arthur Gretton']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ME']",2016-11-14 17:28:27+00:00
http://arxiv.org/abs/1611.04482v1,Practical Secure Aggregation for Federated Learning on User-Held Data,"Secure Aggregation protocols allow a collection of mutually distrust parties,
each holding a private value, to collaboratively compute the sum of those
values without revealing the values themselves. We consider training a deep
neural network in the Federated Learning model, using distributed stochastic
gradient descent across user-held training data on mobile devices, wherein
Secure Aggregation protects each user's model gradient. We design a novel,
communication-efficient Secure Aggregation protocol for high-dimensional data
that tolerates up to 1/3 users failing to complete the protocol. For 16-bit
input values, our protocol offers 1.73x communication expansion for $2^{10}$
users and $2^{20}$-dimensional vectors, and 1.98x expansion for $2^{14}$ users
and $2^{24}$ dimensional vectors.","['Keith Bonawitz', 'Vladimir Ivanov', 'Ben Kreuter', 'Antonio Marcedone', 'H. Brendan McMahan', 'Sarvar Patel', 'Daniel Ramage', 'Aaron Segal', 'Karn Seth']","['cs.CR', 'stat.ML']",2016-11-14 17:14:55+00:00
http://arxiv.org/abs/1611.04416v1,On numerical approximation schemes for expectation propagation,"Several numerical approximation strategies for the expectation-propagation
algorithm are studied in the context of large-scale learning: the Laplace
method, a faster variant of it, Gaussian quadrature, and a deterministic
version of variational sampling (i.e., combining quadrature with variational
approximation). Experiments in training linear binary classifiers show that the
expectation-propagation algorithm converges best using variational sampling,
while it also converges well using Laplace-style methods with smooth factors
but tends to be unstable with non-differentiable ones. Gaussian quadrature
yields unstable behavior or convergence to a sub-optimal solution in most
experiments.",['Alexis Roche'],"['stat.CO', 'cs.LG', 'stat.ML']",2016-11-14 15:21:23+00:00
http://arxiv.org/abs/1611.04281v4,Statistical mechanics of the inverse Ising problem and the optimal objective function,"The inverse Ising problem seeks to reconstruct the parameters of an Ising
Hamiltonian on the basis of spin configurations sampled from the Boltzmann
measure. Over the last decade, many applications of the inverse Ising problem
have arisen, driven by the advent of large-scale data across different
scientific disciplines. Recently, strategies to solve the inverse Ising problem
based on convex optimisation have proven to be very successful. These
approaches maximise particular objective functions with respect to the model
parameters. Examples are the pseudolikelihood method and interaction screening.
In this paper, we establish a link between approaches to the inverse Ising
problem based on convex optimisation and the statistical physics of disordered
systems. We characterise the performance of an arbitrary objective function and
calculate the objective function which optimally reconstructs the model
parameters. We evaluate the optimal objective function within a
replica-symmetric ansatz and compare the results of the optimal objective
function with other reconstruction methods. Apart from giving a theoretical
underpinning to solving the inverse Ising problem by convex optimisation, the
optimal objective function outperforms state-of-the-art methods, albeit by a
small margin.",['Johannes Berg'],"['cond-mat.dis-nn', 'q-bio.QM', 'stat.ML']",2016-11-14 08:30:47+00:00
http://arxiv.org/abs/1611.04231v3,Identity Matters in Deep Learning,"An emerging design principle in deep learning is that each layer of a deep
artificial neural network should be able to easily express the identity
transformation. This idea not only motivated various normalization techniques,
such as \emph{batch normalization}, but was also key to the immense success of
\emph{residual networks}.
  In this work, we put the principle of \emph{identity parameterization} on a
more solid theoretical footing alongside further empirical progress. We first
give a strikingly simple proof that arbitrarily deep linear residual networks
have no spurious local optima. The same result for linear feed-forward networks
in their standard parameterization is substantially more delicate. Second, we
show that residual networks with ReLu activations have universal finite-sample
expressivity in the sense that the network can represent any function of its
sample provided that the model has more parameters than the sample size.
  Directly inspired by our theory, we experiment with a radically simple
residual architecture consisting of only residual convolutional layers and ReLu
activations, but no batch normalization, dropout, or max pool. Our model
improves significantly on previous all-convolutional networks on the CIFAR10,
CIFAR100, and ImageNet classification benchmarks.","['Moritz Hardt', 'Tengyu Ma']","['cs.LG', 'cs.NE', 'stat.ML']",2016-11-14 02:44:18+00:00
http://arxiv.org/abs/1611.04218v1,Preference Completion from Partial Rankings,"We propose a novel and efficient algorithm for the collaborative preference
completion problem, which involves jointly estimating individualized rankings
for a set of entities over a shared set of items, based on a limited number of
observed affinity values. Our approach exploits the observation that while
preferences are often recorded as numerical scores, the predictive quantity of
interest is the underlying rankings. Thus, attempts to closely match the
recorded scores may lead to overfitting and impair generalization performance.
Instead, we propose an estimator that directly fits the underlying preference
order, combined with nuclear norm constraints to encourage low--rank
parameters. Besides (approximate) correctness of the ranking order, the
proposed estimator makes no generative assumption on the numerical scores of
the observations. One consequence is that the proposed estimator can fit any
consistent partial ranking over a subset of the items represented as a directed
acyclic graph (DAG), generalizing standard techniques that can only fit
preference scores. Despite this generality, for supervision representing total
or blockwise total orders, the computational complexity of our algorithm is
within a $\log$ factor of the standard algorithms for nuclear norm
regularization based estimates for matrix completion. We further show promising
empirical results for a novel and challenging application of collaboratively
ranking of the associations between brain--regions and cognitive neuroscience
terms.","['Suriya Gunasekar', 'Oluwasanmi Koyejo', 'Joydeep Ghosh']","['stat.ML', 'cs.LG']",2016-11-14 01:17:14+00:00
http://arxiv.org/abs/1611.04208v4,Joint mean and covariance estimation with unreplicated matrix-variate data,"It has been proposed that complex populations, such as those that arise in
genomics studies, may exhibit dependencies among observations as well as among
variables. This gives rise to the challenging problem of analyzing unreplicated
high-dimensional data with unknown mean and dependence structures.
Matrix-variate approaches that impose various forms of (inverse) covariance
sparsity allow flexible dependence structures to be estimated, but cannot
directly be applied when the mean and covariance matrices are estimated
jointly. We present a practical method utilizing generalized least squares and
penalized (inverse) covariance estimation to address this challenge. We
establish consistency and obtain rates of convergence for estimating the mean
parameters and covariance matrices. The advantages of our approaches are: (i)
dependence graphs and covariance structures can be estimated in the presence of
unknown mean structure, (ii) the mean structure becomes more efficiently
estimated when accounting for the dependence structure among observations; and
(iii) inferences about the mean parameters become correctly calibrated. We use
simulation studies and analysis of genomic data from a twin study of ulcerative
colitis to illustrate the statistical convergence and the performance of our
methods in practical settings. Several lines of evidence show that the test
statistics for differential gene expression produced by our methods are
correctly calibrated and improve power over conventional methods.","['Michael Hornstein', 'Roger Fan', 'Kerby Shedden', 'Shuheng Zhou']",['stat.ML'],2016-11-13 23:54:03+00:00
http://arxiv.org/abs/1611.04199v1,Realistic risk-mitigating recommendations via inverse classification,"Inverse classification, the process of making meaningful perturbations to a
test point such that it is more likely to have a desired classification, has
previously been addressed using data from a single static point in time. Such
an approach yields inflated probability estimates, stemming from an implicitly
made assumption that recommendations are implemented instantaneously. We
propose using longitudinal data to alleviate such issues in two ways. First, we
use past outcome probabilities as features in the present. Use of such past
probabilities ties historical behavior to the present, allowing for more
information to be taken into account when making initial probability estimates
and subsequently performing inverse classification. Secondly, following inverse
classification application, optimized instances' unchangeable features
(e.g.,~age) are updated using values from the next longitudinal time period.
Optimized test instance probabilities are then reassessed. Updating the
unchangeable features in this manner reflects the notion that improvements in
outcome likelihood, which result from following the inverse classification
recommendations, do not materialize instantaneously. As our experiments
demonstrate, more realistic estimates of probability can be obtained by
factoring in such considerations.","['Michael T. Lash', 'W. Nick Street']","['cs.LG', 'stat.ML']",2016-11-13 22:53:51+00:00
http://arxiv.org/abs/1611.04149v1,Accelerated Variance Reduced Block Coordinate Descent,"Algorithms with fast convergence, small number of data access, and low
per-iteration complexity are particularly favorable in the big data era, due to
the demand for obtaining \emph{highly accurate solutions} to problems with
\emph{a large number of samples} in \emph{ultra-high} dimensional space.
Existing algorithms lack at least one of these qualities, and thus are
inefficient in handling such big data challenge. In this paper, we propose a
method enjoying all these merits with an accelerated convergence rate
$O(\frac{1}{k^2})$. Empirical studies on large scale datasets with more than
one million features are conducted to show the effectiveness of our methods in
practice.","['Zebang Shen', 'Hui Qian', 'Chao Zhang', 'Tengfei Zhou']","['stat.ML', 'cs.LG']",2016-11-13 16:01:10+00:00
http://arxiv.org/abs/1611.04069v2,Low-rank and Adaptive Sparse Signal (LASSI) Models for Highly Accelerated Dynamic Imaging,"Sparsity-based approaches have been popular in many applications in image
processing and imaging. Compressed sensing exploits the sparsity of images in a
transform domain or dictionary to improve image recovery from undersampled
measurements. In the context of inverse problems in dynamic imaging, recent
research has demonstrated the promise of sparsity and low-rank techniques. For
example, the patches of the underlying data are modeled as sparse in an
adaptive dictionary domain, and the resulting image and dictionary estimation
from undersampled measurements is called dictionary-blind compressed sensing,
or the dynamic image sequence is modeled as a sum of low-rank and sparse (in
some transform domain) components (L+S model) that are estimated from limited
measurements. In this work, we investigate a data-adaptive extension of the L+S
model, dubbed LASSI, where the temporal image sequence is decomposed into a
low-rank component and a component whose spatiotemporal (3D) patches are sparse
in some adaptive dictionary domain. We investigate various formulations and
efficient methods for jointly estimating the underlying dynamic signal
components and the spatiotemporal dictionary from limited measurements. We also
obtain efficient sparsity penalized dictionary-blind compressed sensing methods
as special cases of our LASSI approaches. Our numerical experiments demonstrate
the promising performance of LASSI schemes for dynamic magnetic resonance image
reconstruction from limited k-t space data compared to recent methods such as
k-t SLR and L+S, and compared to the proposed dictionary-blind compressed
sensing method.","['Saiprasad Ravishankar', 'Brian E. Moore', 'Raj Rao Nadakuditi', 'Jeffrey A. Fessler']","['stat.ML', 'cs.LG']",2016-11-13 02:21:07+00:00
http://arxiv.org/abs/1611.04067v2,Error Metrics for Learning Reliable Manifolds from Streaming Data,"Spectral dimensionality reduction is frequently used to identify
low-dimensional structure in high-dimensional data. However, learning
manifolds, especially from the streaming data, is computationally and memory
expensive. In this paper, we argue that a stable manifold can be learned using
only a fraction of the stream, and the remaining stream can be mapped to the
manifold in a significantly less costly manner. Identifying the transition
point at which the manifold is stable is the key step. We present error metrics
that allow us to identify the transition point for a given stream by
quantitatively assessing the quality of a manifold learned using Isomap. We
further propose an efficient mapping algorithm, called S-Isomap, that can be
used to map new samples onto the stable manifold. We describe experiments on a
variety of data sets that show that the proposed approach is computationally
efficient without sacrificing accuracy.","['Frank Schoeneman', 'Suchismit Mahapatra', 'Varun Chandola', 'Nils Napp', 'Jaroslaw Zola']",['stat.ML'],2016-11-13 02:14:01+00:00
http://arxiv.org/abs/1611.04051v1,GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution,"Generative Adversarial Networks (GAN) have limitations when the goal is to
generate sequences of discrete elements. The reason for this is that samples
from a distribution on discrete objects such as the multinomial are not
differentiable with respect to the distribution parameters. This problem can be
avoided by using the Gumbel-softmax distribution, which is a continuous
approximation to a multinomial distribution parameterized in terms of the
softmax function. In this work, we evaluate the performance of GANs based on
recurrent neural networks with Gumbel-softmax output distributions in the task
of generating sequences of discrete elements.","['Matt J. Kusner', 'JosÃ© Miguel HernÃ¡ndez-Lobato']","['stat.ML', 'cs.LG']",2016-11-12 22:54:45+00:00
http://arxiv.org/abs/1611.04035v2,Entropic Causal Inference,"We consider the problem of identifying the causal direction between two
discrete random variables using observational data. Unlike previous work, we
keep the most general functional model but make an assumption on the unobserved
exogenous variable: Inspired by Occam's razor, we assume that the exogenous
variable is simple in the true causal direction. We quantify simplicity using
R\'enyi entropy. Our main result is that, under natural assumptions, if the
exogenous variable has low $H_0$ entropy (cardinality) in the true direction,
it must have high $H_0$ entropy in the wrong direction. We establish several
algorithmic hardness results about estimating the minimum entropy exogenous
variable. We show that the problem of finding the exogenous variable with
minimum entropy is equivalent to the problem of finding minimum joint entropy
given $n$ marginal distributions, also known as minimum entropy coupling
problem. We propose an efficient greedy algorithm for the minimum entropy
coupling problem, that for $n=2$ provably finds a local optimum. This gives a
greedy algorithm for finding the exogenous variable with minimum $H_1$ (Shannon
Entropy). Our greedy entropy-based causal inference algorithm has similar
performance to the state of the art additive noise models in real datasets. One
advantage of our approach is that we make no use of the values of random
variables but only their distributions. Our method can therefore be used for
causal inference for both ordinal and also categorical data, unlike additive
noise models.","['Murat Kocaoglu', 'Alexandros G. Dimakis', 'Sriram Vishwanath', 'Babak Hassibi']","['cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",2016-11-12 18:56:34+00:00
http://arxiv.org/abs/1611.03993v2,Riemannian Tensor Completion with Side Information,"By restricting the iterate on a nonlinear manifold, the recently proposed
Riemannian optimization methods prove to be both efficient and effective in low
rank tensor completion problems. However, existing methods fail to exploit the
easily accessible side information, due to their format mismatch. Consequently,
there is still room for improvement in such methods. To fill the gap, in this
paper, a novel Riemannian model is proposed to organically integrate the
original model and the side information by overcoming their inconsistency. For
this particular model, an efficient Riemannian conjugate gradient descent
solver is devised based on a new metric that captures the curvature of the
objective.Numerical experiments suggest that our solver is more accurate than
the state-of-the-art without compromising the efficiency.","['Tengfei Zhou', 'Hui Qian', 'Zebang Shen', 'Congfu Xu']","['stat.ML', 'cs.LG', 'cs.NA']",2016-11-12 11:58:17+00:00
http://arxiv.org/abs/1611.03981v1,Dual Teaching: A Practical Semi-supervised Wrapper Method,"Semi-supervised wrapper methods are concerned with building effective
supervised classifiers from partially labeled data. Though previous works have
succeeded in some fields, it is still difficult to apply semi-supervised
wrapper methods to practice because the assumptions those methods rely on tend
to be unrealistic in practice. For practical use, this paper proposes a novel
semi-supervised wrapper method, Dual Teaching, whose assumptions are easy to
set up. Dual Teaching adopts two external classifiers to estimate the false
positives and false negatives of the base learner. Only if the recall of every
external classifier is greater than zero and the sum of the precision is
greater than one, Dual Teaching will train a base learner from partially
labeled data as effectively as the fully-labeled-data-trained classifier. The
effectiveness of Dual Teaching is proved in both theory and practice.","['Fuqaing Liu', 'Chenwei Deng', 'Fukun Bi', 'Yiding Yang']","['cs.LG', 'stat.ML']",2016-11-12 10:23:53+00:00
http://arxiv.org/abs/1611.03979v1,"Kernel regression, minimax rates and effective dimensionality: beyond the regular case","We investigate if kernel regularization methods can achieve minimax
convergence rates over a source condition regularity assumption for the target
function. These questions have been considered in past literature, but only
under specific assumptions about the decay, typically polynomial, of the
spectrum of the the kernel mapping covariance operator. In the perspective of
distribution-free results, we investigate this issue under much weaker
assumption on the eigenvalue decay, allowing for more complex behavior that can
reflect different structure of the data at different scales.","['Gilles Blanchard', 'Nicole MÃ¼cke']",['stat.ML'],2016-11-12 09:51:28+00:00
