id,title,abstract,authors,categories,date
http://arxiv.org/abs/1504.00386v1,"Signatures of Infinity: Nonergodicity and Resource Scaling in Prediction, Complexity, and Learning","We introduce a simple analysis of the structural complexity of
infinite-memory processes built from random samples of stationary, ergodic
finite-memory component processes. Such processes are familiar from the well
known multi-arm Bandit problem. We contrast our analysis with
computation-theoretic and statistical inference approaches to understanding
their complexity. The result is an alternative view of the relationship between
predictability, complexity, and learning that highlights the distinct ways in
which informational and correlational divergences arise in complex ergodic and
nonergodic processes. We draw out consequences for the resource divergences
that delineate the structural hierarchy of ergodic processes and for processes
that are themselves hierarchical.","['James P. Crutchfield', 'Sarah Marzen']","['cond-mat.stat-mech', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2015-04-01 20:55:10+00:00
http://arxiv.org/abs/1504.00377v1,Bayesian Clustering of Shapes of Curves,"Unsupervised clustering of curves according to their shapes is an important
problem with broad scientific applications. The existing model-based clustering
techniques either rely on simple probability models (e.g., Gaussian) that are
not generally valid for shape analysis or assume the number of clusters. We
develop an efficient Bayesian method to cluster curve data using an elastic
shape metric that is based on joint registration and comparison of shapes of
curves. The elastic-inner product matrix obtained from the data is modeled
using a Wishart distribution whose parameters are assigned carefully chosen
prior distributions to allow for automatic inference on the number of clusters.
Posterior is sampled through an efficient Markov chain Monte Carlo procedure
based on the Chinese restaurant process to infer (1) the posterior distribution
on the number of clusters, and (2) clustering configuration of shapes. This
method is demonstrated on a variety of synthetic data and real data examples on
protein structure analysis, cell shape analysis in microscopy images, and
clustering of shaped from MPEG7 database.","['Zhengwu Zhang', 'Debdeep Pati', 'Anuj Srivastava']","['stat.ML', 'cs.LG']",2015-04-01 20:35:33+00:00
http://arxiv.org/abs/1504.00298v3,Bayesian model comparison with un-normalised likelihoods,"Models for which the likelihood function can be evaluated only up to a
parameter-dependent unknown normalising constant, such as Markov random field
models, are used widely in computer science, statistical physics, spatial
statistics, and network analysis. However, Bayesian analysis of these models
using standard Monte Carlo methods is not possible due to the intractability of
their likelihood functions. Several methods that permit exact, or close to
exact, simulation from the posterior distribution have recently been developed.
However, estimating the evidence and Bayes' factors (BFs) for these models
remains challenging in general. This paper describes new random weight
importance sampling and sequential Monte Carlo methods for estimating BFs that
use simulation to circumvent the evaluation of the intractable likelihood, and
compares them to existing methods. In some cases we observe an advantage in the
use of biased weight estimates. An initial investigation into the theoretical
and empirical properties of this class of methods is presented. Some support
for the use of biased estimates is presented, but we advocate caution in the
use of such estimates.","['Richard G. Everitt', 'Adam M. Johansen', 'Ellen Rowing', 'Melina Evdemon-Hogan']","['stat.CO', 'physics.data-an', 'stat.ME', 'stat.ML']",2015-04-01 17:10:25+00:00
http://arxiv.org/abs/1504.00284v3,A New Vision of Collaborative Active Learning,"Active learning (AL) is a learning paradigm where an active learner has to
train a model (e.g., a classifier) which is in principal trained in a
supervised way, but in AL it has to be done by means of a data set with
initially unlabeled samples. To get labels for these samples, the active
learner has to ask an oracle (e.g., a human expert) for labels. The goal is to
maximize the performance of the model and to minimize the number of queries at
the same time. In this article, we first briefly discuss the state of the art
and own, preliminary work in the field of AL. Then, we propose the concept of
collaborative active learning (CAL). With CAL, we will overcome some of the
harsh limitations of current AL. In particular, we envision scenarios where an
expert may be wrong for various reasons, there might be several or even many
experts with different expertise, the experts may label not only samples but
also knowledge at a higher level such as rules, and we consider that the
labeling costs depend on many conditions. Moreover, in a CAL process human
experts will profit by improving their own knowledge, too.","['Adrian Calma', 'Tobias Reitmaier', 'Bernhard Sick', 'Paul Lukowicz', 'Mark Embrechts']","['cs.LG', 'stat.ML']",2015-04-01 16:39:26+00:00
http://arxiv.org/abs/1504.00091v2,Learning in the Presence of Corruption,"In supervised learning one wishes to identify a pattern present in a joint
distribution $P$, of instances, label pairs, by providing a function $f$ from
instances to labels that has low risk $\mathbb{E}_{P}\ell(y,f(x))$. To do so,
the learner is given access to $n$ iid samples drawn from $P$. In many real
world problems clean samples are not available. Rather, the learner is given
access to samples from a corrupted distribution $\tilde{P}$ from which to
learn, while the goal of predicting the clean pattern remains. There are many
different types of corruption one can consider, and as of yet there is no
general means to compare the relative ease of learning under these different
corruption processes. In this paper we develop a general framework for tackling
such problems as well as introducing upper and lower bounds on the risk for
learning in the presence of corruption. Our ultimate goal is to be able to make
informed economic decisions in regards to the acquisition of data sets. For a
certain subclass of corruption processes (those that are
\emph{reconstructible}) we achieve this goal in a particular sense. Our lower
bounds are in terms of the coefficient of ergodicity, a simple to calculate
property of stochastic matrices. Our upper bounds proceed via a generalization
of the method of unbiased estimators appearing in recent work of Natarajan et
al and implicit in the earlier work of Kearns.","['Brendan van Rooyen', 'Robert C. Williamson']","['stat.ML', 'cs.LG']",2015-04-01 02:54:38+00:00
http://arxiv.org/abs/1504.00083v1,A Theory of Feature Learning,"Feature Learning aims to extract relevant information contained in data sets
in an automated fashion. It is driving force behind the current deep learning
trend, a set of methods that have had widespread empirical success. What is
lacking is a theoretical understanding of different feature learning schemes.
This work provides a theoretical framework for feature learning and then
characterizes when features can be learnt in an unsupervised fashion. We also
provide means to judge the quality of features via rate-distortion theory and
its generalizations.","['Brendan van Rooyen', 'Robert C. Williamson']","['stat.ML', 'cs.LG']",2015-04-01 02:31:55+00:00
http://arxiv.org/abs/1504.00064v1,Crowdsourcing Feature Discovery via Adaptively Chosen Comparisons,"We introduce an unsupervised approach to efficiently discover the underlying
features in a data set via crowdsourcing. Our queries ask crowd members to
articulate a feature common to two out of three displayed examples. In addition
we also ask the crowd to provide binary labels to the remaining examples based
on the discovered features. The triples are chosen adaptively based on the
labels of the previously discovered features on the data set. In two natural
models of features, hierarchical and independent, we show that a simple
adaptive algorithm, using ""two-out-of-three"" similarity queries, recovers all
features with less labor than any nonadaptive algorithm. Experimental results
validate the theoretical findings.","['James Y. Zou', 'Kamalika Chaudhuri', 'Adam Tauman Kalai']","['stat.ML', 'cs.LG']",2015-03-31 23:27:03+00:00
http://arxiv.org/abs/1504.00052v1,Improved Error Bounds Based on Worst Likely Assignments,"Error bounds based on worst likely assignments use permutation tests to
validate classifiers. Worst likely assignments can produce effective bounds
even for data sets with 100 or fewer training examples. This paper introduces a
statistic for use in the permutation tests of worst likely assignments that
improves error bounds, especially for accurate classifiers, which are typically
the classifiers of interest.",['Eric Bax'],"['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.PR']",2015-03-31 21:48:56+00:00
http://arxiv.org/abs/1503.09113v2,On the Projective Geometry of Kalman Filter,"Convergence of the Kalman filter is best analyzed by studying the contraction
of the Riccati map in the space of positive definite (covariance) matrices. In
this paper, we explore how this contraction property relates to a more
fundamental non-expansiveness property of filtering maps in the space of
probability distributions endowed with the Hilbert metric. This is viewed as a
preliminary step towards improving the convergence analysis of filtering
algorithms over general graphical models.","['Francesca Paola Carli', 'Rodolphe Sepulchre']","['math.OC', 'stat.ML']",2015-03-31 16:27:22+00:00
http://arxiv.org/abs/1503.09105v14,Two Timescale Stochastic Approximation with Controlled Markov noise and Off-policy temporal difference learning,"We present for the first time an asymptotic convergence analysis of two
time-scale stochastic approximation driven by `controlled' Markov noise. In
particular, both the faster and slower recursions have non-additive controlled
Markov noise components in addition to martingale difference noise. We analyze
the asymptotic behavior of our framework by relating it to limiting
differential inclusions in both time-scales that are defined in terms of the
ergodic occupation measures associated with the controlled Markov processes.
Finally, we present a solution to the off-policy convergence problem for
temporal difference learning with linear function approximation, using our
results.","['Prasenjit Karmakar', 'Shalabh Bhatnagar']","['math.DS', 'cs.AI', 'stat.ML']",2015-03-31 16:10:55+00:00
http://arxiv.org/abs/1503.09022v3,Multi-label Classification using Labels as Hidden Nodes,"Competitive methods for multi-label classification typically invest in
learning labels together. To do so in a beneficial way, analysis of label
dependence is often seen as a fundamental step, separate and prior to
constructing a classifier. Some methods invest up to hundreds of times more
computational effort in building dependency models, than training the final
classifier itself. We extend some recent discussion in the literature and
provide a deeper analysis, namely, developing the view that label dependence is
often introduced by an inadequate base classifier, rather than being inherent
to the data or underlying concept; showing how even an exhaustive analysis of
label dependence may not lead to an optimal classification structure. Viewing
labels as additional features (a transformation of the input), we create
neural-network inspired novel methods that remove the emphasis of a prior
dependency structure. Our methods have an important advantage particular to
multi-label data: they leverage labels to create effective units in middle
layers, rather than learning these units from scratch in an unsupervised
fashion with gradient-based methods. Results are promising. The methods we
propose perform competitively, and also have very important qualities of
scalability.","['Jesse Read', 'Jaakko Hollmén']","['stat.ML', 'cs.LG']",2015-03-31 12:29:13+00:00
http://arxiv.org/abs/1503.08985v2,Iterative Regularization for Learning with Convex Loss Functions,"We consider the problem of supervised learning with convex loss functions and
propose a new form of iterative regularization based on the subgradient method.
Unlike other regularization approaches, in iterative regularization no
constraint or penalization is considered, and generalization is achieved by
(early) stopping an empirical iteration. We consider a nonparametric setting,
in the framework of reproducing kernel Hilbert spaces, and prove finite sample
bounds on the excess risk under general regularity conditions. Our study
provides a new class of efficient regularized learning algorithms and gives
insights on the interplay between statistics and optimization in machine
learning.","['Junhong Lin', 'Lorenzo Rosasco', 'Ding-Xuan Zhou']","['stat.ML', 'math.OC']",2015-03-31 09:49:30+00:00
http://arxiv.org/abs/1503.08855v1,Decentralized learning for wireless communications and networking,"This chapter deals with decentralized learning algorithms for in-network
processing of graph-valued data. A generic learning problem is formulated and
recast into a separable form, which is iteratively minimized using the
alternating-direction method of multipliers (ADMM) so as to gain the desired
degree of parallelization. Without exchanging elements from the distributed
training sets and keeping inter-node communications at affordable levels, the
local (per-node) learners consent to the desired quantity inferred globally,
meaning the one obtained if the entire training data set were centrally
available. Impact of the decentralized learning framework to contemporary
wireless communications and networking tasks is illustrated through case
studies including target tracking using wireless sensor networks, unveiling
Internet traffic anomalies, power system state estimation, as well as spectrum
cartography for wireless cognitive radio networks.","['Georgios B. Giannakis', 'Qing Ling', 'Gonzalo Mateos', 'Ioannis D. Schizas', 'Hao Zhu']","['math.OC', 'cs.IT', 'cs.LG', 'cs.MA', 'cs.SY', 'math.IT', 'stat.ML']",2015-03-30 21:18:38+00:00
http://arxiv.org/abs/1503.08727v1,A Parzen-based distance between probability measures as an alternative of summary statistics in Approximate Bayesian Computation,"Approximate Bayesian Computation (ABC) are likelihood-free Monte Carlo
methods. ABC methods use a comparison between simulated data, using different
parameters drew from a prior distribution, and observed data. This comparison
process is based on computing a distance between the summary statistics from
the simulated data and the observed data. For complex models, it is usually
difficult to define a methodology for choosing or constructing the summary
statistics. Recently, a nonparametric ABC has been proposed, that uses a
dissimilarity measure between discrete distributions based on empirical kernel
embeddings as an alternative for summary statistics. The nonparametric ABC
outperforms other methods including ABC, kernel ABC or synthetic likelihood
ABC. However, it assumes that the probability distributions are discrete, and
it is not robust when dealing with few observations. In this paper, we propose
to apply kernel embeddings using an smoother density estimator or Parzen
estimator for comparing the empirical data distributions, and computing the ABC
posterior. Synthetic data and real data were used to test the Bayesian
inference of our method. We compare our method with respect to state-of-the-art
methods, and demonstrate that our method is a robust estimator of the posterior
distribution in terms of the number of observations.","['Carlos D. Zuluaga', 'Edgar A. Valencia', 'Mauricio A. Álvarez']",['stat.ML'],2015-03-30 16:10:23+00:00
http://arxiv.org/abs/1503.08542v1,Nonparametric Relational Topic Models through Dependent Gamma Processes,"Traditional Relational Topic Models provide a way to discover the hidden
topics from a document network. Many theoretical and practical tasks, such as
dimensional reduction, document clustering, link prediction, benefit from this
revealed knowledge. However, existing relational topic models are based on an
assumption that the number of hidden topics is known in advance, and this is
impractical in many real-world applications. Therefore, in order to relax this
assumption, we propose a nonparametric relational topic model in this paper.
Instead of using fixed-dimensional probability distributions in its generative
model, we use stochastic processes. Specifically, a gamma process is assigned
to each document, which represents the topic interest of this document.
Although this method provides an elegant solution, it brings additional
challenges when mathematically modeling the inherent network structure of
typical document network, i.e., two spatially closer documents tend to have
more similar topics. Furthermore, we require that the topics are shared by all
the documents. In order to resolve these challenges, we use a subsampling
strategy to assign each document a different gamma process from the global
gamma process, and the subsampling probabilities of documents are assigned with
a Markov Random Field constraint that inherits the document network structure.
Through the designed posterior inference algorithm, we can discover the hidden
topics and its number simultaneously. Experimental results on both synthetic
and real-world network datasets demonstrate the capabilities of learning the
hidden topics and, more importantly, the number of topics.","['Junyu Xuan', 'Jie Lu', 'Guangquan Zhang', 'Richard Yi Da Xu', 'Xiangfeng Luo']","['stat.ML', 'cs.CL', 'cs.IR', 'cs.LG']",2015-03-30 05:40:41+00:00
http://arxiv.org/abs/1503.08535v1,Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process,"Incorporating the side information of text corpus, i.e., authors, time
stamps, and emotional tags, into the traditional text mining models has gained
significant interests in the area of information retrieval, statistical natural
language processing, and machine learning. One branch of these works is the
so-called Author Topic Model (ATM), which incorporates the authors's interests
as side information into the classical topic model. However, the existing ATM
needs to predefine the number of topics, which is difficult and inappropriate
in many real-world settings. In this paper, we propose an Infinite Author Topic
(IAT) model to resolve this issue. Instead of assigning a discrete probability
on fixed number of topics, we use a stochastic process to determine the number
of topics from the data itself. To be specific, we extend a gamma-negative
binomial process to three levels in order to capture the
author-document-keyword hierarchical structure. Furthermore, each document is
assigned a mixed gamma process that accounts for the multi-author's
contribution towards this document. An efficient Gibbs sampling inference
algorithm with each conditional distribution being closed-form is developed for
the IAT model. Experiments on several real-world datasets show the capabilities
of our IAT model to learn the hidden topics, authors' interests on these topics
and the number of topics simultaneously.","['Junyu Xuan', 'Jie Lu', 'Guangquan Zhang', 'Richard Yi Da Xu', 'Xiangfeng Luo']","['stat.ML', 'cs.IR', 'cs.LG']",2015-03-30 05:03:37+00:00
http://arxiv.org/abs/1503.08479v1,"Active Authentication on Mobile Devices via Stylometry, Application Usage, Web Browsing, and GPS Location","Active authentication is the problem of continuously verifying the identity
of a person based on behavioral aspects of their interaction with a computing
device. In this study, we collect and analyze behavioral biometrics data from
200subjects, each using their personal Android mobile device for a period of at
least 30 days. This dataset is novel in the context of active authentication
due to its size, duration, number of modalities, and absence of restrictions on
tracked activity. The geographical colocation of the subjects in the study is
representative of a large closed-world environment such as an organization
where the unauthorized user of a device is likely to be an insider threat:
coming from within the organization. We consider four biometric modalities: (1)
text entered via soft keyboard, (2) applications used, (3) websites visited,
and (4) physical location of the device as determined from GPS (when outdoors)
or WiFi (when indoors). We implement and test a classifier for each modality
and organize the classifiers as a parallel binary decision fusion architecture.
We are able to characterize the performance of the system with respect to
intruder detection time and to quantify the contribution of each modality to
the overall performance.","['Lex Fridman', 'Steven Weber', 'Rachel Greenstadt', 'Moshe Kam']","['cs.CR', 'stat.ML']",2015-03-29 18:59:23+00:00
http://arxiv.org/abs/1503.08471v3,Cross-validation of matching correlation analysis by resampling matching weights,"The strength of association between a pair of data vectors is represented by
a nonnegative real number, called matching weight. For dimensionality
reduction, we consider a linear transformation of data vectors, and define a
matching error as the weighted sum of squared distances between transformed
vectors with respect to the matching weights. Given data vectors and matching
weights, the optimal linear transformation minimizing the matching error is
solved by the spectral graph embedding of Yan et al. (2007). This method is a
generalization of the canonical correlation analysis, and will be called as
matching correlation analysis (MCA). In this paper, we consider a novel
sampling scheme where the observed matching weights are randomly sampled from
underlying true matching weights with small probability, whereas the data
vectors are treated as constants. We then investigate a cross-validation by
resampling the matching weights. Our asymptotic theory shows that the
cross-validation, if rescaled properly, computes an unbiased estimate of the
matching error with respect to the true matching weights. Existing ideas of
cross-validation for resampling data vectors, instead of resampling matching
weights, are not applicable here. MCA can be used for data vectors from
multiple domains with different dimensions via an embarrassingly simple idea of
coding the data vectors. This method will be called as cross-domain matching
correlation analysis (CDMCA), and an interesting connection to the classical
associative memory model of neural networks is also discussed.",['Hidetoshi Shimodaira'],"['stat.ML', 'cs.LG']",2015-03-29 18:21:22+00:00
http://arxiv.org/abs/1503.08363v1,Active Model Aggregation via Stochastic Mirror Descent,"We consider the problem of learning convex aggregation of models, that is as
good as the best convex aggregation, for the binary classification problem.
Working in the stream based active learning setting, where the active learner
has to make a decision on-the-fly, if it wants to query for the label of the
point currently seen in the stream, we propose a stochastic-mirror descent
algorithm, called SMD-AMA, with entropy regularization. We establish an excess
risk bounds for the loss of the convex aggregate returned by SMD-AMA to be of
the order of $O\left(\sqrt{\frac{\log(M)}{{T^{1-\mu}}}}\right)$, where $\mu\in
[0,1)$ is an algorithm dependent parameter, that trades-off the number of
labels queried, and excess risk.",['Ravi Ganti'],"['stat.ML', 'cs.AI', 'cs.LG']",2015-03-28 22:54:12+00:00
http://arxiv.org/abs/1503.08356v4,Efficient Online Minimization for Low-Rank Subspace Clustering,"Low-rank representation~(LRR) has been a significant method for segmenting
data that are generated from a union of subspaces. It is, however, known that
solving the LRR program is challenging in terms of time complexity and memory
footprint, in that the size of the nuclear norm regularized matrix is
$n$-by-$n$ (where $n$ is the number of samples). In this paper, we thereby
develop a fast online implementation of LRR that reduces the memory cost from
$O(n^2)$ to $O(pd)$, with $p$ being the ambient dimension and $d$ being some
estimated rank~($d < p \ll n$). The crux for this end is a non-convex
reformulation of the LRR program, which pursues the basis dictionary that
generates the (uncorrupted) observations. We build the theoretical guarantee
that the sequence of the solutions produced by our algorithm converges to a
stationary point of the empirical and the expected loss function
asymptotically. Extensive experiments on synthetic and realistic datasets
further substantiate that our algorithm is fast, robust and memory efficient.","['Jie Shen', 'Ping Li', 'Huan Xu']",['stat.ML'],2015-03-28 21:56:35+00:00
http://arxiv.org/abs/1503.08348v1,Sparse Linear Regression With Missing Data,"This paper proposes a fast and accurate method for sparse regression in the
presence of missing data. The underlying statistical model encapsulates the
low-dimensional structure of the incomplete data matrix and the sparsity of the
regression coefficients, and the proposed algorithm jointly learns the
low-dimensional structure of the data and a linear regressor with sparse
coefficients. The proposed stochastic optimization method, Sparse Linear
Regression with Missing Data (SLRM), performs an alternating minimization
procedure and scales well with the problem size. Large deviation inequalities
shed light on the impact of the various problem-dependent parameters on the
expected squared loss of the learned regressor. Extensive simulations on both
synthetic and real datasets show that SLRM performs better than competing
algorithms in a variety of contexts.","['Ravi Ganti', 'Rebecca M. Willett']","['stat.ML', 'cs.LG', 'stat.ME']",2015-03-28 21:03:32+00:00
http://arxiv.org/abs/1503.08329v2,Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm,"We propose an extensive analysis of the behavior of majority votes in binary
classification. In particular, we introduce a risk bound for majority votes,
called the C-bound, that takes into account the average quality of the voters
and their average disagreement. We also propose an extensive PAC-Bayesian
analysis that shows how the C-bound can be estimated from various observations
contained in the training data. The analysis intends to be self-contained and
can be used as introductory material to PAC-Bayesian statistical learning
theory. It starts from a general PAC-Bayesian perspective and ends with
uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler
divergence and others allow kernel functions to be used as voters (via the
sample compression setting). Finally, out of the analysis, we propose the MinCq
learning algorithm that basically minimizes the C-bound. MinCq reduces to a
simple quadratic program. Aside from being theoretically grounded, MinCq
achieves state-of-the-art performance, as shown in our extensive empirical
comparison with both AdaBoost and the Support Vector Machine.","['Pascal Germain', 'Alexandre Lacasse', 'François Laviolette', 'Mario Marchand', 'Jean-Francis Roy']","['stat.ML', 'cs.LG']",2015-03-28 17:19:49+00:00
http://arxiv.org/abs/1503.08272v1,Robust Bayesian compressive sensing with data loss recovery for structural health monitoring signals,"The application of compressive sensing (CS) to structural health monitoring
is an emerging research topic. The basic idea in CS is to use a
specially-designed wireless sensor to sample signals that are sparse in some
basis (e.g. wavelet basis) directly in a compressed form, and then to
reconstruct (decompress) these signals accurately using some inversion
algorithm after transmission to a central processing unit. However, most
signals in structural health monitoring are only approximately sparse, i.e.
only a relatively small number of the signal coefficients in some basis are
significant, but the other coefficients are usually not exactly zero. In this
case, perfect reconstruction from compressed measurements is not expected. A
new Bayesian CS algorithm is proposed in which robust treatment of the
uncertain parameters is explored, including integration over the
prediction-error precision parameter to remove it as a ""nuisance"" parameter.
The performance of the new CS algorithm is investigated using compressed data
from accelerometers installed on a space-frame structure and on a cable-stayed
bridge. Compared with other state-of-the-art CS methods including our
previously-published Bayesian method which uses MAP (maximum a posteriori)
estimation of the prediction-error precision parameter, the new algorithm shows
superior performance in reconstruction robustness and posterior uncertainty
quantification. Furthermore, our method can be utilized for recovery of lost
data during wireless transmission, regardless of the level of sparseness in the
signal.","['Yong Huang', 'James L. Beck', 'Stephen Wu', 'Hui Li']","['stat.AP', 'stat.CO', 'stat.ML']",2015-03-28 06:14:45+00:00
http://arxiv.org/abs/1503.08195v2,"Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet Representations, and Forecast Rankings","In the practice of point prediction, it is desirable that forecasters receive
a directive in the form of a statistical functional, such as the mean or a
quantile of the predictive distribution. When evaluating and comparing
competing forecasts, it is then critical that the scoring function used for
these purposes be consistent for the functional at hand, in the sense that the
expected score is minimized when following the directive.
  We show that any scoring function that is consistent for a quantile or an
expectile functional, respectively, can be represented as a mixture of extremal
scoring functions that form a linearly parameterized family. Scoring functions
for the mean value and probability forecasts of binary events constitute
important examples. The quantile and expectile functionals along with the
respective extremal scoring functions admit appealing economic interpretations
in terms of thresholds in decision making.
  The Choquet type mixture representations give rise to simple checks of
whether a forecast dominates another in the sense that it is preferable under
any consistent scoring function. In empirical settings it suffices to compare
the average scores for only a finite number of extremal elements. Plots of the
average scores with respect to the extremal scoring functions, which we call
Murphy diagrams, permit detailed comparisons of the relative merits of
competing forecasts.","['Werner Ehm', 'Tilmann Gneiting', 'Alexander Jordan', 'Fabian Krüger']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2015-03-27 19:35:55+00:00
http://arxiv.org/abs/1503.07990v3,Estimating a common covariance matrix for network meta-analysis of gene expression datasets in diffuse large B-cell lymphoma,"The estimation of covariance matrices of gene expressions has many
applications in cancer systems biology. Many gene expression studies, however,
are hampered by low sample size and it has therefore become popular to increase
sample size by collecting gene expression data across studies. Motivated by the
traditional meta-analysis using random effects models, we present a
hierarchical random covariance model and use it for the meta-analysis of gene
correlation networks across 11 large-scale gene expression studies of diffuse
large B-cell lymphoma (DLBCL). We suggest to use a maximum likelihood estimator
for the underlying common covariance matrix and introduce an EM algorithm for
estimation. By simulation experiments comparing the estimated covariance
matrices by cophenetic correlation and Kullback-Leibler divergence the
suggested estimator showed to perform better or not worse than a simple pooled
estimator. In a posthoc analysis of the estimated common covariance matrix for
the DLBCL data we were able to identify novel biologically meaningful gene
correlation networks with eigengenes of prognostic value. In conclusion, the
method seems to provide a generally applicable framework for meta-analysis,
when multiple features are measured and believed to share a common covariance
matrix obscured by study dependent noise.","['Anders Ellern Bilgrau', 'Rasmus Froberg Brøndum', 'Poul Svante Eriksen', 'Karen Dybkær', 'Martin Bøgsted']","['stat.ML', 'q-bio.GN', 'stat.ME']",2015-03-27 08:43:50+00:00
http://arxiv.org/abs/1503.07970v1,Bayesian Cross Validation and WAIC for Predictive Prior Design in Regular Asymptotic Theory,"Prior design is one of the most important problems in both statistics and
machine learning. The cross validation (CV) and the widely applicable
information criterion (WAIC) are predictive measures of the Bayesian
estimation, however, it has been difficult to apply them to find the optimal
prior because their mathematical properties in prior evaluation have been
unknown and the region of the hyperparameters is too wide to be examined. In
this paper, we derive a new formula by which the theoretical relation among CV,
WAIC, and the generalization loss is clarified and the optimal hyperparameter
can be directly found.
  By the formula, three facts are clarified about predictive prior design.
Firstly, CV and WAIC have the same second order asymptotic expansion, hence
they are asymptotically equivalent to each other as the optimizer of the
hyperparameter. Secondly, the hyperparameter which minimizes CV or WAIC makes
the average generalization loss to be minimized asymptotically but does not the
random generalization loss. And lastly, by using the mathematical relation
between priors, the variances of the optimized hyperparameters by CV and WAIC
are made smaller with small computational costs. Also we show that the
optimized hyperparameter by DIC or the marginal likelihood does not minimize
the average or random generalization loss in general.",['Sumio Watanabe'],"['cs.LG', 'stat.ML']",2015-03-27 06:21:06+00:00
http://arxiv.org/abs/1503.07906v1,Generalized K-fan Multimodal Deep Model with Shared Representations,"Multimodal learning with deep Boltzmann machines (DBMs) is an generative
approach to fuse multimodal inputs, and can learn the shared representation via
Contrastive Divergence (CD) for classification and information retrieval tasks.
However, it is a 2-fan DBM model, and cannot effectively handle multiple
prediction tasks. Moreover, this model cannot recover the hidden
representations well by sampling from the conditional distribution when more
than one modalities are missing. In this paper, we propose a K-fan deep
structure model, which can handle the multi-input and muti-output learning
problems effectively. In particular, the deep structure has K-branch for
different inputs where each branch can be composed of a multi-layer deep model,
and a shared representation is learned in an discriminative manner to tackle
multimodal tasks. Given the deep structure, we propose two objective functions
to handle two multi-input and multi-output tasks: joint visual restoration and
labeling, and the multi-view multi-calss object recognition tasks. To estimate
the model parameters, we initialize the deep model parameters with CD to
maximize the joint distribution, and then we use backpropagation to update the
model according to specific objective function. The experimental results
demonstrate that the model can effectively leverages multi-source information
and predict multiple tasks well over competitive baselines.","['Gang Chen', 'Sargur N. Srihari']","['cs.LG', 'stat.ML', '68T10', 'I.2.6']",2015-03-26 21:17:46+00:00
http://arxiv.org/abs/1503.07810v6,Interpretable Classification Models for Recidivism Prediction,"We investigate a long-debated question, which is how to create predictive
models of recidivism that are sufficiently accurate, transparent, and
interpretable to use for decision-making. This question is complicated as these
models are used to support different decisions, from sentencing, to determining
release on probation, to allocating preventative social services. Each use case
might have an objective other than classification accuracy, such as a desired
true positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is
a point on the receiver operator characteristic (ROC) curve. We use popular
machine learning methods to create models along the full ROC curve on a wide
range of recidivism prediction problems. We show that many methods (SVM, Ridge
Regression) produce equally accurate models along the full ROC curve. However,
methods that designed for interpretability (CART, C5.0) cannot be tuned to
produce models that are accurate and/or interpretable. To handle this
shortcoming, we use a new method known as SLIM (Supersparse Linear Integer
Models) to produce accurate, transparent, and interpretable models along the
full ROC curve. These models can be used for decision-making for many different
use cases, since they are just as accurate as the most powerful black-box
machine learning models, but completely transparent, and highly interpretable.","['Jiaming Zeng', 'Berk Ustun', 'Cynthia Rudin']","['stat.ML', 'stat.AP']",2015-03-26 18:21:29+00:00
http://arxiv.org/abs/1503.07689v3,Likelihood-free Model Choice,"This document is an invited chapter covering the specificities of ABC model
choice, intended for the incoming Handbook of ABC by Sisson, Fan, and Beaumont
(2017). Beyond exposing the potential pitfalls of ABC based posterior
probabilities, the review emphasizes mostly the solution proposed by Pudlo et
al. (2016) on the use of random forests for aggregating summary statistics and
and for estimating the posterior probability of the most likely model via a
secondary random fores.","['Jean-Michel Marin', 'Pierre Pudlo', 'Arnaud Estoup', 'Christian P. Robert']","['stat.ME', 'stat.CO', 'stat.ML']",2015-03-26 11:17:22+00:00
http://arxiv.org/abs/1503.07508v1,Stable Feature Selection from Brain sMRI,"Neuroimage analysis usually involves learning thousands or even millions of
variables using only a limited number of samples. In this regard, sparse
models, e.g. the lasso, are applied to select the optimal features and achieve
high diagnosis accuracy. The lasso, however, usually results in independent
unstable features. Stability, a manifest of reproducibility of statistical
results subject to reasonable perturbations to data and the model, is an
important focus in statistics, especially in the analysis of high dimensional
data. In this paper, we explore a nonnegative generalized fused lasso model for
stable feature selection in the diagnosis of Alzheimer's disease. In addition
to sparsity, our model incorporates two important pathological priors: the
spatial cohesion of lesion voxels and the positive correlation between the
features and the disease labels. To optimize the model, we propose an efficient
algorithm by proving a novel link between total variation and fast network flow
algorithms via conic duality. Experiments show that the proposed nonnegative
model performs much better in exploring the intrinsic structure of data via
selecting stable features compared with other state-of-the-arts.","['Bo Xin', 'Lingjing Hu', 'Yizhou Wang', 'Wen Gao']","['cs.LG', 'stat.ML']",2015-03-25 19:30:14+00:00
http://arxiv.org/abs/1503.07368v3,Quantized Nonparametric Estimation over Sobolev Ellipsoids,"We formulate the notion of minimax estimation under storage or communication
constraints, and prove an extension to Pinsker's theorem for nonparametric
estimation over Sobolev ellipsoids. Placing limits on the number of bits used
to encode any estimator, we give tight lower and upper bounds on the excess
risk due to quantization in terms of the number of bits, the signal size, and
the noise level. This establishes the Pareto optimal tradeoff between storage
and risk under quantization constraints for Sobolev spaces. Our results and
proof techniques combine elements of rate distortion theory and minimax
analysis. The proposed quantized estimation scheme, which shows achievability
of the lower bounds, is adaptive in the usual statistical sense, achieving the
optimal quantized minimax rate without knowledge of the smoothness parameter of
the Sobolev space. It is also adaptive in a computational sense, as it
constructs the code only after observing the data, to dynamically allocate more
codewords to blocks where the estimated signal size is large. Simulations are
included that illustrate the effect of quantization on statistical risk.","['Yuancheng Zhu', 'John Lafferty']","['math.ST', 'stat.ML', 'stat.TH']",2015-03-25 13:08:49+00:00
http://arxiv.org/abs/1503.07340v2,A Bayesian Approach to Sparse plus Low rank Network Identification,"We consider the problem of modeling multivariate time series with
parsimonious dynamical models which can be represented as sparse dynamic
Bayesian networks with few latent nodes. This structure translates into a
sparse plus low rank model. In this paper, we propose a Gaussian regression
approach to identify such a model.","['Mattia Zorzi', 'Alessandro Chiuso']","['math.OC', 'stat.ML']",2015-03-25 11:32:03+00:00
http://arxiv.org/abs/1503.07240v1,Regularized Minimax Conditional Entropy for Crowdsourcing,"There is a rapidly increasing interest in crowdsourcing for data labeling. By
crowdsourcing, a large number of labels can be often quickly gathered at low
cost. However, the labels provided by the crowdsourcing workers are usually not
of high quality. In this paper, we propose a minimax conditional entropy
principle to infer ground truth from noisy crowdsourced labels. Under this
principle, we derive a unique probabilistic labeling model jointly
parameterized by worker ability and item difficulty. We also propose an
objective measurement principle, and show that our method is the only method
which satisfies this objective measurement principle. We validate our method
through a variety of real crowdsourcing datasets with binary, multiclass or
ordinal labels.","['Dengyong Zhou', 'Qiang Liu', 'John C. Platt', 'Christopher Meek', 'Nihar B. Shah']","['cs.LG', 'stat.ML']",2015-03-25 00:10:11+00:00
http://arxiv.org/abs/1503.07211v1,Universal Approximation of Markov Kernels by Shallow Stochastic Feedforward Networks,"We establish upper bounds for the minimal number of hidden units for which a
binary stochastic feedforward network with sigmoid activation probabilities and
a single hidden layer is a universal approximator of Markov kernels. We show
that each possible probabilistic assignment of the states of $n$ output units,
given the states of $k\geq1$ input units, can be approximated arbitrarily well
by a network with $2^{k-1}(2^{n-1}-1)$ hidden units.",['Guido Montufar'],"['cs.LG', 'stat.ML', '82C32']",2015-03-24 21:38:59+00:00
http://arxiv.org/abs/1503.07077v1,Rotation-invariant convolutional neural networks for galaxy morphology prediction,"Measuring the morphological parameters of galaxies is a key requirement for
studying their formation and evolution. Surveys such as the Sloan Digital Sky
Survey (SDSS) have resulted in the availability of very large collections of
images, which have permitted population-wide analyses of galaxy morphology.
Morphological analysis has traditionally been carried out mostly via visual
inspection by trained experts, which is time-consuming and does not scale to
large ($\gtrsim10^4$) numbers of images.
  Although attempts have been made to build automated classification systems,
these have not been able to achieve the desired level of accuracy. The Galaxy
Zoo project successfully applied a crowdsourcing strategy, inviting online
users to classify images by answering a series of questions. Unfortunately,
even this approach does not scale well enough to keep up with the increasing
availability of galaxy images.
  We present a deep neural network model for galaxy morphology classification
which exploits translational and rotational symmetry. It was developed in the
context of the Galaxy Challenge, an international competition to build the best
model for morphology classification based on annotated images from the Galaxy
Zoo project.
  For images with high agreement among the Galaxy Zoo participants, our model
is able to reproduce their consensus with near-perfect accuracy ($> 99\%$) for
most questions. Confident model predictions are highly accurate, which makes
the model suitable for filtering large collections of images and forwarding
challenging images to experts for manual annotation. This approach greatly
reduces the experts' workload without affecting accuracy. The application of
these algorithms to larger sets of training data will be critical for analysing
results from future surveys such as the LSST.","['Sander Dieleman', 'Kyle W. Willett', 'Joni Dambre']","['astro-ph.IM', 'astro-ph.GA', 'cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']",2015-03-24 15:34:06+00:00
http://arxiv.org/abs/1503.06944v3,PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers,"In this paper, we provide two main contributions in PAC-Bayesian theory for
domain adaptation where the objective is to learn, from a source distribution,
a well-performing majority vote on a different target distribution. On the one
hand, we propose an improvement of the previous approach proposed by Germain et
al. (2013), that relies on a novel distribution pseudodistance based on a
disagreement averaging, allowing us to derive a new tighter PAC-Bayesian domain
adaptation bound for the stochastic Gibbs classifier. We specialize it to
linear classifiers, and design a learning algorithm which shows interesting
results on a synthetic problem and on a popular sentiment annotation task. On
the other hand, we generalize these results to multisource domain adaptation
allowing us to take into account different source domains. This study opens the
door to tackle domain adaptation tasks by making use of all the PAC-Bayesian
tools.","['Pascal Germain', 'Amaury Habrard', 'François Laviolette', 'Emilie Morvant']","['stat.ML', 'cs.LG']",2015-03-24 08:17:44+00:00
http://arxiv.org/abs/1503.06910v1,"Penalty, Shrinkage, and Preliminary Test Estimators under Full Model Hypothesis","This paper considers a multiple regression model and compares, under full
model hypothesis, analytically as well as by simulation, the performance
characteristics of some popular penalty estimators such as ridge regression,
LASSO, adaptive LASSO, SCAD, and elastic net versus Least Squares Estimator,
restricted estimator, preliminary test estimator, and Stein-type estimators
when the dimension of the parameter space is smaller than the sample space
dimension. We find that RR uniformly dominates LSE, RE, PTE, SE and PRSE while
LASSO, aLASSO, SCAD, and EN uniformly dominates LSE only. Further, it is
observed that neither penalty estimators nor Stein-type estimator dominate one
another.","['Enayetur Raheem', 'A. K. Md. Ehsanes Saleh']","['math.ST', 'stat.CO', 'stat.ME', 'stat.ML', 'stat.TH']",2015-03-24 04:40:53+00:00
http://arxiv.org/abs/1503.06567v2,On some provably correct cases of variational inference for topic models,"Variational inference is a very efficient and popular heuristic used in
various forms in the context of latent variable models. It's closely related to
Expectation Maximization (EM), and is applied when exact EM is computationally
infeasible. Despite being immensely popular, current theoretical understanding
of the effectiveness of variaitonal inference based algorithms is very limited.
In this work we provide the first analysis of instances where variational
inference algorithms converge to the global optimum, in the setting of topic
models.
  More specifically, we show that variational inference provably learns the
optimal parameters of a topic model under natural assumptions on the topic-word
matrix and the topic priors. The properties that the topic word matrix must
satisfy in our setting are related to the topic expansion assumption introduced
in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora
et al., 2012c). The assumptions on the topic priors are related to the well
known Dirichlet prior, introduced to the area of topic modeling by (Blei et
al., 2003).
  It is well known that initialization plays a crucial role in how well
variational based algorithms perform in practice. The initializations that we
use are fairly natural. One of them is similar to what is currently used in
LDA-c, the most popular implementation of variational inference for topic
models. The other one is an overlapping clustering algorithm, inspired by a
work by (Arora et al., 2014) on dictionary learning, which is very simple and
efficient.
  While our primary goal is to provide insights into when variational inference
might work in practice, the multiplicative, rather than the additive nature of
the variational inference updates forces us to use fairly non-standard proof
arguments, which we believe will be of general interest.","['Pranjal Awasthi', 'Andrej Risteski']","['cs.LG', 'cs.DS', 'stat.ML']",2015-03-23 09:20:39+00:00
http://arxiv.org/abs/1503.06452v1,Unsupervised model compression for multilayer bootstrap networks,"Recently, multilayer bootstrap network (MBN) has demonstrated promising
performance in unsupervised dimensionality reduction. It can learn compact
representations in standard data sets, i.e. MNIST and RCV1. However, as a
bootstrap method, the prediction complexity of MBN is high. In this paper, we
propose an unsupervised model compression framework for this general problem of
unsupervised bootstrap methods. The framework compresses a large unsupervised
bootstrap model into a small model by taking the bootstrap model and its
application together as a black box and learning a mapping function from the
input of the bootstrap model to the output of the application by a supervised
learner. To specialize the framework, we propose a new technique, named
compressive MBN. It takes MBN as the unsupervised bootstrap model and deep
neural network (DNN) as the supervised learner. Our initial result on MNIST
showed that compressive MBN not only maintains the high prediction accuracy of
MBN but also is over thousands of times faster than MBN at the prediction
stage. Our result suggests that the new technique integrates the effectiveness
of MBN on unsupervised learning and the effectiveness and efficiency of DNN on
supervised learning together for the effectiveness and efficiency of
compressive MBN on unsupervised learning.",['Xiao-Lei Zhang'],"['cs.LG', 'cs.NE', 'stat.ML']",2015-03-22 18:22:28+00:00
http://arxiv.org/abs/1503.06432v1,Indian Buffet process for model selection in convolved multiple-output Gaussian processes,"Multi-output Gaussian processes have received increasing attention during the
last few years as a natural mechanism to extend the powerful flexibility of
Gaussian processes to the setup of multiple output variables. The key point
here is the ability to design kernel functions that allow exploiting the
correlations between the outputs while fulfilling the positive definiteness
requisite for the covariance function. Alternatives to construct these
covariance functions are the linear model of coregionalization and process
convolutions. Each of these methods demand the specification of the number of
latent Gaussian process used to build the covariance function for the outputs.
We propose in this paper, the use of an Indian Buffet process as a way to
perform model selection over the number of latent Gaussian processes. This type
of model is particularly important in the context of latent force models, where
the latent forces are associated to physical quantities like protein profiles
or latent forces in mechanical systems. We use variational inference to
estimate posterior distributions over the variables involved, and show examples
of the model performance over artificial data, a motion capture dataset, and a
gene expression dataset.","['Cristian Guarnizo', 'Mauricio A. Álvarez']",['stat.ML'],2015-03-22 14:15:04+00:00
http://arxiv.org/abs/1503.06429v1,Asymmetric Distributions from Constrained Mixtures,"This paper introduces constrained mixtures for continuous distributions,
characterized by a mixture of distributions where each distribution has a shape
similar to the base distribution and disjoint domains. This new concept is used
to create generalized asymmetric versions of the Laplace and normal
distributions, which are shown to define exponential families, with known
conjugate priors, and to have maximum likelihood estimates for the original
parameters, with known closed-form expressions. The asymmetric and symmetric
normal distributions are compared in a linear regression example, showing that
the asymmetric version performs at least as well as the symmetric one, and in a
real world time-series problem, where a hidden Markov model is used to fit a
stock index, indicating that the asymmetric version provides higher likelihood
and may learn distribution models over states and transition distributions with
considerably less entropy.","['Conrado S. Miranda', 'Fernando J. Von Zuben']","['stat.ML', 'cs.LG']",2015-03-22 13:55:10+00:00
http://arxiv.org/abs/1503.06410v2,"What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes","The F-measure or F-score is one of the most commonly used single number
measures in Information Retrieval, Natural Language Processing and Machine
Learning, but it is based on a mistake, and the flawed assumptions render it
unsuitable for use in most contexts! Fortunately, there are better
alternatives.",['David M. W. Powers'],"['cs.IR', 'cs.CL', 'cs.LG', 'cs.NE', 'stat.CO', 'stat.ML', '68T05, 68Q32, 91E45', 'D.2.8; I.2.6; I.2.7; I.4.6; I.5.1']",2015-03-22 11:32:34+00:00
http://arxiv.org/abs/1503.06388v3,"Adaptive Concentration of Regression Trees, with Application to Random Forests","We study the convergence of the predictive surface of regression trees and
forests. To support our analysis we introduce a notion of adaptive
concentration for regression trees. This approach breaks tree training into a
model selection phase in which we pick the tree splits, followed by a model
fitting phase where we find the best regression model consistent with these
splits. We then show that the fitted regression tree concentrates around the
optimal predictor with the same splits: as d and n get large, the discrepancy
is with high probability bounded on the order of sqrt(log(d) log(n)/k)
uniformly over the whole regression surface, where d is the dimension of the
feature space, n is the number of training examples, and k is the minimum leaf
size for each tree. We also provide rate-matching lower bounds for this
adaptive concentration statement. From a practical perspective, our result
enables us to prove consistency results for adaptively grown forests in high
dimensions, and to carry out valid post-selection inference in the sense of
Berk et al. [2013] for subgroups defined by tree leaves.","['Stefan Wager', 'Guenther Walther']","['math.ST', 'stat.ML', 'stat.TH']",2015-03-22 06:07:14+00:00
http://arxiv.org/abs/1503.06379v3,Relaxed Leverage Sampling for Low-rank Matrix Completion,"We consider the problem of exact recovery of any $m\times n$ matrix of rank
$\varrho$ from a small number of observed entries via the standard nuclear norm
minimization framework. Such low-rank matrices have degrees of freedom
$(m+n)\varrho - \varrho^2$. We show that any arbitrary low-rank matrices can be
recovered exactly from a $\Theta\left(((m+n)\varrho -
\varrho^2)\log^2(m+n)\right)$ randomly sampled entries, thus matching the lower
bound on the required number of entries (in terms of degrees of freedom), with
an additional factor of $O(\log^2(m+n))$. To achieve this bound on sample size
we observe each entry with probabilities proportional to the sum of
corresponding row and column leverage scores, minus their product. We show that
this relaxation in sampling probabilities (as opposed to sum of leverage scores
in Chen et al, 2014) can give us an $O(\varrho^2\log^2(m+n))$ additive
improvement on the (best known) sample size obtained by Chen et al, 2014, for
the nuclear norm minimization. Experiments on real data corroborate the
theoretical improvement on sample size. Further, exact recovery of $(a)$
incoherent matrices (with restricted leverage scores), and $(b)$ matrices with
only one of the row or column spaces to be incoherent, can be performed using
our relaxed leverage score sampling, via nuclear norm minimization, without
knowing the leverage scores a priori. In such settings also we can achieve
improvement on sample size.",['Abhisek Kundu'],"['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2015-03-22 03:27:15+00:00
http://arxiv.org/abs/1503.06267v1,Hierarchical sparse Bayesian learning: theory and application for inferring structural damage from incomplete modal data,"Structural damage due to excessive loading or environmental degradation
typically occurs in localized areas in the absence of collapse. This prior
information about the spatial sparseness of structural damage is exploited here
by a hierarchical sparse Bayesian learning framework with the goal of reducing
the source of ill-conditioning in the stiffness loss inversion problem for
damage detection. Sparse Bayesian learning methodologies automatically prune
away irrelevant or inactive features from a set of potential candidates, and so
they are effective probabilistic tools for producing sparse explanatory
subsets. We have previously proposed such an approach to establish the
probability of localized stiffness reductions that serve as a proxy for damage
by using noisy incomplete modal data from before and after possible damage. The
core idea centers on a specific hierarchical Bayesian model that promotes
spatial sparseness in the inferred stiffness reductions in a way that is
consistent with the Bayesian Ockham razor. In this paper, we improve the theory
of our previously proposed sparse Bayesian learning approach by eliminating an
approximation and, more importantly, incorporating a constraint on stiffness
increases. Our approach has many appealing features that are summarized at the
end of the paper. We validate the approach by applying it to the Phase II
simulated and experimental benchmark studies sponsored by the IASC-ASCE Task
Group on Structural Health Monitoring. The results show that it can reliably
detect, locate and assess damage by inferring substructure stiffness losses
from the identified modal parameters. The occurrence of missed and false damage
alerts is effectively suppressed.","['Yong Huang', 'James L. Beck']","['stat.AP', 'stat.ME', 'stat.ML']",2015-03-21 05:45:16+00:00
http://arxiv.org/abs/1503.06250v1,Fast Imbalanced Classification of Healthcare Data with Missing Values,"In medical domain, data features often contain missing values. This can
create serious bias in the predictive modeling. Typical standard data mining
methods often produce poor performance measures. In this paper, we propose a
new method to simultaneously classify large datasets and reduce the effects of
missing values. The proposed method is based on a multilevel framework of the
cost-sensitive SVM and the expected maximization imputation method for missing
values, which relies on iterated regression analyses. We compare classification
results of multilevel SVM-based algorithms on public benchmark datasets with
imbalanced classes and missing values as well as real data in health
applications, and show that our multilevel SVM-based method produces fast, and
more accurate and robust classification results.","['Talayeh Razzaghi', 'Oleg Roderick', 'Ilya Safro', 'Nick Marko']","['stat.ML', 'cs.LG']",2015-03-21 00:13:54+00:00
http://arxiv.org/abs/1503.06239v1,Block-Wise MAP Inference for Determinantal Point Processes with Application to Change-Point Detection,"Existing MAP inference algorithms for determinantal point processes (DPPs)
need to calculate determinants or conduct eigenvalue decomposition generally at
the scale of the full kernel, which presents a great challenge for real-world
applications. In this paper, we introduce a class of DPPs, called BwDPPs, that
are characterized by an almost block diagonal kernel matrix and thus can allow
efficient block-wise MAP inference. Furthermore, BwDPPs are successfully
applied to address the difficulty of selecting change-points in the problem of
change-point detection (CPD), which results in a new BwDPP-based CPD method,
named BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is
first created based on existing well-studied metrics. Then, these change-point
candidates are treated as DPP items, and DPP-based subset selection is
conducted to give the final estimate of the change-points that favours both
quality and diversity. The effectiveness of BwDppCpd is demonstrated through
extensive experiments on five real-world datasets.","['Jinye Zhang', 'Zhijian Ou']","['cs.LG', 'cs.AI', 'stat.ME', 'stat.ML']",2015-03-20 22:01:45+00:00
http://arxiv.org/abs/1503.06236v5,Nonparametric Estimation of Band-limited Probability Density Functions,"In this paper, a nonparametric maximum likelihood (ML) estimator for
band-limited (BL) probability density functions (pdfs) is proposed. The BLML
estimator is consistent and computationally efficient. To compute the BLML
estimator, three approximate algorithms are presented: a binary quadratic
programming (BQP) algorithm for medium scale problems, a Trivial algorithm for
large-scale problems that yields a consistent estimate if the underlying pdf is
strictly positive and BL, and a fast implementation of the Trivial algorithm
that exploits the band-limited assumption and the Nyquist sampling theorem
(""BLMLQuick""). All three BLML estimators outperform kernel density estimation
(KDE) algorithms (adaptive and higher order KDEs) with respect to the mean
integrated squared error for data generated from both BL and infinite-band
pdfs. Further, the BLMLQuick estimate is remarkably faster than the KD
algorithms. Finally, the BLML method is applied to estimate the conditional
intensity function of a neuronal spike train (point process) recorded from a
rat's entorhinal cortex grid cell, for which it outperforms state-of-the-art
estimators used in neuroscience.","['Rahul Agarwal', 'Zhe Chen', 'Sridevi V. Sarma']","['stat.ML', 'math.ST', 'stat.ME', 'stat.TH']",2015-03-20 21:51:39+00:00
http://arxiv.org/abs/1503.06134v2,A Bennett Inequality for the Missing Mass,"Novel concentration inequalities are obtained for the missing mass, i.e. the
total probability mass of the outcomes not observed in the sample. We derive
distribution-free deviation bounds with sublinear exponents in deviation size
for missing mass and improve the results of Berend and Kontorovich (2013) and
Yari Saeed Khanloo and Haffari (2015) for small deviations which is the most
important case in learning theory.",['Bahman Yari Saeed Khanloo'],['stat.ML'],2015-03-20 16:08:56+00:00
http://arxiv.org/abs/1503.06060v1,Country-scale Exploratory Analysis of Call Detail Records through the Lens of Data Grid Models,"Call Detail Records (CDRs) are data recorded by telecommunications companies,
consisting of basic informations related to several dimensions of the calls
made through the network: the source, destination, date and time of calls. CDRs
data analysis has received much attention in the recent years since it might
reveal valuable information about human behavior. It has shown high added value
in many application domains like e.g., communities analysis or network
planning. In this paper, we suggest a generic methodology for summarizing
information contained in CDRs data. The method is based on a parameter-free
estimation of the joint distribution of the variables that describe the calls.
We also suggest several well-founded criteria that allows one to browse the
summary at various granularities and to explore the summary by means of
insightful visualizations. The method handles network graph data, temporal
sequence data as well as user mobility data stemming from original CDRs data.
We show the relevance of our methodology for various case studies on real-world
CDRs data from Ivory Coast.","['Romain Guigourès', 'Dominique Gay', 'Marc Boullé', 'Fabrice Clérot', 'Fabrice Rossi']","['cs.DB', 'stat.ML', 'stat.ML - Machine Learning']",2015-03-20 13:13:42+00:00
http://arxiv.org/abs/1503.06058v3,Sequential Monte Carlo Methods for System Identification,"One of the key challenges in identifying nonlinear and possibly non-Gaussian
state space models (SSMs) is the intractability of estimating the system state.
Sequential Monte Carlo (SMC) methods, such as the particle filter (introduced
more than two decades ago), provide numerical solutions to the nonlinear state
estimation problems arising in SSMs. When combined with additional
identification techniques, these algorithms provide solid solutions to the
nonlinear system identification problem. We describe two general strategies for
creating such combinations and discuss why SMC is a natural tool for
implementing these strategies.","['Thomas B. Schön', 'Fredrik Lindsten', 'Johan Dahlin', 'Johan Wågberg', 'Christian A. Naesseth', 'Andreas Svensson', 'Liang Dai']","['stat.CO', 'math.OC', 'stat.ML']",2015-03-20 13:06:38+00:00
http://arxiv.org/abs/1503.05743v1,"Implementation of a Practical Distributed Calculation System with Browsers and JavaScript, and Application to Distributed Deep Learning","Deep learning can achieve outstanding results in various fields. However, it
requires so significant computational power that graphics processing units
(GPUs) and/or numerous computers are often required for the practical
application. We have developed a new distributed calculation framework called
""Sashimi"" that allows any computer to be used as a distribution node only by
accessing a website. We have also developed a new JavaScript neural network
framework called ""Sukiyaki"" that uses general purpose GPUs with web browsers.
Sukiyaki performs 30 times faster than a conventional JavaScript library for
deep convolutional neural networks (deep CNNs) learning. The combination of
Sashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the
distributed deep learning of deep CNNs only with web browsers on various
devices. The libraries that comprise the proposed methods are available under
MIT license at http://mil-tokyo.github.io/.","['Ken Miura', 'Tatsuya Harada']","['cs.DC', 'cs.LG', 'cs.MS', 'cs.NE', 'stat.ML']",2015-03-19 12:41:29+00:00
http://arxiv.org/abs/1503.05724v3,A Neural Transfer Function for a Smooth and Differentiable Transition Between Additive and Multiplicative Interactions,"Existing approaches to combine both additive and multiplicative neural units
either use a fixed assignment of operations or require discrete optimization to
determine what function a neuron should perform. This leads either to an
inefficient distribution of computational resources or an extensive increase in
the computational complexity of the training procedure.
  We present a novel, parameterizable transfer function based on the
mathematical concept of non-integer functional iteration that allows the
operation each neuron performs to be smoothly and, most importantly,
differentiablely adjusted between addition and multiplication. This allows the
decision between addition and multiplication to be integrated into the standard
backpropagation training procedure.","['Sebastian Urban', 'Patrick van der Smagt']","['stat.ML', 'cs.LG', 'cs.NE']",2015-03-19 11:48:14+00:00
http://arxiv.org/abs/1503.05684v1,Non-parametric Bayesian Models of Response Function in Dynamic Image Sequences,"Estimation of response functions is an important task in dynamic medical
imaging. This task arises for example in dynamic renal scintigraphy, where
impulse response or retention functions are estimated, or in functional
magnetic resonance imaging where hemodynamic response functions are required.
These functions can not be observed directly and their estimation is
complicated because the recorded images are subject to superposition of
underlying signals. Therefore, the response functions are estimated via blind
source separation and deconvolution. Performance of this algorithm heavily
depends on the used models of the response functions. Response functions in
real image sequences are rather complicated and finding a suitable parametric
form is problematic. In this paper, we study estimation of the response
functions using non-parametric Bayesian priors. These priors were designed to
favor desirable properties of the functions, such as sparsity or smoothness.
These assumptions are used within hierarchical priors of the blind source
separation and deconvolution algorithm. Comparison of the resulting algorithms
with these priors is performed on synthetic dataset as well as on real datasets
from dynamic renal scintigraphy. It is shown that flexible non-parametric
priors improve estimation of response functions in both cases. MATLAB
implementation of the resulting algorithms is freely available for download.","['Ondřej Tichý', 'Václav Šmídl']",['stat.ML'],2015-03-19 09:53:34+00:00
http://arxiv.org/abs/1503.05671v7,Optimizing Neural Networks with Kronecker-factored Approximate Curvature,"We propose an efficient method for approximating natural gradient descent in
neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC).
K-FAC is based on an efficiently invertible approximation of a neural network's
Fisher information matrix which is neither diagonal nor low-rank, and in some
cases is completely non-sparse. It is derived by approximating various large
blocks of the Fisher (corresponding to entire layers) as being the Kronecker
product of two much smaller matrices. While only several times more expensive
to compute than the plain stochastic gradient, the updates produced by K-FAC
make much more progress optimizing the objective, which results in an algorithm
that can be much faster than stochastic gradient descent with momentum in
practice. And unlike some previously proposed approximate
natural-gradient/Newton methods which use high-quality non-diagonal curvature
matrices (such as Hessian-free optimization), K-FAC works very well in highly
stochastic optimization regimes. This is because the cost of storing and
inverting K-FAC's approximation to the curvature matrix does not depend on the
amount of data used to estimate it, which is a feature typically associated
only with diagonal or low-rank approximations to the curvature matrix.","['James Martens', 'Roger Grosse']","['cs.LG', 'cs.NE', 'stat.ML']",2015-03-19 08:30:24+00:00
http://arxiv.org/abs/1503.05567v1,The Knowledge Gradient Policy Using A Sparse Additive Belief Model,"We propose a sequential learning policy for noisy discrete global
optimization and ranking and selection (R\&S) problems with high dimensional
sparse belief functions, where there are hundreds or even thousands of
features, but only a small portion of these features contain explanatory power.
We aim to identify the sparsity pattern and select the best alternative before
the finite budget is exhausted. We derive a knowledge gradient policy for
sparse linear models (KGSpLin) with group Lasso penalty. This policy is a
unique and novel hybrid of Bayesian R\&S with frequentist learning.
Particularly, our method naturally combines B-spline basis expansion and
generalizes to the nonparametric additive model (KGSpAM) and functional ANOVA
model. Theoretically, we provide the estimation error bounds of the posterior
mean estimate and the functional estimate. Controlled experiments show that the
algorithm efficiently learns the correct set of nonzero parameters even when
the model is imbedded with hundreds of dummy parameters. Also it outperforms
the knowledge gradient for a linear model.","['Yan Li', 'Han Liu', 'Warren Powell']","['stat.ML', 'cs.IT', 'cs.SY', 'math.IT']",2015-03-18 20:01:22+00:00
http://arxiv.org/abs/1503.05526v1,Interpretable Aircraft Engine Diagnostic via Expert Indicator Aggregation,"Detecting early signs of failures (anomalies) in complex systems is one of
the main goal of preventive maintenance. It allows in particular to avoid
actual failures by (re)scheduling maintenance operations in a way that
optimizes maintenance costs. Aircraft engine health monitoring is one
representative example of a field in which anomaly detection is crucial.
Manufacturers collect large amount of engine related data during flights which
are used, among other applications, to detect anomalies. This article
introduces and studies a generic methodology that allows one to build automatic
early signs of anomaly detection in a way that builds upon human expertise and
that remains understandable by human operators who make the final maintenance
decision. The main idea of the method is to generate a very large number of
binary indicators based on parametric anomaly scores designed by experts,
complemented by simple aggregations of those scores. A feature selection method
is used to keep only the most discriminant indicators which are used as inputs
of a Naive Bayes classifier. This give an interpretable classifier based on
interpretable anomaly detectors whose parameters have been optimized indirectly
by the selection process. The proposed methodology is evaluated on simulated
data designed to reproduce some of the anomaly types observed in real world
engines.","['Tsirizo Rabenoro', 'Jérôme Lacaille', 'Marie Cottrell', 'Fabrice Rossi']","['stat.ML', 'cs.LG', 'math.ST', 'stat.AP', 'stat.TH']",2015-03-18 18:30:34+00:00
http://arxiv.org/abs/1503.05509v4,Differentiating the multipoint Expected Improvement for optimal batch design,"This work deals with parallel optimization of expensive objective functions
which are modeled as sample realizations of Gaussian processes. The study is
formalized as a Bayesian optimization problem, or continuous multi-armed bandit
problem, where a batch of q > 0 arms is pulled in parallel at each iteration.
Several algorithms have been developed for choosing batches by trading off
exploitation and exploration. As of today, the maximum Expected Improvement
(EI) and Upper Confidence Bound (UCB) selection rules appear as the most
prominent approaches for batch selection. Here, we build upon recent work on
the multipoint Expected Improvement criterion, for which an analytic expansion
relying on Tallis' formula was recently established. The computational burden
of this selection rule being still an issue in application, we derive a
closed-form expression for the gradient of the multipoint Expected Improvement,
which aims at facilitating its maximization using gradient-based ascent
algorithms. Substantial computational savings are shown in application. In
addition, our algorithms are tested numerically and compared to
state-of-the-art UCB-based batch-sequential algorithms. Combining starting
designs relying on UCB with gradient-based EI local optimization finally
appears as a sound option for batch design in distributed Gaussian Process
optimization.","['Sébastien Marmin', 'Clément Chevalier', 'David Ginsbourger']","['stat.ML', 'math.ST', 'stat.TH']",2015-03-18 17:45:44+00:00
http://arxiv.org/abs/1503.05479v2,Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm,"We consider the problem of recovering a low-rank tensor from its noisy
observation. Previous work has shown a recovery guarantee with signal to noise
ratio $O(n^{\lceil K/2 \rceil /2})$ for recovering a $K$th order rank one
tensor of size $n\times \cdots \times n$ by recursive unfolding. In this paper,
we first improve this bound to $O(n^{K/4})$ by a much simpler approach, but
with a more careful analysis. Then we propose a new norm called the subspace
norm, which is based on the Kronecker products of factors obtained by the
proposed simple estimator. The imposed Kronecker structure allows us to show a
nearly ideal $O(\sqrt{n}+\sqrt{H^{K-1}})$ bound, in which the parameter $H$
controls the blend from the non-convex estimator to mode-wise nuclear norm
minimization. Furthermore, we empirically demonstrate that the subspace norm
achieves the nearly ideal denoising performance even with $H=O(1)$.","['Qinqing Zheng', 'Ryota Tomioka']","['cs.LG', 'cs.AI', 'stat.ML']",2015-03-18 16:45:04+00:00
http://arxiv.org/abs/1503.05471v1,Shared latent subspace modelling within Gaussian-Binary Restricted Boltzmann Machines for NIST i-Vector Challenge 2014,"This paper presents a novel approach to speaker subspace modelling based on
Gaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is
based on the idea of shared factors as in the Probabilistic Linear Discriminant
Analysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,
herein the speaker factor is shared over all vectors of the speaker. Then
Maximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.
Various new scoring techniques for speaker verification using GRBM are
proposed. The results for NIST i-vector Challenge 2014 dataset are presented.","['Danila Doroshin', 'Alexander Yamshinin', 'Nikolay Lubimov', 'Marina Nastasenko', 'Mikhail Kotov', 'Maxim Tkachenko']","['cs.LG', 'cs.NE', 'cs.SD', 'stat.ML', '62M45', 'I.2.6; I.5.1']",2015-03-18 16:28:18+00:00
http://arxiv.org/abs/1503.05160v1,Improved LASSO,"We propose an improved LASSO estimation technique based on Stein-rule. We
shrink classical LASSO estimator using preliminary test, shrinkage, and
positive-rule shrinkage principle. Simulation results have been carried out for
various configurations of correlation coefficients ($r$), size of the parameter
vector ($\beta$), error variance ($\sigma^2$) and number of non-zero
coefficients ($k$) in the model parameter vector. Several real data examples
have been used to demonstrate the practical usefulness of the proposed
estimators. Our study shows that the risk ordering given by LSE $>$ LASSO $>$
Stein-type LASSO $>$ Stein-type positive rule LASSO, remains the same uniformly
in the divergence parameter $\Delta^2$ as in the traditional case.","['A. K. Md. Ehsanes Saleh', 'Enayetur Raheem']","['math.ST', 'stat.AP', 'stat.ML', 'stat.TH']",2015-03-17 18:47:28+00:00
http://arxiv.org/abs/1503.05087v2,Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits,"We propose a sample-efficient alternative for importance weighting for
situations where one only has sample access to the probability distribution
that generates the observations. Our new method, called Geometric Resampling
(GR), is described and analyzed in the context of online combinatorial
optimization under semi-bandit feedback, where a learner sequentially selects
its actions from a combinatorial decision set so as to minimize its cumulative
loss. In particular, we show that the well-known Follow-the-Perturbed-Leader
(FPL) prediction method coupled with Geometric Resampling yields the first
computationally efficient reduction from offline to online optimization in this
setting. We provide a thorough theoretical analysis for the resulting
algorithm, showing that its performance is on par with previous, inefficient
solutions. Our main contribution is showing that, despite the relatively large
variance induced by the GR procedure, our performance guarantees hold with high
probability rather than only in expectation. As a side result, we also improve
the best known regret bounds for FPL in online combinatorial optimization with
full feedback, closing the perceived performance gap between FPL and
exponential weights in this setting.","['Gergely Neu', 'Gábor Bartók']","['cs.LG', 'stat.ML']",2015-03-17 15:26:15+00:00
http://arxiv.org/abs/1503.05459v1,Hypoelliptic Diffusion Maps I: Tangent Bundles,"We introduce the concept of Hypoelliptic Diffusion Maps (HDM), a framework
generalizing Diffusion Maps in the context of manifold learning and
dimensionality reduction. Standard non-linear dimensionality reduction methods
(e.g., LLE, ISOMAP, Laplacian Eigenmaps, Diffusion Maps) focus on mining
massive data sets using weighted affinity graphs; Orientable Diffusion Maps and
Vector Diffusion Maps enrich these graphs by attaching to each node also some
local geometry. HDM likewise considers a scenario where each node possesses
additional structure, which is now itself of interest to investigate.
Virtually, HDM augments the original data set with attached structures, and
provides tools for studying and organizing the augmented ensemble. The goal is
to obtain information on individual structures attached to the nodes and on the
relationship between structures attached to nearby nodes, so as to study the
underlying manifold from which the nodes are sampled. In this paper, we analyze
HDM on tangent bundles, revealing its intimate connection with sub-Riemannian
geometry and a family of hypoelliptic differential operators. In a later paper,
we shall consider more general fibre bundles.",['Tingran Gao'],"['math.ST', 'stat.ML', 'stat.TH', '58J65, 58A30, 62-07', 'I.2.6']",2015-03-17 03:36:50+00:00
http://arxiv.org/abs/1503.04585v3,Statistical Analysis of Loopy Belief Propagation in Random Fields,"Loopy belief propagation (LBP), which is equivalent to the Bethe
approximation in statistical mechanics, is a message-passing-type inference
method that is widely used to analyze systems based on Markov random fields
(MRFs). In this paper, we propose a message-passing-type method to analytically
evaluate the quenched average of LBP in random fields by using the replica
cluster variation method. The proposed analytical method is applicable to
general pair-wise MRFs with random fields whose distributions differ from each
other and can give the quenched averages of the Bethe free energies over random
fields, which are consistent with numerical results. The order of its
computational cost is equivalent to that of standard LBP. In the latter part of
this paper, we describe the application of the proposed method to Bayesian
image restoration, in which we observed that our theoretical results are in
good agreement with the numerical results for natural images.","['Muneki Yasuda', 'Shun Kataoka', 'Kazuyuki Tanaka']","['stat.ML', 'cond-mat.dis-nn', 'cs.CV']",2015-03-16 10:08:01+00:00
http://arxiv.org/abs/1503.04567v2,Learning Mixed Membership Community Models in Social Tagging Networks through Tensor Methods,"Community detection in graphs has been extensively studied both in theory and
in applications. However, detecting communities in hypergraphs is more
challenging. In this paper, we propose a tensor decomposition approach for
guaranteed learning of communities in a special class of hypergraphs modeling
social tagging systems or folksonomies. A folksonomy is a tripartite 3-uniform
hypergraph consisting of (user, tag, resource) hyperedges. We posit a
probabilistic mixed membership community model, and prove that the tensor
method consistently learns the communities under efficient sample complexity
and separation requirements.","['Anima Anandkumar', 'Hanie Sedghi']","['cs.LG', 'cs.SI', 'stat.ML']",2015-03-16 08:27:54+00:00
http://arxiv.org/abs/1503.04549v2,High-dimensional quadratic classifiers in non-sparse settings,"We consider high-dimensional quadratic classifiers in non-sparse settings.
The target of classification rules is not Bayes error rates in the context. The
classifier based on the Mahalanobis distance does not always give a preferable
performance even if the populations are normal distributions having known
covariance matrices. The quadratic classifiers proposed in this paper draw
information about heterogeneity effectively through both the differences of
expanding mean vectors and covariance matrices. We show that they hold a
consistency property in which misclassification rates tend to zero as the
dimension goes to infinity under non-sparse settings. We verify that they are
asymptotically distributed as a normal distribution under certain conditions.
We also propose a quadratic classifier after feature selection by using both
the differences of mean vectors and covariance matrices. Finally, we discuss
performances of the classifiers in actual data analyses. The proposed
classifiers achieve highly accurate classification with very low computational
costs.","['Makoto Aoshima', 'Kazuyoshi Yata']","['stat.ML', 'math.ST', 'stat.TH', 'Primary 62H30, secondary 62H10']",2015-03-16 07:35:22+00:00
http://arxiv.org/abs/1503.04474v2,Statistical Estimation and Clustering of Group-invariant Orientation Parameters,"We treat the problem of estimation of orientation parameters whose values are
invariant to transformations from a spherical symmetry group. Previous work has
shown that any such group-invariant distribution must satisfy a restricted
finite mixture representation, which allows the orientation parameter to be
estimated using an Expectation Maximization (EM) maximum likelihood (ML)
estimation algorithm. In this paper, we introduce two parametric models for
this spherical symmetry group estimation problem: 1) the hyperbolic Von Mises
Fisher (VMF) mixture distribution and 2) the Watson mixture distribution. We
also introduce a new EM-ML algorithm for clustering samples that come from
mixtures of group-invariant distributions with different parameters. We apply
the models to the problem of mean crystal orientation estimation under the
spherically symmetric group associated with the crystal form, e.g., cubic or
octahedral or hexahedral. Simulations and experiments establish the advantages
of the extended EM-VMF and EM-Watson estimators for data acquired by Electron
Backscatter Diffraction (EBSD) microscopy of a polycrystalline Nickel alloy
sample.","['Yu-Hui Chen', 'Dennis Wei', 'Gregory Newstadt', 'Marc DeGraef', 'Jeffrey Simmons', 'Alfred Hero']","['stat.ML', 'physics.data-an']",2015-03-15 20:46:40+00:00
http://arxiv.org/abs/1503.04337v3,Communication-efficient sparse regression: a one-shot approach,"We devise a one-shot approach to distributed sparse regression in the
high-dimensional setting. The key idea is to average ""debiased"" or
""desparsified"" lasso estimators. We show the approach converges at the same
rate as the lasso as long as the dataset is not split across too many machines.
We also extend the approach to generalized linear models.","['Jason D. Lee', 'Yuekai Sun', 'Qiang Liu', 'Jonathan E. Taylor']","['stat.ML', 'cs.LG']",2015-03-14 19:43:30+00:00
http://arxiv.org/abs/1503.03964v1,Interactive Restless Multi-armed Bandit Game and Swarm Intelligence Effect,"We obtain the conditions for the emergence of the swarm intelligence effect
in an interactive game of restless multi-armed bandit (rMAB). A player competes
with multiple agents. Each bandit has a payoff that changes with a probability
$p_{c}$ per round. The agents and player choose one of three options: (1)
Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among
$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good
bandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:
(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability
for Observe in learning. The parameters $(c,p_{obs})$ are uniformly
distributed. We determine the optimal strategies for the player using complete
knowledge about the rMAB. We show whether or not social or asocial learning is
more optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence
effect. We conduct a laboratory experiment (67 subjects) and observe the swarm
intelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning
is far more optimal than asocial learning.","['Shunsuke Yoshida', 'Masato Hisakado', 'Shintaro Mori']","['cs.AI', 'cs.LG', 'physics.data-an', 'stat.ML']",2015-03-13 06:53:01+00:00
http://arxiv.org/abs/1503.03903v1,Approximating Sparse PCA from Incomplete Data,"We study how well one can recover sparse principal components of a data
matrix using a sketch formed from a few of its elements. We show that for a
wide class of optimization problems, if the sketch is close (in the spectral
norm) to the original data matrix, then one can recover a near optimal solution
to the optimization problem by using the sketch. In particular, we use this
approach to obtain sparse principal components and show that for \math{m} data
points in \math{n} dimensions, \math{O(\epsilon^{-2}\tilde k\max\{m,n\})}
elements gives an \math{\epsilon}-additive approximation to the sparse PCA
problem (\math{\tilde k} is the stable rank of the data matrix). We demonstrate
our algorithms extensively on image, text, biological and financial data. The
results show that not only are we able to recover the sparse PCAs from the
incomplete data, but by using our sparse sketch, the running time drops by a
factor of five or more.","['Abhisek Kundu', 'Petros Drineas', 'Malik Magdon-Ismail']","['cs.LG', 'cs.IT', 'cs.NA', 'math.IT', 'stat.ML']",2015-03-12 22:16:55+00:00
http://arxiv.org/abs/1503.03893v1,Compact Nonlinear Maps and Circulant Extensions,"Kernel approximation via nonlinear random feature maps is widely used in
speeding up kernel machines. There are two main challenges for the conventional
kernel approximation methods. First, before performing kernel approximation, a
good kernel has to be chosen. Picking a good kernel is a very challenging
problem in itself. Second, high-dimensional maps are often required in order to
achieve good performance. This leads to high computational cost in both
generating the nonlinear maps, and in the subsequent learning and prediction
process. In this work, we propose to optimize the nonlinear maps directly with
respect to the classification objective in a data-dependent fashion. The
proposed approach achieves kernel approximation and kernel learning in a joint
framework. This leads to much more compact maps without hurting the
performance. As a by-product, the same framework can also be used to achieve
more compact kernel maps to approximate a known kernel. We also introduce
Circulant Nonlinear Maps, which uses a circulant-structured projection matrix
to speed up the nonlinear maps for high-dimensional data.","['Felix X. Yu', 'Sanjiv Kumar', 'Henry Rowley', 'Shih-Fu Chang']","['stat.ML', 'cs.LG']",2015-03-12 21:19:13+00:00
http://arxiv.org/abs/1503.03879v1,Qualitative inequalities for squared partial correlations of a Gaussian random vector,"We describe various sets of conditional independence relationships,
sufficient for qualitatively comparing non-vanishing squared partial
correlations of a Gaussian random vector. These sufficient conditions are
satisfied by several graphical Markov models. Rules for comparing degree of
association among the vertices of such Gaussian graphical models are also
developed. We apply these rules to compare conditional dependencies on Gaussian
trees. In particular for trees, we show that such dependence can be completely
characterized by the length of the paths joining the dependent vertices to each
other and to the vertices conditioned on. We also apply our results to
postulate rules for model selection for polytree models. Our rules apply to
mutual information of Gaussian random vectors as well.",['Sanjay Chaudhuri'],"['math.ST', 'stat.AP', 'stat.CO', 'stat.ME', 'stat.ML', 'stat.TH']",2015-03-12 20:15:35+00:00
http://arxiv.org/abs/1503.03701v4,Hierarchical learning of grids of microtopics,"The counting grid is a grid of microtopics, sparse word/feature
distributions. The generative model associated with the grid does not use these
microtopics individually. Rather, it groups them in overlapping rectangular
windows and uses these grouped microtopics as either mixture or admixture
components. This paper builds upon the basic counting grid model and it shows
that hierarchical reasoning helps avoid bad local minima, produces better
classification accuracy and, most interestingly, allows for extraction of large
numbers of coherent microtopics even from small datasets. We evaluate this in
terms of consistency, diversity and clarity of the indexed content, as well as
in a user study on word intrusion tasks. We demonstrate that these models work
well as a technique for embedding raw images and discuss interesting parallels
between hierarchical CG models and other deep architectures.","['Nebojsa Jojic', 'Alessandro Perina', 'Dongwoo Kim']","['stat.ML', 'cs.IR', 'cs.LG']",2015-03-12 12:59:25+00:00
http://arxiv.org/abs/1503.03673v1,Functional Inverse Regression in an Enlarged Dimension Reduction Space,"We consider an enlarged dimension reduction space in functional inverse
regression. Our operator and functional analysis based approach facilitates a
compact and rigorous formulation of the functional inverse regression problem.
It also enables us to expand the possible space where the dimension reduction
functions belong. Our formulation provides a unified framework so that the
classical notions, such as covariance standardization, Mahalanobis distance,
SIR and linear discriminant analysis, can be naturally and smoothly carried out
in our enlarged space. This enlarged dimension reduction space also links to
the linear discriminant space of Gaussian measures on a separable Hilbert
space.","['Ting-Li Chen', 'Su-Yun Huang', 'Yanyuan Ma', 'I-Ping Tu']","['math.ST', 'stat.ML', 'stat.TH']",2015-03-12 11:14:38+00:00
http://arxiv.org/abs/1503.03613v1,On the Impossibility of Learning the Missing Mass,"This paper shows that one cannot learn the probability of rare events without
imposing further structural assumptions. The event of interest is that of
obtaining an outcome outside the coverage of an i.i.d. sample from a discrete
distribution. The probability of this event is referred to as the ""missing
mass"". The impossibility result can then be stated as: the missing mass is not
distribution-free PAC-learnable in relative error. The proof is
semi-constructive and relies on a coupling argument using a dithered geometric
distribution. This result formalizes the folklore that in order to predict rare
events, one necessarily needs distributions with ""heavy tails"".","['Elchanan Mossel', 'Mesrob I. Ohannessian']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.PR', 'math.ST', 'stat.TH', '62G05, 62G20, 62G32 (Primary), 62C20, 62E10, 62N99 (Secondary)', 'G.3; I.2.6; I.2.7; I.5']",2015-03-12 07:27:24+00:00
http://arxiv.org/abs/1503.03585v8,Deep Unsupervised Learning using Nonequilibrium Thermodynamics,"A central problem in machine learning involves modeling complex data-sets
using highly flexible families of probability distributions in which learning,
sampling, inference, and evaluation are still analytically or computationally
tractable. Here, we develop an approach that simultaneously achieves both
flexibility and tractability. The essential idea, inspired by non-equilibrium
statistical physics, is to systematically and slowly destroy structure in a
data distribution through an iterative forward diffusion process. We then learn
a reverse diffusion process that restores structure in data, yielding a highly
flexible and tractable generative model of the data. This approach allows us to
rapidly learn, sample from, and evaluate probabilities in deep generative
models with thousands of layers or time steps, as well as to compute
conditional and posterior probabilities under the learned model. We
additionally release an open source reference implementation of the algorithm.","['Jascha Sohl-Dickstein', 'Eric A. Weiss', 'Niru Maheswaranathan', 'Surya Ganguli']","['cs.LG', 'cond-mat.dis-nn', 'q-bio.NC', 'stat.ML']",2015-03-12 04:51:37+00:00
http://arxiv.org/abs/1503.03525v2,Online Matrix Completion and Online Robust PCA,"This work studies two interrelated problems - online robust PCA (RPCA) and
online low-rank matrix completion (MC). In recent work by Cand\`{e}s et al.,
RPCA has been defined as a problem of separating a low-rank matrix (true data),
$L:=[\ell_1, \ell_2, \dots \ell_{t}, \dots , \ell_{t_{\max}}]$ and a sparse
matrix (outliers), $S:=[x_1, x_2, \dots x_{t}, \dots, x_{t_{\max}}]$ from their
sum, $M:=L+S$. Our work uses this definition of RPCA. An important application
where both these problems occur is in video analytics in trying to separate
sparse foregrounds (e.g., moving objects) and slowly changing backgrounds.
  While there has been a large amount of recent work on both developing and
analyzing batch RPCA and batch MC algorithms, the online problem is largely
open. In this work, we develop a practical modification of our recently
proposed algorithm to solve both the online RPCA and online MC problems. The
main contribution of this work is that we obtain correctness results for the
proposed algorithms under mild assumptions. The assumptions that we need are:
(a) a good estimate of the initial subspace is available (easy to obtain using
a short sequence of background-only frames in video surveillance); (b) the
$\ell_t$'s obey a `slow subspace change' assumption; (c) the basis vectors for
the subspace from which $\ell_t$ is generated are dense (non-sparse); (d) the
support of $x_t$ changes by at least a certain amount at least every so often;
and (e) algorithm parameters are appropriately set","['Brian Lois', 'Namrata Vaswani']","['cs.IT', 'math.IT', 'stat.ML']",2015-03-11 22:20:16+00:00
http://arxiv.org/abs/1503.03524v1,Describing and Understanding Neighborhood Characteristics through Online Social Media,"Geotagged data can be used to describe regions in the world and discover
local themes. However, not all data produced within a region is necessarily
specifically descriptive of that area. To surface the content that is
characteristic for a region, we present the geographical hierarchy model (GHM),
a probabilistic model based on the assumption that data observed in a region is
a random mixture of content that pertains to different levels of a hierarchy.
We apply the GHM to a dataset of 8 million Flickr photos in order to
discriminate between content (i.e., tags) that specifically characterizes a
region (e.g., neighborhood) and content that characterizes surrounding areas or
more general themes. Knowledge of the discriminative and non-discriminative
terms used throughout the hierarchy enables us to quantify the uniqueness of a
given region and to compare similar but distant regions. Our evaluation
demonstrates that our model improves upon traditional Naive Bayes
classification by 47% and hierarchical TF-IDF by 27%. We further highlight the
differences and commonalities with human reasoning about what is locally
characteristic for a neighborhood, distilled from ten interviews and a survey
that covered themes such as time, events, and prior regional knowledge","['Mohamed Kafsi', 'Henriette Cramer', 'Bart Thomee', 'David A. Shamma']","['stat.ML', 'cs.SI']",2015-03-11 22:13:38+00:00
http://arxiv.org/abs/1503.03517v1,Switching to Learn,"A network of agents attempt to learn some unknown state of the world drawn by
nature from a finite set. Agents observe private signals conditioned on the
true state, and form beliefs about the unknown state accordingly. Each agent
may face an identification problem in the sense that she cannot distinguish the
truth in isolation. However, by communicating with each other, agents are able
to benefit from side observations to learn the truth collectively. Unlike many
distributed algorithms which rely on all-time communication protocols, we
propose an efficient method by switching between Bayesian and non-Bayesian
regimes. In this model, agents exchange information only when their private
signals are not informative enough; thence, by switching between the two
regimes, agents efficiently learn the truth using only a few rounds of
communications. The proposed algorithm preserves learnability while incurring a
lower communication cost. We also verify our theoretical findings by simulation
examples.","['Shahin Shahrampour', 'Mohammad Amin Rahimian', 'Ali Jadbabaie']","['cs.LG', 'math.OC', 'stat.ML']",2015-03-11 22:05:50+00:00
http://arxiv.org/abs/1503.03467v5,Multigrid with rough coefficients and Multiresolution operator decomposition from Hierarchical Information Games,"We introduce a near-linear complexity (geometric and meshless/algebraic)
multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficients
with rigorous a-priori accuracy and performance estimates. The method is
discovered through a decision/game theory formulation of the problems of (1)
identifying restriction and interpolation operators (2) recovering a signal
from incomplete measurements based on norm constraints on its image under a
linear operator (3) gambling on the value of the solution of the PDE based on a
hierarchy of nested measurements of its solution or source term. The resulting
elementary gambles form a hierarchy of (deterministic) basis functions of
$H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands
with respect to the scalar product induced by the energy norm of the PDE (2)
enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) induce
an orthogonal multiresolution operator decomposition. The operating diagram of
the multigrid method is that of an inverted pyramid in which gamblets are
computed locally (by virtue of their exponential decay), hierarchically (from
fine to coarse scales) and the PDE is decomposed into a hierarchy of
independent linear systems with uniformly bounded condition numbers. The
resulting algorithm is parallelizable both in space (via localization) and in
bandwith/subscale (subscales can be computed independently from each other).
Although the method is deterministic it has a natural Bayesian interpretation
under the measure of probability emerging (as a mixed strategy) from the
information game formulation and multiresolution approximations form a
martingale with respect to the filtration induced by the hierarchy of nested
measurements.",['Houman Owhadi'],"['math.NA', 'cs.AI', 'math.ST', 'stat.ML', 'stat.TH', '68T99, 65N55, 65F99, 65N75, 62C99, 42C40, 60G42, 68Q25']",2015-03-11 19:52:13+00:00
http://arxiv.org/abs/1503.03438v3,A mathematical motivation for complex-valued convolutional networks,"A complex-valued convolutional network (convnet) implements the repeated
application of the following composition of three operations, recursively
applying the composition to an input vector of nonnegative real numbers: (1)
convolution with complex-valued vectors followed by (2) taking the absolute
value of every entry of the resulting vectors followed by (3) local averaging.
For processing real-valued random vectors, complex-valued convnets can be
viewed as ""data-driven multiscale windowed power spectra,"" ""data-driven
multiscale windowed absolute spectra,"" ""data-driven multiwavelet absolute
values,"" or (in their most general configuration) ""data-driven nonlinear
multiwavelet packets."" Indeed, complex-valued convnets can calculate multiscale
windowed spectra when the convnet filters are windowed complex-valued
exponentials. Standard real-valued convnets, using rectified linear units
(ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max.
pooling, etc., do not obviously exhibit the same exact correspondence with
data-driven wavelets (whereas for complex-valued convnets, the correspondence
is much more than just a vague analogy). Courtesy of the exact correspondence,
the remarkably rich and rigorous body of mathematical analysis for wavelets
applies directly to (complex-valued) convnets.","['Joan Bruna', 'Soumith Chintala', 'Yann LeCun', 'Serkan Piantino', 'Arthur Szlam', 'Mark Tygert']","['cs.LG', 'cs.NE', 'stat.ML']",2015-03-11 18:24:13+00:00
http://arxiv.org/abs/1503.03355v1,Automatic Unsupervised Tensor Mining with Quality Assessment,"A popular tool for unsupervised modelling and mining multi-aspect data is
tensor decomposition. In an exploratory setting, where and no labels or ground
truth are available how can we automatically decide how many components to
extract? How can we assess the quality of our results, so that a domain expert
can factor this quality measure in the interpretation of our results? In this
paper, we introduce AutoTen, a novel automatic unsupervised tensor mining
algorithm with minimal user intervention, which leverages and improves upon
heuristics that assess the result quality. We extensively evaluate AutoTen's
performance on synthetic data, outperforming existing baselines on this very
hard problem. Finally, we apply AutoTen on a variety of real datasets,
providing insights and discoveries. We view this work as a step towards a fully
automated, unsupervised tensor mining tool that can be easily adopted by
practitioners in academia and industry.",['Evangelos E. Papalexakis'],"['stat.ML', 'cs.LG', 'cs.NA', 'stat.AP']",2015-03-11 14:34:46+00:00
http://arxiv.org/abs/1503.03231v1,Adaptive-Rate Sparse Signal Reconstruction With Application in Compressive Background Subtraction,"We propose and analyze an online algorithm for reconstructing a sequence of
signals from a limited number of linear measurements. The signals are assumed
sparse, with unknown support, and evolve over time according to a generic
nonlinear dynamical model. Our algorithm, based on recent theoretical results
for $\ell_1$-$\ell_1$ minimization, is recursive and computes the number of
measurements to be taken at each time on-the-fly. As an example, we apply the
algorithm to compressive video background subtraction, a problem that can be
stated as follows: given a set of measurements of a sequence of images with a
static background, simultaneously reconstruct each image while separating its
foreground from the background. The performance of our method is illustrated on
sequences of real images: we observe that it allows a dramatic reduction in the
number of measurements with respect to state-of-the-art compressive background
subtraction schemes.","['Joao F. C. Mota', 'Nikos Deligiannis', 'Aswin C. Sankaranarayanan', 'Volkan Cevher', 'Miguel R. D. Rodrigues']","['math.OC', 'cs.CV', 'cs.IT', 'math.IT', 'stat.ML']",2015-03-11 09:16:39+00:00
http://arxiv.org/abs/1503.03188v2,Optimal prediction for sparse linear models? Lower bounds for coordinate-separable M-estimators,"For the problem of high-dimensional sparse linear regression, it is known
that an $\ell_0$-based estimator can achieve a $1/n$ ""fast"" rate on the
prediction error without any conditions on the design matrix, whereas in
absence of restrictive conditions on the design matrix, popular polynomial-time
methods only guarantee the $1/\sqrt{n}$ ""slow"" rate. In this paper, we show
that the slow rate is intrinsic to a broad class of M-estimators. In
particular, for estimators based on minimizing a least-squares cost function
together with a (possibly non-convex) coordinate-wise separable regularizer,
there is always a ""bad"" local optimum such that the associated prediction error
is lower bounded by a constant multiple of $1/\sqrt{n}$. For convex
regularizers, this lower bound applies to all global optima. The theory is
applicable to many popular estimators, including convex $\ell_1$-based methods
as well as M-estimators based on nonconvex regularizers, including the SCAD
penalty or the MCP regularizer. In addition, for a broad class of nonconvex
regularizers, we show that the bad local optima are very common, in that a
broad class of local minimization algorithms with random initialization will
typically converge to a bad solution.","['Yuchen Zhang', 'Martin J. Wainwright', 'Michael I. Jordan']","['math.ST', 'stat.ML', 'stat.TH']",2015-03-11 06:22:44+00:00
http://arxiv.org/abs/1503.03148v1,A Neurodynamical System for finding a Minimal VC Dimension Classifier,"The recently proposed Minimal Complexity Machine (MCM) finds a hyperplane
classifier by minimizing an exact bound on the Vapnik-Chervonenkis (VC)
dimension. The VC dimension measures the capacity of a learning machine, and a
smaller VC dimension leads to improved generalization. On many benchmark
datasets, the MCM generalizes better than SVMs and uses far fewer support
vectors than the number used by SVMs. In this paper, we describe a neural
network based on a linear dynamical system, that converges to the MCM solution.
The proposed MCM dynamical system is conducive to an analogue circuit
implementation on a chip or simulation using Ordinary Differential Equation
(ODE) solvers. Numerical experiments on benchmark datasets from the UCI
repository show that the proposed approach is scalable and accurate, as we
obtain improved accuracies and fewer number of support vectors (upto 74.3%
reduction) with the MCM dynamical system.","['Jayadeva', 'Sumit Soman', 'Amit Bhaya']","['cs.LG', 'stat.ML', '70G660, 68T05', 'I.5.1; I.5.5; G.1.7; I.2.6']",2015-03-11 02:10:26+00:00
http://arxiv.org/abs/1503.03132v1,L_1-regularized Boltzmann machine learning using majorizer minimization,"We propose an inference method to estimate sparse interactions and biases
according to Boltzmann machine learning. The basis of this method is $L_1$
regularization, which is often used in compressed sensing, a technique for
reconstructing sparse input signals from undersampled outputs. $L_1$
regularization impedes the simple application of the gradient method, which
optimizes the cost function that leads to accurate estimations, owing to the
cost function's lack of smoothness. In this study, we utilize the majorizer
minimization method, which is a well-known technique implemented in
optimization problems, to avoid the non-smoothness of the cost function. By
using the majorizer minimization method, we elucidate essentially relevant
biases and interactions from given data with seemingly strongly-correlated
components.",['Masayuki Ohzeki'],"['stat.ML', 'cond-mat.dis-nn', 'cs.LG']",2015-03-11 00:21:51+00:00
http://arxiv.org/abs/1503.03082v2,Learning the Structure for Structured Sparsity,"Structured sparsity has recently emerged in statistics, machine learning and
signal processing as a promising paradigm for learning in high-dimensional
settings. All existing methods for learning under the assumption of structured
sparsity rely on prior knowledge on how to weight (or how to penalize)
individual subsets of variables during the subset selection process, which is
not available in general. Inferring group weights from data is a key open
research problem in structured sparsity.In this paper, we propose a Bayesian
approach to the problem of group weight learning. We model the group weights as
hyperparameters of heavy-tailed priors on groups of variables and derive an
approximate inference scheme to infer these hyperparameters. We empirically
show that we are able to recover the model hyperparameters when the data are
generated from the model, and we demonstrate the utility of learning weights in
synthetic and real denoising problems.","['Nino Shervashidze', 'Francis Bach']",['stat.ML'],2015-03-10 20:09:13+00:00
http://arxiv.org/abs/1503.02978v2,Kernel Meets Sieve: Post-Regularization Confidence Bands for Sparse Additive Model,"We develop a novel procedure for constructing confidence bands for components
of a sparse additive model. Our procedure is based on a new kernel-sieve hybrid
estimator that combines two most popular nonparametric estimation methods in
the literature, the kernel regression and the spline method, and is of interest
in its own right. Existing methods for fitting sparse additive model are
primarily based on sieve estimators, while the literature on confidence bands
for nonparametric models are primarily based upon kernel or local polynomial
estimators. Our kernel-sieve hybrid estimator combines the best of both worlds
and allows us to provide a simple procedure for constructing confidence bands
in high-dimensional sparse additive models. We prove that the confidence bands
are asymptotically honest by studying approximation with a Gaussian process.
Thorough numerical results on both synthetic data and real-world neuroscience
data are provided to demonstrate the efficacy of the theory.","['Junwei Lu', 'Mladen Kolar', 'Han Liu']","['stat.ML', 'math.ST', 'stat.TH']",2015-03-10 16:26:31+00:00
http://arxiv.org/abs/1503.02893v1,Robust recovery of complex exponential signals from random Gaussian projections via low rank Hankel matrix reconstruction,"This paper explores robust recovery of a superposition of $R$ distinct
complex exponential functions from a few random Gaussian projections. We assume
that the signal of interest is of $2N-1$ dimensional and $R<<2N-1$. This
framework covers a large class of signals arising from real applications in
biology, automation, imaging science, etc. To reconstruct such a signal, our
algorithm is to seek a low-rank Hankel matrix of the signal by minimizing its
nuclear norm subject to the consistency on the sampled data. Our theoretical
results show that a robust recovery is possible as long as the number of
projections exceeds $O(R\ln^2N)$. No incoherence or separation condition is
required in our proof. Our method can be applied to spectral compressed sensing
where the signal of interest is a superposition of $R$ complex sinusoids.
Compared to existing results, our result here does not need any separation
condition on the frequencies, while achieving better or comparable bounds on
the number of measurements. Furthermore, our method provides theoretical
guidance on how many samples are required in the state-of-the-art non-uniform
sampling in NMR spectroscopy. The performance of our algorithm is further
demonstrated by numerical experiments.","['Jian-Feng Cai', 'Xiaobo Qu', 'Weiyu Xu', 'Gui-Bo Ye']","['cs.IT', 'math.IT', 'math.NA', 'math.OC', 'stat.ML']",2015-03-10 13:16:45+00:00
http://arxiv.org/abs/1503.02817v1,Minimax Optimal Rates of Estimation in High Dimensional Additive Models: Universal Phase Transition,"We establish minimax optimal rates of convergence for estimation in a high
dimensional additive model assuming that it is approximately sparse. Our
results reveal an interesting phase transition behavior universal to this class
of high dimensional problems. In the {\it sparse regime} when the components
are sufficiently smooth or the dimensionality is sufficiently large, the
optimal rates are identical to those for high dimensional linear regression,
and therefore there is no additional cost to entertain a nonparametric model.
Otherwise, in the so-called {\it smooth regime}, the rates coincide with the
optimal rates for estimating a univariate function, and therefore they are
immune to the ""curse of dimensionality"".","['Ming Yuan', 'Ding-Xuan Zhou']","['math.ST', 'cs.IT', 'math.IT', 'stat.ML', 'stat.TH']",2015-03-10 08:56:34+00:00
http://arxiv.org/abs/1503.02768v2,Novel Bernstein-like Concentration Inequalities for the Missing Mass,"We are concerned with obtaining novel concentration inequalities for the
missing mass, i.e. the total probability mass of the outcomes not observed in
the sample. We not only derive - for the first time - distribution-free
Bernstein-like deviation bounds with sublinear exponents in deviation size for
missing mass, but also improve the results of McAllester and Ortiz (2003)
andBerend and Kontorovich (2013, 2012) for small deviations which is the most
interesting case in learning theory. It is known that the majority of standard
inequalities cannot be directly used to analyze heterogeneous sums i.e. sums
whose terms have large difference in magnitude. Our generic and intuitive
approach shows that the heterogeneity issue introduced in McAllester and Ortiz
(2003) is resolvable at least in the case of missing mass via regulating the
terms using our novel thresholding technique.","['Bahman Yari Saeed Khanloo', 'Gholamreza Haffari']",['stat.ML'],2015-03-10 04:38:46+00:00
http://arxiv.org/abs/1503.02761v2,An Adaptive Online HDP-HMM for Segmentation and Classification of Sequential Data,"In the recent years, the desire and need to understand sequential data has
been increasing, with particular interest in sequential contexts such as
patient monitoring, understanding daily activities, video surveillance, stock
market and the like. Along with the constant flow of data, it is critical to
classify and segment the observations on-the-fly, without being limited to a
rigid number of classes. In addition, the model needs to be capable of updating
its parameters to comply with possible evolutions. This interesting problem,
however, is not adequately addressed in the literature since many studies focus
on offline classification over a pre-defined class set. In this paper, we
propose a principled solution to this gap by introducing an adaptive online
system based on Markov switching models with hierarchical Dirichlet process
priors. This infinite adaptive online approach is capable of segmenting and
classifying the sequential data over unlimited number of classes, while meeting
the memory and delay constraints of streaming contexts. The model is further
enhanced by introducing a learning rate, responsible for balancing the extent
to which the model sustains its previous learning (parameters) or adapts to the
new streaming observations. Experimental results on several variants of
stationary and evolving synthetic data and two video datasets, TUM Assistive
Kitchen and collatedWeizmann, show remarkable performance in segmentation and
classification, particularly for evolutionary sequences with changing
distributions and/or containing new, unseen classes.","['Ava Bargi', 'Richard Yi Da Xu', 'Massimo Piccardi']","['stat.ML', 'cs.LG']",2015-03-10 03:27:34+00:00
http://arxiv.org/abs/1503.02698v2,Graphical Exponential Screening,"In high dimensions we propose and analyze an aggregation estimator of the
precision matrix for Gaussian graphical models. This estimator, called
graphical Exponential Screening (gES), linearly combines a suitable set of
individual estimators with different underlying graphs, and balances the
estimation error and sparsity. We study the risk of this aggregation estimator
and show that it is comparable to that of the best estimator based on a single
graph, chosen by an oracle. Numerical performance of our method is investigated
using both simulated and real datasets, in comparison with some state-of-art
estimation procedures.",['Zhe Liu'],['stat.ML'],2015-03-09 21:10:48+00:00
http://arxiv.org/abs/1503.02596v3,A Characterization of Deterministic Sampling Patterns for Low-Rank Matrix Completion,"Low-rank matrix completion (LRMC) problems arise in a wide variety of
applications. Previous theory mainly provides conditions for completion under
missing-at-random samplings. This paper studies deterministic conditions for
completion. An incomplete $d \times N$ matrix is finitely rank-$r$ completable
if there are at most finitely many rank-$r$ matrices that agree with all its
observed entries. Finite completability is the tipping point in LRMC, as a few
additional samples of a finitely completable matrix guarantee its unique
completability. The main contribution of this paper is a deterministic sampling
condition for finite completability. We use this to also derive deterministic
sampling conditions for unique completability that can be efficiently verified.
We also show that under uniform random sampling schemes, these conditions are
satisfied with high probability if $O(\max\{r,\log d\})$ entries per column are
observed. These findings have several implications on LRMC regarding lower
bounds, sample and computational complexity, the role of coherence, adaptive
settings and the validation of any completion algorithm. We complement our
theoretical results with experiments that support our findings and motivate
future analysis of uncharted sampling regimes.","['Daniel L. Pimentel-Alarcón', 'Nigel Boston', 'Robert D. Nowak']","['stat.ML', 'cs.LG', 'math.AG']",2015-03-09 18:12:58+00:00
http://arxiv.org/abs/1503.02551v2,Kernel-Based Just-In-Time Learning for Passing Expectation Propagation Messages,"We propose an efficient nonparametric strategy for learning a message
operator in expectation propagation (EP), which takes as input the set of
incoming messages to a factor node, and produces an outgoing message as output.
This learned operator replaces the multivariate integral required in classical
EP, which may not have an analytic expression. We use kernel-based regression,
which is trained on a set of probability distributions representing the
incoming messages, and the associated outgoing messages. The kernel approach
has two main advantages: first, it is fast, as it is implemented using a novel
two-layer random feature representation of the input message distributions;
second, it has principled uncertainty estimates, and can be cheaply updated
online, meaning it can request and incorporate new training data when it
encounters inputs on which it is uncertain. In experiments, our approach is
able to solve learning problems where a single message operator is required for
multiple, substantially different data sets (logistic regression for a variety
of classification problems), where it is essential to accurately assess
uncertainty and to efficiently and robustly update the message operator.","['Wittawat Jitkrittum', 'Arthur Gretton', 'Nicolas Heess', 'S. M. Ali Eslami', 'Balaji Lakshminarayanan', 'Dino Sejdinovic', 'Zoltán Szabó']","['stat.ML', 'cs.LG', '62F15, 46e22, 62-09, 62F30', 'G.3; I.2.6']",2015-03-09 16:30:17+00:00
http://arxiv.org/abs/1503.02533v2,A Smoothed Dual Approach for Variational Wasserstein Problems,"Variational problems that involve Wasserstein distances have been recently
proposed to summarize and learn from probability measures. Despite being
conceptually simple, such problems are computationally challenging because they
involve minimizing over quantities (Wasserstein distances) that are themselves
hard to compute. We show that the dual formulation of Wasserstein variational
problems introduced recently by Carlier et al. (2014) can be regularized using
an entropic smoothing, which leads to smooth, differentiable, convex
optimization problems that are simpler to implement and numerically more
stable. We illustrate the versatility of this approach by applying it to the
computation of Wasserstein barycenters and gradient flows of spacial
regularization functionals.","['Marco Cuturi', 'Gabriel Peyré']","['stat.ML', 'math.OC']",2015-03-09 15:49:21+00:00
http://arxiv.org/abs/1503.02531v1,Distilling the Knowledge in a Neural Network,"A very simple way to improve the performance of almost any machine learning
algorithm is to train many different models on the same data and then to
average their predictions. Unfortunately, making predictions using a whole
ensemble of models is cumbersome and may be too computationally expensive to
allow deployment to a large number of users, especially if the individual
models are large neural nets. Caruana and his collaborators have shown that it
is possible to compress the knowledge in an ensemble into a single model which
is much easier to deploy and we develop this approach further using a different
compression technique. We achieve some surprising results on MNIST and we show
that we can significantly improve the acoustic model of a heavily used
commercial system by distilling the knowledge in an ensemble of models into a
single model. We also introduce a new type of ensemble composed of one or more
full models and many specialist models which learn to distinguish fine-grained
classes that the full models confuse. Unlike a mixture of experts, these
specialist models can be trained rapidly and in parallel.","['Geoffrey Hinton', 'Oriol Vinyals', 'Jeff Dean']","['stat.ML', 'cs.LG', 'cs.NE']",2015-03-09 15:44:49+00:00
http://arxiv.org/abs/1503.02424v2,Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs,"Standard sparse pseudo-input approximations to the Gaussian process (GP)
cannot handle complex functions well. Sparse spectrum alternatives attempt to
answer this but are known to over-fit. We suggest the use of variational
inference for the sparse spectrum approximation to avoid both issues. We model
the covariance function with a finite Fourier series approximation and treat it
as a random variable. The random covariance function has a posterior, on which
a variational distribution is placed. The variational distribution transforms
the random covariance function to fit the data. We study the properties of our
approximate inference, compare it to alternative ones, and extend it to the
distributed and stochastic domains. Our approximation captures complex
functions better than standard approaches and avoids over-fitting.","['Yarin Gal', 'Richard Turner']",['stat.ML'],2015-03-09 11:04:58+00:00
http://arxiv.org/abs/1503.02398v5,Learning Co-Sparse Analysis Operators with Separable Structures,"In the co-sparse analysis model a set of filters is applied to a signal out
of the signal class of interest yielding sparse filter responses. As such, it
may serve as a prior in inverse problems, or for structural analysis of signals
that are known to belong to the signal class. The more the model is adapted to
the class, the more reliable it is for these purposes. The task of learning
such operators for a given class is therefore a crucial problem. In many
applications, it is also required that the filter responses are obtained in a
timely manner, which can be achieved by filters with a separable structure. Not
only can operators of this sort be efficiently used for computing the filter
responses, but they also have the advantage that less training samples are
required to obtain a reliable estimate of the operator. The first contribution
of this work is to give theoretical evidence for this claim by providing an
upper bound for the sample complexity of the learning process. The second is a
stochastic gradient descent (SGD) method designed to learn an analysis operator
with separable structures, which includes a novel and efficient step size
selection rule. Numerical experiments are provided that link the sample
complexity to the convergence speed of the SGD algorithm.","['Matthias Seibert', 'Julian Wörmann', 'Rémi Gribonval', 'Martin Kleinsteuber']","['cs.LG', 'stat.ML']",2015-03-09 08:53:33+00:00
http://arxiv.org/abs/1503.02356v1,Mathematical understanding of detailed balance condition violation and its application to Langevin dynamics,"We develop an efficient sampling method by simulating Langevin dynamics with
an artificial force rather than a natural force by using the gradient of the
potential energy. The standard technique for sampling following the
predetermined distribution such as the Gibbs-Boltzmann one is performed under
the detailed balance condition. In the present study, we propose a modified
Langevin dynamics violating the detailed balance condition on the
transition-probability formulation. We confirm that the numerical
implementation of the proposed method actually demonstrates two major
beneficial improvements: acceleration of the relaxation to the predetermined
distribution and reduction of the correlation time between two different
realizations in the steady state.","['M. Ohzeki', 'A. Ichiki']","['cond-mat.stat-mech', 'stat.ML']",2015-03-09 01:46:06+00:00
