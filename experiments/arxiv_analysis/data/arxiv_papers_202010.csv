id,title,abstract,authors,categories,date
http://arxiv.org/abs/2012.00168v2,"A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series","Irregularly sampled time series data arise naturally in many application
domains including biology, ecology, climate science, astronomy, and health.
Such data represent fundamental challenges to many classical models from
machine learning and statistics due to the presence of non-uniform intervals
between observations. However, there has been significant progress within the
machine learning community over the last decade on developing specialized
models and architectures for learning from irregularly sampled univariate and
multivariate time series data. In this survey, we first describe several axes
along which approaches to learning from irregularly sampled time series differ
including what data representations they are based on, what modeling primitives
they leverage to deal with the fundamental problem of irregular sampling, and
what inference tasks they are designed to perform. We then survey the recent
literature organized primarily along the axis of modeling primitives. We
describe approaches based on temporal discretization, interpolation,
recurrence, attention and structural invariance. We discuss similarities and
differences between approaches and highlight primary strengths and weaknesses.","['Satya Narayan Shukla', 'Benjamin M. Marlin']","['cs.LG', 'stat.ML']",2020-11-30 23:41:47+00:00
http://arxiv.org/abs/2012.00152v1,Every Model Learned by Gradient Descent Is Approximately a Kernel Machine,"Deep learning's successes are often attributed to its ability to
automatically discover new representations of the data, rather than relying on
handcrafted features like other learning methods. We show, however, that deep
networks learned by the standard gradient descent algorithm are in fact
mathematically approximately equivalent to kernel machines, a learning method
that simply memorizes the data and uses it directly for prediction via a
similarity function (the kernel). This greatly enhances the interpretability of
deep network weights, by elucidating that they are effectively a superposition
of the training examples. The network architecture incorporates knowledge of
the target function into the kernel. This improved understanding should lead to
better learning algorithms.",['Pedro Domingos'],"['cs.LG', 'cs.NE', 'stat.ML', 'I.2.6; I.5.1']",2020-11-30 23:02:47+00:00
http://arxiv.org/abs/2012.00123v2,A Hypergradient Approach to Robust Regression without Correspondence,"We consider a variant of regression problem, where the correspondence between
input and output data is not available. Such shuffled data is commonly observed
in many real world problems. Taking flow cytometry as an example, the measuring
instruments may not be able to maintain the correspondence between the samples
and the measurements. Due to the combinatorial nature of the problem, most
existing methods are only applicable when the sample size is small, and limited
to linear regression models. To overcome such bottlenecks, we propose a new
computational framework -- ROBOT -- for the shuffled regression problem, which
is applicable to large data and complex nonlinear models. Specifically, we
reformulate the regression without correspondence as a continuous optimization
problem. Then by exploiting the interaction between the regression model and
the data correspondence, we develop a hypergradient approach based on
differentiable programming techniques. Such a hypergradient approach
essentially views the data correspondence as an operator of the regression, and
therefore allows us to find a better descent direction for the model parameter
by differentiating through the data correspondence. ROBOT can be further
extended to the inexact correspondence setting, where there may not be an exact
alignment between the input and output data. Thorough numerical experiments
show that ROBOT achieves better performance than existing methods in both
linear and nonlinear regression tasks, including real-world applications such
as flow cytometry and multi-object tracking.","['Yujia Xie', 'Yixiu Mao', 'Simiao Zuo', 'Hongteng Xu', 'Xiaojing Ye', 'Tuo Zhao', 'Hongyuan Zha']","['cs.LG', 'stat.ML']",2020-11-30 21:47:38+00:00
http://arxiv.org/abs/2012.00113v6,The FEDHC Bayesian network learning algorithm,"The paper proposes a new hybrid Bayesian network learning algorithm, termed
Forward Early Dropping Hill Climbing (FEDHC), devised to work with either
continuous or categorical variables. Further, the paper manifests that the only
implementation of MMHC in the statistical software \textit{R}, is prohibitively
expensive and a new implementation is offered. Further, specifically for the
case of continuous data, a robust to outliers version of FEDHC, that can be
adopted by other BN learning algorithms, is proposed. The FEDHC is tested via
Monte Carlo simulations that distinctly show it is computationally efficient,
and produces Bayesian networks of similar to, or of higher accuracy than MMHC
and PCHC. Finally, an application of FEDHC, PCHC and MMHC algorithms to real
data, from the field of economics, is demonstrated using the statistical
software \textit{R}.",['Michail Tsagris'],"['stat.ML', 'cs.LG', '62H22']",2020-11-30 21:36:25+00:00
http://arxiv.org/abs/2012.00110v1,Representing and Denoising Wearable ECG Recordings,"Modern wearable devices are embedded with a range of noninvasive biomarker
sensors that hold promise for improving detection and treatment of disease. One
such sensor is the single-lead electrocardiogram (ECG) which measures
electrical signals in the heart. The benefits of the sheer volume of ECG
measurements with rich longitudinal structure made possible by wearables come
at the price of potentially noisier measurements compared to clinical ECGs,
e.g., due to movement. In this work, we develop a statistical model to simulate
a structured noise process in ECGs derived from a wearable sensor, design a
beat-to-beat representation that is conducive for analyzing variation, and
devise a factor analysis-based method to denoise the ECG. We study synthetic
data generated using a realistic ECG simulator and a structured noise model. At
varying levels of signal-to-noise, we quantitatively measure an upper bound on
performance and compare estimates from linear and non-linear models. Finally,
we apply our method to a set of ECGs collected by wearables in a mobile health
study.","['Jeffrey Chan', 'Andrew C. Miller', 'Emily B. Fox']","['stat.ML', 'cs.LG', 'stat.AP']",2020-11-30 21:33:11+00:00
http://arxiv.org/abs/2012.00091v1,Contagion Dynamics for Manifold Learning,"Contagion maps exploit activation times in threshold contagions to assign
vectors in high-dimensional Euclidean space to the nodes of a network. A point
cloud that is the image of a contagion map reflects both the structure
underlying the network and the spreading behaviour of the contagion on it.
Intuitively, such a point cloud exhibits features of the network's underlying
structure if the contagion spreads along that structure, an observation which
suggests contagion maps as a viable manifold-learning technique. We test
contagion maps as a manifold-learning tool on a number of different real-world
and synthetic data sets, and we compare their performance to that of Isomap,
one of the most well-known manifold-learning algorithms. We find that, under
certain conditions, contagion maps are able to reliably detect underlying
manifold structure in noisy data, while Isomap fails due to noise-induced
error. This consolidates contagion maps as a technique for manifold learning.",['Barbara I. Mahler'],"['stat.ML', 'cs.LG', 'math.AT', '57Z25 (Primary) 55N31 (Secondary)']",2020-11-30 20:58:21+00:00
http://arxiv.org/abs/2012.00745v5,Double machine learning for sample selection models,"This paper considers the evaluation of discretely distributed treatments when
outcomes are only observed for a subpopulation due to sample selection or
outcome attrition. For identification, we combine a selection-on-observables
assumption for treatment assignment with either selection-on-observables or
instrumental variable assumptions concerning the outcome attrition/sample
selection process. We also consider dynamic confounding, meaning that
covariates that jointly affect sample selection and the outcome may (at least
partly) be influenced by the treatment. To control in a data-driven way for a
potentially high dimensional set of pre- and/or post-treatment covariates, we
adapt the double machine learning framework for treatment evaluation to sample
selection problems. We make use of (a) Neyman-orthogonal, doubly robust, and
efficient score functions, which imply the robustness of treatment effect
estimation to moderate regularization biases in the machine learning-based
estimation of the outcome, treatment, or sample selection models and (b) sample
splitting (or cross-fitting) to prevent overfitting bias. We demonstrate that
the proposed estimators are asymptotically normal and root-n consistent under
specific regularity conditions concerning the machine learners and investigate
their finite sample properties in a simulation study. We also apply our
proposed methodology to the Job Corps data for evaluating the effect of
training on hourly wages which are only observed conditional on employment. The
estimator is available in the causalweight package for the statistical software
R.","['Michela Bia', 'Martin Huber', 'Lukáš Lafférs']","['econ.EM', 'stat.ME', 'stat.ML']",2020-11-30 19:40:21+00:00
http://arxiv.org/abs/2012.09785v1,Use of Bayesian Nonparametric methods for Estimating the Measurements in High Clutter,"Robust tracking of a target in a clutter environment is an important and
challenging task. In recent years, the nearest neighbor methods and
probabilistic data association filters were proposed. However, the performance
of these methods diminishes as the number of measurements increases. In this
paper, we propose a robust generative approach to effectively model multiple
sensor measurements for tracking a moving target in an environment with high
clutter. We assume a time-dependent number of measurements that include sensor
observations with unknown origin, some of which may only contain clutter with
no additional information. We robustly and accurately estimate the trajectory
of the moving target in a high clutter environment with an unknown number of
clutters by employing Bayesian nonparametric modeling. In particular, we employ
a class of joint Bayesian nonparametric models to construct the joint prior
distribution of target and clutter measurements such that the conditional
distributions follow a Dirichlet process. The marginalized Dirichlet process
prior of the target measurements is then used in a Bayesian tracker to estimate
the dynamically-varying target state. We show through experiments that the
tracking performance and effectiveness of our proposed framework are increased
by suppressing high clutter measurements. In addition, we show that our
proposed method outperforms existing methods such as nearest neighbor and
probability data association filters.","['Bahman Moraffah', 'Christ Richmond', 'Raha Moraffah', 'Antonia Papandreou-Suppappola']","['cs.LG', 'eess.SP', 'stat.ML']",2020-11-30 18:32:34+00:00
http://arxiv.org/abs/2011.15091v4,Inductive Biases for Deep Learning of Higher-Level Cognition,"A fascinating hypothesis is that human and animal intelligence could be
explained by a few principles (rather than an encyclopedic list of heuristics).
If that hypothesis was correct, we could more easily both understand our own
intelligence and build intelligent machines. Just like in physics, the
principles themselves would not be sufficient to predict the behavior of
complex systems like brains, and substantial computation might be needed to
simulate human-like intelligence. This hypothesis would suggest that studying
the kind of inductive biases that humans and animals exploit could help both
clarify these principles and provide inspiration for AI research and
neuroscience theories. Deep learning already exploits several key inductive
biases, and this work considers a larger list, focusing on those which concern
mostly higher-level and sequential conscious processing. The objective of
clarifying these particular principles is that they could potentially help us
build AI systems benefiting from humans' abilities in terms of flexible
out-of-distribution and systematic generalization, which is currently an area
where a large gap exists between state-of-the-art machine learning and human
intelligence.","['Anirudh Goyal', 'Yoshua Bengio']","['cs.LG', 'cs.AI', 'stat.ML']",2020-11-30 18:29:25+00:00
http://arxiv.org/abs/2011.15056v2,General Invertible Transformations for Flow-based Generative Modeling,"In this paper, we present a new class of invertible transformations with an
application to flow-based generative models. We indicate that many well-known
invertible transformations in reversible logic and reversible neural networks
could be derived from our proposition. Next, we propose two new coupling layers
that are important building blocks of flow-based generative models. In the
experiments on digit data, we present how these new coupling layers could be
used in Integer Discrete Flows (IDF), and that they achieve better results than
standard coupling layers used in IDF and RealNVP.",['Jakub M. Tomczak'],"['cs.LG', 'stat.ML']",2020-11-30 17:54:43+00:00
http://arxiv.org/abs/2011.15045v3,Unsupervised Deep Video Denoising,"Deep convolutional neural networks (CNNs) for video denoising are typically
trained with supervision, assuming the availability of clean videos. However,
in many applications, such as microscopy, noiseless videos are not available.
To address this, we propose an Unsupervised Deep Video Denoiser (UDVD), a CNN
architecture designed to be trained exclusively with noisy data. The
performance of UDVD is comparable to the supervised state-of-the-art, even when
trained only on a single short noisy video. We demonstrate the promise of our
approach in real-world imaging applications by denoising raw video,
fluorescence-microscopy and electron-microscopy data. In contrast to many
current approaches to video denoising, UDVD does not require explicit motion
compensation. This is advantageous because motion compensation is
computationally expensive, and can be unreliable when the input data are noisy.
A gradient-based analysis reveals that UDVD automatically adapts to local
motion in the input noisy videos. Thus, the network learns to perform implicit
motion compensation, even though it is only trained for denoising.","['Dev Yashpal Sheth', 'Sreyas Mohan', 'Joshua L. Vincent', 'Ramon Manzorro', 'Peter A. Crozier', 'Mitesh M. Khapra', 'Eero P. Simoncelli', 'Carlos Fernandez-Granda']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2020-11-30 17:45:08+00:00
http://arxiv.org/abs/2011.15007v2,RealCause: Realistic Causal Inference Benchmarking,"There are many different causal effect estimators in causal inference.
However, it is unclear how to choose between these estimators because there is
no ground-truth for causal effects. A commonly used option is to simulate
synthetic data, where the ground-truth is known. However, the best causal
estimators on synthetic data are unlikely to be the best causal estimators on
real data. An ideal benchmark for causal estimators would both (a) yield
ground-truth values of the causal effects and (b) be representative of real
data. Using flexible generative models, we provide a benchmark that both yields
ground-truth and is realistic. Using this benchmark, we evaluate over 1500
different causal estimators and provide evidence that it is rational to choose
hyperparameters for causal estimators using predictive metrics.","['Brady Neal', 'Chin-Wei Huang', 'Sunand Raghupathi']","['cs.LG', 'cs.AI', 'stat.ML']",2020-11-30 17:12:18+00:00
http://arxiv.org/abs/2011.15001v1,Variance based sensitivity analysis for Monte Carlo and importance sampling reliability assessment with Gaussian processes,"Running a reliability analysis on engineering problems involving complex
numerical models can be computationally very expensive, requiring advanced
simulation methods to reduce the overall numerical cost. Gaussian process based
active learning methods for reliability analysis have emerged as a promising
way for reducing this computational cost. The learning phase of these methods
consists in building a Gaussian process surrogate model of the performance
function and using the uncertainty structure of the Gaussian process to enrich
iteratively this surrogate model. For that purpose a learning criterion has to
be defined. Then, the estimation of the probability of failure is typically
obtained by a classification of a population evaluated on the final surrogate
model. Hence, the estimator of the probability of failure holds two different
uncertainty sources related to the surrogate model approximation and to the
sampling based integration technique. In this paper, we propose a methodology
to quantify the sensitivity of the probability of failure estimator to both
uncertainty sources. This analysis also enables to control the whole error
associated to the failure probability estimate and thus provides an accuracy
criterion on the estimation. Thus, an active learning approach integrating this
analysis to reduce the main source of error and stopping when the global
variability is sufficiently low is introduced. The approach is proposed for
both a Monte Carlo based method as well as an importance sampling based method,
seeking to improve the estimation of rare event probabilities. Performance of
the proposed strategy is then assessed on several examples.","['Morgane Menz', 'Sylvain Dubreuil', 'Jérôme Morio', 'Christian Gogu', 'Nathalie Bartoli', 'Marie Chiron']","['stat.ML', 'cs.LG']",2020-11-30 17:06:28+00:00
http://arxiv.org/abs/2011.14965v2,A Deep Learning Approach for Predicting Spatiotemporal Dynamics From Sparsely Observed Data,"In this paper, we consider the problem of learning prediction models for
spatiotemporal physical processes driven by unknown partial differential
equations (PDEs). We propose a deep learning framework that learns the
underlying dynamics and predicts its evolution using sparsely distributed data
sites. Deep learning has shown promising results in modeling physical dynamics
in recent years. However, most of the existing deep learning methods for
modeling physical dynamics either focus on solving known PDEs or require data
in a dense grid when the governing PDEs are unknown. In contrast, our method
focuses on learning prediction models for unknown PDE-driven dynamics only from
sparsely observed data. The proposed method is spatial dimension-independent
and geometrically flexible. We demonstrate our method in the forecasting task
for the two-dimensional wave equation and the Burgers-Fisher equation in
multiple geometries with different boundary conditions, and the ten-dimensional
heat equation.","['Priyabrata Saha', 'Saibal Mukhopadhyay']","['stat.ML', 'cs.LG', 'cs.NA', 'math.AP', 'math.NA']",2020-11-30 16:38:00+00:00
http://arxiv.org/abs/2011.14923v1,Towards constraining warm dark matter with stellar streams through neural simulation-based inference,"A statistical analysis of the observed perturbations in the density of
stellar streams can in principle set stringent contraints on the mass function
of dark matter subhaloes, which in turn can be used to constrain the mass of
the dark matter particle. However, the likelihood of a stellar density with
respect to the stream and subhaloes parameters involves solving an intractable
inverse problem which rests on the integration of all possible forward
realisations implicitly defined by the simulation model. In order to infer the
subhalo abundance, previous analyses have relied on Approximate Bayesian
Computation (ABC) together with domain-motivated but handcrafted summary
statistics. Here, we introduce a likelihood-free Bayesian inference pipeline
based on Amortised Approximate Likelihood Ratios (AALR), which automatically
learns a mapping between the data and the simulator parameters and obviates the
need to handcraft a possibly insufficient summary statistic. We apply the
method to the simplified case where stellar streams are only perturbed by dark
matter subhaloes, thus neglecting baryonic substructures, and describe several
diagnostics that demonstrate the effectiveness of the new method and the
statistical quality of the learned estimator.","['Joeri Hermans', 'Nilanjan Banik', 'Christoph Weniger', 'Gianfranco Bertone', 'Gilles Louppe']","['astro-ph.GA', 'astro-ph.CO', 'astro-ph.IM', 'stat.ML']",2020-11-30 15:53:43+00:00
http://arxiv.org/abs/2012.01929v1,A Stochastic Path-Integrated Differential EstimatoR Expectation Maximization Algorithm,"The Expectation Maximization (EM) algorithm is of key importance for
inference in latent variable models including mixture of regressors and
experts, missing observations. This paper introduces a novel EM algorithm,
called \texttt{SPIDER-EM}, for inference from a training set of size $n$, $n
\gg 1$. At the core of our algorithm is an estimator of the full conditional
expectation in the {\sf E}-step, adapted from the stochastic path-integrated
differential estimator ({\tt SPIDER}) technique. We derive finite-time
complexity bounds for smooth non-convex likelihood: we show that for
convergence to an $\epsilon$-approximate stationary point, the complexity
scales as $K_{\operatorname{Opt}} (n,\epsilon )={\cal O}(\epsilon^{-1})$ and
$K_{\operatorname{CE}}( n,\epsilon ) = n+ \sqrt{n} {\cal O}(\epsilon^{-1} )$,
where $K_{\operatorname{Opt}}( n,\epsilon )$ and $K_{\operatorname{CE}}(n,
\epsilon )$ are respectively the number of {\sf M}-steps and the number of
per-sample conditional expectations evaluations. This improves over the
state-of-the-art algorithms. Numerical results support our findings.","['Gersende Fort', 'Eric Moulines', 'Hoi-To Wai']","['cs.LG', 'cs.AI', 'math.ST', 'stat.ML', 'stat.TH']",2020-11-30 15:49:31+00:00
http://arxiv.org/abs/2011.14721v4,Probabilistic Load Forecasting Based on Adaptive Online Learning,"Load forecasting is crucial for multiple energy management tasks such as
scheduling generation capacity, planning supply and demand, and minimizing
energy trade costs. Such relevance has increased even more in recent years due
to the integration of renewable energies, electric cars, and microgrids.
Conventional load forecasting techniques obtain single-value load forecasts by
exploiting consumption patterns of past load demand. However, such techniques
cannot assess intrinsic uncertainties in load demand, and cannot capture
dynamic changes in consumption patterns. To address these problems, this paper
presents a method for probabilistic load forecasting based on the adaptive
online learning of hidden Markov models. We propose learning and forecasting
techniques with theoretical guarantees, and experimentally assess their
performance in multiple scenarios. In particular, we develop adaptive online
learning techniques that update model parameters recursively, and sequential
prediction techniques that obtain probabilistic forecasts using the most recent
parameters. The performance of the method is evaluated using multiple datasets
corresponding with regions that have different sizes and display assorted
time-varying consumption patterns. The results show that the proposed method
can significantly improve the performance of existing techniques for a wide
range of scenarios.","['Verónica Álvarez', 'Santiago Mazuelas', 'José A. Lozano']","['cs.LG', 'stat.ML']",2020-11-30 12:02:26+00:00
http://arxiv.org/abs/2011.14654v2,Feature Space Singularity for Out-of-Distribution Detection,"Out-of-Distribution (OoD) detection is important for building safe artificial
intelligence systems. However, current OoD detection methods still cannot meet
the performance requirements for practical deployment. In this paper, we
propose a simple yet effective algorithm based on a novel observation: in a
trained neural network, OoD samples with bounded norms well concentrate in the
feature space. We call the center of OoD features the Feature Space Singularity
(FSS), and denote the distance of a sample feature to FSS as FSSD. Then, OoD
samples can be identified by taking a threshold on the FSSD. Our analysis of
the phenomenon reveals why our algorithm works. We demonstrate that our
algorithm achieves state-of-the-art performance on various OoD detection
benchmarks. Besides, FSSD also enjoys robustness to slight corruption in test
data and can be further enhanced by ensembling. These make FSSD a promising
algorithm to be employed in real world. We release our code at
\url{https://github.com/megvii-research/FSSD_OoD_Detection}.","['Haiwen Huang', 'Zhihan Li', 'Lulu Wang', 'Sishuo Chen', 'Bin Dong', 'Xinyu Zhou']","['stat.ML', 'cs.LG']",2020-11-30 09:47:20+00:00
http://arxiv.org/abs/2011.14620v1,RegFlow: Probabilistic Flow-based Regression for Future Prediction,"Predicting future states or actions of a given system remains a fundamental,
yet unsolved challenge of intelligence, especially in the scope of complex and
non-deterministic scenarios, such as modeling behavior of humans. Existing
approaches provide results under strong assumptions concerning unimodality of
future states, or, at best, assuming specific probability distributions that
often poorly fit to real-life conditions. In this work we introduce a robust
and flexible probabilistic framework that allows to model future predictions
with virtually no constrains regarding the modality or underlying probability
distribution. To achieve this goal, we leverage a hypernetwork architecture and
train a continuous normalizing flow model. The resulting method dubbed RegFlow
achieves state-of-the-art results on several benchmark datasets, outperforming
competing approaches by a significant margin.","['Maciej Zięba', 'Marcin Przewięźlikowski', 'Marek Śmieja', 'Jacek Tabor', 'Tomasz Trzcinski', 'Przemysław Spurek']","['cs.LG', 'cs.AI', 'stat.ML']",2020-11-30 08:45:37+00:00
http://arxiv.org/abs/2011.14580v2,Robust and Private Learning of Halfspaces,"In this work, we study the trade-off between differential privacy and
adversarial robustness under L2-perturbations in the context of learning
halfspaces. We prove nearly tight bounds on the sample complexity of robust
private learning of halfspaces for a large regime of parameters. A highlight of
our results is that robust and private learning is harder than robust or
private learning alone. We complement our theoretical analysis with
experimental results on the MNIST and USPS datasets, for a learning algorithm
that is both differentially private and adversarially robust.","['Badih Ghazi', 'Ravi Kumar', 'Pasin Manurangsi', 'Thao Nguyen']","['cs.LG', 'cs.CR', 'cs.DS', 'stat.ML']",2020-11-30 06:59:20+00:00
http://arxiv.org/abs/2011.14572v2,Gradient Sparsification Can Improve Performance of Differentially-Private Convex Machine Learning,"We use gradient sparsification to reduce the adverse effect of differential
privacy noise on performance of private machine learning models. To this aim,
we employ compressed sensing and additive Laplace noise to evaluate
differentially-private gradients. Noisy privacy-preserving gradients are used
to perform stochastic gradient descent for training machine learning models.
Sparsification, achieved by setting the smallest gradient entries to zero, can
reduce the convergence speed of the training algorithm. However, by
sparsification and compressed sensing, the dimension of communicated gradient
and the magnitude of additive noise can be reduced. The interplay between these
effects determines whether gradient sparsification improves the performance of
differentially-private machine learning models. We investigate this
analytically in the paper. We prove that, for small privacy budgets,
compression can improve performance of privacy-preserving machine learning
models. However, for large privacy budgets, compression does not necessarily
improve the performance. Intuitively, this is because the effect of
privacy-preserving noise is minimal in large privacy budget regime and thus
improvements from gradient sparsification cannot compensate for its slower
convergence.",['Farhad Farokhi'],"['cs.LG', 'cs.CR', 'math.OC', 'stat.ML']",2020-11-30 06:37:06+00:00
http://arxiv.org/abs/2011.14549v1,Persistent Reductions in Regularized Loss Minimization for Variable Selection,"In the context of regularized loss minimization with polyhedral gauges, we
show that for a broad class of loss functions (possibly non-smooth and
non-convex) and under a simple geometric condition on the input data it is
possible to efficiently identify a subset of features which are guaranteed to
have zero coefficients in all optimal solutions in all problems with loss
functions from said class, before any iterative optimization has been performed
for the original problem. This procedure is standalone, takes only the data as
input, and does not require any calls to the loss function. Therefore, we term
this procedure as a persistent reduction for the aforementioned class of
regularized loss minimization problems. This reduction can be efficiently
implemented via an extreme ray identification subroutine applied to a
polyhedral cone formed from the datapoints. We employ an existing
output-sensitive algorithm for extreme ray identification which makes our
guarantee and algorithm applicable in ultra-high dimensional problems.",['Amin Jalali'],"['stat.ML', 'cs.LG', 'math.OC']",2020-11-30 04:59:44+00:00
http://arxiv.org/abs/2011.14496v1,Blind signal decomposition of various word embeddings based on join and individual variance explained,"In recent years, natural language processing (NLP) has become one of the most
important areas with various applications in human's life. As the most
fundamental task, the field of word embedding still requires more attention and
research. Currently, existing works about word embedding are focusing on
proposing novel embedding algorithms and dimension reduction techniques on
well-trained word embeddings. In this paper, we propose to use a novel joint
signal separation method - JIVE to jointly decompose various trained word
embeddings into joint and individual components. Through this decomposition
framework, we can easily investigate the similarity and difference among
different word embeddings. We conducted extensive empirical study on word2vec,
FastText and GLoVE trained on different corpus and with different dimensions.
We compared the performance of different decomposed components based on
sentiment analysis on Twitter and Stanford sentiment treebank. We found that by
mapping different word embeddings into the joint component, sentiment
performance can be greatly improved for the original word embeddings with lower
performance. Moreover, we found that by concatenating different components
together, the same model can achieve better performance. These findings provide
great insights into the word embeddings and our work offer a new of generating
word embeddings by fusing.","['Yikai Wang', 'Weijian Li']","['cs.CL', 'cs.AI', 'stat.ML']",2020-11-30 01:36:29+00:00
http://arxiv.org/abs/2011.14495v2,Soft-Robust Algorithms for Batch Reinforcement Learning,"In reinforcement learning, robust policies for high-stakes decision-making
problems with limited data are usually computed by optimizing the percentile
criterion, which minimizes the probability of a catastrophic failure.
Unfortunately, such policies are typically overly conservative as the
percentile criterion is non-convex, difficult to optimize, and ignores the mean
performance. To overcome these shortcomings, we study the soft-robust
criterion, which uses risk measures to balance the mean and percentile
criterion better. In this paper, we establish the soft-robust criterion's
fundamental properties, show that it is NP-hard to optimize, and propose and
analyze two algorithms to approximately optimize it. Our theoretical analyses
and empirical evaluations demonstrate that our algorithms compute much less
conservative solutions than the existing approximate methods for optimizing the
percentile-criterion.","['Elita A. Lobo', 'Mohammad Ghavamzadeh', 'Marek Petrik']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2020-11-30 01:36:16+00:00
http://arxiv.org/abs/2011.14439v5,Scaling Down Deep Learning with MNIST-1D,"Although deep learning models have taken on commercial and political
relevance, key aspects of their training and operation remain poorly
understood. This has sparked interest in science of deep learning projects,
many of which require large amounts of time, money, and electricity. But how
much of this research really needs to occur at scale? In this paper, we
introduce MNIST-1D: a minimalist, procedurally generated, low-memory, and
low-compute alternative to classic deep learning benchmarks. Although the
dimensionality of MNIST-1D is only 40 and its default training set size only
4000, MNIST-1D can be used to study inductive biases of different deep
architectures, find lottery tickets, observe deep double descent, metalearn an
activation function, and demonstrate guillotine regularization in
self-supervised learning. All these experiments can be conducted on a GPU or
often even on a CPU within minutes, allowing for fast prototyping, educational
use cases, and cutting-edge research on a low budget.","['Sam Greydanus', 'Dmitry Kobak']","['cs.LG', 'cs.NE', 'stat.ML']",2020-11-29 20:08:37+00:00
http://arxiv.org/abs/2011.14420v2,Improving Neural Network with Uniform Sparse Connectivity,"Neural network forms the foundation of deep learning and numerous AI
applications. Classical neural networks are fully connected, expensive to train
and prone to overfitting. Sparse networks tend to have convoluted structure
search, suboptimal performance and limited usage. We proposed the novel uniform
sparse network (USN) with even and sparse connectivity within each layer. USN
has one striking property that its performance is independent of the
substantial topology variation and enormous model space, thus offers a
search-free solution to all above mentioned issues of neural networks. USN
consistently and substantially outperforms the state-of-the-art sparse network
models in prediction accuracy, speed and robustness. It even achieves higher
prediction accuracy than the fully connected network with only 0.55% parameters
and 1/4 computing time and resources. Importantly, USN is conceptually simple
as a natural generalization of fully connected network with multiple
improvements in accuracy, robustness and scalability. USN can replace the
latter in a range of applications, data types and deep learning architectures.
We have made USN open source at https://github.com/datapplab/sparsenet.",['Weijun Luo'],"['cs.LG', 'stat.ML']",2020-11-29 19:00:05+00:00
http://arxiv.org/abs/2011.14317v3,FROCC: Fast Random projection-based One-Class Classification,"We present Fast Random projection-based One-Class Classification (FROCC), an
extremely efficient method for one-class classification. Our method is based on
a simple idea of transforming the training data by projecting it onto a set of
random unit vectors that are chosen uniformly and independently from the unit
sphere, and bounding the regions based on separation of the data. FROCC can be
naturally extended with kernels. We theoretically prove that FROCC generalizes
well in the sense that it is stable and has low bias. FROCC achieves up to 3.1
percent points better ROC, with 1.2--67.8x speedup in training and test times
over a range of state-of-the-art benchmarks including the SVM and the deep
learning based models for the OCC task.","['Arindam Bhattacharya', 'Sumanth Varambally', 'Amitabha Bagchi', 'Srikanta Bedathur']","['cs.LG', 'stat.ML']",2020-11-29 08:56:59+00:00
http://arxiv.org/abs/2011.14301v1,Automated Prostate Cancer Diagnosis Based on Gleason Grading Using Convolutional Neural Network,"The Gleason grading system using histological images is the most powerful
diagnostic and prognostic predictor of prostate cancer. The current standard
inspection is evaluating Gleason H&E-stained histopathology images by
pathologists. However, it is complicated, time-consuming, and subject to
observers. Deep learning (DL) based-methods that automatically learn image
features and achieve higher generalization ability have attracted significant
attention. However, challenges remain especially using DL to train the whole
slide image (WSI), a predominant clinical source in the current diagnostic
setting, containing billions of pixels, morphological heterogeneity, and
artifacts. Hence, we proposed a convolutional neural network (CNN)-based
automatic classification method for accurate grading of PCa using whole slide
histopathology images. In this paper, a data augmentation method named
Patch-Based Image Reconstruction (PBIR) was proposed to reduce the high
resolution and increase the diversity of WSIs. In addition, a distribution
correction (DC) module was developed to enhance the adaption of pretrained
model to the target dataset by adjusting the data distribution. Besides, a
Quadratic Weighted Mean Square Error (QWMSE) function was presented to reduce
the misdiagnosis caused by equal Euclidean distances. Our experiments indicated
the combination of PBIR, DC, and QWMSE function was necessary for achieving
superior expert-level performance, leading to the best results (0.8885
quadratic-weighted kappa coefficient).","['Haotian Xie', 'Yong Zhang', 'Jun Wang', 'Jingjing Zhang', 'Yifan Ma', 'Zhaogang Yang']","['eess.IV', 'cs.CV', 'stat.ML']",2020-11-29 06:42:08+00:00
http://arxiv.org/abs/2011.14269v4,Generalization and Memorization: The Bias Potential Model,"Models for learning probability distributions such as generative models and
density estimators behave quite differently from models for learning functions.
One example is found in the memorization phenomenon, namely the ultimate
convergence to the empirical distribution, that occurs in generative
adversarial networks (GANs). For this reason, the issue of generalization is
more subtle than that for supervised learning. For the bias potential model, we
show that dimension-independent generalization accuracy is achievable if early
stopping is adopted, despite that in the long term, the model either memorizes
the samples or diverges.","['Hongkang Yang', 'Weinan E']","['stat.ML', 'cs.LG', '68T07, 60-08']",2020-11-29 04:04:54+00:00
http://arxiv.org/abs/2011.14267v1,Minimax Sample Complexity for Turn-based Stochastic Game,"The empirical success of Multi-agent reinforcement learning is encouraging,
while few theoretical guarantees have been revealed. In this work, we prove
that the plug-in solver approach, probably the most natural reinforcement
learning algorithm, achieves minimax sample complexity for turn-based
stochastic game (TBSG). Specifically, we plan in an empirical TBSG by utilizing
a `simulator' that allows sampling from arbitrary state-action pair. We show
that the empirical Nash equilibrium strategy is an approximate Nash equilibrium
strategy in the true TBSG and give both problem-dependent and
problem-independent bound. We develop absorbing TBSG and reward perturbation
techniques to tackle the complex statistical dependence. The key idea is
artificially introducing a suboptimality gap in TBSG and then the Nash
equilibrium strategy lies in a finite set.","['Qiwen Cui', 'Lin F. Yang']","['cs.LG', 'cs.GT', 'stat.ML']",2020-11-29 03:58:45+00:00
http://arxiv.org/abs/2011.14238v4,Approximate Cross-validated Mean Estimates for Bayesian Hierarchical Regression Models,"We introduce a novel procedure for obtaining cross-validated predictive
estimates for Bayesian hierarchical regression models (BHRMs). Bayesian
hierarchical models are popular for their ability to model complex dependence
structures and provide probabilistic uncertainty estimates, but can be
computationally expensive to run. Cross-validation (CV) is therefore not a
common practice to evaluate the predictive performance of BHRMs. Our method
circumvents the need to re-run computationally costly estimation methods for
each cross-validation fold and makes CV more feasible for large BHRMs. By
conditioning on the variance-covariance parameters, we shift the CV problem
from probability-based sampling to a simple and familiar optimization problem.
In many cases, this produces estimates which are equivalent to full CV. We
provide theoretical results and demonstrate its efficacy on publicly available
data and in simulations.","['Amy X. Zhang', 'Le Bao', 'Changcheng Li', 'Michael J. Daniels']","['stat.ML', 'cs.LG', 'stat.CO']",2020-11-29 00:00:20+00:00
http://arxiv.org/abs/2011.14211v1,Curvature Regularization to Prevent Distortion in Graph Embedding,"Recent research on graph embedding has achieved success in various
applications. Most graph embedding methods preserve the proximity in a graph
into a manifold in an embedding space. We argue an important but neglected
problem about this proximity-preserving strategy: Graph topology patterns,
while preserved well into an embedding manifold by preserving proximity, may
distort in the ambient embedding Euclidean space, and hence to detect them
becomes difficult for machine learning models. To address the problem, we
propose curvature regularization, to enforce flatness for embedding manifolds,
thereby preventing the distortion. We present a novel angle-based sectional
curvature, termed ABS curvature, and accordingly three kinds of curvature
regularization to induce flat embedding manifolds during graph embedding. We
integrate curvature regularization into five popular proximity-preserving
embedding methods, and empirical results in two applications show significant
improvements on a wide range of open graph datasets.","['Hongbin Pei', 'Bingzhe Wei', 'Kevin Chen-Chuan Chang', 'Chunxu Zhang', 'Bo Yang']","['cs.LG', 'cs.CV', 'stat.ML']",2020-11-28 20:16:24+00:00
http://arxiv.org/abs/2011.14204v1,Class-agnostic Object Detection,"Object detection models perform well at localizing and classifying objects
that they are shown during training. However, due to the difficulty and cost
associated with creating and annotating detection datasets, trained models
detect a limited number of object types with unknown objects treated as
background content. This hinders the adoption of conventional detectors in
real-world applications like large-scale object matching, visual grounding,
visual relation prediction, obstacle detection (where it is more important to
determine the presence and location of objects than to find specific types),
etc. We propose class-agnostic object detection as a new problem that focuses
on detecting objects irrespective of their object-classes. Specifically, the
goal is to predict bounding boxes for all objects in an image but not their
object-classes. The predicted boxes can then be consumed by another system to
perform application-specific classification, retrieval, etc. We propose
training and evaluation protocols for benchmarking class-agnostic detectors to
advance future research in this domain. Finally, we propose (1) baseline
methods and (2) a new adversarial learning framework for class-agnostic
detection that forces the model to exclude class-specific information from
features used for predictions. Experimental results show that adversarial
learning improves class-agnostic detection efficacy.","['Ayush Jaiswal', 'Yue Wu', 'Pradeep Natarajan', 'Premkumar Natarajan']","['cs.CV', 'cs.LG', 'stat.ML']",2020-11-28 19:22:38+00:00
http://arxiv.org/abs/2011.14185v2,Optimal and Safe Estimation for High-Dimensional Semi-Supervised Learning,"We consider the estimation problem in high-dimensional semi-supervised
learning. Our goal is to investigate when and how the unlabeled data can be
exploited to improve the estimation of the regression parameters of linear
model in light of the fact that such linear models may be misspecified in data
analysis. We first establish the minimax lower bound for parameter estimation
in the semi-supervised setting, and show that this lower bound cannot be
achieved by supervised estimators using the labeled data only. We propose an
optimal semi-supervised estimator that can attain this lower bound and
therefore improves the supervised estimators, provided that the conditional
mean function can be consistently estimated with a proper rate. We further
propose a safe semi-supervised estimator. We view it safe, because this
estimator is always at least as good as the supervised estimators. We also
extend our idea to the aggregation of multiple semi-supervised estimators
caused by different misspecifications of the conditional mean function.
Extensive numerical simulations and a real data analysis are conducted to
illustrate our theoretical results.","['Siyi Deng', 'Yang Ning', 'Jiwei Zhao', 'Heping Zhang']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2020-11-28 18:26:46+00:00
http://arxiv.org/abs/2011.14145v2,A Backward SDE Method for Uncertainty Quantification in Deep Learning,"We develop a probabilistic machine learning method, which formulates a class
of stochastic neural networks by a stochastic optimal control problem. An
efficient stochastic gradient descent algorithm is introduced under the
stochastic maximum principle framework. Numerical experiments for applications
of stochastic neural networks are carried out to validate the effectiveness of
our methodology.","['Richard Archibald', 'Feng Bao', 'Yanzhao Cao', 'He Zhang']","['cs.LG', 'math.OC', 'stat.ML']",2020-11-28 15:19:36+00:00
http://arxiv.org/abs/2011.14126v5,Risk-Monotonicity in Statistical Learning,"Acquisition of data is a difficult task in many applications of machine
learning, and it is only natural that one hopes and expects the population risk
to decrease (better performance) monotonically with increasing data points. It
turns out, somewhat surprisingly, that this is not the case even for the most
standard algorithms that minimize the empirical risk. Non-monotonic behavior of
the risk and instability in training have manifested and appeared in the
popular deep learning paradigm under the description of double descent. These
problems highlight the current lack of understanding of learning algorithms and
generalization. It is, therefore, crucial to pursue this concern and provide a
characterization of such behavior. In this paper, we derive the first
consistent and risk-monotonic (in high probability) algorithms for a general
statistical learning setting under weak assumptions, consequently answering
some questions posed by Viering et al. 2019 on how to avoid non-monotonic
behavior of risk curves. We further show that risk monotonicity need not
necessarily come at the price of worse excess risk rates. To achieve this, we
derive new empirical Bernstein-like concentration inequalities of independent
interest that hold for certain non-i.i.d.~processes such as Martingale
Difference Sequences.",['Zakaria Mhammedi'],"['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2020-11-28 12:52:12+00:00
http://arxiv.org/abs/2011.14066v1,On Generalization of Adaptive Methods for Over-parameterized Linear Regression,"Over-parameterization and adaptive methods have played a crucial role in the
success of deep learning in the last decade. The widespread use of
over-parameterization has forced us to rethink generalization by bringing forth
new phenomena, such as implicit regularization of optimization algorithms and
double descent with training progression. A series of recent works have started
to shed light on these areas in the quest to understand -- why do neural
networks generalize well? The setting of over-parameterized linear regression
has provided key insights into understanding this mysterious behavior of neural
networks.
  In this paper, we aim to characterize the performance of adaptive methods in
the over-parameterized linear regression setting. First, we focus on two
sub-classes of adaptive methods depending on their generalization performance.
For the first class of adaptive methods, the parameter vector remains in the
span of the data and converges to the minimum norm solution like gradient
descent (GD). On the other hand, for the second class of adaptive methods, the
gradient rotation caused by the pre-conditioner matrix results in an in-span
component of the parameter vector that converges to the minimum norm solution
and the out-of-span component that saturates. Our experiments on
over-parameterized linear regression and deep neural networks support this
theory.","['Vatsal Shah', 'Soumya Basu', 'Anastasios Kyrillidis', 'Sujay Sanghavi']","['stat.ML', 'cs.LG']",2020-11-28 04:19:32+00:00
http://arxiv.org/abs/2011.14048v2,Is Support Set Diversity Necessary for Meta-Learning?,"Meta-learning is a popular framework for learning with limited data in which
an algorithm is produced by training over multiple few-shot learning tasks. For
classification problems, these tasks are typically constructed by sampling a
small number of support and query examples from a subset of the classes. While
conventional wisdom is that task diversity should improve the performance of
meta-learning, in this work we find evidence to the contrary: we propose a
modification to traditional meta-learning approaches in which we keep the
support sets fixed across tasks, thus reducing task diversity. Surprisingly, we
find that not only does this modification not result in adverse effects, it
almost always improves the performance for a variety of datasets and
meta-learning methods. We also provide several initial analyses to understand
this phenomenon. Our work serves to: (i) more closely investigate the effect of
support set construction for the problem of meta-learning, and (ii) suggest a
simple, general, and competitive baseline for few-shot learning.","['Amrith Setlur', 'Oscar Li', 'Virginia Smith']","['cs.LG', 'stat.ML']",2020-11-28 02:28:42+00:00
http://arxiv.org/abs/2011.14047v2,Learning from Incomplete Features by Simultaneous Training of Neural Networks and Sparse Coding,"In this paper, the problem of training a classifier on a dataset with
incomplete features is addressed. We assume that different subsets of features
(random or structured) are available at each data instance. This situation
typically occurs in the applications when not all the features are collected
for every data sample. A new supervised learning method is developed to train a
general classifier, such as a logistic regression or a deep neural network,
using only a subset of features per sample, while assuming sparse
representations of data vectors on an unknown dictionary. Sufficient conditions
are identified, such that, if it is possible to train a classifier on
incomplete observations so that their reconstructions are well separated by a
hyperplane, then the same classifier also correctly separates the original
(unobserved) data samples. Extensive simulation results on synthetic and
well-known datasets are presented that validate our theoretical findings and
demonstrate the effectiveness of the proposed method compared to traditional
data imputation approaches and one state-of-the-art algorithm.","['Cesar F. Caiafa', 'Ziyao Wang', 'Jordi Solé-Casals', 'Qibin Zhao']","['cs.LG', 'stat.ML']",2020-11-28 02:20:39+00:00
http://arxiv.org/abs/2011.14033v7,A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit,"In this paper, we consider the contextual variant of the MNL-Bandit problem.
More specifically, we consider a dynamic set optimization problem, where a
decision-maker offers a subset (assortment) of products to a consumer and
observes the response in every round. Consumers purchase products to maximize
their utility. We assume that a set of attributes describe the products, and
the mean utility of a product is linear in the values of these attributes. We
model consumer choice behavior using the widely used Multinomial Logit (MNL)
model and consider the decision maker problem of dynamically learning the model
parameters while optimizing cumulative revenue over the selling horizon $T$.
Though this problem has attracted considerable attention in recent times, many
existing methods often involve solving an intractable non-convex optimization
problem. Their theoretical performance guarantees depend on a problem-dependent
parameter which could be prohibitively large. In particular, existing
algorithms for this problem have regret bounded by $O(\sqrt{\kappa d T})$,
where $\kappa$ is a problem-dependent constant that can have an exponential
dependency on the number of attributes. In this paper, we propose an optimistic
algorithm and show that the regret is bounded by $O(\sqrt{dT} + \kappa)$,
significantly improving the performance over existing methods. Further, we
propose a convex relaxation of the optimization step, which allows for
tractable decision-making while retaining the favourable regret guarantee.","['Priyank Agrawal', 'Theja Tulabandhula', 'Vashist Avadhanula']","['cs.LG', 'stat.ML']",2020-11-28 00:20:36+00:00
http://arxiv.org/abs/2011.14032v1,Predicting cardiovascular risk from national administrative databases using a combined survival analysis and deep learning approach,"AIMS. This study compared the performance of deep learning extensions of
survival analysis models with traditional Cox proportional hazards (CPH) models
for deriving cardiovascular disease (CVD) risk prediction equations in national
health administrative datasets. METHODS. Using individual person linkage of
multiple administrative datasets, we constructed a cohort of all New Zealand
residents aged 30-74 years who interacted with publicly funded health services
during 2012, and identified hospitalisations and deaths from CVD over five
years of follow-up. After excluding people with prior CVD or heart failure,
sex-specific deep learning and CPH models were developed to estimate the risk
of fatal or non-fatal CVD events within five years. The proportion of explained
time-to-event occurrence, calibration, and discrimination were compared between
models across the whole study population and in specific risk groups. FINDINGS.
First CVD events occurred in 61,927 of 2,164,872 people. Among diagnoses and
procedures, the largest 'local' hazard ratios were associated by the deep
learning models with tobacco use in women (2.04, 95%CI: 1.99-2.10) and with
chronic obstructive pulmonary disease with acute lower respiratory infection in
men (1.56, 95%CI: 1.50-1.62). Other identified predictors (e.g. hypertension,
chest pain, diabetes) aligned with current knowledge about CVD risk predictors.
The deep learning models significantly outperformed the CPH models on the basis
of proportion of explained time-to-event occurrence (Royston and Sauerbrei's
R-squared: 0.468 vs. 0.425 in women and 0.383 vs. 0.348 in men), calibration,
and discrimination (all p<0.0001). INTERPRETATION. Deep learning extensions of
survival analysis models can be applied to large health administrative
databases to derive interpretable CVD risk prediction equations that are more
accurate than traditional CPH models.","['Sebastiano Barbieri', 'Suneela Mehta', 'Billy Wu', 'Chrianna Bharat', 'Katrina Poppe', 'Louisa Jorm', 'Rod Jackson']","['cs.LG', 'stat.ML']",2020-11-28 00:10:25+00:00
http://arxiv.org/abs/2011.14031v1,Voting based ensemble improves robustness of defensive models,"Developing robust models against adversarial perturbations has been an active
area of research and many algorithms have been proposed to train individual
robust models. Taking these pretrained robust models, we aim to study whether
it is possible to create an ensemble to further improve robustness. Several
previous attempts tackled this problem by ensembling the soft-label prediction
and have been proved vulnerable based on the latest attack methods. In this
paper, we show that if the robust training loss is diverse enough, a simple
hard-label based voting ensemble can boost the robust error over each
individual model. Furthermore, given a pool of robust models, we develop a
principled way to select which models to ensemble. Finally, to verify the
improved robustness, we conduct extensive experiments to study how to attack a
voting-based ensemble and develop several new white-box attacks. On CIFAR-10
dataset, by ensembling several state-of-the-art pre-trained defense models, our
method can achieve a 59.8% robust accuracy, outperforming all the existing
defensive models without using additional data.","['Devvrit', 'Minhao Cheng', 'Cho-Jui Hsieh', 'Inderjit Dhillon']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2020-11-28 00:08:45+00:00
http://arxiv.org/abs/2011.13967v1,Equivalence of Convergence Rates of Posterior Distributions and Bayes Estimators for Functions and Nonparametric Functionals,"We study the posterior contraction rates of a Bayesian method with Gaussian
process priors in nonparametric regression and its plug-in property for
differential operators. For a general class of kernels, we establish
convergence rates of the posterior measure of the regression function and its
derivatives, which are both minimax optimal up to a logarithmic factor for
functions in certain classes. Our calculation shows that the rate-optimal
estimation of the regression function and its derivatives share the same choice
of hyperparameter, indicating that the Bayes procedure remarkably adapts to the
order of derivatives and enjoys a generalized plug-in property that extends
real-valued functionals to function-valued functionals. This leads to a
practically simple method for estimating the regression function and its
derivatives, whose finite sample performance is assessed using simulations.
  Our proof shows that, under certain conditions, to any convergence rate of
Bayes estimators there corresponds the same convergence rate of the posterior
distributions (i.e., posterior contraction rate), and vice versa. This
equivalence holds for a general class of Gaussian processes and covers the
regression function and its derivative functionals, under both the $L_2$ and
$L_{\infty}$ norms. In addition to connecting these two fundamental large
sample properties in Bayesian and non-Bayesian regimes, such equivalence
enables a new routine to establish posterior contraction rates by calculating
convergence rates of nonparametric point estimators.
  At the core of our argument is an operator-theoretic framework for kernel
ridge regression and equivalent kernel techniques. We derive a range of sharp
non-asymptotic bounds that are pivotal in establishing convergence rates of
nonparametric point estimators and the equivalence theory, which may be of
independent interest.","['Zejian Liu', 'Meng Li']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH', '62G20']",2020-11-27 19:11:56+00:00
http://arxiv.org/abs/2011.13897v2,Latent Skill Planning for Exploration and Transfer,"To quickly solve new tasks in complex environments, intelligent agents need
to build up reusable knowledge. For example, a learned world model captures
knowledge about the environment that applies to new tasks. Similarly, skills
capture general behaviors that can apply to new tasks. In this paper, we
investigate how these two approaches can be integrated into a single
reinforcement learning agent. Specifically, we leverage the idea of partial
amortization for fast adaptation at test time. For this, actions are produced
by a policy that is learned over time while the skills it conditions on are
chosen using online planning. We demonstrate the benefits of our design
decisions across a suite of challenging locomotion tasks and demonstrate
improved sample efficiency in single tasks as well as in transfer from one task
to another, as compared to competitive baselines. Videos are available at:
https://sites.google.com/view/latent-skill-planning/","['Kevin Xie', 'Homanga Bharadhwaj', 'Danijar Hafner', 'Animesh Garg', 'Florian Shkurti']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2020-11-27 18:40:03+00:00
http://arxiv.org/abs/2011.13885v1,Offline Learning from Demonstrations and Unlabeled Experience,"Behavior cloning (BC) is often practical for robot learning because it allows
a policy to be trained offline without rewards, by supervised learning on
expert demonstrations. However, BC does not effectively leverage what we will
refer to as unlabeled experience: data of mixed and unknown quality without
reward annotations. This unlabeled data can be generated by a variety of
sources such as human teleoperation, scripted policies and other agents on the
same robot. Towards data-driven offline robot learning that can use this
unlabeled experience, we introduce Offline Reinforced Imitation Learning
(ORIL). ORIL first learns a reward function by contrasting observations from
demonstrator and unlabeled trajectories, then annotates all data with the
learned reward, and finally trains an agent via offline reinforcement learning.
Across a diverse set of continuous control and simulated robotic manipulation
tasks, we show that ORIL consistently outperforms comparable BC agents by
effectively leveraging unlabeled experience.","['Konrad Zolna', 'Alexander Novikov', 'Ksenia Konyushkova', 'Caglar Gulcehre', 'Ziyu Wang', 'Yusuf Aytar', 'Misha Denil', 'Nando de Freitas', 'Scott Reed']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2020-11-27 18:20:04+00:00
http://arxiv.org/abs/2011.13863v1,Knowledge transfer across cell lines using Hybrid Gaussian Process models with entity embedding vectors,"To date, a large number of experiments are performed to develop a biochemical
process. The generated data is used only once, to take decisions for
development. Could we exploit data of already developed processes to make
predictions for a novel process, we could significantly reduce the number of
experiments needed. Processes for different products exhibit differences in
behaviour, typically only a subset behave similar. Therefore, effective
learning on multiple product spanning process data requires a sensible
representation of the product identity. We propose to represent the product
identity (a categorical feature) by embedding vectors that serve as input to a
Gaussian Process regression model. We demonstrate how the embedding vectors can
be learned from process data and show that they capture an interpretable notion
of product similarity. The improvement in performance is compared to
traditional one-hot encoding on a simulated cross product learning task. All in
all, the proposed method could render possible significant reductions in
wet-lab experiments.","['Clemens Hutter', 'Moritz von Stosch', 'Mariano Nicolas Cruz Bournazou', 'Alessandro Butté']","['q-bio.QM', 'cs.LG', 'stat.ML']",2020-11-27 17:38:15+00:00
http://arxiv.org/abs/2011.13831v1,Deep orthogonal linear networks are shallow,"We consider the problem of training a deep orthogonal linear network, which
consists of a product of orthogonal matrices, with no non-linearity in-between.
We show that training the weights with Riemannian gradient descent is
equivalent to training the whole factorization by gradient descent. This means
that there is no effect of overparametrization and implicit bias at all in this
setting: training such a deep, overparametrized, network is perfectly
equivalent to training a one-layer shallow network.",['Pierre Ablin'],"['stat.ML', 'cs.LG']",2020-11-27 16:57:19+00:00
http://arxiv.org/abs/2011.14894v1,Uncertainty-driven ensembles of deep architectures for multiclass classification. Application to COVID-19 diagnosis in chest X-ray images,"Respiratory diseases kill million of people each year. Diagnosis of these
pathologies is a manual, time-consuming process that has inter and
intra-observer variability, delaying diagnosis and treatment. The recent
COVID-19 pandemic has demonstrated the need of developing systems to automatize
the diagnosis of pneumonia, whilst Convolutional Neural Network (CNNs) have
proved to be an excellent option for the automatic classification of medical
images. However, given the need of providing a confidence classification in
this context it is crucial to quantify the reliability of the model's
predictions. In this work, we propose a multi-level ensemble classification
system based on a Bayesian Deep Learning approach in order to maximize
performance while quantifying the uncertainty of each classification decision.
This tool combines the information extracted from different architectures by
weighting their results according to the uncertainty of their predictions.
Performance of the Bayesian network is evaluated in a real scenario where
simultaneously differentiating between four different pathologies: control vs
bacterial pneumonia vs viral pneumonia vs COVID-19 pneumonia. A three-level
decision tree is employed to divide the 4-class classification into three
binary classifications, yielding an accuracy of 98.06% and overcoming the
results obtained by recent literature. The reduced preprocessing needed for
obtaining this high performance, in addition to the information provided about
the reliability of the predictions evidence the applicability of the system to
be used as an aid for clinicians.","['Juan E. Arco', 'A. Ortiz', 'J. Ramirez', 'F. J. Martinez-Murcia', 'Yu-Dong Zhang', 'Juan M. Gorriz']","['eess.IV', 'cs.CV', 'stat.ML']",2020-11-27 14:06:25+00:00
http://arxiv.org/abs/2011.13719v1,A Study on the Uncertainty of Convolutional Layers in Deep Neural Networks,"This paper shows a Min-Max property existing in the connection weights of the
convolutional layers in a neural network structure, i.e., the LeNet.
Specifically, the Min-Max property means that, during the back
propagation-based training for LeNet, the weights of the convolutional layers
will become far away from their centers of intervals, i.e., decreasing to their
minimum or increasing to their maximum. From the perspective of uncertainty, we
demonstrate that the Min-Max property corresponds to minimizing the fuzziness
of the model parameters through a simplified formulation of convolution. It is
experimentally confirmed that the model with the Min-Max property has a
stronger adversarial robustness, thus this property can be incorporated into
the design of loss function. This paper points out a changing tendency of
uncertainty in the convolutional layers of LeNet structure, and gives some
insights to the interpretability of convolution.","['Haojing Shen', 'Sihong Chen', 'Ran Wang']","['cs.LG', 'stat.ML']",2020-11-27 13:06:36+00:00
http://arxiv.org/abs/2011.13704v2,Direct Evolutionary Optimization of Variational Autoencoders With Binary Latents,"Discrete latent variables are considered important for real world data, which
has motivated research on Variational Autoencoders (VAEs) with discrete
latents. However, standard VAE training is not possible in this case, which has
motivated different strategies to manipulate discrete distributions in order to
train discrete VAEs similarly to conventional ones. Here we ask if it is also
possible to keep the discrete nature of the latents fully intact by applying a
direct discrete optimization for the encoding model. The approach is
consequently strongly diverting from standard VAE-training by sidestepping
sampling approximation, reparameterization trick and amortization. Discrete
optimization is realized in a variational setting using truncated posteriors in
conjunction with evolutionary algorithms. For VAEs with binary latents, we (A)
show how such a discrete variational method ties into gradient ascent for
network weights, and (B) how the decoder is used to select latent states for
training. Conventional amortized training is more efficient and applicable to
large neural networks. However, using smaller networks, we here find direct
discrete optimization to be efficiently scalable to hundreds of latents. More
importantly, we find the effectiveness of direct optimization to be highly
competitive in `zero-shot' learning. In contrast to large supervised networks,
the here investigated VAEs can, e.g., denoise a single image without previous
training on clean data and/or training on large image datasets. More generally,
the studied approach shows that training of VAEs is indeed possible without
sampling-based approximation and reparameterization, which may be interesting
for the analysis of VAE-training in general. For `zero-shot' settings a direct
optimization, furthermore, makes VAEs competitive where they have previously
been outperformed by non-generative approaches.","['Enrico Guiraud', 'Jakob Drefs', 'Jörg Lücke']","['stat.ML', 'cs.LG', '65C20, 68T07', 'G.3.0; I.2.6; I.4.0; I.5.1']",2020-11-27 12:42:12+00:00
http://arxiv.org/abs/2011.13694v2,Clustering with missing data: which equivalent for Rubin's rules?,"Multiple imputation (MI) is a popular method for dealing with missing values.
However, the suitable way for applying clustering after MI remains unclear: how
to pool partitions? How to assess the clustering instability when data are
incomplete? By answering both questions, this paper proposed a complete view of
clustering with missing data using MI. The problem of partitions pooling is
here addressed using consensus clustering while, based on the bootstrap theory,
we explain how to assess the instability related to observed and missing data.
The new rules for pooling partitions and instability assessment are
theoretically argued and extensively studied by simulation. Partitions pooling
improves accuracy, while measuring instability with missing data enlarges the
data analysis possibilities: it allows assessment of the dependence of the
clustering to the imputation model, as well as a convenient way for choosing
the number of clusters when data are incomplete, as illustrated on a real data
set.","['Vincent Audigier', 'Ndèye Niang']","['stat.ME', 'stat.ML', '62H30 62-08', 'G.3']",2020-11-27 12:09:31+00:00
http://arxiv.org/abs/2011.13680v2,"Riemannian Gaussian distributions, random matrix ensembles and diffusion kernels","We show that the Riemannian Gaussian distributions on symmetric spaces,
introduced in recent years, are of standard random matrix type. We exploit this
to compute analytically marginals of the probability density functions. This
can be done fully, using Stieltjes-Wigert orthogonal polynomials, for the case
of the space of Hermitian matrices, where the distributions have already
appeared in the physics literature. For the case when the symmetric space is
the space of $m \times m$ symmetric positive definite matrices, we show how to
efficiently compute by evaluating Pfaffians at specific values of $m$.
Equivalently, we can obtain the same result by constructing specific skew
orthogonal polynomials with regards to the log-normal weight function (skew
Stieltjes-Wigert polynomials). Other symmetric spaces are studied and the same
type of result is obtained for the quaternionic case. Moreover, we show how the
probability density functions are a particular case of diffusion reproducing
kernels of the Karlin-McGregor type, describing non-intersecting Brownian
motions, which are also diffusion processes in the Weyl chamber of Lie groups.","['Leonardo Santilli', 'Miguel Tierz']","['math-ph', 'math.MP', 'math.ST', 'stat.ML', 'stat.TH']",2020-11-27 11:41:29+00:00
http://arxiv.org/abs/2011.13600v1,Distributed Variational Bayesian Algorithms Over Sensor Networks,"Distributed inference/estimation in Bayesian framework in the context of
sensor networks has recently received much attention due to its broad
applicability. The variational Bayesian (VB) algorithm is a technique for
approximating intractable integrals arising in Bayesian inference. In this
paper, we propose two novel distributed VB algorithms for general Bayesian
inference problem, which can be applied to a very general class of
conjugate-exponential models. In the first approach, the global natural
parameters at each node are optimized using a stochastic natural gradient that
utilizes the Riemannian geometry of the approximation space, followed by an
information diffusion step for cooperation with the neighbors. In the second
method, a constrained optimization formulation for distributed estimation is
established in natural parameter space and solved by alternating direction
method of multipliers (ADMM). An application of the distributed
inference/estimation of a Bayesian Gaussian mixture model is then presented, to
evaluate the effectiveness of the proposed algorithms. Simulations on both
synthetic and real datasets demonstrate that the proposed algorithms have
excellent performance, which are almost as good as the corresponding
centralized VB algorithm relying on all data available in a fusion center.","['Junhao Hua', 'Chunguang Li']","['stat.ML', 'cs.AI']",2020-11-27 08:12:18+00:00
http://arxiv.org/abs/2012.07527v1,Regularizing Recurrent Neural Networks via Sequence Mixup,"In this paper, we extend a class of celebrated regularization techniques
originally proposed for feed-forward neural networks, namely Input Mixup (Zhang
et al., 2017) and Manifold Mixup (Verma et al., 2018), to the realm of
Recurrent Neural Networks (RNN). Our proposed methods are easy to implement and
have a low computational complexity, while leverage the performance of simple
neural architectures in a variety of tasks. We have validated our claims
through several experiments on real-world datasets, and also provide an
asymptotic theoretical analysis to further investigate the properties and
potential impacts of our proposed techniques. Applying sequence mixup to
BiLSTM-CRF model (Huang et al., 2015) to Named Entity Recognition task on
CoNLL-2003 data (Sang and De Meulder, 2003) has improved the F-1 score on the
test stage and reduced the loss, considerably.","['Armin Karamzade', 'Amir Najafi', 'Seyed Abolfazl Motahari']","['cs.CL', 'cs.LG', 'stat.ML']",2020-11-27 05:43:40+00:00
http://arxiv.org/abs/2011.13456v2,Score-Based Generative Modeling through Stochastic Differential Equations,"Creating noise from data is easy; creating data from noise is generative
modeling. We present a stochastic differential equation (SDE) that smoothly
transforms a complex data distribution to a known prior distribution by slowly
injecting noise, and a corresponding reverse-time SDE that transforms the prior
distribution back into the data distribution by slowly removing the noise.
Crucially, the reverse-time SDE depends only on the time-dependent gradient
field (\aka, score) of the perturbed data distribution. By leveraging advances
in score-based generative modeling, we can accurately estimate these scores
with neural networks, and use numerical SDE solvers to generate samples. We
show that this framework encapsulates previous approaches in score-based
generative modeling and diffusion probabilistic modeling, allowing for new
sampling procedures and new modeling capabilities. In particular, we introduce
a predictor-corrector framework to correct errors in the evolution of the
discretized reverse-time SDE. We also derive an equivalent neural ODE that
samples from the same distribution as the SDE, but additionally enables exact
likelihood computation, and improved sampling efficiency. In addition, we
provide a new way to solve inverse problems with score-based models, as
demonstrated with experiments on class-conditional generation, image
inpainting, and colorization. Combined with multiple architectural
improvements, we achieve record-breaking performance for unconditional image
generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a
competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity
generation of 1024 x 1024 images for the first time from a score-based
generative model.","['Yang Song', 'Jascha Sohl-Dickstein', 'Diederik P. Kingma', 'Abhishek Kumar', 'Stefano Ermon', 'Ben Poole']","['cs.LG', 'stat.ML']",2020-11-26 19:39:10+00:00
http://arxiv.org/abs/2011.13219v2,Message-Aware Graph Attention Networks for Large-Scale Multi-Robot Path Planning,"The domains of transport and logistics are increasingly relying on autonomous
mobile robots for the handling and distribution of passengers or resources. At
large system scales, finding decentralized path planning and coordination
solutions is key to efficient system performance. Recently, Graph Neural
Networks (GNNs) have become popular due to their ability to learn communication
policies in decentralized multi-agent systems. Yet, vanilla GNNs rely on
simplistic message aggregation mechanisms that prevent agents from prioritizing
important information. To tackle this challenge, in this paper, we extend our
previous work that utilizes GNNs in multi-agent path planning by incorporating
a novel mechanism to allow for message-dependent attention. Our Message-Aware
Graph Attention neTwork (MAGAT) is based on a key-query-like mechanism that
determines the relative importance of features in the messages received from
various neighboring robots. We show that MAGAT is able to achieve a performance
close to that of a coupled centralized expert algorithm. Further, ablation
studies and comparisons to several benchmark models show that our attention
mechanism is very effective across different robot densities and performs
stably in different constraints in communication bandwidth. Experiments
demonstrate that our model is able to generalize well in previously unseen
problem instances, and that it achieves a 47\% improvement over the benchmark
success rate, even in very large-scale instances that are $\times$100 larger
than the training instances.","['Qingbiao Li', 'Weizhe Lin', 'Zhe Liu', 'Amanda Prorok']","['cs.RO', 'cs.DC', 'cs.LG', 'cs.MA', 'stat.ML']",2020-11-26 10:37:13+00:00
http://arxiv.org/abs/2011.13161v1,Positive-Unlabelled Survival Data Analysis,"In this paper, we consider a novel framework of positive-unlabeled data in
which as positive data survival times are observed for subjects who have events
during the observation time as positive data and as unlabeled data censoring
times are observed but whether the event occurs or not are unknown for some
subjects. We consider two cases: (1) when censoring time is observed in
positive data, and (2) when it is not observed. For both cases, we developed
parametric models, nonparametric models, and machine learning models and the
estimation strategies for these models. Simulation studies show that under this
data setup, traditional survival analysis may yield severely biased results,
while the proposed estimation method can provide valid results.","['Tomoki Toyabe', 'Yasuhiro Hasegawa', 'Takahiro Hoshino']","['stat.ME', 'stat.ML']",2020-11-26 07:11:41+00:00
http://arxiv.org/abs/2011.13094v2,Combinatorial Bayesian Optimization with Random Mapping Functions to Convex Polytopes,"Bayesian optimization is a popular method for solving the problem of global
optimization of an expensive-to-evaluate black-box function. It relies on a
probabilistic surrogate model of the objective function, upon which an
acquisition function is built to determine where next to evaluate the objective
function. In general, Bayesian optimization with Gaussian process regression
operates on a continuous space. When input variables are categorical or
discrete, an extra care is needed. A common approach is to use one-hot encoded
or Boolean representation for categorical variables which might yield a
combinatorial explosion problem. In this paper we present a method for Bayesian
optimization in a combinatorial space, which can operate well in a large
combinatorial space. The main idea is to use a random mapping which embeds the
combinatorial space into a convex polytope in a continuous space, on which all
essential process is performed to determine a solution to the black-box
optimization in the combinatorial space. We describe our combinatorial Bayesian
optimization algorithm and present its regret analysis. Numerical experiments
demonstrate that our method shows satisfactory performance compared to existing
methods.","['Jungtaek Kim', 'Seungjin Choi', 'Minsu Cho']","['stat.ML', 'cs.LG']",2020-11-26 02:22:41+00:00
http://arxiv.org/abs/2011.13077v4,Functional Time Series Forecasting: Functional Singular Spectrum Analysis Approaches,"In this paper, we propose two nonparametric methods used in the forecasting
of functional time-dependent data, namely functional singular spectrum analysis
recurrent forecasting and vector forecasting. Both algorithms utilize the
results of functional singular spectrum analysis and past observations in order
to predict future data points where recurrent forecasting predicts one function
at a time and the vector forecasting makes predictions using functional
vectors. We compare our forecasting methods to a gold standard algorithm used
in the prediction of functional, time-dependent data by way of simulation and
real data and we find our techniques do better for periodic stochastic
processes.","['Jordan Trinka', 'Hossein Haghbin', 'Mehdi Maadooliat']","['stat.ML', 'cs.LG', '62M10, 62M20, 46N30, 60G35']",2020-11-26 00:36:57+00:00
http://arxiv.org/abs/2011.13034v3,Accommodating Picky Customers: Regret Bound and Exploration Complexity for Multi-Objective Reinforcement Learning,"In this paper we consider multi-objective reinforcement learning where the
objectives are balanced using preferences. In practice, the preferences are
often given in an adversarial manner, e.g., customers can be picky in many
applications. We formalize this problem as an episodic learning problem on a
Markov decision process, where transitions are unknown and a reward function is
the inner product of a preference vector with pre-specified multi-objective
reward functions. We consider two settings. In the online setting, the agent
receives a (adversarial) preference every episode and proposes policies to
interact with the environment. We provide a model-based algorithm that achieves
a nearly minimax optimal regret bound
$\widetilde{\mathcal{O}}\bigl(\sqrt{\min\{d,S\}\cdot H^2 SAK}\bigr)$, where $d$
is the number of objectives, $S$ is the number of states, $A$ is the number of
actions, $H$ is the length of the horizon, and $K$ is the number of episodes.
Furthermore, we consider preference-free exploration, i.e., the agent first
interacts with the environment without specifying any preference and then is
able to accommodate arbitrary preference vector up to $\epsilon$ error. Our
proposed algorithm is provably efficient with a nearly optimal trajectory
complexity $\widetilde{\mathcal{O}}\bigl({\min\{d,S\}\cdot H^3
SA}/{\epsilon^2}\bigr)$. This result partly resolves an open problem raised by
\citet{jin2020reward}.","['Jingfeng Wu', 'Vladimir Braverman', 'Lin F. Yang']","['cs.LG', 'stat.ML']",2020-11-25 21:45:04+00:00
http://arxiv.org/abs/2011.12946v3,Exploratory LQG Mean Field Games with Entropy Regularization,"We study a general class of entropy-regularized multi-variate LQG mean field
games (MFGs) in continuous time with $K$ distinct sub-population of agents. We
extend the notion of actions to action distributions (exploratory actions), and
explicitly derive the optimal action distributions for individual agents in the
limiting MFG. We demonstrate that the optimal set of action distributions
yields an $\epsilon$-Nash equilibrium for the finite-population
entropy-regularized MFG. Furthermore, we compare the resulting solutions with
those of classical LQG MFGs and establish the equivalence of their existence.","['Dena Firoozi', 'Sebastian Jaimungal']","['math.OC', 'cs.SY', 'eess.SY', 'math.PR', 'stat.ML']",2020-11-25 18:51:09+00:00
http://arxiv.org/abs/2011.12916v3,Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes,"Motivated by objects such as electric fields or fluid streams, we study the
problem of learning stochastic fields, i.e. stochastic processes whose samples
are fields like those occurring in physics and engineering. Considering general
transformations such as rotations and reflections, we show that spatial
invariance of stochastic fields requires an inference model to be equivariant.
Leveraging recent advances from the equivariance literature, we study
equivariance in two classes of models. Firstly, we fully characterise
equivariant Gaussian processes. Secondly, we introduce Steerable Conditional
Neural Processes (SteerCNPs), a new, fully equivariant member of the Neural
Process family. In experiments with Gaussian process vector fields, images, and
real-world weather data, we observe that SteerCNPs significantly improve the
performance of previous models and equivariance leads to improvements in
transfer learning tasks.","['Peter Holderrieth', 'Michael Hutchinson', 'Yee Whye Teh']","['cs.LG', 'stat.ML']",2020-11-25 18:00:40+00:00
http://arxiv.org/abs/2011.12913v2,"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation","While knowledge distillation (transfer) has been attracting attentions from
the research community, the recent development in the fields has heightened the
need for reproducible studies and highly generalized frameworks to lower
barriers to such high-quality, reproducible deep learning research. Several
researchers voluntarily published frameworks used in their knowledge
distillation studies to help other interested researchers reproduce their
original work. Such frameworks, however, are usually neither well generalized
nor maintained, thus researchers are still required to write a lot of code to
refactor/build on the frameworks for introducing new methods, models, datasets
and designing experiments. In this paper, we present our developed open-source
framework built on PyTorch and dedicated for knowledge distillation studies.
The framework is designed to enable users to design experiments by declarative
PyYAML configuration files, and helps researchers complete the recently
proposed ML Code Completeness Checklist. Using the developed framework, we
demonstrate its various efficient training strategies, and implement a variety
of knowledge distillation methods. We also reproduce some of their original
experimental results on the ImageNet and COCO datasets presented at major
machine learning conferences such as ICLR, NeurIPS, CVPR and ECCV, including
recent state-of-the-art methods. All the source code, configurations, log files
and trained model weights are publicly available at
https://github.com/yoshitomo-matsubara/torchdistill .",['Yoshitomo Matsubara'],"['cs.LG', 'cs.CV', 'stat.ML']",2020-11-25 17:51:30+00:00
http://arxiv.org/abs/2011.12829v2,All You Need is a Good Functional Prior for Bayesian Deep Learning,"The Bayesian treatment of neural networks dictates that a prior distribution
is specified over their weight and bias parameters. This poses a challenge
because modern neural networks are characterized by a large number of
parameters, and the choice of these priors has an uncontrolled effect on the
induced functional prior, which is the distribution of the functions obtained
by sampling the parameters from their prior distribution. We argue that this is
a hugely limiting aspect of Bayesian deep learning, and this work tackles this
limitation in a practical and effective way. Our proposal is to reason in terms
of functional priors, which are easier to elicit, and to ""tune"" the priors of
neural network parameters in a way that they reflect such functional priors.
Gaussian processes offer a rigorous framework to define prior distributions
over functions, and we propose a novel and robust framework to match their
prior with the functional prior of neural networks based on the minimization of
their Wasserstein distance. We provide vast experimental evidence that coupling
these priors with scalable Markov chain Monte Carlo sampling offers
systematically large performance improvements over alternative choices of
priors and state-of-the-art approximate Bayesian deep learning approaches. We
consider this work a considerable step in the direction of making the
long-standing challenge of carrying out a fully Bayesian treatment of neural
networks, including convolutional neural networks, a concrete possibility.","['Ba-Hien Tran', 'Simone Rossi', 'Dimitrios Milios', 'Maurizio Filippone']","['stat.ML', 'cs.LG']",2020-11-25 15:36:16+00:00
http://arxiv.org/abs/2011.12747v1,Symmetry-Aware Actor-Critic for 3D Molecular Design,"Automating molecular design using deep reinforcement learning (RL) has the
potential to greatly accelerate the search for novel materials. Despite recent
progress on leveraging graph representations to design molecules, such methods
are fundamentally limited by the lack of three-dimensional (3D) information. In
light of this, we propose a novel actor-critic architecture for 3D molecular
design that can generate molecular structures unattainable with previous
approaches. This is achieved by exploiting the symmetries of the design process
through a rotationally covariant state-action representation based on a
spherical harmonics series expansion. We demonstrate the benefits of our
approach on several 3D molecular design tasks, where we find that building in
such symmetries significantly improves generalization and the quality of
generated molecules.","['Gregor N. C. Simm', 'Robert Pinsler', 'Gábor Csányi', 'José Miguel Hernández-Lobato']","['stat.ML', 'cs.LG', 'physics.chem-ph']",2020-11-25 14:04:33+00:00
http://arxiv.org/abs/2011.12659v1,Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints,"We introduce Constr-DRKM, a deep kernel method for the unsupervised learning
of disentangled data representations. We propose augmenting the original deep
restricted kernel machine formulation for kernel PCA by orthogonality
constraints on the latent variables to promote disentanglement and to make it
possible to carry out optimization without first defining a stabilized
objective. After illustrating an end-to-end training procedure based on a
quadratic penalty optimization algorithm with warm start, we quantitatively
evaluate the proposed method's effectiveness in disentangled feature learning.
We demonstrate on four benchmark datasets that this approach performs similarly
overall to $\beta$-VAE on a number of disentanglement metrics when few training
points are available, while being less sensitive to randomness and
hyperparameter selection than $\beta$-VAE. We also present a deterministic
initialization of Constr-DRKM's training algorithm that significantly improves
the reproducibility of the results. Finally, we empirically evaluate and
discuss the role of the number of layers in the proposed methodology, examining
the influence of each principal component in every layer and showing that
components in lower layers act as local feature detectors capturing the broad
trends of the data distribution, while components in deeper layers use the
representation learned by previous layers and more accurately reproduce
higher-level features.","['Francesco Tonin', 'Panagiotis Patrinos', 'Johan A. K. Suykens']","['cs.LG', 'stat.ML']",2020-11-25 11:40:10+00:00
http://arxiv.org/abs/2011.12651v2,Feature space approximation for kernel-based supervised learning,"We propose a method for the approximation of high- or even
infinite-dimensional feature vectors, which play an important role in
supervised learning. The goal is to reduce the size of the training data,
resulting in lower storage consumption and computational complexity.
Furthermore, the method can be regarded as a regularization technique, which
improves the generalizability of learned target functions. We demonstrate
significant improvements in comparison to the computation of data-driven
predictions involving the full training data set. The method is applied to
classification and regression problems from different application areas such as
image recognition, system identification, and oceanographic time series
analysis.","['Patrick Gelß', 'Stefan Klus', 'Ingmar Schuster', 'Christof Schütte']","['stat.ML', 'cs.LG']",2020-11-25 11:23:58+00:00
http://arxiv.org/abs/2011.12596v1,MTCRNN: A multi-scale RNN for directed audio texture synthesis,"Audio textures are a subset of environmental sounds, often defined as having
stable statistical characteristics within an adequately large window of time
but may be unstructured locally. They include common everyday sounds such as
from rain, wind, and engines. Given that these complex sounds contain patterns
on multiple timescales, they are a challenge to model with traditional methods.
We introduce a novel modelling approach for textures, combining recurrent
neural networks trained at different levels of abstraction with a conditioning
strategy that allows for user-directed synthesis. We demonstrate the model's
performance on a variety of datasets, examine its performance on various
metrics, and discuss some potential applications.","['M. Huzaifah', 'L. Wyse']","['cs.SD', 'eess.AS', 'stat.ML', 'I.2.6; J.5']",2020-11-25 09:13:53+00:00
http://arxiv.org/abs/2011.12581v3,Overcoming Catastrophic Forgetting via Direction-Constrained Optimization,"This paper studies a new design of the optimization algorithm for training
deep learning models with a fixed architecture of the classification network in
a continual learning framework. The training data is non-stationary and the
non-stationarity is imposed by a sequence of distinct tasks. We first analyze a
deep model trained on only one learning task in isolation and identify a region
in network parameter space, where the model performance is close to the
recovered optimum. We provide empirical evidence that this region resembles a
cone that expands along the convergence direction. We study the principal
directions of the trajectory of the optimizer after convergence and show that
traveling along a few top principal directions can quickly bring the parameters
outside the cone but this is not the case for the remaining directions. We
argue that catastrophic forgetting in a continual learning setting can be
alleviated when the parameters are constrained to stay within the intersection
of the plausible cones of individual tasks that were so far encountered during
training. Based on this observation we present our direction-constrained
optimization (DCO) method, where for each task we introduce a linear
autoencoder to approximate its corresponding top forbidden principal
directions. They are then incorporated into the loss function in the form of a
regularization term for the purpose of learning the coming tasks without
forgetting. Furthermore, in order to control the memory growth as the number of
tasks increases, we propose a memory-efficient version of our algorithm called
compressed DCO (DCO-COMP) that allocates a memory of fixed size for storing all
autoencoders. We empirically demonstrate that our algorithm performs favorably
compared to other state-of-art regularization-based continual learning methods.","['Yunfei Teng', 'Anna Choromanska', 'Murray Campbell', 'Songtao Lu', 'Parikshit Ram', 'Lior Horesh']","['cs.LG', 'stat.ML']",2020-11-25 08:45:21+00:00
http://arxiv.org/abs/2011.12574v1,Enhanced Scene Specificity with Sparse Dynamic Value Estimation,"Multi-scene reinforcement learning involves training the RL agent across
multiple scenes / levels from the same task, and has become essential for many
generalization applications. However, the inclusion of multiple scenes leads to
an increase in sample variance for policy gradient computations, often
resulting in suboptimal performance with the direct application of traditional
methods (e.g. PPO, A3C). One strategy for variance reduction is to consider
each scene as a distinct Markov decision process (MDP) and learn a joint value
function dependent on both state (s) and MDP (M). However, this is non-trivial
as the agent is usually unaware of the underlying level at train / test times
in multi-scene RL. Recently, Singh et al. [1] tried to address this by
proposing a dynamic value estimation approach that models the true joint value
function distribution as a Gaussian mixture model (GMM). In this paper, we
argue that the error between the true scene-specific value function and the
predicted dynamic estimate can be further reduced by progressively enforcing
sparse cluster assignments once the agent has explored most of the state space.
The resulting agents not only show significant improvements in the final reward
score across a range of OpenAI ProcGen environments, but also exhibit increased
navigation efficiency while completing a game level.","['Jaskirat Singh', 'Liang Zheng']","['cs.LG', 'stat.ML']",2020-11-25 08:35:16+00:00
http://arxiv.org/abs/2011.12547v2,Implicit bias of deep linear networks in the large learning rate phase,"Most theoretical studies explaining the regularization effect in deep
learning have only focused on gradient descent with a sufficient small learning
rate or even gradient flow (infinitesimal learning rate). Such researches,
however, have neglected a reasonably large learning rate applied in most
practical applications. In this work, we characterize the implicit bias effect
of deep linear networks for binary classification using the logistic loss in
the large learning rate regime, inspired by the seminal work by Lewkowycz et
al. [26] in a regression setting with squared loss. They found a learning rate
regime with a large stepsize named the catapult phase, where the loss grows at
the early stage of training and eventually converges to a minimum that is
flatter than those found in the small learning rate regime. We claim that
depending on the separation conditions of data, the gradient descent iterates
will converge to a flatter minimum in the catapult phase. We rigorously prove
this claim under the assumption of degenerate data by overcoming the difficulty
of the non-constant Hessian of logistic loss and further characterize the
behavior of loss and Hessian for non-separable data. Finally, we demonstrate
that flatter minima in the space spanned by non-separable data along with the
learning rate in the catapult phase can lead to better generalization
empirically.","['Wei Huang', 'Weitao Du', 'Richard Yi Da Xu', 'Chunrui Liu']","['cs.LG', 'stat.ML']",2020-11-25 06:50:30+00:00
http://arxiv.org/abs/2011.12542v1,Wasserstein k-means with sparse simplex projection,"This paper presents a proposal of a faster Wasserstein $k$-means algorithm
for histogram data by reducing Wasserstein distance computations and exploiting
sparse simplex projection. We shrink data samples, centroids, and the ground
cost matrix, which leads to considerable reduction of the computations used to
solve optimal transport problems without loss of clustering quality.
Furthermore, we dynamically reduced the computational complexity by removing
lower-valued data samples and harnessing sparse simplex projection while
keeping the degradation of clustering quality lower. We designate this proposed
algorithm as sparse simplex projection based Wasserstein $k$-means, or SSPW
$k$-means. Numerical evaluations conducted with comparison to results obtained
using Wasserstein $k$-means algorithm demonstrate the effectiveness of the
proposed SSPW $k$-means for real-world datasets","['Takumi Fukunaga', 'Hiroyuki Kasai']","['cs.LG', 'stat.ML']",2020-11-25 06:37:45+00:00
http://arxiv.org/abs/2011.12532v1,Consistency-aware and Inconsistency-aware Graph-based Multi-view Clustering,"Multi-view data analysis has gained increasing popularity because multi-view
data are frequently encountered in machine learning applications. A simple but
promising approach for clustering of multi-view data is multi-view clustering
(MVC), which has been developed extensively to classify given subjects into
some clustered groups by learning latent common features that are shared across
multi-view data. Among existing approaches, graph-based multi-view clustering
(GMVC) achieves state-of-the-art performance by leveraging a shared graph
matrix called the unified matrix. However, existing methods including GMVC do
not explicitly address inconsistent parts of input graph matrices.
Consequently, they are adversely affected by unacceptable clustering
performance. To this end, this paper proposes a new GMVC method that
incorporates consistent and inconsistent parts lying across multiple views.
This proposal is designated as CI-GMVC. Numerical evaluations of real-world
datasets demonstrate the effectiveness of the proposed CI-GMVC.","['Mitsuhiko Horie', 'Hiroyuki Kasai']","['cs.LG', 'stat.ML']",2020-11-25 06:00:42+00:00
http://arxiv.org/abs/2011.12509v1,Modern Multiple Imputation with Functional Data,"This work considers the problem of fitting functional models with sparsely
and irregularly sampled functional data. It overcomes the limitations of the
state-of-the-art methods, which face major challenges in the fitting of more
complex non-linear models. Currently, many of these models cannot be
consistently estimated unless the number of observed points per curve grows
sufficiently quickly with the sample size, whereas, we show numerically that a
modified approach with more modern multiple imputation methods can produce
better estimates in general. We also propose a new imputation approach that
combines the ideas of {\it MissForest} with {\it Local Linear Forest} and
compare their performance with {\it PACE} and several other multivariate
multiple imputation methods. This work is motivated by a longitudinal study on
smoking cessation, in which the Electronic Health Records (EHR) from Penn State
PaTH to Health allow for the collection of a great deal of data, with highly
variable sampling. To illustrate our approach, we explore the relation between
relapse and diastolic blood pressure. We also consider a variety of simulation
schemes with varying levels of sparsity to validate our methods.","['Aniruddha Rajendra Rao', 'Matthew Reimherr']","['stat.ME', 'cs.LG', 'stat.ML']",2020-11-25 04:22:30+00:00
http://arxiv.org/abs/2011.12508v1,Causal inference using deep neural networks,"Causal inference from observation data is a core problem in many scientific
fields. Here we present a general supervised deep learning framework that
infers causal interactions by transforming the input vectors to an image-like
representation for every pair of inputs. Given a training dataset we first
construct a normalized empirical probability density distribution (NEPDF)
matrix. We then train a convolutional neural network (CNN) on NEPDFs for
causality predictions. We tested the method on several different simulated and
real world data and compared it to prior methods for causal inference. As we
show, the method is general, can efficiently handle very large datasets and
improves upon prior methods.","['Ye Yuan', 'Xueying Ding', 'Ziv Bar-Joseph']","['cs.LG', 'stat.ML']",2020-11-25 04:22:14+00:00
http://arxiv.org/abs/2011.12478v2,Minimax Estimation of Distances on a Surface and Minimax Manifold Learning in the Isometric-to-Convex Setting,"We start by considering the problem of estimating intrinsic distances on a
smooth submanifold. We show that minimax optimality can be obtained via a
reconstruction of the surface, and discuss the use of a particular mesh
construction -- the tangential Delaunay complex -- for that purpose. We then
turn to manifold learning and argue that a variant of Isomap where the
distances are instead computed on a reconstructed surface is minimax optimal
for the isometric variant of the problem.","['Ery Arias-Castro', 'Phong Alain Chau']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2020-11-25 01:57:51+00:00
http://arxiv.org/abs/2011.12433v2,Optimal Mean Estimation without a Variance,"We study the problem of heavy-tailed mean estimation in settings where the
variance of the data-generating distribution does not exist. Concretely, given
a sample $\mathbf{X} = \{X_i\}_{i = 1}^n$ from a distribution $\mathcal{D}$
over $\mathbb{R}^d$ with mean $\mu$ which satisfies the following
\emph{weak-moment} assumption for some ${\alpha \in [0, 1]}$: \begin{equation*}
\forall \|v\| = 1: \mathbb{E}_{X \thicksim \mathcal{D}}[\lvert \langle X - \mu,
v\rangle \rvert^{1 + \alpha}] \leq 1, \end{equation*} and given a target
failure probability, $\delta$, our goal is to design an estimator which attains
the smallest possible confidence interval as a function of $n,d,\delta$. For
the specific case of $\alpha = 1$, foundational work of Lugosi and Mendelson
exhibits an estimator achieving subgaussian confidence intervals, and
subsequent work has led to computationally efficient versions of this
estimator. Here, we study the case of general $\alpha$, and establish the
following information-theoretic lower bound on the optimal attainable
confidence interval: \begin{equation*} \Omega \left(\sqrt{\frac{d}{n}} +
\left(\frac{d}{n}\right)^{\frac{\alpha}{(1 + \alpha)}} + \left(\frac{\log 1 /
\delta}{n}\right)^{\frac{\alpha}{(1 + \alpha)}}\right). \end{equation*}
Moreover, we devise a computationally-efficient estimator which achieves this
lower bound.","['Yeshwanth Cherapanamjeri', 'Nilesh Tripuraneni', 'Peter L. Bartlett', 'Michael I. Jordan']","['math.ST', 'cs.DS', 'cs.LG', 'stat.ML', 'stat.TH']",2020-11-24 22:39:21+00:00
http://arxiv.org/abs/2011.12428v2,"Align, then memorise: the dynamics of learning with feedback alignment","Direct Feedback Alignment (DFA) is emerging as an efficient and biologically
plausible alternative to the ubiquitous backpropagation algorithm for training
deep neural networks. Despite relying on random feedback weights for the
backward pass, DFA successfully trains state-of-the-art models such as
Transformers. On the other hand, it notoriously fails to train convolutional
networks. An understanding of the inner workings of DFA to explain these
diverging results remains elusive. Here, we propose a theory for the success of
DFA. We first show that learning in shallow networks proceeds in two steps: an
alignment phase, where the model adapts its weights to align the approximate
gradient with the true gradient of the loss function, is followed by a
memorisation phase, where the model focuses on fitting the data. This two-step
process has a degeneracy breaking effect: out of all the low-loss solutions in
the landscape, a network trained with DFA naturally converges to the solution
which maximises gradient alignment. We also identify a key quantity underlying
alignment in deep linear networks: the conditioning of the alignment matrices.
The latter enables a detailed understanding of the impact of data structure on
alignment, and suggests a simple explanation for the well-known failure of DFA
to train convolutional neural networks. Numerical experiments on MNIST and
CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and
show that the align-then-memorise process occurs sequentially from the bottom
layers of the network to the top.","['Maria Refinetti', ""Stéphane d'Ascoli"", 'Ruben Ohana', 'Sebastian Goldt']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG', 'cs.NE']",2020-11-24 22:21:27+00:00
http://arxiv.org/abs/2011.12413v2,Wide-band butterfly network: stable and efficient inversion via multi-frequency neural networks,"We introduce an end-to-end deep learning architecture called the wide-band
butterfly network (WideBNet) for approximating the inverse scattering map from
wide-band scattering data. This architecture incorporates tools from
computational harmonic analysis, such as the butterfly factorization, and
traditional multi-scale methods, such as the Cooley-Tukey FFT algorithm, to
drastically reduce the number of trainable parameters to match the inherent
complexity of the problem. As a result WideBNet is efficient: it requires fewer
training points than off-the-shelf architectures, and has stable training
dynamics, thus it can rely on standard weight initialization strategies. The
architecture automatically adapts to the dimensions of the data with only a few
hyper-parameters that the user must specify. WideBNet is able to produce images
that are competitive with optimization-based approaches, but at a fraction of
the cost, and we also demonstrate numerically that it learns to super-resolve
scatterers in the full aperture scattering setup.","['Matthew Li', 'Laurent Demanet', 'Leonardo Zepeda-Núñez']","['cs.LG', 'cs.NA', 'cs.NE', 'math.NA', 'stat.ML']",2020-11-24 21:48:43+00:00
http://arxiv.org/abs/2011.12392v1,Geom-SPIDER-EM: Faster Variance Reduced Stochastic Expectation Maximization for Nonconvex Finite-Sum Optimization,"The Expectation Maximization (EM) algorithm is a key reference for inference
in latent variable models; unfortunately, its computational cost is prohibitive
in the large scale learning setting. In this paper, we propose an extension of
the Stochastic Path-Integrated Differential EstimatoR EM (SPIDER-EM) and derive
complexity bounds for this novel algorithm, designed to solve smooth nonconvex
finite-sum optimization problems. We show that it reaches the same state of the
art complexity bounds as SPIDER-EM; and provide conditions for a linear rate of
convergence. Numerical results support our findings.","['Gersende Fort', 'Eric Moulines', 'Hoi-To Wai']","['stat.ML', 'cs.LG', 'stat.ME']",2020-11-24 21:20:53+00:00
http://arxiv.org/abs/2011.12382v2,Reinforced optimal control,"Least squares Monte Carlo methods are a popular numerical approximation
method for solving stochastic control problems. Based on dynamic programming,
their key feature is the approximation of the conditional expectation of future
rewards by linear least squares regression. Hence, the choice of basis
functions is crucial for the accuracy of the method. Earlier work by some of us
[Belomestny, Schoenmakers, Spokoiny, Zharkynbay. Commun.~Math.~Sci.,
18(1):109-121, 2020](arXiv:1808.02341) proposes to reinforce the basis
functions in the case of optimal stopping problems by already computed value
functions for later times, thereby considerably improving the accuracy with
limited additional computational cost. We extend the reinforced regression
method to a general class of stochastic control problems, while considerably
improving the method's efficiency, as demonstrated by substantial numerical
examples as well as theoretical analysis.","['Christian Bayer', 'Denis Belomestny', 'Paul Hager', 'Paolo Pigato', 'John Schoenmakers', 'Vladimir Spokoiny']","['math.OC', 'cs.NA', 'math.NA', 'stat.ML', '91G20, 93E24']",2020-11-24 20:57:44+00:00
http://arxiv.org/abs/2011.12379v2,Invariant Representation Learning for Treatment Effect Estimation,"The defining challenge for causal inference from observational data is the
presence of `confounders', covariates that affect both treatment assignment and
the outcome. To address this challenge, practitioners collect and adjust for
the covariates, hoping that they adequately correct for confounding. However,
including every observed covariate in the adjustment runs the risk of including
`bad controls', variables that induce bias when they are conditioned on. The
problem is that we do not always know which variables in the covariate set are
safe to adjust for and which are not. To address this problem, we develop
Nearly Invariant Causal Estimation (NICE). NICE uses invariant risk
minimization (IRM) [Arj19] to learn a representation of the covariates that,
under some assumptions, strips out bad controls but preserves sufficient
information to adjust for confounding. Adjusting for the learned
representation, rather than the covariates themselves, avoids the induced bias
and provides valid causal inferences. We evaluate NICE on both synthetic and
semi-synthetic data. When the covariates contain unknown collider variables and
other bad controls, NICE performs better than adjusting for all the covariates.","['Claudia Shi', 'Victor Veitch', 'David Blei']","['cs.LG', 'stat.ML']",2020-11-24 20:53:24+00:00
http://arxiv.org/abs/2011.12378v1,A Non-linear Function-on-Function Model for Regression with Time Series Data,"In the last few decades, building regression models for non-scalar variables,
including time series, text, image, and video, has attracted increasing
interests of researchers from the data analytic community. In this paper, we
focus on a multivariate time series regression problem. Specifically, we aim to
learn mathematical mappings from multiple chronologically measured numerical
variables within a certain time interval S to multiple numerical variables of
interest over time interval T. Prior arts, including the multivariate
regression model, the Seq2Seq model, and the functional linear models, suffer
from several limitations. The first two types of models can only handle
regularly observed time series. Besides, the conventional multivariate
regression models tend to be biased and inefficient, as they are incapable of
encoding the temporal dependencies among observations from the same time
series. The sequential learning models explicitly use the same set of
parameters along time, which has negative impacts on accuracy. The
function-on-function linear model in functional data analysis (a branch of
statistics) is insufficient to capture complex correlations among the
considered time series and suffer from underfitting easily. In this paper, we
propose a general functional mapping that embraces the function-on-function
linear model as a special case. We then propose a non-linear
function-on-function model using the fully connected neural network to learn
the mapping from data, which addresses the aforementioned concerns in the
existing approaches. For the proposed model, we describe in detail the
corresponding numerical implementation procedures. The effectiveness of the
proposed model is demonstrated through the application to two real-world
problems.","['Qiyao Wang', 'Haiyan Wang', 'Chetan Gupta', 'Aniruddha Rajendra Rao', 'Hamed Khorasgani']","['cs.LG', 'stat.ML']",2020-11-24 20:51:27+00:00
http://arxiv.org/abs/2011.12363v3,C-Learning: Horizon-Aware Cumulative Accessibility Estimation,"Multi-goal reaching is an important problem in reinforcement learning needed
to achieve algorithmic generalization. Despite recent advances in this field,
current algorithms suffer from three major challenges: high sample complexity,
learning only a single way of reaching the goals, and difficulties in solving
complex motion planning tasks. In order to address these limitations, we
introduce the concept of cumulative accessibility functions, which measure the
reachability of a goal from a given state within a specified horizon. We show
that these functions obey a recurrence relation, which enables learning from
offline interactions. We also prove that optimal cumulative accessibility
functions are monotonic in the planning horizon. Additionally, our method can
trade off speed and reliability in goal-reaching by suggesting multiple paths
to a single goal depending on the provided horizon. We evaluate our approach on
a set of multi-goal discrete and continuous control tasks. We show that our
method outperforms state-of-the-art goal-reaching algorithms in success rate,
sample complexity, and path optimality. Our code is available at
https://github.com/layer6ai-labs/CAE, and additional visualizations can be
found at https://sites.google.com/view/learning-cae/.","['Panteha Naderian', 'Gabriel Loaiza-Ganem', 'Harry J. Braviner', 'Anthony L. Caterini', 'Jesse C. Cresswell', 'Tong Li', 'Animesh Garg']","['cs.LG', 'cs.AI', 'stat.ML']",2020-11-24 20:34:31+00:00
http://arxiv.org/abs/2011.12328v1,Generalized Variational Continual Learning,"Continual learning deals with training models on new tasks and datasets in an
online fashion. One strand of research has used probabilistic regularization
for continual learning, with two of the main approaches in this vein being
Online Elastic Weight Consolidation (Online EWC) and Variational Continual
Learning (VCL). VCL employs variational inference, which in other settings has
been improved empirically by applying likelihood-tempering. We show that
applying this modification to VCL recovers Online EWC as a limiting case,
allowing for interpolation between the two approaches. We term the general
algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning
effect of VI, we take inspiration from a common multi-task architecture, neural
networks with task-specific FiLM layers, and find that this addition leads to
significant performance gains, specifically for variational methods. In the
small-data regime, GVCL strongly outperforms existing baselines. In larger
datasets, GVCL with FiLM layers outperforms or is competitive with existing
baselines in terms of accuracy, whilst also providing significantly better
calibration.","['Noel Loo', 'Siddharth Swaroop', 'Richard E. Turner']","['cs.LG', 'stat.ML']",2020-11-24 19:07:39+00:00
http://arxiv.org/abs/2011.12245v2,Effect of barren plateaus on gradient-free optimization,"Barren plateau landscapes correspond to gradients that vanish exponentially
in the number of qubits. Such landscapes have been demonstrated for variational
quantum algorithms and quantum neural networks with either deep circuits or
global cost functions. For obvious reasons, it is expected that gradient-based
optimizers will be significantly affected by barren plateaus. However, whether
or not gradient-free optimizers are impacted is a topic of debate, with some
arguing that gradient-free approaches are unaffected by barren plateaus. Here
we show that, indeed, gradient-free optimizers do not solve the barren plateau
problem. Our main result proves that cost function differences, which are the
basis for making decisions in a gradient-free optimization, are exponentially
suppressed in a barren plateau. Hence, without exponential precision,
gradient-free optimizers will not make progress in the optimization. We
numerically confirm this by training in a barren plateau with several
gradient-free optimizers (Nelder-Mead, Powell, and COBYLA algorithms), and show
that the numbers of shots required in the optimization grows exponentially with
the number of qubits.","['Andrew Arrasmith', 'M. Cerezo', 'Piotr Czarnik', 'Lukasz Cincio', 'Patrick J. Coles']","['quant-ph', 'cs.LG', 'stat.ML']",2020-11-24 17:41:13+00:00
http://arxiv.org/abs/2011.12216v3,Energy-Based Models for Continual Learning,"We motivate Energy-Based Models (EBMs) as a promising model class for
continual learning problems. Instead of tackling continual learning via the use
of external memory, growing models, or regularization, EBMs change the
underlying training objective to cause less interference with previously
learned information. Our proposed version of EBMs for continual learning is
simple, efficient, and outperforms baseline methods by a large margin on
several benchmarks. Moreover, our proposed contrastive divergence-based
training objective can be combined with other continual learning methods,
resulting in substantial boosts in their performance. We further show that EBMs
are adaptable to a more general continual learning setting where the data
distribution changes without the notion of explicitly delineated tasks. These
observations point towards EBMs as a useful building block for future continual
learning methods.","['Shuang Li', 'Yilun Du', 'Gido M. van de Ven', 'Igor Mordatch']","['cs.LG', 'cs.AI', 'stat.ML']",2020-11-24 17:08:13+00:00
http://arxiv.org/abs/2011.12919v2,Analyzing the Machine Learning Conference Review Process,"Mainstream machine learning conferences have seen a dramatic increase in the
number of participants, along with a growing range of perspectives, in recent
years. Members of the machine learning community are likely to overhear
allegations ranging from randomness of acceptance decisions to institutional
bias. In this work, we critically analyze the review process through a
comprehensive study of papers submitted to ICLR between 2017 and 2020. We
quantify reproducibility/randomness in review scores and acceptance decisions,
and examine whether scores correlate with paper impact. Our findings suggest
strong institutional bias in accept/reject decisions, even after controlling
for paper quality. Furthermore, we find evidence for a gender gap, with female
authors receiving lower scores, lower acceptance rates, and fewer citations per
paper than their male counterparts. We conclude our work with recommendations
for future conference organizers.","['David Tran', 'Alex Valtchanov', 'Keshav Ganapathy', 'Raymond Feng', 'Eric Slud', 'Micah Goldblum', 'Tom Goldstein']","['cs.LG', 'cs.AI', 'stat.ML']",2020-11-24 15:40:27+00:00
http://arxiv.org/abs/2011.12160v2,Wyner-Ziv Estimators for Distributed Mean Estimation with Side Information and Optimization,"Communication efficient distributed mean estimation is an important primitive
that arises in many distributed learning and optimization scenarios such as
federated learning. Without any probabilistic assumptions on the underlying
data, we study the problem of distributed mean estimation where the server has
access to side information. We propose \emph{Wyner-Ziv estimators}, which are
communication and computationally efficient and near-optimal when an upper
bound for the distance between the side information and the data is known. As a
corollary, we also show that our algorithms provide efficient schemes for the
classic Wyner-Ziv problem in information theory. In a different direction, when
there is no knowledge assumed about the distance between side information and
the data, we present an alternative Wyner-Ziv estimator that uses correlated
sampling. This latter setting offers {\em universal recovery guarantees}, and
perhaps will be of interest in practice when the number of users is large and
keeping track of the distances between the data and the side information may
not be possible.
  With this mean estimator at our disposal, we revisit basic problems in
decentralized optimization and compression where our Wyner-Ziv estimator yields
algorithms with almost optimal performance. First, we consider the problem of
communication constrained distributed optimization and provide an algorithm
which attains the optimal convergence rate by exploiting the fact that the
gradient estimates are close to each other. Specifically, the gradient
compression scheme in our algorithm first uses half of the parties to form side
information and then uses our Wyner-Ziv estimator to compress the remaining
half of the gradient estimates.","['Prathamesh Mayekar', 'Shubham Jha', 'Ananda Theertha Suresh', 'Himanshu Tyagi']","['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2020-11-24 15:19:55+00:00
http://arxiv.org/abs/2011.12151v3,Tensor Kernel Recovery for Spatio-Temporal Hawkes Processes,"We estimate the general influence functions for spatio-temporal Hawkes
processes using a tensor recovery approach by formulating the location
dependent influence function that captures the influence of historical events
as a tensor kernel. We assume a low-rank structure for the tensor kernel and
cast the estimation problem as a convex optimization problem using the Fourier
transformed nuclear norm (TNN). We provide theoretical performance guarantees
for our approach and present an algorithm to solve the optimization problem.
Moreover, we demonstrate the efficiency of our estimation with numerical
simulations.","['Heejune Sheen', 'Xiaonan Zhu', 'Yao Xie']","['stat.ML', 'cs.LG']",2020-11-24 15:05:26+00:00
http://arxiv.org/abs/2011.11981v1,Deep-learning based discovery of partial differential equations in integral form from sparse and noisy data,"Data-driven discovery of partial differential equations (PDEs) has attracted
increasing attention in recent years. Although significant progress has been
made, certain unresolved issues remain. For example, for PDEs with high-order
derivatives, the performance of existing methods is unsatisfactory, especially
when the data are sparse and noisy. It is also difficult to discover
heterogeneous parametric PDEs where heterogeneous parameters are embedded in
the partial differential operators. In this work, a new framework combining
deep-learning and integral form is proposed to handle the above-mentioned
problems simultaneously, and improve the accuracy and stability of PDE
discovery. In the framework, a deep neural network is firstly trained with
observation data to generate meta-data and calculate derivatives. Then, a
unified integral form is defined, and the genetic algorithm is employed to
discover the best structure. Finally, the value of parameters is calculated,
and whether the parameters are constants or variables is identified. Numerical
experiments proved that our proposed algorithm is more robust to noise and more
accurate compared with existing methods due to the utilization of integral
form. Our proposed algorithm is also able to discover PDEs with high-order
derivatives or heterogeneous parameters accurately with sparse and noisy data.","['Hao Xu', 'Dongxiao Zhang', 'Nanzhe Wang']","['cs.LG', 'cs.NA', 'math.NA', 'physics.comp-ph', 'stat.ML']",2020-11-24 09:18:39+00:00
http://arxiv.org/abs/2011.11966v1,Provably robust blind source separation of linear-quadratic near-separable mixtures,"In this work, we consider the problem of blind source separation (BSS) by
departing from the usual linear model and focusing on the linear-quadratic (LQ)
model. We propose two provably robust and computationally tractable algorithms
to tackle this problem under separability assumptions which require the sources
to appear as samples in the data set. The first algorithm generalizes the
successive nonnegative projection algorithm (SNPA), designed for linear BSS,
and is referred to as SNPALQ. By explicitly modeling the product terms inherent
to the LQ model along the iterations of the SNPA scheme, the nonlinear
contributions of the mixing are mitigated, thus improving the separation
quality. SNPALQ is shown to be able to recover the ground truth factors that
generated the data, even in the presence of noise. The second algorithm is a
brute-force (BF) algorithm, which is used as a post-processing step for SNPALQ.
It enables to discard the spurious (mixed) samples extracted by SNPALQ, thus
broadening its applicability. The BF is in turn shown to be robust to noise
under easier-to-check and milder conditions than SNPALQ. We show that SNPALQ
with and without the BF postprocessing is relevant in realistic numerical
experiments.","['Christophe Kervazo', 'Nicolas Gillis', 'Nicolas Dobigeon']","['eess.SP', 'cs.LG', 'cs.NA', 'eess.IV', 'math.NA', 'stat.ML']",2020-11-24 08:53:40+00:00
http://arxiv.org/abs/2011.11884v3,SMG: A Shuffling Gradient-Based Method with Momentum,"We combine two advanced ideas widely used in optimization for machine
learning: shuffling strategy and momentum technique to develop a novel
shuffling gradient-based method with momentum, coined Shuffling Momentum
Gradient (SMG), for non-convex finite-sum optimization problems. While our
method is inspired by momentum techniques, its update is fundamentally
different from existing momentum-based methods. We establish state-of-the-art
convergence rates of SMG for any shuffling strategy using either constant or
diminishing learning rate under standard assumptions (i.e.$L$-smoothness and
bounded variance). When the shuffling strategy is fixed, we develop another new
algorithm that is similar to existing momentum methods, and prove the same
convergence rates for this algorithm under the $L$-smoothness and bounded
gradient assumptions. We demonstrate our algorithms via numerical simulations
on standard datasets and compare them with existing shuffling methods. Our
tests have shown encouraging performance of the new algorithms.","['Trang H. Tran', 'Lam M. Nguyen', 'Quoc Tran-Dinh']","['math.OC', 'cs.LG', 'stat.ML']",2020-11-24 04:12:35+00:00
http://arxiv.org/abs/2011.11877v2,InstaHide's Sample Complexity When Mixing Two Private Images,"Training neural networks usually require large numbers of sensitive training
data, and how to protect the privacy of training data has thus become a
critical topic in deep learning research. InstaHide is a state-of-the-art
scheme to protect training data privacy with only minor effects on test
accuracy, and its security has become a salient question. In this paper, we
systematically study recent attacks on InstaHide and present a unified
framework to understand and analyze these attacks. We find that existing
attacks either do not have a provable guarantee or can only recover a single
private image. On the current InstaHide challenge setup, where each InstaHide
image is a mixture of two private images, we present a new algorithm to recover
all the private images with a provable guarantee and optimal sample complexity.
In addition, we also provide a computational hardness result on retrieving all
InstaHide images. Our results demonstrate that InstaHide is not
information-theoretically secure but computationally secure in the worst case,
even when mixing two private images.","['Baihe Huang', 'Zhao Song', 'Runzhou Tao', 'Junze Yin', 'Ruizhe Zhang', 'Danyang Zhuo']","['cs.LG', 'cs.CC', 'cs.CR', 'cs.DS', 'stat.ML']",2020-11-24 03:41:03+00:00
http://arxiv.org/abs/2011.11820v2,An end-to-end data-driven optimisation framework for constrained trajectories,"Many real-world problems require to optimise trajectories under constraints.
Classical approaches are based on optimal control methods but require an exact
knowledge of the underlying dynamics, which could be challenging or even out of
reach. In this paper, we leverage data-driven approaches to design a new
end-to-end framework which is dynamics-free for optimised and realistic
trajectories. We first decompose the trajectories on function basis, trading
the initial infinite dimension problem on a multivariate functional space for a
parameter optimisation problem. A maximum \emph{a posteriori} approach which
incorporates information from data is used to obtain a new optimisation problem
which is regularised. The penalised term focuses the search on a region
centered on data and includes estimated linear constraints in the problem. We
apply our data-driven approach to two settings in aeronautics and sailing
routes optimisation, yielding commanding results. The developed approach has
been implemented in the Python library PyRotor.","['Florent Dewez', 'Benjamin Guedj', 'Arthur Talpaert', 'Vincent Vandewalle']","['stat.AP', 'cs.LG', 'math.OC', 'stat.ML']",2020-11-24 00:54:17+00:00
http://arxiv.org/abs/2012.02704v5,Random Sampling High Dimensional Model Representation Gaussian Process Regression (RS-HDMR-GPR) for representing multidimensional functions with machine-learned lower-dimensional terms allowing insight with a general method,"We present a Python implementation for RS-HDMR-GPR (Random Sampling High
Dimensional Model Representation Gaussian Process Regression). The method
builds representations of multivariate functions with lower-dimensional terms,
either as an expansion over orders of coupling or using terms of only a given
dimensionality. This facilitates, in particular, recovering functional
dependence from sparse data. The code also allows for imputation of missing
values of the variables and for a significant pruning of the useful number of
HDMR terms. The code can also be used for estimating relative importance of
different combinations of input variables, thereby adding an element of insight
to a general machine learning method. The capabilities of this regression tool
are demonstrated on test cases involving synthetic analytic functions, the
potential energy surface of the water molecule, kinetic energy densities of
materials (crystalline magnesium, aluminum, and silicon), and financial market
data.","['Owen Ren', 'Mohamed Ali Boussaidi', 'Dmitry Voytsekhovsky', 'Manabu Ihara', 'Sergei Manzhos']","['stat.CO', 'stat.ML']",2020-11-24 00:12:05+00:00
http://arxiv.org/abs/2011.14821v2,Discovering Causal Structure with Reproducing-Kernel Hilbert Space $ε$-Machines,"We merge computational mechanics' definition of causal states
(predictively-equivalent histories) with reproducing-kernel Hilbert space
(RKHS) representation inference. The result is a widely-applicable method that
infers causal structure directly from observations of a system's behaviors
whether they are over discrete or continuous events or time. A structural
representation -- a finite- or infinite-state kernel $\epsilon$-machine -- is
extracted by a reduced-dimension transform that gives an efficient
representation of causal states and their topology. In this way, the system
dynamics are represented by a stochastic (ordinary or partial) differential
equation that acts on causal states. We introduce an algorithm to estimate the
associated evolution operator. Paralleling the Fokker-Plank equation, it
efficiently evolves causal-state distributions and makes predictions in the
original data space via an RKHS functional mapping. We demonstrate these
techniques, together with their predictive abilities, on discrete-time,
discrete-value infinite Markov-order processes generated by finite-state hidden
Markov models with (i) finite or (ii) uncountably-infinite causal states and
(iii) continuous-time, continuous-value processes generated by thermally-driven
chaotic flows. The method robustly estimates causal structure in the presence
of varying external and measurement noise levels and for very high dimensional
data.","['Nicolas Brodu', 'James P. Crutchfield']","['cs.LG', 'cond-mat.stat-mech', 'stat.ML']",2020-11-23 23:41:16+00:00
http://arxiv.org/abs/2011.11780v1,Probabilistic modeling of discrete structural response with application to composite plate penetration models,"Discrete response of structures is often a key probabilistic quantity of
interest. For example, one may need to identify the probability of a binary
event, such as, whether a structure has buckled or not. In this study, an
adaptive domain-based decomposition and classification method, combined with
sparse grid sampling, is used to develop an efficient classification surrogate
modeling algorithm for such discrete outputs. An assumption of monotonic
behaviour of the output with respect to all model parameters, based on the
physics of the problem, helps to reduce the number of model evaluations and
makes the algorithm more efficient. As an application problem, this paper deals
with the development of a computational framework for generation of
probabilistic penetration response of S-2 glass/SC-15 epoxy composite plates
under ballistic impact. This enables the computationally feasible generation of
the probabilistic velocity response (PVR) curve or the $V_0-V_{100}$ curve as a
function of the impact velocity, and the ballistic limit velocity prediction as
a function of the model parameters. The PVR curve incorporates the variability
of the model input parameters and describes the probability of penetration of
the plate as a function of impact velocity.","['Anindya Bhaduri', 'Christopher S. Meyer', 'John W. Gillespie Jr.', 'Bazle Z. Haque', 'Michael D. Shields', 'Lori Graham-Brady']","['stat.AP', 'stat.ML']",2020-11-23 22:45:09+00:00
http://arxiv.org/abs/2011.14006v2,Offset-free setpoint tracking using neural network controllers,"In this paper, we present a method to analyze local and global stability in
offset-free setpoint tracking using neural network controllers and we provide
ellipsoidal inner approximations of the corresponding region of attraction. We
consider a feedback interconnection of a linear plant in connection with a
neural network controller and an integrator, which allows for offset-free
tracking of a desired piecewise constant reference that enters the controller
as an external input. Exploiting the fact that activation functions used in
neural networks are slope-restricted, we derive linear matrix inequalities to
verify stability using Lyapunov theory. After stating a global stability
result, we present less conservative local stability conditions (i) for a given
reference and (ii) for any reference from a certain set. The latter result even
enables guaranteed tracking under setpoint changes using a reference governor
which can lead to a significant increase of the region of attraction. Finally,
we demonstrate the applicability of our analysis by verifying stability and
offset-free tracking of a neural network controller that was trained to
stabilize a linearized inverted pendulum.","['Patricia Pauli', 'Johannes Köhler', 'Julian Berberich', 'Anne Koch', 'Frank Allgöwer']","['eess.SY', 'cs.LG', 'cs.SY', 'stat.ML']",2020-11-23 20:13:13+00:00
http://arxiv.org/abs/2011.11660v3,Differentially Private Learning Needs Better Features (or Much More Data),"We demonstrate that differentially private machine learning has not yet
reached its ""AlexNet moment"" on many canonical vision tasks: linear models
trained on handcrafted features significantly outperform end-to-end deep neural
networks for moderate privacy budgets. To exceed the performance of handcrafted
features, we show that private learning requires either much more private data,
or access to features learned on public data from a similar domain. Our work
introduces simple yet strong baselines for differentially private learning that
can inform the evaluation of future progress in this area.","['Florian Tramèr', 'Dan Boneh']","['cs.LG', 'cs.CR', 'stat.ML']",2020-11-23 19:00:52+00:00
