id,title,abstract,authors,categories,date
http://arxiv.org/abs/1902.03517v2,Biadversarial Variational Autoencoder,"In the original version of the Variational Autoencoder, Kingma et al. assume
Gaussian distributions for the approximate posterior during the inference and
for the output during the generative process. This assumptions are good for
computational reasons, e.g. we can easily optimize the parameters of a neural
network using the reparametrization trick and the KL divergence between two
Gaussians can be computed in closed form. However it results in blurry images
due to its difficulty to represent multimodal distributions. We show that using
two adversarial networks, we can optimize the parameters without any Gaussian
assumptions.",['Arnaud Fickinger'],"['cs.LG', 'stat.ML']",2019-02-09 23:57:06+00:00
http://arxiv.org/abs/1902.03515v1,Multi-Domain Translation by Learning Uncoupled Autoencoders,"Multi-domain translation seeks to learn a probabilistic coupling between
marginal distributions that reflects the correspondence between different
domains. We assume that data from different domains are generated from a shared
latent representation based on a structural equation model. Under this
assumption, we show that the problem of computing a probabilistic coupling
between marginals is equivalent to learning multiple uncoupled autoencoders
that embed to a given shared latent distribution. In addition, we propose a new
framework and algorithm for multi-domain translation based on learning the
shared latent distribution and training autoencoders under distributional
constraints. A key practical advantage of our framework is that new
autoencoders (i.e., new domains) can be added sequentially to the model without
retraining on the other domains, which we demonstrate experimentally on image
as well as genomics datasets.","['Karren D. Yang', 'Caroline Uhler']","['cs.LG', 'stat.ML', '68T01']",2019-02-09 23:46:22+00:00
http://arxiv.org/abs/1902.03511v4,Nonparametric Density Estimation & Convergence Rates for GANs under Besov IPM Losses,"We study the problem of estimating a nonparametric probability density under
a large family of losses called Besov IPMs, which include, for example,
$\mathcal{L}^p$ distances, total variation distance, and generalizations of
both Wasserstein and Kolmogorov-Smirnov distances. For a wide variety of
settings, we provide both lower and upper bounds, identifying precisely how the
choice of loss function and assumptions on the data interact to determine the
minimax optimal convergence rate. We also show that linear distribution
estimates, such as the empirical distribution or kernel density estimator,
often fail to converge at the optimal rate. Our bounds generalize, unify, or
improve several recent and classical results. Moreover, IPMs can be used to
formalize a statistical model of generative adversarial networks (GANs). Thus,
we show how our results imply bounds on the statistical error of a GAN,
showing, for example, that GANs can strictly outperform the best linear
estimator.","['Ananya Uppal', 'Shashank Singh', 'Barnabás Póczos']","['math.ST', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML', 'stat.TH']",2019-02-09 23:24:43+00:00
http://arxiv.org/abs/1902.03510v2,Inverse Projection Representation and Category Contribution Rate for Robust Tumor Recognition,"Sparse representation based classification (SRC) methods have achieved
remarkable results. SRC, however, still suffer from requiring enough training
samples, insufficient use of test samples and instability of representation. In
this paper, a stable inverse projection representation based classification
(IPRC) is presented to tackle these problems by effectively using test samples.
An IPR is firstly proposed and its feasibility and stability are analyzed. A
classification criterion named category contribution rate is constructed to
match the IPR and complete classification. Moreover, a statistical measure is
introduced to quantify the stability of representation-based classification
methods. Based on the IPRC technique, a robust tumor recognition framework is
presented by interpreting microarray gene expression data, where a two-stage
hybrid gene selection method is introduced to select informative genes.
Finally, the functional analysis of candidate's pathogenicity-related genes is
given. Extensive experiments on six public tumor microarray gene expression
datasets demonstrate the proposed technique is competitive with
state-of-the-art methods.","['Xiao-Hui Yang', 'Li Tian', 'Yun-Mei Chen', 'Li-Jun Yang', 'Shuang Xu', 'Wen-Ming Wu']","['q-bio.QM', 'cs.CV', 'cs.LG', 'stat.ML']",2019-02-09 23:07:22+00:00
http://arxiv.org/abs/1902.03501v2,Assessing the Local Interpretability of Machine Learning Models,"The increasing adoption of machine learning tools has led to calls for
accountability via model interpretability. But what does it mean for a machine
learning model to be interpretable by humans, and how can this be assessed? We
focus on two definitions of interpretability that have been introduced in the
machine learning literature: simulatability (a user's ability to run a model on
a given input) and ""what if"" local explainability (a user's ability to
correctly determine a model's prediction under local changes to the input,
given knowledge of the model's original prediction). Through a user study with
1,000 participants, we test whether humans perform well on tasks that mimic the
definitions of simulatability and ""what if"" local explainability on models that
are typically considered locally interpretable. To track the relative
interpretability of models, we employ a simple metric, the runtime operation
count on the simulatability task. We find evidence that as the number of
operations increases, participant accuracy on the local interpretability tasks
decreases. In addition, this evidence is consistent with the common intuition
that decision trees and logistic regression models are interpretable and are
more interpretable than neural networks.","['Dylan Slack', 'Sorelle A. Friedler', 'Carlos Scheidegger', 'Chitradeep Dutta Roy']","['cs.LG', 'cs.HC', 'stat.ML']",2019-02-09 21:49:36+00:00
http://arxiv.org/abs/1902.03498v3,Space lower bounds for linear prediction in the streaming model,"We show that fundamental learning tasks, such as finding an approximate
linear separator or linear regression, require memory at least \emph{quadratic}
in the dimension, in a natural streaming setting. This implies that such
problems cannot be solved (at least in this setting) by scalable
memory-efficient streaming algorithms. Our results build on a memory lower
bound for a simple linear-algebraic problem -- finding orthogonal vectors --
and utilize the estimates on the packing of the Grassmannian, the manifold of
all linear subspaces of fixed dimension.","['Yuval Dagan', 'Gil Kur', 'Ohad Shamir']","['cs.LG', 'stat.ML']",2019-02-09 21:44:40+00:00
http://arxiv.org/abs/1902.03493v3,Deep Algorithm Unrolling for Blind Image Deblurring,"Blind image deblurring remains a topic of enduring interest. Learning based
approaches, especially those that employ neural networks have emerged to
complement traditional model based methods and in many cases achieve vastly
enhanced performance. That said, neural network approaches are generally
empirically designed and the underlying structures are difficult to interpret.
In recent years, a promising technique called algorithm unrolling has been
developed that has helped connect iterative algorithms such as those for sparse
coding to neural network architectures. However, such connections have not been
made yet for blind image deblurring. In this paper, we propose a neural network
architecture based on this idea. We first present an iterative algorithm that
may be considered as a generalization of the traditional total-variation
regularization method in the gradient domain. We then unroll the algorithm to
construct a neural network for image deblurring which we refer to as Deep
Unrolling for Blind Deblurring (DUBLID). Key algorithm parameters are learned
with the help of training images. Our proposed deep network DUBLID achieves
significant practical performance gains while enjoying interpretability at the
same time. Extensive experimental results show that DUBLID outperforms many
state-of-the-art methods and in addition is computationally faster.","['Yuelong Li', 'Mohammad Tofighi', 'Junyi Geng', 'Vishal Monga', 'Yonina C. Eldar']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2019-02-09 21:19:35+00:00
http://arxiv.org/abs/1902.05399v2,An Algorithm Unrolling Approach to Deep Image Deblurring,"While neural networks have achieved vastly enhanced performance over
traditional iterative methods in many cases, they are generally empirically
designed and the underlying structures are difficult to interpret. The
algorithm unrolling approach has helped connect iterative algorithms to neural
network architectures. However, such connections have not been made yet for
blind image deblurring. In this paper, we propose a neural network architecture
that advances this idea. We first present an iterative algorithm that may be
considered a generalization of the traditional total-variation regularization
method on the gradient domain, and subsequently unroll the half-quadratic
splitting algorithm to construct a neural network. Our proposed deep network
achieves significant practical performance gains while enjoying
interpretability at the same time. Experimental results show that our approach
outperforms many state-of-the-art methods.","['Yuelong Li', 'Mohammad Tofighi', 'Vishal Monga', 'Yonina C. Eldar']","['cs.CV', 'cs.LG', 'stat.ML']",2019-02-09 21:19:11+00:00
http://arxiv.org/abs/1902.03475v1,Pure Exploration with Multiple Correct Answers,"We determine the sample complexity of pure exploration bandit problems with
multiple good answers. We derive a lower bound using a new game equilibrium
argument. We show how continuity and convexity properties of single-answer
problems ensures that the Track-and-Stop algorithm has asymptotically optimal
sample complexity. However, that convexity is lost when going to the
multiple-answer setting. We present a new algorithm which extends
Track-and-Stop to the multiple-answer case and has asymptotic sample complexity
matching the lower bound.","['Rémy Degenne', 'Wouter M. Koolen']","['cs.LG', 'stat.ML']",2019-02-09 19:08:22+00:00
http://arxiv.org/abs/1902.03468v4,Synthetic Data Generators: Sequential and Private,"We study the sample complexity of private synthetic data generation over an
unbounded sized class of statistical queries, and show that any class that is
privately proper PAC learnable admits a private synthetic data generator
(perhaps non-efficient). Previous work on synthetic data generators focused on
the case that the query class $\mathcal{D}$ is finite and obtained sample
complexity bounds that scale logarithmically with the size $|\mathcal{D}|$.
Here we construct a private synthetic data generator whose sample complexity is
independent of the domain size, and we replace finiteness with the assumption
that $\mathcal{D}$ is privately PAC learnable (a formally weaker task, hence we
obtain equivalence between the two tasks).","['Olivier Bousquet', 'Roi Livni', 'Shay Moran']","['cs.LG', 'stat.ML']",2019-02-09 18:35:39+00:00
http://arxiv.org/abs/1902.03466v2,Hierarchical Multi-task Deep Neural Network Architecture for End-to-End Driving,"A novel hierarchical Deep Neural Network (DNN) model is presented to address
the task of end-to-end driving. The model consists of a master classifier
network which determines the driving task required from an input stereo image
and directs said image to one of a set of subservient network regression models
that perform inference and output a steering command. These subservient
networks are designed and trained for a specific driving task: straightaway,
swerve maneuver, tight turn, gradual turn, and chicane. Using this modular
network strategy allows for two primary advantages: an overall reduction in the
amount of data required to train the complete system, and for model tailoring
where more complex models can be used for more challenging tasks while
simplified networks can handle more mundane tasks. It is this latter facet of
the model that makes the approach attractive to a number of applications beyond
the current vehicle steering strategy.","['Jose Solomon', 'Francois Charette']","['cs.LG', 'cs.CV', 'cs.RO', 'stat.ML']",2019-02-09 18:23:51+00:00
http://arxiv.org/abs/1902.03453v2,Distance metric learning based on structural neighborhoods for dimensionality reduction and classification performance improvement,"Distance metric learning can be viewed as one of the fundamental interests in
pattern recognition and machine learning, which plays a pivotal role in the
performance of many learning methods. One of the effective methods in learning
such a metric is to learn it from a set of labeled training samples. The issue
of data imbalance is the most important challenge of recent methods. This
research tries not only to preserve the local structures but also covers the
issue of imbalanced datasets. To do this, the proposed method first tries to
extract a low dimensional manifold from the input data. Then, it learns the
local neighborhood structures and the relationship of the data points in the
ambient space based on the adjacencies of the same data points on the embedded
low dimensional manifold. Using the local neighborhood relationships extracted
from the manifold space, the proposed method learns the distance metric in a
way which minimizes the distance between similar data and maximizes their
distance from the dissimilar data points. The evaluations of the proposed
method on numerous datasets from the UCI repository of machine learning, and
also the KDDCup98 dataset as the most imbalance dataset, justify the supremacy
of the proposed approach in comparison with other approaches especially when
the imbalance factor is high.","['Mostafa Razavi Ghods', 'Mohammad Hossein Moattar', 'Yahya Forghani']","['cs.LG', 'stat.ML']",2019-02-09 17:33:37+00:00
http://arxiv.org/abs/1902.03444v1,Venn GAN: Discovering Commonalities and Particularities of Multiple Distributions,"We propose a GAN design which models multiple distributions effectively and
discovers their commonalities and particularities. Each data distribution is
modeled with a mixture of $K$ generator distributions. As the generators are
partially shared between the modeling of different true data distributions,
shared ones captures the commonality of the distributions, while non-shared
ones capture unique aspects of them. We show the effectiveness of our method on
various datasets (MNIST, Fashion MNIST, CIFAR-10, Omniglot, CelebA) with
compelling results.","['Yasin Yazıcı', 'Bruno Lecouat', 'Chuan-Sheng Foo', 'Stefan Winkler', 'Kim-Hui Yap', 'Georgios Piliouras', 'Vijay Chandrasekhar']","['cs.LG', 'stat.ML']",2019-02-09 16:57:17+00:00
http://arxiv.org/abs/1902.03427v1,Low-pass filtering as Bayesian inference,"We propose a Bayesian nonparametric method for low-pass filtering that can
naturally handle unevenly-sampled and noise-corrupted observations. The
proposed model is constructed as a latent-factor model for time series, where
the latent factors are Gaussian processes with non-overlapping spectra. With
this construction, the low-pass version of the time series can be identified as
the low-frequency latent component, and therefore it can be found by means of
Bayesian inference. We show that the model admits exact training and can be
implemented with minimal numerical approximations. Finally, the proposed model
is validated against standard linear filters on synthetic and real-world time
series.","['Cristobal Valenzuela', 'Felipe Tobar']","['stat.ML', 'cs.LG', 'eess.SP']",2019-02-09 14:09:32+00:00
http://arxiv.org/abs/1902.05400v1,Data-Driven Vehicle Trajectory Forecasting,"An active area of research is to increase the safety of self-driving
vehicles. Although safety cannot be guarenteed completely, the capability of a
vehicle to predict the future trajectories of its surrounding vehicles could
help ensure this notion of safety to a greater deal. We cast the trajectory
forecast problem in a multi-time step forecasting problem and develop a
Convolutional Neural Network based approach to learn from trajectory sequences
generated from completely raw dataset in real-time. Results show improvement
over baselines.","['Shayan Jawed', 'Eya Boumaiza', 'Josif Grabocka', 'Lars Schmidt-Thieme']","['cs.CV', 'cs.LG', 'cs.RO', 'stat.ML']",2019-02-09 12:09:48+00:00
http://arxiv.org/abs/1902.03394v2,A stochastic version of Stein Variational Gradient Descent for efficient sampling,"We propose in this work RBM-SVGD, a stochastic version of Stein Variational
Gradient Descent (SVGD) method for efficiently sampling from a given
probability measure and thus useful for Bayesian inference. The method is to
apply the Random Batch Method (RBM) for interacting particle systems proposed
by Jin et al to the interacting particle systems in SVGD. While keeping the
behaviors of SVGD, it reduces the computational cost, especially when the
interacting kernel has long range. Numerical examples verify the efficiency of
this new version of SVGD.","['Lei Li', 'Yingzhou Li', 'Jian-Guo Liu', 'Zibu Liu', 'Jianfeng Lu']","['stat.ML', 'cs.LG', 'math.PR', '62D05, 65C35']",2019-02-09 09:22:24+00:00
http://arxiv.org/abs/1902.03393v2,Improved Knowledge Distillation via Teacher Assistant,"Despite the fact that deep neural networks are powerful models and achieve
appealing results on many tasks, they are too large to be deployed on edge
devices like smartphones or embedded sensor nodes. There have been efforts to
compress these networks, and a popular method is knowledge distillation, where
a large (teacher) pre-trained network is used to train a smaller (student)
network. However, in this paper, we show that the student network performance
degrades when the gap between student and teacher is large. Given a fixed
student network, one cannot employ an arbitrarily large teacher, or in other
words, a teacher can effectively transfer its knowledge to students up to a
certain size, not smaller. To alleviate this shortcoming, we introduce
multi-step knowledge distillation, which employs an intermediate-sized network
(teacher assistant) to bridge the gap between the student and the teacher.
Moreover, we study the effect of teacher assistant size and extend the
framework to multi-step distillation. Theoretical analysis and extensive
experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet
architectures substantiate the effectiveness of our proposed approach.","['Seyed-Iman Mirzadeh', 'Mehrdad Farajtabar', 'Ang Li', 'Nir Levine', 'Akihiro Matsukawa', 'Hassan Ghasemzadeh']","['cs.LG', 'cs.AI', 'stat.ML']",2019-02-09 09:06:01+00:00
http://arxiv.org/abs/1902.03376v1,Measuring Patient Similarities via a Deep Architecture with Medical Concept Embedding,"Evaluating the clinical similarities between pairwise patients is a
fundamental problem in healthcare informatics. A proper patient similarity
measure enables various downstream applications, such as cohort study and
treatment comparative effectiveness research. One major carrier for conducting
patient similarity research is Electronic Health Records(EHRs), which are
usually heterogeneous, longitudinal, and sparse. Though existing studies on
learning patient similarity from EHRs have shown being useful in solving real
clinical problems, their applicability is limited due to the lack of medical
interpretations. Moreover, most previous methods assume a vector-based
representation for patients, which typically requires aggregation of medical
events over a certain time period. As a consequence, temporal information will
be lost. In this paper, we propose a patient similarity evaluation framework
based on the temporal matching of longitudinal patient EHRs. Two efficient
methods are presented, unsupervised and supervised, both of which preserve the
temporal properties in EHRs. The supervised scheme takes a convolutional neural
network architecture and learns an optimal representation of patient clinical
records with medical concept embedding. The empirical results on real-world
clinical data demonstrate substantial improvement over the baselines. We make
our code and sample data available for further study.","['Zihao Zhu', 'Changchang Yin', 'Buyue Qian', 'Yu Cheng', 'Jishang Wei', 'Fei Wang']","['stat.ML', 'cs.AI', 'cs.LG']",2019-02-09 05:36:18+00:00
http://arxiv.org/abs/1902.03364v2,Latent Representations of Dynamical Systems: When Two is Better Than One,"A popular approach for predicting the future of dynamical systems involves
mapping them into a lower-dimensional ""latent space"" where prediction is
easier. We show that the information-theoretically optimal approach uses
different mappings for present and future, in contrast to state-of-the-art
machine-learning approaches where both mappings are the same. We illustrate
this dichotomy by predicting the time-evolution of coupled harmonic oscillators
with dissipation and thermal noise, showing how the optimal 2-mapping method
significantly outperforms principal component analysis and all other approaches
that use a single latent representation, and discuss the intuitive reason why
two representations are better than one. We conjecture that a single latent
representation is optimal only for time-reversible processes, not for e.g.
text, speech, music or out-of-equilibrium physical systems.",['Max Tegmark'],"['physics.data-an', 'stat.ML']",2019-02-09 03:14:39+00:00
http://arxiv.org/abs/1902.03362v2,Sinogram interpolation for sparse-view micro-CT with deep learning neural network,"In sparse-view Computed Tomography (CT), only a small number of projection
images are taken around the object, and sinogram interpolation method has a
significant impact on final image quality. When the amount of sparsity (the
amount of missing views in sinogram data) is not high, conventional
interpolation methods have yielded good results. When the amount of sparsity is
high, more advanced sinogram interpolation methods are needed. Recently,
several deep learning (DL) based sinogram interpolation methods have been
proposed. However, those DL-based methods have mostly tested so far on computer
simulated sinogram data rather experimentally acquired sinogram data. In this
study, we developed a sinogram interpolation method for sparse-view micro-CT
based on the combination of U-Net and residual learning. We applied the method
to sinogram data obtained from sparse-view micro-CT experiments, where the
sparsity reached 90%. The interpolated sinogram by the DL neural network was
fed to FBP algorithm for reconstruction. The result shows that both RMSE and
SSIM of CT image are greatly improved. The experimental results demonstrate
that this sinogram interpolation method produce significantly better results
over standard linear interpolation methods when the sinogram data are extremely
sparse.","['Xu Dong', 'Swapnil Vekhande', 'Guohua Cao']","['physics.med-ph', 'cs.LG', 'stat.ML']",2019-02-09 02:46:12+00:00
http://arxiv.org/abs/1902.03356v3,Meta-Curvature,"We propose meta-curvature (MC), a framework to learn curvature information
for better generalization and fast model adaptation. MC expands on the
model-agnostic meta-learner (MAML) by learning to transform the gradients in
the inner optimization such that the transformed gradients achieve better
generalization performance to a new task. For training large scale neural
networks, we decompose the curvature matrix into smaller matrices in a novel
scheme where we capture the dependencies of the model's parameters with a
series of tensor products. We demonstrate the effects of our proposed method on
several few-shot learning tasks and datasets. Without any task specific
techniques and architectures, the proposed method achieves substantial
improvement upon previous MAML variants and outperforms the recent
state-of-the-art methods. Furthermore, we observe faster convergence rates of
the meta-training process. Finally, we present an analysis that explains better
generalization performance with the meta-trained curvature.","['Eunbyung Park', 'Junier B. Oliva']","['cs.LG', 'stat.ML']",2019-02-09 02:34:53+00:00
http://arxiv.org/abs/1902.03350v1,Bayesian Nonparametric Adaptive Spectral Density Estimation for Financial Time Series,"Discrimination between non-stationarity and long-range dependency is a
difficult and long-standing issue in modelling financial time series. This
paper uses an adaptive spectral technique which jointly models the
non-stationarity and dependency of financial time series in a non-parametric
fashion assuming that the time series consists of a finite, but unknown number,
of locally stationary processes, the locations of which are also unknown. The
model allows a non-parametric estimate of the dependency structure by modelling
the auto-covariance function in the spectral domain. All our estimates are made
within a Bayesian framework where we use aReversible Jump Markov Chain Monte
Carlo algorithm for inference. We study the frequentist properties of our
estimates via a simulation study, and present a novel way of generating time
series data from a nonparametric spectrum. Results indicate that our techniques
perform well across a range of data generating processes. We apply our method
to a number of real examples and our results indicate that several financial
time series exhibit both long-range dependency and non-stationarity.","['Nick James', 'Roman Marchant', 'Richard Gerlach', 'Sally Cripps']","['q-fin.ST', 'cs.LG', 'stat.AP', 'stat.ML']",2019-02-09 01:58:48+00:00
http://arxiv.org/abs/1902.05401v2,Improving Deep Image Clustering With Spatial Transformer Layers,"Image clustering is an important but challenging task in machine learning. As
in most image processing areas, the latest improvements came from models based
on the deep learning approach. However, classical deep learning methods have
problems to deal with spatial image transformations like scale and rotation. In
this paper, we propose the use of visual attention techniques to reduce this
problem in image clustering methods. We evaluate the combination of a deep
image clustering model called Deep Adaptive Clustering (DAC) with the Spatial
Transformer Networks (STN). The proposed model is evaluated in the datasets
MNIST and FashionMNIST and outperformed the baseline model.","['Thiago V. M. Souza', 'Cleber Zanchettin']","['cs.CV', 'cs.LG', 'stat.ML']",2019-02-09 01:56:24+00:00
http://arxiv.org/abs/1902.03336v2,Nonlinear Discovery of Slow Molecular Modes using State-Free Reversible VAMPnets,"The success of enhanced sampling molecular simulations that accelerate along
collective variables (CVs) is predicated on the availability of variables
coincident with the slow collective motions governing the long-time
conformational dynamics of a system. It is challenging to intuit these slow CVs
for all but the simplest molecular systems, and their data-driven discovery
directly from molecular simulation trajectories has been a central focus of the
molecular simulation community to both unveil the important physical mechanisms
and to drive enhanced sampling. In this work, we introduce state-free
reversible VAMPnets (SRV) as a deep learning architecture that learns nonlinear
CV approximants to the leading slow eigenfunctions of the spectral
decomposition of the transfer operator that evolves equilibrium-scaled
probability distributions through time. Orthogonality of the learned CVs is
naturally imposed within network training without added regularization. The CVs
are inherently explicit and differentiable functions of the input coordinates
making them well-suited to use in enhanced sampling calculations. We
demonstrate the utility of SRVs in capturing parsimonious nonlinear
representations of complex system dynamics in applications to 1D and 2D toy
systems where the true eigenfunctions are exactly calculable and to molecular
dynamics simulations of alanine dipeptide and the WW domain protein.","['Wei Chen', 'Hythem Sidky', 'Andrew L Ferguson']","['stat.ML', 'cs.LG', 'physics.bio-ph']",2019-02-09 00:17:47+00:00
http://arxiv.org/abs/1902.03327v1,Censored Quantile Regression Forests,"Random forests are powerful non-parametric regression method but are severely
limited in their usage in the presence of randomly censored observations, and
naively applied can exhibit poor predictive performance due to the incurred
biases. Based on a local adaptive representation of random forests, we develop
its regression adjustment for randomly censored regression quantile models.
Regression adjustment is based on new estimating equations that adapt to
censoring and lead to quantile score whenever the data do not exhibit
censoring. The proposed procedure named censored quantile regression forest,
allows us to estimate quantiles of time-to-event without any parametric
modeling assumption. We establish its consistency under mild model
specifications. Numerical studies showcase a clear advantage of the proposed
procedure.","['Alexander Hanbo Li', 'Jelena Bradic']","['stat.ML', 'cs.LG', 'econ.EM', 'stat.ME']",2019-02-08 23:29:50+00:00
http://arxiv.org/abs/1902.03306v2,A simple and efficient architecture for trainable activation functions,"Learning automatically the best activation function for the task is an active
topic in neural network research. At the moment, despite promising results, it
is still difficult to determine a method for learning an activation function
that is at the same time theoretically simple and easy to implement. Moreover,
most of the methods proposed so far introduce new parameters or adopt different
learning techniques. In this work we propose a simple method to obtain trained
activation function which adds to the neural network local subnetworks with a
small amount of neurons. Experiments show that this approach could lead to
better result with respect to using a pre-defined activation function, without
introducing a large amount of extra parameters that need to be learned.","['Andrea Apicella', 'Francesco Isgrò', 'Roberto Prevete']","['cs.LG', 'cs.NE', 'stat.ML']",2019-02-08 22:13:54+00:00
http://arxiv.org/abs/1902.03283v1,Machine learning and chord based feature engineering for genre prediction in popular Brazilian music,"Music genre can be hard to describe: many factors are involved, such as
style, music technique, and historical context. Some genres even have
overlapping characteristics. Looking for a better understanding of how music
genres are related to musical harmonic structures, we gathered data about the
music chords for thousands of popular Brazilian songs. Here, 'popular' does not
only refer to the genre named MPB (Brazilian Popular Music) but to nine
different genres that were considered particular to the Brazilian case. The
main goals of the present work are to extract and engineer harmonically related
features from chords data and to use it to classify popular Brazilian music
genres towards establishing a connection between harmonic relationships and
Brazilian genres. We also emphasize the generalization of the method for
obtaining the data, allowing for the replication and direct extension of this
work. Our final model is a combination of multiple classification trees, also
known as the random forest model. We found that features extracted from
harmonic elements can satisfactorily predict music genre for the Brazilian
case, as well as features obtained from the Spotify API. The variables
considered in this work also give an intuition about how they relate to the
genres.","['Bruna D. Wundervald', 'Walmes M. Zeviani']","['cs.IR', 'cs.LG', 'cs.SD', 'eess.AS', 'stat.ML']",2019-02-08 20:38:18+00:00
http://arxiv.org/abs/1902.03272v2,Scalable Holistic Linear Regression,"We propose a new scalable algorithm for holistic linear regression building
on Bertsimas & King (2016). Specifically, we develop new theory to model
significance and multicollinearity as lazy constraints rather than checking the
conditions iteratively. The resulting algorithm scales with the number of
samples $n$ in the 10,000s, compared to the low 100s in the previous framework.
Computational results on real and synthetic datasets show it greatly improves
from previous algorithms in accuracy, false detection rate, computational time
and scalability.","['Dimitris Bertsimas', 'Michael Lingzhi Li']","['stat.ML', 'cs.LG', 'math.OC']",2019-02-08 20:01:47+00:00
http://arxiv.org/abs/1902.03266v2,Discovering Context Effects from Raw Choice Data,"Many applications in preference learning assume that decisions come from the
maximization of a stable utility function. Yet a large experimental literature
shows that individual choices and judgements can be affected by ""irrelevant""
aspects of the context in which they are made. An important class of such
contexts is the composition of the choice set. In this work, our goal is to
discover such choice set effects from raw choice data. We introduce an
extension of the Multinomial Logit (MNL) model, called the context dependent
random utility model (CDM), which allows for a particular class of choice set
effects. We show that the CDM can be thought of as a second-order approximation
to a general choice system, can be inferred optimally using maximum likelihood
and, importantly, is easily interpretable. We apply the CDM to both real and
simulated choice data to perform principled exploratory analyses for the
presence of choice set effects.","['Arjun Seshadri', 'Alexander Peysakhovich', 'Johan Ugander']","['cs.LG', 'cs.GT', 'stat.ML']",2019-02-08 19:37:00+00:00
http://arxiv.org/abs/1902.03264v3,FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary,"We present a novel method of compression of deep Convolutional Neural
Networks (CNNs) by weight sharing through a new representation of convolutional
filters. The proposed method reduces the number of parameters of each
convolutional layer by learning a 1D vector termed Filter Summary (FS). The
convolutional filters are located in FS as overlapping 1D segments, and nearby
filters in FS share weights in their overlapping regions in a natural way. The
resultant neural network based on such weight sharing scheme, termed Filter
Summary CNNs or FSNet, has a FS in each convolution layer instead of a set of
independent filters in the conventional convolution layer. FSNet has the same
architecture as that of the baseline CNN to be compressed, and each convolution
layer of FSNet has the same number of filters from FS as that of the basline
CNN in the forward process. With compelling computational acceleration ratio,
the parameter space of FSNet is much smaller than that of the baseline CNN. In
addition, FSNet is quantization friendly. FSNet with weight quantization leads
to even higher compression ratio without noticeable performance loss. We
further propose Differentiable FSNet where the way filters share weights is
learned in a differentiable and end-to-end manner. Experiments demonstrate the
effectiveness of FSNet in compression of CNNs for computer vision tasks
including image classification and object detection, and the effectiveness of
DFSNet is evidenced by the task of Neural Architecture Search.","['Yingzhen Yang', 'Jiahui Yu', 'Nebojsa Jojic', 'Jun Huan', 'Thomas S. Huang']","['cs.LG', 'cs.AI', 'stat.ML']",2019-02-08 19:26:46+00:00
http://arxiv.org/abs/1902.03251v2,Invariant-equivariant representation learning for multi-class data,"Representations learnt through deep neural networks tend to be highly
informative, but opaque in terms of what information they learn to encode. We
introduce an approach to probabilistic modelling that learns to represent data
with two separate deep representations: an invariant representation that
encodes the information of the class from which the data belongs, and an
equivariant representation that encodes the symmetry transformation defining
the particular data point within the class manifold (equivariant in the sense
that the representation varies naturally with symmetry transformations). This
approach is based primarily on the strategic routing of data through the two
latent variables, and thus is conceptually transparent, easy to implement, and
in-principle generally applicable to any data comprised of discrete classes of
continuous distributions (e.g. objects in images, topics in language,
individuals in behavioural data). We demonstrate qualitatively compelling
representation learning and competitive quantitative performance, in both
supervised and semi-supervised settings, versus comparable modelling approaches
in the literature with little fine tuning.",['Ilya Feige'],"['stat.ML', 'cs.AI', 'cs.LG']",2019-02-08 19:01:13+00:00
http://arxiv.org/abs/1902.03249v1,Insertion Transformer: Flexible Sequence Generation via Insertion Operations,"We present the Insertion Transformer, an iterative, partially autoregressive
model for sequence generation based on insertion operations. Unlike typical
autoregressive models which rely on a fixed, often left-to-right ordering of
the output, our approach accommodates arbitrary orderings by allowing for
tokens to be inserted anywhere in the sequence during decoding. This
flexibility confers a number of advantages: for instance, not only can our
model be trained to follow specific orderings such as left-to-right generation
or a binary tree traversal, but it can also be trained to maximize entropy over
all valid insertions for robustness. In addition, our model seamlessly
accommodates both fully autoregressive generation (one insertion at a time) and
partially autoregressive generation (simultaneous insertions at multiple
locations). We validate our approach by analyzing its performance on the WMT
2014 English-German machine translation task under various settings for
training and decoding. We find that the Insertion Transformer outperforms many
prior non-autoregressive approaches to translation at comparable or better
levels of parallelism, and successfully recovers the performance of the
original Transformer while requiring only logarithmically many iterations
during decoding.","['Mitchell Stern', 'William Chan', 'Jamie Kiros', 'Jakob Uszkoreit']","['cs.CL', 'cs.LG', 'stat.ML']",2019-02-08 19:00:04+00:00
http://arxiv.org/abs/1902.03229v2,Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces,"Bayesian optimization is known to be difficult to scale to high dimensions,
because the acquisition step requires solving a non-convex optimization problem
in the same search space. In order to scale the method and keep its benefits,
we propose an algorithm (LineBO) that restricts the problem to a sequence of
iteratively chosen one-dimensional sub-problems that can be solved efficiently.
We show that our algorithm converges globally and obtains a fast local rate
when the function is strongly convex. Further, if the objective has an
invariant subspace, our method automatically adapts to the effective dimension
without changing the algorithm. When combined with the SafeOpt algorithm to
solve the sub-problems, we obtain the first safe Bayesian optimization
algorithm with theoretical guarantees applicable in high-dimensional settings.
We evaluate our method on multiple synthetic benchmarks, where we obtain
competitive performance. Further, we deploy our algorithm to optimize the beam
intensity of the Swiss Free Electron Laser with up to 40 parameters while
satisfying safe operation constraints.","['Johannes Kirschner', 'Mojmír Mutný', 'Nicole Hiller', 'Rasmus Ischebeck', 'Andreas Krause']","['cs.LG', 'stat.ML']",2019-02-08 18:41:24+00:00
http://arxiv.org/abs/1902.03228v1,A Smoother Way to Train Structured Prediction Models,"We present a framework to train a structured prediction model by performing
smoothing on the inference algorithm it builds upon. Smoothing overcomes the
non-smoothness inherent to the maximum margin structured prediction objective,
and paves the way for the use of fast primal gradient-based optimization
algorithms. We illustrate the proposed framework by developing a novel primal
incremental optimization algorithm for the structural support vector machine.
The proposed algorithm blends an extrapolation scheme for acceleration and an
adaptive smoothing scheme and builds upon the stochastic variance-reduced
gradient algorithm. We establish its worst-case global complexity bound and
study several practical variants, including extensions to deep structured
prediction. We present experimental results on two real-world problems, namely
named entity recognition and visual object localization. The experimental
results show that the proposed framework allows us to build upon efficient
inference algorithms to develop large-scale optimization algorithms for
structured prediction which can achieve competitive performance on the two
real-world problems.","['Krishna Pillutla', 'Vincent Roulet', 'Sham M. Kakade', 'Zaid Harchaoui']","['stat.ML', 'cs.LG', 'math.OC']",2019-02-08 18:38:51+00:00
http://arxiv.org/abs/1902.03223v3,Robust Streaming PCA,"We consider streaming principal component analysis when the stochastic
data-generating model is subject to perturbations. While existing models assume
a fixed covariance, we adopt a robust perspective where the covariance matrix
belongs to a temporal uncertainty set. Under this setting, we provide
fundamental limits on convergence of any algorithm recovering principal
components. We analyze the convergence of the noisy power method and Oja's
algorithm, both studied for the stationary data generating model, and argue
that the noisy power method is rate-optimal in our setting. Finally, we
demonstrate the validity of our analysis through numerical experiments on
synthetic and real-world dataset.","['Daniel Bienstock', 'Minchan Jeong', 'Apurv Shukla', 'Se-Young Yun']","['stat.ML', 'cs.LG']",2019-02-08 18:31:38+00:00
http://arxiv.org/abs/1902.03210v2,Tensor Variable Elimination for Plated Factor Graphs,"A wide class of machine learning algorithms can be reduced to variable
elimination on factor graphs. While factor graphs provide a unifying notation
for these algorithms, they do not provide a compact way to express repeated
structure when compared to plate diagrams for directed graphical models. To
exploit efficient tensor algebra in graphs with plates of variables, we
generalize undirected factor graphs to plated factor graphs and variable
elimination to a tensor variable elimination algorithm that operates directly
on plated factor graphs. Moreover, we generalize complexity bounds based on
treewidth and characterize the class of plated factor graphs for which
inference is tractable. As an application, we integrate tensor variable
elimination into the Pyro probabilistic programming language to enable exact
inference in discrete latent variable models with repeated structure. We
validate our methods with experiments on both directed and undirected graphical
models, including applications to polyphonic music modeling, animal movement
modeling, and latent sentiment analysis.","['Fritz Obermeyer', 'Eli Bingham', 'Martin Jankowiak', 'Justin Chiu', 'Neeraj Pradhan', 'Alexander Rush', 'Noah Goodman']","['stat.ML', 'cs.LG']",2019-02-08 18:00:08+00:00
http://arxiv.org/abs/1902.03175v2,Scalable Nonparametric Sampling from Multimodal Posteriors with the Posterior Bootstrap,"Increasingly complex datasets pose a number of challenges for Bayesian
inference. Conventional posterior sampling based on Markov chain Monte Carlo
can be too computationally intensive, is serial in nature and mixes poorly
between posterior modes. Further, all models are misspecified, which brings
into question the validity of the conventional Bayesian update. We present a
scalable Bayesian nonparametric learning routine that enables posterior
sampling through the optimization of suitably randomized objective functions. A
Dirichlet process prior on the unknown data distribution accounts for model
misspecification, and admits an embarrassingly parallel posterior bootstrap
algorithm that generates independent and exact samples from the nonparametric
posterior distribution. Our method is particularly adept at sampling from
multimodal posterior distributions via a random restart mechanism. We
demonstrate our method on Gaussian mixture model and sparse logistic regression
examples.","['Edwin Fong', 'Simon Lyddon', 'Chris Holmes']","['stat.ML', 'cs.LG', 'stat.ME']",2019-02-08 16:37:25+00:00
http://arxiv.org/abs/1902.03151v2,Discretization based Solutions for Secure Machine Learning against Adversarial Attacks,"Adversarial examples are perturbed inputs that are designed (from a deep
learning network's (DLN) parameter gradients) to mislead the DLN during test
time. Intuitively, constraining the dimensionality of inputs or parameters of a
network reduces the 'space' in which adversarial examples exist. Guided by this
intuition, we demonstrate that discretization greatly improves the robustness
of DLNs against adversarial attacks. Specifically, discretizing the input space
(or allowed pixel levels from 256 values or 8-bit to 4 values or 2-bit)
extensively improves the adversarial robustness of DLNs for a substantial range
of perturbations for minimal loss in test accuracy. Furthermore, we find that
Binary Neural Networks (BNNs) and related variants are intrinsically more
robust than their full precision counterparts in adversarial scenarios.
Combining input discretization with BNNs furthers the robustness even waiving
the need for adversarial training for certain magnitude of perturbation values.
We evaluate the effect of discretization on MNIST, CIFAR10, CIFAR100 and
Imagenet datasets. Across all datasets, we observe maximal adversarial
resistance with 2-bit input discretization that incurs an adversarial accuracy
loss of just ~1-2% as compared to clean test accuracy.","['Priyadarshini Panda', 'Indranil Chakraborty', 'Kaushik Roy']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2019-02-08 15:38:24+00:00
http://arxiv.org/abs/1902.03149v1,Distributional reinforcement learning with linear function approximation,"Despite many algorithmic advances, our theoretical understanding of practical
distributional reinforcement learning methods remains limited. One exception is
Rowland et al. (2018)'s analysis of the C51 algorithm in terms of the Cram\'er
distance, but their results only apply to the tabular setting and ignore C51's
use of a softmax to produce normalized distributions. In this paper we adapt
the Cram\'er distance to deal with arbitrary vectors. From it we derive a new
distributional algorithm which is fully Cram\'er-based and can be combined to
linear function approximation, with formal guarantees in the context of policy
evaluation. In allowing the model's prediction to be any real vector, we lose
the probabilistic interpretation behind the method, but otherwise maintain the
appealing properties of distributional approaches. To the best of our
knowledge, ours is the first proof of convergence of a distributional algorithm
combined with function approximation. Perhaps surprisingly, our results provide
evidence that Cram\'er-based distributional methods may perform worse than
directly approximating the value function.","['Marc G. Bellemare', 'Nicolas Le Roux', 'Pablo Samuel Castro', 'Subhodeep Moitra']","['cs.LG', 'stat.ML']",2019-02-08 15:31:42+00:00
http://arxiv.org/abs/1902.03127v1,Bounded Fuzzy Possibilistic Method,"This paper introduces Bounded Fuzzy Possibilistic Method (BFPM) by addressing
several issues that previous clustering/classification methods have not
considered. In fuzzy clustering, object's membership values should sum to 1.
Hence, any object may obtain full membership in at most one cluster.
Possibilistic clustering methods remove this restriction. However, BFPM differs
from previous fuzzy and possibilistic clustering approaches by allowing the
membership function to take larger values with respect to all clusters.
Furthermore, in BFPM, a data object can have full membership in multiple
clusters or even in all clusters. BFPM relaxes the boundary conditions
(restrictions) in membership assignment. The proposed methodology satisfies the
necessity of obtaining full memberships and overcomes the issues with
conventional methods on dealing with overlapping. Analysing the objects'
movements from their own cluster to another (mutation) is also proposed in this
paper. BFPM has been applied in different domains in geometry, set theory,
anomaly detection, risk management, diagnosis diseases, and other disciplines.
Validity and comparison indexes have been also used to evaluate the accuracy of
BFPM. BFPM has been evaluated in terms of accuracy, fuzzification constant
(different norms), objects' movement analysis, and covering diversity. The
promising results prove the importance of considering the proposed methodology
in learning methods to track the behaviour of data objects, in addition to
obtain accurate results.",['Hossein Yazdani'],"['cs.LG', 'cs.AI', 'stat.ML']",2019-02-08 14:53:59+00:00
http://arxiv.org/abs/1902.03926v1,Speech enhancement with variational autoencoders and alpha-stable distributions,"This paper focuses on single-channel semi-supervised speech enhancement. We
learn a speaker-independent deep generative speech model using the framework of
variational autoencoders. The noise model remains unsupervised because we do
not assume prior knowledge of the noisy recording environment. In this context,
our contribution is to propose a noise model based on alpha-stable
distributions, instead of the more conventional Gaussian non-negative matrix
factorization approach found in previous studies. We develop a Monte Carlo
expectation-maximization algorithm for estimating the model parameters at test
time. Experimental results show the superiority of the proposed approach both
in terms of perceptual quality and intelligibility of the enhanced speech
signal.","['Simon Leglaive', 'Umut Simsekli', 'Antoine Liutkus', 'Laurent Girin', 'Radu Horaud']","['cs.SD', 'eess.AS', 'stat.ML']",2019-02-08 14:50:47+00:00
http://arxiv.org/abs/1902.02783v3,Humor in Word Embeddings: Cockamamie Gobbledegook for Nincompoops,"While humor is often thought to be beyond the reach of Natural Language
Processing, we show that several aspects of single-word humor correlate with
simple linear directions in Word Embeddings. In particular: (a) the word
vectors capture multiple aspects discussed in humor theories from various
disciplines; (b) each individual's sense of humor can be represented by a
vector, which can predict differences in people's senses of humor on new,
unrated, words; and (c) upon clustering humor ratings of multiple demographic
groups, different humor preferences emerge across the different groups. Humor
ratings are taken from the work of Engelthaler and Hills (2017) as well as from
an original crowdsourcing study of 120,000 words. Our dataset further includes
annotations for the theoretically-motivated humor features we identify.","['Limor Gultchin', 'Genevieve Patterson', 'Nancy Baym', 'Nathaniel Swinger', 'Adam Tauman Kalai']","['cs.CL', 'cs.LG', 'stat.ML']",2019-02-08 14:36:43+00:00
http://arxiv.org/abs/1902.03102v1,Using Background Knowledge to Rank Itemsets,"Assessing the quality of discovered results is an important open problem in
data mining. Such assessment is particularly vital when mining itemsets, since
commonly many of the discovered patterns can be easily explained by background
knowledge. The simplest approach to screen uninteresting patterns is to compare
the observed frequency against the independence model. Since the parameters for
the independence model are the column margins, we can view such screening as a
way of using the column margins as background knowledge.
  In this paper we study techniques for more flexible approaches for infusing
background knowledge. Namely, we show that we can efficiently use additional
knowledge such as row margins, lazarus counts, and bounds of ones. We
demonstrate that these statistics describe forms of data that occur in practice
and have been studied in data mining.
  To infuse the information efficiently we use a maximum entropy approach. In
its general setting, solving a maximum entropy model is infeasible, but we
demonstrate that for our setting it can be solved in polynomial time.
Experiments show that more sophisticated models fit the data better and that
using more information improves the frequency prediction of itemsets.","['Nikolaj Tatti', 'Michael Mampaey']","['cs.LG', 'cs.DS', 'stat.ML']",2019-02-08 14:36:13+00:00
http://arxiv.org/abs/1902.03086v2,Affine Invariant Covariance Estimation for Heavy-Tailed Distributions,"In this work we provide an estimator for the covariance matrix of a
heavy-tailed multivariate distributionWe prove that the proposed estimator
$\widehat{\mathbf{S}}$ admits an \textit{affine-invariant} bound of the form
\[(1-\varepsilon) \mathbf{S} \preccurlyeq \widehat{\mathbf{S}} \preccurlyeq
(1+\varepsilon) \mathbf{S}\]in high probability, where $\mathbf{S}$ is the
unknown covariance matrix, and $\preccurlyeq$ is the positive semidefinite
order on symmetric matrices. The result only requires the existence of
fourth-order moments, and allows for $\varepsilon = O(\sqrt{\kappa^4
d\log(d/\delta)/n})$ where $\kappa^4$ is a measure of kurtosis of the
distribution, $d$ is the dimensionality of the space, $n$ is the sample size,
and $1-\delta$ is the desired confidence level. More generally, we can allow
for regularization with level $\lambda$, then $d$ gets replaced with the
degrees of freedom number. Denoting $\text{cond}(\mathbf{S})$ the condition
number of $\mathbf{S}$, the computational cost of the novel estimator is $O(d^2
n + d^3\log(\text{cond}(\mathbf{S})))$, which is comparable to the cost of the
sample covariance estimator in the statistically interesing regime $n \ge d$.
We consider applications of our estimator to eigenvalue estimation with
relative error, and to ridge regression with heavy-tailed random design.","['Dmitrii Ostrovskii', 'Alessandro Rudi']","['math.ST', 'math.PR', 'stat.ML', 'stat.TH']",2019-02-08 14:13:24+00:00
http://arxiv.org/abs/1902.03081v2,Size Independent Neural Transfer for RDDL Planning,"Neural planners for RDDL MDPs produce deep reactive policies in an offline
fashion. These scale well with large domains, but are sample inefficient and
time-consuming to train from scratch for each new problem. To mitigate this,
recent work has studied neural transfer learning, so that a generic planner
trained on other problems of the same domain can rapidly transfer to a new
problem. However, this approach only transfers across problems of the same
size. We present the first method for neural transfer of RDDL MDPs that can
transfer across problems of different sizes. Our architecture has two key
innovations to achieve size independence: (1) a state encoder, which outputs a
fixed length state embedding by max pooling over varying number of object
embeddings, (2) a single parameter-tied action decoder that projects object
embeddings into action probabilities for the final policy. On the two
challenging RDDL domains of SysAdmin and Game Of Life, our approach powerfully
transfers across problem sizes and has superior learning curves over training
from scratch.","['Sankalp Garg', 'Aniket Bajpai', 'Mausam']","['cs.LG', 'stat.ML']",2019-02-08 14:01:48+00:00
http://arxiv.org/abs/1902.03079v4,Reinforcement Learning from Hierarchical Critics,"In this study, we investigate the use of global information to speed up the
learning process and increase the cumulative rewards of reinforcement learning
(RL) in competition tasks. Within the actor-critic RL, we introduce multiple
cooperative critics from two levels of the hierarchy and propose a
reinforcement learning from hierarchical critics (RLHC) algorithm. In our
approach, each agent receives value information from local and global critics
regarding a competition task and accesses multiple cooperative critics in a
top-down hierarchy. Thus, each agent not only receives low-level details but
also considers coordination from higher levels, thereby obtaining global
information to improve the training performance. Then, we test the proposed
RLHC algorithm against the benchmark algorithm, proximal policy optimisation
(PPO), for two experimental scenarios performed in a Unity environment
consisting of tennis and soccer agents' competitions. The results showed that
RLHC outperforms the benchmark on both competition tasks.","['Zehong Cao', 'Chin-Teng Lin']","['cs.LG', 'cs.MA', 'stat.ML']",2019-02-08 13:55:11+00:00
http://arxiv.org/abs/1902.03077v1,Knowledge Graph Fact Prediction via Knowledge-Enriched Tensor Factorization,"We present a family of novel methods for embedding knowledge graphs into
real-valued tensors. These tensor-based embeddings capture the ordered
relations that are typical in the knowledge graphs represented by semantic web
languages like RDF. Unlike many previous models, our methods can easily use
prior background knowledge provided by users or extracted automatically from
existing knowledge graphs. In addition to providing more robust methods for
knowledge graph embedding, we provide a provably-convergent, linear tensor
factorization algorithm. We demonstrate the efficacy of our models for the task
of predicting new facts across eight different knowledge graphs, achieving
between 5% and 50% relative improvement over existing state-of-the-art
knowledge graph embedding techniques. Our empirical evaluation shows that all
of the tensor decomposition models perform well when the average degree of an
entity in a graph is high, with constraint-based models doing better on graphs
with a small number of highly similar relations and regularization-based models
dominating for graphs with relations of varying degrees of similarity.","['Ankur Padia', 'Kostantinos Kalpakis', 'Francis Ferraro', 'Tim Finin']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2019-02-08 13:42:51+00:00
http://arxiv.org/abs/1902.05402v1,Spectral-Spatial Diffusion Geometry for Hyperspectral Image Clustering,"An unsupervised learning algorithm to cluster hyperspectral image (HSI) data
is proposed that exploits spatially-regularized random walks. Markov diffusions
are defined on the space of HSI spectra with transitions constrained to near
spatial neighbors. The explicit incorporation of spatial regularity into the
diffusion construction leads to smoother random processes that are more adapted
for unsupervised machine learning than those based on spectra alone. The
regularized diffusion process is subsequently used to embed the
high-dimensional HSI into a lower dimensional space through diffusion
distances. Cluster modes are computed using density estimation and diffusion
distances, and all other points are labeled according to these modes. The
proposed method has low computational complexity and performs competitively
against state-of-the-art HSI clustering algorithms on real data. In particular,
the proposed spatial regularization confers an empirical advantage over
non-regularized methods.","['James M. Murphy', 'Mauro Maggioni']","['cs.CV', 'cs.LG', 'stat.ML']",2019-02-08 13:28:30+00:00
http://arxiv.org/abs/1902.03055v3,K-nn active learning under local smoothness condition,"There is a large body of work on convergence rates either in passive or
active learning. Here we outline some of the results that have been obtained,
more specifically in a nonparametric setting under assumptions about the
smoothness and the margin noise. We also discuss the relative merits of these
underlying assumptions by putting active learning in perspective with recent
work on passive learning. We provide a novel active learning algorithm with a
rate of convergence better than in passive learning, using a particular
smoothness assumption customized for $k$-nearest neighbors. This smoothness
assumption provides a dependence on the marginal distribution of the instance
space unlike other recent algorithms.
  Our algorithm thus avoids the strong density assumption that supposes the
existence of the density function of the marginal distribution of the instance
space and is therefore more generally applicable.","['Boris Ndjia Njike', 'Xavier Siebert']","['stat.ML', 'cs.LG']",2019-02-08 12:32:49+00:00
http://arxiv.org/abs/1902.03047v1,Collaboration based Multi-Label Learning,"It is well-known that exploiting label correlations is crucially important to
multi-label learning. Most of the existing approaches take label correlations
as prior knowledge, which may not correctly characterize the real relationships
among labels. Besides, label correlations are normally used to regularize the
hypothesis space, while the final predictions are not explicitly correlated. In
this paper, we suggest that for each individual label, the final prediction
involves the collaboration between its own prediction and the predictions of
other labels. Based on this assumption, we first propose a novel method to
learn the label correlations via sparse reconstruction in the label space.
Then, by seamlessly integrating the learned label correlations into model
training, we propose a novel multi-label learning approach that aims to
explicitly account for the correlated predictions of labels while training the
desired model simultaneously. Extensive experimental results show that our
approach outperforms the state-of-the-art counterparts.","['Lei Feng', 'Bo An', 'Shuo He']","['cs.LG', 'stat.ML']",2019-02-08 12:18:27+00:00
http://arxiv.org/abs/1902.03045v1,Partial Label Learning with Self-Guided Retraining,"Partial label learning deals with the problem where each training instance is
assigned a set of candidate labels, only one of which is correct. This paper
provides the first attempt to leverage the idea of self-training for dealing
with partially labeled examples. Specifically, we propose a unified formulation
with proper constraints to train the desired model and perform pseudo-labeling
jointly. For pseudo-labeling, unlike traditional self-training that manually
differentiates the ground-truth label with enough high confidence, we introduce
the maximum infinity norm regularization on the modeling outputs to
automatically achieve this consideratum, which results in a convex-concave
optimization problem. We show that optimizing this convex-concave problem is
equivalent to solving a set of quadratic programming (QP) problems. By
proposing an upper-bound surrogate objective function, we turn to solving only
one QP problem for improving the optimization efficiency. Extensive experiments
on synthesized and real-world datasets demonstrate that the proposed approach
significantly outperforms the state-of-the-art partial label learning
approaches.","['Lei Feng', 'Bo An']","['cs.LG', 'stat.ML']",2019-02-08 12:12:14+00:00
http://arxiv.org/abs/1902.03043v2,A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion from Heartbeat,"Automatic prediction of emotion promises to revolutionise human-computer
interaction. Recent trends involve fusion of multiple data modalities - audio,
visual, and physiological - to classify emotional state. However, in practice,
collection of physiological data `in the wild' is currently limited to
heartbeat time series of the kind generated by affordable wearable heart
monitors. Furthermore, real-world applications of emotion prediction often
require some measure of uncertainty over model output, in order to inform
downstream decision-making. We present here an end-to-end deep learning model
for classifying emotional valence from unimodal heartbeat time series. We
further propose a Bayesian framework for modelling uncertainty over these
valence predictions, and describe a probabilistic procedure for choosing to
accept or reject model output according to the intended application. We
benchmarked our framework against two established datasets and achieved peak
classification accuracy of 90%. These results lay the foundation for
applications of affective computing in real-world domains such as healthcare,
where a high premium is placed on non-invasive collection of data, and
predictive certainty.","['Ross Harper', 'Joshua Southern']","['cs.LG', 'cs.HC', 'stat.ML']",2019-02-08 12:10:45+00:00
http://arxiv.org/abs/1902.03035v1,Bandit Principal Component Analysis,"We consider a partial-feedback variant of the well-studied online PCA problem
where a learner attempts to predict a sequence of $d$-dimensional vectors in
terms of a quadratic loss, while only having limited feedback about the
environment's choices. We focus on a natural notion of bandit feedback where
the learner only observes the loss associated with its own prediction. Based on
the classical observation that this decision-making problem can be lifted to
the space of density matrices, we propose an algorithm that is shown to achieve
a regret of $O(d^{3/2}\sqrt{T})$ after $T$ rounds in the worst case. We also
prove data-dependent bounds that improve on the basic result when the loss
matrices of the environment have bounded rank or the loss of the best action is
bounded. One version of our algorithm runs in $O(d)$ time per trial which
massively improves over every previously known online PCA method. We complement
these results by a lower bound of $\Omega(d\sqrt{T})$.","['Wojciech Kotłowski', 'Gergely Neu']","['cs.LG', 'stat.ML']",2019-02-08 11:57:48+00:00
http://arxiv.org/abs/1902.03002v4,Covariance and Correlation Kernels on a Graph in the Generalized Bag-of-Paths Formalism,"This work derives closed-form expressions computing the expectation of
co-presence and of number of co-occurrences of nodes on paths sampled from a
network according to general path weights (a bag of paths). The underlying idea
is that two nodes are considered as similar when they often appear together on
(preferably short) paths of the network. The different expressions are obtained
for both regular and hitting paths and serve as a basis for computing new
covariance and correlation measures between nodes, which are valid positive
semi-definite kernels on a graph. Experiments on semi-supervised classification
problems show that the introduced similarity measures provide competitive
results compared to other state-of-the-art distance and similarity measures
between nodes.","['Guillaume Guex', 'Sylvain Courtain', 'Marco Saerens']","['cs.LG', 'stat.ML']",2019-02-08 10:16:02+00:00
http://arxiv.org/abs/1902.03000v1,Distribution of residual autocorrelations for multiplicative seasonal ARMA models with uncorrelated but non-independent error terms,"In this paper we consider portmanteau tests for testing the adequacy of
multiplicative seasonal autoregressive moving-average (SARMA) models under the
assumption that the errors are uncorrelated but not necessarily independent.We
relax the standard independence assumption on the error term in order to extend
the range of application of the SARMA models.We study the asymptotic
distributions of residual and normalized residual empirical autocovariances and
autocorrelations underweak assumptions on the noise. We establish the
asymptotic behaviour of the proposed statistics. A set of Monte Carlo
experiments and an application to monthly mean total sunspot number are
presented.","['Yacouba Boubacar Maïnassara', 'Abdoulkarim Ilmi Amir']","['math.ST', 'stat.AP', 'stat.ML', 'stat.TH']",2019-02-08 10:07:55+00:00
http://arxiv.org/abs/1902.06679v2,Link Prediction via Higher-Order Motif Features,"Link prediction requires predicting which new links are likely to appear in a
graph. Being able to predict unseen links with good accuracy has important
applications in several domains such as social media, security, transportation,
and recommendation systems. A common approach is to use features based on the
common neighbors of an unconnected pair of nodes to predict whether the pair
will form a link in the future. In this paper, we present an approach for link
prediction that relies on higher-order analysis of the graph topology, well
beyond common neighbors. We treat the link prediction problem as a supervised
classification problem, and we propose a set of features that depend on the
patterns or motifs that a pair of nodes occurs in. By using motifs of sizes 3,
4, and 5, our approach captures a high level of detail about the graph topology
within the neighborhood of the pair of nodes, which leads to a higher
classification accuracy. In addition to proposing the use of motif-based
features, we also propose two optimizations related to constructing the
classification dataset from the graph. First, to ensure that positive and
negative examples are treated equally when extracting features, we propose
adding the negative examples to the graph as an alternative to the common
approach of removing the positive ones. Second, we show that it is important to
control for the shortest-path distance when sampling pairs of nodes to form
negative examples, since the difficulty of prediction varies with the
shortest-path distance. We experimentally demonstrate that using off-the-shelf
classifiers with a well constructed classification dataset results in up to 10
percentage points increase in accuracy over prior topology-based and feature
learning methods.","['Ghadeer Abuoda', 'Gianmarco De Francisci Morales', 'Ashraf Aboulnaga']","['cs.SI', 'cs.LG', 'stat.ML']",2019-02-08 10:01:04+00:00
http://arxiv.org/abs/1902.02992v2,A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning,"Hyperbolic space is a geometry that is known to be well-suited for
representation learning of data with an underlying hierarchical structure. In
this paper, we present a novel hyperbolic distribution called
\textit{pseudo-hyperbolic Gaussian}, a Gaussian-like distribution on hyperbolic
space whose density can be evaluated analytically and differentiated with
respect to the parameters. Our distribution enables the gradient-based learning
of the probabilistic models on hyperbolic space that could never have been
considered before. Also, we can sample from this hyperbolic probability
distribution without resorting to auxiliary means like rejection sampling. As
applications of our distribution, we develop a hyperbolic-analog of variational
autoencoder and a method of probabilistic word embedding on hyperbolic space.
We demonstrate the efficacy of our distribution on various datasets including
MNIST, Atari 2600 Breakout, and WordNet.","['Yoshihiro Nagano', 'Shoichiro Yamaguchi', 'Yasuhiro Fujita', 'Masanori Koyama']","['stat.ML', 'cs.LG']",2019-02-08 09:42:06+00:00
http://arxiv.org/abs/1902.02979v4,Fair Decisions Despite Imperfect Predictions,"Consequential decisions are increasingly informed by sophisticated
data-driven predictive models. However, to consistently learn accurate
predictive models, one needs access to ground truth labels. Unfortunately, in
practice, labels may only exist conditional on certain decisions---if a loan is
denied, there is not even an option for the individual to pay back the loan.
Hence, the observed data distribution depends on how decisions are being made.
In this paper, we show that in this selective labels setting, learning a
predictor directly only from available labeled data is suboptimal in terms of
both fairness and utility. To avoid this undesirable behavior, we propose to
directly learn decision policies that maximize utility under fairness
constraints and thereby take into account how decisions affect which data is
observed in the future. Our results suggest the need for a paradigm shift in
the context of fair machine learning from the currently prevalent idea of
simply building predictive models from a single static dataset via risk
minimization, to a more interactive notion of ""learning to decide"". In
particular, such policies should not entirely neglect part of the input space,
drawing connections to explore/exploit tradeoffs in reinforcement learning,
data missingness, and potential outcomes in causal inference. Experiments on
synthetic and real-world data illustrate the favorable properties of learning
to decide in terms of utility and fairness.","['Niki Kilbertus', 'Manuel Gomez-Rodriguez', 'Bernhard Schölkopf', 'Krikamol Muandet', 'Isabel Valera']","['cs.LG', 'cs.CY', 'stat.ML']",2019-02-08 08:50:31+00:00
http://arxiv.org/abs/1902.02970v1,Binarized Knowledge Graph Embeddings,"Tensor factorization has become an increasingly popular approach to knowledge
graph completion(KGC), which is the task of automatically predicting missing
facts in a knowledge graph. However, even with a simple model like
CANDECOMP/PARAFAC(CP) tensor decomposition, KGC on existing knowledge graphs is
impractical in resource-limited environments, as a large amount of memory is
required to store parameters represented as 32-bit or 64-bit floating point
numbers. This limitation is expected to become more stringent as existing
knowledge graphs, which are already huge, keep steadily growing in scale. To
reduce the memory requirement, we present a method for binarizing the
parameters of the CP tensor decomposition by introducing a quantization
function to the optimization problem. This method replaces floating
point-valued parameters with binary ones after training, which drastically
reduces the model size at run time. We investigate the trade-off between the
quality and size of tensor factorization models for several KGC benchmark
datasets. In our experiments, the proposed method successfully reduced the
model size by more than an order of magnitude while maintaining the task
performance. Moreover, a fast score computation technique can be developed with
bitwise operations.","['Koki Kishimoto', 'Katsuhiko Hayashi', 'Genki Akai', 'Masashi Shimbo', 'Kazunori Komatani']","['cs.LG', 'cs.IR', 'stat.ML']",2019-02-08 08:27:25+00:00
http://arxiv.org/abs/1902.02953v2,Correlated bandits or: How to minimize mean-squared error online,"While the objective in traditional multi-armed bandit problems is to find the
arm with the highest mean, in many settings, finding an arm that best captures
information about other arms is of interest. This objective, however, requires
learning the underlying correlation structure and not just the means of the
arms. Sensors placement for industrial surveillance and cellular network
monitoring are a few applications, where the underlying correlation structure
plays an important role. Motivated by such applications, we formulate the
correlated bandit problem, where the objective is to find the arm with the
lowest mean-squared error (MSE) in estimating all the arms. To this end, we
derive first an MSE estimator, based on sample variances and covariances, and
show that our estimator exponentially concentrates around the true MSE. Under a
best-arm identification framework, we propose a successive rejects type
algorithm and provide bounds on the probability of error in identifying the
best arm. Using minmax theory, we also derive fundamental performance limits
for the correlated bandit problem.","['Vinay Praneeth Boda', 'Prashanth L. A']","['cs.LG', 'stat.ML']",2019-02-08 06:38:29+00:00
http://arxiv.org/abs/1902.02950v2,Differentiable Physics-informed Graph Networks,"While physics conveys knowledge of nature built from an interplay between
observations and theory, it has been considered less importantly in deep neural
networks. Especially, there are few works leveraging physics behaviors when the
knowledge is given less explicitly. In this work, we propose a novel
architecture called Differentiable Physics-informed Graph Networks (DPGN) to
incorporate implicit physics knowledge which is given from domain experts by
informing it in latent space. Using the concept of DPGN, we demonstrate that
climate prediction tasks are significantly improved. Besides the experiment
results, we validate the effectiveness of the proposed module and provide
further applications of DPGN, such as inductive learning and multistep
predictions.","['Sungyong Seo', 'Yan Liu']","['cs.LG', 'stat.ML']",2019-02-08 06:24:41+00:00
http://arxiv.org/abs/1902.02948v1,EILearn: Learning Incrementally Using Previous Knowledge Obtained From an Ensemble of Classifiers,"We propose an algorithm for incremental learning of classifiers. The proposed
method enables an ensemble of classifiers to learn incrementally by
accommodating new training data. We use an effective mechanism to overcome the
stability-plasticity dilemma. In incremental learning, the general convention
is to use only the knowledge acquired in the previous phase but not the
previously seen data. We follow this convention by retaining the previously
acquired knowledge which is relevant and using it along with the current data.
The performance of each classifier is monitored to eliminate the poorly
performing classifiers in the subsequent phases. Experimental results show that
the proposed approach outperforms the existing incremental learning approaches.","['Shivang Agarwal', 'C. Ravindranath Chowdary', 'Shripriya Maheshwari']","['cs.LG', 'stat.ML']",2019-02-08 06:06:22+00:00
http://arxiv.org/abs/1902.02947v1,Understanding the One-Pixel Attack: Propagation Maps and Locality Analysis,"Deep neural networks were shown to be vulnerable to single pixel
modifications. However, the reason behind such phenomena has never been
elucidated. Here, we propose Propagation Maps which show the influence of the
perturbation in each layer of the network. Propagation Maps reveal that even in
extremely deep networks such as Resnet, modification in one pixel easily
propagates until the last layer. In fact, this initial local perturbation is
also shown to spread becoming a global one and reaching absolute difference
values that are close to the maximum value of the original feature maps in a
given layer. Moreover, we do a locality analysis in which we demonstrate that
nearby pixels of the perturbed one in the one-pixel attack tend to share the
same vulnerability, revealing that the main vulnerability lies in neither
neurons nor pixels but receptive fields. Hopefully, the analysis conducted in
this work together with a new technique called propagation maps shall shed
light into the inner workings of other adversarial samples and be the basis of
new defense systems to come.","['Danilo Vasconcellos Vargas', 'Jiawei Su']","['cs.LG', 'cs.CR', 'cs.CV', 'cs.NE', 'stat.ML']",2019-02-08 06:06:01+00:00
http://arxiv.org/abs/1902.02940v1,Generating the support with extreme value losses,"When optimizing against the mean loss over a distribution of predictions in
the context of a regression task, then even if there is a distribution of
targets the optimal prediction distribution is always a delta function at a
single value. Methods of constructing generative models need to overcome this
tendency. We consider a simple method of summarizing the prediction error, such
that the optimal strategy corresponds to outputting a distribution of
predictions with a support that matches the support of the distribution of
targets --- optimizing against the minimal value of the loss given a set of
samples from the prediction distribution, rather than the mean. We show that
models trained against this loss learn to capture the support of the target
distribution and, when combined with an auxiliary classifier-like prediction
task, can be projected via rejection sampling to reproduce the full
distribution of targets. The resulting method works well compared to other
generative modeling approaches particularly in low dimensional spaces with
highly non-trivial distributions, due to mode collapse solutions being globally
suboptimal with respect to the extreme value loss. However, the method is less
suited to high-dimensional spaces such as images due to the scaling of the
number of samples needed in order to accurately estimate the extreme value loss
when the dimension of the data manifold becomes large.",['Nicholas Guttenberg'],"['cs.LG', 'stat.ML']",2019-02-08 04:58:50+00:00
http://arxiv.org/abs/1902.02934v1,Mode Collapse and Regularity of Optimal Transportation Maps,"This work builds the connection between the regularity theory of optimal
transportation map, Monge-Amp\`{e}re equation and GANs, which gives a theoretic
understanding of the major drawbacks of GANs: convergence difficulty and mode
collapse.
  According to the regularity theory of Monge-Amp\`{e}re equation, if the
support of the target measure is disconnected or just non-convex, the optimal
transportation mapping is discontinuous. General DNNs can only approximate
continuous mappings. This intrinsic conflict leads to the convergence
difficulty and mode collapse in GANs.
  We test our hypothesis that the supports of real data distribution are in
general non-convex, therefore the discontinuity is unavoidable using an
Autoencoder combined with discrete optimal transportation map (AE-OT framework)
on the CelebA data set. The testing result is positive. Furthermore, we propose
to approximate the continuous Brenier potential directly based on discrete
Brenier theory to tackle mode collapse. Comparing with existing method, this
method is more accurate and effective.","['Na Lei', 'Yang Guo', 'Dongsheng An', 'Xin Qi', 'Zhongxuan Luo', 'Shing-Tung Yau', 'Xianfeng Gu']","['cs.LG', 'stat.ML']",2019-02-08 04:30:38+00:00
http://arxiv.org/abs/1902.02930v1,Multi-task Learning for Target-dependent Sentiment Classification,"Detecting and aggregating sentiments toward people, organizations, and events
expressed in unstructured social media have become critical text mining
operations. Early systems detected sentiments over whole passages, whereas more
recently, target-specific sentiments have been of greater interest. In this
paper, we present MTTDSC, a multi-task target-dependent sentiment
classification system that is informed by feature representation learnt for the
related auxiliary task of passage-level sentiment classification. The auxiliary
task uses a gated recurrent unit (GRU) and pools GRU states, followed by an
auxiliary fully-connected layer that outputs passage-level predictions. In the
main task, these GRUs contribute auxiliary per-token representations over and
above word embeddings. The main task has its own, separate GRUs. The auxiliary
and main GRUs send their states to a different fully connected layer, trained
for the main task. Extensive experiments using two auxiliary datasets and three
benchmark datasets (of which one is new, introduced by us) for the main task
demonstrate that MTTDSC outperforms state-of-the-art baselines. Using
word-level sensitivity analysis, we present anecdotal evidence that prior
systems can make incorrect target-specific predictions because they miss
sentiments expressed by words independent of target.","['Divam Gupta', 'Kushagra Singh', 'Soumen Chakrabarti', 'Tanmoy Chakraborty']","['cs.LG', 'stat.ML']",2019-02-08 03:58:09+00:00
http://arxiv.org/abs/1902.02918v2,Certified Adversarial Robustness via Randomized Smoothing,"We show how to turn any classifier that classifies well under Gaussian noise
into a new classifier that is certifiably robust to adversarial perturbations
under the $\ell_2$ norm. This ""randomized smoothing"" technique has been
proposed recently in the literature, but existing guarantees are loose. We
prove a tight robustness guarantee in $\ell_2$ norm for smoothing with Gaussian
noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a
certified top-1 accuracy of 49% under adversarial perturbations with $\ell_2$
norm less than 0.5 (=127/255). No certified defense has been shown feasible on
ImageNet except for smoothing. On smaller-scale datasets where competing
approaches to certified $\ell_2$ robustness are viable, smoothing delivers
higher certified accuracies. Our strong empirical results suggest that
randomized smoothing is a promising direction for future research into
adversarially robust classification. Code and models are available at
http://github.com/locuslab/smoothing.","['Jeremy M Cohen', 'Elan Rosenfeld', 'J. Zico Kolter']","['cs.LG', 'stat.ML']",2019-02-08 02:08:19+00:00
http://arxiv.org/abs/1902.02907v1,Source Traces for Temporal Difference Learning,"This paper motivates and develops source traces for temporal difference (TD)
learning in the tabular setting. Source traces are like eligibility traces, but
model potential histories rather than immediate ones. This allows TD errors to
be propagated to potential causal states and leads to faster generalization.
Source traces can be thought of as the model-based, backward view of successor
representations (SR), and share many of the same benefits. This view, however,
suggests several new ideas. First, a TD($\lambda$)-like source learning
algorithm is proposed and its convergence is proven. Then, a novel algorithm
for learning the source map (or SR matrix) is developed and shown to outperform
the previous algorithm. Finally, various approaches to using the source/SR
model are explored, and it is shown that source traces can be effectively
combined with other model-based methods like Dyna and experience replay.",['Silviu Pitis'],"['cs.LG', 'cs.AI', 'stat.ML']",2019-02-08 01:21:17+00:00
http://arxiv.org/abs/1902.02904v1,Modeling Heterogeneity in Mode-Switching Behavior Under a Mobility-on-Demand Transit System: An Interpretable Machine Learning Approach,"Recent years have witnessed an increased focus on interpretability and the
use of machine learning to inform policy analysis and decision making. This
paper applies machine learning to examine travel behavior and, in particular,
on modeling changes in travel modes when individuals are presented with a novel
(on-demand) mobility option. It addresses the following question: Can machine
learning be applied to model individual taste heterogeneity (preference
heterogeneity for travel modes and response heterogeneity to travel attributes)
in travel mode choice? This paper first develops a high-accuracy classifier to
predict mode-switching behavior under a hypothetical Mobility-on-Demand Transit
system (i.e., stated-preference data), which represents the case study
underlying this research. We show that this classifier naturally captures
individual heterogeneity available in the data. Moreover, the paper derives
insights on heterogeneous switching behaviors through the generation of
marginal effects and elasticities by current travel mode, partial dependence
plots, and individual conditional expectation plots. The paper also proposes
two new model-agnostic interpretation tools for machine learning, i.e.,
conditional partial dependence plots and conditional individual partial
dependence plots, specifically designed to examine response heterogeneity. The
results on the case study show that the machine-learning classifier, together
with model-agnostic interpretation tools, provides valuable insights on travel
mode switching behavior for different individuals and population segments. For
example, the existing drivers are more sensitive to additional pickups than
people using other travel modes, and current transit users are generally
willing to share rides but reluctant to take any additional transfers.","['Xilei Zhao', 'Xiang Yan', 'Pascal Van Hentenryck']","['cs.LG', 'cs.AI', 'stat.ML']",2019-02-08 01:15:09+00:00
http://arxiv.org/abs/1902.03240v1,Land Use Classification Using Multi-neighborhood LBPs,"In this paper we propose the use of multiple local binary patterns(LBPs) to
effectively classify land use images. We use the UC Merced 21 class land use
image dataset. Task is challenging for classification as the dataset contains
intra class variability and inter class similarities. Our proposed method of
using multi-neighborhood LBPs combined with nearest neighbor classifier is able
to achieve an accuracy of 77.76%. Further class wise analysis is conducted and
suitable suggestion are made for further improvements to classification
accuracy.",['Harjot Singh Parmar'],"['cs.LG', 'stat.ML']",2019-02-07 23:37:27+00:00
http://arxiv.org/abs/1902.02885v2,Contextual Online False Discovery Rate Control,"Multiple hypothesis testing, a situation when we wish to consider many
hypotheses, is a core problem in statistical inference that arises in almost
every scientific field. In this setting, controlling the false discovery rate
(FDR), which is the expected proportion of type I error, is an important
challenge for making meaningful inferences. In this paper, we consider the
problem of controlling FDR in an online manner. Concretely, we consider an
ordered, possibly infinite, sequence of hypotheses, arriving one at each
timestep, and for each hypothesis we observe a p-value along with a set of
features specific to that hypothesis. The decision whether or not to reject the
current hypothesis must be made immediately at each timestep, before the next
hypothesis is observed. The model of multi-dimensional feature set provides a
very general way of leveraging the auxiliary information in the data which
helps in maximizing the number of discoveries.
  We propose a new class of powerful online testing procedures, where the
rejections thresholds (significance levels) are learnt sequentially by
incorporating contextual information and previous results. We prove that any
rule in this class controls online FDR under some standard assumptions. We then
focus on a subclass of these procedures, based on weighting significance
levels, to derive a practical algorithm that learns a parametric weight
function in an online fashion to gain more discoveries. We also theoretically
prove, in a stylized setting, that our proposed procedures would lead to an
increase in the achieved statistical power over a popular online testing
procedure proposed by Javanmard & Montanari (2018). Finally, we demonstrate the
favorable performance of our procedure, by comparing it to state-of-the-art
online multiple testing procedures, on both synthetic data and real data
generated from different applications.","['Shiyun Chen', 'Shiva Kasiviswanathan']","['math.ST', 'stat.ML', 'stat.TH']",2019-02-07 23:24:19+00:00
http://arxiv.org/abs/1902.02881v1,Combining learning rate decay and weight decay with complexity gradient descent - Part I,"The role of $L^2$ regularization, in the specific case of deep neural
networks rather than more traditional machine learning models, is still not
fully elucidated. We hypothesize that this complex interplay is due to the
combination of overparameterization and high dimensional phenomena that take
place during training and make it unamenable to standard convex optimization
methods. Using insights from statistical physics and random fields theory, we
introduce a parameter factoring in both the level of the loss function and its
remaining nonconvexity: the \emph{complexity}. We proceed to show that it is
desirable to proceed with \emph{complexity gradient descent}. We then show how
to use this intuition to derive novel and efficient annealing schemes for the
strength of $L^2$ regularization when performing standard stochastic gradient
descent in deep neural networks.","['Pierre H. Richemond', 'Yike Guo']","['cs.LG', 'stat.ML']",2019-02-07 23:12:57+00:00
http://arxiv.org/abs/1902.02880v1,Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks,"Can multilayer neural networks -- typically constructed as highly complex
structures with many nonlinearly activated neurons across layers -- behave in a
non-trivial way that yet simplifies away a major part of their complexities? In
this work, we uncover a phenomenon in which the behavior of these complex
networks -- under suitable scalings and stochastic gradient descent dynamics --
becomes independent of the number of neurons as this number grows sufficiently
large. We develop a formalism in which this many-neurons limiting behavior is
captured by a set of equations, thereby exposing a previously unknown operating
regime of these networks. While the current pursuit is mathematically
non-rigorous, it is complemented with several experiments that validate the
existence of this behavior.",['Phan-Minh Nguyen'],"['cs.LG', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'stat.ML']",2019-02-07 23:06:41+00:00
http://arxiv.org/abs/1902.02860v3,Crop Yield Prediction Using Deep Neural Networks,"Crop yield is a highly complex trait determined by multiple factors such as
genotype, environment, and their interactions. Accurate yield prediction
requires fundamental understanding of the functional relationship between yield
and these interactive factors, and to reveal such relationship requires both
comprehensive datasets and powerful algorithms. In the 2018 Syngenta Crop
Challenge, Syngenta released several large datasets that recorded the genotype
and yield performances of 2,267 maize hybrids planted in 2,247 locations
between 2008 and 2016 and asked participants to predict the yield performance
in 2017. As one of the winning teams, we designed a deep neural network (DNN)
approach that took advantage of state-of-the-art modeling and solution
techniques. Our model was found to have a superior prediction accuracy, with a
root-mean-square-error (RMSE) being 12% of the average yield and 50% of the
standard deviation for the validation dataset using predicted weather data.
With perfect weather data, the RMSE would be reduced to 11% of the average
yield and 46% of the standard deviation. We also performed feature selection
based on the trained DNN model, which successfully decreased the dimension of
the input space without significant drop in the prediction accuracy. Our
computational results suggested that this model significantly outperformed
other popular methods such as Lasso, shallow neural networks (SNN), and
regression tree (RT). The results also revealed that environmental factors had
a greater effect on the crop yield than genotype.","['Saeed Khaki', 'Lizhi Wang']","['cs.LG', 'stat.AP', 'stat.ML']",2019-02-07 21:54:00+00:00
http://arxiv.org/abs/1902.02823v1,Compatible Natural Gradient Policy Search,"Trust-region methods have yielded state-of-the-art results in policy search.
A common approach is to use KL-divergence to bound the region of trust
resulting in a natural gradient policy update. We show that the natural
gradient and trust region optimization are equivalent if we use the natural
parameterization of a standard exponential policy distribution in combination
with compatible value function approximation. Moreover, we show that standard
natural gradient updates may reduce the entropy of the policy according to a
wrong schedule leading to premature convergence. To control entropy reduction
we introduce a new policy search method called compatible policy search (COPOS)
which bounds entropy loss. The experimental results show that COPOS yields
state-of-the-art results in challenging continuous control tasks and in
discrete partially observable tasks.","['Joni Pajarinen', 'Hong Linh Thai', 'Riad Akrour', 'Jan Peters', 'Gerhard Neumann']","['cs.LG', 'stat.ML']",2019-02-07 20:03:17+00:00
http://arxiv.org/abs/1902.02812v3,Cooperative Training of Fast Thinking Initializer and Slow Thinking Solver for Conditional Learning,"This paper studies the problem of learning the conditional distribution of a
high-dimensional output given an input, where the output and input may belong
to two different domains, e.g., the output is a photo image and the input is a
sketch image. We solve this problem by cooperative training of a fast thinking
initializer and slow thinking solver. The initializer generates the output
directly by a non-linear transformation of the input as well as a noise vector
that accounts for latent variability in the output. The slow thinking solver
learns an objective function in the form of a conditional energy function, so
that the output can be generated by optimizing the objective function, or more
rigorously by sampling from the conditional energy-based model. We propose to
learn the two models jointly, where the fast thinking initializer serves to
initialize the sampling of the slow thinking solver, and the solver refines the
initial output by an iterative algorithm. The solver learns from the difference
between the refined output and the observed output, while the initializer
learns from how the solver refines its initial output. We demonstrate the
effectiveness of the proposed method on various conditional learning tasks,
e.g., class-to-image generation, image-to-image translation, and image
recovery. The advantage of our method over GAN-based methods is that our method
is equipped with a slow thinking process that refines the solution guided by a
learned objective function.","['Jianwen Xie', 'Zilong Zheng', 'Xiaolin Fang', 'Song-Chun Zhu', 'Ying Nian Wu']","['stat.ML', 'cs.LG']",2019-02-07 19:30:44+00:00
http://arxiv.org/abs/1902.02808v1,ML Health: Fitness Tracking for Production Models,"Deployment of machine learning (ML) algorithms in production for extended
periods of time has uncovered new challenges such as monitoring and management
of real-time prediction quality of a model in the absence of labels. However,
such tracking is imperative to prevent catastrophic business outcomes resulting
from incorrect predictions. The scale of these deployments makes manual
monitoring prohibitive, making automated techniques to track and raise alerts
imperative. We present a framework, ML Health, for tracking potential drops in
the predictive performance of ML models in the absence of labels. The framework
employs diagnostic methods to generate alerts for further investigation. We
develop one such method to monitor potential problems when production data
patterns do not match training data distributions. We demonstrate that our
method performs better than standard ""distance metrics"", such as RMSE,
KL-Divergence, and Wasserstein at detecting issues with mismatched data sets.
Finally, we present a working system that incorporates the ML Health approach
to monitor and manage ML deployments within a realistic full production ML
lifecycle.","['Sindhu Ghanta', 'Sriram Subramanian', 'Lior Khermosh', 'Swaminathan Sundararaman', 'Harshil Shah', 'Yakov Goldberg', 'Drew Roselli', 'Nisha Talagala']","['cs.LG', 'stat.ML']",2019-02-07 19:15:51+00:00
http://arxiv.org/abs/1902.02778v1,KLUCB Approach to Copeland Bandits,"Multi-armed bandit(MAB) problem is a reinforcement learning framework where
an agent tries to maximise her profit by proper selection of actions through
absolute feedback for each action. The dueling bandits problem is a variation
of MAB problem in which an agent chooses a pair of actions and receives
relative feedback for the chosen action pair. The dueling bandits problem is
well suited for modelling a setting in which it is not possible to provide
quantitative feedback for each action, but qualitative feedback for each action
is preferred as in the case of human feedback. The dueling bandits have been
successfully applied in applications such as online rank elicitation,
information retrieval, search engine improvement and clinical online
recommendation. We propose a new method called Sup-KLUCB for K-armed dueling
bandit problem specifically Copeland bandit problem by converting it into a
standard MAB problem. Instead of using MAB algorithm independently for each
action in a pair as in Sparring and in Self-Sparring algorithms, we combine a
pair of action and use it as one action. Previous UCB algorithms such as
Relative Upper Confidence Bound(RUCB) can be applied only in case of Condorcet
dueling bandits, whereas this algorithm applies to general Copeland dueling
bandits, including Condorcet dueling bandits as a special case. Our empirical
results outperform state of the art Double Thompson Sampling(DTS) in case of
Copeland dueling bandits.","['Nischal Agrawal', 'Prasanna Chaporkar']","['cs.LG', 'stat.ML']",2019-02-07 18:59:16+00:00
http://arxiv.org/abs/1902.02767v2,Hybrid Models with Deep and Invertible Features,"We propose a neural hybrid model consisting of a linear model defined on a
set of features computed by a deep, invertible transformation (i.e. a
normalizing flow). An attractive property of our model is that both
p(features), the density of the features, and p(targets | features), the
predictive distribution, can be computed exactly in a single feed-forward pass.
We show that our hybrid model, despite the invertibility constraints, achieves
similar accuracy to purely predictive models. Moreover the generative component
remains a good model of the input features despite the hybrid optimization
objective. This offers additional capabilities such as detection of
out-of-distribution inputs and enabling semi-supervised learning. The
availability of the exact joint density p(targets, features) also allows us to
compute many quantities readily, making our hybrid model a useful building
block for downstream applications of probabilistic deep learning.","['Eric Nalisnick', 'Akihiro Matsukawa', 'Yee Whye Teh', 'Dilan Gorur', 'Balaji Lakshminarayanan']","['cs.LG', 'stat.ML']",2019-02-07 18:49:47+00:00
http://arxiv.org/abs/1902.02725v1,Metaoptimization on a Distributed System for Deep Reinforcement Learning,"Training intelligent agents through reinforcement learning is a notoriously
unstable procedure. Massive parallelization on GPUs and distributed systems has
been exploited to generate a large amount of training experiences and
consequently reduce instabilities, but the success of training remains strongly
influenced by the choice of the hyperparameters. To overcome this issue, we
introduce HyperTrick, a new metaoptimization algorithm, and show its effective
application to tune hyperparameters in the case of deep reinforcement learning,
while learning to play different Atari games on a distributed system. Our
analysis provides evidence of the interaction between the identification of the
optimal hyperparameters and the learned policy, that is typical of the case of
metaoptimization for deep reinforcement learning. When compared with
state-of-the-art metaoptimization algorithms, HyperTrick is characterized by a
simpler implementation and it allows learning similar policies, while making a
more effective use of the computational resources in a distributed system.","['Greg Heinrich', 'Iuri Frosio']","['cs.LG', 'stat.ML']",2019-02-07 16:48:31+00:00
http://arxiv.org/abs/1902.02721v4,Variational Recurrent Neural Networks for Graph Classification,"We address the problem of graph classification based only on structural
information. Inspired by natural language processing techniques (NLP), our
model sequentially embeds information to estimate class membership
probabilities. Besides, we experiment with NLP-like variational regularization
techniques, making the model predict the next node in the sequence as it reads
it. We experimentally show that our model achieves state-of-the-art
classification results on several standard molecular datasets. Finally, we
perform a qualitative analysis and give some insights on whether the node
prediction helps the model better classify graphs.","['Edouard Pineau', 'Nathan de Lara']","['cs.LG', 'stat.ML']",2019-02-07 16:34:53+00:00
http://arxiv.org/abs/1902.02719v2,Sparse Regression and Adaptive Feature Generation for the Discovery of Dynamical Systems,"We study the performance of sparse regression methods and propose new
techniques to distill the governing equations of dynamical systems from data.
We first look at the generic methodology of learning interpretable equation
forms from data, proposed by Brunton et al., followed by performance of LASSO
for this purpose. We then propose a new algorithm that uses the dual of LASSO
optimization for higher accuracy and stability. In the second part, we propose
a novel algorithm that learns the candidate function library in a completely
data-driven manner to distill the governing equations of the dynamical system.
This is achieved via sequentially thresholded ridge regression (STRidge) over a
orthogonal polynomial space. The performance of the three discussed methods is
illustrated by looking the Lorenz 63 system and the quadratic Lorenz system.",['Chinmay S. Kulkarni'],"['cs.LG', 'stat.ML']",2019-02-07 16:32:45+00:00
http://arxiv.org/abs/1902.02671v2,BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,"Multi-task learning shares information between related tasks, sometimes
reducing the number of parameters required. State-of-the-art results across
multiple natural language understanding tasks in the GLUE benchmark have
previously used transfer from a single large task: unsupervised pre-training
with BERT, where a separate BERT model was fine-tuned for each task. We explore
multi-task approaches that share a single BERT model with a small number of
additional task-specific parameters. Using new adaptation modules, PALs or
`projected attention layers', we match the performance of separately fine-tuned
models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain
state-of-the-art results on the Recognizing Textual Entailment dataset.","['Asa Cooper Stickland', 'Iain Murray']","['cs.LG', 'cs.CL', 'stat.ML']",2019-02-07 15:05:46+00:00
http://arxiv.org/abs/1902.02661v4,"Bayesian Reinforcement Learning via Deep, Sparse Sampling","We address the problem of Bayesian reinforcement learning using efficient
model-based online planning. We propose an optimism-free Bayes-adaptive
algorithm to induce deeper and sparser exploration with a theoretical bound on
its performance relative to the Bayes optimal policy, with a lower
computational complexity. The main novelty is the use of a candidate policy
generator, to generate long-term options in the planning tree (over beliefs),
which allows us to create much sparser and deeper trees. Experimental results
on different environments show that in comparison to the state-of-the-art, our
algorithm is both computationally more efficient, and obtains significantly
higher reward in discrete environments.","['Divya Grover', 'Debabrota Basu', 'Christos Dimitrakakis']","['cs.LG', 'cs.AI', 'stat.ML']",2019-02-07 14:52:37+00:00
http://arxiv.org/abs/1902.02660v1,Bounds for the VC Dimension of 1NN Prototype Sets,"In Statistical Learning, the Vapnik-Chervonenkis (VC) dimension is an
important combinatorial property of classifiers. To our knowledge, no
theoretical results yet exist for the VC dimension of edited nearest-neighbour
(1NN) classifiers with reference set of fixed size. Related theoretical results
are scattered in the literature and their implications have not been made
explicit. We collect some relevant results and use them to provide explicit
lower and upper bounds for the VC dimension of 1NN classifiers with a prototype
set of fixed size. We discuss the implications of these bounds for the size of
training set needed to learn such a classifier to a given accuracy. Further, we
provide a new lower bound for the two-dimensional case, based on a new
geometrical argument.","['Iain A. D. Gunn', 'Ludmila I. Kuncheva']","['cs.LG', 'stat.ML']",2019-02-07 14:52:33+00:00
http://arxiv.org/abs/1902.03083v2,Hide and Speak: Towards Deep Neural Networks for Speech Steganography,"Steganography is the science of hiding a secret message within an ordinary
public message, which is referred to as Carrier. Traditionally, digital signal
processing techniques, such as least significant bit encoding, were used for
hiding messages. In this paper, we explore the use of deep neural networks as
steganographic functions for speech data. We showed that steganography models
proposed for vision are less suitable for speech, and propose a new model that
includes the short-time Fourier transform and inverse-short-time Fourier
transform as differentiable layers within the network, thus imposing a vital
constraint on the network outputs. We empirically demonstrated the
effectiveness of the proposed method comparing to deep learning based on
several speech datasets and analyzed the results quantitatively and
qualitatively. Moreover, we showed that the proposed approach could be applied
to conceal multiple messages in a single carrier using multiple decoders or a
single conditional decoder. Lastly, we evaluated our model under different
channel distortions. Qualitative experiments suggest that modifications to the
carrier are unnoticeable by human listeners and that the decoded messages are
highly intelligible.","['Felix Kreuk', 'Yossi Adi', 'Bhiksha Raj', 'Rita Singh', 'Joseph Keshet']","['cs.SD', 'cs.CR', 'cs.LG', 'eess.AS', 'stat.ML']",2019-02-07 13:48:28+00:00
http://arxiv.org/abs/1902.02603v2,Radial and Directional Posteriors for Bayesian Neural Networks,"We propose a new variational family for Bayesian neural networks. We
decompose the variational posterior into two components, where the radial
component captures the strength of each neuron in terms of its magnitude; while
the directional component captures the statistical dependencies among the
weight parameters. The dependencies learned via the directional density provide
better modeling performance compared to the widely-used Gaussian
mean-field-type variational family. In addition, the strength of input and
output neurons learned via the radial density provides a structured way to
compress neural networks. Indeed, experiments show that our variational family
improves predictive performance and yields compressed networks simultaneously.","['Changyong Oh', 'Kamil Adamczewski', 'Mijung Park']","['stat.ML', 'cs.LG']",2019-02-07 13:06:43+00:00
http://arxiv.org/abs/1902.03124v1,Heterogeneous Edge Embeddings for Friend Recommendation,"We propose a friend recommendation system (an application of link prediction)
using edge embeddings on social networks. Most real-world social networks are
multi-graphs, where different kinds of relationships (e.g. chat, friendship)
are possible between a pair of users. Existing network embedding techniques do
not leverage signals from different edge types and thus perform inadequately on
link prediction in such networks. We propose a method to mine network
representation that effectively exploits heterogeneity in multi-graphs. We
evaluate our model on a real-world, active social network where this system is
deployed for friend recommendation for millions of users. Our method
outperforms various state-of-the-art baselines on Hike's social network in
terms of accuracy as well as user satisfaction.","['Janu Verma', 'Srishti Gupta', 'Debdoot Mukherjee', 'Tanmoy Chakraborty']","['cs.SI', 'cs.LG', 'stat.ML']",2019-02-07 10:42:08+00:00
http://arxiv.org/abs/1902.02554v1,Random Matrix Improved Covariance Estimation for a Large Class of Metrics,"Relying on recent advances in statistical estimation of covariance distances
based on random matrix theory, this article proposes an improved covariance and
precision matrix estimation for a wide family of metrics. The method is shown
to largely outperform the sample covariance matrix estimate and to compete with
state-of-the-art methods, while at the same time being computationally simpler.
Applications to linear and quadratic discriminant analyses also demonstrate
significant gains, therefore suggesting practical interest to statistical
machine learning.","['Malik Tiomoko', 'Florent Bouchard', 'Guillaume Ginholac', 'Romain Couillet']","['stat.ML', 'cs.LG']",2019-02-07 10:24:26+00:00
http://arxiv.org/abs/1902.02544v1,Online Clustering by Penalized Weighted GMM,"With the dawn of the Big Data era, data sets are growing rapidly. Data is
streaming from everywhere - from cameras, mobile phones, cars, and other
electronic devices. Clustering streaming data is a very challenging problem.
Unlike the traditional clustering algorithms where the dataset can be stored
and scanned multiple times, clustering streaming data has to satisfy
constraints such as limit memory size, real-time response, unknown data
statistics and an unknown number of clusters. In this paper, we present a novel
online clustering algorithm which can be used to cluster streaming data without
knowing the number of clusters a priori. Results on both synthetic and real
datasets show that the proposed algorithm produces partitions which are close
to what you could get if you clustered the whole data at one time.","['Shlomo Bugdary', 'Shay Maymon']","['cs.LG', 'cs.CV', 'stat.ML']",2019-02-07 09:50:08+00:00
http://arxiv.org/abs/1902.02533v1,Ensemble Prediction of Time to Event Outcomes with Competing Risks: A Case Study of Surgical Complications in Crohn's Disease,"We develop a novel algorithm to predict the occurrence of major abdominal
surgery within 5 years following Crohn's disease diagnosis using a panel of 29
baseline covariates from the Swedish population registers. We model
pseudo-observations based on the Aalen-Johansen estimator of the cause-specific
cumulative incidence with an ensemble of modern machine learning approaches.
Pseudo-observation pre-processing easily extends all existing or new machine
learning procedures to right-censored event history data. We propose
pseudo-observation based estimators for the area under the time varying ROC
curve, for optimizing the ensemble, and the predictiveness curve, for
evaluating and summarizing predictive performance.","['Michael C Sachs', 'Andrea Discacciati', 'Åsa Everhov', 'Ola Olén', 'Erin E Gabriel']","['stat.AP', 'stat.ML']",2019-02-07 09:14:40+00:00
http://arxiv.org/abs/1902.03125v2,High-performance stock index trading: making effective use of a deep LSTM neural network,"We present a deep long short-term memory (LSTM)-based neural network for
predicting asset prices, together with a successful trading strategy for
generating profits based on the model's predictions. Our work is motivated by
the fact that the effectiveness of any prediction model is inherently coupled
to the trading strategy it is used with, and vise versa. This highlights the
difficulty in developing models and strategies which are jointly optimal, but
also points to avenues of investigation which are broader than prevailing
approaches. Our LSTM model is structurally simple and generates predictions
based on price observations over a modest number of past trading days. The
model's architecture is tuned to promote profitability, as opposed to accuracy,
under a strategy that does not trade simply based on whether the price is
predicted to rise or fall, but rather takes advantage of the distribution of
predicted returns, and the fact that a prediction's position within that
distribution carries useful information about the expected profitability of a
trade. The proposed model and trading strategy were tested on the S&P 500, Dow
Jones Industrial Average (DJIA), NASDAQ and Russel 2000 stock indices, and
achieved cumulative returns of 340%, 185%, 371% and 360%, respectively, over
2010-2018, far outperforming the benchmark buy-and-hold strategy as well as
other recent efforts.","['Chariton Chalvatzis', 'Dimitrios Hristu-Varsakelis']","['q-fin.ST', 'cs.LG', 'stat.ML']",2019-02-07 09:08:07+00:00
http://arxiv.org/abs/1902.02527v1,Adaptive Posterior Learning: few-shot learning with a surprise-based memory module,"The ability to generalize quickly from few observations is crucial for
intelligent systems. In this paper we introduce APL, an algorithm that
approximates probability distributions by remembering the most surprising
observations it has encountered. These past observations are recalled from an
external memory module and processed by a decoder network that can combine
information from different memory slots to generalize beyond direct recall. We
show this algorithm can perform as well as state of the art baselines on
few-shot classification benchmarks with a smaller memory footprint. In
addition, its memory compression allows it to scale to thousands of unknown
labels. Finally, we introduce a meta-learning reasoning task which is more
challenging than direct classification. In this setting, APL is able to
generalize with fewer than one example per class via deductive reasoning.","['Tiago Ramalho', 'Marta Garnelo']","['cs.LG', 'stat.ML']",2019-02-07 09:00:51+00:00
http://arxiv.org/abs/1902.02517v1,Model Selection for Simulator-based Statistical Models: A Kernel Approach,"We propose a novel approach to model selection for simulator-based
statistical models. The proposed approach defines a mixture of candidate
models, and then iteratively updates the weight coefficients for those models
as well as the parameters in each model simultaneously; this is done by
recursively applying Bayes' rule, using the recently proposed kernel recursive
ABC algorithm. The practical advantage of the method is that it can be used
even when a modeler lacks appropriate prior knowledge about the parameters in
each model. We demonstrate the effectiveness of the proposed approach with a
number of experiments, including model selection for dynamical systems in
ecology and epidemiology.","['Takafumi Kajihara', 'Motonobu Kanagawa', 'Yuuki Nakaguchi', 'Kanishka Khandelwal', 'Kenji Fukumiziu']","['stat.ML', 'cs.LG']",2019-02-07 08:22:31+00:00
http://arxiv.org/abs/1902.02509v4,Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso,"Sparsity promoting norms are frequently used in high dimensional regression.
A limitation of such Lasso-type estimators is that the optimal regularization
parameter depends on the unknown noise level. Estimators such as the
concomitant Lasso address this dependence by jointly estimating the noise level
and the regression coefficients. Additionally, in many applications, the data
is obtained by averaging multiple measurements: this reduces the noise
variance, but it dramatically reduces sample sizes and prevents refined noise
modeling. In this work, we propose a concomitant estimator that can cope with
complex noise structure by using non-averaged measurements. The resulting
optimization problem is convex and amenable, thanks to smoothing theory, to
state-of-the-art optimization techniques that leverage the sparsity of the
solutions. Practical benefits are demonstrated on toy datasets, realistic
simulated data and real neuroimaging data.","['Quentin Bertrand', 'Mathurin Massias', 'Alexandre Gramfort', 'Joseph Salmon']","['stat.ML', 'cs.LG', 'math.OC', 'stat.AP']",2019-02-07 07:44:54+00:00
http://arxiv.org/abs/1902.02502v2,Spatial Mixture Models with Learnable Deep Priors for Perceptual Grouping,"Humans perceive the seemingly chaotic world in a structured and compositional
way with the prerequisite of being able to segregate conceptual entities from
the complex visual scenes. The mechanism of grouping basic visual elements of
scenes into conceptual entities is termed as perceptual grouping. In this work,
we propose a new type of spatial mixture models with learnable priors for
perceptual grouping. Different from existing methods, the proposed method
disentangles the attributes of an object into ``shape'' and ``appearance''
which are modeled separately by the mixture weights and the mixture components.
More specifically, each object in the visual scene is fully characterized by
one latent representation, which is in turn transformed into parameters of the
mixture weight and the mixture component by two neural networks. The mixture
weights focus on modeling spatial dependencies (i.e., shape) and the mixture
components deal with intra-object variations (i.e., appearance). In addition,
the background is separately modeled as a special component complementary to
the foreground objects. Our extensive empirical tests on two perceptual
grouping datasets demonstrate that the proposed method outperforms the
state-of-the-art methods under most experimental configurations. The learned
conceptual entities are generalizable to novel visual scenes and insensitive to
the diversity of objects. Code is available at
https://github.com/jinyangyuan/learnable-deep-priors.","['Jinyang Yuan', 'Bin Li', 'Xiangyang Xue']","['cs.LG', 'stat.ML']",2019-02-07 07:33:12+00:00
http://arxiv.org/abs/1902.02495v3,Cost-Effective Incentive Allocation via Structured Counterfactual Inference,"We address a practical problem ubiquitous in modern marketing campaigns, in
which a central agent tries to learn a policy for allocating strategic
financial incentives to customers and observes only bandit feedback. In
contrast to traditional policy optimization frameworks, we take into account
the additional reward structure and budget constraints common in this setting,
and develop a new two-step method for solving this constrained counterfactual
policy optimization problem. Our method first casts the reward estimation
problem as a domain adaptation problem with supplementary structure, and then
subsequently uses the estimators for optimizing the policy with constraints. We
also establish theoretical error bounds for our estimation procedure and we
empirically show that the approach leads to significant improvement on both
synthetic and real datasets.","['Romain Lopez', 'Chenchen Li', 'Xiang Yan', 'Junwu Xiong', 'Michael I. Jordan', 'Yuan Qi', 'Le Song']","['stat.ML', 'cs.LG']",2019-02-07 07:02:34+00:00
http://arxiv.org/abs/1902.02476v2,A Simple Baseline for Bayesian Uncertainty in Deep Learning,"We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose
approach for uncertainty representation and calibration in deep learning.
Stochastic Weight Averaging (SWA), which computes the first moment of
stochastic gradient descent (SGD) iterates with a modified learning rate
schedule, has recently been shown to improve generalization in deep learning.
With SWAG, we fit a Gaussian using the SWA solution as the first moment and a
low rank plus diagonal covariance also derived from the SGD iterates, forming
an approximate posterior distribution over neural network weights; we then
sample from this Gaussian distribution to perform Bayesian model averaging. We
empirically find that SWAG approximates the shape of the true posterior, in
accordance with results describing the stationary distribution of SGD iterates.
Moreover, we demonstrate that SWAG performs well on a wide variety of tasks,
including out of sample detection, calibration, and transfer learning, in
comparison to many popular alternatives including MC dropout, KFAC Laplace,
SGLD, and temperature scaling.","['Wesley Maddox', 'Timur Garipov', 'Pavel Izmailov', 'Dmitry Vetrov', 'Andrew Gordon Wilson']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2019-02-07 05:15:46+00:00
http://arxiv.org/abs/1902.03129v3,Towards Automatic Concept-based Explanations,"Interpretability has become an important topic of research as more machine
learning (ML) models are deployed and widely used to make important decisions.
  Most of the current explanation methods provide explanations through feature
importance scores, which identify features that are important for each
individual input. However, how to systematically summarize and interpret such
per sample feature importance scores itself is challenging. In this work, we
propose principles and desiderata for \emph{concept} based explanation, which
goes beyond per-sample features to identify higher-level human-understandable
concepts that apply across the entire dataset. We develop a new algorithm, ACE,
to automatically extract visual concepts. Our systematic experiments
demonstrate that \alg discovers concepts that are human-meaningful, coherent
and important for the neural network's predictions.","['Amirata Ghorbani', 'James Wexler', 'James Zou', 'Been Kim']","['stat.ML', 'cs.CV', 'cs.LG']",2019-02-07 03:18:54+00:00
http://arxiv.org/abs/1902.02449v3,Empirically Accelerating Scaled Gradient Projection Using Deep Neural Network For Inverse Problems In Image Processing,"Recently, deep neural networks (DNNs) have shown advantages in accelerating
optimization algorithms. One approach is to unfold finite number of iterations
of conventional optimization algorithms and to learn parameters in the
algorithms. However, these are forward methods and are indeed neither iterative
nor convergent. Here, we present a novel DNN-based convergent iterative
algorithm that accelerates conventional optimization algorithms. We train a DNN
to yield parameters in scaled gradient projection method. So far, these
parameters have been chosen heuristically, but have shown to be crucial for
good empirical performance. In simulation results, the proposed method
significantly improves the empirical convergence rate over conventional
optimization methods for various large-scale inverse problems in image
processing.","['Byung Hyun Lee', 'Se Young Chun']","['cs.LG', 'cs.CV', 'stat.ML']",2019-02-07 02:19:53+00:00
