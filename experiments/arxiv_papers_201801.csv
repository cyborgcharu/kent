id,title,abstract,authors,categories,date
http://arxiv.org/abs/1802.05355v1,The Role of Information Complexity and Randomization in Representation Learning,"A grand challenge in representation learning is to learn the different
explanatory factors of variation behind the high dimen- sional data. Encoder
models are often determined to optimize performance on training data when the
real objective is to generalize well to unseen data. Although there is enough
numerical evidence suggesting that noise injection (during training) at the
representation level might improve the generalization ability of encoders, an
information-theoretic understanding of this principle remains elusive. This
paper presents a sample-dependent bound on the generalization gap of the
cross-entropy loss that scales with the information complexity (IC) of the
representations, meaning the mutual information between inputs and their
representations. The IC is empirically investigated for standard multi-layer
neural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the
gap and the IC appear to be in direct correlation, suggesting that SGD selects
encoders to implicitly minimize the IC. We specialize the IC to study the role
of Dropout on the generalization capacity of deep encoders which is shown to be
directly related to the encoder capacity, being a measure of the
distinguishability among samples from their representations. Our results
support some recent regularization methods.","['Matías Vera', 'Pablo Piantanida', 'Leonardo Rey Vega']","['stat.ML', 'cs.LG']",2018-02-14 23:31:11+00:00
http://arxiv.org/abs/1802.05351v3,Stealing Hyperparameters in Machine Learning,"Hyperparameters are critical in machine learning, as different
hyperparameters often result in models with significantly different
performance. Hyperparameters may be deemed confidential because of their
commercial value and the confidentiality of the proprietary algorithms that the
learner uses to learn them. In this work, we propose attacks on stealing the
hyperparameters that are learned by a learner. We call our attacks
hyperparameter stealing attacks. Our attacks are applicable to a variety of
popular machine learning algorithms such as ridge regression, logistic
regression, support vector machine, and neural network. We evaluate the
effectiveness of our attacks both theoretically and empirically. For instance,
we evaluate our attacks on Amazon Machine Learning. Our results demonstrate
that our attacks can accurately steal hyperparameters. We also study
countermeasures. Our results highlight the need for new defenses against our
hyperparameter stealing attacks for certain machine learning algorithms.","['Binghui Wang', 'Neil Zhenqiang Gong']","['cs.CR', 'cs.LG', 'stat.ML']",2018-02-14 22:58:31+00:00
http://arxiv.org/abs/1802.05335v3,Multimodal Generative Models for Scalable Weakly-Supervised Learning,"Multiple modalities often co-occur when describing natural phenomena.
Learning a joint representation of these modalities should yield deeper and
more useful representations. Previous generative approaches to multi-modal
input either do not learn a joint distribution or require additional
computation to handle missing data. Here, we introduce a multimodal variational
autoencoder (MVAE) that uses a product-of-experts inference network and a
sub-sampled training paradigm to solve the multi-modal inference problem.
Notably, our model shares parameters to efficiently learn under any combination
of missing modalities. We apply the MVAE on four datasets and match
state-of-the-art performance using many fewer parameters. In addition, we show
that the MVAE is directly applicable to weakly-supervised learning, and is
robust to incomplete supervision. We then consider two case studies, one of
learning image transformations---edge detection, colorization,
segmentation---as a set of modalities, followed by one of machine translation
between two languages. We find appealing results across this range of tasks.","['Mike Wu', 'Noah Goodman']","['cs.LG', 'stat.ML']",2018-02-14 22:08:05+00:00
http://arxiv.org/abs/1802.05319v1,500+ Times Faster Than Deep Learning (A Case Study Exploring Faster Methods for Text Mining StackOverflow),"Deep learning methods are useful for high-dimensional data and are becoming
widely used in many areas of software engineering. Deep learners utilizes
extensive computational power and can take a long time to train-- making it
difficult to widely validate and repeat and improve their results. Further,
they are not the best solution in all domains. For example, recent results show
that for finding related Stack Overflow posts, a tuned SVM performs similarly
to a deep learner, but is significantly faster to train. This paper extends
that recent result by clustering the dataset, then tuning very learners within
each cluster. This approach is over 500 times faster than deep learning (and
over 900 times faster if we use all the cores on a standard laptop computer).
Significantly, this faster approach generates classifiers nearly as good
(within 2\% F1 Score) as the much slower deep learning method. Hence we
recommend this faster methods since it is much easier to reproduce and utilizes
far fewer CPU resources. More generally, we recommend that before researchers
release research results, that they compare their supposedly sophisticated
methods against simpler alternatives (e.g applying simpler learners to build
local models).","['Suvodeep Majumder', 'Nikhila Balaji', 'Katie Brey', 'Wei Fu', 'Tim Menzies']","['cs.SE', 'cs.LG', 'stat.ML']",2018-02-14 20:57:48+00:00
http://arxiv.org/abs/1802.05315v2,Online Learning for Non-Stationary A/B Tests,"The rollout of new versions of a feature in modern applications is a manual
multi-stage process, as the feature is released to ever larger groups of users,
while its performance is carefully monitored. This kind of A/B testing is
ubiquitous, but suboptimal, as the monitoring requires heavy human
intervention, is not guaranteed to capture consistent, but short-term
fluctuations in performance, and is inefficient, as better versions take a long
time to reach the full population.
  In this work we formulate this question as that of expert learning, and give
a new algorithm Follow-The-Best-Interval, FTBI, that works in dynamic,
non-stationary environments. Our approach is practical, simple, and efficient,
and has rigorous guarantees on its performance. Finally, we perform a thorough
evaluation on synthetic and real world datasets and show that our approach
outperforms current state-of-the-art methods.","['Andrés Muñoz Medina', 'Sergei Vassilvitskii', 'Dong Yin']","['cs.LG', 'stat.ML']",2018-02-14 20:42:51+00:00
http://arxiv.org/abs/1802.05313v2,Reinforcement Learning from Imperfect Demonstrations,"Robust real-world learning should benefit from both demonstrations and
interactions with the environment. Current approaches to learning from
demonstration and reward perform supervised learning on expert demonstration
data and use reinforcement learning to further improve performance based on the
reward received from the environment. These tasks have divergent losses which
are difficult to jointly optimize and such methods can be very sensitive to
noisy demonstrations. We propose a unified reinforcement learning algorithm,
Normalized Actor-Critic (NAC), that effectively normalizes the Q-function,
reducing the Q-values of actions unseen in the demonstration data. NAC learns
an initial policy network from demonstrations and refines the policy in the
environment, surpassing the demonstrator's performance. Crucially, both
learning from demonstration and interactive refinement use the same objective,
unlike prior approaches that combine distinct supervised and reinforcement
losses. This makes NAC robust to suboptimal demonstration data since the method
is not forced to mimic all of the examples in the dataset. We show that our
unified reinforcement learning algorithm can learn robustly and outperform
existing baselines when evaluated on several realistic driving games.","['Yang Gao', 'Huazhe Xu', 'Ji Lin', 'Fisher Yu', 'Sergey Levine', 'Trevor Darrell']","['cs.AI', 'cs.LG', 'stat.ML']",2018-02-14 20:37:38+00:00
http://arxiv.org/abs/1802.05312v2,Learning Deep Disentangled Embeddings with the F-Statistic Loss,"Deep-embedding methods aim to discover representations of a domain that make
explicit the domain's class structure and thereby support few-shot learning.
Disentangling methods aim to make explicit compositional or factorial
structure. We combine these two active but independent lines of research and
propose a new paradigm suitable for both goals. We propose and evaluate a novel
loss function based on the $F$ statistic, which describes the separation of two
or more distributions. By ensuring that distinct classes are well separated on
a subset of embedding dimensions, we obtain embeddings that are useful for
few-shot learning. By not requiring separation on all dimensions, we encourage
the discovery of disentangled representations. Our embedding method matches or
beats state-of-the-art, as evaluated by performance on recall@$k$ and few-shot
learning tasks. Our method also obtains performance superior to a variety of
alternatives on disentangling, as evaluated by two key properties of a
disentangled representation: modularity and explicitness. The goal of our work
is to obtain more interpretable, manipulable, and generalizable deep
representations of concepts and categories.","['Karl Ridgeway', 'Michael C. Mozer']","['cs.LG', 'cs.AI', 'stat.ML']",2018-02-14 20:28:38+00:00
http://arxiv.org/abs/1802.05283v4,NeVAE: A Deep Generative Model for Molecular Graphs,"Deep generative models have been praised for their ability to learn smooth
latent representation of images, text, and audio, which can then be used to
generate new, plausible data. However, current generative models are unable to
work with molecular graphs due to their unique characteristics-their underlying
structure is not Euclidean or grid-like, they remain isomorphic under
permutation of the nodes labels, and they come with a different number of nodes
and edges. In this paper, we first propose a novel variational autoencoder for
molecular graphs, whose encoder and decoder are specially designed to account
for the above properties by means of several technical innovations. Moreover,
in contrast with the state of the art, our decoder is able to provide the
spatial coordinates of the atoms of the molecules it generates. Then, we
develop a gradient-based algorithm to optimize the decoder of our model so that
it learns to generate molecules that maximize the value of certain property of
interest and, given a molecule of interest, it is able to optimize the spatial
configuration of its atoms for greater stability. Experiments reveal that our
variational autoencoder can discover plausible, diverse and novel molecules
more effectively than several state of the art models. Moreover, for several
properties of interest, our optimized decoder is able to identify molecules
with property values 121% higher than those identified by several state of the
art methods based on Bayesian optimization and reinforcement learning","['Bidisha Samanta', 'Abir De', 'Gourhari Jana', 'Pratim Kumar Chattaraj', 'Niloy Ganguly', 'Manuel Gomez-Rodriguez']","['cs.LG', 'physics.soc-ph', 'stat.ML']",2018-02-14 19:00:34+00:00
http://arxiv.org/abs/1802.05251v1,Differentially Private Empirical Risk Minimization Revisited: Faster and More General,"In this paper we study the differentially private Empirical Risk Minimization
(ERM) problem in different settings. For smooth (strongly) convex loss function
with or without (non)-smooth regularization, we give algorithms that achieve
either optimal or near optimal utility bounds with less gradient complexity
compared with previous work. For ERM with smooth convex loss function in
high-dimensional ($p\gg n$) setting, we give an algorithm which achieves the
upper bound with less gradient complexity than previous ones. At last, we
generalize the expected excess empirical risk from convex loss functions to
non-convex ones satisfying the Polyak-Lojasiewicz condition and give a tighter
upper bound on the utility than the one in \cite{ijcai2017-548}.","['Di Wang', 'Minwei Ye', 'Jinhui Xu']","['cs.LG', 'cs.CR', 'stat.ML']",2018-02-14 18:20:48+00:00
http://arxiv.org/abs/1802.05249v2,Distributionally Robust Submodular Maximization,"Submodular functions have applications throughout machine learning, but in
many settings, we do not have direct access to the underlying function $f$. We
focus on stochastic functions that are given as an expectation of functions
over a distribution $P$. In practice, we often have only a limited set of
samples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by
maximizing the sum of $f_i$. However, this ignores generalization to the true
(unknown) distribution. In this paper, we achieve better performance on the
actual underlying function $f$ by directly optimizing a combination of bias and
variance. Algorithmically, we accomplish this by showing how to carry out
distributionally robust optimization (DRO) for submodular functions, providing
efficient algorithms backed by theoretical guarantees which leverage several
novel contributions to the general theory of DRO. We also show compelling
empirical evidence that DRO improves generalization to the unknown stochastic
submodular function.","['Matthew Staib', 'Bryan Wilder', 'Stefanie Jegelka']","['cs.LG', 'math.OC', 'stat.ML']",2018-02-14 18:16:43+00:00
http://arxiv.org/abs/1802.05234v1,Necessary and Sufficient Null Space Condition for Nuclear Norm Minimization in Low-Rank Matrix Recovery,"Low-rank matrix recovery has found many applications in science and
engineering such as machine learning, signal processing, collaborative
filtering, system identification, and Euclidean embedding. But the low-rank
matrix recovery problem is an NP hard problem and thus challenging. A commonly
used heuristic approach is the nuclear norm minimization. In [12,14,15], the
authors established the necessary and sufficient null space conditions for
nuclear norm minimization to recover every possible low-rank matrix with rank
at most r (the strong null space condition). In addition, in [12], Oymak et al.
established a null space condition for successful recovery of a given low-rank
matrix (the weak null space condition) using nuclear norm minimization, and
derived the phase transition for the nuclear norm minimization. In this paper,
we show that the weak null space condition in [12] is only a sufficient
condition for successful matrix recovery using nuclear norm minimization, and
is not a necessary condition as claimed in [12]. In this paper, we further give
a weak null space condition for low-rank matrix recovery, which is both
necessary and sufficient for the success of nuclear norm minimization. At the
core of our derivation are an inequality for characterizing the nuclear norms
of block matrices, and the conditions for equality to hold in that inequality.","['Jirong Yi', 'Weiyu Xu']","['math.OC', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT', 'stat.ML']",2018-02-14 17:44:22+00:00
http://arxiv.org/abs/1802.05214v3,Learning Privacy Preserving Encodings through Adversarial Training,"We present a framework to learn privacy-preserving encodings of images that
inhibit inference of chosen private attributes, while allowing recovery of
other desirable information. Rather than simply inhibiting a given fixed
pre-trained estimator, our goal is that an estimator be unable to learn to
accurately predict the private attributes even with knowledge of the encoding
function. We use a natural adversarial optimization-based formulation for
this---training the encoding function against a classifier for the private
attribute, with both modeled as deep neural networks. The key contribution of
our work is a stable and convergent optimization approach that is successful at
learning an encoder with our desired properties---maintaining utility while
inhibiting inference of private attributes, not just within the adversarial
optimization, but also by classifiers that are trained after the encoder is
fixed. We adopt a rigorous experimental protocol for verification wherein
classifiers are trained exhaustively till saturation on the fixed encoders. We
evaluate our approach on tasks of real-world complexity---learning
high-dimensional encodings that inhibit detection of different scene
categories---and find that it yields encoders that are resilient at maintaining
privacy.","['Francesco Pittaluga', 'Sanjeev J. Koppal', 'Ayan Chakrabarti']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2018-02-14 17:04:07+00:00
http://arxiv.org/abs/1802.05196v1,Generative Models for Spear Phishing Posts on Social Media,"Historically, machine learning in computer security has prioritized defense:
think intrusion detection systems, malware classification, and botnet traffic
identification. Offense can benefit from data just as well. Social networks,
with their access to extensive personal data, bot-friendly APIs, colloquial
syntax, and prevalence of shortened links, are the perfect venues for spreading
machine-generated malicious content. We aim to discover what capabilities an
adversary might utilize in such a domain. We present a long short-term memory
(LSTM) neural network that learns to socially engineer specific users into
clicking on deceptive URLs. The model is trained with word vector
representations of social media posts, and in order to make a click-through
more likely, it is dynamically seeded with topics extracted from the target's
timeline. We augment the model with clustering to triage high value targets
based on their level of social engagement, and measure success of the LSTM's
phishing expedition using click-rates of IP-tracked links. We achieve state of
the art success rates, tripling those of historic email attack campaigns, and
outperform humans manually performing the same task.","['John Seymour', 'Philip Tully']","['cs.CR', 'cs.CY', 'cs.LG', 'stat.ML']",2018-02-14 16:40:02+00:00
http://arxiv.org/abs/1802.05193v2,Security Analysis and Enhancement of Model Compressed Deep Learning Systems under Adversarial Attacks,"DNN is presenting human-level performance for many complex intelligent tasks
in real-world applications. However, it also introduces ever-increasing
security concerns. For example, the emerging adversarial attacks indicate that
even very small and often imperceptible adversarial input perturbations can
easily mislead the cognitive function of deep learning systems (DLS). Existing
DNN adversarial studies are narrowly performed on the ideal software-level DNN
models with a focus on single uncertainty factor, i.e. input perturbations,
however, the impact of DNN model reshaping on adversarial attacks, which is
introduced by various hardware-favorable techniques such as hash-based weight
compression during modern DNN hardware implementation, has never been
discussed. In this work, we for the first time investigate the multi-factor
adversarial attack problem in practical model optimized deep learning systems
by jointly considering the DNN model-reshaping (e.g. HashNet based deep
compression) and the input perturbations. We first augment adversarial example
generating method dedicated to the compressed DNN models by incorporating the
software-based approaches and mathematical modeled DNN reshaping. We then
conduct a comprehensive robustness and vulnerability analysis of deep
compressed DNN models under derived adversarial attacks. A defense technique
named ""gradient inhibition"" is further developed to ease the generating of
adversarial examples thus to effectively mitigate adversarial attacks towards
both software and hardware-oriented DNNs. Simulation results show that
""gradient inhibition"" can decrease the average success rate of adversarial
attacks from 87.99% to 4.77% (from 86.74% to 4.64%) on MNIST (CIFAR-10)
benchmark with marginal accuracy degradation across various DNNs.","['Qi Liu', 'Tao Liu', 'Zihao Liu', 'Yanzhi Wang', 'Yier Jin', 'Wujie Wen']","['cs.LG', 'cs.CR', 'stat.ML']",2018-02-14 16:31:35+00:00
http://arxiv.org/abs/1802.05187v2,On the Blindspots of Convolutional Networks,"Deep convolutional network has been the state-of-the-art approach for a wide
variety of tasks over the last few years. Its successes have, in many cases,
turned it into the default model in quite a few domains. In this work, we will
demonstrate that convolutional networks have limitations that may, in some
cases, hinder it from learning properties of the data, which are easily
recognizable by traditional, less demanding, models. To this end, we present a
series of competitive analysis studies on image recognition and text analysis
tasks, for which convolutional networks are known to provide state-of-the-art
results. In our studies, we inject a truth-revealing signal, indiscernible for
the network, thus hitting time and again the network's blind spots. The signal
does not impair the network's existing performances, but it does provide an
opportunity for a significant performance boost by models that can capture it.
The various forms of the carefully designed signals shed a light on the
strengths and weaknesses of convolutional network, which may provide insights
for both theoreticians that study the power of deep architectures, and for
practitioners that consider applying convolutional networks to the task at
hand.","['Elad Hoffer', 'Shai Fine', 'Daniel Soudry']","['stat.ML', 'cs.LG']",2018-02-14 16:24:54+00:00
http://arxiv.org/abs/1802.05155v5,A Diffusion Approximation Theory of Momentum SGD in Nonconvex Optimization,"Momentum Stochastic Gradient Descent (MSGD) algorithm has been widely applied
to many nonconvex optimization problems in machine learning, e.g., training
deep neural networks, variational Bayesian inference, and etc. Despite its
empirical success, there is still a lack of theoretical understanding of
convergence properties of MSGD. To fill this gap, we propose to analyze the
algorithmic behavior of MSGD by diffusion approximations for nonconvex
optimization problems with strict saddle points and isolated local optima. Our
study shows that the momentum helps escape from saddle points, but hurts the
convergence within the neighborhood of optima (if without the step size
annealing or momentum annealing). Our theoretical discovery partially
corroborates the empirical success of MSGD in training deep neural networks.","['Tianyi Liu', 'Zhehui Chen', 'Enlu Zhou', 'Tuo Zhao']","['cs.LG', 'math.OC', 'stat.ML']",2018-02-14 15:26:59+00:00
http://arxiv.org/abs/1802.05141v2,Deep Learning and Data Assimilation for Real-Time Production Prediction in Natural Gas Wells,"The prediction of the gas production from mature gas wells, due to their
complex end-of-life behavior, is challenging and crucial for operational
decision making. In this paper, we apply a modified deep LSTM model for
prediction of the gas flow rates in mature gas wells, including the
uncertainties in input parameters. Additionally, due to changes in the system
in time and in order to increase the accuracy and robustness of the prediction,
the Ensemble Kalman Filter (EnKF) is used to update the flow rate predictions
based on new observations. The developed approach was tested on the data from
two mature gas production wells in which their production is highly dynamic and
suffering from salt deposition. The results show that the flow predictions
using the EnKF updated model leads to better Jeffreys' J-divergences than the
predictions without the EnKF model updating scheme.","['Kelvin Loh', 'Pejman Shoeibi Omrani', 'Ruud van der Linden']","['cs.LG', 'cs.AI', 'physics.flu-dyn', 'physics.geo-ph', 'stat.ML']",2018-02-14 15:03:09+00:00
http://arxiv.org/abs/1802.05101v2,Democratizing AI: Non-expert design of prediction tasks,"Non-experts have long made important contributions to machine learning (ML)
by contributing training data, and recent work has shown that non-experts can
also help with feature engineering by suggesting novel predictive features.
However, non-experts have only contributed features to prediction tasks already
posed by experienced ML practitioners. Here we study how non-experts can design
prediction tasks themselves, what types of tasks non-experts will design, and
whether predictive models can be automatically trained on data sourced for
their tasks. We use a crowdsourcing platform where non-experts design
predictive tasks that are then categorized and ranked by the crowd.
Crowdsourced data are collected for top-ranked tasks and predictive models are
then trained and evaluated automatically using those data. We show that
individuals without ML experience can collectively construct useful datasets
and that predictive models can be learned on these datasets, but challenges
remain. The prediction tasks designed by non-experts covered a broad range of
domains, from politics and current events to health behavior, demographics, and
more. Proper instructions are crucial for non-experts, so we also conducted a
randomized trial to understand how different instructions may influence the
types of prediction tasks being proposed. In general, understanding better how
non-experts can contribute to ML can further leverage advances in Automatic ML
and has important implications as ML continues to drive workplace automation.",['James P. Bagrow'],"['cs.HC', 'cs.AI', 'cs.CY', 'stat.ML']",2018-02-14 14:16:13+00:00
http://arxiv.org/abs/1803.04497v2,Automated software vulnerability detection with machine learning,"Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87.","['Jacob A. Harer', 'Louis Y. Kim', 'Rebecca L. Russell', 'Onur Ozdemir', 'Leonard R. Kosta', 'Akshay Rangamani', 'Lei H. Hamilton', 'Gabriel I. Centeno', 'Jonathan R. Key', 'Paul M. Ellingwood', 'Erik Antelman', 'Alan Mackay', 'Marc W. McConley', 'Jeffrey M. Opper', 'Peter Chin', 'Tomo Lazovich']","['cs.SE', 'cs.LG', 'stat.ML']",2018-02-14 13:00:05+00:00
http://arxiv.org/abs/1802.05074v5,L4: Practical loss-based stepsize adaptation for deep learning,"We propose a stepsize adaptation scheme for stochastic gradient descent. It
operates directly with the loss function and rescales the gradient in order to
make fixed predicted progress on the loss. We demonstrate its capabilities by
conclusively improving the performance of Adam and Momentum optimizers. The
enhanced optimizers with default hyperparameters consistently outperform their
constant stepsize counterparts, even the best ones, without a measurable
increase in computational cost. The performance is validated on multiple
architectures including dense nets, CNNs, ResNets, and the recurrent
Differential Neural Computer on classical datasets MNIST, fashion MNIST,
CIFAR10 and others.","['Michal Rolinek', 'Georg Martius']","['cs.LG', 'stat.ML']",2018-02-14 12:47:37+00:00
http://arxiv.org/abs/1802.05550v1,ICA based on Split Generalized Gaussian,"Independent Component Analysis (ICA) - one of the basic tools in data
analysis - aims to find a coordinate system in which the components of the data
are independent. Most popular ICA methods use kurtosis as a metric of
non-Gaussianity to maximize, such as FastICA and JADE. However, their
assumption of fourth-order moment (kurtosis) may not always be satisfied in
practice. One of the possible solution is to use third-order moment (skewness)
instead of kurtosis, which was applied in $ICA_{SG}$ and EcoICA.
  In this paper we present a competitive approach to ICA based on the Split
Generalized Gaussian distribution (SGGD), which is well adapted to heavy-tailed
as well as asymmetric data. Consequently, we obtain a method which works better
than the classical approaches, in both cases: heavy tails and non-symmetric
data. \end{abstract}","['P. Spurek', 'P. Rola', 'J. Tabor', 'A. Czechowski']",['stat.ML'],2018-02-14 12:32:56+00:00
http://arxiv.org/abs/1802.05046v2,Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis,"Causal inference analysis is the estimation of the effects of actions on
outcomes. In the context of healthcare data this means estimating the outcome
of counter-factual treatments (i.e. including treatments that were not
observed) on a patient's outcome. Compared to classic machine learning methods,
evaluation and validation of causal inference analysis is more challenging
because ground truth data of counter-factual outcome can never be obtained in
any real-world scenario. Here, we present a comprehensive framework for
benchmarking algorithms that estimate causal effect. The framework includes
unlabeled data for prediction, labeled data for validation, and code for
automatic evaluation of algorithm predictions using both established and novel
metrics. The data is based on real-world covariates, and the treatment
assignments and outcomes are based on simulations, which provides the basis for
validation. In this framework we address two questions: one of scaling, and the
other of data-censoring. The framework is available as open source code at
https://github.com/IBM-HRL-MLHLS/IBM-Causal-Inference-Benchmarking-Framework","['Yishai Shimoni', 'Chen Yanover', 'Ehud Karavani', 'Yaara Goldschmnidt']","['stat.ME', 'cs.LG', 'stat.ML']",2018-02-14 11:41:56+00:00
http://arxiv.org/abs/1802.05036v1,Robust Continuous Co-Clustering,"Clustering consists of grouping together samples giving their similar
properties. The problem of modeling simultaneously groups of samples and
features is known as Co-Clustering. This paper introduces ROCCO - a Robust
Continuous Co-Clustering algorithm. ROCCO is a scalable, hyperparameter-free,
easy and ready to use algorithm to address Co-Clustering problems in practice
over massive cross-domain datasets. It operates by learning a graph-based
two-sided representation of the input matrix. The underlying proposed
optimization problem is non-convex, which assures a flexible pool of solutions.
Moreover, we prove that it can be solved with a near linear time complexity on
the input size. An exhaustive large-scale experimental testbed conducted with
both synthetic and real-world datasets demonstrates ROCCO's properties in
practice: (i) State-of-the-art performance in cross-domain real-world problems
including Biomedicine and Text Mining; (ii) very low sensitivity to
hyperparameter settings; (iii) robustness to noise and (iv) a linear empirical
scalability in practice. These results highlight ROCCO as a powerful
general-purpose co-clustering algorithm for cross-domain practitioners,
regardless of their technical background.","['Xiao He', 'Luis Moreira-Matias']","['cs.LG', 'stat.ML']",2018-02-14 11:07:16+00:00
http://arxiv.org/abs/1802.05035v1,Nonnegative PARAFAC2: a flexible coupling approach,"Modeling variability in tensor decomposition methods is one of the challenges
of source separation. One possible solution to account for variations from one
data set to another, jointly analysed, is to resort to the PARAFAC2 model.
However, so far imposing constraints on the mode with variability has not been
possible. In the following manuscript, a relaxation of the PARAFAC2 model is
introduced, that allows for imposing nonnegativity constraints on the varying
mode. An algorithm to compute the proposed flexible PARAFAC2 model is derived,
and its performance is studied on both synthetic and chemometrics data.","['Jeremy E. Cohen', 'Rasmus Bro']",['stat.ML'],2018-02-14 11:03:17+00:00
http://arxiv.org/abs/1802.05027v2,Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care,"Patients in the intensive care unit (ICU) require constant and close
supervision. To assist clinical staff in this task, hospitals use monitoring
systems that trigger audiovisual alarms if their algorithms indicate that a
patient's condition may be worsening. However, current monitoring systems are
extremely sensitive to movement artefacts and technical errors. As a result,
they typically trigger hundreds to thousands of false alarms per patient per
day - drowning the important alarms in noise and adding to the exhaustion of
clinical staff. In this setting, data is abundantly available, but obtaining
trustworthy annotations by experts is laborious and expensive. We frame the
problem of false alarm reduction from multivariate time series as a
machine-learning task and address it with a novel multitask network
architecture that utilises distant supervision through multiple related
auxiliary tasks in order to reduce the number of expensive labels required for
training. We show that our approach leads to significant improvements over
several state-of-the-art baselines on real-world ICU data and provide new
insights on the importance of task selection and architectural choices in
distantly supervised multitask learning.","['Patrick Schwab', 'Emanuela Keller', 'Carl Muroi', 'David J. Mack', 'Christian Strässle', 'Walter Karlen']","['cs.LG', 'cs.AI', 'stat.ML']",2018-02-14 10:35:08+00:00
http://arxiv.org/abs/1802.04960v2,Vertex nomination: The canonical sampling and the extended spectral nomination schemes,"Suppose that one particular block in a stochastic block model is of interest,
but block labels are only observed for a few of the vertices in the network.
Utilizing a graph realized from the model and the observed block labels, the
vertex nomination task is to order the vertices with unobserved block labels
into a ranked nomination list with the goal of having an abundance of
interesting vertices near the top of the list. There are vertex nomination
schemes in the literature, including the optimally precise canonical nomination
scheme~$\mathcal{L}^C$ and the consistent spectral partitioning nomination
scheme~$\mathcal{L}^P$. While the canonical nomination scheme $\mathcal{L}^C$
is provably optimally precise, it is computationally intractable, being
impractical to implement even on modestly sized graphs. With this in mind, an
approximation of the canonical scheme---denoted the {\it canonical sampling
nomination scheme} $\mathcal{L}^{CS}$---is introduced; $\mathcal{L}^{CS}$
relies on a scalable, Markov chain Monte Carlo-based approximation of
$\mathcal{L}^{C}$, and converges to $\mathcal{L}^{C}$ as the amount of sampling
goes to infinity. The spectral partitioning nomination scheme is also extended
to the {\it extended spectral partitioning nomination scheme},
$\mathcal{L}^{EP}$, which introduces a novel semisupervised clustering
framework to improve upon the precision of $\mathcal{L}^P$. Real-data and
simulation experiments are employed to illustrate the precision of these vertex
nomination schemes, as well as their empirical computational complexity.
Keywords: vertex nomination, Markov chain Monte Carlo, spectral partitioning,
Mclust MSC[2010]: 60J22, 65C40, 62H30, 62H25","['Jordan Yoder', 'Li Chen', 'Henry Pao', 'Eric Bridgeford', 'Keith Levin', 'Donniell Fishkind', 'Carey Priebe', 'Vince Lyzinski']",['stat.ML'],2018-02-14 05:20:03+00:00
http://arxiv.org/abs/1802.04956v4,D2KE: From Distance to Kernel and Embedding,"For many machine learning problem settings, particularly with structured
inputs such as sequences or sets of objects, a distance measure between inputs
can be specified more naturally than a feature representation. However, most
standard machine models are designed for inputs with a vector feature
representation. In this work, we consider the estimation of a function
$f:\mathcal{X} \rightarrow \R$ based solely on a dissimilarity measure
$d:\mathcal{X}\times\mathcal{X} \rightarrow \R$ between inputs. In particular,
we propose a general framework to derive a family of \emph{positive definite
kernels} from a given dissimilarity measure, which subsumes the widely-used
\emph{representative-set method} as a special case, and relates to the
well-known \emph{distance substitution kernel} in a limiting case. We show that
functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are
Lipschitz-continuous w.r.t. the given distance metric. We provide a tractable
algorithm to estimate a function from this RKHS, and show that it enjoys better
generalizability than Nearest-Neighbor estimates. Our approach draws from the
literature of Random Features, but instead of deriving feature maps from an
existing kernel, we construct novel kernels from a random feature map, that we
specify given the distance measure. We conduct classification experiments with
such disparate domains as strings, time series, and sets of vectors, where our
proposed framework compares favorably to existing distance-based learning
methods such as $k$-nearest-neighbors, distance-substitution kernels,
pseudo-Euclidean embedding, and the representative-set method.","['Lingfei Wu', 'Ian En-Hsu Yen', 'Fangli Xu', 'Pradeep Ravikumar', 'Michael Witbrock']","['stat.ML', 'cs.LG']",2018-02-14 04:58:13+00:00
http://arxiv.org/abs/1802.04944v2,Edge Attention-based Multi-Relational Graph Convolutional Networks,"Graph convolutional network (GCN) is generalization of convolutional neural
network (CNN) to work with arbitrarily structured graphs. A binary adjacency
matrix is commonly used in training a GCN. Recently, the attention mechanism
allows the network to learn a dynamic and adaptive aggregation of the
neighborhood. We propose a new GCN model on the graphs where edges are
characterized in multiple views or precisely in terms of multiple
relationships. For instance, in chemical graph theory, compound structures are
often represented by the hydrogen-depleted molecular graph where nodes
correspond to atoms and edges correspond to chemical bonds. Multiple attributes
can be important to characterize chemical bonds, such as atom pair (the types
of atoms that a bond connects), aromaticity, and whether a bond is in a ring.
The different attributes lead to different graph representations for the same
molecule. There is growing interests in both chemistry and machine learning
fields to directly learn molecular properties of compounds from the molecular
graph, instead of from fingerprints predefined by chemists. The proposed GCN
model, which we call edge attention-based multi-relational GCN (EAGCN), jointly
learns attention weights and node features in graph convolution. For each bond
attribute, a real-valued attention matrix is used to replace the binary
adjacency matrix. By designing a dictionary for the edge attention, and forming
the attention matrix of each molecule by looking up the dictionary, the EAGCN
exploits correspondence between bonds in different molecules. The prediction of
compound properties is based on the aggregated node features, which is
independent of the varying molecule (graph) size. We demonstrate the efficacy
of the EAGCN on multiple chemical datasets: Tox21, HIV, Freesolv, and
Lipophilicity, and interpret the resultant attention weights.","['Chao Shang', 'Qinqing Liu', 'Ko-Shin Chen', 'Jiangwen Sun', 'Jin Lu', 'Jinfeng Yi', 'Jinbo Bi']","['stat.ML', 'cs.LG']",2018-02-14 03:52:58+00:00
http://arxiv.org/abs/1802.04942v5,Isolating Sources of Disentanglement in Variational Autoencoders,"We decompose the evidence lower bound to show the existence of a term
measuring the total correlation between latent variables. We use this to
motivate our $\beta$-TCVAE (Total Correlation Variational Autoencoder), a
refinement of the state-of-the-art $\beta$-VAE objective for learning
disentangled representations, requiring no additional hyperparameters during
training. We further propose a principled classifier-free measure of
disentanglement called the mutual information gap (MIG). We perform extensive
quantitative and qualitative experiments, in both restricted and non-restricted
settings, and show a strong relation between total correlation and
disentanglement, when the latent variables model is trained using our
framework.","['Ricky T. Q. Chen', 'Xuechen Li', 'Roger Grosse', 'David Duvenaud']","['cs.LG', 'cs.AI', 'stat.ML']",2018-02-14 03:48:06+00:00
http://arxiv.org/abs/1802.04920v2,DVAE++: Discrete Variational Autoencoders with Overlapping Transformations,"Training of discrete latent variable models remains challenging because
passing gradient information through discrete units is difficult. We propose a
new class of smoothing transformations based on a mixture of two overlapping
distributions, and show that the proposed transformation can be used for
training binary latent models with either directed or undirected priors. We
derive a new variational bound to efficiently train with Boltzmann machine
priors. Using this bound, we develop DVAE++, a generative model with a global
discrete prior and a hierarchy of convolutional continuous variables.
Experiments on several benchmarks show that overlapping transformations
outperform other recent continuous relaxations of discrete latent variables
including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and
discrete variational autoencoders (Rolfe 2016).","['Arash Vahdat', 'William G. Macready', 'Zhengbing Bian', 'Amir Khoshaman', 'Evgeny Andriyash']","['cs.LG', 'stat.ML']",2018-02-14 01:39:05+00:00
http://arxiv.org/abs/1802.04918v1,Prophit: Causal inverse classification for multiple continuously valued treatment policies,"Inverse classification uses an induced classifier as a queryable oracle to
guide test instances towards a preferred posterior class label. The result
produced from the process is a set of instance-specific feature perturbations,
or recommendations, that optimally improve the probability of the class label.
In this work, we adopt a causal approach to inverse classification, eliciting
treatment policies (i.e., feature perturbations) for models induced with causal
properties. In so doing, we solve a long-standing problem of eliciting
multiple, continuously valued treatment policies, using an updated framework
and corresponding set of assumptions, which we term the inverse classification
potential outcomes framework (ICPOF), along with a new measure, referred to as
the individual future estimated effects ($i$FEE). We also develop the
approximate propensity score (APS), based on Gaussian processes, to weight
treatments, much like the inverse propensity score weighting used in past
works. We demonstrate the viability of our methods on student performance.","['Michael T. Lash', 'Qihang Lin', 'W. Nick Street']","['cs.LG', 'stat.ML']",2018-02-14 01:33:01+00:00
http://arxiv.org/abs/1802.04911v3,Large-Scale Sparse Inverse Covariance Estimation via Thresholding and Max-Det Matrix Completion,"The sparse inverse covariance estimation problem is commonly solved using an
$\ell_{1}$-regularized Gaussian maximum likelihood estimator known as
""graphical lasso"", but its computational cost becomes prohibitive for large
data sets. A recent line of results showed--under mild assumptions--that the
graphical lasso estimator can be retrieved by soft-thresholding the sample
covariance matrix and solving a maximum determinant matrix completion (MDMC)
problem. This paper proves an extension of this result, and describes a
Newton-CG algorithm to efficiently solve the MDMC problem. Assuming that the
thresholded sample covariance matrix is sparse with a sparse Cholesky
factorization, we prove that the algorithm converges to an $\epsilon$-accurate
solution in $O(n\log(1/\epsilon))$ time and $O(n)$ memory. The algorithm is
highly efficient in practice: we solve the associated MDMC problems with as
many as 200,000 variables to 7-9 digits of accuracy in less than an hour on a
standard laptop computer running MATLAB.","['Richard Y. Zhang', 'Salar Fattahi', 'Somayeh Sojoudi']","['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",2018-02-14 01:00:10+00:00
http://arxiv.org/abs/1802.04908v1,Conditional Density Estimation with Bayesian Normalising Flows,"Modeling complex conditional distributions is critical in a variety of
settings. Despite a long tradition of research into conditional density
estimation, current methods employ either simple parametric forms or are
difficult to learn in practice. This paper employs normalising flows as a
flexible likelihood model and presents an efficient method for fitting them to
complex densities. These estimators must trade-off between modeling
distributional complexity, functional complexity and heteroscedasticity without
overfitting. We recognize these trade-offs as modeling decisions and develop a
Bayesian framework for placing priors over these conditional density estimators
using variational Bayesian neural networks. We evaluate this method on several
small benchmark regression datasets, on some of which it obtains state of the
art performance. Finally, we apply the method to two spatial density modeling
tasks with over 1 million datapoints using the New York City yellow taxi
dataset and the Chicago crime dataset.","['Brian L Trippe', 'Richard E Turner']",['stat.ML'],2018-02-14 00:44:47+00:00
http://arxiv.org/abs/1802.04907v4,Compressive Sensing Using Iterative Hard Thresholding with Low Precision Data Representation: Theory and Applications,"Modern scientific instruments produce vast amounts of data, which can
overwhelm the processing ability of computer systems. Lossy compression of data
is an intriguing solution, but comes with its own drawbacks, such as potential
signal loss, and the need for careful optimization of the compression ratio. In
this work, we focus on a setting where this problem is especially acute:
compressive sensing frameworks for interferometry and medical imaging. We ask
the following question: can the precision of the data representation be lowered
for all inputs, with recovery guarantees and practical performance? Our first
contribution is a theoretical analysis of the normalized Iterative Hard
Thresholding (IHT) algorithm when all input data, meaning both the measurement
matrix and the observation vector are quantized aggressively. We present a
variant of low precision normalized {IHT} that, under mild conditions, can
still provide recovery guarantees. The second contribution is the application
of our quantization framework to radio astronomy and magnetic resonance
imaging. We show that lowering the precision of the data can significantly
accelerate image recovery. We evaluate our approach on telescope data and
samples of brain images using CPU and FPGA implementations achieving up to a 9x
speed-up with negligible loss of recovery quality.","['Nezihe Merve Gürel', 'Kaan Kara', 'Alen Stojanov', 'Tyler Smith', 'Thomas Lemmin', 'Dan Alistarh', 'Markus Püschel', 'Ce Zhang']","['stat.ML', 'cs.LG']",2018-02-14 00:41:30+00:00
http://arxiv.org/abs/1802.04893v2,Uncertainty Estimation via Stochastic Batch Normalization,"In this work, we investigate Batch Normalization technique and propose its
probabilistic interpretation. We propose a probabilistic model and show that
Batch Normalization maximazes the lower bound of its marginalized
log-likelihood. Then, according to the new probabilistic model, we design an
algorithm which acts consistently during train and test. However, inference
becomes computationally inefficient. To reduce memory and computational cost,
we propose Stochastic Batch Normalization -- an efficient approximation of
proper inference procedure. This method provides us with a scalable uncertainty
estimation technique. We demonstrate the performance of Stochastic Batch
Normalization on popular architectures (including deep convolutional
architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.","['Andrei Atanov', 'Arsenii Ashukha', 'Dmitry Molchanov', 'Kirill Neklyudov', 'Dmitry Vetrov']","['stat.ML', 'cs.LG']",2018-02-13 23:22:16+00:00
http://arxiv.org/abs/1802.04889v1,Understanding Membership Inferences on Well-Generalized Learning Models,"Membership Inference Attack (MIA) determines the presence of a record in a
machine learning model's training data by querying the model. Prior work has
shown that the attack is feasible when the model is overfitted to its training
data or when the adversary controls the training algorithm. However, when the
model is not overfitted and the adversary does not control the training
algorithm, the threat is not well understood. In this paper, we report a study
that discovers overfitting to be a sufficient but not a necessary condition for
an MIA to succeed. More specifically, we demonstrate that even a
well-generalized model contains vulnerable instances subject to a new
generalized MIA (GMIA). In GMIA, we use novel techniques for selecting
vulnerable instances and detecting their subtle influences ignored by
overfitting metrics. Specifically, we successfully identify individual records
with high precision in real-world datasets by querying black-box machine
learning models. Further we show that a vulnerable record can even be
indirectly attacked by querying other related records and existing
generalization techniques are found to be less effective in protecting the
vulnerable instances. Our findings sharpen the understanding of the fundamental
cause of the problem: the unique influences the training instance may have on
the model.","['Yunhui Long', 'Vincent Bindschaedler', 'Lei Wang', 'Diyue Bu', 'Xiaofeng Wang', 'Haixu Tang', 'Carl A. Gunter', 'Kai Chen']","['cs.CR', 'cs.LG', 'stat.ML']",2018-02-13 23:05:05+00:00
http://arxiv.org/abs/1802.04876v2,Uncertainty Quantification for Online Learning and Stochastic Approximation via Hierarchical Incremental Gradient Descent,"Stochastic gradient descent (SGD) is an immensely popular approach for online
learning in settings where data arrives in a stream or data sizes are very
large. However, despite an ever- increasing volume of work on SGD, much less is
known about the statistical inferential properties of SGD-based predictions.
Taking a fully inferential viewpoint, this paper introduces a novel procedure
termed HiGrad to conduct statistical inference for online learning, without
incurring additional computational cost compared with SGD. The HiGrad procedure
begins by performing SGD updates for a while and then splits the single thread
into several threads, and this procedure hierarchically operates in this
fashion along each thread. With predictions provided by multiple threads in
place, a t-based confidence interval is constructed by decorrelating
predictions using covariance structures given by a Donsker-style extension of
the Ruppert--Polyak averaging scheme, which is a technical contribution of
independent interest. Under certain regularity conditions, the HiGrad
confidence interval is shown to attain asymptotically exact coverage
probability. Finally, the performance of HiGrad is evaluated through extensive
simulation studies and a real data example. An R package higrad has been
developed to implement the method.","['Weijie J. Su', 'Yuancheng Zhu']","['stat.ML', 'cs.DC', 'math.OC', 'stat.ME']",2018-02-13 22:15:10+00:00
http://arxiv.org/abs/1802.04874v3,GILBO: One Metric to Measure Them All,"We propose a simple, tractable lower bound on the mutual information
contained in the joint generative density of any latent variable generative
model: the GILBO (Generative Information Lower BOund). It offers a
data-independent measure of the complexity of the learned latent variable
description, giving the log of the effective description length. It is
well-defined for both VAEs and GANs. We compute the GILBO for 800 GANs and VAEs
each trained on four datasets (MNIST, FashionMNIST, CIFAR-10 and CelebA) and
discuss the results.","['Alexander A. Alemi', 'Ian Fischer']","['stat.ML', 'cs.LG']",2018-02-13 21:58:48+00:00
http://arxiv.org/abs/1802.04868v2,SimplE Embedding for Link Prediction in Knowledge Graphs,"Knowledge graphs contain knowledge about the world and provide a structured
representation of this knowledge. Current knowledge graphs contain only a small
subset of what is true in the world. Link prediction approaches aim at
predicting new links for a knowledge graph given the existing links among the
entities. Tensor factorization approaches have proved promising for such link
prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is
among the first tensor factorization approaches. CP generally performs poorly
for link prediction as it learns two independent embedding vectors for each
entity, whereas they are really tied. We present a simple enhancement of CP
(which we call SimplE) to allow the two embeddings of each entity to be learned
dependently. The complexity of SimplE grows linearly with the size of
embeddings. The embeddings learned through SimplE are interpretable, and
certain types of background knowledge can be incorporated into these embeddings
through weight tying. We prove SimplE is fully expressive and derive a bound on
the size of its embeddings for full expressivity. We show empirically that,
despite its simplicity, SimplE outperforms several state-of-the-art tensor
factorization techniques. SimplE's code is available on GitHub at
https://github.com/Mehran-k/SimplE.","['Seyed Mehran Kazemi', 'David Poole']","['stat.ML', 'cs.LG']",2018-02-13 21:41:32+00:00
http://arxiv.org/abs/1802.04865v1,Learning Confidence for Out-of-Distribution Detection in Neural Networks,"Modern neural networks are very powerful predictive models, but they are
often incapable of recognizing when their predictions may be wrong. Closely
related to this is the task of out-of-distribution detection, where a network
must determine whether or not an input is outside of the set on which it is
expected to safely perform. To jointly address these issues, we propose a
method of learning confidence estimates for neural networks that is simple to
implement and produces intuitively interpretable outputs. We demonstrate that
on the task of out-of-distribution detection, our technique surpasses recently
proposed techniques which construct confidence based on the network's output
distribution, without requiring any additional labels or access to
out-of-distribution examples. Additionally, we address the problem of
calibrating out-of-distribution detectors, where we demonstrate that
misclassified in-distribution examples can be used as a proxy for
out-of-distribution examples.","['Terrance DeVries', 'Graham W. Taylor']","['stat.ML', 'cs.LG']",2018-02-13 21:31:36+00:00
http://arxiv.org/abs/1802.04852v4,Persistence Codebooks for Topological Data Analysis,"Persistent homology (PH) is a rigorous mathematical theory that provides a
robust descriptor of data in the form of persistence diagrams (PDs) which are
2D multisets of points. Their variable size makes them, however, difficult to
combine with typical machine learning workflows. In this paper we introduce
persistence codebooks, a novel expressive and discriminative fixed-size
vectorized representation of PDs. To this end, we adapt bag-of-words (BoW),
vectors of locally aggregated descriptors (VLAD) and Fischer vectors (FV) for
the quantization of PDs. Persistence codebooks represent PDs in a convenient
way for machine learning and statistical analysis and have a number of
favorable practical and theoretical properties including 1-Wasserstein
stability. We evaluate the presented representations on several heterogeneous
datasets and show their (high) discriminative power. Our approach achieves
state-of-the-art performance and beyond in much less time than alternative
approaches.","['Bartosz Zielinski', 'Michal Lipinski', 'Mateusz Juda', 'Matthias Zeppelzauer', 'Pawel Dlotko']","['stat.ML', 'cs.LG', 'math.AT']",2018-02-13 20:49:01+00:00
http://arxiv.org/abs/1802.04849v2,Clustering and Semi-Supervised Classification for Clickstream Data via Mixture Models,"Finite mixture models have been used for unsupervised learning for some time,
and their use within the semi-supervised paradigm is becoming more commonplace.
Clickstream data is one of the various emerging data types that demands
particular attention because there is a notable paucity of statistical learning
approaches currently available. A mixture of first-order continuous time Markov
models is introduced for unsupervised and semi-supervised learning of
clickstream data. This approach assumes continuous time, which distinguishes it
from existing mixture model-based approaches; practically, this allows account
to be taken of the amount of time each user spends on each webpage. The
approach is evaluated, and compared to the discrete time approach, using
simulated and real data.","['Michael P. B. Gallaugher', 'Paul D. McNicholas']","['stat.ME', 'stat.ML']",2018-02-13 20:43:04+00:00
http://arxiv.org/abs/1802.04846v5,State Space Gaussian Processes with Non-Gaussian Likelihood,"We provide a comprehensive overview and tooling for GP modeling with
non-Gaussian likelihoods using state space methods. The state space formulation
allows for solving one-dimensional GP models in $\mathcal{O}(n)$ time and
memory complexity. While existing literature has focused on the connection
between GP regression and state space methods, the computational primitives
allowing for inference using general likelihoods in combination with the
Laplace approximation (LA), variational Bayes (VB), and assumed density
filtering (ADF, a.k.a. single-sweep expectation propagation, EP) schemes has
been largely overlooked. We present means of combining the efficient
$\mathcal{O}(n)$ state space methodology with existing inference methods. We
extend existing methods, and provide unifying code implementing all approaches.","['Hannes Nickisch', 'Arno Solin', 'Alexander Grigorievskiy']",['stat.ML'],2018-02-13 20:34:44+00:00
http://arxiv.org/abs/1802.04838v1,Network Estimation from Point Process Data,"Consider observing a collection of discrete events within a network that
reflect how network nodes influence one another. Such data are common in spike
trains recorded from biological neural networks, interactions within a social
network, and a variety of other settings. Data of this form may be modeled as
self-exciting point processes, in which the likelihood of future events depends
on the past events. This paper addresses the problem of estimating
self-excitation parameters and inferring the underlying functional network
structure from self-exciting point process data. Past work in this area was
limited by strong assumptions which are addressed by the novel approach here.
Specifically, in this paper we (1) incorporate saturation in a point process
model which both ensures stability and models non-linear thresholding effects;
(2) impose general low-dimensional structural assumptions that include
sparsity, group sparsity and low-rankness that allows bounds to be developed in
the high-dimensional setting; and (3) incorporate long-range memory effects
through moving average and higher-order auto-regressive components. Using our
general framework, we provide a number of novel theoretical guarantees for
high-dimensional self-exciting point processes that reflect the role played by
the underlying network structure and long-term memory. We also provide
simulations and real data examples to support our methodology and main results.","['Benjamin Mark', 'Garvesh Raskutti', 'Rebecca Willett']","['stat.ML', 'cs.IT', 'math.IT', 'math.ST', 'stat.TH']",2018-02-13 20:06:07+00:00
http://arxiv.org/abs/1802.04826v4,Leveraging the Exact Likelihood of Deep Latent Variable Models,"Deep latent variable models (DLVMs) combine the approximation abilities of
deep neural networks and the statistical foundations of generative models.
Variational methods are commonly used for inference; however, the exact
likelihood of these models has been largely overlooked. The purpose of this
work is to study the general properties of this quantity and to show how they
can be leveraged in practice. We focus on important inferential problems that
rely on the likelihood: estimation and missing data imputation. First, we
investigate maximum likelihood estimation for DLVMs: in particular, we show
that most unconstrained models used for continuous data have an unbounded
likelihood function. This problematic behaviour is demonstrated to be a source
of mode collapse. We also show how to ensure the existence of maximum
likelihood estimates, and draw useful connections with nonparametric mixture
models. Finally, we describe an algorithm for missing data imputation using the
exact conditional likelihood of a deep latent variable model. On several data
sets, our algorithm consistently and significantly outperforms the usual
imputation scheme used for DLVMs.","['Pierre-Alexandre Mattei', 'Jes Frellsen']","['stat.ML', 'cs.LG', 'stat.ME', '62H25']",2018-02-13 19:27:38+00:00
http://arxiv.org/abs/1802.04791v2,Stochastic Variance-Reduced Hamilton Monte Carlo Methods,"We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for sampling
from a smooth and strongly log-concave distribution. At the core of our
proposed method is a variance reduction technique inspired by the recent
advance in stochastic optimization. We show that, to achieve $\epsilon$
accuracy in 2-Wasserstein distance, our algorithm achieves $\tilde
O(n+\kappa^{2}d^{1/2}/\epsilon+\kappa^{4/3}d^{1/3}n^{2/3}/\epsilon^{2/3})$
gradient complexity (i.e., number of component gradient evaluations), which
outperforms the state-of-the-art HMC and stochastic gradient HMC methods in a
wide regime. We also extend our algorithm for sampling from smooth and general
log-concave distributions, and prove the corresponding gradient complexity as
well. Experiments on both synthetic and real data demonstrate the superior
performance of our algorithm.","['Difan Zou', 'Pan Xu', 'Quanquan Gu']","['stat.ML', 'cs.LG', 'stat.CO']",2018-02-13 18:54:54+00:00
http://arxiv.org/abs/1802.04784v4,MONK -- Outlier-Robust Mean Embedding Estimation by Median-of-Means,"Mean embeddings provide an extremely flexible and powerful tool in machine
learning and statistics to represent probability distributions and define a
semi-metric (MMD, maximum mean discrepancy; also called N-distance or energy
distance), with numerous successful applications. The representation is
constructed as the expectation of the feature map defined by a kernel. As a
mean, its classical empirical estimator, however, can be arbitrary severely
affected even by a single outlier in case of unbounded features. To the best of
our knowledge, unfortunately even the consistency of the existing few
techniques trying to alleviate this serious sensitivity bottleneck is unknown.
In this paper, we show how the recently emerged principle of median-of-means
can be used to design estimators for kernel mean embedding and MMD with
excessive resistance properties to outliers, and optimal sub-Gaussian deviation
bounds under mild assumptions.","['Matthieu Lerasle', 'Zoltan Szabo', 'Timothee Mathieu', 'Guillaume Lecue']","['stat.ML', 'cs.IT', 'math.FA', 'math.IT', 'math.ST', 'stat.TH', '46E22, 94A15, 62Gxx, 47B32', 'G.3; H.1.1; I.2.6']",2018-02-13 18:35:25+00:00
http://arxiv.org/abs/1802.04782v4,Improving Quadrature for Constrained Integrands,"We present an improved Bayesian framework for performing inference of affine
transformations of constrained functions. We focus on quadrature with
nonnegative functions, a common task in Bayesian inference. We consider
constraints on the range of the function of interest, such as nonnegativity or
boundedness. Although our framework is general, we derive explicit
approximation schemes for these constraints, and argue for the use of a log
transformation for functions with high dynamic range such as likelihood
surfaces. We propose a novel method for optimizing hyperparameters in this
framework: we optimize the marginal likelihood in the original space, as
opposed to in the transformed space. The result is a model that better explains
the actual data. Experiments on synthetic and real-world data demonstrate our
framework achieves superior estimates using less wall-clock time than existing
Bayesian quadrature procedures.","['Henry Chai', 'Roman Garnett']","['cs.LG', 'stat.ML']",2018-02-13 18:30:52+00:00
http://arxiv.org/abs/1802.04765v1,Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control,"Deep reinforcement learning has demonstrated increasing capabilities for
continuous control problems, including agents that can move with skill and
agility through their environment. An open problem in this setting is that of
developing good strategies for integrating or merging policies for multiple
skills, where each individual skill is a specialist in a specific skill and its
associated state distribution. We extend policy distillation methods to the
continuous action setting and leverage this technique to combine expert
policies, as evaluated in the domain of simulated bipedal locomotion across
different classes of terrain. We also introduce an input injection method for
augmenting an existing policy network to exploit new input features. Lastly,
our method uses transfer learning to assist in the efficient acquisition of new
skills. The combination of these methods allows a policy to be incrementally
augmented with new skills. We compare our progressive learning and integration
via distillation (PLAID) method against three alternative baselines.","['Glen Berseth', 'Cheng Xie', 'Paul Cernek', 'Michiel Van de Panne']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2018-02-13 17:57:21+00:00
http://arxiv.org/abs/1802.04742v2,Quantifying Uncertainty in Discrete-Continuous and Skewed Data with Bayesian Deep Learning,"Deep Learning (DL) methods have been transforming computer vision with
innovative adaptations to other domains including climate change. For DL to
pervade Science and Engineering (S&E) applications where risk management is a
core component, well-characterized uncertainty estimates must accompany
predictions. However, S&E observations and model-simulations often follow
heavily skewed distributions and are not well modeled with DL approaches, since
they usually optimize a Gaussian, or Euclidean, likelihood loss. Recent
developments in Bayesian Deep Learning (BDL), which attempts to capture
uncertainties from noisy observations, aleatoric, and from unknown model
parameters, epistemic, provide us a foundation. Here we present a
discrete-continuous BDL model with Gaussian and lognormal likelihoods for
uncertainty quantification (UQ). We demonstrate the approach by developing UQ
estimates on `DeepSD', a super-resolution based DL model for Statistical
Downscaling (SD) in climate applied to precipitation, which follows an
extremely skewed distribution. We find that the discrete-continuous models
outperform a basic Gaussian distribution in terms of predictive accuracy and
uncertainty calibration. Furthermore, we find that the lognormal distribution,
which can handle skewed distributions, produces quality uncertainty estimates
at the extremes. Such results may be important across S&E, as well as other
domains such as finance and economics, where extremes are often of significant
interest. Furthermore, to our knowledge, this is the first UQ model in SD where
both aleatoric and epistemic uncertainties are characterized.","['Thomas Vandal', 'Evan Kodra', 'Jennifer Dy', 'Sangram Ganguly', 'Ramakrishna Nemani', 'Auroop R. Ganguly']","['cs.LG', 'cs.AI', 'stat.ML']",2018-02-13 17:07:13+00:00
http://arxiv.org/abs/1802.04734v1,Substation Signal Matching with a Bagged Token Classifier,"Currently, engineers at substation service providers match customer data with
the corresponding internally used signal names manually. This paper proposes a
machine learning method to automate this process based on substation signal
mapping data from a repository of executed projects. To this end, a bagged
token classifier is proposed, letting words (tokens) in the customer signal
name vote for provider signal names. In our evaluation, the proposed method
exhibits better performance in terms of both accuracy and efficiency over
standard classifiers.","['Qin Wang', 'Sandro Schoenborn', 'Yvonne-Anne Pignolet', 'Theo Widmer', 'Carsten Franke']","['stat.ML', 'cs.LG']",2018-02-13 16:57:31+00:00
http://arxiv.org/abs/1802.04725v2,Superposition-Assisted Stochastic Optimization for Hawkes Processes,"We consider the learning of multi-agent Hawkes processes, a model containing
multiple Hawkes processes with shared endogenous impact functions and different
exogenous intensities. In the framework of stochastic maximum likelihood
estimation, we explore the associated risk bound. Further, we consider the
superposition of Hawkes processes within the model, and demonstrate that under
certain conditions such an operation is beneficial for tightening the risk
bound. Accordingly, we propose a stochastic optimization algorithm assisted
with a diversity-driven superposition strategy, achieving better learning
results with improved convergence properties. The effectiveness of the proposed
method is verified on synthetic data, and its potential to solve the cold-start
problem of sequential recommendation systems is demonstrated on real-world
data.","['Hongteng Xu', 'Xu Chen', 'Lawrence Carin']",['stat.ML'],2018-02-13 16:44:40+00:00
http://arxiv.org/abs/1802.04715v3,Online Variance Reduction for Stochastic Optimization,"Modern stochastic optimization methods often rely on uniform sampling which
is agnostic to the underlying characteristics of the data. This might degrade
the convergence by yielding estimates that suffer from a high variance. A
possible remedy is to employ non-uniform importance sampling techniques, which
take the structure of the dataset into account. In this work, we investigate a
recently proposed setting which poses variance reduction as an online
optimization problem with bandit feedback. We devise a novel and efficient
algorithm for this setting that finds a sequence of importance sampling
distributions competitive with the best fixed distribution in hindsight, the
first result of this kind. While we present our method for sampling datapoints,
it naturally extends to selecting coordinates or even blocks of thereof.
Empirical validations underline the benefits of our method in several settings.","['Zalán Borsos', 'Andreas Krause', 'Kfir Y. Levy']","['stat.ML', 'cs.LG']",2018-02-13 16:28:45+00:00
http://arxiv.org/abs/1802.04712v4,Attention-based Deep Multiple Instance Learning,"Multiple instance learning (MIL) is a variation of supervised learning where
a single class label is assigned to a bag of instances. In this paper, we state
the MIL problem as learning the Bernoulli distribution of the bag label where
the bag label probability is fully parameterized by neural networks.
Furthermore, we propose a neural network-based permutation-invariant
aggregation operator that corresponds to the attention mechanism. Notably, an
application of the proposed attention-based operator provides insight into the
contribution of each instance to the bag label. We show empirically that our
approach achieves comparable performance to the best MIL methods on benchmark
MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and
two real-life histopathology datasets without sacrificing interpretability.","['Maximilian Ilse', 'Jakub M. Tomczak', 'Max Welling']","['cs.LG', 'stat.ML']",2018-02-13 16:27:19+00:00
http://arxiv.org/abs/1802.04697v2,Learning to Search with MCTSnets,"Planning problems are among the most important and well-studied problems in
artificial intelligence. They are most typically solved by tree search
algorithms that simulate ahead into the future, evaluate future states, and
back-up those evaluations to the root of a search tree. Among these algorithms,
Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely
used. A typical implementation of MCTS uses cleverly designed rules, optimized
to the particular characteristics of the domain. These rules control where the
simulation traverses, what to evaluate in the states that are reached, and how
to back-up those evaluations. In this paper we instead learn where, what and
how to search. Our architecture, which we call an MCTSnet, incorporates
simulation-based search inside a neural network, by expanding, evaluating and
backing-up a vector embedding. The parameters of the network are trained
end-to-end using gradient-based optimisation. When applied to small searches in
the well known planning problem Sokoban, the learned search algorithm
significantly outperformed MCTS baselines.","['Arthur Guez', 'Théophane Weber', 'Ioannis Antonoglou', 'Karen Simonyan', 'Oriol Vinyals', 'Daan Wierstra', 'Rémi Munos', 'David Silver']","['cs.AI', 'cs.LG', 'stat.ML']",2018-02-13 16:10:10+00:00
http://arxiv.org/abs/1802.04687v2,Neural Relational Inference for Interacting Systems,"Interacting systems are prevalent in nature, from dynamical systems in
physics to complex societal dynamics. The interplay of components can give rise
to complex behavior, which can often be explained using a simple model of the
system's constituent parts. In this work, we introduce the neural relational
inference (NRI) model: an unsupervised model that learns to infer interactions
while simultaneously learning the dynamics purely from observational data. Our
model takes the form of a variational auto-encoder, in which the latent code
represents the underlying interaction graph and the reconstruction is based on
graph neural networks. In experiments on simulated physical systems, we show
that our NRI model can accurately recover ground-truth interactions in an
unsupervised manner. We further demonstrate that we can find an interpretable
structure and predict complex dynamics in real motion capture and sports
tracking data.","['Thomas Kipf', 'Ethan Fetaya', 'Kuan-Chieh Wang', 'Max Welling', 'Richard Zemel']","['stat.ML', 'cs.LG']",2018-02-13 15:35:11+00:00
http://arxiv.org/abs/1802.04684v1,Unsupervised Evaluation and Weighted Aggregation of Ranked Predictions,"Learning algorithms that aggregate predictions from an ensemble of diverse
base classifiers consistently outperform individual methods. Many of these
strategies have been developed in a supervised setting, where the accuracy of
each base classifier can be empirically measured and this information is
incorporated in the training process. However, the reliance on labeled data
precludes the application of ensemble methods to many real world problems where
labeled data has not been curated. To this end we developed a new theoretical
framework for binary classification, the Strategy for Unsupervised Multiple
Method Aggregation (SUMMA), to estimate the performances of base classifiers
and an optimal strategy for ensemble learning from unlabeled data.","['Mehmet Eren Ahsen', 'Robert Vogel', 'Gustavo Stolovitzky']","['stat.ML', 'cs.LG']",2018-02-13 15:28:20+00:00
http://arxiv.org/abs/1802.04676v1,Variable Selection and Task Grouping for Multi-Task Learning,"We consider multi-task learning, which simultaneously learns related
prediction tasks, to improve generalization performance. We factorize a
coefficient matrix as the product of two matrices based on a low-rank
assumption. These matrices have sparsities to simultaneously perform variable
selection and learn and overlapping group structure among the tasks. The
resulting bi-convex objective function is minimized by alternating optimization
where sub-problems are solved using alternating direction method of multipliers
and accelerated proximal gradient descent. Moreover, we provide the performance
bound of the proposed method. The effectiveness of the proposed method is
validated for both synthetic and real-world datasets.","['Jun-Yong Jeong', 'Chi-Hyuck Jun']",['stat.ML'],2018-02-13 15:15:32+00:00
http://arxiv.org/abs/1802.04630v2,A probabilistic framework for multi-view feature learning with many-to-many associations via neural networks,"A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is
proposed for multi-view feature learning with many-to-many associations so that
it generalizes various existing multi-view methods. PMvGE is a probabilistic
model for predicting new associations via graph embedding of the nodes of data
vectors with links of their associations. Multi-view data vectors with
many-to-many associations are transformed by neural networks to feature vectors
in a shared space, and the probability of new association between two data
vectors is modeled by the inner product of their feature vectors. While
existing multi-view feature learning techniques can treat only either of
many-to-many association or non-linear transformation, PMvGE can treat both
simultaneously. By combining Mercer's theorem and the universal approximation
theorem, we prove that PMvGE learns a wide class of similarity measures across
views. Our likelihood-based estimator enables efficient computation of
non-linear transformations of data vectors in large-scale datasets by minibatch
SGD, and numerical experiments illustrate that PMvGE outperforms existing
multi-view methods.","['Akifumi Okuno', 'Tetsuya Hada', 'Hidetoshi Shimodaira']",['stat.ML'],2018-02-13 14:09:18+00:00
http://arxiv.org/abs/1802.04626v1,Barista - a Graphical Tool for Designing and Training Deep Neural Networks,"In recent years, the importance of deep learning has significantly increased
in pattern recognition, computer vision, and artificial intelligence research,
as well as in industry. However, despite the existence of multiple deep
learning frameworks, there is a lack of comprehensible and easy-to-use
high-level tools for the design, training, and testing of deep neural networks
(DNNs). In this paper, we introduce Barista, an open-source graphical
high-level interface for the Caffe deep learning framework. While Caffe is one
of the most popular frameworks for training DNNs, editing prototext files in
order to specify the net architecture and hyper parameters can become a
cumbersome and error-prone task. Instead, Barista offers a fully graphical user
interface with a graph-based net topology editor and provides an end-to-end
training facility for DNNs, which allows researchers to focus on solving their
problems without having to write code, edit text files, or manually parse
logged data.","['Soeren Klemm', 'Aaron Scherzinger', 'Dominik Drees', 'Xiaoyi Jiang']","['cs.LG', 'stat.ML']",2018-02-13 14:02:48+00:00
http://arxiv.org/abs/1802.04617v1,Fast Global Convergence via Landscape of Empirical Loss,"While optimizing convex objective (loss) functions has been a powerhouse for
machine learning for at least two decades, non-convex loss functions have
attracted fast growing interests recently, due to many desirable properties
such as superior robustness and classification accuracy, compared with their
convex counterparts. The main obstacle for non-convex estimators is that it is
in general intractable to find the optimal solution. In this paper, we study
the computational issues for some non-convex M-estimators. In particular, we
show that the stochastic variance reduction methods converge to the global
optimal with linear rate, by exploiting the statistical property of the
population loss. En route, we improve the convergence analysis for the batch
gradient method in \cite{mei2016landscape}.","['Chao Qu', 'Yan Li', 'Huan Xu']","['stat.ML', 'cs.LG']",2018-02-13 13:37:11+00:00
http://arxiv.org/abs/1802.04591v2,First Order Generative Adversarial Networks,"GANs excel at learning high dimensional distributions, but they can update
generator parameters in directions that do not correspond to the steepest
descent direction of the objective. Prominent examples of problematic update
directions include those used in both Goodfellow's original GAN and the
WGAN-GP. To formally describe an optimal update direction, we introduce a
theoretical framework which allows the derivation of requirements on both the
divergence and corresponding method for determining an update direction, with
these requirements guaranteeing unbiased mini-batch updates in the direction of
steepest descent. We propose a novel divergence which approximates the
Wasserstein distance while regularizing the critic's first order information.
Together with an accompanying update direction, this divergence fulfills the
requirements for unbiased steepest descent updates. We verify our method, the
First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a
new state of the art on the One Billion Word language generation task. Code to
reproduce experiments is available.","['Calvin Seward', 'Thomas Unterthiner', 'Urs Bergmann', 'Nikolay Jetchev', 'Sepp Hochreiter']","['cs.LG', 'stat.ML']",2018-02-13 12:42:58+00:00
http://arxiv.org/abs/1802.04564v2,Diversity-Driven Exploration Strategy for Deep Reinforcement Learning,"Efficient exploration remains a challenging research problem in reinforcement
learning, especially when an environment contains large state spaces, deceptive
local optima, or sparse rewards. To tackle this problem, we present a
diversity-driven approach for exploration, which can be easily combined with
both off- and on-policy reinforcement learning algorithms. We show that by
simply adding a distance measure to the loss function, the proposed methodology
significantly enhances an agent's exploratory behaviors, and thus preventing
the policy from being trapped in local optima. We further propose an adaptive
scaling method for stabilizing the learning process. Our experimental results
in Atari 2600 show that our method outperforms baseline approaches in several
tasks in terms of mean scores and exploration efficiency.","['Zhang-Wei Hong', 'Tzu-Yun Shann', 'Shih-Yang Su', 'Yi-Hsiang Chang', 'Chun-Yi Lee']","['cs.AI', 'stat.ML']",2018-02-13 11:18:41+00:00
http://arxiv.org/abs/1802.04551v2,Analysis of Minimax Error Rate for Crowdsourcing and Its Application to Worker Clustering Model,"While crowdsourcing has become an important means to label data, there is
great interest in estimating the ground truth from unreliable labels produced
by crowdworkers. The Dawid and Skene (DS) model is one of the most well-known
models in the study of crowdsourcing. Despite its practical popularity,
theoretical error analysis for the DS model has been conducted only under
restrictive assumptions on class priors, confusion matrices, or the number of
labels each worker provides. In this paper, we derive a minimax error rate
under more practical setting for a broader class of crowdsourcing models
including the DS model as a special case. We further propose the worker
clustering model, which is more practical than the DS model under real
crowdsourcing settings. The wide applicability of our theoretical analysis
allows us to immediately investigate the behavior of this proposed model, which
can not be analyzed by existing studies. Experimental results showed that there
is a strong similarity between the lower bound of the minimax error rate
derived by our theoretical analysis and the empirical error of the estimated
value.","['Hideaki Imamura', 'Issei Sato', 'Masashi Sugiyama']","['stat.ML', 'cs.HC', 'cs.LG']",2018-02-13 10:47:14+00:00
http://arxiv.org/abs/1802.04537v3,Tighter Variational Bounds are Not Necessarily Better,"We provide theoretical and empirical evidence that using tighter evidence
lower bounds (ELBOs) can be detrimental to the process of learning an inference
network by reducing the signal-to-noise ratio of the gradient estimator. Our
results call into question common implicit assumptions that tighter ELBOs are
better variational objectives for simultaneous model learning and inference
amortization schemes. Based on our insights, we introduce three new algorithms:
the partially importance weighted auto-encoder (PIWAE), the multiply importance
weighted auto-encoder (MIWAE), and the combination importance weighted
auto-encoder (CIWAE), each of which includes the standard importance weighted
auto-encoder (IWAE) as a special case. We show that each can deliver
improvements over IWAE, even when performance is measured by the IWAE target
itself. Furthermore, our results suggest that PIWAE may be able to deliver
simultaneous improvements in the training of both the inference and generative
networks.","['Tom Rainforth', 'Adam R. Kosiorek', 'Tuan Anh Le', 'Chris J. Maddison', 'Maximilian Igl', 'Frank Wood', 'Yee Whye Teh']","['stat.ML', 'cs.LG']",2018-02-13 10:17:32+00:00
http://arxiv.org/abs/1802.04502v2,Legendre Decomposition for Tensors,"We present a novel nonnegative tensor decomposition method, called Legendre
decomposition, which factorizes an input tensor into a multiplicative
combination of parameters. Thanks to the well-developed theory of information
geometry, the reconstructed tensor is unique and always minimizes the KL
divergence from an input tensor. We empirically show that Legendre
decomposition can more accurately reconstruct tensors than other nonnegative
tensor decomposition methods.","['Mahito Sugiyama', 'Hiroyuki Nakahara', 'Koji Tsuda']","['stat.ML', 'cs.LG']",2018-02-13 08:27:49+00:00
http://arxiv.org/abs/1802.04477v4,A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization,"We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth
finite-sum problems. In particular, the objective function is given by the
summation of a differentiable (possibly nonconvex) component, together with a
possibly non-differentiable but convex component. We propose a proximal
stochastic gradient algorithm based on variance reduction, called ProxSVRG+.
Our main contribution lies in the analysis of ProxSVRG+. It recovers several
existing convergence results and improves/generalizes them (in terms of the
number of stochastic gradient oracle calls and proximal oracle calls). In
particular, ProxSVRG+ generalizes the best results given by the SCSG algorithm,
recently proposed by [Lei et al., 2017] for the smooth nonconvex case.
ProxSVRG+ is also more straightforward than SCSG and yields simpler analysis.
Moreover, ProxSVRG+ outperforms the deterministic proximal gradient descent
(ProxGD) for a wide range of minibatch sizes, which partially solves an open
problem proposed in [Reddi et al., 2016b]. Also, ProxSVRG+ uses much less
proximal oracle calls than ProxSVRG [Reddi et al., 2016b]. Moreover, for
nonconvex functions satisfied Polyak-\L{}ojasiewicz condition, we prove that
ProxSVRG+ achieves a global linear convergence rate without restart unlike
ProxSVRG. Thus, it can \emph{automatically} switch to the faster linear
convergence in some regions as long as the objective function satisfies the PL
condition locally in these regions. ProxSVRG+ also improves ProxGD and
ProxSVRG/SAGA, and generalizes the results of SCSG in this case. Finally, we
conduct several experiments and the experimental results are consistent with
the theoretical results.","['Zhize Li', 'Jian Li']","['math.OC', 'cs.DS', 'cs.LG', 'stat.ML']",2018-02-13 06:34:22+00:00
http://arxiv.org/abs/1802.04475v1,Graph-Based Ascent Algorithms for Function Maximization,"We study the problem of finding the maximum of a function defined on the
nodes of a connected graph. The goal is to identify a node where the function
obtains its maximum. We focus on local iterative algorithms, which traverse the
nodes of the graph along a path, and the next iterate is chosen from the
neighbors of the current iterate with probability distribution determined by
the function values at the current iterate and its neighbors. We study two
algorithms corresponding to a Metropolis-Hastings random walk with different
transition kernels: (i) The first algorithm is an exponentially weighted random
walk governed by a parameter $\gamma$. (ii) The second algorithm is defined
with respect to the graph Laplacian and a smoothness parameter $k$. We derive
convergence rates for the two algorithms in terms of total variation distance
and hitting times. We also provide simulations showing the relative convergence
rates of our algorithms in comparison to an unbiased random walk, as a function
of the smoothness of the graph function. Our algorithms may be categorized as a
new class of ""descent-based"" methods for function maximization on the nodes of
a graph.","['Muni Sreenivas Pydi', 'Varun Jog', 'Po-Ling Loh']","['cs.SI', 'cs.NA', 'math.OC', 'stat.ML']",2018-02-13 06:31:15+00:00
http://arxiv.org/abs/1802.04474v2,Deep Neural Networks Learn Non-Smooth Functions Effectively,"We theoretically discuss why deep neural networks (DNNs) performs better than
other models in some cases by investigating statistical properties of DNNs for
non-smooth functions. While DNNs have empirically shown higher performance than
other standard methods, understanding its mechanism is still a challenging
problem. From an aspect of the statistical theory, it is known many standard
methods attain the optimal rate of generalization errors for smooth functions
in large sample asymptotics, and thus it has not been straightforward to find
theoretical advantages of DNNs. This paper fills this gap by considering
learning of a certain class of non-smooth functions, which was not covered by
the previous theory. We derive the generalization error of estimators by DNNs
with a ReLU activation, and show that convergence rates of the generalization
by DNNs are almost optimal to estimate the non-smooth functions, while some of
the popular models do not attain the optimal rate. In addition, our theoretical
result provides guidelines for selecting an appropriate number of layers and
edges of DNNs. We provide numerical experiments to support the theoretical
results.","['Masaaki Imaizumi', 'Kenji Fukumizu']",['stat.ML'],2018-02-13 06:24:27+00:00
http://arxiv.org/abs/1802.09902v4,Attention-Based Guided Structured Sparsity of Deep Neural Networks,"Network pruning is aimed at imposing sparsity in a neural network
architecture by increasing the portion of zero-valued weights for reducing its
size regarding energy-efficiency consideration and increasing evaluation speed.
In most of the conducted research efforts, the sparsity is enforced for network
pruning without any attention to the internal network characteristics such as
unbalanced outputs of the neurons or more specifically the distribution of the
weights and outputs of the neurons. That may cause severe accuracy drop due to
uncontrolled sparsity. In this work, we propose an attention mechanism that
simultaneously controls the sparsity intensity and supervised network pruning
by keeping important information bottlenecks of the network to be active. On
CIFAR-10, the proposed method outperforms the best baseline method by 6% and
reduced the accuracy drop by 2.6x at the same level of sparsity.","['Amirsina Torfi', 'Rouzbeh A. Shirvani', 'Sobhan Soleymani', 'Nasser M. Nasrabadi']","['cs.LG', 'stat.ML']",2018-02-13 04:24:49+00:00
http://arxiv.org/abs/1802.04457v1,Predicting Adversarial Examples with High Confidence,"It has been suggested that adversarial examples cause deep learning models to
make incorrect predictions with high confidence. In this work, we take the
opposite stance: an overly confident model is more likely to be vulnerable to
adversarial examples. This work is one of the most proactive approaches taken
to date, as we link robustness with non-calibrated model confidence on noisy
images, providing a data-augmentation-free path forward. The adversarial
examples phenomenon is most easily explained by the trend of increasing
non-regularized model capacity, while the diversity and number of samples in
common datasets has remained flat. Test accuracy has incorrectly been
associated with true generalization performance, ignoring that training and
test splits are often extremely similar in terms of the overall representation
space. The transferability property of adversarial examples was previously used
as evidence against overfitting arguments, a perceived random effect, but
overfitting is not always random.","['Angus Galloway', 'Graham W. Taylor', 'Medhat Moussa']","['cs.LG', 'stat.ML']",2018-02-13 03:45:28+00:00
http://arxiv.org/abs/1802.04443v1,On Characterizing the Capacity of Neural Networks using Algebraic Topology,"The learnability of different neural architectures can be characterized
directly by computable measures of data complexity. In this paper, we reframe
the problem of architecture selection as understanding how data determines the
most expressive and generalizable architectures suited to that data, beyond
inductive bias. After suggesting algebraic topology as a measure for data
complexity, we show that the power of a network to express the topological
complexity of a dataset in its decision region is a strictly limiting factor in
its ability to generalize. We then provide the first empirical characterization
of the topological capacity of neural networks. Our empirical analysis shows
that at every level of dataset complexity, neural networks exhibit topological
phase transitions. This observation allowed us to connect existing theory to
empirically driven conjectures on the choice of architectures for
fully-connected neural networks.","['William H. Guss', 'Ruslan Salakhutdinov']","['cs.LG', 'cs.CG', 'cs.NE', 'math.AT', 'stat.ML']",2018-02-13 02:32:10+00:00
http://arxiv.org/abs/1802.04431v3,Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding,"As spacecraft send back increasing amounts of telemetry data, improved
anomaly detection systems are needed to lessen the monitoring burden placed on
operations engineers and reduce operational risk. Current spacecraft monitoring
systems only target a subset of anomaly types and often require costly expert
knowledge to develop and maintain due to challenges involving scale and
complexity. We demonstrate the effectiveness of Long Short-Term Memory (LSTMs)
networks, a type of Recurrent Neural Network (RNN), in overcoming these issues
using expert-labeled telemetry anomaly data from the Soil Moisture Active
Passive (SMAP) satellite and the Mars Science Laboratory (MSL) rover,
Curiosity. We also propose a complementary unsupervised and nonparametric
anomaly thresholding approach developed during a pilot implementation of an
anomaly detection system for SMAP, and offer false positive mitigation
strategies along with other key improvements and lessons learned during
development.","['Kyle Hundman', 'Valentino Constantinou', 'Christopher Laporte', 'Ian Colwell', 'Tom Soderstrom']","['cs.LG', 'stat.ML']",2018-02-13 02:09:32+00:00
http://arxiv.org/abs/1802.04422v1,A comparative study of fairness-enhancing interventions in machine learning,"Computers are increasingly used to make decisions that have significant
impact in people's lives. Often, these predictions can affect different
population subgroups disproportionately. As a result, the issue of fairness has
received much recent interest, and a number of fairness-enhanced classifiers
and predictors have appeared in the literature. This paper seeks to study the
following questions: how do these different techniques fundamentally compare to
one another, and what accounts for the differences? Specifically, we seek to
bring attention to many under-appreciated aspects of such fairness-enhancing
interventions. Concretely, we present the results of an open benchmark we have
developed that lets us compare a number of different algorithms under a variety
of fairness measures, and a large number of existing datasets. We find that
although different algorithms tend to prefer specific formulations of fairness
preservations, many of these measures strongly correlate with one another. In
addition, we find that fairness-preserving algorithms tend to be sensitive to
fluctuations in dataset composition (simulated in our benchmark by varying
training-test splits), indicating that fairness interventions might be more
brittle than previously thought.","['Sorelle A. Friedler', 'Carlos Scheidegger', 'Suresh Venkatasubramanian', 'Sonam Choudhary', 'Evan P. Hamilton', 'Derek Roth']","['stat.ML', 'cs.CY', 'cs.LG']",2018-02-13 01:31:51+00:00
http://arxiv.org/abs/1802.04420v2,Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent,"A major challenge in understanding the generalization of deep learning is to
explain why (stochastic) gradient descent can exploit the network architecture
to find solutions that have good generalization performance when using high
capacity models. We find simple but realistic examples showing that this
phenomenon exists even when learning linear classifiers --- between two linear
networks with the same capacity, the one with a convolutional layer can
generalize better than the other when the data distribution has some underlying
spatial structure. We argue that this difference results from a combination of
the convolution architecture, data distribution and gradient descent, all of
which are necessary to be included in a meaningful analysis. We provide a
general analysis of the generalization performance as a function of data
distribution and convolutional filter size, given gradient descent as the
optimization algorithm, then interpret the results using concrete examples.
Experimental results show that our analysis is able to explain what happens in
our introduced examples.","['Yifan Wu', 'Barnabas Poczos', 'Aarti Singh']","['cs.LG', 'stat.ML']",2018-02-13 01:20:21+00:00
http://arxiv.org/abs/1803.04364v1,Maturation Trajectories of Cortical Resting-State Networks Depend on the Mediating Frequency Band,"The functional significance of resting state networks and their abnormal
manifestations in psychiatric disorders are firmly established, as is the
importance of the cortical rhythms in mediating these networks. Resting state
networks are known to undergo substantial reorganization from childhood to
adulthood, but whether distinct cortical rhythms, which are generated by
separable neural mechanisms and are often manifested abnormally in psychiatric
conditions, mediate maturation differentially, remains unknown. Using
magnetoencephalography (MEG) to map frequency band specific maturation of
resting state networks from age 7 to 29 in 162 participants (31 independent),
we found significant changes with age in networks mediated by the beta
(13-30Hz) and gamma (31-80Hz) bands. More specifically, gamma band mediated
networks followed an expected asymptotic trajectory, but beta band mediated
networks followed a linear trajectory. Network integration increased with age
in gamma band mediated networks, while local segregation increased with age in
beta band mediated networks. Spatially, the hubs that changed in importance
with age in the beta band mediated networks had relatively little overlap with
those that showed the greatest changes in the gamma band mediated networks.
These findings are relevant for our understanding of the neural mechanisms of
cortical maturation, in both typical and atypical development.","['Sheraz Khan', 'Javeria Hashmi', 'Fahimeh Mamashli', 'Konstantinos Michmizos', 'Manfred Kitzbichler', 'Hari Bharadwaj', 'Yousra Bekhti', 'Santosh Ganesan', 'Keri A Garel', 'Susan Whitfield-Gabrieli', 'Randy Gollub', 'Jian Kong', 'Lucia M Vaina', 'Kunjan Rana', 'Steven Stufflebeam', 'Matti Hamalainen', 'Tal Kenet']","['q-bio.NC', 'cs.DM', 'stat.ML']",2018-02-13 01:04:40+00:00
http://arxiv.org/abs/1802.04412v4,Efficient Exploration through Bayesian Deep Q-Networks,"We study reinforcement learning (RL) in high dimensional episodic Markov
decision processes (MDP). We consider value-based RL when the optimal Q-value
is a linear function of d-dimensional state-action feature representation. For
instance, in deep-Q networks (DQN), the Q-value is a linear function of the
feature representation layer (output layer). We propose two algorithms, one
based on optimism, LINUCB, and another based on posterior sampling, LINPSRL. We
guarantee frequentist and Bayesian regret upper bounds of O(d sqrt{T}) for
these two algorithms, where T is the number of episodes. We extend these
methods to deep RL and propose Bayesian deep Q-networks (BDQN), which uses an
efficient Thompson sampling algorithm for high dimensional RL. We deploy the
double DQN (DDQN) approach, and instead of learning the last layer of Q-network
using linear regression, we use Bayesian linear regression, resulting in an
approximated posterior over Q-function. This allows us to directly incorporate
the uncertainty over the Q-function and deploy Thompson sampling on the learned
posterior distribution resulting in efficient exploration/exploitation
trade-off. We empirically study the behavior of BDQN on a wide range of Atari
games. Since BDQN carries out more efficient exploration and exploitation, it
is able to reach higher return substantially faster compared to DDQN.","['Kamyar Azizzadenesheli', 'Animashree Anandkumar']","['cs.AI', 'cs.LG', 'stat.ML']",2018-02-13 00:48:17+00:00
http://arxiv.org/abs/1802.04407v2,Adversarially Regularized Graph Autoencoder for Graph Embedding,"Graph embedding is an effective method to represent graph data in a low
dimensional space for graph analytics. Most existing embedding algorithms
typically focus on preserving the topological structure or minimizing the
reconstruction errors of graph data, but they have mostly ignored the data
distribution of the latent codes from the graphs, which often results in
inferior embedding in real-world graph data. In this paper, we propose a novel
adversarial graph embedding framework for graph data. The framework encodes the
topological structure and node content in a graph to a compact representation,
on which a decoder is trained to reconstruct the graph structure. Furthermore,
the latent representation is enforced to match a prior distribution via an
adversarial training scheme. To learn a robust embedding, two variants of
adversarial approaches, adversarially regularized graph autoencoder (ARGA) and
adversarially regularized variational graph autoencoder (ARVGA), are developed.
Experimental studies on real-world graphs validate our design and demonstrate
that our algorithms outperform baselines by a wide margin in link prediction,
graph clustering, and graph visualization tasks.","['Shirui Pan', 'Ruiqi Hu', 'Guodong Long', 'Jing Jiang', 'Lina Yao', 'Chengqi Zhang']","['cs.LG', 'stat.ML']",2018-02-13 00:29:11+00:00
http://arxiv.org/abs/1802.06875v2,LSALSA: Accelerated Source Separation via Learned Sparse Coding,"We propose an efficient algorithm for the generalized sparse coding (SC)
inference problem. The proposed framework applies to both the single dictionary
setting, where each data point is represented as a sparse combination of the
columns of one dictionary matrix, as well as the multiple dictionary setting as
given in morphological component analysis (MCA), where the goal is to separate
a signal into additive parts such that each part has distinct sparse
representation within a corresponding dictionary. Both the SC task and its
generalization via MCA have been cast as $\ell_1$-regularized least-squares
optimization problems. To accelerate traditional acquisition of sparse codes,
we propose a deep learning architecture that constitutes a trainable
time-unfolded version of the Split Augmented Lagrangian Shrinkage Algorithm
(SALSA), a special case of the Alternating Direction Method of Multipliers
(ADMM). We empirically validate both variants of the algorithm, that we refer
to as LSALSA (learned-SALSA), on image vision tasks and demonstrate that at
inference our networks achieve vast improvements in terms of the running time,
the quality of estimated sparse codes, and visual clarity on both classic SC
and MCA problems. Finally, we present a theoretical framework for analyzing
LSALSA network: we show that the proposed approach exactly implements a
truncated ADMM applied to a new, learned cost function with curvature modified
by one of the learned parameterized matrices. We extend a very recent
Stochastic Alternating Optimization analysis framework to show that a gradient
descent step along this learned loss landscape is equivalent to a modified
gradient descent step along the original loss landscape. In this framework, the
acceleration achieved by LSALSA could potentially be explained by the network's
ability to learn a correction to the gradient direction of steeper descent.","['Benjamin Cowen', 'Apoorva Nandini Saridena', 'Anna Choromanska']","['cs.LG', 'stat.ML']",2018-02-13 00:10:00+00:00
http://arxiv.org/abs/1802.04403v3,TVAE: Triplet-Based Variational Autoencoder using Metric Learning,"Deep metric learning has been demonstrated to be highly effective in learning
semantic representation and encoding information that can be used to measure
data similarity, by relying on the embedding learned from metric learning. At
the same time, variational autoencoder (VAE) has widely been used to
approximate inference and proved to have a good performance for directed
probabilistic models. However, for traditional VAE, the data label or feature
information are intractable. Similarly, traditional representation learning
approaches fail to represent many salient aspects of the data. In this project,
we propose a novel integrated framework to learn latent embedding in VAE by
incorporating deep metric learning. The features are learned by optimizing a
triplet loss on the mean vectors of VAE in conjunction with standard evidence
lower bound (ELBO) of VAE. This approach, which we call Triplet based
Variational Autoencoder (TVAE), allows us to capture more fine-grained
information in the latent embedding. Our model is tested on MNIST data set and
achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma &
Welling, 2013) achieves triplet accuracy of 75.08%.","['Haque Ishfaq', 'Assaf Hoogi', 'Daniel Rubin']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG', '68T30 (Primary), 68T01 (Secondary)']",2018-02-13 00:05:19+00:00
http://arxiv.org/abs/1802.04397v4,Identifiability of Nonparametric Mixture Models and Bayes Optimal Clustering,"Motivated by problems in data clustering, we establish general conditions
under which families of nonparametric mixture models are identifiable, by
introducing a novel framework involving clustering overfitted \emph{parametric}
(i.e. misspecified) mixture models. These identifiability conditions generalize
existing conditions in the literature, and are flexible enough to include for
example mixtures of Gaussian mixtures. In contrast to the recent literature on
estimating nonparametric mixtures, we allow for general nonparametric mixture
components, and instead impose regularity assumptions on the underlying mixing
measure. As our primary application, we apply these results to partition-based
clustering, generalizing the notion of a Bayes optimal partition from classical
parametric model-based clustering to nonparametric settings. Furthermore, this
framework is constructive so that it yields a practical algorithm for learning
identified mixtures, which is illustrated through several examples on real
data. The key conceptual device in the analysis is the convex, metric geometry
of probability measures on metric spaces and its connection to the Wasserstein
convergence of mixing measures. The result is a flexible framework for
nonparametric clustering with formal consistency guarantees.","['Bryon Aragam', 'Chen Dan', 'Eric P. Xing', 'Pradeep Ravikumar']","['math.ST', 'cs.AI', 'cs.LG', 'stat.ML', 'stat.TH']",2018-02-12 23:53:52+00:00
http://arxiv.org/abs/1802.04376v1,Few-Shot Learning with Metric-Agnostic Conditional Embeddings,"Learning high quality class representations from few examples is a key
problem in metric-learning approaches to few-shot learning. To accomplish this,
we introduce a novel architecture where class representations are conditioned
for each few-shot trial based on a target image. We also deviate from
traditional metric-learning approaches by training a network to perform
comparisons between classes rather than relying on a static metric comparison.
This allows the network to decide what aspects of each class are important for
the comparison at hand. We find that this flexible architecture works well in
practice, achieving state-of-the-art performance on the Caltech-UCSD birds
fine-grained classification task.","['Nathan Hilliard', 'Lawrence Phillips', 'Scott Howland', 'Artëm Yankov', 'Courtney D. Corley', 'Nathan O. Hodas']","['cs.LG', 'stat.ML']",2018-02-12 21:56:12+00:00
http://arxiv.org/abs/1802.04374v4,Tempered Adversarial Networks,"Generative adversarial networks (GANs) have been shown to produce realistic
samples from high-dimensional distributions, but training them is considered
hard. A possible explanation for training instabilities is the inherent
imbalance between the networks: While the discriminator is trained directly on
both real and fake samples, the generator only has control over the fake
samples it produces since the real data distribution is fixed by the choice of
a given dataset. We propose a simple modification that gives the generator
control over the real samples which leads to a tempered learning process for
both generator and discriminator. The real data distribution passes through a
lens before being revealed to the discriminator, balancing the generator and
discriminator by gradually revealing more detailed features necessary to
produce high-quality results. The proposed module automatically adjusts the
learning process to the current strength of the networks, yet is generic and
easy to add to any GAN variant. In a number of experiments, we show that this
can improve quality, stability and/or convergence speed across a range of
different GAN architectures (DCGAN, LSGAN, WGAN-GP).","['Mehdi S. M. Sajjadi', 'Giambattista Parascandolo', 'Arash Mehrjou', 'Bernhard Schölkopf']","['stat.ML', 'cs.CR', 'cs.LG']",2018-02-12 21:49:07+00:00
http://arxiv.org/abs/1802.04365v1,Learning a Neural-network-based Representation for Open Set Recognition,"Open set recognition problems exist in many domains. For example in security,
new malware classes emerge regularly; therefore malware classification systems
need to identify instances from unknown classes in addition to discriminating
between known classes. In this paper we present a neural network based
representation for addressing the open set recognition problem. In this
representation instances from the same class are close to each other while
instances from different classes are further apart, resulting in statistically
significant improvement when compared to other approaches on three datasets
from two different domains.","['Mehadi Hassen', 'Philip K. Chan']","['cs.LG', 'cs.CR', 'stat.ML']",2018-02-12 21:20:30+00:00
http://arxiv.org/abs/1802.04364v4,Junction Tree Variational Autoencoder for Molecular Graph Generation,"We seek to automate the design of molecules based on specific chemical
properties. In computational terms, this task involves continuous embedding and
generation of molecular graphs. Our primary contribution is the direct
realization of molecular graphs, a task previously approached by generating
linear SMILES strings instead of graphs. Our junction tree variational
autoencoder generates molecular graphs in two phases, by first generating a
tree-structured scaffold over chemical substructures, and then combining them
into a molecule with a graph message passing network. This approach allows us
to incrementally expand molecules while maintaining chemical validity at every
step. We evaluate our model on multiple tasks ranging from molecular generation
to optimization. Across these tasks, our model outperforms previous
state-of-the-art baselines by a significant margin.","['Wengong Jin', 'Regina Barzilay', 'Tommi Jaakkola']","['cs.LG', 'cs.NE', 'stat.ML']",2018-02-12 21:19:39+00:00
http://arxiv.org/abs/1802.04350v5,Cost-Aware Learning for Improved Identifiability with Multiple Experiments,"We analyze the sample complexity of learning from multiple experiments where
the experimenter has a total budget for obtaining samples. In this problem, the
learner should choose a hypothesis that performs well with respect to multiple
experiments, and their related data distributions. Each collected sample is
associated with a cost which depends on the particular experiments. In our
setup, a learner performs $m$ experiments, while incurring a total cost $C$. We
first show that learning from multiple experiments allows to improve
identifiability. Additionally, by using a Rademacher complexity approach, we
show that the gap between the training and generalization error is
$O(C^{-1/2})$. We also provide some examples for linear prediction, two-layer
neural networks and kernel methods.","['Longyun Guo', 'Jean Honorio', 'John Morgan']","['cs.LG', 'stat.ML']",2018-02-12 20:31:58+00:00
http://arxiv.org/abs/1802.04346v2,Gaining Free or Low-Cost Transparency with Interpretable Partial Substitute,"This work addresses the situation where a black-box model with good
predictive performance is chosen over its interpretable competitors, and we
show interpretability is still achievable in this case. Our solution is to find
an interpretable substitute on a subset of data where the black-box model is
overkill or nearly overkill while leaving the rest to the black-box. This
transparency is obtained at minimal cost or no cost of the predictive
performance. Under this framework, we develop a Hybrid Rule Sets (HyRS) model
that uses decision rules to capture the subspace of data where the rules are as
accurate or almost as accurate as the black-box provided. To train a HyRS, we
devise an efficient search algorithm that iteratively finds the optimal model
and exploits theoretically grounded strategies to reduce computation. Our
framework is agnostic to the black-box during training. Experiments on
structured and text data show that HyRS obtains an effective trade-off between
transparency and interpretability.",['Tong Wang'],"['cs.LG', 'stat.ML']",2018-02-12 20:24:47+00:00
http://arxiv.org/abs/1802.04325v2,Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation,"Modern reinforcement learning algorithms reach super-human performance on
many board and video games, but they are sample inefficient, i.e. they
typically require significantly more playing experience than humans to reach an
equal performance level. To improve sample efficiency, an agent may build a
model of the environment and use planning methods to update its policy. In this
article we introduce Variational State Tabulation (VaST), which maps an
environment with a high-dimensional state space (e.g. the space of visual
inputs) to an abstract tabular model. Prioritized sweeping with small backups,
a highly efficient planning method, can then be used to update state-action
values. We show how VaST can rapidly learn to maximize reward in tasks like 3D
navigation and efficiently adapt to sudden changes in rewards or transition
probabilities.","['Dane Corneil', 'Wulfram Gerstner', 'Johanni Brea']","['cs.LG', 'cs.AI', 'stat.ML']",2018-02-12 19:38:44+00:00
http://arxiv.org/abs/1802.04310v1,Stochastic quasi-Newton with adaptive step lengths for large-scale problems,"We provide a numerically robust and fast method capable of exploiting the
local geometry when solving large-scale stochastic optimisation problems. Our
key innovation is an auxiliary variable construction coupled with an inverse
Hessian approximation computed using a receding history of iterates and
gradients. It is the Markov chain nature of the classic stochastic gradient
algorithm that enables this development. The construction offers a mechanism
for stochastic line search adapting the step length. We numerically evaluate
and compare against current state-of-the-art with encouraging performance on
real-world benchmark problems where the number of observations and unknowns is
in the order of millions.","['Adrian Wills', 'Thomas Schön']","['stat.ML', 'cs.LG']",2018-02-12 19:13:28+00:00
http://arxiv.org/abs/1802.04307v3,A Fast Proximal Point Method for Computing Exact Wasserstein Distance,"Wasserstein distance plays increasingly important roles in machine learning,
stochastic programming and image processing. Major efforts have been under way
to address its high computational complexity, some leading to approximate or
regularized variations such as Sinkhorn distance. However, as we will
demonstrate, regularized variations with large regularization parameter will
degradate the performance in several important machine learning applications,
and small regularization parameter will fail due to numerical stability issues
with existing algorithms. We address this challenge by developing an Inexact
Proximal point method for exact Optimal Transport problem (IPOT) with the
proximal operator approximately evaluated at each iteration using projections
to the probability simplex. The algorithm (a) converges to exact Wasserstein
distance with theoretical guarantee and robust regularization parameter
selection, (b) alleviates numerical stability issue, (c) has similar
computational complexity to Sinkhorn, and (d) avoids the shrinking problem when
apply to generative models. Furthermore, a new algorithm is proposed based on
IPOT to obtain sharper Wasserstein barycenter.","['Yujia Xie', 'Xiangfeng Wang', 'Ruijia Wang', 'Hongyuan Zha']","['stat.ML', 'cs.LG']",2018-02-12 19:06:59+00:00
http://arxiv.org/abs/1802.04302v2,Evaluating Compositionality in Sentence Embeddings,"An important challenge for human-like AI is compositional semantics. Recent
research has attempted to address this by using deep neural networks to learn
vector space embeddings of sentences, which then serve as input to other tasks.
We present a new dataset for one such task, `natural language inference' (NLI),
that cannot be solved using only word-level knowledge and requires some
compositionality. We find that the performance of state of the art sentence
embeddings (InferSent; Conneau et al., 2017) on our new dataset is poor. We
analyze the decision rules learned by InferSent and find that they are
consistent with simple heuristics that are ecologically valid in its training
dataset. Further, we find that augmenting training with our dataset improves
test performance on our dataset without loss of performance on the original
training dataset. This highlights the importance of structured datasets in
better understanding and improving AI systems.","['Ishita Dasgupta', 'Demi Guo', 'Andreas Stuhlmüller', 'Samuel J. Gershman', 'Noah D. Goodman']","['cs.CL', 'stat.ML']",2018-02-12 19:02:52+00:00
http://arxiv.org/abs/1802.04240v2,Reinforcement Learning for Solving the Vehicle Routing Problem,"We present an end-to-end framework for solving the Vehicle Routing Problem
(VRP) using reinforcement learning. In this approach, we train a single model
that finds near-optimal solutions for problem instances sampled from a given
distribution, only by observing the reward signals and following feasibility
rules. Our model represents a parameterized stochastic policy, and by applying
a policy gradient algorithm to optimize its parameters, the trained model
produces the solution as a sequence of consecutive actions in real time,
without the need to re-train for every new problem instance. On capacitated
VRP, our approach outperforms classical heuristics and Google's OR-Tools on
medium-sized instances in solution quality with comparable computation time
(after training). We demonstrate how our approach can handle problems with
split delivery and explore the effect of such deliveries on the solution
quality. Our proposed framework can be applied to other variants of the VRP
such as the stochastic VRP, and has the potential to be applied more generally
to combinatorial optimization problems.","['Mohammadreza Nazari', 'Afshin Oroojlooy', 'Lawrence V. Snyder', 'Martin Takáč']","['cs.AI', 'cs.LG', 'stat.ML']",2018-02-12 18:41:57+00:00
http://arxiv.org/abs/1802.04223v2,SparseMAP: Differentiable Sparse Structured Inference,"Structured prediction requires searching over a combinatorial number of
structures. To tackle it, we introduce SparseMAP: a new method for sparse
structured inference, and its natural loss function. SparseMAP automatically
selects only a few global structures: it is situated between MAP inference,
which picks a single structure, and marginal inference, which assigns
probability mass to all structures, including implausible ones. Importantly,
SparseMAP can be computed using only calls to a MAP oracle, making it
applicable to problems with intractable marginal inference, e.g., linear
assignment. Sparsity makes gradient backpropagation efficient regardless of the
structure, enabling us to augment deep neural networks with generic and sparse
structured hidden layers. Experiments in dependency parsing and natural
language inference reveal competitive accuracy, improved interpretability, and
the ability to capture natural language ambiguities, which is attractive for
pipeline systems.","['Vlad Niculae', 'André F. T. Martins', 'Mathieu Blondel', 'Claire Cardie']","['stat.ML', 'cs.CL', 'cs.LG', '68T50', 'I.2.6; I.2.6']",2018-02-12 18:07:34+00:00
http://arxiv.org/abs/1802.04220v3,Augment and Reduce: Stochastic Inference for Large Categorical Distributions,"Categorical distributions are ubiquitous in machine learning, e.g., in
classification, language models, and recommendation systems. However, when the
number of possible outcomes is very large, using categorical distributions
becomes computationally expensive, as the complexity scales linearly with the
number of outcomes. To address this problem, we propose augment and reduce
(A&R), a method to alleviate the computational complexity. A&R uses two ideas:
latent variable augmentation and stochastic variational inference. It maximizes
a lower bound on the marginal likelihood of the data. Unlike existing methods
which are specific to softmax, A&R is more general and is amenable to other
categorical models, such as multinomial probit. On several large-scale
classification problems, we show that A&R provides a tighter bound on the
marginal likelihood and has better predictive performance than existing
approaches.","['Francisco J. R. Ruiz', 'Michalis K. Titsias', 'Adji B. Dieng', 'David M. Blei']","['stat.ML', 'cs.LG']",2018-02-12 18:04:06+00:00
http://arxiv.org/abs/1802.04204v1,Fast Interactive Image Retrieval using large-scale unlabeled data,"An interactive image retrieval system learns which images in the database
belong to a user's query concept, by analyzing the example images and feedback
provided by the user. The challenge is to retrieve the relevant images with
minimal user interaction. In this work, we propose to solve this problem by
posing it as a binary classification task of classifying all images in the
database as being relevant or irrelevant to the user's query concept. Our
method combines active learning with graph-based semi-supervised learning
(GSSL) to tackle this problem. Active learning reduces the number of user
interactions by querying the labels of the most informative points and GSSL
allows to use abundant unlabeled data along with the limited labeled data
provided by the user. To efficiently find the most informative point, we use an
uncertainty sampling based method that queries the label of the point nearest
to the decision boundary of the classifier. We estimate this decision boundary
using our heuristic of adaptive threshold. To utilize huge volumes of unlabeled
data we use an efficient approximation based method that reduces the complexity
of GSSL from $O(n^3)$ to $O(n)$, making GSSL scalable. We make the classifier
robust to the diversity and noisy labels associated with images in large
databases by incorporating information from multiple modalities such as visual
information extracted from deep learning based models and semantic information
extracted from the WordNet. High F1 scores within few relevance feedback rounds
in our experiments with concepts defined on AnimalWithAttributes and Imagenet
(1.2 million images) datasets indicate the effectiveness and scalability of our
approach.","['Akshay Mehra', 'Jihun Hamm', 'Mikhail Belkin']","['cs.LG', 'stat.ML']",2018-02-12 17:45:28+00:00
http://arxiv.org/abs/1802.04198v1,client2vec: Towards Systematic Baselines for Banking Applications,"The workflow of data scientists normally involves potentially inefficient
processes such as data mining, feature engineering and model selection. Recent
research has focused on automating this workflow, partly or in its entirety, to
improve productivity. We choose the former approach and in this paper share our
experience in designing the client2vec: an internal library to rapidly build
baselines for banking applications. Client2vec uses marginalized stacked
denoising autoencoders on current account transactions data to create vector
embeddings which represent the behaviors of our clients. These representations
can then be used in, and optimized against, a variety of tasks such as client
segmentation, profiling and targeting. Here we detail how we selected the
algorithmic machinery of client2vec and the data it works on and present
experimental results on several business cases.","['Leonardo Baldassini', 'Jose Antonio Rodríguez Serrano']","['stat.ML', 'cs.LG']",2018-02-12 17:31:43+00:00
http://arxiv.org/abs/1802.04181v2,State Representation Learning for Control: An Overview,"Representation learning algorithms are designed to learn abstract features
that characterize data. State representation learning (SRL) focuses on a
particular kind of representation learning where learned features are in low
dimension, evolve through time, and are influenced by actions of an agent. The
representation is learned to capture the variation in the environment generated
by the agent's actions; this kind of representation is particularly suitable
for robotics and control scenarios. In particular, the low dimension
characteristic of the representation helps to overcome the curse of
dimensionality, provides easier interpretation and utilization by humans and
can help improve performance and speed in policy learning algorithms such as
reinforcement learning.
  This survey aims at covering the state-of-the-art on state representation
learning in the most recent years. It reviews different SRL methods that
involve interaction with the environment, their implementations and their
applications in robotics control tasks (simulated or real). In particular, it
highlights how generic learning objectives are differently exploited in the
reviewed algorithms. Finally, it discusses evaluation methods to assess the
representation learned and summarizes current and future lines of research.","['Timothée Lesort', 'Natalia Díaz-Rodríguez', 'Jean-François Goudou', 'David Filliat']","['cs.AI', 'cs.LG', 'stat.ML']",2018-02-12 16:53:48+00:00
http://arxiv.org/abs/1802.04170v2,Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches,"Healthcare companies must submit pharmaceutical drugs or medical devices to
regulatory bodies before marketing new technology. Regulatory bodies frequently
require transparent and interpretable computational modelling to justify a new
healthcare technology, but researchers may have several competing models for a
biological system and too little data to discriminate between the models. In
design of experiments for model discrimination, the goal is to design maximally
informative physical experiments in order to discriminate between rival
predictive models. Prior work has focused either on analytical approaches,
which cannot manage all functions, or on data-driven approaches, which may have
computational difficulties or lack interpretable marginal predictive
distributions. We develop a methodology introducing Gaussian process surrogates
in lieu of the original mechanistic models. We thereby extend existing design
and model discrimination methods developed for analytical models to cases of
non-analytical models in a computationally efficient manner.","['Simon Olofsson', 'Marc Peter Deisenroth', 'Ruth Misener']","['stat.AP', 'stat.ML']",2018-02-12 16:34:06+00:00
http://arxiv.org/abs/1803.04478v1,Bridge type classification: supervised learning on a modified NBI dataset,"A key phase in the bridge design process is the selection of the structural
system. Due to budget and time constraints, engineers typically rely on
engineering judgment and prior experience when selecting a structural system,
often considering a limited range of design alternatives. The objective of this
study was to explore the suitability of supervised machine learning as a
preliminary design aid that provides guidance to engineers with regards to the
statistically optimal bridge type to choose, ultimately improving the
likelihood of optimized design, design standardization, and reduced maintenance
costs. In order to devise this supervised learning system, data for over
600,000 bridges from the National Bridge Inventory database were analyzed. Key
attributes for determining the bridge structure type were identified through
three feature selection techniques. Potentially useful attributes like seismic
intensity and historic data on the cost of materials (steel and concrete) were
then added from the US Geological Survey (USGS) database and Engineering News
Record. Decision tree, Bayes network and Support Vector Machines were used for
predicting the bridge design type. Due to state-to-state variations in material
availability, material costs, and design codes, supervised learning models
based on the complete data set did not yield favorable results. Supervised
learning models were then trained and tested using 10-fold cross validation on
data for each state. Inclusion of seismic data improved the model performance
noticeably. The data was then resampled to reduce the bias of the models
towards more common design types, and the supervised learning models thus
constructed showed further improvements in performance. The average recall and
precision for the state models was 88.6% and 88.0% using Decision Trees, 84.0%
and 83.7% using Bayesian Networks, and 80.8% and 75.6% using SVM.","['Achyuthan Jootoo', 'David Lattanzi']","['stat.ML', 'cs.LG', 'stat.AP']",2018-02-12 16:33:30+00:00
http://arxiv.org/abs/1802.04145v3,DCFNet: Deep Neural Network with Decomposed Convolutional Filters,"Filters in a Convolutional Neural Network (CNN) contain model parameters
learned from enormous amounts of data. In this paper, we suggest to decompose
convolutional filters in CNN as a truncated expansion with pre-fixed bases,
namely the Decomposed Convolutional Filters network (DCFNet), where the
expansion coefficients remain learned from data. Such a structure not only
reduces the number of trainable parameters and computation, but also imposes
filter regularity by bases truncation. Through extensive experiments, we
consistently observe that DCFNet maintains accuracy for image classification
tasks with a significant reduction of model parameters, particularly with
Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we
analyze the representation stability of DCFNet with respect to input
variations, and prove representation stability under generic assumptions on the
expansion coefficients. The analysis is consistent with the empirical
observations.","['Qiang Qiu', 'Xiuyuan Cheng', 'Robert Calderbank', 'Guillermo Sapiro']","['stat.ML', 'cs.CV', 'cs.LG']",2018-02-12 15:58:54+00:00
