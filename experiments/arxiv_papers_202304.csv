id,title,abstract,authors,categories,date
http://arxiv.org/abs/2305.11997v3,Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees,"There is an emerging interest in generating robust counterfactual
explanations that would remain valid if the model is updated or changed even
slightly. Towards finding robust counterfactuals, existing literature often
assumes that the original model $m$ and the new model $M$ are bounded in the
parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$.
However, models can often change significantly in the parameter space with
little to no change in their predictions or accuracy on the given dataset. In
this work, we introduce a mathematical abstraction termed
$\textit{naturally-occurring}$ model change, which allows for arbitrary changes
in the parameter space such that the change in predictions on points that lie
on the data manifold is limited. Next, we propose a measure -- that we call
$\textit{Stability}$ -- to quantify the robustness of counterfactuals to
potential model changes for differentiable models, e.g., neural networks. Our
main contribution is to show that counterfactuals with sufficiently high value
of $\textit{Stability}$ as defined by our measure will remain valid after
potential $\textit{naturally-occurring}$ model changes with high probability
(leveraging concentration bounds for Lipschitz function of independent
Gaussians). Since our quantification depends on the local Lipschitz constant
around a data point which is not always available, we also examine practical
relaxations of our proposed measure and demonstrate experimentally how they can
be incorporated to find robust counterfactuals for neural networks that are
close, realistic, and remain valid after potential model changes. This work
also has interesting connections with model multiplicity, also known as, the
Rashomon effect.","['Faisal Hamman', 'Erfaun Noorani', 'Saumitra Mishra', 'Daniele Magazzeni', 'Sanghamitra Dutta']","['stat.ML', 'cs.AI', 'cs.CY', 'cs.IT', 'cs.LG', 'math.IT']",2023-05-19 20:48:05+00:00
http://arxiv.org/abs/2305.11982v2,Sequential Memory with Temporal Predictive Coding,"Forming accurate memory of sequential stimuli is a fundamental function of
biological agents. However, the computational mechanism underlying sequential
memory in the brain remains unclear. Inspired by neuroscience theories and
recent successes in applying predictive coding (PC) to \emph{static} memory
tasks, in this work we propose a novel PC-based model for \emph{sequential}
memory, called \emph{temporal predictive coding} (tPC). We show that our tPC
models can memorize and retrieve sequential inputs accurately with a
biologically plausible neural implementation. Importantly, our analytical study
reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN)
with an implicit statistical whitening process, which leads to more stable
performance in sequential memory tasks of structured inputs. Moreover, we find
that tPC exhibits properties consistent with behavioral observations and
theories in neuroscience, thereby strengthening its biological relevance. Our
work establishes a possible computational mechanism underlying sequential
memory in the brain that can also be theoretically interpreted using existing
memory model frameworks.","['Mufeng Tang', 'Helen Barron', 'Rafal Bogacz']","['q-bio.NC', 'cs.LG', 'stat.ML']",2023-05-19 20:03:31+00:00
http://arxiv.org/abs/2305.11965v1,Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization,"In this paper, we aim to optimize a contrastive loss with individualized
temperatures in a principled and systematic manner for self-supervised
learning. The common practice of using a global temperature parameter $\tau$
ignores the fact that ``not all semantics are created equal"", meaning that
different anchor data may have different numbers of samples with similar
semantics, especially when data exhibits long-tails. First, we propose a new
robust contrastive loss inspired by distributionally robust optimization (DRO),
providing us an intuition about the effect of $\tau$ and a mechanism for
automatic temperature individualization. Then, we propose an efficient
stochastic algorithm for optimizing the robust contrastive loss with a provable
convergence guarantee without using large mini-batch sizes. Theoretical and
experimental results show that our algorithm automatically learns a suitable
$\tau$ for each sample. Specifically, samples with frequent semantics use large
temperatures to keep local semantic structures, while samples with rare
semantics use small temperatures to induce more separable features. Our method
not only outperforms prior strong baselines (e.g., SimCLR, CLIP) on unimodal
and bimodal datasets with larger improvements on imbalanced data but also is
less sensitive to hyper-parameters. To our best knowledge, this is the first
methodical approach to optimizing a contrastive loss with individualized
temperatures.","['Zi-Hao Qiu', 'Quanqi Hu', 'Zhuoning Yuan', 'Denny Zhou', 'Lijun Zhang', 'Tianbao Yang']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2023-05-19 19:25:56+00:00
http://arxiv.org/abs/2305.11857v4,Computing high-dimensional optimal transport by flow neural networks,"Flow-based models are widely used in generative tasks, including normalizing
flow, where a neural network transports from a data distribution $P$ to a
normal distribution. This work develops a flow-based model that transports from
$P$ to an arbitrary $Q$ where both distributions are only accessible via finite
samples. We propose to learn the dynamic optimal transport between $P$ and $Q$
by training a flow neural network. The model is trained to optimally find an
invertible transport map between $P$ and $Q$ by minimizing the transport cost.
The trained optimal transport flow subsequently allows for performing many
downstream tasks, including infinitesimal density ratio estimation (DRE) and
distribution interpolation in the latent space for generative models. The
effectiveness of the proposed model on high-dimensional data is demonstrated by
strong empirical performance on high-dimensional DRE, OT baselines, and
image-to-image translation.","['Chen Xu', 'Xiuyuan Cheng', 'Yao Xie']","['stat.ML', 'cs.LG', 'stat.ME']",2023-05-19 17:48:21+00:00
http://arxiv.org/abs/2305.11854v4,Multimodal Web Navigation with Instruction-Finetuned Foundation Models,"The progress of autonomous web navigation has been hindered by the dependence
on billions of exploratory interactions via online reinforcement learning, and
domain-specific model designs that make it difficult to leverage generalization
from rich out-of-domain data. In this work, we study data-driven offline
training for web agents with vision-language foundation models. We propose an
instruction-following multimodal agent, WebGUM, that observes both webpage
screenshots and HTML pages and outputs web navigation actions, such as click
and type. WebGUM is trained by jointly finetuning an instruction-finetuned
language model and a vision encoder with temporal and local perception on a
large corpus of demonstrations. We empirically demonstrate this recipe improves
the agent's ability of grounded multimodal perception, HTML comprehension, and
multi-step reasoning, outperforming prior works by a significant margin. On the
MiniWoB, we improve over the previous best offline methods by more than 45.8%,
even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the
WebShop benchmark, our 3-billion-parameter model achieves superior performance
to the existing SoTA, PaLM-540B. Furthermore, WebGUM exhibits strong positive
transfer to the real-world planning tasks on the Mind2Web. We also collect 347K
high-quality demonstrations using our trained models, 38 times larger than
prior work, and make them available to promote future research in this
direction.","['Hiroki Furuta', 'Kuang-Huei Lee', 'Ofir Nachum', 'Yutaka Matsuo', 'Aleksandra Faust', 'Shixiang Shane Gu', 'Izzeddin Gur']","['cs.LG', 'cs.AI', 'stat.ML']",2023-05-19 17:44:34+00:00
http://arxiv.org/abs/2305.11832v1,Improving Multimodal Joint Variational Autoencoders through Normalizing Flows and Correlation Analysis,"We propose a new multimodal variational autoencoder that enables to generate
from the joint distribution and conditionally to any number of complex
modalities. The unimodal posteriors are conditioned on the Deep Canonical
Correlation Analysis embeddings which preserve the shared information across
modalities leading to more coherent cross-modal generations. Furthermore, we
use Normalizing Flows to enrich the unimodal posteriors and achieve more
diverse data generation. Finally, we propose to use a Product of Experts for
inferring one modality from several others which makes the model scalable to
any number of modalities. We demonstrate that our method improves likelihood
estimates, diversity of the generations and in particular coherence metrics in
the conditional generations on several datasets.","['Agathe Senellart', 'Clément Chadebec', 'Stéphanie Allassonnière']","['stat.ML', 'cs.LG']",2023-05-19 17:15:34+00:00
http://arxiv.org/abs/2305.11798v1,The probability flow ODE is provably fast,"We provide the first polynomial-time convergence guarantees for the
probability flow ODE implementation (together with a corrector step) of
score-based generative modeling. Our analysis is carried out in the wake of
recent results obtaining such guarantees for the SDE-based implementation
(i.e., denoising diffusion probabilistic modeling or DDPM), but requires the
development of novel techniques for studying deterministic dynamics without
contractivity. Through the use of a specially chosen corrector step based on
the underdamped Langevin diffusion, we obtain better dimension dependence than
prior works on DDPM ($O(\sqrt{d})$ vs. $O(d)$, assuming smoothness of the data
distribution), highlighting potential advantages of the ODE framework.","['Sitan Chen', 'Sinho Chewi', 'Holden Lee', 'Yuanzhi Li', 'Jianfeng Lu', 'Adil Salim']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2023-05-19 16:33:05+00:00
http://arxiv.org/abs/2305.11788v2,Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability,"Recent research has observed that in machine learning optimization, gradient
descent (GD) often operates at the edge of stability (EoS) [Cohen, et al.,
2021], where the stepsizes are set to be large, resulting in non-monotonic
losses induced by the GD iterates. This paper studies the convergence and
implicit bias of constant-stepsize GD for logistic regression on linearly
separable data in the EoS regime. Despite the presence of local oscillations,
we prove that the logistic loss can be minimized by GD with \emph{any} constant
stepsize over a long time scale. Furthermore, we prove that with \emph{any}
constant stepsize, the GD iterates tend to infinity when projected to a
max-margin direction (the hard-margin SVM direction) and converge to a fixed
vector that minimizes a strongly convex potential when projected to the
orthogonal complement of the max-margin direction. In contrast, we also show
that in the EoS regime, GD iterates may diverge catastrophically under the
exponential loss, highlighting the superiority of the logistic loss. These
theoretical findings are in line with numerical simulations and complement
existing theories on the convergence and implicit bias of GD for logistic
regression, which are only applicable when the stepsizes are sufficiently
small.","['Jingfeng Wu', 'Vladimir Braverman', 'Jason D. Lee']","['cs.LG', 'stat.ML']",2023-05-19 16:24:47+00:00
http://arxiv.org/abs/2305.11774v3,Multi-objective optimisation via the R2 utilities,"The goal of multi-objective optimisation is to identify a collection of
points which describe the best possible trade-offs between the multiple
objectives. In order to solve this vector-valued optimisation problem,
practitioners often appeal to the use of scalarisation functions in order to
transform the multi-objective problem into a collection of single-objective
problems. This set of scalarised problems can then be solved using traditional
single-objective optimisation techniques. In this work, we formalise this
convention into a general mathematical framework. We show how this strategy
effectively recasts the original multi-objective optimisation problem into a
single-objective optimisation problem defined over sets. An appropriate class
of objective functions for this new problem are the R2 utilities, which are
utility functions that are defined as a weighted integral over the scalarised
optimisation problems. As part of our work, we show that these utilities are
monotone and submodular set functions which can be optimised effectively using
greedy optimisation algorithms. We then analyse the performance of these greedy
algorithms both theoretically and empirically. Our analysis largely focusses on
Bayesian optimisation, which is a popular probabilistic framework for black-box
optimisation.","['Ben Tu', 'Nikolas Kantas', 'Robert M. Lee', 'Behrang Shafei']","['math.OC', 'cs.LG', 'stat.ML']",2023-05-19 16:01:35+00:00
http://arxiv.org/abs/2305.11766v2,Transfer operators on graphs: Spectral clustering and beyond,"Graphs and networks play an important role in modeling and analyzing complex
interconnected systems such as transportation networks, integrated circuits,
power grids, citation graphs, and biological and artificial neural networks.
Graph clustering algorithms can be used to detect groups of strongly connected
vertices and to derive coarse-grained models. We define transfer operators such
as the Koopman operator and the Perron-Frobenius operator on graphs, study
their spectral properties, introduce Galerkin projections of these operators,
and illustrate how reduced representations can be estimated from data. In
particular, we show that spectral clustering of undirected graphs can be
interpreted in terms of eigenfunctions of the Koopman operator and propose
novel clustering algorithms for directed graphs based on generalized transfer
operators. We demonstrate the efficacy of the resulting algorithms on several
benchmark problems and provide different interpretations of clusters.","['Stefan Klus', 'Maia Trower']","['stat.ML', 'cs.LG', 'cs.SI', 'math.DS']",2023-05-19 15:52:08+00:00
http://arxiv.org/abs/2305.11765v1,Tester-Learners for Halfspaces: Universal Algorithms,"We give the first tester-learner for halfspaces that succeeds universally
over a wide class of structured distributions. Our universal tester-learner
runs in fully polynomial time and has the following guarantee: the learner
achieves error $O(\mathrm{opt}) + \epsilon$ on any labeled distribution that
the tester accepts, and moreover, the tester accepts whenever the marginal is
any distribution that satisfies a Poincar\'e inequality. In contrast to prior
work on testable learning, our tester is not tailored to any single target
distribution but rather succeeds for an entire target class of distributions.
The class of Poincar\'e distributions includes all strongly log-concave
distributions, and, assuming the Kannan--L\'{o}vasz--Simonovits (KLS)
conjecture, includes all log-concave distributions. In the special case where
the label noise is known to be Massart, our tester-learner achieves error
$\mathrm{opt} + \epsilon$ while accepting all log-concave distributions
unconditionally (without assuming KLS). Our tests rely on checking
hypercontractivity of the unknown distribution using a sum-of-squares (SOS)
program, and crucially make use of the fact that Poincar\'e distributions are
certifiably hypercontractive in the SOS framework.","['Aravind Gollakota', 'Adam R. Klivans', 'Konstantinos Stavropoulos', 'Arsen Vasilyan']","['cs.LG', 'cs.DS', 'stat.ML']",2023-05-19 15:52:06+00:00
http://arxiv.org/abs/2305.11650v6,Moment Matching Denoising Gibbs Sampling,"Energy-Based Models (EBMs) offer a versatile framework for modeling complex
data distributions. However, training and sampling from EBMs continue to pose
significant challenges. The widely-used Denoising Score Matching (DSM) method
for scalable EBM training suffers from inconsistency issues, causing the energy
model to learn a `noisy' data distribution. In this work, we propose an
efficient sampling framework: (pseudo)-Gibbs sampling with moment matching,
which enables effective sampling from the underlying clean model when given a
`noisy' model that has been well-trained via DSM. We explore the benefits of
our approach compared to related methods and demonstrate how to scale the
method to high-dimensional datasets.","['Mingtian Zhang', 'Alex Hawkins-Hooker', 'Brooks Paige', 'David Barber']","['stat.ML', 'cs.LG']",2023-05-19 12:58:25+00:00
http://arxiv.org/abs/2305.11640v2,Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern,"This paper studies the open problem of conformalized entry prediction in a
row/column-exchangeable matrix. The matrix setting presents novel and unique
challenges, but there exists little work on this interesting topic. We
meticulously define the problem, differentiate it from closely related
problems, and rigorously delineate the boundary between achievable and
impossible goals. We then propose two practical algorithms. The first method
provides a fast emulation of the full conformal prediction, while the second
method leverages the technique of algorithmic stability for acceleration. Both
methods are computationally efficient and can effectively safeguard coverage
validity in presence of arbitrary missing pattern. Further, we quantify the
impact of missingness on prediction accuracy and establish fundamental limit
results. Empirical evidence from synthetic and real-world data sets
corroborates the superior performance of our proposed methods.","['Meijia Shao', 'Yuan Zhang']","['cs.LG', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2023-05-19 12:44:34+00:00
http://arxiv.org/abs/2305.11586v2,Bayesian approach to Gaussian process regression with uncertain inputs,"Conventional Gaussian process regression exclusively assumes the existence of
noise in the output data of model observations. In many scientific and
engineering applications, however, the input locations of observational data
may also be compromised with uncertainties owing to modeling assumptions,
measurement errors, etc. In this work, we propose a Bayesian method that
integrates the variability of input data into Gaussian process regression.
Considering two types of observables -- noise-corrupted outputs with fixed
inputs and those with prior-distribution-defined uncertain inputs, a posterior
distribution is estimated via a Bayesian framework to infer the uncertain data
locations. Thereafter, such quantified uncertainties of inputs are incorporated
into Gaussian process predictions by means of marginalization. The
effectiveness of this new regression technique is demonstrated through several
numerical examples, in which a consistently good performance of generalization
is observed, while a substantial reduction in the predictive uncertainties is
achieved by the Bayesian inference of uncertain inputs.","['Dongwei Ye', 'Mengwu Guo']","['cs.LG', 'cs.CE', 'stat.ML']",2023-05-19 10:53:08+00:00
http://arxiv.org/abs/2305.11575v1,The Deep Promotion Time Cure Model,"We propose a novel method for predicting time-to-event in the presence of
cure fractions based on flexible survivals models integrated into a deep neural
network framework. Our approach allows for non-linear relationships and
high-dimensional interactions between covariates and survival and is suitable
for large-scale applications. Furthermore, we allow the method to incorporate
an identified predictor formed of an additive decomposition of interpretable
linear and non-linear effects and add an orthogonalization layer to capture
potential higher dimensional interactions. We demonstrate the usefulness and
computational efficiency of our method via simulations and apply it to a large
portfolio of US mortgage loans. Here, we find not only a better predictive
performance of our framework but also a more realistic picture of covariate
effects.","['Victor Medina-Olivares', 'Stefan Lessmann', 'Nadja Klein']","['stat.ML', 'cs.LG']",2023-05-19 10:22:55+00:00
http://arxiv.org/abs/2305.11567v2,TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series,"Temporally indexed data are essential in a wide range of fields and of
interest to machine learning researchers. Time series data, however, are often
scarce or highly sensitive, which precludes the sharing of data between
researchers and industrial organizations and the application of existing and
new data-intensive ML methods. A possible solution to this bottleneck is to
generate synthetic data. In this work, we introduce Time Series Generative
Modeling (TSGM), an open-source framework for the generative modeling of
synthetic time series. TSGM includes a broad repertoire of machine learning
methods: generative models, probabilistic, and simulator-based approaches. The
framework enables users to evaluate the quality of the produced data from
different angles: similarity, downstream effectiveness, predictive consistency,
diversity, and privacy. The framework is extensible, which allows researchers
to rapidly implement their own methods and compare them in a shareable
environment. TSGM was tested on open datasets and in production and proved to
be beneficial in both cases. Additionally to the library, the project allows
users to employ command line interfaces for synthetic data generation which
lowers the entry threshold for those without a programming background.","['Alexander Nikitin', 'Letizia Iannucci', 'Samuel Kaski']","['cs.LG', 'stat.ML']",2023-05-19 10:11:21+00:00
http://arxiv.org/abs/2305.11509v6,From Random Search to Bandit Learning in Metric Measure Spaces,"Random Search is one of the most widely-used method for Hyperparameter
Optimization, and is critical to the success of deep learning models. Despite
its astonishing performance, little non-heuristic theory has been developed to
describe the underlying working mechanism. This paper gives a theoretical
accounting of Random Search. We introduce the concept of \emph{scattering
dimension} that describes the landscape of the underlying function, and
quantifies the performance of random search. We show that, when the environment
is noise-free, the output of random search converges to the optimal value in
probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T}
\right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering
dimension of the underlying function. When the observed function values are
corrupted by bounded $iid$ noise, the output of random search converges to the
optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left(
\frac{1}{T} \right)^{ \frac{1}{d_s + 1} } \right) $. In addition, based on the
principles of random search, we introduce an algorithm, called BLiN-MOS, for
Lipschitz bandits in doubling metric spaces that are also endowed with a
probability measure, and show that under mild conditions, BLiN-MOS achieves a
regret rate of order $ \widetilde{\mathcal{O}} \left( T^{ \frac{d_z}{d_z + 1} }
\right) $, where $d_z$ is the zooming dimension of the problem instance.","['Chuying Han', 'Yasong Feng', 'Tianyu Wang']","['cs.LG', 'stat.ML']",2023-05-19 08:18:49+00:00
http://arxiv.org/abs/2305.11493v1,Accelerating Convergence in Global Non-Convex Optimization with Reversible Diffusion,"Langevin Dynamics has been extensively employed in global non-convex
optimization due to the concentration of its stationary distribution around the
global minimum of the potential function at low temperatures. In this paper, we
propose to utilize a more comprehensive class of stochastic processes, known as
reversible diffusion, and apply the Euler-Maruyama discretization for global
non-convex optimization. We design the diffusion coefficient to be larger when
distant from the optimum and smaller when near, thus enabling accelerated
convergence while regulating discretization error, a strategy inspired by
landscape modifications. Our proposed method can also be seen as a time change
of Langevin Dynamics, and we prove convergence with respect to KL divergence,
investigating the trade-off between convergence speed and discretization error.
The efficacy of our proposed method is demonstrated through numerical
experiments.",['Ryo Fujino'],"['math.OC', 'stat.ML']",2023-05-19 07:49:40+00:00
http://arxiv.org/abs/2305.11475v3,Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models,"Generalized Additive Models (GAMs) have recently experienced a resurgence in
popularity due to their interpretability, which arises from expressing the
target value as a sum of non-linear transformations of the features. Despite
the current enthusiasm for GAMs, their susceptibility to concurvity - i.e.,
(possibly non-linear) dependencies between the features - has hitherto been
largely overlooked. Here, we demonstrate how concurvity can severly impair the
interpretability of GAMs and propose a remedy: a conceptually simple, yet
effective regularizer which penalizes pairwise correlations of the non-linearly
transformed feature variables. This procedure is applicable to any
differentiable additive model, such as Neural Additive Models or NeuralProphet,
and enhances interpretability by eliminating ambiguities due to self-canceling
feature contributions. We validate the effectiveness of our regularizer in
experiments on synthetic as well as real-world datasets for time-series and
tabular data. Our experiments show that concurvity in GAMs can be reduced
without significantly compromising prediction quality, improving
interpretability and reducing variance in the feature importances.","['Julien Siems', 'Konstantin Ditschuneit', 'Winfried Ripken', 'Alma Lindborg', 'Maximilian Schambach', 'Johannes S. Otterbach', 'Martin Genzel']","['cs.LG', 'stat.ML']",2023-05-19 06:55:49+00:00
http://arxiv.org/abs/2305.11463v4,Generative Sliced MMD Flows with Riesz Kernels,"Maximum mean discrepancy (MMD) flows suffer from high computational costs in
large scale computations. In this paper, we show that MMD flows with Riesz
kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which
allow their efficient computation. We prove that the MMD of Riesz kernels,
which is also known as energy distance, coincides with the MMD of their sliced
version. As a consequence, the computation of gradients of MMDs can be
performed in the one-dimensional setting. Here, for $r=1$, a simple sorting
algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to
$O((M+N)\log(M+N))$ for two measures with $M$ and $N$ support points. As
another interesting follow-up result, the MMD of compactly supported measures
can be estimated from above and below by the Wasserstein-1 distance. For the
implementations we approximate the gradient of the sliced MMD by using only a
finite number $P$ of slices. We show that the resulting error has complexity
$O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to
train generative models by approximating MMD gradient flows by neural networks
even for image applications. We demonstrate the efficiency of our model by
image generation on MNIST, FashionMNIST and CIFAR10.","['Johannes Hertrich', 'Christian Wald', 'Fabian Altekrüger', 'Paul Hagemann']","['cs.LG', 'math.PR', 'stat.ML']",2023-05-19 06:33:57+00:00
http://arxiv.org/abs/2305.11420v2,Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence,"Decentralized learning has recently been attracting increasing attention for
its applications in parallel computation and privacy preservation. Many recent
studies stated that the underlying network topology with a faster consensus
rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for
decentralized learning. However, a topology with a fast consensus rate, e.g.,
the exponential graph, generally has a large maximum degree, which incurs
significant communication costs. Thus, seeking topologies with both a fast
consensus rate and small maximum degree is important. In this study, we propose
a novel topology combining both a fast consensus rate and small maximum degree
called the Base-$(k + 1)$ Graph. Unlike the existing topologies, the Base-$(k +
1)$ Graph enables all nodes to reach the exact consensus after a finite number
of iterations for any number of nodes and maximum degree k. Thanks to this
favorable property, the Base-$(k + 1)$ Graph endows Decentralized SGD (DSGD)
with both a faster convergence rate and more communication efficiency than the
exponential graph. We conducted experiments with various topologies,
demonstrating that the Base-$(k + 1)$ Graph enables various decentralized
learning methods to achieve higher accuracy with better communication
efficiency than the existing topologies.","['Yuki Takezawa', 'Ryoma Sato', 'Han Bao', 'Kenta Niwa', 'Makoto Yamada']","['cs.LG', 'cs.DC', 'stat.ML']",2023-05-19 04:08:07+00:00
http://arxiv.org/abs/2305.11400v3,Mode-Aware Continual Learning for Conditional Generative Adversarial Networks,"The main challenge in continual learning for generative models is to
effectively learn new target modes with limited samples while preserving
previously learned ones. To this end, we introduce a new continual learning
approach for conditional generative adversarial networks by leveraging a
mode-affinity score specifically designed for generative modeling. First, the
generator produces samples of existing modes for subsequent replay. The
discriminator is then used to compute the mode similarity measure, which
identifies a set of closest existing modes to the target. Subsequently, a label
for the target mode is generated and given as a weighted average of the labels
within this set. We extend the continual learning model by training it on the
target data with the newly-generated label, while performing memory replay to
mitigate the risk of catastrophic forgetting. Experimental results on benchmark
datasets demonstrate the gains of our continual learning approach over the
state-of-the-art methods, even when using fewer training samples.","['Cat P. Le', 'Juncheng Dong', 'Ahmed Aloui', 'Vahid Tarokh']","['cs.LG', 'stat.ML']",2023-05-19 03:00:31+00:00
http://arxiv.org/abs/2305.11379v1,Generalized Precision Matrix for Scalable Estimation of Nonparametric Markov Networks,"A Markov network characterizes the conditional independence structure, or
Markov property, among a set of random variables. Existing work focuses on
specific families of distributions (e.g., exponential families) and/or certain
structures of graphs, and most of them can only handle variables of a single
data type (continuous or discrete). In this work, we characterize the
conditional independence structure in general distributions for all data types
(i.e., continuous, discrete, and mixed-type) with a Generalized Precision
Matrix (GPM). Besides, we also allow general functional relations among
variables, thus giving rise to a Markov network structure learning algorithm in
one of the most general settings. To deal with the computational challenge of
the problem, especially for large graphs, we unify all cases under the same
umbrella of a regularized score matching framework. We validate the theoretical
results and demonstrate the scalability empirically in various settings.","['Yujia Zheng', 'Ignavier Ng', 'Yewen Fan', 'Kun Zhang']","['cs.LG', 'stat.ML']",2023-05-19 01:53:10+00:00
http://arxiv.org/abs/2305.11353v1,Meta-learning for heterogeneous treatment effect estimation with closed-form solvers,"This article proposes a meta-learning method for estimating the conditional
average treatment effect (CATE) from a few observational data. The proposed
method learns how to estimate CATEs from multiple tasks and uses the knowledge
for unseen tasks. In the proposed method, based on the meta-learner framework,
we decompose the CATE estimation problem into sub-problems. For each
sub-problem, we formulate our estimation models using neural networks with
task-shared and task-specific parameters. With our formulation, we can obtain
optimal task-specific parameters in a closed form that are differentiable with
respect to task-shared parameters, making it possible to perform effective
meta-learning. The task-shared parameters are trained such that the expected
CATE estimation performance in few-shot settings is improved by minimizing the
difference between a CATE estimated with a large amount of data and one
estimated with just a few data. Our experimental results demonstrate that our
method outperforms the existing meta-learning approaches and CATE estimation
methods.","['Tomoharu Iwata', 'Yoichi Chikahara']","['stat.ML', 'cs.AI', 'cs.LG']",2023-05-19 00:07:38+00:00
http://arxiv.org/abs/2305.11308v2,MCD: A Model-Agnostic Counterfactual Search Method For Multi-modal Design Modifications,"Designers may often ask themselves how to adjust their design concepts to
achieve demanding functional goals. To answer such questions, designers must
often consider counterfactuals, weighing design alternatives and their
projected performance. This paper introduces Multi-objective Counterfactuals
for Design (MCD), a computational tool that automates and streamlines the
counterfactual search process and recommends targeted design modifications that
meet designers' unique requirements. MCD improves upon existing counterfactual
search methods by supporting multi-objective requirements, which are crucial in
design problems, and by decoupling the counterfactual search and sampling
processes, thus enhancing efficiency and facilitating objective trade-off
visualization. The paper showcases MCD's capabilities in complex engineering
tasks using three demonstrative bicycle design challenges. In the first, MCD
effectively identifies design modifications that quantifiably enhance
functional performance, strengthening the bike frame and saving weight. In the
second, MCD modifies parametric bike models in a cross-modal fashion to
resemble subjective text prompts or reference images. In a final
multidisciplinary case study, MCD tackles all the quantitative and subjective
design requirements introduced in the first two problems, while simultaneously
customizing a bike design to an individual rider's biomechanical attributes. By
exploring hypothetical design alterations and their impact on multiple design
objectives, MCD recommends effective design modifications for practitioners
seeking to make targeted enhancements to their designs. The code, test
problems, and datasets used in the paper are available to the public at
decode.mit.edu/projects/counterfactuals/.","['Lyle Regenwetter', 'Yazan Abu Obaideh', 'Faez Ahmed']","['cs.AI', 'stat.ML']",2023-05-18 21:10:58+00:00
http://arxiv.org/abs/2305.11283v5,On the Statistical Efficiency of Mean-Field Reinforcement Learning with General Function Approximation,"In this paper, we study the fundamental statistical efficiency of
Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG)
with general model-based function approximation. We introduce a new concept
called Mean-Field Model-Based Eluder Dimension (MF-MBED), which characterizes
the inherent complexity of mean-field model classes. We show that a rich family
of Mean-Field RL problems exhibits low MF-MBED. Additionally, we propose
algorithms based on maximal likelihood estimation, which can return an
$\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for
MFG. The overall sample complexity depends only polynomially on MF-MBED, which
is potentially much lower than the size of state-action space. Compared with
previous works, our results only require the minimal assumptions including
realizability and Lipschitz continuity.","['Jiawei Huang', 'Batuhan Yardim', 'Niao He']","['cs.LG', 'cs.AI', 'stat.ML']",2023-05-18 20:00:04+00:00
http://arxiv.org/abs/2305.11278v1,Real-Time Variational Method for Learning Neural Trajectory and its Dynamics,"Latent variable models have become instrumental in computational neuroscience
for reasoning about neural computation. This has fostered the development of
powerful offline algorithms for extracting latent neural trajectories from
neural recordings. However, despite the potential of real time alternatives to
give immediate feedback to experimentalists, and enhance experimental design,
they have received markedly less attention. In this work, we introduce the
exponential family variational Kalman filter (eVKF), an online recursive
Bayesian method aimed at inferring latent trajectories while simultaneously
learning the dynamical system generating them. eVKF works for arbitrary
likelihoods and utilizes the constant base measure exponential family to model
the latent state stochasticity. We derive a closed-form variational analogue to
the predict step of the Kalman filter which leads to a provably tighter bound
on the ELBO compared to another online variational method. We validate our
method on synthetic and real-world data, and, notably, show that it achieves
competitive performance","['Matthew Dowling', 'Yuan Zhao', 'Il Memming Park']","['stat.ML', 'cs.LG', 'q-bio.NC']",2023-05-18 19:52:46+00:00
http://arxiv.org/abs/2305.11241v2,"Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison","Evidence Networks can enable Bayesian model comparison when state-of-the-art
methods (e.g. nested sampling) fail and even when likelihoods or priors are
intractable or unknown. Bayesian model comparison, i.e. the computation of
Bayes factors or evidence ratios, can be cast as an optimization problem.
Though the Bayesian interpretation of optimal classification is well-known,
here we change perspective and present classes of loss functions that result in
fast, amortized neural estimators that directly estimate convenient functions
of the Bayes factor. This mitigates numerical inaccuracies associated with
estimating individual model probabilities. We introduce the leaky parity-odd
power (l-POP) transform, leading to the novel ``l-POP-Exponential'' loss
function. We explore neural density estimation for data probability in
different models, showing it to be less accurate and scalable than Evidence
Networks. Multiple real-world and synthetic examples illustrate that Evidence
Networks are explicitly independent of dimensionality of the parameter space
and scale mildly with the complexity of the posterior probability density
function. This simple yet powerful approach has broad implications for model
inference tasks. As an application of Evidence Networks to real-world data we
compute the Bayes factor for two models with gravitational lensing data of the
Dark Energy Survey. We briefly discuss applications of our methods to other,
related problems of model comparison and evaluation in implicit inference
settings.","['Niall Jeffrey', 'Benjamin D. Wandelt']","['cs.LG', 'astro-ph.CO', 'astro-ph.IM', 'stat.ML']",2023-05-18 18:14:53+00:00
http://arxiv.org/abs/2305.11165v2,The noise level in linear regression with dependent data,"We derive upper bounds for random design linear regression with dependent
($\beta$-mixing) data absent any realizability assumptions. In contrast to the
strictly realizable martingale noise regime, no sharp instance-optimal
non-asymptotics are available in the literature. Up to constant factors, our
analysis correctly recovers the variance term predicted by the Central Limit
Theorem -- the noise level of the problem -- and thus exhibits graceful
degradation as we introduce misspecification. Past a burn-in, our result is
sharp in the moderate deviations regime, and in particular does not inflate the
leading order term by mixing time factors.","['Ingvar Ziemann', 'Stephen Tu', 'George J. Pappas', 'Nikolai Matni']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2023-05-18 17:55:52+00:00
http://arxiv.org/abs/2305.11164v3,Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study,"The rise of machine learning (ML) systems has exacerbated their carbon
footprint due to increased capabilities and model sizes. However, there is
scarce knowledge on how the carbon footprint of ML models is actually measured,
reported, and evaluated. In light of this, the paper aims to analyze the
measurement of the carbon footprint of 1,417 ML models and associated datasets
on Hugging Face, which is the most popular repository for pretrained ML models.
The goal is to provide insights and recommendations on how to report and
optimize the carbon efficiency of ML models. The study includes the first
repository mining study on the Hugging Face Hub API on carbon emissions. This
study seeks to answer two research questions: (1) how do ML model creators
measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects
impact the carbon emissions of training ML models? The study yielded several
key findings. These include a stalled proportion of carbon emissions-reporting
models, a slight decrease in reported carbon footprint on Hugging Face over the
past 2 years, and a continued dominance of NLP as the main application domain.
Furthermore, the study uncovers correlations between carbon emissions and
various attributes such as model size, dataset size, and ML application
domains. These results highlight the need for software measurements to improve
energy reporting practices and promote carbon-efficient model development
within the Hugging Face community. In response to this issue, two
classifications are proposed: one for categorizing models based on their carbon
emission reporting practices and another for their carbon efficiency. The aim
of these classification proposals is to foster transparency and sustainable
model development within the ML community.","['Joel Castaño', 'Silverio Martínez-Fernández', 'Xavier Franch', 'Justus Bogner']","['cs.LG', 'cs.CY', 'cs.IR', 'stat.ML']",2023-05-18 17:52:58+00:00
http://arxiv.org/abs/2305.11132v2,Attacks on Online Learners: a Teacher-Student Analysis,"Machine learning models are famously vulnerable to adversarial attacks: small
ad-hoc perturbations of the data that can catastrophically alter the model
predictions. While a large literature has studied the case of test-time attacks
on pre-trained models, the important case of attacks in an online learning
setting has received little attention so far. In this work, we use a
control-theoretical perspective to study the scenario where an attacker may
perturb data labels to manipulate the learning dynamics of an online learner.
We perform a theoretical analysis of the problem in a teacher-student setup,
considering different attack strategies, and obtaining analytical results for
the steady state of simple linear learners. These results enable us to prove
that a discontinuous transition in the learner's accuracy occurs when the
attack strength exceeds a critical threshold. We then study empirically attacks
on learners with complex architectures using real data, confirming the insights
of our theoretical analysis. Our findings show that greedy attacks can be
extremely efficient, especially when data stream in small batches.","['Riccardo Giuseppe Margiotta', 'Sebastian Goldt', 'Guido Sanguinetti']","['stat.ML', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.CR', 'cs.LG']",2023-05-18 17:26:03+00:00
http://arxiv.org/abs/2305.11097v1,Statistical Foundations of Prior-Data Fitted Networks,"Prior-data fitted networks (PFNs) were recently proposed as a new paradigm
for machine learning. Instead of training the network to an observed training
set, a fixed model is pre-trained offline on small, simulated training sets
from a variety of tasks. The pre-trained model is then used to infer class
probabilities in-context on fresh training sets with arbitrary size and
distribution. Empirically, PFNs achieve state-of-the-art performance on tasks
with similar size to the ones used in pre-training. Surprisingly, their
accuracy further improves when passed larger data sets during inference. This
article establishes a theoretical foundation for PFNs and illuminates the
statistical mechanisms governing their behavior. While PFNs are motivated by
Bayesian ideas, a purely frequentistic interpretation of PFNs as pre-tuned, but
untrained predictors explains their behavior. A predictor's variance vanishes
if its sensitivity to individual training samples does and the bias vanishes
only if it is appropriately localized around the test feature. The transformer
architecture used in current PFN implementations ensures only the former. These
findings shall prove useful for designing architectures with favorable
empirical behavior.",['Thomas Nagler'],"['stat.ML', 'cs.LG']",2023-05-18 16:34:21+00:00
http://arxiv.org/abs/2305.11055v2,Small noise analysis for Tikhonov and RKHS regularizations,"Regularization plays a pivotal role in ill-posed machine learning and inverse
problems. However, the fundamental comparative analysis of various
regularization norms remains open. We establish a small noise analysis
framework to assess the effects of norms in Tikhonov and RKHS regularizations,
in the context of ill-posed linear inverse problems with Gaussian noise. This
framework studies the convergence rates of regularized estimators in the small
noise limit and reveals the potential instability of the conventional
L2-regularizer. We solve such instability by proposing an innovative class of
adaptive fractional RKHS regularizers, which covers the L2 Tikhonov and RKHS
regularizations by adjusting the fractional smoothness parameter. A surprising
insight is that over-smoothing via these fractional RKHSs consistently yields
optimal convergence rates, but the optimal hyper-parameter may decay too fast
to be selected in practice.","['Quanjun Lang', 'Fei Lu']","['stat.ML', 'cs.LG']",2023-05-18 15:50:33+00:00
http://arxiv.org/abs/2305.11046v2,Difference of Submodular Minimization via DC Programming,"Minimizing the difference of two submodular (DS) functions is a problem that
naturally occurs in various machine learning problems. Although it is well
known that a DS problem can be equivalently formulated as the minimization of
the difference of two convex (DC) functions, existing algorithms do not fully
exploit this connection. A classical algorithm for DC problems is called the DC
algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that
we apply to the DC program corresponding to DS minimization. We extend existing
convergence properties of DCA, and connect them to convergence properties on
the DS problem. Our results on DCA match the theoretical guarantees satisfied
by existing DS algorithms, while providing a more complete characterization of
convergence properties. In the case of CDCA, we obtain a stronger local
minimality guarantee. Our numerical results show that our proposed algorithms
outperform existing baselines on two applications: speech corpus selection and
feature selection.","['Marwa El Halabi', 'George Orfanides', 'Tim Hoheisel']","['cs.LG', 'cs.DM', 'cs.DS', 'math.OC', 'stat.ML']",2023-05-18 15:39:02+00:00
http://arxiv.org/abs/2305.11042v2,A unified framework for information-theoretic generalization bounds,"This paper presents a general methodology for deriving information-theoretic
generalization bounds for learning algorithms. The main technical tool is a
probabilistic decorrelation lemma based on a change of measure and a relaxation
of Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelation
lemma in combination with other techniques, such as symmetrization, couplings,
and chaining in the space of probability measures, we obtain new upper bounds
on the generalization error, both in expectation and in high probability, and
recover as special cases many of the existing generalization bounds, including
the ones based on mutual information, conditional mutual information,
stochastic chaining, and PAC-Bayes inequalities. In addition, the
Fernique-Talagrand upper bound on the expected supremum of a subgaussian
process emerges as a special case.","['Yifeng Chu', 'Maxim Raginsky']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2023-05-18 15:36:20+00:00
http://arxiv.org/abs/2305.11041v1,High-dimensional Asymptotics of Denoising Autoencoders,"We address the problem of denoising data from a Gaussian mixture using a
two-layer non-linear autoencoder with tied weights and a skip connection. We
consider the high-dimensional limit where the number of training samples and
the input dimension jointly tend to infinity while the number of hidden units
remains bounded. We provide closed-form expressions for the denoising
mean-squared test error. Building on this result, we quantitatively
characterize the advantage of the considered architecture over the autoencoder
without the skip connection that relates closely to principal component
analysis. We further show that our results accurately capture the learning
curves on a range of real data sets.","['Hugo Cui', 'Lenka Zdeborová']","['cs.LG', 'cond-mat.dis-nn', 'stat.ML']",2023-05-18 15:35:11+00:00
http://arxiv.org/abs/2305.11032v2,Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL,"While policy optimization algorithms have played an important role in recent
empirical success of Reinforcement Learning (RL), the existing theoretical
understanding of policy optimization remains rather limited -- they are either
restricted to tabular MDPs or suffer from highly suboptimal sample complexity,
especial in online RL where exploration is necessary. This paper proposes a
simple efficient policy optimization framework -- Optimistic NPG for online RL.
Optimistic NPG can be viewed as a simple combination of the classic natural
policy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy
evaluation subroutines to encourage exploration. For $d$-dimensional linear
MDPs, Optimistic NPG is computationally efficient, and learns an
$\varepsilon$-optimal policy within $\tilde{O}(d^2/\varepsilon^3)$ samples,
which is the first computationally efficient algorithm whose sample complexity
has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improves
over state-of-the-art results of policy optimization algorithms [Zanette et
al., 2021] by a factor of $d$. In the realm of general function approximation,
which subsumes linear MDPs, Optimistic NPG, to our best knowledge, stands as
the first policy optimization algorithm that achieves polynomial sample
complexity for learning near-optimal policies.","['Qinghua Liu', 'Gellért Weisz', 'András György', 'Chi Jin', 'Csaba Szepesvári']","['cs.LG', 'stat.ML']",2023-05-18 15:19:26+00:00
http://arxiv.org/abs/2305.11022v1,Massively Parallel Reweighted Wake-Sleep,"Reweighted wake-sleep (RWS) is a machine learning method for performing
Bayesian inference in a very general class of models. RWS draws $K$ samples
from an underlying approximate posterior, then uses importance weighting to
provide a better estimate of the true posterior. RWS then updates its
approximate posterior towards the importance-weighted estimate of the true
posterior. However, recent work [Chattergee and Diaconis, 2018] indicates that
the number of samples required for effective importance weighting is
exponential in the number of latent variables. Attaining such a large number of
importance samples is intractable in all but the smallest models. Here, we
develop massively parallel RWS, which circumvents this issue by drawing $K$
samples of all $n$ latent variables, and individually reasoning about all $K^n$
possible combinations of samples. While reasoning about $K^n$ combinations
might seem intractable, the required computations can be performed in
polynomial time by exploiting conditional independencies in the generative
model. We show considerable improvements over standard ""global"" RWS, which
draws $K$ samples from the full joint.","['Thomas Heap', 'Gavin Leech', 'Laurence Aitchison']","['cs.LG', 'cs.NE', 'stat.ML']",2023-05-18 15:03:56+00:00
http://arxiv.org/abs/2305.11005v2,Mode Connectivity in Auction Design,"Optimal auction design is a fundamental problem in algorithmic game theory.
This problem is notoriously difficult already in very simple settings. Recent
work in differentiable economics showed that neural networks can efficiently
learn known optimal auction mechanisms and discover interesting new ones. In an
attempt to theoretically justify their empirical success, we focus on one of
the first such networks, RochetNet, and a generalized version for affine
maximizer auctions. We prove that they satisfy mode connectivity, i.e., locally
optimal solutions are connected by a simple, piecewise linear path such that
every solution on the path is almost as good as one of the two local optima.
Mode connectivity has been recently investigated as an intriguing empirical and
theoretically justifiable property of neural networks used for prediction
problems. Our results give the first such analysis in the context of
differentiable economics, where neural networks are used directly for solving
non-convex optimization problems.","['Christoph Hertrich', 'Yixin Tao', 'László A. Végh']","['cs.GT', 'cs.LG', 'cs.NE', 'stat.ML']",2023-05-18 14:36:07+00:00
http://arxiv.org/abs/2305.11001v1,Dynamic Term Structure Models with Nonlinearities using Gaussian Processes,"The importance of unspanned macroeconomic variables for Dynamic Term
Structure Models has been intensively discussed in the literature. To our best
knowledge the earlier studies considered only linear interactions between the
economy and the real-world dynamics of interest rates in DTSMs. We propose a
generalized modelling setup for Gaussian DTSMs which allows for unspanned
nonlinear associations between the two and we exploit it in forecasting.
Specifically, we construct a custom sequential Monte Carlo estimation and
forecasting scheme where we introduce Gaussian Process priors to model
nonlinearities. Sequential scheme we propose can also be used with dynamic
portfolio optimization to assess the potential of generated economic value to
investors. The methodology is presented using US Treasury data and selected
macroeconomic indices. Namely, we look at core inflation and real economic
activity. We contrast the results obtained from the nonlinear model with those
stemming from an application of a linear model. Unlike for real economic
activity, in case of core inflation we find that, compared to linear models,
application of nonlinear models leads to statistically significant gains in
economic value across considered maturities.","['Tomasz Dubiel-Teleszynski', 'Konstantinos Kalogeropoulos', 'Nikolaos Karouzakis']","['stat.AP', 'stat.ME', 'stat.ML', '62F15 (Primary) 62L12, 62M20, 62P20, 91B55, 91B70, 91B82, 91B84,\n  91G30 (Secondary)']",2023-05-18 14:24:17+00:00
http://arxiv.org/abs/2305.10898v2,Estimation Beyond Data Reweighting: Kernel Method of Moments,"Moment restrictions and their conditional counterparts emerge in many areas
of machine learning and statistics ranging from causal inference to
reinforcement learning. Estimators for these tasks, generally called methods of
moments, include the prominent generalized method of moments (GMM) which has
recently gained attention in causal inference. GMM is a special case of the
broader family of empirical likelihood estimators which are based on
approximating a population distribution by means of minimizing a
$\varphi$-divergence to an empirical distribution. However, the use of
$\varphi$-divergences effectively limits the candidate distributions to
reweightings of the data samples. We lift this long-standing limitation and
provide a method of moments that goes beyond data reweighting. This is achieved
by defining an empirical likelihood estimator based on maximum mean discrepancy
which we term the kernel method of moments (KMM). We provide a variant of our
estimator for conditional moment restrictions and show that it is
asymptotically first-order optimal for such problems. Finally, we show that our
method achieves competitive performance on several conditional moment
restriction tasks.","['Heiner Kremer', 'Yassine Nemmour', 'Bernhard Schölkopf', 'Jia-Jie Zhu']","['cs.LG', 'stat.ML']",2023-05-18 11:52:43+00:00
http://arxiv.org/abs/2305.10886v1,Minimum-Risk Recalibration of Classifiers,"Recalibrating probabilistic classifiers is vital for enhancing the
reliability and accuracy of predictive models. Despite the development of
numerous recalibration algorithms, there is still a lack of a comprehensive
theory that integrates calibration and sharpness (which is essential for
maintaining predictive power). In this paper, we introduce the concept of
minimum-risk recalibration within the framework of mean-squared-error (MSE)
decomposition, offering a principled approach for evaluating and recalibrating
probabilistic classifiers. Using this framework, we analyze the uniform-mass
binning (UMB) recalibration method and establish a finite-sample risk upper
bound of order $\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of bins and $n$
is the sample size. By balancing calibration and sharpness, we further
determine that the optimal number of bins for UMB scales with $n^{1/3}$,
resulting in a risk bound of approximately $O(n^{-2/3})$. Additionally, we
tackle the challenge of label shift by proposing a two-stage approach that
adjusts the recalibration function using limited labeled data from the target
domain. Our results show that transferring a calibrated classifier requires
significantly fewer target samples compared to recalibrating from scratch. We
validate our theoretical findings through numerical simulations, which confirm
the tightness of the proposed bounds, the optimal number of bins, and the
effectiveness of label shift adaptation.","['Zeyu Sun', 'Dogyoon Song', 'Alfred Hero']","['cs.LG', 'stat.ME', 'stat.ML']",2023-05-18 11:27:02+00:00
http://arxiv.org/abs/2305.10880v3,Functional sufficient dimension reduction through information maximization with application to classification,"Considering the case where the response variable is a categorical variable
and the predictor is a random function, two novel functional sufficient
dimensional reduction (FSDR) methods are proposed based on mutual information
and square loss mutual information. Compared to the classical FSDR methods,
such as functional sliced inverse regression and functional sliced average
variance estimation, the proposed methods are appealing because they are
capable of estimating multiple effective dimension reduction directions in the
case of a relatively small number of categories, especially for the binary
response. Moreover, the proposed methods do not require the restrictive linear
conditional mean assumption and the constant covariance assumption. They avoid
the inverse problem of the covariance operator which is often encountered in
the functional sufficient dimension reduction. The functional principal
component analysis with truncation be used as a regularization mechanism. Under
some mild conditions, the statistical consistency of the proposed methods is
established. It is demonstrated that the two methods are competitive compared
with some existing FSDR methods by simulations and real data analyses.","['Xinyu Li', 'Jianjun Xu', 'Wenquan Cui', 'Haoyang Cheng']","['stat.ML', 'cs.LG']",2023-05-18 11:18:48+00:00
http://arxiv.org/abs/2305.10718v2,Discounted Thompson Sampling for Non-Stationary Bandit Problems,"Non-stationary multi-armed bandit (NS-MAB) problems have recently received
significant attention. NS-MAB are typically modelled in two scenarios: abruptly
changing, where reward distributions remain constant for a certain period and
change at unknown time steps, and smoothly changing, where reward distributions
evolve smoothly based on unknown dynamics. In this paper, we propose Discounted
Thompson Sampling (DS-TS) with Gaussian priors to address both non-stationary
settings. Our algorithm passively adapts to changes by incorporating a
discounted factor into Thompson Sampling. DS-TS method has been experimentally
validated, but analysis of the regret upper bound is currently lacking. Under
mild assumptions, we show that DS-TS with Gaussian priors can achieve nearly
optimal regret bound on the order of $\tilde{O}(\sqrt{TB_T})$ for abruptly
changing and $\tilde{O}(T^{\beta})$ for smoothly changing, where $T$ is the
number of time steps, $B_T$ is the number of breakpoints, $\beta$ is associated
with the smoothly changing environment and $\tilde{O}$ hides the parameters
independent of $T$ as well as logarithmic terms. Furthermore, empirical
comparisons between DS-TS and other non-stationary bandit algorithms
demonstrate its competitive performance. Specifically, when prior knowledge of
the maximum expected reward is available, DS-TS has the potential to outperform
state-of-the-art algorithms.","['Han Qi', 'Yue Wang', 'Li Zhu']","['cs.LG', 'stat.ML']",2023-05-18 05:29:52+00:00
http://arxiv.org/abs/2305.10697v2,The Blessing of Heterogeneity in Federated Q-Learning: Linear Speedup and Beyond,"When the data used for reinforcement learning (RL) are collected by multiple
agents in a distributed manner, federated versions of RL algorithms allow
collaborative learning without the need for agents to share their local data.
In this paper, we consider federated Q-learning, which aims to learn an optimal
Q-function by periodically aggregating local Q-estimates trained on local data
alone. Focusing on infinite-horizon tabular Markov decision processes, we
provide sample complexity guarantees for both the synchronous and asynchronous
variants of federated Q-learning. In both cases, our bounds exhibit a linear
speedup with respect to the number of agents and near-optimal dependencies on
other salient problem parameters.
  In the asynchronous setting, existing analyses of federated Q-learning, which
adopt an equally weighted averaging of local Q-estimates, require that every
agent covers the entire state-action space. In contrast, our improved sample
complexity scales inverse proportionally to the minimum entry of the average
stationary state-action occupancy distribution of all agents, thus only
requiring the agents to collectively cover the entire state-action space,
unveiling the blessing of heterogeneity in enabling collaborative learning by
relaxing the coverage requirement of the single-agent case. However, its sample
complexity still suffers when the local trajectories are highly heterogeneous.
In response, we propose a novel federated Q-learning algorithm with importance
averaging, giving larger weights to more frequently visited state-action pairs,
which achieves a robust linear speedup as if all trajectories are centrally
processed, regardless of the heterogeneity of local behavior policies.","['Jiin Woo', 'Gauri Joshi', 'Yuejie Chi']","['cs.LG', 'stat.ML']",2023-05-18 04:18:59+00:00
http://arxiv.org/abs/2305.10664v3,Posterior Inference on Shallow Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance,"From the classical and influential works of Neal (1996), it is known that the
infinite width scaling limit of a Bayesian neural network with one hidden layer
is a Gaussian process, when the network weights have bounded prior variance.
Neal's result has been extended to networks with multiple hidden layers and to
convolutional neural networks, also with Gaussian process scaling limits. The
tractable properties of Gaussian processes then allow straightforward posterior
inference and uncertainty quantification, considerably simplifying the study of
the limit process compared to a network of finite width. Neural network weights
with unbounded variance, however, pose unique challenges. In this case, the
classical central limit theorem breaks down and it is well known that the
scaling limit is an $\alpha$-stable process under suitable conditions. However,
current literature is primarily limited to forward simulations under these
processes and the problem of posterior inference under such a scaling limit
remains largely unaddressed, unlike in the Gaussian process case. To this end,
our contribution is an interpretable and computationally efficient procedure
for posterior inference, using a conditionally Gaussian representation, that
then allows full use of the Gaussian process machinery for tractable posterior
inference and uncertainty quantification in the non-Gaussian regime.","['Jorge Loría', 'Anindya Bhadra']","['stat.ML', 'cs.LG']",2023-05-18 02:55:00+00:00
http://arxiv.org/abs/2305.10636v1,Augmented Message Passing Stein Variational Gradient Descent,"Stein Variational Gradient Descent (SVGD) is a popular particle-based method
for Bayesian inference. However, its convergence suffers from the variance
collapse, which reduces the accuracy and diversity of the estimation. In this
paper, we study the isotropy property of finite particles during the
convergence process and show that SVGD of finite particles cannot spread across
the entire sample space. Instead, all particles tend to cluster around the
particle center within a certain range and we provide an analytical bound for
this cluster. To further improve the effectiveness of SVGD for high-dimensional
problems, we propose the Augmented Message Passing SVGD (AUMP-SVGD) method,
which is a two-stage optimization procedure that does not require sparsity of
the target distribution, unlike the MP-SVGD method. Our algorithm achieves
satisfactory accuracy and overcomes the variance collapse problem in various
benchmark problems.","['Jiankui Zhou', 'Yue Qiu']","['cs.LG', 'stat.ML', '62-08, 62G09', 'G.3; I.2']",2023-05-18 01:13:04+00:00
http://arxiv.org/abs/2305.10633v1,Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models,"We focus on the task of learning a single index model $\sigma(w^\star \cdot
x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions.
Prior work has shown that the sample complexity of learning $w^\star$ is
governed by the information exponent $k^\star$ of the link function $\sigma$,
which is defined as the index of the first nonzero Hermite coefficient of
$\sigma$. Ben Arous et al. (2021) showed that $n \gtrsim d^{k^\star-1}$ samples
suffice for learning $w^\star$ and that this is tight for online SGD. However,
the CSQ lower bound for gradient based methods only shows that $n \gtrsim
d^{k^\star/2}$ samples are necessary. In this work, we close the gap between
the upper and lower bounds by showing that online SGD on a smoothed loss learns
$w^\star$ with $n \gtrsim d^{k^\star/2}$ samples. We also draw connections to
statistical analyses of tensor PCA and to the implicit regularization effects
of minibatch SGD on empirical losses.","['Alex Damian', 'Eshaan Nichani', 'Rong Ge', 'Jason D. Lee']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2023-05-18 01:10:11+00:00
http://arxiv.org/abs/2305.10572v2,Tensor Products and Hyperdimensional Computing,"Following up on a previous analysis of graph embeddings, we generalize and
expand some results to the general setting of vector symbolic architectures
(VSA) and hyperdimensional computing (HDC). Importantly, we explore the
mathematical relationship between superposition, orthogonality, and tensor
product. We establish the tensor product representation as the central
representation, with a suite of unique properties. These include it being the
most general and expressive representation, as well as being the most
compressed representation that has errorrless unbinding and detection.",['Frank Qiu'],"['stat.ML', 'cs.LG', '68T30']",2023-05-17 21:18:35+00:00
http://arxiv.org/abs/2305.10564v2,Counterfactually Comparing Abstaining Classifiers,"Abstaining classifiers have the option to abstain from making predictions on
inputs that they are unsure about. These classifiers are becoming increasingly
popular in high-stakes decision-making problems, as they can withhold uncertain
predictions to improve their reliability and safety. When evaluating black-box
abstaining classifier(s), however, we lack a principled approach that accounts
for what the classifier would have predicted on its abstentions. These missing
predictions matter when they can eventually be utilized, either directly or as
a backup option in a failure mode. In this paper, we introduce a novel approach
and perspective to the problem of evaluating and comparing abstaining
classifiers by treating abstentions as missing data. Our evaluation approach is
centered around defining the counterfactual score of an abstaining classifier,
defined as the expected performance of the classifier had it not been allowed
to abstain. We specify the conditions under which the counterfactual score is
identifiable: if the abstentions are stochastic, and if the evaluation data is
independent of the training data (ensuring that the predictions are missing at
random), then the score is identifiable. Note that, if abstentions are
deterministic, then the score is unidentifiable because the classifier can
perform arbitrarily poorly on its abstentions. Leveraging tools from
observational causal inference, we then develop nonparametric and doubly robust
methods to efficiently estimate this quantity under identification. Our
approach is examined in both simulated and real data experiments.","['Yo Joong Choe', 'Aditya Gangrade', 'Aaditya Ramdas']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.ME']",2023-05-17 20:46:57+00:00
http://arxiv.org/abs/2305.11908v1,Sequential Best-Arm Identification with Application to Brain-Computer Interface,"A brain-computer interface (BCI) is a technology that enables direct
communication between the brain and an external device or computer system. It
allows individuals to interact with the device using only their thoughts, and
holds immense potential for a wide range of applications in medicine,
rehabilitation, and human augmentation. An electroencephalogram (EEG) and
event-related potential (ERP)-based speller system is a type of BCI that allows
users to spell words without using a physical keyboard, but instead by
recording and interpreting brain signals under different stimulus presentation
paradigms. Conventional non-adaptive paradigms treat each word selection
independently, leading to a lengthy learning process. To improve the sampling
efficiency, we cast the problem as a sequence of best-arm identification tasks
in multi-armed bandits. Leveraging pre-trained large language models (LLMs), we
utilize the prior knowledge learned from previous tasks to inform and
facilitate subsequent tasks. To do so in a coherent way, we propose a
sequential top-two Thompson sampling (STTS) algorithm under the
fixed-confidence setting and the fixed-budget setting. We study the theoretical
property of the proposed algorithm, and demonstrate its substantial empirical
improvement through both synthetic data analysis as well as a P300 BCI speller
simulator example.","['Xin Zhou', 'Botao Hao', 'Jian Kang', 'Tor Lattimore', 'Lexin Li']","['cs.HC', 'cs.LG', 'q-bio.NC', 'stat.ML']",2023-05-17 18:49:44+00:00
http://arxiv.org/abs/2305.10513v1,Learning Pose Image Manifolds Using Geometry-Preserving GANs and Elasticae,"This paper investigates the challenge of learning image manifolds,
specifically pose manifolds, of 3D objects using limited training data. It
proposes a DNN approach to manifold learning and for predicting images of
objects for novel, continuous 3D rotations. The approach uses two distinct
concepts: (1) Geometric Style-GAN (Geom-SGAN), which maps images to
low-dimensional latent representations and maintains the (first-order) manifold
geometry. That is, it seeks to preserve the pairwise distances between base
points and their tangent spaces, and (2) uses Euler's elastica to smoothly
interpolate between directed points (points + tangent directions) in the
low-dimensional latent space. When mapped back to the larger image space, the
resulting interpolations resemble videos of rotating objects. Extensive
experiments establish the superiority of this framework in learning paths on
rotation manifolds, both visually and quantitatively, relative to
state-of-the-art GANs and VAEs.","['Shenyuan Liang', 'Pavan Turaga', 'Anuj Srivastava']","['cs.CV', 'stat.ML']",2023-05-17 18:45:56+00:00
http://arxiv.org/abs/2305.10500v2,Learning Likelihood Ratios with Neural Network Classifiers,"The likelihood ratio is a crucial quantity for statistical inference in
science that enables hypothesis testing, construction of confidence intervals,
reweighting of distributions, and more. Many modern scientific applications,
however, make use of data- or simulation-driven models for which computing the
likelihood ratio can be very difficult or even impossible. By applying the
so-called ``likelihood ratio trick,'' approximations of the likelihood ratio
may be computed using clever parametrizations of neural network-based
classifiers. A number of different neural network setups can be defined to
satisfy this procedure, each with varying performance in approximating the
likelihood ratio when using finite training data. We present a series of
empirical studies detailing the performance of several common loss functionals
and parametrizations of the classifier output in approximating the likelihood
ratio of two univariate and multivariate Gaussian distributions as well as
simulated high-energy particle physics datasets.","['Shahzar Rizvi', 'Mariel Pettee', 'Benjamin Nachman']","['hep-ph', 'physics.data-an', 'stat.AP', 'stat.ML']",2023-05-17 18:11:38+00:00
http://arxiv.org/abs/2305.10413v3,On Consistency of Signature Using Lasso,"Signatures are iterated path integrals of continuous and discrete-time
processes, and their universal nonlinearity linearizes the problem of feature
selection in time series data analysis. This paper studies the consistency of
signature using Lasso regression, both theoretically and numerically. We
establish conditions under which the Lasso regression is consistent both
asymptotically and in finite sample. Furthermore, we show that the Lasso
regression is more consistent with the It\^o signature for time series and
processes that are closer to the Brownian motion and with weaker
inter-dimensional correlations, while it is more consistent with the
Stratonovich signature for mean-reverting time series and processes. We
demonstrate that signature can be applied to learn nonlinear functions and
option prices with high accuracy, and the performance depends on properties of
the underlying process and the choice of the signature.","['Xin Guo', 'Binnan Wang', 'Ruixun Zhang', 'Chaoyi Zhao']","['stat.ML', 'cs.LG', 'math.ST', 'stat.AP', 'stat.TH']",2023-05-17 17:48:52+00:00
http://arxiv.org/abs/2305.10391v2,Optimality of Message-Passing Architectures for Sparse Graphs,"We study the node classification problem on feature-decorated graphs in the
sparse setting, i.e., when the expected degree of a node is $O(1)$ in the
number of nodes, in the fixed-dimensional asymptotic regime, i.e., the
dimension of the feature data is fixed while the number of nodes is large. Such
graphs are typically known to be locally tree-like. We introduce a notion of
Bayes optimality for node classification tasks, called asymptotic local Bayes
optimality, and compute the optimal classifier according to this criterion for
a fairly general statistical data model with arbitrary distributions of the
node features and edge connectivity. The optimal classifier is implementable
using a message-passing graph neural network architecture. We then compute the
generalization error of this classifier and compare its performance against
existing learning methods theoretically on a well-studied statistical model
with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find
that the optimal message-passing architecture interpolates between a standard
MLP in the regime of low graph signal and a typical convolution in the regime
of high graph signal. Furthermore, we prove a corresponding non-asymptotic
result.","['Aseem Baranwal', 'Kimon Fountoulakis', 'Aukosh Jagannath']","['cs.LG', 'stat.ML']",2023-05-17 17:31:20+00:00
http://arxiv.org/abs/2305.10379v3,Active Learning in Symbolic Regression with Physical Constraints,"Evolutionary symbolic regression (SR) fits a symbolic equation to data, which
gives a concise interpretable model. We explore using SR as a method to propose
which data to gather in an active learning setting with physical constraints.
SR with active learning proposes which experiments to do next. Active learning
is done with query by committee, where the Pareto frontier of equations is the
committee. The physical constraints improve proposed equations in very low data
settings. These approaches reduce the data required for SR and achieves state
of the art results in data required to rediscover known equations.","['Jorge Medina', 'Andrew D. White']","['cs.LG', 'cs.NE', 'physics.chem-ph', 'stat.ML']",2023-05-17 17:07:25+00:00
http://arxiv.org/abs/2305.10282v1,Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning,"This paper studies tabular reinforcement learning (RL) in the hybrid setting,
which assumes access to both an offline dataset and online interactions with
the unknown environment. A central question boils down to how to efficiently
utilize online data collection to strengthen and complement the offline dataset
and enable effective policy fine-tuning. Leveraging recent advances in
reward-agnostic exploration and model-based offline RL, we design a three-stage
hybrid RL algorithm that beats the best of both worlds -- pure offline RL and
pure online RL -- in terms of sample complexities. The proposed algorithm does
not require any reward information during data collection. Our theory is
developed based on a new notion called single-policy partial concentrability,
which captures the trade-off between distribution mismatch and miscoverage and
guides the interplay between offline and online data.","['Gen Li', 'Wenhao Zhan', 'Jason D. Lee', 'Yuejie Chi', 'Yuxin Chen']","['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2023-05-17 15:17:23+00:00
http://arxiv.org/abs/2305.10227v1,Reaching Kesten-Stigum Threshold in the Stochastic Block Model under Node Corruptions,"We study robust community detection in the context of node-corrupted
stochastic block model, where an adversary can arbitrarily modify all the edges
incident to a fraction of the $n$ vertices. We present the first
polynomial-time algorithm that achieves weak recovery at the Kesten-Stigum
threshold even in the presence of a small constant fraction of corrupted nodes.
Prior to this work, even state-of-the-art robust algorithms were known to break
under such node corruption adversaries, when close to the Kesten-Stigum
threshold.
  We further extend our techniques to the $Z_2$ synchronization problem, where
our algorithm reaches the optimal recovery threshold in the presence of similar
strong adversarial perturbations.
  The key ingredient of our algorithm is a novel identifiability proof that
leverages the push-out effect of the Grothendieck norm of principal
submatrices.","['Jingqiu Ding', ""Tommaso d'Orsi"", 'Yiding Hua', 'David Steurer']","['cs.LG', 'cs.SI', 'stat.ML']",2023-05-17 14:03:47+00:00
http://arxiv.org/abs/2305.10219v1,"Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection","Support Vector Machine (SVM) is a robust machine learning algorithm with
broad applications in classification, regression, and outlier detection. SVM
requires tuning the regularization parameter (RP) which controls the model
capacity and the generalization performance. Conventionally, the optimum RP is
found by comparison of a range of values through the Cross-Validation (CV)
procedure. In addition, for non-linearly separable data, the SVM uses kernels
where a set of kernels, each with a set of parameters, denoted as a grid of
kernels, are considered. The optimal choice of RP and the grid of kernels is
through the grid-search of CV. By stochastically analyzing the behavior of the
regularization parameter, this work shows that the SVM performance can be
modeled as a function of separability and scatteredness (S&S) of the data.
Separability is a measure of the distance between classes, and scatteredness is
the ratio of the spread of data points. In particular, for the hinge loss cost
function, an S&S ratio-based table provides the optimum RP. The S&S ratio is a
powerful value that can automatically detect linear or non-linear separability
before using the SVM algorithm. The provided S&S ratio-based table can also
provide the optimum kernel and its parameters before using the SVM algorithm.
Consequently, the computational complexity of the CV grid-search is reduced to
only one time use of the SVM. The simulation results on the real dataset
confirm the superiority and efficiency of the proposed approach in the sense of
computational complexity over the grid-search CV method.","['Mahdi Shamsi', 'Soosan Beheshti']","['stat.ML', 'cs.AI', 'cs.LG', 'eess.SP']",2023-05-17 13:51:43+00:00
http://arxiv.org/abs/2305.10187v1,Evaluating Dynamic Conditional Quantile Treatment Effects with Applications in Ridesharing,"Many modern tech companies, such as Google, Uber, and Didi, utilize online
experiments (also known as A/B testing) to evaluate new policies against
existing ones. While most studies concentrate on average treatment effects,
situations with skewed and heavy-tailed outcome distributions may benefit from
alternative criteria, such as quantiles. However, assessing dynamic quantile
treatment effects (QTE) remains a challenge, particularly when dealing with
data from ride-sourcing platforms that involve sequential decision-making
across time and space. In this paper, we establish a formal framework to
calculate QTE conditional on characteristics independent of the treatment.
Under specific model assumptions, we demonstrate that the dynamic conditional
QTE (CQTE) equals the sum of individual CQTEs across time, even though the
conditional quantile of cumulative rewards may not necessarily equate to the
sum of conditional quantiles of individual rewards. This crucial insight
significantly streamlines the estimation and inference processes for our target
causal estimand. We then introduce two varying coefficient decision process
(VCDP) models and devise an innovative method to test the dynamic CQTE.
Moreover, we expand our approach to accommodate data from spatiotemporal
dependent experiments and examine both conditional quantile direct and indirect
effects. To showcase the practical utility of our method, we apply it to three
real-world datasets from a ride-sourcing platform. Theoretical findings and
comprehensive simulation studies further substantiate our proposal.","['Ting Li', 'Chengchun Shi', 'Zhaohua Lu', 'Yi Li', 'Hongtu Zhu']","['stat.ME', 'cs.LG', 'stat.ML']",2023-05-17 13:12:48+00:00
http://arxiv.org/abs/2305.10185v1,Algorithms for Boolean Matrix Factorization using Integer Programming,"Boolean matrix factorization (BMF) approximates a given binary input matrix
as the product of two smaller binary factors. As opposed to binary matrix
factorization which uses standard arithmetic, BMF uses the Boolean OR and
Boolean AND operations to perform matrix products, which leads to lower
reconstruction errors. BMF is an NP-hard problem. In this paper, we first
propose an alternating optimization (AO) strategy that solves the subproblem in
one factor matrix in BMF using an integer program (IP). We also provide two
ways to initialize the factors within AO. Then, we show how several solutions
of BMF can be combined optimally using another IP. This allows us to come up
with a new algorithm: it generates several solutions using AO and then combines
them in an optimal way. Experiments show that our algorithms (available on
gitlab) outperform the state of the art on medium-scale problems.","['Christos Kolomvakis', 'Arnaud Vandaele', 'Nicolas Gillis']","['math.OC', 'cs.LG', 'eess.SP', 'stat.ML']",2023-05-17 13:09:23+00:00
http://arxiv.org/abs/2305.10158v1,A Global-Local Approximation Framework for Large-Scale Gaussian Process Modeling,"In this work, we propose a novel framework for large-scale Gaussian process
(GP) modeling. Contrary to the global, and local approximations proposed in the
literature to address the computational bottleneck with exact GP modeling, we
employ a combined global-local approach in building the approximation. Our
framework uses a subset-of-data approach where the subset is a union of a set
of global points designed to capture the global trend in the data, and a set of
local points specific to a given testing location to capture the local trend
around the testing location. The correlation function is also modeled as a
combination of a global, and a local kernel. The performance of our framework,
which we refer to as TwinGP, is on par or better than the state-of-the-art GP
modeling methods at a fraction of their computational cost.","['Akhil Vakayil', 'Roshan Joseph']","['stat.ML', 'cs.LG']",2023-05-17 12:19:59+00:00
http://arxiv.org/abs/2305.10114v1,Automatic Hyperparameter Tuning in Sparse Matrix Factorization,"We study the problem of hyperparameter tuning in sparse matrix factorization
under Bayesian framework. In the prior work, an analytical solution of sparse
matrix factorization with Laplace prior was obtained by variational Bayes
method under several approximations. Based on this solution, we propose a novel
numerical method of hyperparameter tuning by evaluating the zero point of
normalization factor in sparse matrix prior. We also verify that our method
shows excellent performance for ground-truth sparse matrix reconstruction by
comparing it with the widely-used algorithm of sparse principal component
analysis.","['Ryota Kawasumi', 'Koujin Takeda']","['stat.ML', 'cond-mat.dis-nn', 'cs.IT', 'cs.LG', 'math.IT']",2023-05-17 10:40:17+00:00
http://arxiv.org/abs/2305.10089v2,A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization,"We prove Wasserstein inverse reinforcement learning enables the learner's
reward values to imitate the expert's reward values in a finite iteration for
multi-objective optimizations. Moreover, we prove Wasserstein inverse
reinforcement learning enables the learner's optimal solutions to imitate the
expert's optimal solutions for multi-objective optimizations with lexicographic
order.","['Akira Kitaoka', 'Riki Eto']","['cs.LG', 'cs.AI', 'stat.ML']",2023-05-17 09:48:02+00:00
http://arxiv.org/abs/2305.10042v1,Optimal Weighted Random Forests,"The random forest (RF) algorithm has become a very popular prediction method
for its great flexibility and promising accuracy. In RF, it is conventional to
put equal weights on all the base learners (trees) to aggregate their
predictions. However, the predictive performances of different trees within the
forest can be very different due to the randomization of the embedded bootstrap
sampling and feature selection. In this paper, we focus on RF for regression
and propose two optimal weighting algorithms, namely the 1 Step Optimal
Weighted RF (1step-WRF$_\mathrm{opt}$) and 2 Steps Optimal Weighted RF
(2steps-WRF$_\mathrm{opt}$), that combine the base learners through the weights
determined by weight choice criteria. Under some regularity conditions, we show
that these algorithms are asymptotically optimal in the sense that the
resulting squared loss and risk are asymptotically identical to those of the
infeasible but best possible model averaging estimator. Numerical studies
conducted on real-world data sets indicate that these algorithms outperform the
equal-weight forest and two other weighted RFs proposed in existing literature
in most cases.","['Xinyu Chen', 'Dalei Yu', 'Xinyu Zhang']","['stat.ML', 'cs.LG']",2023-05-17 08:36:43+00:00
http://arxiv.org/abs/2305.10015v3,Utility Theory of Synthetic Data Generation,"Synthetic data algorithms are widely employed in industries to generate
artificial data for downstream learning tasks. While existing research
primarily focuses on empirically evaluating utility of synthetic data, its
theoretical understanding is largely lacking. This paper bridges the
practice-theory gap by establishing relevant utility theory in a statistical
learning framework. It considers two utility metrics: generalization and
ranking of models trained on synthetic data. The former is defined as the
generalization difference between models trained on synthetic and on real data.
By deriving analytical bounds for this utility metric, we demonstrate that the
synthetic feature distribution does not need to be similar as that of real data
for ensuring comparable generalization of synthetic models, provided proper
model specifications in downstream learning tasks. The latter utility metric
studies the relative performance of models trained on synthetic data. In
particular, we discover that the distribution of synthetic data is not
necessarily similar as the real one to ensure consistent model comparison.
Interestingly, consistent model comparison is still achievable even when
synthetic responses are not well generated, as long as downstream models are
separable by a generalization gap. Finally, extensive experiments on
non-parametric models and deep neural networks have been conducted to validate
these theoretical findings.","['Shirong Xu', 'Will Wei Sun', 'Guang Cheng']","['stat.ML', 'cs.LG']",2023-05-17 07:49:16+00:00
http://arxiv.org/abs/2305.09957v2,Deep quantum neural networks form Gaussian processes,"It is well known that artificial neural networks initialized from independent
and identically distributed priors converge to Gaussian processes in the limit
of large number of neurons per hidden layer. In this work we prove an analogous
result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of
certain models based on Haar random unitary or orthogonal deep QNNs converge to
Gaussian processes in the limit of large Hilbert space dimension $d$. The
derivation of this result is more nuanced than in the classical case due to the
role played by the input states, the measurement observable, and the fact that
the entries of unitary matrices are not independent. An important consequence
of our analysis is that the ensuing Gaussian processes cannot be used to
efficiently predict the outputs of the QNN via Bayesian statistics.
Furthermore, our theorems imply that the concentration of measure phenomenon in
Haar random QNNs is worse than previously thought, as we prove that expectation
values and gradients concentrate as $\mathcal{O}\left(\frac{1}{e^d
\sqrt{d}}\right)$. Finally, we discuss how our results improve our
understanding of concentration in $t$-designs.","['Diego García-Martín', 'Martin Larocca', 'M. Cerezo']","['quant-ph', 'cs.LG', 'stat.ML']",2023-05-17 05:32:45+00:00
http://arxiv.org/abs/2305.09930v1,Model-based Validation as Probabilistic Inference,"Estimating the distribution over failures is a key step in validating
autonomous systems. Existing approaches focus on finding failures for a small
range of initial conditions or make restrictive assumptions about the
properties of the system under test. We frame estimating the distribution over
failure trajectories for sequential systems as Bayesian inference. Our
model-based approach represents the distribution over failure trajectories
using rollouts of system dynamics and computes trajectory gradients using
automatic differentiation. Our approach is demonstrated in an inverted pendulum
control system, an autonomous vehicle driving scenario, and a partially
observable lunar lander. Sampling is performed using an off-the-shelf
implementation of Hamiltonian Monte Carlo with multiple chains to capture
multimodality and gradient smoothing for safe trajectories. In all experiments,
we observed improvements in sample efficiency and parameter space coverage
compared to black-box baseline approaches. This work is open sourced.","['Harrison Delecki', 'Anthony Corso', 'Mykel J. Kochenderfer']","['cs.RO', 'cs.LG', 'stat.ML']",2023-05-17 03:27:36+00:00
http://arxiv.org/abs/2305.09869v1,A Signed Subgraph Encoding Approach via Linear Optimization for Link Sign Prediction,"In this paper, we consider the problem of inferring the sign of a link based
on limited sign data in signed networks. Regarding this link sign prediction
problem, SDGNN (Signed Directed Graph Neural Networks) provides the best
prediction performance currently to the best of our knowledge. In this paper,
we propose a different link sign prediction architecture call SELO (Subgraph
Encoding via Linear Optimization), which obtains overall leading prediction
performances compared the state-of-the-art algorithm SDGNN. The proposed model
utilizes a subgraph encoding approach to learn edge embeddings for signed
directed networks. In particular, a signed subgraph encoding approach is
introduced to embed each subgraph into a likelihood matrix instead of the
adjacency matrix through a linear optimization method. Comprehensive
experiments are conducted on six real-world signed networks with AUC, F1,
micro-F1, and Macro-F1 as the evaluation metrics. The experiment results show
that the proposed SELO model outperforms existing baseline feature-based
methods and embedding-based methods on all the six real-world networks and in
all the four evaluation metrics.","['Zhihong Fang', 'Shaolin Tan', 'Yaonan Wang']","['cs.LG', 'cs.SI', 'stat.ML']",2023-05-17 00:46:01+00:00
http://arxiv.org/abs/2305.09828v1,Mimetic Initialization of Self-Attention Layers,"It is notoriously difficult to train Transformers on small datasets;
typically, large pre-trained models are instead used as the starting point. We
explore the weights of such pre-trained Transformers (particularly for vision)
to attempt to find reasons for this discrepancy. Surprisingly, we find that
simply initializing the weights of self-attention layers so that they ""look""
more like their pre-trained counterparts allows us to train vanilla
Transformers faster and to higher final accuracies, particularly on vision
tasks such as CIFAR-10 and ImageNet classification, where we see gains in
accuracy of over 5% and 4%, respectively. Our initialization scheme is closed
form, learning-free, and very simple: we set the product of the query and key
weights to be approximately the identity, and the product of the value and
projection weights to approximately the negative identity. As this mimics the
patterns we saw in pre-trained Transformers, we call the technique ""mimetic
initialization"".","['Asher Trockman', 'J. Zico Kolter']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2023-05-16 22:12:25+00:00
http://arxiv.org/abs/2305.09739v2,Outage Performance and Novel Loss Function for an ML-Assisted Resource Allocation: An Exact Analytical Framework,"We introduce a novel loss function to minimize the outage probability of an
ML-based resource allocation system. A single-user multi-resource greedy
allocation strategy constitutes our application scenario, for which an ML
binary classification predictor assists in selecting a resource satisfying the
established outage criterium. While other resource allocation policies may be
suitable, they are not the focus of our study. Instead, our primary emphasis is
on theoretically developing this loss function and leveraging it to train an ML
model to address the outage probability challenge. With no access to future
channel state information, this predictor foresees each resource's likely
future outage status. When the predictor encounters a resource it believes will
be satisfactory, it allocates it to the user. Our main result establishes exact
and asymptotic expressions for this system's outage probability. These
expressions reveal that focusing solely on the optimization of the per-resource
outage probability conditioned on the ML predictor recommending resource
allocation (a strategy that appears to be most appropriate) may produce
inadequate predictors that reject every resource. They also reveal that
focusing on standard metrics, like precision, false-positive rate, or recall,
may not produce optimal predictors. With our result, we formulate a
theoretically optimal, differentiable loss function to train our predictor. We
then compare predictors trained using this and traditional loss functions
namely, binary cross-entropy (BCE), mean squared error (MSE), and mean absolute
error (MAE). In all scenarios, predictors trained using our novel loss function
provide superior outage probability performance. Moreover, in some cases, our
loss function outperforms predictors trained with BCE, MAE, and MSE by multiple
orders of magnitude.","['Nidhi Simmons', 'David E Simmons', 'Michel Daoud Yacoub']","['eess.SP', 'stat.ML']",2023-05-16 18:23:52+00:00
http://arxiv.org/abs/2305.09659v2,Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage,"In this paper, we study distributionally robust offline reinforcement
learning (robust offline RL), which seeks to find an optimal policy purely from
an offline dataset that can perform well in perturbed environments. In
specific, we propose a generic algorithm framework called Doubly Pessimistic
Model-based Policy Optimization ($P^2MPO$), which features a novel combination
of a flexible model estimation subroutine and a doubly pessimistic policy
optimization step. Notably, the double pessimism principle is crucial to
overcome the distributional shifts incurred by (i) the mismatch between the
behavior policy and the target policies; and (ii) the perturbation of the
nominal model. Under certain accuracy conditions on the model estimation
subroutine, we prove that $P^2MPO$ is sample-efficient with robust partial
coverage data, which only requires the offline data to have good coverage of
the distributions induced by the optimal robust policy and the perturbed models
around the nominal model.
  By tailoring specific model estimation subroutines for concrete examples of
RMDPs, including tabular RMDPs, factored RMDPs, kernel and neural RMDPs, we
prove that $P^2MPO$ enjoys a $\tilde{\mathcal{O}}(n^{-1/2})$ convergence rate,
where $n$ is the dataset size. We highlight that all these examples, except
tabular RMDPs, are first identified and proven tractable by this work.
Furthermore, we continue our study of robust offline RL in the robust Markov
games (RMGs). By extending the double pessimism principle identified for
single-agent RMDPs, we propose another algorithm framework that can efficiently
find the robust Nash equilibria among players using only robust unilateral
(partial) coverage data. To our best knowledge, this work proposes the first
general learning principle -- double pessimism -- for robust offline RL and
shows that it is provably efficient with general function approximation.","['Jose Blanchet', 'Miao Lu', 'Tong Zhang', 'Han Zhong']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2023-05-16 17:58:05+00:00
http://arxiv.org/abs/2305.09626v1,Balancing Risk and Reward: An Automated Phased Release Strategy,"Phased releases are a common strategy in the technology industry for
gradually releasing new products or updates through a sequence of A/B tests in
which the number of treated units gradually grows until full deployment or
deprecation. Performing phased releases in a principled way requires selecting
the proportion of units assigned to the new release in a way that balances the
risk of an adverse effect with the need to iterate and learn from the
experiment rapidly. In this paper, we formalize this problem and propose an
algorithm that automatically determines the release percentage at each stage in
the schedule, balancing the need to control risk while maximizing ramp-up
speed. Our framework models the challenge as a constrained batched bandit
problem that ensures that our pre-specified experimental budget is not depleted
with high probability. Our proposed algorithm leverages an adaptive Bayesian
approach in which the maximal number of units assigned to the treatment is
determined by the posterior distribution, ensuring that the probability of
depleting the remaining budget is low. Notably, our approach analytically
solves the ramp sizes by inverting probability bounds, eliminating the need for
challenging rare-event Monte Carlo simulation. It only requires computing means
and variances of outcome subsets, making it highly efficient and
parallelizable.","['Yufan Li', 'Jialiang Mao', 'Iavor Bojinov']","['stat.ML', 'cs.LG', 'stat.ME']",2023-05-16 17:27:34+00:00
http://arxiv.org/abs/2305.09619v1,The Power of Learned Locally Linear Models for Nonlinear Policy Optimization,"A common pipeline in learning-based control is to iteratively estimate a
model of system dynamics, and apply a trajectory optimization algorithm -
e.g.~$\mathtt{iLQR}$ - on the learned model to minimize a target cost. This
paper conducts a rigorous analysis of a simplified variant of this strategy for
general nonlinear systems. We analyze an algorithm which iterates between
estimating local linear models of nonlinear system dynamics and performing
$\mathtt{iLQR}$-like policy updates. We demonstrate that this algorithm attains
sample complexity polynomial in relevant problem parameters, and, by
synthesizing locally stabilizing gains, overcomes exponential dependence in
problem horizon. Experimental results validate the performance of our
algorithm, and compare to natural deep-learning baselines.","['Daniel Pfrommer', 'Max Simchowitz', 'Tyler Westenbroek', 'Nikolai Matni', 'Stephen Tu']","['cs.LG', 'math.OC', 'stat.ML']",2023-05-16 17:13:00+00:00
http://arxiv.org/abs/2305.09605v3,To smooth a cloud or to pin it down: Guarantees and Insights on Score Matching in Denoising Diffusion Models,"Denoising diffusion models are a class of generative models which have
recently achieved state-of-the-art results across many domains. Gradual noise
is added to the data using a diffusion process, which transforms the data
distribution into a Gaussian. Samples from the generative model are then
obtained by simulating an approximation of the time reversal of this diffusion
initialized by Gaussian samples. Recent research has explored adapting
diffusion models for sampling and inference tasks. In this paper, we leverage
known connections to stochastic control akin to the F\""ollmer drift to extend
established neural network approximation results for the F\""ollmer drift to
denoising diffusion models and samplers.","['Francisco Vargas', 'Teodora Reu', 'Anna Kerekes', 'Michael M Bronstein']","['stat.ML', 'cs.LG']",2023-05-16 16:56:19+00:00
http://arxiv.org/abs/2305.09565v1,Toward Falsifying Causal Graphs Using a Permutation-Based Test,"Understanding the causal relationships among the variables of a system is
paramount to explain and control its behaviour. Inferring the causal graph from
observational data without interventions, however, requires a lot of strong
assumptions that are not always realistic. Even for domain experts it can be
challenging to express the causal graph. Therefore, metrics that quantitatively
assess the goodness of a causal graph provide helpful checks before using it in
downstream tasks. Existing metrics provide an absolute number of
inconsistencies between the graph and the observed data, and without a
baseline, practitioners are left to answer the hard question of how many such
inconsistencies are acceptable or expected. Here, we propose a novel
consistency metric by constructing a surrogate baseline through node
permutations. By comparing the number of inconsistencies with those on the
surrogate baseline, we derive an interpretable metric that captures whether the
DAG fits significantly better than random. Evaluating on both simulated and
real data sets from various domains, including biology and cloud monitoring, we
demonstrate that the true DAG is not falsified by our metric, whereas the wrong
graphs given by a hypothetical user are likely to be falsified.","['Elias Eulig', 'Atalanti A. Mastakouri', 'Patrick Blöbaum', 'Michaela Hardt', 'Dominik Janzing']","['stat.ML', 'cs.LG']",2023-05-16 16:02:18+00:00
http://arxiv.org/abs/2305.09557v2,Learning from Aggregated Data: Curated Bags versus Random Bags,"Protecting user privacy is a major concern for many machine learning systems
that are deployed at scale and collect from a diverse set of population. One
way to address this concern is by collecting and releasing data labels in an
aggregated manner so that the information about a single user is potentially
combined with others. In this paper, we explore the possibility of training
machine learning models with aggregated data labels, rather than individual
labels. Specifically, we consider two natural aggregation procedures suggested
by practitioners: curated bags where the data points are grouped based on
common features and random bags where the data points are grouped randomly in
bag of similar sizes. For the curated bag setting and for a broad range of loss
functions, we show that we can perform gradient-based learning without any
degradation in performance that may result from aggregating data. Our method is
based on the observation that the sum of the gradients of the loss function on
individual data examples in a curated bag can be computed from the aggregate
label without the need for individual labels. For the random bag setting, we
provide a generalization risk bound based on the Rademacher complexity of the
hypothesis class and show how empirical risk minimization can be regularized to
achieve the smallest risk bound. In fact, in the random bag setting, there is a
trade-off between size of the bag and the achievable error rate as our bound
indicates. Finally, we conduct a careful empirical study to confirm our
theoretical findings. In particular, our results suggest that aggregate
learning can be an effective method for preserving user privacy while
maintaining model accuracy.","['Lin Chen', 'Gang Fu', 'Amin Karbasi', 'Vahab Mirrokni']","['cs.LG', 'cs.AI', 'stat.ML']",2023-05-16 15:53:45+00:00
http://arxiv.org/abs/2305.09536v1,A Comparative Study of Methods for Estimating Conditional Shapley Values and When to Use Them,"Shapley values originated in cooperative game theory but are extensively used
today as a model-agnostic explanation framework to explain predictions made by
complex machine learning models in the industry and academia. There are several
algorithmic approaches for computing different versions of Shapley value
explanations. Here, we focus on conditional Shapley values for predictive
models fitted to tabular data. Estimating precise conditional Shapley values is
difficult as they require the estimation of non-trivial conditional
expectations. In this article, we develop new methods, extend earlier proposed
approaches, and systematize the new refined and existing methods into different
method classes for comparison and evaluation. The method classes use either
Monte Carlo integration or regression to model the conditional expectations. We
conduct extensive simulation studies to evaluate how precisely the different
method classes estimate the conditional expectations, and thereby the
conditional Shapley values, for different setups. We also apply the methods to
several real-world data experiments and provide recommendations for when to use
the different method classes and approaches. Roughly speaking, we recommend
using parametric methods when we can specify the data distribution almost
correctly, as they generally produce the most accurate Shapley value
explanations. When the distribution is unknown, both generative methods and
regression models with a similar form as the underlying predictive model are
good and stable options. Regression-based methods are often slow to train but
produce the Shapley value explanations quickly once trained. The vice versa is
true for Monte Carlo-based methods, making the different methods appropriate in
different practical situations.","['Lars Henry Berge Olsen', 'Ingrid Kristine Glad', 'Martin Jullum', 'Kjersti Aas']","['stat.ML', 'cs.LG', 'stat.CO']",2023-05-16 15:27:17+00:00
http://arxiv.org/abs/2305.09446v2,A Probabilistic Transformation of Distance-Based Outliers,"The scores of distance-based outlier detection methods are difficult to
interpret, making it challenging to determine a cut-off threshold between
normal and outlier data points without additional context. We describe a
generic transformation of distance-based outlier scores into interpretable,
probabilistic estimates. The transformation is ranking-stable and increases the
contrast between normal and outlier data points. Determining distance
relationships between data points is necessary to identify the nearest-neighbor
relationships in the data, yet, most of the computed distances are typically
discarded. We show that the distances to other data points can be used to model
distance probability distributions and, subsequently, use the distributions to
turn distance-based outlier scores into outlier probabilities. Our experiments
show that the probabilistic transformation does not impact detection
performance over numerous tabular and image benchmark datasets but results in
interpretable outlier scores with increased contrast between normal and outlier
samples. Our work generalizes to a wide range of distance-based outlier
detection methods, and because existing distance computations are used, it adds
no significant computational overhead.","['David Muhr', 'Michael Affenzeller', 'Josef Küng']","['cs.LG', 'stat.ML', '62R07, 62R20', 'I.5.1; I.5.2']",2023-05-16 14:05:30+00:00
http://arxiv.org/abs/2305.09385v1,Lp- and Risk Consistency of Localized SVMs,"Kernel-based regularized risk minimizers, also called support vector machines
(SVMs), are known to possess many desirable properties but suffer from their
super-linear computational requirements when dealing with large data sets. This
problem can be tackled by using localized SVMs instead, which also offer the
additional advantage of being able to apply different hyperparameters to
different regions of the input space. In this paper, localized SVMs are
analyzed with regards to their consistency. It is proven that they inherit
$L_p$- as well as risk consistency from global SVMs under very weak conditions
and even if the regions underlying the localized SVMs are allowed to change as
the size of the training data set increases.",['Hannes Köhler'],"['stat.ML', 'cs.LG']",2023-05-16 12:11:08+00:00
http://arxiv.org/abs/2305.09282v2,Errors-in-variables Fréchet Regression with Low-rank Covariate Approximation,"Fr\'echet regression has emerged as a promising approach for regression
analysis involving non-Euclidean response variables. However, its practical
applicability has been hindered by its reliance on ideal scenarios with
abundant and noiseless covariate data. In this paper, we present a novel
estimation method that tackles these limitations by leveraging the low-rank
structure inherent in the covariate matrix. Our proposed framework combines the
concepts of global Fr\'echet regression and principal component regression,
aiming to improve the efficiency and accuracy of the regression estimator. By
incorporating the low-rank structure, our method enables more effective
modeling and estimation, particularly in high-dimensional and
errors-in-variables regression settings. We provide a theoretical analysis of
the proposed estimator's large-sample properties, including a comprehensive
rate analysis of bias, variance, and additional variations due to measurement
errors. Furthermore, our numerical experiments provide empirical evidence that
supports the theoretical findings, demonstrating the superior performance of
our approach. Overall, this work introduces a promising framework for
regression analysis of non-Euclidean variables, effectively addressing the
challenges associated with limited and noisy covariate data, with potential
applications in diverse fields.","['Kyunghee Han', 'Dogyoon Song']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2023-05-16 08:37:54+00:00
http://arxiv.org/abs/2305.09126v3,Transfer Learning for Causal Effect Estimation,"We present a Transfer Causal Learning (TCL) framework when target and source
domains share the same covariate/feature spaces, aiming to improve causal
effect estimation accuracy in limited data. Limited data is very common in
medical applications, where some rare medical conditions, such as sepsis, are
of interest. Our proposed method, named \texttt{$\ell_1$-TCL}, incorporates
$\ell_1$ regularized TL for nuisance models (e.g., propensity score model); the
TL estimator of the nuisance parameters is plugged into downstream average
causal/treatment effect estimators (e.g., inverse probability weighted
estimator). We establish non-asymptotic recovery guarantees for the
\texttt{$\ell_1$-TCL} with generalized linear model (GLM) under the sparsity
assumption in the high-dimensional setting, and demonstrate the empirical
benefits of \texttt{$\ell_1$-TCL} through extensive numerical simulation for
GLM and recent neural network nuisance models. Our method is subsequently
extended to real data and generates meaningful insights consistent with medical
literature, a case where all baseline methods fail.","['Song Wei', 'Hanyu Zhang', 'Ronald Moore', 'Rishikesan Kamaleswaran', 'Yao Xie']","['cs.LG', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2023-05-16 03:13:55+00:00
http://arxiv.org/abs/2305.09088v1,The Hessian perspective into the Nature of Convolutional Neural Networks,"While Convolutional Neural Networks (CNNs) have long been investigated and
applied, as well as theorized, we aim to provide a slightly different
perspective into their nature -- through the perspective of their Hessian maps.
The reason is that the loss Hessian captures the pairwise interaction of
parameters and therefore forms a natural ground to probe how the architectural
aspects of CNN get manifested in its structure and properties. We develop a
framework relying on Toeplitz representation of CNNs, and then utilize it to
reveal the Hessian structure and, in particular, its rank. We prove tight upper
bounds (with linear activations), which closely follow the empirical trend of
the Hessian rank and hold in practice in more general settings. Overall, our
work generalizes and establishes the key insight that, even in CNNs, the
Hessian rank grows as the square root of the number of parameters.","['Sidak Pal Singh', 'Thomas Hofmann', 'Bernhard Schölkopf']","['cs.LG', 'stat.ML']",2023-05-16 01:15:00+00:00
http://arxiv.org/abs/2305.09046v1,Convex optimization over a probability simplex,"We propose a new iteration scheme, the Cauchy-Simplex, to optimize convex
problems over the probability simplex $\{w\in\mathbb{R}^n\ |\ \sum_i w_i=1\
\textrm{and}\ w_i\geq0\}$. Other works have taken steps to enforce positivity
or unit normalization automatically but never simultaneously within a unified
setting. This paper presents a natural framework for manifestly requiring the
probability condition. Specifically, we map the simplex to the positive
quadrant of a unit sphere, envisage gradient descent in latent variables, and
map the result back in a way that only depends on the simplex variable.
Moreover, proving rigorous convergence results in this formulation leads
inherently to tools from information theory (e.g. cross entropy and KL
divergence). Each iteration of the Cauchy-Simplex consists of simple
operations, making it well-suited for high-dimensional problems. We prove that
it has a convergence rate of ${O}(1/T)$ for convex functions, and numerical
experiments of projection onto convex hulls show faster convergence than
similar algorithms. Finally, we apply our algorithm to online learning problems
and prove the convergence of the average regret for (1) Prediction with expert
advice and (2) Universal Portfolios.","['James Chok', 'Geoffrey M. Vasil']","['math.OC', 'cs.LG', 'cs.NA', 'math.NA', 'q-fin.PM', 'stat.ML', '65K10, 68W27, 68W40, 91G10, 97U40']",2023-05-15 22:14:22+00:00
http://arxiv.org/abs/2305.09044v1,Scalable and Robust Tensor Ring Decomposition for Large-scale Data,"Tensor ring (TR) decomposition has recently received increased attention due
to its superior expressive performance for high-order tensors. However, the
applicability of traditional TR decomposition algorithms to real-world
applications is hindered by prevalent large data sizes, missing entries, and
corruption with outliers. In this work, we propose a scalable and robust TR
decomposition algorithm capable of handling large-scale tensor data with
missing entries and gross corruptions. We first develop a novel auto-weighted
steepest descent method that can adaptively fill the missing entries and
identify the outliers during the decomposition process. Further, taking
advantage of the tensor ring model, we develop a novel fast Gram matrix
computation (FGMC) approach and a randomized subtensor sketching (RStS)
strategy which yield significant reduction in storage and computational
complexity. Experimental results demonstrate that the proposed method
outperforms existing TR decomposition methods in the presence of outliers, and
runs significantly faster than existing robust tensor completion algorithms.","['Yicong He', 'George K. Atia']","['cs.LG', 'stat.ML']",2023-05-15 22:08:47+00:00
http://arxiv.org/abs/2305.09028v2,SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels,"Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence
model with impressive results. They require O(n log n) computational complexity
and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and
decay bias calls. We aim to reduce both. We first note that the RPE is a
non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are
pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior
near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow.
For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix
decomposition. For the sparse component's action, we do a small 1D convolution.
For the low rank component, we replace the RPE MLP with linear interpolation
and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015)
for O(n) complexity: we provide rigorous error analysis. For causal models,
""fast"" causal masking (Katharopoulos et. al. 2020) negates SKI's benefits.
Working in the frequency domain, we avoid an explicit decay bias. To enforce
causality, we represent the kernel via the real part of its frequency response
using the RPE and compute the imaginary part via a Hilbert transform. This
maintains O(n log n) complexity but achieves an absolute speedup. Modeling the
frequency response directly is also competitive for bidirectional training,
using one fewer FFT. We set a speed state of the art on Long Range Arena (Tay
et. al. 2020) with minimal score degradation.","['Alexander Moreno', 'Jonathan Mei', 'Luke Walters']","['stat.ML', 'cs.LG']",2023-05-15 21:25:35+00:00
http://arxiv.org/abs/2305.08969v1,A Causal Inference Framework for Leveraging External Controls in Hybrid Trials,"We consider the challenges associated with causal inference in settings where
data from a randomized trial is augmented with control data from an external
source to improve efficiency in estimating the average treatment effect (ATE).
Through the development of a formal causal inference framework, we outline
sufficient causal assumptions about the exchangeability between the internal
and external controls to identify the ATE and establish the connection to a
novel graphical criteria. We propose estimators, review efficiency bounds,
develop an approach for efficient doubly-robust estimation even when unknown
nuisance models are estimated with flexible machine learning methods, and
demonstrate finite-sample performance through a simulation study. To illustrate
the ideas and methods, we apply the framework to a trial investigating the
effect of risdisplam on motor function in patients with spinal muscular atrophy
for which there exists an external set of control patients from a previous
trial.","['Michael Valancius', 'Herb Pang', 'Jiawen Zhu', 'Stephen R Cole', 'Michele Jonsson Funk', 'Michael R Kosorok']","['stat.ME', 'stat.ML']",2023-05-15 19:15:32+00:00
http://arxiv.org/abs/2305.08841v2,A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes,"The proximal policy optimization (PPO) algorithm stands as one of the most
prosperous methods in the field of reinforcement learning (RL). Despite its
success, the theoretical understanding of PPO remains deficient. Specifically,
it is unclear whether PPO or its optimistic variants can effectively solve
linear Markov decision processes (MDPs), which are arguably the simplest models
in RL with function approximation. To bridge this gap, we propose an optimistic
variant of PPO for episodic adversarial linear MDPs with full-information
feedback, and establish a $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret for
it. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of each
episode, and $K$ is the number of episodes. Compared with existing policy-based
algorithms, we achieve the state-of-the-art regret bound in both stochastic
linear MDPs and adversarial linear MDPs with full information. Additionally,
our algorithm design features a novel multi-batched updating mechanism and the
theoretical analysis utilizes a new covering number argument of value and
policy classes, which might be of independent interest.","['Han Zhong', 'Tong Zhang']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2023-05-15 17:55:24+00:00
http://arxiv.org/abs/2305.08791v1,Fair Information Spread on Social Networks with Community Structure,"Information spread through social networks is ubiquitous. Influence maximiza-
tion (IM) algorithms aim to identify individuals who will generate the greatest
spread through the social network if provided with information, and have been
largely devel- oped with marketing in mind. In social networks with community
structure, which are very common, IM algorithms focused solely on maximizing
spread may yield signifi- cant disparities in information coverage between
communities, which is problematic in settings such as public health messaging.
While some IM algorithms aim to remedy disparity in information coverage using
node attributes, none use the empirical com- munity structure within the
network itself, which may be beneficial since communities directly affect the
spread of information. Further, the use of empirical network struc- ture allows
us to leverage community detection techniques, making it possible to run
fair-aware algorithms when there are no relevant node attributes available, or
when node attributes do not accurately capture network community structure. In
contrast to other fair IM algorithms, this work relies on fitting a model to
the social network which is then used to determine a seed allocation strategy
for optimal fair information spread. We develop an algorithm to determine
optimal seed allocations for expected fair coverage, defined through maximum
entropy, provide some theoretical guarantees under appropriate conditions, and
demonstrate its empirical accuracy on both simu- lated and real networks.
Because this algorithm relies on a fitted network model and not on the network
directly, it is well-suited for partially observed and noisy social networks.","['Octavio Mesner', 'Elizaveta Levina', 'Ji Zhu']","['stat.ML', 'cs.LG']",2023-05-15 16:51:18+00:00
http://arxiv.org/abs/2305.08687v1,Accelerated Algorithms for Nonlinear Matrix Decomposition with the ReLU function,"In this paper, we study the following nonlinear matrix decomposition (NMD)
problem: given a sparse nonnegative matrix $X$, find a low-rank matrix $\Theta$
such that $X \approx f(\Theta)$, where $f$ is an element-wise nonlinear
function. We focus on the case where $f(\cdot) = \max(0, \cdot)$, the rectified
unit (ReLU) non-linear activation. We refer to the corresponding problem as
ReLU-NMD. We first provide a brief overview of the existing approaches that
were developed to tackle ReLU-NMD. Then we introduce two new algorithms: (1)
aggressive accelerated NMD (A-NMD) which uses an adaptive Nesterov
extrapolation to accelerate an existing algorithm, and (2) three-block NMD
(3B-NMD) which parametrizes $\Theta = WH$ and leads to a significant reduction
in the computational cost. We also propose an effective initialization strategy
based on the nuclear norm as a proxy for the rank function. We illustrate the
effectiveness of the proposed algorithms (available on gitlab) on synthetic and
real-world data sets.","['Giovanni Seraghiti', 'Atharva Awari', 'Arnaud Vandaele', 'Margherita Porcelli', 'Nicolas Gillis']","['cs.LG', 'eess.SP', 'math.OC', 'stat.ML']",2023-05-15 14:43:27+00:00
http://arxiv.org/abs/2305.08658v3,"On the connections between optimization algorithms, Lyapunov functions, and differential equations: theory and insights","We revisit the general framework introduced by Fazylab et al. (SIAM J. Optim.
28, 2018) to construct Lyapunov functions for optimization algorithms in
discrete and continuous time. For smooth, strongly convex objective functions,
we relax the requirements necessary for such a construction. As a result we are
able to prove for Polyak's ordinary differential equations and for a
two-parameter family of Nesterov algorithms rates of convergence that improve
on those available in the literature. We analyse the interpretation of Nesterov
algorithms as discretizations of the Polyak equation. We show that the
algorithms are instances of Additive Runge-Kutta integrators and discuss the
reasons why most discretizations of the differential equation do not result in
optimization algorithms with acceleration. We also introduce a modification of
Polyak's equation and study its convergence properties. Finally we extend the
general framework to the stochastic scenario and consider an application to
random algorithms with acceleration for overparameterized models; again we are
able to prove convergence rates that improve on those in the literature.","['Paul Dobson', 'Jesus Maria Sanz-Serna', 'Konstantinos Zygalakis']","['math.OC', 'cs.NA', 'math.NA', 'stat.ML', '65L06, 65L20, 90C25, 93C15']",2023-05-15 14:03:16+00:00
http://arxiv.org/abs/2305.08657v2,Meta-models for transfer learning in source localisation,"In practice, non-destructive testing (NDT) procedures tend to consider
experiments (and their respective models) as distinct, conducted in isolation
and associated with independent data. In contrast, this work looks to capture
the interdependencies between acoustic emission (AE) experiments (as
meta-models) and then use the resulting functions to predict the model
hyperparameters for previously unobserved systems. We utilise a Bayesian
multilevel approach (similar to deep Gaussian Processes) where a higher level
meta-model captures the inter-task relationships. Our key contribution is how
knowledge of the experimental campaign can be encoded between tasks as well as
within tasks. We present an example of AE time-of-arrival mapping for source
localisation, to illustrate how multilevel models naturally lend themselves to
representing aggregate systems in engineering. We constrain the meta-model
based on domain knowledge, then use the inter-task functions for transfer
learning, predicting hyperparameters for models of previously unobserved
experiments (for a specific design).","['Lawrence A. Bull', 'Matthew R. Jones', 'Elizabeth J. Cross', 'Andrew Duncan', 'Mark Girolami']","['stat.ML', 'cs.LG', 'stat.AP']",2023-05-15 14:02:35+00:00
http://arxiv.org/abs/2305.08642v2,Topological Interpretability for Deep-Learning,"With the growing adoption of AI-based systems across everyday life, the need
to understand their decision-making mechanisms is correspondingly increasing.
The level at which we can trust the statistical inferences made from AI-based
decision systems is an increasing concern, especially in high-risk systems such
as criminal justice or medical diagnosis, where incorrect inferences may have
tragic consequences. Despite their successes in providing solutions to problems
involving real-world data, deep learning (DL) models cannot quantify the
certainty of their predictions. These models are frequently quite confident,
even when their solutions are incorrect.
  This work presents a method to infer prominent features in two DL
classification models trained on clinical and non-clinical text by employing
techniques from topological and geometric data analysis. We create a graph of a
model's feature space and cluster the inputs into the graph's vertices by the
similarity of features and prediction statistics. We then extract subgraphs
demonstrating high-predictive accuracy for a given label. These subgraphs
contain a wealth of information about features that the DL model has recognized
as relevant to its decisions. We infer these features for a given label using a
distance metric between probability measures, and demonstrate the stability of
our method compared to the LIME and SHAP interpretability methods. This work
establishes that we may gain insights into the decision mechanism of a DL
model. This method allows us to ascertain if the model is making its decisions
based on information germane to the problem or identifies extraneous patterns
within the data.","['Adam Spannaus', 'Heidi A. Hanson', 'Lynne Penberthy', 'Georgia Tourassi']","['stat.ML', 'cs.LG']",2023-05-15 13:38:13+00:00
http://arxiv.org/abs/2305.08637v3,Double-Weighting for Covariate Shift Adaptation,"Supervised learning is often affected by a covariate shift in which the
marginal distributions of instances (covariates $x$) of training and testing
samples $\mathrm{p}_\text{tr}(x)$ and $\mathrm{p}_\text{te}(x)$ are different
but the label conditionals coincide. Existing approaches address such covariate
shift by either using the ratio
$\mathrm{p}_\text{te}(x)/\mathrm{p}_\text{tr}(x)$ to weight training samples
(reweighted methods) or using the ratio
$\mathrm{p}_\text{tr}(x)/\mathrm{p}_\text{te}(x)$ to weight testing samples
(robust methods). However, the performance of such approaches can be poor under
support mismatch or when the above ratios take large values. We propose a
minimax risk classification (MRC) approach for covariate shift adaptation that
avoids such limitations by weighting both training and testing samples. In
addition, we develop effective techniques that obtain both sets of weights and
generalize the conventional kernel mean matching method. We provide novel
generalization bounds for our method that show a significant increase in the
effective sample size compared with reweighted methods. The proposed method
also achieves enhanced classification performance in both synthetic and
empirical experiments.","['José I. Segovia-Martín', 'Santiago Mazuelas', 'Anqi Liu']","['stat.ML', 'cs.LG']",2023-05-15 13:31:09+00:00
http://arxiv.org/abs/2305.08529v3,Kernel-based Joint Independence Tests for Multivariate Stationary and Non-stationary Time Series,"Multivariate time series data that capture the temporal evolution of
interconnected systems are ubiquitous in diverse areas. Understanding the
complex relationships and potential dependencies among co-observed variables is
crucial for the accurate statistical modelling and analysis of such systems.
Here, we introduce kernel-based statistical tests of joint independence in
multivariate time series by extending the $d$-variable Hilbert-Schmidt
independence criterion (dHSIC) to encompass both stationary and non-stationary
processes, thus allowing broader real-world applications. By leveraging
resampling techniques tailored for both single- and multiple-realisation time
series, we show how the method robustly uncovers significant higher-order
dependencies in synthetic examples, including frequency mixing data and logic
gates, as well as real-world climate, neuroscience, and socioeconomic data. Our
method adds to the mathematical toolbox for the analysis of multivariate time
series and can aid in uncovering high-order interactions in data.","['Zhaolu Liu', 'Robert L. Peach', 'Felix Laumann', 'Sara Vallejo Mengod', 'Mauricio Barahona']","['stat.ME', 'math.ST', 'stat.AP', 'stat.ML', 'stat.TH']",2023-05-15 10:38:24+00:00
http://arxiv.org/abs/2305.08501v1,Label Smoothing is Robustification against Model Misspecification,"Label smoothing (LS) adopts smoothed targets in classification tasks. For
example, in binary classification, instead of the one-hot target $(1,0)^\top$
used in conventional logistic regression (LR), LR with LS (LSLR) uses the
smoothed target $(1-\frac{\alpha}{2},\frac{\alpha}{2})^\top$ with a smoothing
level $\alpha\in(0,1)$, which causes squeezing of values of the logit. Apart
from the common regularization-based interpretation of LS that leads to an
inconsistent probability estimator, we regard LSLR as modifying the loss
function and consistent estimator for probability estimation. In order to study
the significance of each of these two modifications by LSLR, we introduce a
modified LSLR (MLSLR) that uses the same loss function as LSLR and the same
consistent estimator as LR, while not squeezing the logits. For the loss
function modification, we theoretically show that MLSLR with a larger smoothing
level has lower efficiency with correctly-specified models, while it exhibits
higher robustness against model misspecification than LR. Also, for the
modification of the probability estimator, an experimental comparison between
LSLR and MLSLR showed that this modification and squeezing of the logits in
LSLR have negative effects on the probability estimation and classification
performance. The understanding of the properties of LS provided by these
comparisons allows us to propose MLSLR as an improvement over LSLR.","['Ryoya Yamasaki', 'Toshiyuki Tanaka']","['stat.ML', 'cs.LG']",2023-05-15 09:57:04+00:00
http://arxiv.org/abs/2305.08463v3,Convergence Analysis of Mean Shift,"The mean shift (MS) algorithm seeks a mode of the kernel density estimate
(KDE). This study presents a convergence guarantee of the mode estimate
sequence generated by the MS algorithm and an evaluation of the convergence
rate, under fairly mild conditions, with the help of the argument concerning
the {\L}ojasiewicz inequality. Our findings extend existing ones covering
analytic kernels and the Epanechnikov kernel. Those are significant in that
they cover the biweight kernel, which is optimal among non-negative kernels in
terms of the asymptotic statistical efficiency for the KDE-based mode
estimation.","['Ryoya Yamasaki', 'Toshiyuki Tanaka']","['stat.ML', 'cs.LG']",2023-05-15 09:04:55+00:00
http://arxiv.org/abs/2305.08457v1,MolHF: A Hierarchical Normalizing Flow for Molecular Graph Generation,"Molecular de novo design is a critical yet challenging task in scientific
fields, aiming to design novel molecular structures with desired property
profiles. Significant progress has been made by resorting to generative models
for graphs. However, limited attention is paid to hierarchical generative
models, which can exploit the inherent hierarchical structure (with rich
semantic information) of the molecular graphs and generate complex molecules of
larger size that we shall demonstrate to be difficult for most existing models.
The primary challenge to hierarchical generation is the non-differentiable
issue caused by the generation of intermediate discrete coarsened graph
structures. To sidestep this issue, we cast the tricky hierarchical generation
problem over discrete spaces as the reverse process of hierarchical
representation learning and propose MolHF, a new hierarchical flow-based model
that generates molecular graphs in a coarse-to-fine manner. Specifically, MolHF
first generates bonds through a multi-scale architecture, then generates atoms
based on the coarsened graph structure at each scale. We demonstrate that MolHF
achieves state-of-the-art performance in random generation and property
optimization, implying its high capacity to model data distribution.
Furthermore, MolHF is the first flow-based model that can be applied to model
larger molecules (polymer) with more than 100 heavy atoms. The code and models
are available at https://github.com/violet-sto/MolHF.","['Yiheng Zhu', 'Zhenqiu Ouyang', 'Ben Liao', 'Jialu Wu', 'Yixuan Wu', 'Chang-Yu Hsieh', 'Tingjun Hou', 'Jian Wu']","['cs.LG', 'stat.ML']",2023-05-15 08:59:35+00:00
http://arxiv.org/abs/2305.08404v2,Theoretical Analysis of Inductive Biases in Deep Convolutional Networks,"In this paper, we provide a theoretical analysis of the inductive biases in
convolutional neural networks (CNNs). We start by examining the universality of
CNNs, i.e., the ability to approximate any continuous functions. We prove that
a depth of $\mathcal{O}(\log d)$ suffices for deep CNNs to achieve this
universality, where $d$ in the input dimension. Additionally, we establish that
learning sparse functions with CNNs requires only
$\widetilde{\mathcal{O}}(\log^2d)$ samples, indicating that deep CNNs can
efficiently capture {\em long-range} sparse correlations. These results are
made possible through a novel combination of the multichanneling and
downsampling when increasing the network depth. We also delve into the distinct
roles of weight sharing and locality in CNNs. To this end, we compare the
performance of CNNs, locally-connected networks (LCNs), and fully-connected
networks (FCNs) on a simple regression task, where LCNs can be viewed as CNNs
without weight sharing. On the one hand, we prove that LCNs require
${\Omega}(d)$ samples while CNNs need only $\widetilde{\mathcal{O}}(\log^2d)$
samples, highlighting the critical role of weight sharing. On the other hand,
we prove that FCNs require $\Omega(d^2)$ samples, whereas LCNs need only
$\widetilde{\mathcal{O}}(d)$ samples, underscoring the importance of locality.
These provable separations quantify the difference between the two biases, and
the major observation behind our proof is that weight sharing and locality
break different symmetries in the learning process.","['Zihao Wang', 'Lei Wu']","['cs.LG', 'stat.ML']",2023-05-15 07:40:07+00:00
http://arxiv.org/abs/2305.08359v1,Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs,"Recent studies have shown that episodic reinforcement learning (RL) is no
harder than bandits when the total reward is bounded by $1$, and proved regret
bounds that have a polylogarithmic dependence on the planning horizon $H$.
However, it remains an open question that if such results can be carried over
to adversarial RL, where the reward is adversarially chosen at each episode. In
this paper, we answer this question affirmatively by proposing the first
horizon-free policy search algorithm. To tackle the challenges caused by
exploration and adversarially chosen reward, our algorithm employs (1) a
variance-uncertainty-aware weighted least square estimator for the transition
kernel; and (2) an occupancy measure-based technique for the online search of a
\emph{stochastic} policy. We show that our algorithm achieves an
$\tilde{O}\big((d+\log (|\mathcal{S}|^2 |\mathcal{A}|))\sqrt{K}\big)$ regret
with full-information feedback, where $d$ is the dimension of a known feature
mapping linearly parametrizing the unknown transition kernel of the MDP, $K$ is
the number of episodes, $|\mathcal{S}|$ and $|\mathcal{A}|$ are the
cardinalities of the state and action spaces. We also provide hardness results
and regret lower bounds to justify the near optimality of our algorithm and the
unavoidability of $\log|\mathcal{S}|$ and $\log|\mathcal{A}|$ in the regret
bound.","['Kaixuan Ji', 'Qingyue Zhao', 'Jiafan He', 'Weitong Zhang', 'Quanquan Gu']","['cs.LG', 'math.OC', 'stat.ML']",2023-05-15 05:37:32+00:00
