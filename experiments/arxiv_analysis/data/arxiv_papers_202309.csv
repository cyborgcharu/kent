id,title,abstract,authors,categories,date
http://arxiv.org/abs/2310.10869v1,Approximation properties of slice-matching operators,"Iterative slice-matching procedures are efficient schemes for transferring a
source measure to a target measure, especially in high dimensions. These
schemes have been successfully used in applications such as color transfer and
shape retrieval, and are guaranteed to converge under regularity assumptions.
In this paper, we explore approximation properties related to a single step of
such iterative schemes by examining an associated slice-matching operator,
depending on a source measure, a target measure, and slicing directions. In
particular, we demonstrate an invariance property with respect to the source
measure, an equivariance property with respect to the target measure, and
Lipschitz continuity concerning the slicing directions. We furthermore
establish error bounds corresponding to approximating the target measure by one
step of the slice-matching scheme and characterize situations in which the
slice-matching operator recovers the optimal transport map between two
measures. We also investigate connections to affine registration problems with
respect to (sliced) Wasserstein distances. These connections can be also be
viewed as extensions to the invariance and equivariance properties of the
slice-matching operator and illustrate the extent to which slice-matching
schemes incorporate affine effects.","['Shiying Li', 'Caroline Moosmueller']","['math.NA', 'cs.CV', 'cs.NA', 'math.OC', 'stat.ML', '49Q22, 68T10, 41A65, 65D18']",2023-10-16 22:32:43+00:00
http://arxiv.org/abs/2310.10843v1,Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow,"Density estimation, which estimates the distribution of data, is an important
category of probabilistic machine learning. A family of density estimators is
mixture models, such as Gaussian Mixture Model (GMM) by expectation
maximization. Another family of density estimators is the generative models
which generate data from input latent variables. One of the generative models
is the Masked Autoregressive Flow (MAF) which makes use of normalizing flows
and autoregressive networks. In this paper, we use the density estimators for
classification, although they are often used for estimating the distribution of
data. We model the likelihood of classes of data by density estimation,
specifically using GMM and MAF. The proposed classifiers outperform simpler
classifiers such as linear discriminant analysis which model the likelihood
using only a single Gaussian distribution. This work opens the research door
for proposing other probabilistic classifiers based on joint density
estimation.","['Benyamin Ghojogh', 'Milad Amir Toutounchian']","['stat.ML', 'cs.LG', 'cs.NE']",2023-10-16 21:37:22+00:00
http://arxiv.org/abs/2310.10807v1,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small
input perturbations that are adversarially constructed. Adversarial training is
an effective approach to defend against it. Formulated as a min-max problem, it
searches for the best solution when the training data were corrupted by the
worst-case attacks. Linear models are among the simple models where
vulnerabilities can be observed and are the focus of our study. In this case,
adversarial training leads to a convex optimization problem which can be
formulated as the minimization of a finite sum. We provide a comparative
analysis between the solution of adversarial training in linear regression and
other regularization methods. Our main findings are that: (A) Adversarial
training yields the minimum-norm interpolating solution in the
overparameterized regime (more parameters than data), as long as the maximum
disturbance radius is smaller than a threshold. And, conversely, the
minimum-norm interpolator is the solution to adversarial training with a given
radius. (B) Adversarial training can be equivalent to parameter shrinking
methods (ridge regression and Lasso). This happens in the underparametrized
region, for an appropriate choice of adversarial radius and zero-mean
symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial
training -- as in square-root Lasso -- the choice of adversarial radius for
optimal bounds does not depend on the additive noise variance. We confirm our
theoretical findings with numerical examples.","['Antônio H. Ribeiro', 'Dave Zachariah', 'Francis Bach', 'Thomas B. Schön']","['stat.ML', 'cs.CR', 'cs.LG', 'math.OC']",2023-10-16 20:09:58+00:00
http://arxiv.org/abs/2310.10791v1,Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs,"Neural tangent kernels (NTKs) provide a theoretical regime to analyze the
learning and generalization behavior of over-parametrized neural networks. For
a supervised learning task, the association between the eigenvectors of the NTK
kernel and given data (a concept referred to as alignment in this paper) can
govern the rate of convergence of gradient descent, as well as generalization
to unseen data. Building upon this concept, we investigate NTKs and alignment
in the context of graph neural networks (GNNs), where our analysis reveals that
optimizing alignment translates to optimizing the graph representation or the
graph shift operator in a GNN. Our results further establish the theoretical
guarantees on the optimality of the alignment for a two-layer GNN and these
guarantees are characterized by the graph shift operator being a function of
the cross-covariance between the input and the output data. The theoretical
insights drawn from the analysis of NTKs are validated by our experiments
focused on a multi-variate time series prediction task for a publicly available
dataset. Specifically, they demonstrate that GNNs with cross-covariance as the
graph shift operator indeed outperform those that operate on the covariance
matrix from only the input data.","['Shervin Khalafi', 'Saurabh Sihag', 'Alejandro Ribeiro']","['cs.LG', 'stat.ML']",2023-10-16 19:54:21+00:00
http://arxiv.org/abs/2310.10767v1,Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models,"Neural networks with wide layers have attracted significant attention due to
their equivalence to Gaussian processes, enabling perfect fitting of training
data while maintaining generalization performance, known as benign overfitting.
However, existing results mainly focus on shallow or finite-depth networks,
necessitating a comprehensive analysis of wide neural networks with
infinite-depth layers, such as neural ordinary differential equations (ODEs)
and deep equilibrium models (DEQs). In this paper, we specifically investigate
the deep equilibrium model (DEQ), an infinite-depth neural network with shared
weight matrices across layers. Our analysis reveals that as the width of DEQ
layers approaches infinity, it converges to a Gaussian process, establishing
what is known as the Neural Network and Gaussian Process (NNGP) correspondence.
Remarkably, this convergence holds even when the limits of depth and width are
interchanged, which is not observed in typical infinite-depth Multilayer
Perceptron (MLP) networks. Furthermore, we demonstrate that the associated
Gaussian vector remains non-degenerate for any pairwise distinct input data,
ensuring a strictly positive smallest eigenvalue of the corresponding kernel
matrix using the NNGP kernel. These findings serve as fundamental elements for
studying the training and generalization of DEQs, laying the groundwork for
future research in this area.","['Tianxiang Gao', 'Xiaokai Huo', 'Hailiang Liu', 'Hongyang Gao']","['cs.LG', 'stat.ML']",2023-10-16 19:00:43+00:00
http://arxiv.org/abs/2310.10745v2,Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder,"The Koopman operator presents an attractive approach to achieve global
linearization of nonlinear systems, making it a valuable method for simplifying
the understanding of complex dynamics. While data-driven methodologies have
exhibited promise in approximating finite Koopman operators, they grapple with
various challenges, such as the judicious selection of observables,
dimensionality reduction, and the ability to predict complex system behaviors
accurately. This study presents a novel approach termed Mori-Zwanzig
autoencoder (MZ-AE) to robustly approximate the Koopman operator in
low-dimensional spaces. The proposed method leverages a nonlinear autoencoder
to extract key observables for approximating a finite invariant Koopman
subspace and integrates a non-Markovian correction mechanism using the
Mori-Zwanzig formalism. Consequently, this approach yields a closed
representation of dynamics within the latent manifold of the nonlinear
autoencoder, thereby enhancing the precision and stability of the Koopman
operator approximation. Demonstrations showcase the technique's ability to
capture regime transitions in the flow around a cylinder. It also provides a
low dimensional approximation for Kuramoto-Sivashinsky with promising
short-term predictability and robust long-term statistical performance. By
bridging the gap between data-driven techniques and the mathematical
foundations of Koopman theory, MZ-AE offers a promising avenue for improved
understanding and prediction of complex nonlinear dynamics.","['Priyam Gupta', 'Peter J. Schmid', 'Denis Sipp', 'Taraneh Sayadi', 'Georgios Rigas']","['cs.LG', 'math.DS', 'physics.flu-dyn', 'stat.ML']",2023-10-16 18:22:02+00:00
http://arxiv.org/abs/2310.10649v3,A Computational Framework for Solving Wasserstein Lagrangian Flows,"The dynamical formulation of the optimal transport can be extended through
various choices of the underlying geometry (kinetic energy), and the
regularization of density paths (potential energy). These combinations yield
different variational problems (Lagrangians), encompassing many variations of
the optimal transport problem such as the Schr\""odinger bridge, unbalanced
optimal transport, and optimal transport with physical constraints, among
others. In general, the optimal density path is unknown, and solving these
variational problems can be computationally challenging. We propose a novel
deep learning based framework approaching all of these problems from a unified
perspective. Leveraging the dual formulation of the Lagrangians, our method
does not require simulating or backpropagating through the trajectories of the
learned dynamics, and does not need access to optimal couplings. We showcase
the versatility of the proposed framework by outperforming previous approaches
for the single-cell trajectory inference, where incorporating prior knowledge
into the dynamics is crucial for correct predictions.","['Kirill Neklyudov', 'Rob Brekelmans', 'Alexander Tong', 'Lazar Atanackovic', 'Qiang Liu', 'Alireza Makhzani']","['cs.LG', 'math.OC', 'stat.ML']",2023-10-16 17:59:54+00:00
http://arxiv.org/abs/2310.10611v2,IW-GAE: Importance Weighted Group Accuracy Estimation for Improved Calibration and Model Selection in Unsupervised Domain Adaptation,"Distribution shifts pose significant challenges for model calibration and
model selection tasks in the unsupervised domain adaptation problem -- a
scenario where the goal is to perform well in a distribution shifted domain
without labels. In this work, we tackle difficulties coming from distribution
shifts by developing a novel importance weighted group accuracy estimator.
Specifically, we present a new perspective of addressing the model calibration
and model selection tasks by estimating the group accuracy. Then, we formulate
an optimization problem for finding an importance weight that leads to an
accurate group accuracy estimation with theoretical analyses. Our extensive
experiments show that our approach improves state-of-the-art performances by
22% in the model calibration task and 14% in the model selection task.","['Taejong Joo', 'Diego Klabjan']","['cs.LG', 'stat.ML']",2023-10-16 17:35:29+00:00
http://arxiv.org/abs/2310.10603v1,Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems,"Recently, machine learning, particularly message-passing graph neural
networks (MPNNs), has gained traction in enhancing exact optimization
algorithms. For example, MPNNs speed up solving mixed-integer optimization
problems by imitating computational intensive heuristics like strong branching,
which entails solving multiple linear optimization problems (LPs). Despite the
empirical success, the reasons behind MPNNs' effectiveness in emulating linear
optimization remain largely unclear. Here, we show that MPNNs can simulate
standard interior-point methods for LPs, explaining their practical success.
Furthermore, we highlight how MPNNs can serve as a lightweight proxy for
solving LPs, adapting to a given problem instance distribution. Empirically, we
show that MPNNs solve LP relaxations of standard combinatorial optimization
problems close to optimality, often surpassing conventional solvers and
competing approaches in solving time.","['Chendi Qian', 'Didier Chételat', 'Christopher Morris']","['cs.LG', 'cs.AI', 'cs.NE', 'math.OC', 'stat.ML']",2023-10-16 17:31:25+00:00
http://arxiv.org/abs/2310.10559v1,Causal Dynamic Variational Autoencoder for Counterfactual Regression in Longitudinal Data,"Estimating treatment effects over time is relevant in many real-world
applications, such as precision medicine, epidemiology, economy, and marketing.
Many state-of-the-art methods either assume the observations of all confounders
or seek to infer the unobserved ones. We take a different perspective by
assuming unobserved risk factors, i.e., adjustment variables that affect only
the sequence of outcomes. Under unconfoundedness, we target the Individual
Treatment Effect (ITE) estimation with unobserved heterogeneity in the
treatment response due to missing risk factors. We address the challenges posed
by time-varying effects and unobserved adjustment variables. Led by theoretical
results over the validity of the learned adjustment variables and
generalization bounds over the treatment effect, we devise Causal DVAE (CDVAE).
This model combines a Dynamic Variational Autoencoder (DVAE) framework with a
weighting strategy using propensity scores to estimate counterfactual
responses. The CDVAE model allows for accurate estimation of ITE and captures
the underlying heterogeneity in longitudinal data. Evaluations of our model
show superior performance over state-of-the-art models.","['Mouad El Bouchattaoui', 'Myriam Tami', 'Benoit Lepetit', 'Paul-Henry Cournède']","['stat.ML', 'cs.LG']",2023-10-16 16:32:35+00:00
http://arxiv.org/abs/2310.10556v2,Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks,"A recently popular approach to solving reinforcement learning is with data
from human preferences. In fact, human preference data are now used with
classic reinforcement learning algorithms such as actor-critic methods, which
involve evaluating an intermediate policy over a reward learned from human
preference data with distribution shift, known as off-policy evaluation (OPE).
Such algorithm includes (i) learning reward function from human preference
dataset, and (ii) learning expected cumulative reward of a target policy.
Despite the huge empirical success, existing OPE methods with preference data
often lack theoretical understanding and rely heavily on heuristics. In this
paper, we study the sample efficiency of OPE with human preference and
establish a statistical guarantee for it. Specifically, we approach OPE by
learning the value function by fitted-Q-evaluation with a deep neural network.
By appropriately selecting the size of a ReLU network, we show that one can
leverage any low-dimensional manifold structure in the Markov decision process
and obtain a sample-efficient estimator without suffering from the curse of
high data ambient dimensionality. Under the assumption of high reward
smoothness, our results \textit{almost align with the classical OPE results
with observable reward data}. To the best of our knowledge, this is the first
result that establishes a \textit{provably efficient} guarantee for off-policy
evaluation with RLHF.","['Zihao Li', 'Xiang Ji', 'Minshuo Chen', 'Mengdi Wang']","['cs.LG', 'stat.ML']",2023-10-16 16:27:06+00:00
http://arxiv.org/abs/2310.10553v2,TacticAI: an AI assistant for football tactics,"Identifying key patterns of tactics implemented by rival teams, and
developing effective responses, lies at the heart of modern football. However,
doing so algorithmically remains an open research challenge. To address this
unmet need, we propose TacticAI, an AI football tactics assistant developed and
evaluated in close collaboration with domain experts from Liverpool FC. We
focus on analysing corner kicks, as they offer coaches the most direct
opportunities for interventions and improvements. TacticAI incorporates both a
predictive and a generative component, allowing the coaches to effectively
sample and explore alternative player setups for each corner kick routine and
to select those with the highest predicted likelihood of success. We validate
TacticAI on a number of relevant benchmark tasks: predicting receivers and shot
attempts and recommending player position adjustments. The utility of TacticAI
is validated by a qualitative study conducted with football domain experts at
Liverpool FC. We show that TacticAI's model suggestions are not only
indistinguishable from real tactics, but also favoured over existing tactics
90% of the time, and that TacticAI offers an effective corner kick retrieval
system. TacticAI achieves these results despite the limited availability of
gold-standard data, achieving data efficiency through geometric deep learning.","['Zhe Wang', 'Petar Veličković', 'Daniel Hennes', 'Nenad Tomašev', 'Laurel Prince', 'Michael Kaisers', 'Yoram Bachrach', 'Romuald Elie', 'Li Kevin Wenliang', 'Federico Piccinini', 'William Spearman', 'Ian Graham', 'Jerome Connor', 'Yi Yang', 'Adrià Recasens', 'Mina Khan', 'Nathalie Beauguerlange', 'Pablo Sprechmann', 'Pol Moreno', 'Nicolas Heess', 'Michael Bowling', 'Demis Hassabis', 'Karl Tuyls']","['cs.LG', 'cs.MA', 'stat.ML']",2023-10-16 16:25:15+00:00
http://arxiv.org/abs/2310.10545v2,Optimal vintage factor analysis with deflation varimax,"Vintage factor analysis is one important type of factor analysis that aims to
first find a low-dimensional representation of the original data, and then to
seek a rotation such that the rotated low-dimensional representation is
scientifically meaningful. The most widely used vintage factor analysis is the
Principal Component Analysis (PCA) followed by the varimax rotation. Despite
its popularity, little theoretical guarantee can be provided to date mainly
because varimax rotation requires to solve a non-convex optimization over the
set of orthogonal matrices.
  In this paper, we propose a deflation varimax procedure that solves each row
of an orthogonal matrix sequentially. In addition to its net computational gain
and flexibility, we are able to fully establish theoretical guarantees for the
proposed procedure in a broader context. Adopting this new deflation varimax as
the second step after PCA, we further analyze this two step procedure under a
general class of factor models. Our results show that it estimates the factor
loading matrix in the minimax optimal rate when the signal-to-noise-ratio (SNR)
is moderate or large. In the low SNR regime, we offer possible improvement over
using PCA and the deflation varimax when the additive noise under the factor
model is structured. The modified procedure is shown to be minimax optimal in
all SNR regimes. Our theory is valid for finite sample and allows the number of
the latent factors to grow with the sample size as well as the ambient
dimension to grow with, or even exceed, the sample size. Extensive simulation
and real data analysis further corroborate our theoretical findings.","['Xin Bing', 'Dian Jin', 'Yuqian Zhang']","['stat.ML', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT']",2023-10-16 16:14:43+00:00
http://arxiv.org/abs/2310.10534v2,Comparing Comparators in Generalization Bounds,"We derive generic information-theoretic and PAC-Bayesian generalization
bounds involving an arbitrary convex comparator function, which measures the
discrepancy between the training and population loss. The bounds hold under the
assumption that the cumulant-generating function (CGF) of the comparator is
upper-bounded by the corresponding CGF within a family of bounding
distributions. We show that the tightest possible bound is obtained with the
comparator being the convex conjugate of the CGF of the bounding distribution,
also known as the Cram\'er function. This conclusion applies more broadly to
generalization bounds with a similar structure. This confirms the
near-optimality of known bounds for bounded and sub-Gaussian losses and leads
to novel bounds under other bounding distributions.","['Fredrik Hellström', 'Benjamin Guedj']","['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2023-10-16 16:00:58+00:00
http://arxiv.org/abs/2310.10448v1,A Geometric Insight into Equivariant Message Passing Neural Networks on Riemannian Manifolds,"This work proposes a geometric insight into equivariant message passing on
Riemannian manifolds. As previously proposed, numerical features on Riemannian
manifolds are represented as coordinate-independent feature fields on the
manifold. To any coordinate-independent feature field on a manifold comes
attached an equivariant embedding of the principal bundle to the space of
numerical features. We argue that the metric this embedding induces on the
numerical feature space should optimally preserve the principal bundle's
original metric. This optimality criterion leads to the minimization of a
twisted form of the Polyakov action with respect to the graph of this
embedding, yielding an equivariant diffusion process on the associated vector
bundle. We obtain a message passing scheme on the manifold by discretizing the
diffusion equation flow for a fixed time step. We propose a higher-order
equivariant diffusion process equivalent to diffusion on the cartesian product
of the base manifold. The discretization of the higher-order diffusion process
on a graph yields a new general class of equivariant GNN, generalizing the ACE
and MACE formalism to data on Riemannian manifolds.",['Ilyes Batatia'],"['stat.ML', 'cs.LG']",2023-10-16 14:31:13+00:00
http://arxiv.org/abs/2310.10441v1,Efficiently matching random inhomogeneous graphs via degree profiles,"In this paper, we study the problem of recovering the latent vertex
correspondence between two correlated random graphs with vastly inhomogeneous
and unknown edge probabilities between different pairs of vertices. Inspired by
and extending the matching algorithm via degree profiles by Ding, Ma, Wu and Xu
(2021), we obtain an efficient matching algorithm as long as the minimal
average degree is at least $\Omega(\log^{2} n)$ and the minimal correlation is
at least $1 - O(\log^{-2} n)$.","['Jian Ding', 'Yumou Fei', 'Yuanzheng Wang']","['cs.DS', 'math.PR', 'math.ST', 'stat.ML', 'stat.TH']",2023-10-16 14:25:43+00:00
http://arxiv.org/abs/2310.10434v2,Equivariant Matrix Function Neural Networks,"Graph Neural Networks (GNNs), especially message-passing neural networks
(MPNNs), have emerged as powerful architectures for learning on graphs in
diverse applications. However, MPNNs face challenges when modeling non-local
interactions in graphs such as large conjugated molecules, and social networks
due to oversmoothing and oversquashing. Although Spectral GNNs and traditional
neural networks such as recurrent neural networks and transformers mitigate
these challenges, they often lack generalizability, or fail to capture detailed
structural relationships or symmetries in the data. To address these concerns,
we introduce Matrix Function Neural Networks (MFNs), a novel architecture that
parameterizes non-local interactions through analytic matrix equivariant
functions. Employing resolvent expansions offers a straightforward
implementation and the potential for linear scaling with system size. The MFN
architecture achieves stateof-the-art performance in standard graph benchmarks,
such as the ZINC and TU datasets, and is able to capture intricate non-local
interactions in quantum systems, paving the way to new state-of-the-art force
fields.","['Ilyes Batatia', 'Lars L. Schaaf', 'Huajie Chen', 'Gábor Csányi', 'Christoph Ortner', 'Felix A. Faber']","['stat.ML', 'cond-mat.mtrl-sci', 'cs.LG', 'physics.chem-ph']",2023-10-16 14:17:00+00:00
http://arxiv.org/abs/2310.10399v1,Towards Fair and Calibrated Models,"Recent literature has seen a significant focus on building machine learning
models with specific properties such as fairness, i.e., being non-biased with
respect to a given set of attributes, calibration i.e., model confidence being
aligned with its predictive accuracy, and explainability, i.e., ability to be
understandable to humans. While there has been work focusing on each of these
aspects individually, researchers have shied away from simultaneously
addressing more than one of these dimensions. In this work, we address the
problem of building models which are both fair and calibrated. We work with a
specific definition of fairness, which closely matches [Biswas et. al. 2019],
and has the nice property that Bayes optimal classifier has the maximum
possible fairness under our definition. We show that an existing negative
result towards achieving a fair and calibrated model [Kleinberg et. al. 2017]
does not hold for our definition of fairness. Further, we show that ensuring
group-wise calibration with respect to the sensitive attributes automatically
results in a fair model under our definition. Using this result, we provide a
first cut approach for achieving fair and calibrated models, via a simple
post-processing technique based on temperature scaling. We then propose
modifications of existing calibration losses to perform group-wise calibration,
as a way of achieving fair and calibrated models in a variety of settings.
Finally, we perform extensive experimentation of these techniques on a diverse
benchmark of datasets, and present insights on the pareto-optimality of the
resulting solutions.","['Anand Brahmbhatt', 'Vipul Rathore', 'Mausam', 'Parag Singla']","['cs.LG', 'stat.ML']",2023-10-16 13:41:09+00:00
http://arxiv.org/abs/2310.10379v2,Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification,"Meta-learning has demonstrated promising results in few-shot classification
(FSC) by learning to solve new problems using prior knowledge. Bayesian methods
are effective at characterizing uncertainty in FSC, which is crucial in
high-risk fields. In this context, the logistic-softmax likelihood is often
employed as an alternative to the softmax likelihood in multi-class Gaussian
process classification due to its conditional conjugacy property. However, the
theoretical property of logistic-softmax is not clear and previous research
indicated that the inherent uncertainty of logistic-softmax leads to suboptimal
performance. To mitigate these issues, we revisit and redesign the
logistic-softmax likelihood, which enables control of the \textit{a priori}
confidence level through a temperature parameter. Furthermore, we theoretically
and empirically show that softmax can be viewed as a special case of
logistic-softmax and logistic-softmax induces a larger family of data
distribution than softmax. Utilizing modified logistic-softmax, we integrate
the data augmentation technique into the deep kernel based Gaussian process
meta-learning framework, and derive an analytical mean-field approximation for
task-specific updates. Our approach yields well-calibrated uncertainty
estimates and achieves comparable or superior results on standard benchmark
datasets. Code is publicly available at
\url{https://github.com/keanson/revisit-logistic-softmax}.","['Tianjun Ke', 'Haoqun Cao', 'Zenan Ling', 'Feng Zhou']","['cs.LG', 'stat.ML']",2023-10-16 13:20:13+00:00
http://arxiv.org/abs/2310.10375v3,GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers,"As transformers are equivariant to the permutation of input tokens, encoding
the positional information of tokens is necessary for many tasks. However,
since existing positional encoding schemes have been initially designed for NLP
tasks, their suitability for vision tasks, which typically exhibit different
structural properties in their data, is questionable. We argue that existing
positional encoding schemes are suboptimal for 3D vision tasks, as they do not
respect their underlying 3D geometric structure. Based on this hypothesis, we
propose a geometry-aware attention mechanism that encodes the geometric
structure of tokens as relative transformation determined by the geometric
relationship between queries and key-value pairs. By evaluating on multiple
novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view
setting, we show that our attention, called Geometric Transform Attention
(GTA), improves learning efficiency and performance of state-of-the-art
transformer-based NVS models without any additional learned parameters and only
minor computational overhead.","['Takeru Miyato', 'Bernhard Jaeger', 'Max Welling', 'Andreas Geiger']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2023-10-16 13:16:09+00:00
http://arxiv.org/abs/2310.10359v1,An Anytime Algorithm for Good Arm Identification,"In good arm identification (GAI), the goal is to identify one arm whose
average performance exceeds a given threshold, referred to as good arm, if it
exists. Few works have studied GAI in the fixed-budget setting, when the
sampling budget is fixed beforehand, or the anytime setting, when a
recommendation can be asked at any time. We propose APGAI, an anytime and
parameter-free sampling rule for GAI in stochastic bandits. APGAI can be
straightforwardly used in fixed-confidence and fixed-budget settings. First, we
derive upper bounds on its probability of error at any time. They show that
adaptive strategies are more efficient in detecting the absence of good arms
than uniform sampling. Second, when APGAI is combined with a stopping rule, we
prove upper bounds on the expected sampling complexity, holding at any
confidence level. Finally, we show good empirical performance of APGAI on
synthetic and real-world data. Our work offers an extensive overview of the GAI
problem in all settings.","['Marc Jourdan', 'Clémence Réda']","['stat.ML', 'cs.LG']",2023-10-16 12:51:26+00:00
http://arxiv.org/abs/2310.10324v1,Assessing univariate and bivariate risks of late-frost and drought using vine copulas: A historical study for Bavaria,"In light of climate change's impacts on forests, including extreme drought
and late-frost, leading to vitality decline and regional forest die-back, we
assess univariate drought and late-frost risks and perform a joint risk
analysis in Bavaria, Germany, from 1952 to 2020. Utilizing a vast dataset with
26 bioclimatic and topographic variables, we employ vine copula models due to
the data's non-Gaussian and asymmetric dependencies. We use D-vine regression
for univariate and Y-vine regression for bivariate analysis, and propose
corresponding univariate and bivariate conditional probability risk measures.
We identify ""at-risk"" regions, emphasizing the need for forest adaptation due
to climate change.","['Marija Tepegjozova', 'Benjamin F. Meyer', 'Anja Rammig', 'Christian S. Zang', 'Claudia Czado']","['stat.AP', 'stat.ME', 'stat.ML', '62H05, 62P12, 91G70', 'G.3']",2023-10-16 12:08:14+00:00
http://arxiv.org/abs/2310.10245v1,Mask wearing object detection algorithm based on improved YOLOv5,"Wearing a mask is one of the important measures to prevent infectious
diseases. However, it is difficult to detect people's mask-wearing situation in
public places with high traffic flow. To address the above problem, this paper
proposes a mask-wearing face detection model based on YOLOv5l. Firstly,
Multi-Head Attentional Self-Convolution not only improves the convergence speed
of the model but also enhances the accuracy of the model detection. Secondly,
the introduction of Swin Transformer Block is able to extract more useful
feature information, enhance the detection ability of small targets, and
improve the overall accuracy of the model. Our designed I-CBAM module can
improve target detection accuracy. In addition, using enhanced feature fusion
enables the model to better adapt to object detection tasks of different
scales. In the experimentation on the MASK dataset, the results show that the
model proposed in this paper achieved a 1.1% improvement in mAP(0.5) and a 1.3%
improvement in mAP(0.5:0.95) compared to the YOLOv5l model. Our proposed method
significantly enhances the detection capability of mask-wearing.","['Peng Wen', 'Junhu Zhang', 'Haitao Li']","['cs.CV', 'stat.ML']",2023-10-16 10:06:42+00:00
http://arxiv.org/abs/2310.10240v2,On the Properties and Estimation of Pointwise Mutual Information Profiles,"The pointwise mutual information profile, or simply profile, is the
distribution of pointwise mutual information for a given pair of random
variables. One of its important properties is that its expected value is
precisely the mutual information between these random variables. In this paper,
we analytically describe the profiles of multivariate normal distributions and
introduce a novel family of distributions, Bend and Mix Models, for which the
profile can be accurately estimated using Monte Carlo methods. We then show how
Bend and Mix Models can be used to study the limitations of existing mutual
information estimators, investigate the behavior of neural critics used in
variational estimators, and understand the effect of experimental outliers on
mutual information estimation. Finally, we show how Bend and Mix Models can be
used to obtain model-based Bayesian estimates of mutual information, suitable
for problems with available domain expertise in which uncertainty
quantification is necessary.","['Paweł Czyż', 'Frederic Grabowski', 'Julia E. Vogt', 'Niko Beerenwinkel', 'Alexander Marx']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2023-10-16 10:02:24+00:00
http://arxiv.org/abs/2310.10239v1,Structural transfer learning of non-Gaussian DAG,"Directed acyclic graph (DAG) has been widely employed to represent
directional relationships among a set of collected nodes. Yet, the available
data in one single study is often limited for accurate DAG reconstruction,
whereas heterogeneous data may be collected from multiple relevant studies. It
remains an open question how to pool the heterogeneous data together for better
DAG structure reconstruction in the target study. In this paper, we first
introduce a novel set of structural similarity measures for DAG and then
present a transfer DAG learning framework by effectively leveraging information
from auxiliary DAGs of different levels of similarities. Our theoretical
analysis shows substantial improvement in terms of DAG reconstruction in the
target study, even when no auxiliary DAG is overall similar to the target DAG,
which is in sharp contrast to most existing transfer learning methods. The
advantage of the proposed transfer DAG learning is also supported by extensive
numerical experiments on both synthetic data and multi-site brain functional
connectivity network data.","['Mingyang Ren', 'Xin He', 'Junhui Wang']","['stat.ML', 'cs.LG', 'stat.ME']",2023-10-16 10:01:27+00:00
http://arxiv.org/abs/2310.10171v1,On permutation symmetries in Bayesian neural network posteriors: a variational perspective,"The elusive nature of gradient-based optimization in neural networks is tied
to their loss landscape geometry, which is poorly understood. However recent
work has brought solid evidence that there is essentially no loss barrier
between the local solutions of gradient descent, once accounting for
weight-permutations that leave the network's computation unchanged. This raises
questions for approximate inference in Bayesian neural networks (BNNs), where
we are interested in marginalizing over multiple points in the loss landscape.
In this work, we first extend the formalism of marginalized loss barrier and
solution interpolation to BNNs, before proposing a matching algorithm to search
for linearly connected solutions. This is achieved by aligning the
distributions of two independent approximate Bayesian solutions with respect to
permutation matrices. We build on the results of Ainsworth et al. (2023),
reframing the problem as a combinatorial optimization one, using an
approximation to the sum of bilinear assignment problem. We then experiment on
a variety of architectures and datasets, finding nearly zero marginalized loss
barriers for linearly connected solutions.","['Simone Rossi', 'Ankit Singh', 'Thomas Hannagan']","['stat.ML', 'cs.LG']",2023-10-16 08:26:50+00:00
http://arxiv.org/abs/2310.10143v2,An Empirical Study of Self-supervised Learning with Wasserstein Distance,"In this study, we delve into the problem of self-supervised learning (SSL)
utilizing the 1-Wasserstein distance on a tree structure (a.k.a.,
Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance
between two tree-embedded vectors. In SSL methods, the cosine similarity is
often utilized as an objective function; however, it has not been well studied
when utilizing the Wasserstein distance. Training the Wasserstein distance is
numerically challenging. Thus, this study empirically investigates a strategy
for optimizing the SSL with the Wasserstein distance and finds a stable
training procedure. More specifically, we evaluate the combination of two types
of TWD (total variation and ClusterTree) and several probability models,
including the softmax function, the ArcFace probability model, and simplicial
embedding. We propose a simple yet effective Jeffrey divergence-based
regularization method to stabilize optimization. Through empirical experiments
on STL10, CIFAR10, CIFAR100, and SVHN, we find that a simple combination of the
softmax function and TWD can obtain significantly lower results than the
standard SimCLR. Moreover, a simple combination of TWD and SimSiam fails to
train the model. We find that the model performance depends on the combination
of TWD and probability model, and that the Jeffrey divergence regularization
helps in model training. Finally, we show that the appropriate combination of
the TWD and probability model outperforms cosine similarity-based
representation learning.","['Makoto Yamada', 'Yuki Takezawa', 'Guillaume Houry', 'Kira Michaela Dusterwald', 'Deborah Sulem', 'Han Zhao', 'Yao-Hung Hubert Tsai']","['stat.ML', 'cs.LG']",2023-10-16 07:31:30+00:00
http://arxiv.org/abs/2310.10133v1,"Empowering SMPC: Bridging the Gap Between Scalability, Memory Efficiency and Privacy in Neural Network Inference","This paper aims to develop an efficient open-source Secure Multi-Party
Computation (SMPC) repository, that addresses the issue of practical and
scalable implementation of SMPC protocol on machines with moderate
computational resources, while aiming to reduce the execution time. We
implement the ABY2.0 protocol for SMPC, providing developers with effective
tools for building applications on the ABY 2.0 protocol. This article addresses
the limitations of the C++ based MOTION2NX framework for secure neural network
inference, including memory constraints and operation compatibility issues. Our
enhancements include optimizing the memory usage, reducing execution time using
a third-party Helper node, and enhancing efficiency while still preserving data
privacy. These optimizations enable MNIST dataset inference in just 32 seconds
with only 0.2 GB of RAM for a 5-layer neural network. In contrast, the previous
baseline implementation required 8.03 GB of RAM and 200 seconds of execution
time.","['Ramya Burra', 'Anshoo Tandon', 'Srishti Mittal']","['cs.CR', 'stat.ML']",2023-10-16 07:16:09+00:00
http://arxiv.org/abs/2310.10121v2,From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond,"Graph neural networks (GNNs) have demonstrated significant promise in
modelling relational data and have been widely applied in various fields of
interest. The key mechanism behind GNNs is the so-called message passing where
information is being iteratively aggregated to central nodes from their
neighbourhood. Such a scheme has been found to be intrinsically linked to a
physical process known as heat diffusion, where the propagation of GNNs
naturally corresponds to the evolution of heat density. Analogizing the process
of message passing to the heat dynamics allows to fundamentally understand the
power and pitfalls of GNNs and consequently informs better model design.
Recently, there emerges a plethora of works that proposes GNNs inspired from
the continuous dynamics formulation, in an attempt to mitigate the known
limitations of GNNs, such as oversmoothing and oversquashing. In this survey,
we provide the first systematic and comprehensive review of studies that
leverage the continuous perspective of GNNs. To this end, we introduce
foundational ingredients for adapting continuous dynamics to GNNs, along with a
general framework for the design of graph neural dynamics. We then review and
categorize existing works based on their driven mechanisms and underlying
dynamics. We also summarize how the limitations of classic GNNs can be
addressed under the continuous framework. We conclude by identifying multiple
open research directions.","['Andi Han', 'Dai Shi', 'Lequan Lin', 'Junbin Gao']","['cs.LG', 'cs.AI', 'stat.ML']",2023-10-16 06:57:24+00:00
http://arxiv.org/abs/2310.10107v4,Posterior Sampling-based Online Learning for Episodic POMDPs,"Learning in POMDPs is known to be significantly harder than in MDPs. In this
paper, we consider the online learning problem for episodic POMDPs with unknown
transition and observation models. We propose a Posterior Sampling-based
reinforcement learning algorithm for POMDPs (PS4POMDPs), which is much simpler
and more implementable compared to state-of-the-art optimism-based online
learning algorithms for POMDPs. We show that the Bayesian regret of the
proposed algorithm scales as the square root of the number of episodes and is
polynomial in the other parameters. In a general setting, the regret scales
exponentially in the horizon length $H$, and we show that this is inevitable by
providing a lower bound. However, when the POMDP is undercomplete and weakly
revealing (a common assumption in the recent literature), we establish a
polynomial Bayesian regret bound. We finally propose a posterior sampling
algorithm for multi-agent POMDPs, and show it too has sublinear regret.","['Dengwang Tang', 'Dongze Ye', 'Rahul Jain', 'Ashutosh Nayyar', 'Pierluigi Nuzzo']","['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'stat.ML', '93E35']",2023-10-16 06:41:13+00:00
http://arxiv.org/abs/2310.10098v1,PAC Learning Linear Thresholds from Label Proportions,"Learning from label proportions (LLP) is a generalization of supervised
learning in which the training data is available as sets or bags of
feature-vectors (instances) along with the average instance-label of each bag.
The goal is to train a good instance classifier. While most previous works on
LLP have focused on training models on such training data, computational
learnability of LLP was only recently explored by [Saket'21, Saket'22] who
showed worst case intractability of properly learning linear threshold
functions (LTFs) from label proportions. However, their work did not rule out
efficient algorithms for this problem on natural distributions.
  In this work we show that it is indeed possible to efficiently learn LTFs
using LTFs when given access to random bags of some label proportion in which
feature-vectors are, conditioned on their labels, independently sampled from a
Gaussian distribution $N(\mathbf{\mu}, \mathbf{\Sigma})$. Our work shows that a
certain matrix -- formed using covariances of the differences of
feature-vectors sampled from the bags with and without replacement --
necessarily has its principal component, after a transformation, in the
direction of the normal vector of the LTF. Our algorithm estimates the means
and covariance matrices using subgaussian concentration bounds which we show
can be applied to efficiently sample bags for approximating the normal
direction. Using this in conjunction with novel generalization error bounds in
the bag setting, we show that a low error hypothesis LTF can be identified. For
some special cases of the $N(\mathbf{0}, \mathbf{I})$ distribution we provide a
simpler mean estimation based algorithm. We include an experimental evaluation
of our learning algorithms along with a comparison with those of [Saket'21,
Saket'22] and random LTFs, demonstrating the effectiveness of our techniques.","['Anand Brahmbhatt', 'Rishi Saket', 'Aravindan Raghuveer']","['cs.LG', 'stat.ML']",2023-10-16 05:59:34+00:00
http://arxiv.org/abs/2310.10096v2,LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions,"In the task of Learning from Label Proportions (LLP), a model is trained on
groups (a.k.a bags) of instances and their corresponding label proportions to
predict labels for individual instances. LLP has been applied pre-dominantly on
two types of datasets - image and tabular. In image LLP, bags of fixed size are
created by randomly sampling instances from an underlying dataset. Bags created
via this methodology are called random bags. Experimentation on Image LLP has
been mostly on random bags on CIFAR-* and MNIST datasets. Despite being a very
crucial task in privacy sensitive applications, tabular LLP does not yet have a
open, large scale LLP benchmark. One of the unique properties of tabular LLP is
the ability to create feature bags where all the instances in a bag have the
same value for a given feature. It has been shown in prior research that
feature bags are very common in practical, real world applications [Chen et. al
'23, Saket et. al. '22].
  In this paper, we address the lack of a open, large scale tabular benchmark.
First we propose LLP-Bench, a suite of 70 LLP datasets (62 feature bag and 8
random bag datasets) created from the Criteo CTR prediction and the Criteo
Sponsored Search Conversion Logs datasets, the former a classification and the
latter a regression dataset. These LLP datasets represent diverse ways in which
bags can be constructed from underlying tabular data. To the best of our
knowledge, LLP-Bench is the first large scale tabular LLP benchmark with an
extensive diversity in constituent datasets. Second, we propose four metrics
that characterize and quantify the hardness of a LLP dataset. Using these four
metrics we present deep analysis of the 62 feature bag datasets in LLP-Bench.
Finally we present the performance of 9 SOTA and popular tabular LLP techniques
on all the 62 datasets.","['Anand Brahmbhatt', 'Mohith Pokala', 'Rishi Saket', 'Aravindan Raghuveer']","['cs.LG', 'stat.ML']",2023-10-16 05:58:25+00:00
http://arxiv.org/abs/2310.10092v3,Label Differential Privacy via Aggregation,"In many real-world applications, due to recent developments in the privacy
landscape, training data may be aggregated to preserve the privacy of sensitive
training labels. In the learning from label proportions (LLP) framework, the
dataset is partitioned into bags of feature-vectors which are available only
with the sum of the labels per bag. A further restriction, which we call
learning from bag aggregates (LBA) is where instead of individual
feature-vectors, only the (possibly weighted) sum of the feature-vectors per
bag is available. We study whether such aggregation techniques can provide
privacy guarantees under the notion of label differential privacy (label-DP)
previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari
et al.'22].
  It is easily seen that naive LBA and LLP do not provide label-DP. Our main
result however, shows that weighted LBA using iid Gaussian weights with $m$
randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon,
\delta)$-label-DP for any $\varepsilon > 0$ with $\delta \approx
\exp(-\Omega(\sqrt{k}))$ assuming a lower bound on the linear-mse regression
loss. Further, the $\ell_2^2$-regressor which minimizes the loss on the
aggregated dataset has a loss within $\left(1 + o(1)\right)$-factor of the
optimum on the original dataset w.p. $\approx 1 - exp(-\Omega(m))$. We
emphasize that no additive label noise is required.
  The analogous weighted-LLP does not however admit label-DP. Nevertheless, we
show that if additive $N(0, 1)$ noise can be added to any constant fraction of
the instance labels, then the noisy weighted-LLP admits similar label-DP
guarantees without assumptions on the dataset, while preserving the utility of
Lipschitz-bounded neural mse-regression tasks.
  Our work is the first to demonstrate that label-DP can be achieved by
randomly weighted aggregation for regression tasks, using no or little additive
noise.","['Anand Brahmbhatt', 'Rishi Saket', 'Shreyas Havaldar', 'Anshul Nasery', 'Aravindan Raghuveer']","['cs.LG', 'stat.ML']",2023-10-16 05:54:30+00:00
http://arxiv.org/abs/2310.10013v1,Riemannian Residual Neural Networks,"Recent methods in geometric deep learning have introduced various neural
networks to operate over data that lie on Riemannian manifolds. Such networks
are often necessary to learn well over graphs with a hierarchical structure or
to learn over manifold-valued data encountered in the natural sciences. These
networks are often inspired by and directly generalize standard Euclidean
neural networks. However, extending Euclidean networks is difficult and has
only been done for a select few manifolds. In this work, we examine the
residual neural network (ResNet) and show how to extend this construction to
general Riemannian manifolds in a geometrically principled manner. Originally
introduced to help solve the vanishing gradient problem, ResNets have become
ubiquitous in machine learning due to their beneficial learning properties,
excellent empirical results, and easy-to-incorporate nature when building
varied neural networks. We find that our Riemannian ResNets mirror these
desirable properties: when compared to existing manifold neural networks
designed to learn over hyperbolic space and the manifold of symmetric positive
definite matrices, we outperform both kinds of networks in terms of relevant
testing metrics and training dynamics.","['Isay Katsman', 'Eric Ming Chen', 'Sidhanth Holalkere', 'Anna Asch', 'Aaron Lou', 'Ser-Nam Lim', 'Christopher De Sa']","['stat.ML', 'cs.LG']",2023-10-16 02:12:32+00:00
http://arxiv.org/abs/2310.10006v2,Soft ascent-descent as a stable and flexible alternative to flooding,"As a heuristic for improving test accuracy in classification, the ""flooding""
method proposed by Ishida et al. (2020) sets a threshold for the average
surrogate loss at training time; above the threshold, gradient descent is run
as usual, but below the threshold, a switch to gradient ascent is made. While
setting the threshold is non-trivial and is usually done with validation data,
this simple technique has proved remarkably effective in terms of accuracy. On
the other hand, what if we are also interested in other metrics such as model
complexity or average surrogate loss at test time? As an attempt to achieve
better overall performance with less fine-tuning, we propose a softened,
pointwise mechanism called SoftAD (soft ascent-descent) that downweights points
on the borderline, limits the effects of outliers, and retains the
ascent-descent effect of flooding, with no additional computational overhead.
We contrast formal stationarity guarantees with those for flooding, and
empirically demonstrate how SoftAD can realize classification accuracy
competitive with flooding (and the more expensive alternative SAM) while
enjoying a much smaller loss generalization gap and model norm.","['Matthew J. Holland', 'Kosuke Nakatani']","['stat.ML', 'cs.LG']",2023-10-16 02:02:56+00:00
http://arxiv.org/abs/2310.10003v1,Conformal Contextual Robust Optimization,"Data-driven approaches to predict-then-optimize decision-making problems seek
to mitigate the risk of uncertainty region misspecification in safety-critical
settings. Current approaches, however, suffer from considering overly
conservative uncertainty regions, often resulting in suboptimal decisionmaking.
To this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for
leveraging highly informative, nonconvex conformal prediction regions over
high-dimensional spaces based on conditional generative models, which have the
desired distribution-free coverage guarantees. Despite guaranteeing robustness,
such black-box optimization procedures alone inspire little confidence owing to
the lack of explanation of why a particular decision was found to be optimal.
We, therefore, augment CPO to additionally provide semantically meaningful
visual summaries of the uncertainty regions to give qualitative intuition for
the optimal decision. We highlight the CPO framework by demonstrating results
on a suite of simulation-based inference benchmark tasks and a vehicle routing
task based on probabilistic weather prediction.","['Yash Patel', 'Sahana Rayan', 'Ambuj Tewari']","['stat.ME', 'cs.LG', 'stat.ML']",2023-10-16 01:58:27+00:00
http://arxiv.org/abs/2310.09999v1,Outlier Detection Using Generative Models with Theoretical Performance Guarantees,"This paper considers the problem of recovering signals modeled by generative
models from linear measurements contaminated with sparse outliers. We propose
an outlier detection approach for reconstructing the ground-truth signals
modeled by generative models under sparse outliers. We establish theoretical
recovery guarantees for reconstruction of signals using generative models in
the presence of outliers, giving lower bounds on the number of correctable
outliers. Our results are applicable to both linear generator neural networks
and the nonlinear generator neural networks with an arbitrary number of layers.
We propose an iterative alternating direction method of multipliers (ADMM)
algorithm for solving the outlier detection problem via $\ell_1$ norm
minimization, and a gradient descent algorithm for solving the outlier
detection problem via squared $\ell_1$ norm minimization. We conduct extensive
experiments using variational auto-encoder and deep convolutional generative
adversarial networks, and the experimental results show that the signals can be
successfully reconstructed under outliers using our approach. Our approach
outperforms the traditional Lasso and $\ell_2$ minimization approach.","['Jirong Yi', 'Jingchao Gao', 'Tianming Wang', 'Xiaodong Wu', 'Weiyu Xu']","['stat.ML', 'cs.LG', 'eess.SP']",2023-10-16 01:25:34+00:00
http://arxiv.org/abs/2310.09766v2,Pseudo-Bayesian Optimization,"Bayesian Optimization is a popular approach for optimizing expensive
black-box functions. Its key idea is to use a surrogate model to approximate
the objective and, importantly, quantify the associated uncertainty that allows
a sequential search of query points that balance exploitation-exploration.
Gaussian process (GP) has been a primary candidate for the surrogate model,
thanks to its Bayesian-principled uncertainty quantification power and modeling
flexibility. However, its challenges have also spurred an array of alternatives
whose convergence properties could be more opaque. Motivated by these, we study
in this paper an axiomatic framework that elicits the minimal requirements to
guarantee black-box optimization convergence that could apply beyond GP-based
methods. Moreover, we leverage the design freedom in our framework, which we
call Pseudo-Bayesian Optimization, to construct empirically superior
algorithms. In particular, we show how using simple local regression, and a
suitable ""randomized prior"" construction to quantify uncertainty, not only
guarantees convergence but also consistently outperforms state-of-the-art
benchmarks in examples ranging from high-dimensional synthetic experiments to
realistic hyperparameter tuning and robotic applications.","['Haoxian Chen', 'Henry Lam']","['stat.ML', 'cs.LG']",2023-10-15 07:55:28+00:00
http://arxiv.org/abs/2310.09702v2,Inference with Mondrian Random Forests,"Random forests are popular methods for regression and classification
analysis, and many different variants have been proposed in recent years. One
interesting example is the Mondrian random forest, in which the underlying
constituent trees are constructed via a Mondrian process. We give precise bias
and variance characterizations, along with a Berry-Esseen-type central limit
theorem, for the Mondrian random forest regression estimator. By combining
these results with a carefully crafted debiasing approach and an accurate
variance estimator, we present valid statistical inference methods for the
unknown regression function. These methods come with explicitly characterized
error bounds in terms of the sample size, tree complexity parameter, and number
of trees in the forest, and include coverage error rates for feasible
confidence interval estimators. Our novel debiasing procedure for the Mondrian
random forest also allows it to achieve the minimax-optimal point estimation
convergence rate in mean squared error for multivariate $\beta$-H\""older
regression functions, for all $\beta > 0$, provided that the underlying tuning
parameters are chosen appropriately. Efficient and implementable algorithms are
devised for both batch and online learning settings, and we carefully study the
computational complexity of different Mondrian random forest implementations.
Finally, simulations with synthetic data validate our theory and methodology,
demonstrating their excellent finite-sample properties.","['Matias D. Cattaneo', 'Jason M. Klusowski', 'William G. Underwood']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH', '62G08 (Primary), 62G05, 62G20 (Secondary)']",2023-10-15 01:41:42+00:00
http://arxiv.org/abs/2310.09639v3,DPZero: Private Fine-Tuning of Language Models without Backpropagation,"The widespread practice of fine-tuning large language models (LLMs) on
domain-specific data faces two major challenges in memory and privacy. First,
as the size of LLMs continues to grow, the memory demands of gradient-based
training methods via backpropagation become prohibitively high. Second, given
the tendency of LLMs to memorize training data, it is important to protect
potentially sensitive information in the fine-tuning data from being
regurgitated. Zeroth-order methods, which rely solely on forward passes,
substantially reduce memory consumption during training. However, directly
combining them with standard differentially private gradient descent suffers
more as model size grows. To bridge this gap, we introduce DPZero, a novel
private zeroth-order algorithm with nearly dimension-independent rates. The
memory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa
and OPT on several downstream tasks. Our code is available at
https://github.com/Liang137/DPZero.","['Liang Zhang', 'Bingcong Li', 'Kiran Koshy Thekumparampil', 'Sewoong Oh', 'Niao He']","['cs.LG', 'cs.CR', 'math.OC', 'stat.ML']",2023-10-14 18:42:56+00:00
http://arxiv.org/abs/2310.09597v2,Adaptive maximization of social welfare,"We consider the problem of repeatedly choosing policies to maximize social
welfare. Welfare is a weighted sum of private utility and public revenue.
Earlier outcomes inform later policies. Utility is not observed, but indirectly
inferred. Response functions are learned through experimentation. We derive a
lower bound on regret, and a matching adversarial upper bound for a variant of
the Exp3 algorithm. Cumulative regret grows at a rate of $T^{2/3}$. This
implies that (i) welfare maximization is harder than the multi-armed bandit
problem (with a rate of $T^{1/2}$ for finite policy sets), and (ii) our
algorithm achieves the optimal rate. For the stochastic setting, if social
welfare is concave, we can achieve a rate of $T^{1/2}$ (for continuous policy
sets), using a dyadic search algorithm. We analyze an extension to nonlinear
income taxation, and sketch an extension to commodity taxation. We compare our
setting to monopoly pricing (which is easier), and price setting for bilateral
trade (which is harder).","['Nicolo Cesa-Bianchi', 'Roberto Colomboni', 'Maximilian Kasy']","['econ.EM', 'cs.LG', 'stat.ML']",2023-10-14 15:09:56+00:00
http://arxiv.org/abs/2310.09583v2,Two Sides of The Same Coin: Bridging Deep Equilibrium Models and Neural ODEs via Homotopy Continuation,"Deep Equilibrium Models (DEQs) and Neural Ordinary Differential Equations
(Neural ODEs) are two branches of implicit models that have achieved remarkable
success owing to their superior performance and low memory consumption. While
both are implicit models, DEQs and Neural ODEs are derived from different
mathematical formulations. Inspired by homotopy continuation, we establish a
connection between these two models and illustrate that they are actually two
sides of the same coin. Homotopy continuation is a classical method of solving
nonlinear equations based on a corresponding ODE. Given this connection, we
proposed a new implicit model called HomoODE that inherits the property of high
accuracy from DEQs and the property of stability from Neural ODEs. Unlike DEQs,
which explicitly solve an equilibrium-point-finding problem via Newton's
methods in the forward pass, HomoODE solves the equilibrium-point-finding
problem implicitly using a modified Neural ODE via homotopy continuation.
Further, we developed an acceleration method for HomoODE with a shared
learnable initial point. It is worth noting that our model also provides a
better understanding of why Augmented Neural ODEs work as long as the augmented
part is regarded as the equilibrium point to find. Comprehensive experiments
with several image classification tasks demonstrate that HomoODE surpasses
existing implicit models in terms of both accuracy and memory consumption.","['Shutong Ding', 'Tianyu Cui', 'Jingya Wang', 'Ye Shi']","['cs.LG', 'stat.ML']",2023-10-14 13:28:36+00:00
http://arxiv.org/abs/2310.09553v1,ARTree: A Deep Autoregressive Model for Phylogenetic Inference,"Designing flexible probabilistic models over tree topologies is important for
developing efficient phylogenetic inference methods. To do that, previous works
often leverage the similarity of tree topologies via hand-engineered heuristic
features which would require pre-sampled tree topologies and may suffer from
limited approximation capability. In this paper, we propose a deep
autoregressive model for phylogenetic inference based on graph neural networks
(GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf
node addition operations and modeling the involved conditional distributions
based on learnable topological features via GNNs, ARTree can provide a rich
family of distributions over the entire tree topology space that have simple
sampling algorithms and density estimation procedures, without using heuristic
features. We demonstrate the effectiveness and efficiency of our method on a
benchmark of challenging real data tree topology density estimation and
variational Bayesian phylogenetic inference problems.","['Tianyu Xie', 'Cheng Zhang']","['q-bio.PE', 'cs.LG', 'stat.ML']",2023-10-14 10:26:03+00:00
http://arxiv.org/abs/2310.09545v1,A Semiparametric Instrumented Difference-in-Differences Approach to Policy Learning,"Recently, there has been a surge in methodological development for the
difference-in-differences (DiD) approach to evaluate causal effects. Standard
methods in the literature rely on the parallel trends assumption to identify
the average treatment effect on the treated. However, the parallel trends
assumption may be violated in the presence of unmeasured confounding, and the
average treatment effect on the treated may not be useful in learning a
treatment assignment policy for the entire population. In this article, we
propose a general instrumented DiD approach for learning the optimal treatment
policy. Specifically, we establish identification results using a binary
instrumental variable (IV) when the parallel trends assumption fails to hold.
Additionally, we construct a Wald estimator, novel inverse probability
weighting (IPW) estimators, and a class of semiparametric efficient and
multiply robust estimators, with theoretical guarantees on consistency and
asymptotic normality, even when relying on flexible machine learning algorithms
for nuisance parameters estimation. Furthermore, we extend the instrumented DiD
to the panel data setting. We evaluate our methods in extensive simulations and
a real data application.","['Pan Zhao', 'Yifan Cui']","['stat.ME', 'econ.EM', 'math.ST', 'stat.ML', 'stat.TH']",2023-10-14 09:38:32+00:00
http://arxiv.org/abs/2310.09516v1,Efficient Link Prediction via GNN Layers Induced by Negative Sampling,"Graph neural networks (GNNs) for link prediction can loosely be divided into
two broad categories. First, \emph{node-wise} architectures pre-compute
individual embeddings for each node that are later combined by a simple decoder
to make predictions. While extremely efficient at inference time (since node
embeddings are only computed once and repeatedly reused), model expressiveness
is limited such that isomorphic nodes contributing to candidate edges may not
be distinguishable, compromising accuracy. In contrast, \emph{edge-wise}
methods rely on the formation of edge-specific subgraph embeddings to enrich
the representation of pair-wise relationships, disambiguating isomorphic nodes
to improve accuracy, but with the cost of increased model complexity. To better
navigate this trade-off, we propose a novel GNN architecture whereby the
\emph{forward pass} explicitly depends on \emph{both} positive (as is typical)
and negative (unique to our approach) edges to inform more flexible, yet still
cheap node-wise embeddings. This is achieved by recasting the embeddings
themselves as minimizers of a forward-pass-specific energy function (distinct
from the actual training loss) that favors separation of positive and negative
samples. As demonstrated by extensive empirical evaluations, the resulting
architecture retains the inference speed of node-wise models, while producing
competitive accuracy with edge-wise alternatives.","['Yuxin Wang', 'Xiannian Hu', 'Quan Gan', 'Xuanjing Huang', 'Xipeng Qiu', 'David Wipf']","['cs.LG', 'stat.ML']",2023-10-14 07:02:54+00:00
http://arxiv.org/abs/2310.09495v1,Learning In-between Imagery Dynamics via Physical Latent Spaces,"We present a framework designed to learn the underlying dynamics between two
images observed at consecutive time steps. The complex nature of image data and
the lack of temporal information pose significant challenges in capturing the
unique evolving patterns. Our proposed method focuses on estimating the
intermediary stages of image evolution, allowing for interpretability through
latent dynamics while preserving spatial correlations with the image. By
incorporating a latent variable that follows a physical model expressed in
partial differential equations (PDEs), our approach ensures the
interpretability of the learned model and provides insight into corresponding
image dynamics. We demonstrate the robustness and effectiveness of our learning
framework through a series of numerical tests using geoscientific imagery data.","['Jihun Han', 'Yoonsang Lee', 'Anne Gelb']","['cs.LG', 'cs.CV', 'stat.ML', '37M05, 62F99, 68T45']",2023-10-14 05:14:51+00:00
http://arxiv.org/abs/2310.09488v1,ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning,"Long-term time series forecasting (LTSF) is important for various domains but
is confronted by challenges in handling the complex temporal-contextual
relationships. As multivariate input models underperforming some recent
univariate counterparts, we posit that the issue lies in the inefficiency of
existing multivariate LTSF Transformers to model series-wise relationships: the
characteristic differences between series are often captured incorrectly. To
address this, we introduce ARM: a multivariate temporal-contextual adaptive
learning method, which is an enhanced architecture specifically designed for
multivariate LTSF modelling. ARM employs Adaptive Univariate Effect Learning
(AUEL), Random Dropping (RD) training strategy, and Multi-kernel Local
Smoothing (MKLS), to better handle individual series temporal patterns and
correctly learn inter-series dependencies. ARM demonstrates superior
performance on multiple benchmarks without significantly increasing
computational costs compared to vanilla Transformer, thereby advancing the
state-of-the-art in LTSF. ARM is also generally applicable to other LTSF
architecture beyond vanilla Transformer.","['Jiecheng Lu', 'Xu Han', 'Shihao Yang']","['stat.ML', 'cs.LG']",2023-10-14 04:37:38+00:00
http://arxiv.org/abs/2310.09437v2,Signal reconstruction using determinantal sampling,"We study the approximation of a square-integrable function from a finite
number of evaluations on a random set of nodes according to a well-chosen
distribution. This is particularly relevant when the function is assumed to
belong to a reproducing kernel Hilbert space (RKHS). This work proposes to
combine several natural finite-dimensional approximations based two possible
probability distributions of nodes. These distributions are related to
determinantal point processes, and use the kernel of the RKHS to favor
RKHS-adapted regularity in the random design. While previous work on
determinantal sampling relied on the RKHS norm, we prove mean-square guarantees
in $L^2$ norm. We show that determinantal point processes and mixtures thereof
can yield fast convergence rates. Our results also shed light on how the rate
changes as more smoothness is assumed, a phenomenon known as superconvergence.
Besides, determinantal sampling generalizes i.i.d. sampling from the
Christoffel function which is standard in the literature. More importantly,
determinantal sampling guarantees the so-called instance optimality property
for a smaller number of function evaluations than i.i.d. sampling.","['Ayoub Belhadji', 'Rémi Bardenet', 'Pierre Chainais']","['stat.ML', 'cs.NA', 'math.NA']",2023-10-13 23:02:57+00:00
http://arxiv.org/abs/2310.09426v1,Offline Reinforcement Learning for Optimizing Production Bidding Policies,"The online advertising market, with its thousands of auctions run per second,
presents a daunting challenge for advertisers who wish to optimize their spend
under a budget constraint. Thus, advertising platforms typically provide
automated agents to their customers, which act on their behalf to bid for
impression opportunities in real time at scale. Because these proxy agents are
owned by the platform but use advertiser funds to operate, there is a strong
practical need to balance reliability and explainability of the agent with
optimizing power. We propose a generalizable approach to optimizing bidding
policies in production environments by learning from real data using offline
reinforcement learning. This approach can be used to optimize any
differentiable base policy (practically, a heuristic policy based on principles
which the advertiser can easily understand), and only requires data generated
by the base policy itself. We use a hybrid agent architecture that combines
arbitrary base policies with deep neural networks, where only the optimized
base policy parameters are eventually deployed, and the neural network part is
discarded after training. We demonstrate that such an architecture achieves
statistically significant performance gains in both simulated and at-scale
production bidding environments. Our approach does not incur additional
infrastructure, safety, or explainability costs, as it directly optimizes
parameters of existing production routines without replacing them with black
box-style models like neural networks.","['Dmytro Korenkevych', 'Frank Cheng', 'Artsiom Balakir', 'Alex Nikulkov', 'Lingnan Gao', 'Zhihao Cen', 'Zuobing Xu', 'Zheqing Zhu']","['cs.LG', 'stat.ML']",2023-10-13 22:14:51+00:00
http://arxiv.org/abs/2310.09335v1,Statistical guarantees for stochastic Metropolis-Hastings,"A Metropolis-Hastings step is widely used for gradient-based Markov chain
Monte Carlo methods in uncertainty quantification. By calculating acceptance
probabilities on batches, a stochastic Metropolis-Hastings step saves
computational costs, but reduces the effective sample size. We show that this
obstacle can be avoided by a simple correction term. We study statistical
properties of the resulting stationary distribution of the chain if the
corrected stochastic Metropolis-Hastings approach is applied to sample from a
Gibbs posterior distribution in a nonparametric regression setting. Focusing on
deep neural network regression, we prove a PAC-Bayes oracle inequality which
yields optimal contraction rates and we analyze the diameter and show high
coverage probability of the resulting credible sets. With a numerical example
in a high-dimensional parameter space, we illustrate that credible sets and
contraction rates of the stochastic Metropolis-Hastings algorithm indeed behave
similar to those obtained from the classical Metropolis-adjusted Langevin
algorithm.","['Sebastian Bieringer', 'Gregor Kasieczka', 'Maximilian F. Steffen', 'Mathias Trabs']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2023-10-13 18:00:26+00:00
http://arxiv.org/abs/2310.09254v4,GENOT: Entropic (Gromov) Wasserstein Flow Matching with Applications to Single-Cell Genomics,"Single-cell genomics has significantly advanced our understanding of cellular
behavior, catalyzing innovations in treatments and precision medicine. However,
single-cell sequencing technologies are inherently destructive and can only
measure a limited array of data modalities simultaneously. This limitation
underscores the need for new methods capable of realigning cells. Optimal
transport (OT) has emerged as a potent solution, but traditional discrete
solvers are hampered by scalability, privacy, and out-of-sample estimation
issues. These challenges have spurred the development of neural network-based
solvers, known as neural OT solvers, that parameterize OT maps. Yet, these
models often lack the flexibility needed for broader life science applications.
To address these deficiencies, our approach learns stochastic maps (i.e.
transport plans), allows for any cost function, relaxes mass conservation
constraints and integrates quadratic solvers to tackle the complex challenges
posed by the (Fused) Gromov-Wasserstein problem. Utilizing flow matching as a
backbone, our method offers a flexible and effective framework. We demonstrate
its versatility and robustness through applications in cell development
studies, cellular drug response modeling, and cross-modality cell translation,
illustrating significant potential for enhancing therapeutic strategies.","['Dominik Klein', 'Théo Uscidda', 'Fabian Theis', 'Marco Cuturi']","['stat.ML', 'cs.LG']",2023-10-13 17:12:04+00:00
http://arxiv.org/abs/2310.09250v1,"It's an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep Models","Classical wisdom in machine learning holds that the generalization error can
be decomposed into bias and variance, and these two terms exhibit a
\emph{trade-off}. However, in this paper, we show that for an ensemble of deep
learning based classification models, bias and variance are \emph{aligned} at a
sample level, where squared bias is approximately \emph{equal} to variance for
correctly classified sample points. We present empirical evidence confirming
this phenomenon in a variety of deep learning models and datasets. Moreover, we
study this phenomenon from two theoretical perspectives: calibration and neural
collapse. We first show theoretically that under the assumption that the models
are well calibrated, we can observe the bias-variance alignment. Second,
starting from the picture provided by the neural collapse theory, we show an
approximate correlation between bias and variance.","['Lin Chen', 'Michal Lukasik', 'Wittawat Jitkrittum', 'Chong You', 'Sanjiv Kumar']","['cs.LG', 'cs.AI', 'stat.ML']",2023-10-13 17:06:34+00:00
http://arxiv.org/abs/2310.09157v2,The Computational Complexity of Finding Stationary Points in Non-Convex Optimization,"Finding approximate stationary points, i.e., points where the gradient is
approximately zero, of non-convex but smooth objective functions $f$ over
unrestricted $d$-dimensional domains is one of the most fundamental problems in
classical non-convex optimization. Nevertheless, the computational and query
complexity of this problem are still not well understood when the dimension $d$
of the problem is independent of the approximation error. In this paper, we
show the following computational and query complexity results:
  1. The problem of finding approximate stationary points over unrestricted
domains is PLS-complete.
  2. For $d = 2$, we provide a zero-order algorithm for finding
$\varepsilon$-approximate stationary points that requires at most
$O(1/\varepsilon)$ value queries to the objective function.
  3. We show that any algorithm needs at least $\Omega(1/\varepsilon)$ queries
to the objective function and/or its gradient to find $\varepsilon$-approximate
stationary points when $d=2$. Combined with the above, this characterizes the
query complexity of this problem to be $\Theta(1/\varepsilon)$.
  4. For $d = 2$, we provide a zero-order algorithm for finding
$\varepsilon$-KKT points in constrained optimization problems that requires at
most $O(1/\sqrt{\varepsilon})$ value queries to the objective function. This
closes the gap between the works of Bubeck and Mikulincer [2020] and Vavasis
[1993] and characterizes the query complexity of this problem to be
$\Theta(1/\sqrt{\varepsilon})$.
  5. Combining our results with the recent result of Fearnley et al. [2022], we
show that finding approximate KKT points in constrained optimization is
reducible to finding approximate stationary points in unconstrained
optimization but the converse is impossible.","['Alexandros Hollender', 'Manolis Zampetakis']","['math.OC', 'cs.CC', 'cs.LG', 'stat.ML']",2023-10-13 14:52:46+00:00
http://arxiv.org/abs/2310.09149v2,Wasserstein approximation schemes based on Voronoi partitions,"We consider structured approximation of measures in Wasserstein space
$\mathrm{W}_p(\mathbb{R}^d)$ for $p\in[1,\infty)$ using general measure
approximants compactly supported on Voronoi regions derived from a scaled
Voronoi partition of $\mathbb{R}^d$. We show that if a full rank lattice
$\Lambda$ is scaled by a factor of $h\in(0,1]$, then approximation of a measure
based on the Voronoi partition of $h\Lambda$ is $O(h)$ regardless of $d$ or
$p$. We then use a covering argument to show that $N$-term approximations of
compactly supported measures is $O(N^{-\frac1d})$ which matches known rates for
optimal quantizers and empirical measure approximation in most instances.
Additionally, we generalize our construction to nonuniform Voronoi partitions,
highlighting the flexibility and robustness of our approach for various measure
approximation scenarios. Finally, we extend these results to noncompactly
supported measures with sufficient decay. Our findings are pertinent to
applications in computer vision and machine learning where measures are used to
represent structured data such as images.","['Keaton Hamm', 'Varun Khurana']","['stat.ML', 'cs.LG', 'cs.NA', 'math.NA']",2023-10-13 14:43:11+00:00
http://arxiv.org/abs/2310.09129v1,Computing Marginal and Conditional Divergences between Decomposable Models with Applications,"The ability to compute the exact divergence between two high-dimensional
distributions is useful in many applications but doing so naively is
intractable. Computing the alpha-beta divergence -- a family of divergences
that includes the Kullback-Leibler divergence and Hellinger distance -- between
the joint distribution of two decomposable models, i.e chordal Markov networks,
can be done in time exponential in the treewidth of these models. However,
reducing the dissimilarity between two high-dimensional objects to a single
scalar value can be uninformative. Furthermore, in applications such as
supervised learning, the divergence over a conditional distribution might be of
more interest. Therefore, we propose an approach to compute the exact
alpha-beta divergence between any marginal or conditional distribution of two
decomposable models. Doing so tractably is non-trivial as we need to decompose
the divergence between these distributions and therefore, require a
decomposition over the marginal and conditional distributions of these models.
Consequently, we provide such a decomposition and also extend existing work to
compute the marginal and conditional alpha-beta divergence between these
decompositions. We then show how our method can be used to analyze
distributional changes by first applying it to a benchmark image dataset.
Finally, based on our framework, we propose a novel way to quantify the error
in contemporary superconducting quantum computers. Code for all experiments is
available at: https://lklee.dev/pub/2023-icdm/code","['Loong Kuan Lee', 'Geoffrey I. Webb', 'Daniel F. Schmidt', 'Nico Piatkowski']","['cs.LG', 'stat.ML']",2023-10-13 14:17:25+00:00
http://arxiv.org/abs/2310.09123v1,Automatic Music Playlist Generation via Simulation-based Reinforcement Learning,"Personalization of playlists is a common feature in music streaming services,
but conventional techniques, such as collaborative filtering, rely on explicit
assumptions regarding content quality to learn how to make recommendations.
Such assumptions often result in misalignment between offline model objectives
and online user satisfaction metrics. In this paper, we present a reinforcement
learning framework that solves for such limitations by directly optimizing for
user satisfaction metrics via the use of a simulated playlist-generation
environment. Using this simulator we develop and train a modified Deep
Q-Network, the action head DQN (AH-DQN), in a manner that addresses the
challenges imposed by the large state and action space of our RL formulation.
The resulting policy is capable of making recommendations from large and
dynamic sets of candidate items with the expectation of maximizing consumption
metrics. We analyze and evaluate agents offline via simulations that use
environment models trained on both public and proprietary streaming datasets.
We show how these agents lead to better user-satisfaction metrics compared to
baseline methods during online A/B tests. Finally, we demonstrate that
performance assessments produced from our simulator are strongly correlated
with observed online metric results.","['Federico Tomasi', 'Joseph Cauteruccio', 'Surya Kanoria', 'Kamil Ciosek', 'Matteo Rinaldi', 'Zhenwen Dai']","['stat.ML', 'cs.LG']",2023-10-13 14:13:02+00:00
http://arxiv.org/abs/2310.09031v2,MINDE: Mutual Information Neural Diffusion Estimation,"In this work we present a new method for the estimation of Mutual Information
(MI) between random variables. Our approach is based on an original
interpretation of the Girsanov theorem, which allows us to use score-based
diffusion models to estimate the Kullback Leibler divergence between two
densities as a difference between their score functions. As a by-product, our
method also enables the estimation of the entropy of random variables. Armed
with such building blocks, we present a general recipe to measure MI, which
unfolds in two directions: one uses conditional diffusion process, whereas the
other uses joint diffusion processes that allow simultaneous modelling of two
random variables. Our results, which derive from a thorough experimental
protocol over all the variants of our approach, indicate that our method is
more accurate than the main alternatives from the literature, especially for
challenging distributions. Furthermore, our methods pass MI self-consistency
tests, including data processing and additivity under independence, which
instead are a pain-point of existing methods.","['Giulio Franzese', 'Mustapha Bounoua', 'Pietro Michiardi']","['cs.LG', 'stat.ML']",2023-10-13 11:47:41+00:00
http://arxiv.org/abs/2310.09028v1,Subspace Adaptation Prior for Few-Shot Learning,"Gradient-based meta-learning techniques aim to distill useful prior knowledge
from a set of training tasks such that new tasks can be learned more
efficiently with gradient descent. While these methods have achieved successes
in various scenarios, they commonly adapt all parameters of trainable layers
when learning new tasks. This neglects potentially more efficient learning
strategies for a given task distribution and may be susceptible to overfitting,
especially in few-shot learning where tasks must be learned from a limited
number of examples. To address these issues, we propose Subspace Adaptation
Prior (SAP), a novel gradient-based meta-learning algorithm that jointly learns
good initialization parameters (prior knowledge) and layer-wise parameter
subspaces in the form of operation subsets that should be adaptable. In this
way, SAP can learn which operation subsets to adjust with gradient descent
based on the underlying task distribution, simultaneously decreasing the risk
of overfitting when learning new tasks. We demonstrate that this ability is
helpful as SAP yields superior or competitive performance in few-shot image
classification settings (gains between 0.1% and 3.9% in accuracy). Analysis of
the learned subspaces demonstrates that low-dimensional operations often yield
high activation strengths, indicating that they may be important for achieving
good few-shot learning performance. For reproducibility purposes, we publish
all our research code publicly.","['Mike Huisman', 'Aske Plaat', 'Jan N. van Rijn']","['cs.LG', 'cs.AI', 'stat.ML']",2023-10-13 11:40:18+00:00
http://arxiv.org/abs/2310.08939v1,Fast Screening Rules for Optimal Design via Quadratic Lasso Reformulation,"The problems of Lasso regression and optimal design of experiments share a
critical property: their optimal solutions are typically \emph{sparse}, i.e.,
only a small fraction of the optimal variables are non-zero. Therefore, the
identification of the support of an optimal solution reduces the dimensionality
of the problem and can yield a substantial simplification of the calculations.
It has recently been shown that linear regression with a \emph{squared}
$\ell_1$-norm sparsity-inducing penalty is equivalent to an optimal
experimental design problem. In this work, we use this equivalence to derive
safe screening rules that can be used to discard inessential samples. Compared
to previously existing rules, the new tests are much faster to compute,
especially for problems involving a parameter space of high dimension, and can
be used dynamically within any iterative solver, with negligible computational
overhead. Moreover, we show how an existing homotopy algorithm to compute the
regularization path of the lasso method can be reparametrized with respect to
the squared $\ell_1$-penalty. This allows the computation of a Bayes
$c$-optimal design in a finite number of steps and can be several orders of
magnitude faster than standard first-order algorithms. The efficiency of the
new screening rules and of the homotopy algorithm are demonstrated on different
examples based on real data.","['Guillaume Sagnol', 'Luc Pronzato']","['stat.ME', 'math.OC', 'stat.ML', '62K05, 49M29', 'G.3']",2023-10-13 08:10:46+00:00
http://arxiv.org/abs/2310.08858v1,Adam-family Methods with Decoupled Weight Decay in Deep Learning,"In this paper, we investigate the convergence properties of a wide class of
Adam-family methods for minimizing quadratically regularized nonsmooth
nonconvex optimization problems, especially in the context of training
nonsmooth neural networks with weight decay. Motivated by the AdamW method, we
propose a novel framework for Adam-family methods with decoupled weight decay.
Within our framework, the estimators for the first-order and second-order
moments of stochastic subgradients are updated independently of the weight
decay term. Under mild assumptions and with non-diminishing stepsizes for
updating the primary optimization variables, we establish the convergence
properties of our proposed framework. In addition, we show that our proposed
framework encompasses a wide variety of well-known Adam-family methods, hence
offering convergence guarantees for these methods in the training of nonsmooth
neural networks. More importantly, we show that our proposed framework
asymptotically approximates the SGD method, thereby providing an explanation
for the empirical observation that decoupled weight decay enhances
generalization performance for Adam-family methods. As a practical application
of our proposed framework, we propose a novel Adam-family method named Adam
with Decoupled Weight Decay (AdamD), and establish its convergence properties
under mild conditions. Numerical experiments demonstrate that AdamD outperforms
Adam and is comparable to AdamW, in the aspects of both generalization
performance and efficiency.","['Kuangyu Ding', 'Nachuan Xiao', 'Kim-Chuan Toh']","['math.OC', 'cs.AI', 'cs.LG', 'stat.ML']",2023-10-13 04:59:44+00:00
http://arxiv.org/abs/2310.08841v1,Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning in Surgical Robotic Environments,"Most Reinforcement Learning (RL) methods are traditionally studied in an
active learning setting, where agents directly interact with their
environments, observe action outcomes, and learn through trial and error.
However, allowing partially trained agents to interact with real physical
systems poses significant challenges, including high costs, safety risks, and
the need for constant supervision. Offline RL addresses these cost and safety
concerns by leveraging existing datasets and reducing the need for
resource-intensive real-time interactions. Nevertheless, a substantial
challenge lies in the demand for these datasets to be meticulously annotated
with rewards. In this paper, we introduce Optimal Transport Reward (OTR)
labelling, an innovative algorithm designed to assign rewards to offline
trajectories, using a small number of high-quality expert demonstrations. The
core principle of OTR involves employing Optimal Transport (OT) to calculate an
optimal alignment between an unlabeled trajectory from the dataset and an
expert demonstration. This alignment yields a similarity measure that is
effectively interpreted as a reward signal. An offline RL algorithm can then
utilize these reward signals to learn a policy. This approach circumvents the
need for handcrafted rewards, unlocking the potential to harness vast datasets
for policy learning. Leveraging the SurRoL simulation platform tailored for
surgical robot learning, we generate datasets and employ them to train policies
using the OTR algorithm. By demonstrating the efficacy of OTR in a different
domain, we emphasize its versatility and its potential to expedite RL
deployment across a wide range of fields.","['Maryam Zare', 'Parham M. Kebria', 'Abbas Khosravi']","['cs.AI', 'cs.RO', 'stat.ML']",2023-10-13 03:39:15+00:00
http://arxiv.org/abs/2310.08833v2,Optimal Sample Complexity for Average Reward Markov Decision Processes,"We resolve the open question regarding the sample complexity of policy
learning for maximizing the long-run average reward associated with a uniformly
ergodic Markov decision process (MDP), assuming a generative model. In this
context, the existing literature provides a sample complexity upper bound of
$\widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$ and a lower bound of
$\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$. In these expressions, $|S|$ and
$|A|$ denote the cardinalities of the state and action spaces respectively,
$t_{\text{mix}}$ serves as a uniform upper limit for the total variation mixing
times, and $\epsilon$ signifies the error tolerance. Therefore, a notable gap
of $t_{\text{mix}}$ still remains to be bridged. Our primary contribution is
the development of an estimator for the optimal policy of average reward MDPs
with a sample complexity of $\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$.
This marks the first algorithm and analysis to reach the literature's lower
bound. Our new algorithm draws inspiration from ideas in Li et al. (2020), Jin
and Sidford (2021), and Wang et al. (2023). Additionally, we conduct numerical
experiments to validate our theoretical findings.","['Shengbo Wang', 'Jose Blanchet', 'Peter Glynn']","['cs.LG', 'math.OC', 'stat.ML']",2023-10-13 03:08:59+00:00
http://arxiv.org/abs/2310.08824v1,Confounding-Robust Policy Improvement with Human-AI Teams,"Human-AI collaboration has the potential to transform various domains by
leveraging the complementary strengths of human experts and Artificial
Intelligence (AI) systems. However, unobserved confounding can undermine the
effectiveness of this collaboration, leading to biased and unreliable outcomes.
In this paper, we propose a novel solution to address unobserved confounding in
human-AI collaboration by employing the marginal sensitivity model (MSM). Our
approach combines domain expertise with AI-driven statistical modeling to
account for potential confounders that may otherwise remain hidden. We present
a deferral collaboration framework for incorporating the MSM into policy
learning from observational data, enabling the system to control for the
influence of unobserved confounding factors. In addition, we propose a
personalized deferral collaboration system to leverage the diverse expertise of
different human decision-makers. By adjusting for potential biases, our
proposed solution enhances the robustness and reliability of collaborative
outcomes. The empirical and theoretical analyses demonstrate the efficacy of
our approach in mitigating unobserved confounding and improving the overall
performance of human-AI collaborations.","['Ruijiang Gao', 'Mingzhang Yin']","['cs.HC', 'stat.ML']",2023-10-13 02:39:52+00:00
http://arxiv.org/abs/2310.08798v1,Alteration Detection of Tensor Dependence Structure via Sparsity-Exploited Reranking Algorithm,"Tensor-valued data arise frequently from a wide variety of scientific
applications, and many among them can be translated into an alteration
detection problem of tensor dependence structures. In this article, we
formulate the problem under the popularly adopted tensor-normal distributions
and aim at two-sample correlation/partial correlation comparisons of
tensor-valued observations. Through decorrelation and centralization, a
separable covariance structure is employed to pool sample information from
different tensor modes to enhance the power of the test. Additionally, we
propose a novel Sparsity-Exploited Reranking Algorithm (SERA) to further
improve the multiple testing efficiency. The algorithm is approached through
reranking of the p-values derived from the primary test statistics, by
incorporating a carefully constructed auxiliary tensor sequence. Besides the
tensor framework, SERA is also generally applicable to a wide range of
two-sample large-scale inference problems with sparsity structures, and is of
independent interest. The asymptotic properties of the proposed test are
derived and the algorithm is shown to control the false discovery at the
pre-specified level. We demonstrate the efficacy of the proposed method through
intensive simulations and two scientific applications.","['Li Ma', 'Shenghao Qin', 'Yin Xia']","['stat.ME', 'stat.AP', 'stat.ML']",2023-10-13 01:04:22+00:00
http://arxiv.org/abs/2310.08774v2,PhyloGFN: Phylogenetic inference with generative flow networks,"Phylogenetics is a branch of computational biology that studies the
evolutionary relationships among biological entities. Its long history and
numerous applications notwithstanding, inference of phylogenetic trees from
sequence data remains challenging: the high complexity of tree space poses a
significant obstacle for the current combinatorial and probabilistic
techniques. In this paper, we adopt the framework of generative flow networks
(GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and
Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling
complex combinatorial structures, they are a natural choice for exploring and
sampling from the multimodal posterior distribution over tree topologies and
evolutionary distances. We demonstrate that our amortized posterior sampler,
PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real
benchmark datasets. PhyloGFN is competitive with prior works in marginal
likelihood estimation and achieves a closer fit to the target distribution than
state-of-the-art variational inference methods. Our code is available at
https://github.com/zmy1116/phylogfn.","['Mingyang Zhou', 'Zichao Yan', 'Elliot Layne', 'Nikolay Malkin', 'Dinghuai Zhang', 'Moksh Jain', 'Mathieu Blanchette', 'Yoshua Bengio']","['q-bio.PE', 'cs.LG', 'stat.ML']",2023-10-12 23:46:08+00:00
http://arxiv.org/abs/2310.08762v1,Stabilizing Subject Transfer in EEG Classification with Divergence Estimation,"Classification models for electroencephalogram (EEG) data show a large
decrease in performance when evaluated on unseen test sub jects. We reduce this
performance decrease using new regularization techniques during model training.
We propose several graphical models to describe an EEG classification task.
From each model, we identify statistical relationships that should hold true in
an idealized training scenario (with infinite data and a globally-optimal
model) but that may not hold in practice. We design regularization penalties to
enforce these relationships in two stages. First, we identify suitable proxy
quantities (divergences such as Mutual Information and Wasserstein-1) that can
be used to measure statistical independence and dependence relationships.
Second, we provide algorithms to efficiently estimate these quantities during
training using secondary neural network models. We conduct extensive
computational experiments using a large benchmark EEG dataset, comparing our
proposed techniques with a baseline method that uses an adversarial classifier.
We find our proposed methods significantly increase balanced accuracy on test
subjects and decrease overfitting. The proposed methods exhibit a larger
benefit over a greater range of hyperparameters than the baseline method, with
only a small computational cost at training time. These benefits are largest
when used for a fixed training period, though there is still a significant
benefit for a subset of hyperparameters when our techniques are used in
conjunction with early stopping regularization.","['Niklas Smedemark-Margulies', 'Ye Wang', 'Toshiaki Koike-Akino', 'Jing Liu', 'Kieran Parsons', 'Yunus Bicer', 'Deniz Erdogmus']","['cs.LG', 'cs.AI', 'cs.HC', 'eess.SP', 'stat.ML']",2023-10-12 23:06:52+00:00
http://arxiv.org/abs/2310.08741v1,An adaptive ensemble filter for heavy-tailed distributions: tuning-free inflation and localization,"Heavy tails is a common feature of filtering distributions that results from
the nonlinear dynamical and observation processes as well as the uncertainty
from physical sensors. In these settings, the Kalman filter and its ensemble
version - the ensemble Kalman filter (EnKF) - that have been designed under
Gaussian assumptions result in degraded performance. t-distributions are a
parametric family of distributions whose tail-heaviness is modulated by a
degree of freedom $\nu$. Interestingly, Cauchy and Gaussian distributions
correspond to the extreme cases of a t-distribution for $\nu = 1$ and $\nu =
\infty$, respectively. Leveraging tools from measure transport (Spantini et
al., SIAM Review, 2022), we present a generalization of the EnKF whose
prior-to-posterior update leads to exact inference for t-distributions. We
demonstrate that this filter is less sensitive to outlying synthetic
observations generated by the observation model for small $\nu$. Moreover, it
recovers the Kalman filter for $\nu = \infty$. For nonlinear state-space models
with heavy-tailed noise, we propose an algorithm to estimate the
prior-to-posterior update from samples of joint forecast distribution of the
states and observations. We rely on a regularized expectation-maximization (EM)
algorithm to estimate the mean, scale matrix, and degree of freedom of
heavy-tailed \textit{t}-distributions from limited samples (Finegold and Drton,
arXiv preprint, 2014). Leveraging the conditional independence of the joint
forecast distribution, we regularize the scale matrix with an $l1$
sparsity-promoting penalization of the log-likelihood at each iteration of the
EM algorithm. By sequentially estimating the degree of freedom at each analysis
step, our filter can adapt its prior-to-posterior update to the tail-heaviness
of the data. We demonstrate the benefits of this new ensemble filter on
challenging filtering problems.","['Mathieu Le Provost', 'Ricardo Baptista', 'Jeff D. Eldredge', 'Youssef Marzouk']","['stat.CO', 'physics.data-an', 'stat.ML']",2023-10-12 21:56:14+00:00
http://arxiv.org/abs/2310.08672v2,Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal,"In many settings, interventions may be more effective for some individuals
than others, so that targeting interventions may be beneficial. We analyze the
value of targeting in the context of a large-scale field experiment with over
53,000 college students, where the goal was to use ""nudges"" to encourage
students to renew their financial-aid applications before a non-binding
deadline. We begin with baseline approaches to targeting. First, we target
based on a causal forest that estimates heterogeneous treatment effects and
then assigns students to treatment according to those estimated to have the
highest treatment effects. Next, we evaluate two alternative targeting
policies, one targeting students with low predicted probability of renewing
financial aid in the absence of the treatment, the other targeting those with
high probability. The predicted baseline outcome is not the ideal criterion for
targeting, nor is it a priori clear whether to prioritize low, high, or
intermediate predicted probability. Nonetheless, targeting on low baseline
outcomes is common in practice, for example because the relationship between
individual characteristics and treatment effects is often difficult or
impossible to estimate with historical data. We propose hybrid approaches that
incorporate the strengths of both predictive approaches (accurate estimation)
and causal approaches (correct criterion); we show that targeting intermediate
baseline outcomes is most effective in our specific application, while
targeting based on low baseline outcomes is detrimental. In one year of the
experiment, nudging all students improved early filing by an average of 6.4
percentage points over a baseline average of 37% filing, and we estimate that
targeting half of the students using our preferred policy attains around 75% of
this benefit.","['Susan Athey', 'Niall Keleher', 'Jann Spiess']","['econ.EM', 'cs.LG', 'stat.ME', 'stat.ML']",2023-10-12 19:08:45+00:00
http://arxiv.org/abs/2310.08649v1,Time-vectorized numerical integration for systems of ODEs,"Stiff systems of ordinary differential equations (ODEs) and sparse training
data are common in scientific problems. This paper describes efficient,
implicit, vectorized methods for integrating stiff systems of ordinary
differential equations through time and calculating parameter gradients with
the adjoint method. The main innovation is to vectorize the problem both over
the number of independent times series and over a batch or ""chunk"" of
sequential time steps, effectively vectorizing the assembly of the implicit
system of ODEs. The block-bidiagonal structure of the linearized implicit
system for the backward Euler method allows for further vectorization using
parallel cyclic reduction (PCR). Vectorizing over both axes of the input data
provides a higher bandwidth of calculations to the computing device, allowing
even problems with comparatively sparse data to fully utilize modern GPUs and
achieving speed ups of greater than 100x, compared to standard, sequential time
integration. We demonstrate the advantages of implicit, vectorized time
integration with several example problems, drawn from both analytical stiff and
non-stiff ODE models as well as neural ODE models. We also describe and provide
a freely available open-source implementation of the methods developed here.","['Mark C. Messner', 'Tianchen Hu', 'Tianju Chen']","['math.NA', 'cs.LG', 'cs.NA', 'stat.ML']",2023-10-12 18:21:02+00:00
http://arxiv.org/abs/2310.08576v1,Learning to Act from Actionless Videos through Dense Correspondences,"In this work, we present an approach to construct a video-based robot policy
capable of reliably executing diverse tasks across different robots and
environments from few video demonstrations without using any action
annotations. Our method leverages images as a task-agnostic representation,
encoding both the state and action information, and text as a general
representation for specifying robot goals. By synthesizing videos that
``hallucinate'' robot executing actions and in combination with dense
correspondences between frames, our approach can infer the closed-formed action
to execute to an environment without the need of any explicit action labels.
This unique capability allows us to train the policy solely based on RGB videos
and deploy learned policies to various robotic tasks. We demonstrate the
efficacy of our approach in learning policies on table-top manipulation and
navigation tasks. Additionally, we contribute an open-source framework for
efficient video modeling, enabling the training of high-fidelity policy models
with four GPUs within a single day.","['Po-Chen Ko', 'Jiayuan Mao', 'Yilun Du', 'Shao-Hua Sun', 'Joshua B. Tenenbaum']","['cs.RO', 'cs.CV', 'cs.LG', 'stat.ML']",2023-10-12 17:59:23+00:00
http://arxiv.org/abs/2310.08566v2,Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining,"Large transformer models pretrained on offline reinforcement learning
datasets have demonstrated remarkable in-context reinforcement learning (ICRL)
capabilities, where they can make good decisions when prompted with interaction
trajectories from unseen environments. However, when and how transformers can
be trained to perform ICRL have not been theoretically well-understood. In
particular, it is unclear which reinforcement-learning algorithms transformers
can perform in context, and how distribution mismatch in offline training data
affects the learned algorithms. This paper provides a theoretical framework
that analyzes supervised pretraining for ICRL. This includes two recently
proposed training methods -- algorithm distillation and decision-pretrained
transformers. First, assuming model realizability, we prove the
supervised-pretrained transformer will imitate the conditional expectation of
the expert algorithm given the observed trajectory. The generalization error
will scale with model capacity and a distribution divergence factor between the
expert and offline algorithms. Second, we show transformers with ReLU attention
can efficiently approximate near-optimal online reinforcement learning
algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and
UCB-VI for tabular Markov decision processes. This provides the first
quantitative analysis of the ICRL capabilities of transformers pretrained from
offline trajectories.","['Licong Lin', 'Yu Bai', 'Song Mei']","['cs.LG', 'cs.AI', 'cs.CL', 'math.ST', 'stat.ML', 'stat.TH']",2023-10-12 17:55:02+00:00
http://arxiv.org/abs/2310.08495v1,Characterizing climate pathways using feature importance on echo state networks,"The 2022 National Defense Strategy of the United States listed climate change
as a serious threat to national security. Climate intervention methods, such as
stratospheric aerosol injection, have been proposed as mitigation strategies,
but the downstream effects of such actions on a complex climate system are not
well understood. The development of algorithmic techniques for quantifying
relationships between source and impact variables related to a climate event
(i.e., a climate pathway) would help inform policy decisions. Data-driven deep
learning models have become powerful tools for modeling highly nonlinear
relationships and may provide a route to characterize climate variable
relationships. In this paper, we explore the use of an echo state network (ESN)
for characterizing climate pathways. ESNs are a computationally efficient
neural network variation designed for temporal data, and recent work proposes
ESNs as a useful tool for forecasting spatio-temporal climate data. Like other
neural networks, ESNs are non-interpretable black-box models, which poses a
hurdle for understanding variable relationships. We address this issue by
developing feature importance methods for ESNs in the context of
spatio-temporal data to quantify variable relationships captured by the model.
We conduct a simulation study to assess and compare the feature importance
techniques, and we demonstrate the approach on reanalysis climate data. In the
climate application, we select a time period that includes the 1991 volcanic
eruption of Mount Pinatubo. This event was a significant stratospheric aerosol
injection, which we use as a proxy for an artificial stratospheric aerosol
injection. Using the proposed approach, we are able to characterize
relationships between pathway variables associated with this event.","['Katherine Goode', 'Daniel Ries', 'Kellie McClernon']","['stat.ML', 'cs.LG', 'stat.AP']",2023-10-12 16:55:04+00:00
http://arxiv.org/abs/2310.08479v1,Personalised dynamic super learning: an application in predicting hemodiafiltration's convection volumes,"Obtaining continuously updated predictions is a major challenge for
personalised medicine. Leveraging combinations of parametric regressions and
machine learning approaches, the personalised online super learner (POSL) can
achieve such dynamic and personalised predictions. We adapt POSL to predict a
repeated continuous outcome dynamically and propose a new way to validate such
personalised or dynamic prediction models. We illustrate its performance by
predicting the convection volume of patients undergoing hemodiafiltration. POSL
outperformed its candidate learners with respect to median absolute error,
calibration-in-the-large, discrimination, and net benefit. We finally discuss
the choices and challenges underlying the use of POSL.","['Arthur Chatton', 'Michèle Bally', 'Renée Lévesque', 'Ivana Malenica', 'Robert W. Platt', 'Mireille E. Schnitzer']","['stat.ME', 'stat.AP', 'stat.ML']",2023-10-12 16:36:37+00:00
http://arxiv.org/abs/2310.08426v1,Extensions of Heterogeneity in Integration and Prediction (HIP) with R Shiny Application,"Multiple data views measured on the same set of participants is becoming more
common and has the potential to deepen our understanding of many complex
diseases by analyzing these different views simultaneously. Equally important,
many of these complex diseases show evidence of subgroup heterogeneity (e.g.,
by sex or race). HIP (Heterogeneity in Integration and Prediction) is among the
first methods proposed to integrate multiple data views while also accounting
for subgroup heterogeneity to identify common and subgroup-specific markers of
a particular disease. However, HIP is applicable to continuous outcomes and
requires programming expertise by the user. Here we propose extensions to HIP
that accommodate multi-class, Poisson, and Zero-Inflated Poisson outcomes while
retaining the benefits of HIP. Additionally, we introduce an R Shiny
application, accessible on shinyapps.io at
https://multi-viewlearn.shinyapps.io/HIP_ShinyApp/, that provides an interface
with the Python implementation of HIP to allow more researchers to use the
method anywhere and on any device. We applied HIP to identify genes and
proteins common and specific to males and females that are associated with
exacerbation frequency. Although some of the identified genes and proteins show
evidence of a relationship with chronic obstructive pulmonary disease (COPD) in
existing literature, others may be candidates for future research investigating
their relationship with COPD. We demonstrate the use of the Shiny application
with a publicly available data. An R-package for HIP would be made available at
https://github.com/lasandrall/HIP.","['J. Butts', 'C. Wendt', 'R. Bowler', 'C. P. Hersh', 'Q. Long', 'L. Eberly', 'S. E. Safo']","['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']",2023-10-12 15:49:10+00:00
http://arxiv.org/abs/2310.08425v1,Differentially Private Non-convex Learning for Multi-layer Neural Networks,"This paper focuses on the problem of Differentially Private Stochastic
Optimization for (multi-layer) fully connected neural networks with a single
output node. In the first part, we examine cases with no hidden nodes,
specifically focusing on Generalized Linear Models (GLMs). We investigate the
well-specific model where the random noise possesses a zero mean, and the link
function is both bounded and Lipschitz continuous. We propose several
algorithms and our analysis demonstrates the feasibility of achieving an excess
population risk that remains invariant to the data dimension. We also delve
into the scenario involving the ReLU link function, and our findings mirror
those of the bounded link function. We conclude this section by contrasting
well-specified and misspecified models, using ReLU regression as a
representative example.
  In the second part of the paper, we extend our ideas to two-layer neural
networks with sigmoid or ReLU activation functions in the well-specified model.
In the third part, we study the theoretical guarantees of DP-SGD in Abadi et
al. (2016) for fully connected multi-layer neural networks. By utilizing recent
advances in Neural Tangent Kernel theory, we provide the first excess
population risk when both the sample size and the width of the network are
sufficiently large. Additionally, we discuss the role of some parameters in
DP-SGD regarding their utility, both theoretically and empirically.","['Hanpu Shen', 'Cheng-Long Wang', 'Zihang Xiang', 'Yiming Ying', 'Di Wang']","['cs.LG', 'cs.CR', 'stat.ML']",2023-10-12 15:48:14+00:00
http://arxiv.org/abs/2310.08410v1,Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis,"Large language models such as ChatGPT are increasingly explored in medical
domains. However, the absence of standard guidelines for performance evaluation
has led to methodological inconsistencies. This study aims to summarize the
available evidence on evaluating ChatGPT's performance in medicine and provide
direction for future research. We searched ten medical literature databases on
June 15, 2023, using the keyword ""ChatGPT"". A total of 3520 articles were
identified, of which 60 were reviewed and summarized in this paper and 17 were
included in the meta-analysis. The analysis showed that ChatGPT displayed an
overall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing
medical queries. However, the studies varied in question resource,
question-asking process, and evaluation metrics. Moreover, many studies failed
to report methodological details, including the version of ChatGPT and whether
each question was used independently or repeatedly. Our findings revealed that
although ChatGPT demonstrated considerable potential for application in
healthcare, the heterogeneity of the studies and insufficient reporting may
affect the reliability of these results. Further well-designed studies with
comprehensive and transparent reporting are needed to evaluate ChatGPT's
performance in medicine.","['Qiuhong Wei', 'Zhengxiong Yao', 'Ying Cui', 'Bo Wei', 'Zhezhen Jin', 'Ximing Xu']","['stat.ME', 'stat.ML']",2023-10-12 15:26:26+00:00
http://arxiv.org/abs/2310.08391v2,How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?,"Transformers pretrained on diverse tasks exhibit remarkable in-context
learning (ICL) capabilities, enabling them to solve unseen tasks solely based
on input contexts without adjusting model parameters. In this paper, we study
ICL in one of its simplest setups: pretraining a linearly parameterized
single-layer linear attention model for linear regression with a Gaussian
prior. We establish a statistical task complexity bound for the attention model
pretraining, showing that effective pretraining only requires a small number of
independent tasks. Furthermore, we prove that the pretrained model closely
matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by
achieving nearly Bayes optimal risk on unseen tasks under a fixed context
length. These theoretical findings complement prior experimental research and
shed light on the statistical foundations of ICL.","['Jingfeng Wu', 'Difan Zou', 'Zixiang Chen', 'Vladimir Braverman', 'Quanquan Gu', 'Peter L. Bartlett']","['stat.ML', 'cs.LG']",2023-10-12 15:01:43+00:00
http://arxiv.org/abs/2310.08337v3,Neural Diffusion Models,"Diffusion models have shown remarkable performance on many generative tasks.
Despite recent success, most diffusion models are restricted in that they only
allow linear transformation of the data distribution. In contrast, broader
family of transformations can potentially help train generative distributions
more efficiently, simplifying the reverse process and closing the gap between
the true negative log-likelihood and the variational approximation. In this
paper, we present Neural Diffusion Models (NDMs), a generalization of
conventional diffusion models that enables defining and learning time-dependent
non-linear transformations of data. We show how to optimise NDMs using a
variational bound in a simulation-free setting. Moreover, we derive a
time-continuous formulation of NDMs, which allows fast and reliable inference
using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the
utility of NDMs with learnable transformations through experiments on standard
image generation benchmarks, including CIFAR-10, downsampled versions of
ImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms
of likelihood and produce high-quality samples.","['Grigory Bartosh', 'Dmitry Vetrov', 'Christian A. Naesseth']","['cs.LG', 'stat.ML']",2023-10-12 13:54:55+00:00
http://arxiv.org/abs/2310.08331v2,Dealing with uncertainty: balancing exploration and exploitation in deep recurrent reinforcement learning,"Incomplete knowledge of the environment leads an agent to make decisions
under uncertainty. One of the major dilemmas in Reinforcement Learning (RL)
where an autonomous agent has to balance two contrasting needs in making its
decisions is: exploiting the current knowledge of the environment to maximize
the cumulative reward as well as exploring actions that allow improving the
knowledge of the environment, hopefully leading to higher reward values
(exploration-exploitation trade-off). Concurrently, another relevant issue
regards the full observability of the states, which may not be assumed in all
applications. For instance, when 2D images are considered as input in an RL
approach used for finding the best actions within a 3D simulation environment.
In this work, we address these issues by deploying and testing several
techniques to balance exploration and exploitation trade-off on partially
observable systems for predicting steering wheels in autonomous driving
scenarios. More precisely, the final aim is to investigate the effects of using
both adaptive and deterministic exploration strategies coupled with a Deep
Recurrent Q-Network. Additionally, we adapted and evaluated the impact of a
modified quadratic loss function to improve the learning phase of the
underlying Convolutional Recurrent Neural Network. We show that adaptive
methods better approximate the trade-off between exploration and exploitation
and, in general, Softmax and Max-Boltzmann strategies outperform epsilon-greedy
techniques.","['Valentina Zangirolami', 'Matteo Borrotti']","['stat.ML', 'cs.LG']",2023-10-12 13:45:33+00:00
http://arxiv.org/abs/2310.08287v1,A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors,"The distribution of the weights of modern deep neural networks (DNNs) -
crucial for uncertainty quantification and robustness - is an eminently complex
object due to its extremely high dimensionality. This paper proposes one of the
first large-scale explorations of the posterior distribution of deep Bayesian
Neural Networks (BNNs), expanding its study to real-world vision tasks and
architectures. Specifically, we investigate the optimal approach for
approximating the posterior, analyze the connection between posterior quality
and uncertainty quantification, delve into the impact of modes on the
posterior, and explore methods for visualizing the posterior. Moreover, we
uncover weight-space symmetries as a critical aspect for understanding the
posterior. To this extent, we develop an in-depth assessment of the impact of
both permutation and scaling symmetries that tend to obfuscate the Bayesian
posterior. While the first type of transformation is known for duplicating
modes, we explore the relationship between the latter and L2 regularization,
challenging previous misconceptions. Finally, to help the community improve our
understanding of the Bayesian posterior, we will shortly release the first
large-scale checkpoint dataset, including thousands of real-world models and
our codes.","['Olivier Laurent', 'Emanuel Aldea', 'Gianni Franchi']","['stat.ML', 'cs.LG']",2023-10-12 12:45:13+00:00
http://arxiv.org/abs/2310.08237v2,Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift,"Covariate shift occurs prevalently in practice, where the input distributions
of the source and target data are substantially different. Despite its
practical importance in various learning problems, most of the existing methods
only focus on some specific learning tasks and are not well validated
theoretically and numerically. To tackle this problem, we propose a unified
analysis of general nonparametric methods in a reproducing kernel Hilbert space
(RKHS) under covariate shift. Our theoretical results are established for a
general loss belonging to a rich loss function family, which includes many
commonly used methods as special cases, such as mean regression, quantile
regression, likelihood-based classification, and margin-based classification.
Two types of covariate shift problems are the focus of this paper and the sharp
convergence rates are established for a general loss function to provide a
unified theoretical analysis, which concurs with the optimal results in
literature where the squared loss is used. Extensive numerical studies on
synthetic and real examples confirm our theoretical findings and further
illustrate the effectiveness of our proposed method.","['Xingdong Feng', 'Xin He', 'Caixing Wang', 'Chao Wang', 'Jingnan Zhang']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2023-10-12 11:33:15+00:00
http://arxiv.org/abs/2310.08209v1,Conformal inference for regression on Riemannian Manifolds,"Regression on manifolds, and, more broadly, statistics on manifolds, has
garnered significant importance in recent years due to the vast number of
applications for this type of data. Circular data is a classic example, but so
is data in the space of covariance matrices, data on the Grassmannian manifold
obtained as a result of principal component analysis, among many others. In
this work we investigate prediction sets for regression scenarios when the
response variable, denoted by $Y$, resides in a manifold, and the covariable,
denoted by X, lies in Euclidean space. This extends the concepts delineated in
[Lei and Wasserman, 2014] to this novel context. Aligning with traditional
principles in conformal inference, these prediction sets are distribution-free,
indicating that no specific assumptions are imposed on the joint distribution
of $(X, Y)$, and they maintain a non-parametric character. We prove the
asymptotic almost sure convergence of the empirical version of these regions on
the manifold to their population counterparts. The efficiency of this method is
shown through a comprehensive simulation study and an analysis involving
real-world data.","['Alejandro Cholaquidis', 'Fabrice Gamboa', 'Leonardo Moreno']","['stat.ML', 'cs.LG']",2023-10-12 10:56:25+00:00
http://arxiv.org/abs/2310.08150v1,On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks,"Maximum-type statistics of certain functions of the sample covariance matrix
of high-dimensional vector time series are studied to statistically confirm or
reject the null hypothesis that a data set has been collected under normal
conditions. The approach generalizes the case of the maximal deviation of the
sample autocovariances function from its assumed values. Within a linear time
series framework it is shown that Gumbel-type extreme value asymptotics holds
true. As applications we discuss long-only mimimal-variance portfolio
optimization and subportfolio analysis with respect to idiosyncratic risks, ETF
index tracking by sparse tracking portfolios, convolutional deep learners for
image analysis and the analysis of array-of-sensors data.",['Ansgar Steland'],"['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2023-10-12 09:17:46+00:00
http://arxiv.org/abs/2310.08115v1,Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects,"Many causal estimands are only partially identifiable since they depend on
the unobservable joint distribution between potential outcomes. Stratification
on pretreatment covariates can yield sharper partial identification bounds;
however, unless the covariates are discrete with relatively small support, this
approach typically requires consistent estimation of the conditional
distributions of the potential outcomes given the covariates. Thus, existing
approaches may fail under model misspecification or if consistency assumptions
are violated. In this study, we propose a unified and model-agnostic
inferential approach for a wide class of partially identified estimands, based
on duality theory for optimal transport problems. In randomized experiments,
our approach can wrap around any estimates of the conditional distributions and
provide uniformly valid inference, even if the initial estimates are
arbitrarily inaccurate. Also, our approach is doubly robust in observational
studies. Notably, this property allows analysts to use the multiplier bootstrap
to select covariates and models without sacrificing validity even if the true
model is not included. Furthermore, if the conditional distributions are
estimated at semiparametric rates, our approach matches the performance of an
oracle with perfect knowledge of the outcome model. Finally, we propose an
efficient computational framework, enabling implementation on many practical
problems in causal inference.","['Wenlong Ji', 'Lihua Lei', 'Asher Spector']","['econ.EM', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH', '62G15 (Primary), 62G05 (Secondary)', 'G.3; I.2.m']",2023-10-12 08:17:30+00:00
http://arxiv.org/abs/2310.08089v1,Learning Regularized Monotone Graphon Mean-Field Games,"This paper studies two fundamental problems in regularized Graphon Mean-Field
Games (GMFGs). First, we establish the existence of a Nash Equilibrium (NE) of
any $\lambda$-regularized GMFG (for $\lambda\geq 0$). This result relies on
weaker conditions than those in previous works for analyzing both unregularized
GMFGs ($\lambda=0$) and $\lambda$-regularized MFGs, which are special cases of
GMFGs. Second, we propose provably efficient algorithms to learn the NE in
weakly monotone GMFGs, motivated by Lasry and Lions [2007]. Previous literature
either only analyzed continuous-time algorithms or required extra conditions to
analyze discrete-time algorithms. In contrast, we design a discrete-time
algorithm and derive its convergence rate solely under weakly monotone
conditions. Furthermore, we develop and analyze the action-value function
estimation procedure during the online learning process, which is absent from
algorithms for monotone GMFGs. This serves as a sub-module in our optimization
algorithm. The efficiency of the designed algorithm is corroborated by
empirical evaluations.","['Fengzhuo Zhang', 'Vincent Y. F. Tan', 'Zhaoran Wang', 'Zhuoran Yang']","['cs.GT', 'cs.SY', 'eess.SY', 'stat.ML']",2023-10-12 07:34:13+00:00
http://arxiv.org/abs/2310.08055v2,Log-Gaussian Gamma Processes for Training Bayesian Neural Networks in Raman and CARS Spectroscopies,"We propose an approach utilizing gamma-distributed random variables, coupled
with log-Gaussian modeling, to generate synthetic datasets suitable for
training neural networks. This addresses the challenge of limited real
observations in various applications. We apply this methodology to both Raman
and coherent anti-Stokes Raman scattering (CARS) spectra, using experimental
spectra to estimate gamma process parameters. Parameter estimation is performed
using Markov chain Monte Carlo methods, yielding a full Bayesian posterior
distribution for the model which can be sampled for synthetic data generation.
Additionally, we model the additive and multiplicative background functions for
Raman and CARS with Gaussian processes. We train two Bayesian neural networks
to estimate parameters of the gamma process which can then be used to estimate
the underlying Raman spectrum and simultaneously provide uncertainty through
the estimation of parameters of a probability distribution. We apply the
trained Bayesian neural networks to experimental Raman spectra of
phthalocyanine blue, aniline black, naphthol red, and red 264 pigments and also
to experimental CARS spectra of adenosine phosphate, fructose, glucose, and
sucrose. The results agree with deterministic point estimates for the
underlying Raman and CARS spectral signatures.","['Teemu Härkönen', 'Erik M. Vartiainen', 'Lasse Lensu', 'Matthew T. Moores', 'Lassi Roininen']","['stat.AP', 'stat.ML', '62F15, 60G10, 62M45 (Primary) 78M31 (Secondary)']",2023-10-12 06:08:34+00:00
http://arxiv.org/abs/2310.08053v1,Lattice real-time simulations with learned optimal kernels,"We present a simulation strategy for the real-time dynamics of quantum
fields, inspired by reinforcement learning. It builds on the complex Langevin
approach, which it amends with system specific prior information, a necessary
prerequisite to overcome this exceptionally severe sign problem. The
optimization process underlying our machine learning approach is made possible
by deploying inherently stable solvers of the complex Langevin stochastic
process and a novel optimality criterion derived from insight into so-called
boundary terms. This conceptual and technical progress allows us to both
significantly extend the range of real-time simulations in 1+1d scalar field
theory beyond the state-of-the-art and to avoid discretization artifacts that
plagued previous real-time field theory simulations. Limitations of and
promising future directions are discussed.","['Daniel Alvestad', 'Alexander Rothkopf', 'Dénes Sexty']","['hep-lat', 'cond-mat.other', 'hep-th', 'nucl-th', 'stat.ML']",2023-10-12 06:01:01+00:00
http://arxiv.org/abs/2310.08031v2,Local Graph Clustering with Noisy Labels,"The growing interest in machine learning problems over graphs with additional
node information such as texts, images, or labels has popularized methods that
require the costly operation of processing the entire graph. Yet, little effort
has been made to the development of fast local methods (i.e. without accessing
the entire graph) that extract useful information from such data. To that end,
we propose a study of local graph clustering using noisy node labels as a proxy
for additional node information. In this setting, nodes receive initial binary
labels based on cluster affiliation: 1 if they belong to the target cluster and
0 otherwise. Subsequently, a fraction of these labels is flipped. We
investigate the benefits of incorporating noisy labels for local graph
clustering. By constructing a weighted graph with such labels, we study the
performance of graph diffusion-based local clustering method on both the
original and the weighted graphs. From a theoretical perspective, we consider
recovering an unknown target cluster with a single seed node in a random graph
with independent noisy node labels. We provide sufficient conditions on the
label noise under which, with high probability, using diffusion in the weighted
graph yields a more accurate recovery of the target cluster. This approach
proves more effective than using the given labels alone or using diffusion in
the label-free original graph. Empirically, we show that reliable node labels
can be obtained with just a few samples from an attributed graph. Moreover,
utilizing these labels via diffusion in the weighted graph leads to
significantly better local clustering performance across several real-world
datasets, improving F1 scores by up to 13%.","['Artur Back de Luca', 'Kimon Fountoulakis', 'Shenghao Yang']","['cs.LG', 'cs.SI', 'stat.ML']",2023-10-12 04:37:15+00:00
http://arxiv.org/abs/2310.08019v1,Robust 1-bit Compressed Sensing with Iterative Hard Thresholding,"In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector
$x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of
linear measurements that are quantized to just their signs, i.e., from
measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this
paper, we study a noisy version where a fraction of the measurements can be
flipped, potentially by an adversary. In particular, we analyze the Binary
Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a
properly defined loss function used for 1-bit compressed sensing, in this noisy
setting. It is known from recent results that, with
$\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an
estimate within $\epsilon$ error. This result is optimal and universal, meaning
one set of measurements work for all sparse vectors. In this paper, we show
that BIHT also provides better results than all known methods for the noisy
setting. We show that when up to $\tau$-fraction of the sign measurements are
incorrect (adversarial error), with the same number of measurements as before,
BIHT agnostically provides an estimate of $x$ within an
$\tilde{O}(\epsilon+\tau)$ error, maintaining the universality of measurements.
This establishes stability of iterative hard thresholding in the presence of
measurement error. To obtain the result, we use the restricted approximate
invertibility of Gaussian matrices, as well as a tight analysis of the
high-dimensional geometry of the adversarially corrupted measurements.","['Namiko Matsumoto', 'Arya Mazumdar']","['cs.IT', 'cs.DS', 'cs.LG', 'eess.SP', 'math.IT', 'stat.ML']",2023-10-12 03:41:32+00:00
http://arxiv.org/abs/2310.07999v1,LEMON: Lossless model expansion,"Scaling of deep neural networks, especially Transformers, is pivotal for
their surging performance and has further led to the emergence of sophisticated
reasoning capabilities in foundation models. Such scaling generally requires
training large models from scratch with random initialization, failing to
leverage the knowledge acquired by their smaller counterparts, which are
already resource-intensive to obtain. To tackle this inefficiency, we present
$\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a
recipe to initialize scaled models using the weights of their smaller but
pre-trained counterparts. This is followed by model training with an optimized
learning rate scheduler tailored explicitly for the scaled models,
substantially reducing the training time compared to training from scratch.
Notably, LEMON is versatile, ensuring compatibility with various network
structures, including models like Vision Transformers and BERT. Our empirical
results demonstrate that LEMON reduces computational costs by 56.7% for Vision
Transformers and 33.2% for BERT when compared to training from scratch.","['Yite Wang', 'Jiahao Su', 'Hanlin Lu', 'Cong Xie', 'Tianyi Liu', 'Jianbo Yuan', 'Haibin Lin', 'Ruoyu Sun', 'Hongxia Yang']","['cs.LG', 'stat.ML']",2023-10-12 03:02:41+00:00
http://arxiv.org/abs/2310.07983v2,Revisiting Decentralized ProxSkip: Achieving Linear Speedup,"The ProxSkip algorithm for decentralized and federated learning is gaining
increasing attention due to its proven benefits in accelerating communication
complexity while maintaining robustness against data heterogeneity. However,
existing analyses of ProxSkip are limited to the strongly convex setting and do
not achieve linear speedup, where convergence performance increases linearly
with respect to the number of nodes. So far, questions remain open about how
ProxSkip behaves in the non-convex setting and whether linear speedup is
achievable.
  In this paper, we revisit decentralized ProxSkip and address both questions.
We demonstrate that the leading communication complexity of ProxSkip is
$\mathcal{O}\left(\frac{p\sigma^2}{n\epsilon^2}\right)$ for non-convex and
convex settings, and $\mathcal{O}\left(\frac{p\sigma^2}{n\epsilon}\right)$ for
the strongly convex setting, where $n$ represents the number of nodes, $p$
denotes the probability of communication, $\sigma^2$ signifies the level of
stochastic noise, and $\epsilon$ denotes the desired accuracy level. This
result illustrates that ProxSkip achieves linear speedup and can asymptotically
reduce communication overhead proportional to the probability of communication.
Additionally, for the strongly convex setting, we further prove that ProxSkip
can achieve linear speedup with network-independent stepsizes.","['Luyao Guo', 'Sulaiman A. Alghunaim', 'Kun Yuan', 'Laurent Condat', 'Jinde Cao']","['cs.LG', 'math.OC', 'stat.ML']",2023-10-12 02:13:48+00:00
http://arxiv.org/abs/2310.07973v2,Statistical Performance Guarantee for Subgroup Identification with Generic Machine Learning,"Across a wide array of disciplines, many researchers use machine learning
(ML) algorithms to identify a subgroup of individuals who are likely to benefit
from a treatment the most (``exceptional responders'') or those who are harmed
by it. A common approach to this subgroup identification problem consists of
two steps. First, researchers estimate the conditional average treatment effect
(CATE) using an ML algorithm. Next, they use the estimated CATE to select those
individuals who are predicted to be most affected by the treatment, either
positively or negatively. Unfortunately, CATE estimates are often biased and
noisy. In addition, utilizing the same data to both identify a subgroup and
estimate its group average treatment effect results in a multiple testing
problem. To address these challenges, we develop uniform confidence bands for
estimation of the group average treatment effect sorted by generic ML algorithm
(GATES). Using these uniform confidence bands, researchers can identify, with a
statistical guarantee, a subgroup whose GATES exceeds a certain effect size,
regardless of how this effect size is chosen. The validity of the proposed
methodology depends solely on randomization of treatment and random sampling of
units. Importantly, our method does not require modeling assumptions and avoids
a computationally intensive resampling procedure. A simulation study shows that
the proposed uniform confidence bands are reasonably informative and have an
appropriate empirical coverage even when the sample size is as small as 100. We
analyze a clinical trial of late-stage prostate cancer and find a relatively
large proportion of exceptional responders.","['Michael Lingzhi Li', 'Kosuke Imai']","['stat.ME', 'math.OC', 'stat.AP', 'stat.ML']",2023-10-12 01:41:47+00:00
http://arxiv.org/abs/2310.07970v1,Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach,"Surrogate Optimization (SO) algorithms have shown promise for optimizing
expensive black-box functions. However, their performance is heavily influenced
by hyperparameters related to sampling and surrogate fitting, which poses a
challenge to their widespread adoption. We investigate the impact of
hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive
Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm,
but a generic self-adjusting SO algorithm that dynamically tunes its own
hyperparameters while concurrently optimizing the primary objective function,
without requiring additional evaluations. The aim is to improve the
accessibility, effectiveness, and convergence speed of SO algorithms for
practitioners. Our approach identifies and modifies the most influential
hyperparameters specific to each problem and SO approach, reducing the need for
manual tuning without significantly increasing the computational burden.
Experimental results demonstrate the effectiveness of HASSO in enhancing the
performance of various SO algorithms across different global optimization test
problems.","['Nazanin Nezami', 'Hadis Anahideh']","['cs.LG', 'math.OC', 'math.PR', 'stat.ML']",2023-10-12 01:26:05+00:00
http://arxiv.org/abs/2310.07918v4,Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning,"Interpretable policy learning seeks to estimate intelligible decision
policies from observed actions; however, existing models force a tradeoff
between accuracy and interpretability, limiting data-driven interpretations of
human decision-making processes. Fundamentally, existing approaches are
burdened by this tradeoff because they represent the underlying decision
process as a universal policy, when in fact human decisions are dynamic and can
change drastically under different contexts. Thus, we develop Contextualized
Policy Recovery (CPR), which re-frames the problem of modeling complex decision
processes as a multi-task learning problem, where each context poses a unique
task and complex decision policies can be constructed piece-wise from many
simple context-specific policies. CPR models each context-specific policy as a
linear map, and generates new policy models $\textit{on-demand}$ as contexts
are updated with new observations. We provide two flavors of the CPR framework:
one focusing on exact local interpretability, and one retaining full global
interpretability. We assess CPR through studies on simulated and real data,
achieving state-of-the-art performance on predicting antibiotic prescription in
intensive care units ($+22\%$ AUROC vs. previous SOTA) and predicting MRI
prescription for Alzheimer's patients ($+7.7\%$ AUROC vs. previous SOTA). With
this improvement, CPR closes the accuracy gap between interpretable and
black-box methods, allowing high-resolution exploration and analysis of
context-specific decision models.","['Jannik Deuschel', 'Caleb N. Ellington', 'Yingtao Luo', 'Benjamin J. Lengerich', 'Pascal Friederich', 'Eric P. Xing']","['cs.LG', 'cs.AI', 'stat.ML']",2023-10-11 22:17:37+00:00
http://arxiv.org/abs/2310.07894v1,Efficient Integrators for Diffusion Generative Models,"Diffusion models suffer from slow sample generation at inference time.
Therefore, developing a principled framework for fast deterministic/stochastic
sampling for a broader class of diffusion models is a promising direction. We
propose two complementary frameworks for accelerating sample generation in
pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate
integrators generalize DDIM, mapping the reverse diffusion dynamics to a more
amenable space for sampling. In contrast, splitting-based integrators, commonly
used in molecular dynamics, reduce the numerical simulation error by cleverly
alternating between numerical updates involving the data and auxiliary
variables. After extensively studying these methods empirically and
theoretically, we present a hybrid method that leads to the best-reported
performance for diffusion models in augmented spaces. Applied to Phase Space
Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and
stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network
function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing
baselines, respectively. Our code and model checkpoints will be made publicly
available at \url{https://github.com/mandt-lab/PSLD}.","['Kushagra Pandey', 'Maja Rudolph', 'Stephan Mandt']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2023-10-11 21:04:42+00:00
http://arxiv.org/abs/2310.07891v3,A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks,"Feature learning is thought to be one of the fundamental reasons for the
success of deep neural networks. It is rigorously known that in two-layer
fully-connected neural networks under certain conditions, one step of gradient
descent on the first layer can lead to feature learning; characterized by the
appearance of a separated rank-one component -- spike -- in the spectrum of the
feature matrix. However, with a constant gradient descent step size, this spike
only carries information from the linear component of the target function and
therefore learning non-linear components is impossible. We show that with a
learning rate that grows with the sample size, such training in fact introduces
multiple rank-one components, each corresponding to a specific polynomial
feature. We further prove that the limiting large-dimensional and large sample
training and test errors of the updated neural networks are fully characterized
by these spikes. By precisely analyzing the improvement in the training and
test errors, we demonstrate that these non-linear features can enhance
learning.","['Behrad Moniri', 'Donghwan Lee', 'Hamed Hassani', 'Edgar Dobriban']","['stat.ML', 'cs.LG']",2023-10-11 20:55:02+00:00
http://arxiv.org/abs/2310.07852v5,On the Computational Complexity of Private High-dimensional Model Selection,"We consider the problem of model selection in a high-dimensional sparse
linear regression model under privacy constraints. We propose a differentially
private (DP) best subset selection method with strong statistical utility
properties by adopting the well-known exponential mechanism for selecting the
best model. To achieve computational expediency, we propose an efficient
Metropolis-Hastings algorithm and under certain regularity conditions, we
establish that it enjoys polynomial mixing time to its stationary distribution.
As a result, we also establish both approximate differential privacy and
statistical utility for the estimates of the mixed Metropolis-Hastings chain.
Finally, we perform some illustrative experiments on simulated data showing
that our algorithm can quickly identify active features under reasonable
privacy budget constraints.","['Saptarshi Roy', 'Zehua Wang', 'Ambuj Tewari']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2023-10-11 19:53:15+00:00
http://arxiv.org/abs/2310.07838v4,Towards the Fundamental Limits of Knowledge Transfer over Finite Domains,"We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.","['Qingyue Zhao', 'Banghua Zhu']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2023-10-11 19:30:08+00:00
http://arxiv.org/abs/2310.07831v2,Optimal Linear Decay Learning Rate Schedules and Further Refinements,"Learning rate schedules used in practice bear little resemblance to those
recommended by theory. We close much of this theory/practice gap, and as a
consequence are able to derive new problem-adaptive learning rate schedules.
Our main technical contribution is a refined analysis of learning rate
schedules for a wide class of optimization algorithms (including SGD). When
considering only worst-case analysis, our theory predicts that the optimal
choice is the linear decay schedule where the step-size is set proportional to
1 - t/T, where t is the current iteration and T is the total number of steps.
To go beyond this worst-case analysis, we use the observed gradient norms to
derive schedules refined for any particular task. These refined schedules
exhibit learning rate warm-up and rapid learning rate annealing near the end of
training. Ours is the first systematic approach to automatically yield both of
these properties. We perform the most comprehensive evaluation of learning rate
schedules to date, evaluating across 10 diverse deep learning problems, a
series of LLMs, and a suite of logistic regression problems. We validate that
overall, the linear-decay schedule outperforms all commonly used default
schedules including cosine annealing. Our adaptive schedule refinement method
gives further improvements.","['Aaron Defazio', 'Ashok Cutkosky', 'Harsh Mehta', 'Konstantin Mishchenko']","['cs.LG', 'cs.AI', 'stat.ML']",2023-10-11 19:16:35+00:00
http://arxiv.org/abs/2310.07811v2,Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore,"We consider online reinforcement learning (RL) in episodic Markov decision
processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is
assumed that the action-values of all policies can be expressed as linear
functions of state-action features. This class is known to be more general than
linear MDPs, where the transition kernel and the reward function are assumed to
be linear functions of the feature vectors. As our first contribution, we show
that the difference between the two classes is the presence of states in
linearly $q^\pi$-realizable MDPs where for any policy, all the actions have
approximately equal values, and skipping over these states by following an
arbitrarily fixed policy in those states transforms the problem to a linear
MDP. Based on this observation, we derive a novel (computationally inefficient)
learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously
learns what states should be skipped over and runs another learning algorithm
on the linear MDP hidden in the problem. The method returns an
$\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactions
with the MDP, where $H$ is the time horizon and $d$ is the dimension of the
feature vectors, giving the first polynomial-sample-complexity online RL
algorithm for this setting. The results are proved for the misspecified case,
where the sample complexity is shown to degrade gracefully with the
misspecification error.","['Gellért Weisz', 'András György', 'Csaba Szepesvári']","['cs.LG', 'stat.ML']",2023-10-11 18:50:25+00:00
