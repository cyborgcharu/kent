id,title,abstract,authors,categories,date
http://arxiv.org/abs/1912.03321v1,Robust Deep Graph Based Learning for Binary Classification,"Convolutional neural network (CNN)-based feature learning has become state of
the art, since given sufficient training data, CNN can significantly outperform
traditional methods for various classification tasks. However, feature learning
becomes more difficult if some training labels are noisy. With traditional
regularization techniques, CNN often overfits to the noisy training labels,
resulting in sub-par classification performance. In this paper, we propose a
robust binary classifier, based on CNNs, to learn deep metric functions, which
are then used to construct an optimal underlying graph structure used to clean
noisy labels via graph Laplacian regularization (GLR). GLR is posed as a convex
maximum a posteriori (MAP) problem solved via convex quadratic programming
(QP). To penalize samples around the decision boundary, we propose two
regularized loss functions for semi-supervised learning. The binary
classification experiments on three datasets, varying in number and type of
features, demonstrate that given a noisy training dataset, our proposed
networks outperform several state-of-the-art classifiers, including label-noise
robust support vector machine, CNNs with three different robust loss functions,
model-based GLR, and dynamic graph CNN classifiers.","['Minxiang Ye', 'Vladimir Stankovic', 'Lina Stankovic', 'Gene Cheung']","['cs.LG', 'stat.ML']",2019-12-06 19:11:52+00:00
http://arxiv.org/abs/1912.03280v1,Recent advances in deep learning applied to skin cancer detection,"Skin cancer is a major public health problem around the world. Its early
detection is very important to increase patient prognostics. However, the lack
of qualified professionals and medical instruments are significant issues in
this field. In this context, over the past few years, deep learning models
applied to automated skin cancer detection have become a trend. In this paper,
we present an overview of the recent advances reported in this field as well as
a discussion about the challenges and opportunities for improvement in the
current models. In addition, we also present some important aspects regarding
the use of these models in smartphones and indicate future directions we
believe the field will take.","['Andre G. C. Pacheco', 'Renato A. Krohling']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2019-12-06 18:23:30+00:00
http://arxiv.org/abs/1912.03277v3,Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers,"To construct interpretable explanations that are consistent with the original
ML model, counterfactual examples---showing how the model's output changes with
small perturbations to the input---have been proposed. This paper extends the
work in counterfactual explanations by addressing the challenge of feasibility
of such examples. For explanations of ML models in critical domains such as
healthcare and finance, counterfactual examples are useful for an end-user only
to the extent that perturbation of feature inputs is feasible in the real
world. We formulate the problem of feasibility as preserving causal
relationships among input features and present a method that uses (partial)
structural causal models to generate actionable counterfactuals. When
feasibility constraints cannot be easily expressed, we consider an alternative
mechanism where people can label generated CF examples on feasibility: whether
it is feasible to intervene and realize the candidate CF example from the
original input. To learn from this labelled feasibility data, we propose a
modified variational auto encoder loss for generating CF examples that
optimizes for feasibility as people interact with its output. Our experiments
on Bayesian networks and the widely used ''Adult-Income'' dataset show that our
proposed methods can generate counterfactual explanations that better satisfy
feasibility constraints than existing methods.. Code repository can be accessed
here: \textit{https://github.com/divyat09/cf-feasibility}","['Divyat Mahajan', 'Chenhao Tan', 'Amit Sharma']","['cs.LG', 'cs.AI', 'stat.ML']",2019-12-06 18:16:29+00:00
http://arxiv.org/abs/1912.03263v3,Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One,"We propose to reinterpret a standard discriminative classifier of p(y|x) as
an energy based model for the joint distribution p(x,y). In this setting, the
standard class probabilities can be easily computed as well as unnormalized
values of p(x) and p(x|y). Within this framework, standard discriminative
architectures may beused and the model can also be trained on unlabeled data.
We demonstrate that energy based training of the joint distribution improves
calibration, robustness, andout-of-distribution detection while also enabling
our models to generate samplesrivaling the quality of recent GAN approaches. We
improve upon recently proposed techniques for scaling up the training of energy
based models and presentan approach which adds little overhead compared to
standard classification training. Our approach is the first to achieve
performance rivaling the state-of-the-artin both generative and discriminative
learning within one hybrid model.","['Will Grathwohl', 'Kuan-Chieh Wang', 'JÃ¶rn-Henrik Jacobsen', 'David Duvenaud', 'Mohammad Norouzi', 'Kevin Swersky']","['cs.LG', 'cs.CV', 'stat.ML']",2019-12-06 18:00:36+00:00
http://arxiv.org/abs/1912.03250v2,Differentially Private Synthetic Mixed-Type Data Generation For Unsupervised Learning,"We introduce the DP-auto-GAN framework for synthetic data generation, which
combines the low dimensional representation of autoencoders with the
flexibility of Generative Adversarial Networks (GANs). This framework can be
used to take in raw sensitive data and privately train a model for generating
synthetic data that will satisfy similar statistical properties as the original
data. This learned model can generate an arbitrary amount of synthetic data,
which can then be freely shared due to the post-processing guarantee of
differential privacy. Our framework is applicable to unlabeled mixed-type data,
that may include binary, categorical, and real-valued data. We implement this
framework on both binary data (MIMIC-III) and mixed-type data (ADULT), and
compare its performance with existing private algorithms on metrics in
unsupervised settings. We also introduce a new quantitative metric able to
detect diversity, or lack thereof, of synthetic data.","['Uthaipon Tantipongpipat', 'Chris Waites', 'Digvijay Boob', 'Amaresh Ankit Siva', 'Rachel Cummings']","['cs.LG', 'cs.CR', 'stat.ML']",2019-12-06 17:46:07+00:00
http://arxiv.org/abs/1912.03249v2,Gaussian Process Priors for View-Aware Inference,"While frame-independent predictions with deep neural networks have become the
prominent solutions to many computer vision tasks, the potential benefits of
utilizing correlations between frames have received less attention. Even though
probabilistic machine learning provides the ability to encode correlation as
prior knowledge for inference, there is a tangible gap between the theory and
practice of applying probabilistic methods to modern vision problems. For this,
we derive a principled framework to combine information coupling between camera
poses (translation and orientation) with deep models. We proposed a novel view
kernel that generalizes the standard periodic kernel in $\mathrm{SO}(3)$. We
show how this soft-prior knowledge can aid several pose-related vision tasks
like novel view synthesis and predict arbitrary points in the latent space of
generative models, pointing towards a range of new applications for inter-frame
reasoning.","['Yuxin Hou', 'Ari Heljakka', 'Arno Solin']","['stat.ML', 'cs.CV', 'cs.LG']",2019-12-06 17:41:37+00:00
http://arxiv.org/abs/1912.04116v2,MRI correlates of chronic symptoms in mild traumatic brain injury,"Veterans with mild traumatic brain injury (mTBI) have reported auditory and
visual dysfunction that persists beyond the acute incident. The etiology behind
these symptoms is difficult to characterize with current clinical imaging.
These functional deficits may be caused by shear injury or micro-bleeds, which
can be detected with special imaging modalities. We explore these hypotheses in
a pilot study of multi-parametric MRI. We extract over 1,000 imaging and
clinical metrics and project them to a low-dimensional space, where we can
discriminate between healthy controls and patients with mTBI. We also show
correlations between the metric representations and patient symptoms.","['Cailey I. Kerley', 'Kurt G. Schilling', 'Justin Blaber', 'Beth Miller', 'Allen Newton', 'Adam W. Anderson', 'Bennett A. Landman', 'Tonia S. Rex']","['eess.IV', 'cs.LG', 'q-bio.NC', 'q-bio.QM', 'stat.ML']",2019-12-06 17:30:51+00:00
http://arxiv.org/abs/1912.03241v1,VALAN: Vision and Language Agent Navigation,"VALAN is a lightweight and scalable software framework for deep reinforcement
learning based on the SEED RL architecture. The framework facilitates the
development and evaluation of embodied agents for solving grounded language
understanding tasks, such as Vision-and-Language Navigation and
Vision-and-Dialog Navigation, in photo-realistic environments, such as
Matterport3D and Google StreetView. We have added a minimal set of abstractions
on top of SEED RL allowing us to generalize the architecture to solve a variety
of other RL problems. In this article, we will describe VALAN's software
abstraction and architecture, and also present an example of using VALAN to
design agents for instruction-conditioned indoor navigation.","['Larry Lansing', 'Vihan Jain', 'Harsh Mehta', 'Haoshuo Huang', 'Eugene Ie']","['cs.LG', 'stat.ML']",2019-12-06 17:29:43+00:00
http://arxiv.org/abs/1912.03234v1,What Do You Mean I'm Funny? Personalizing the Joke Skill of a Voice-Controlled Virtual Assistant,"A considerable part of the success experienced by Voice-controlled virtual
assistants (VVA) is due to the emotional and personalized experience they
deliver, with humor being a key component in providing an engaging interaction.
In this paper we describe methods used to improve the joke skill of a VVA
through personalization. The first method, based on traditional NLP techniques,
is robust and scalable. The others combine self-attentional network and
multi-task learning to obtain better results, at the cost of added complexity.
A significant challenge facing these systems is the lack of explicit user
feedback needed to provide labels for the models. Instead, we explore the use
of two implicit feedback-based labelling strategies. All models were evaluated
on real production data. Online results show that models trained on any of the
considered labels outperform a heuristic method, presenting a positive
real-world impact on user satisfaction. Offline results suggest that the
deep-learning approaches can improve the joke experience with respect to the
other considered methods.","['Alejandro Mottini', 'Amber Roy Chowdhury']","['cs.LG', 'cs.CL', 'stat.ML']",2019-12-06 17:17:39+00:00
http://arxiv.org/abs/1912.03221v2,Tree bark re-identification using a deep-learning feature descriptor,"The ability to visually re-identify objects is a fundamental capability in
vision systems. Oftentimes, it relies on collections of visual signatures based
on descriptors, such as SIFT or SURF. However, these traditional descriptors
were designed for a certain domain of surface appearances and geometries
(limited relief). Consequently, highly-textured surfaces such as tree bark pose
a challenge to them. In turn, this makes it more difficult to use trees as
identifiable landmarks for navigational purposes (robotics) or to track felled
lumber along a supply chain (logistics). We thus propose to use data-driven
descriptors trained on bark images for tree surface re-identification. To this
effect, we collected a large dataset containing 2,400 bark images with strong
illumination changes, annotated by surface and with the ability to pixel-align
them. We used this dataset to sample from more than 2 million 64x64 pixel
patches to train our novel local descriptors DeepBark and SqueezeBark. Our
DeepBark method has shown a clear advantage against the hand-crafted
descriptors SIFT and SURF. For instance, we demonstrated that DeepBark can
reach a mAP of 87.2% when retrieving 11 relevant bark images, i.e.
corresponding to the same physical surface, to a bark query against 7,900
images. Our work thus suggests that re-identifying tree surfaces in a
challenging illuminations context is possible. We also make public our dataset,
which can be used to benchmark surface re-identification techniques.","['Martin Robert', 'Patrick Dallaire', 'Philippe GiguÃ¨re']","['cs.LG', 'cs.CV', 'stat.ML']",2019-12-06 16:43:02+00:00
http://arxiv.org/abs/1912.03201v1,A Neural Spiking Approach Compared to Deep Feedforward Networks on Stepwise Pixel Erasement,"In real world scenarios, objects are often partially occluded. This requires
a robustness for object recognition against these perturbations. Convolutional
networks have shown good performances in classification tasks. The learned
convolutional filters seem similar to receptive fields of simple cells found in
the primary visual cortex. Alternatively, spiking neural networks are more
biological plausible. We developed a two layer spiking network, trained on
natural scenes with a biologically plausible learning rule. It is compared to
two deep convolutional neural networks using a classification task of stepwise
pixel erasement on MNIST. In comparison to these networks the spiking approach
achieves good accuracy and robustness.","['RenÃ© Larisch', 'Michael Teichmann', 'Fred H. Hamker']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2019-12-06 16:08:45+00:00
http://arxiv.org/abs/1912.04174v1,Deep Bayesian Recurrent Neural Networks for Somatic Variant Calling in Cancer,"The emerging field of precision oncology relies on the accurate pinpointing
of alterations in the molecular profile of a tumor to provide personalized
targeted treatments. Current methodologies in the field commonly include the
application of next generation sequencing technologies to a tumor sample,
followed by the identification of mutations in the DNA known as somatic
variants. The differentiation of these variants from sequencing error poses a
classic classification problem, which has traditionally been approached with
Bayesian statistics, and more recently with supervised machine learning methods
such as neural networks. Although these methods provide greater accuracy,
classic neural networks lack the ability to indicate the confidence of a
variant call. In this paper, we explore the performance of deep Bayesian neural
networks on next generation sequencing data, and their ability to give
probability estimates for somatic variant calls. In addition to demonstrating
similar performance in comparison to standard neural networks, we show that the
resultant output probabilities make these better suited to the disparate and
highly-variable sequencing data-sets these models are likely to encounter in
the real world. We aim to deliver algorithms to oncologists for which model
certainty better reflects accuracy, for improved clinical application. By
moving away from point estimates to reliable confidence intervals, we expect
the resultant clinical and treatment decisions to be more robust and more
informed by the underlying reality of the tumor molecular profile.","['Geoffroy Dubourg-Felonneau', 'Omar Darwish', 'Christopher Parsons', 'Dami Rebergen', 'John W Cassidy', 'Nirmesh Patel', 'Harry W Clifford']","['cs.LG', 'q-bio.GN', 'stat.ML']",2019-12-06 16:01:15+00:00
http://arxiv.org/abs/1912.03193v1,Risk-Averse Trust Region Optimization for Reward-Volatility Reduction,"In real-world decision-making problems, for instance in the fields of
finance, robotics or autonomous driving, keeping uncertainty under control is
as important as maximizing expected returns. Risk aversion has been addressed
in the reinforcement learning literature through risk measures related to the
variance of returns. However, in many cases, the risk is measured not only on a
long-term perspective, but also on the step-wise rewards (e.g., in trading, to
ensure the stability of the investment bank, it is essential to monitor the
risk of portfolio positions on a daily basis). In this paper, we define a novel
measure of risk, which we call reward volatility, consisting of the variance of
the rewards under the state-occupancy measure. We show that the reward
volatility bounds the return variance so that reducing the former also
constrains the latter. We derive a policy gradient theorem with a new objective
function that exploits the mean-volatility relationship, and develop an
actor-only algorithm. Furthermore, thanks to the linearity of the Bellman
equations defined under the new objective function, it is possible to adapt the
well-known policy gradient algorithms with monotonic improvement guarantees
such as TRPO in a risk-averse manner. Finally, we test the proposed approach in
two simulated financial environments.","['Lorenzo Bisi', 'Luca Sabbioni', 'Edoardo Vittori', 'Matteo Papini', 'Marcello Restelli']","['cs.LG', 'math.OC', 'stat.ML']",2019-12-06 15:57:06+00:00
http://arxiv.org/abs/1912.03192v2,Achieving Robustness in the Wild via Adversarial Mixing with Disentangled Representations,"Recent research has made the surprising finding that state-of-the-art deep
learning models sometimes fail to generalize to small variations of the input.
Adversarial training has been shown to be an effective approach to overcome
this problem. However, its application has been limited to enforcing invariance
to analytically defined transformations like $\ell_p$-norm bounded
perturbations. Such perturbations do not necessarily cover plausible real-world
variations that preserve the semantics of the input (such as a change in
lighting conditions). In this paper, we propose a novel approach to express and
formalize robustness to these kinds of real-world transformations of the input.
The two key ideas underlying our formulation are (1) leveraging disentangled
representations of the input to define different factors of variations, and (2)
generating new input images by adversarially composing the representations of
different images. We use a StyleGAN model to demonstrate the efficacy of this
framework. Specifically, we leverage the disentangled latent representations
computed by a StyleGAN model to generate perturbations of an image that are
similar to real-world variations (like adding make-up, or changing the
skin-tone of a person) and train models to be invariant to these perturbations.
Extensive experiments show that our method improves generalization and reduces
the effect of spurious correlations (reducing the error rate of a ""smile""
detector by 21% for example).","['Sven Gowal', 'Chongli Qin', 'Po-Sen Huang', 'Taylan Cemgil', 'Krishnamurthy Dvijotham', 'Timothy Mann', 'Pushmeet Kohli']","['cs.LG', 'cs.CV', 'stat.ML']",2019-12-06 15:56:53+00:00
http://arxiv.org/abs/1912.03154v1,Non-asymptotic error bounds for scaled underdamped Langevin MCMC,"Recent works have derived non-asymptotic upper bounds for convergence of
underdamped Langevin MCMC. We revisit these bound and consider introducing
scaling terms in the underlying underdamped Langevin equation. In particular,
we provide conditions under which an appropriate scaling allows to improve the
error bounds in terms of the condition number of the underlying density of
interest.",['Tim Zajic'],"['stat.ML', 'cs.LG']",2019-12-06 14:39:04+00:00
http://arxiv.org/abs/1912.03126v1,Knowledge extraction from the learning of sequences in a long short term memory (LSTM) architecture,"We introduce a general method to extract knowledge from a recurrent neural
network (Long Short Term Memory) that has learnt to detect if a given input
sequence is valid or not, according to an unknown generative automaton. Based
on the clustering of the hidden states, we explain how to build and validate an
automaton that corresponds to the underlying (unknown) automaton, and allows to
predict if a given sequence is valid or not. The method is illustrated on
artificial grammars (Reber's grammar variations) as well as on a real use-case
whose underlying grammar is unknown.","['Ikram Chraibi Kaadoud', 'Nicolas P. Rougier', 'FrÃ©dÃ©ric Alexandre']","['cs.LG', 'cs.NE', 'stat.ML']",2019-12-06 14:00:21+00:00
http://arxiv.org/abs/1912.05029v2,Continual egocentric object recognition,"We present a framework capable of tackilng the problem of continual object
recognition in a setting which resembles that under whichhumans see and learn.
This setting has a set of unique characteristics:it assumes an egocentric
point-of-view bound to the needs of a singleperson, which implies a relatively
low diversity of data and a coldstart with no data; it requires to operate in
an open world, where newobjects can be encounteredat any time; supervision is
scarce and hasto be solicited to the user, and completelyunsupervised
recognitionof new objects should be possible. Note that this setting differs
fromthe one addressed in the open world recognition literature, where
supervised feedback is always requested to be able to incorporate newobjects.
We propose a first solution to this problem in the form ofa memory-based
incremental framework that is capable of storinginformation of each and any
object it encounters, while using the supervision of the user to learn to
discriminate between known and unknown objects. Our approach is based on four
main features: the useof time and space persistence (i.e., the appearance of
objects changesrelatively slowly), the use of similarity as the main driving
principlefor object recognition and novelty detection, the progressive
introduction of new objects in a developmental fashion and the
selectiveelicitation of user feedback in an online active learning fashion.
Experimental results show the feasibility of open world, generic
objectrecognition, the ability to recognize, memorize and re-identify
newobjects even in complete absence of user supervision, and the utilityof
persistence and incrementality in boosting performance.","['Luca Erculiani', 'Fausto Giunchiglia', 'Andrea Passerini']","['cs.CV', 'cs.LG', 'stat.ML']",2019-12-06 12:10:59+00:00
http://arxiv.org/abs/1912.03074v1,Solving Bernoulli Rank-One Bandits with Unimodal Thompson Sampling,"Stochastic Rank-One Bandits (Katarya et al, (2017a,b)) are a simple framework
for regret minimization problems over rank-one matrices of arms. The initially
proposed algorithms are proved to have logarithmic regret, but do not match the
existing lower bound for this problem. We close this gap by first proving that
rank-one bandits are a particular instance of unimodal bandits, and then
providing a new analysis of Unimodal Thompson Sampling (UTS), initially
proposed by Paladino et al (2017). We prove an asymptotically optimal regret
bound on the frequentist regret of UTS and we support our claims with
simulations showing the significant improvement of our method compared to the
state-of-the-art.","['Cindy Trinh', 'Emilie Kaufmann', 'Claire Vernade', 'Richard Combes']","['stat.ML', 'cs.LG']",2019-12-06 11:53:45+00:00
http://arxiv.org/abs/1912.04067v1,Visualizing Deep Neural Networks for Speech Recognition with Learned Topographic Filter Maps,"The uninformative ordering of artificial neurons in Deep Neural Networks
complicates visualizing activations in deeper layers. This is one reason why
the internal structure of such models is very unintuitive. In neuroscience,
activity of real brains can be visualized by highlighting active regions.
Inspired by those techniques, we train a convolutional speech recognition
model, where filters are arranged in a 2D grid and neighboring filters are
similar to each other. We show, how those topographic filter maps visualize
artificial neuron activations more intuitively. Moreover, we investigate,
whether this causes phoneme-responsive neurons to be grouped in certain regions
of the topographic map.","['Andreas Krug', 'Sebastian Stober']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2019-12-06 10:31:29+00:00
http://arxiv.org/abs/1912.03049v4,Regularization Shortcomings for Continual Learning,"In most machine learning algorithms, training data is assumed to be
independent and identically distributed (iid). When it is not the case, the
algorithm's performances are challenged, leading to the famous phenomenon of
catastrophic forgetting. Algorithms dealing with it are gathered in the
Continual Learning research field. In this paper, we study the regularization
based approaches to continual learning and show that those approaches can not
learn to discriminate classes from different tasks in an elemental continual
benchmark: the class-incremental scenario. We make theoretical reasoning to
prove this shortcoming and illustrate it with examples and experiments.
Moreover, we show that it can have some important consequences on continual
multi-tasks reinforcement learning or in pre-trained models used for continual
learning. We believe that highlighting and understanding the shortcomings of
regularization strategies will help us to use them more efficiently.","['TimothÃ©e Lesort', 'Andrei Stoian', 'David Filliat']","['cs.LG', 'stat.ML']",2019-12-06 10:11:18+00:00
http://arxiv.org/abs/1912.03046v1,Hyperbolic Graph Attention Network,"Graph neural network (GNN) has shown superior performance in dealing with
graphs, which has attracted considerable research attention recently. However,
most of the existing GNN models are primarily designed for graphs in Euclidean
spaces. Recent research has proven that the graph data exhibits non-Euclidean
latent anatomy. Unfortunately, there was rarely study of GNN in non-Euclidean
settings so far. To bridge this gap, in this paper, we study the GNN with
attention mechanism in hyperbolic spaces at the first attempt. The research of
hyperbolic GNN has some unique challenges: since the hyperbolic spaces are not
vector spaces, the vector operations (e.g., vector addition, subtraction, and
scalar multiplication) cannot be carried. To tackle this problem, we employ the
gyrovector spaces, which provide an elegant algebraic formalism for hyperbolic
geometry, to transform the features in a graph; and then we propose the
hyperbolic proximity based attention mechanism to aggregate the features.
Moreover, as mathematical operations in hyperbolic spaces could be more
complicated than those in Euclidean spaces, we further devise a novel
acceleration strategy using logarithmic and exponential mappings to improve the
efficiency of our proposed model. The comprehensive experimental results on
four real-world datasets demonstrate the performance of our proposed hyperbolic
graph attention network model, by comparisons with other state-of-the-art
baseline methods.","['Yiding Zhang', 'Xiao Wang', 'Xunqiang Jiang', 'Chuan Shi', 'Yanfang Ye']","['cs.LG', 'stat.ML']",2019-12-06 09:54:36+00:00
http://arxiv.org/abs/1912.03036v1,Improved PAC-Bayesian Bounds for Linear Regression,"In this paper, we improve the PAC-Bayesian error bound for linear regression
derived in Germain et al. [10]. The improvements are twofold. First, the
proposed error bound is tighter, and converges to the generalization loss with
a well-chosen temperature parameter. Second, the error bound also holds for
training data that are not independently sampled. In particular, the error
bound applies to certain time series generated by well-known classes of
dynamical models, such as ARX models.","['Vera Shalaeva', 'Alireza Fakhrizadeh Esfahani', 'Pascal Germain', 'Mihaly Petreczky']","['cs.LG', 'stat.ML']",2019-12-06 09:24:56+00:00
http://arxiv.org/abs/1912.07367v1,A Model-driven and Data-driven Fusion Framework for Accurate Air Quality Prediction,"Air quality is closely related to public health. Health issues such as
cardiovascular diseases and respiratory diseases, may have connection with long
exposure to highly polluted environment. Therefore, accurate air quality
forecasts are extremely important to those who are vulnerable. To estimate the
variation of several air pollution concentrations, previous researchers used
various approaches, such as the Community Multiscale Air Quality model (CMAQ)
or neural networks. Although CMAQ model considers a coverage of the historic
air pollution data and meteorological variables, extra bias is introduced due
to additional adjustment. In this paper, a combination of model-based strategy
and data-driven method namely the physical-temporal collection(PTC) model is
proposed, aiming to fix the systematic error that traditional models deliver.
In the data-driven part, the first components are the temporal pattern and the
weather pattern to measure important features that contribute to the prediction
performance. The less relevant input variables will be removed to eliminate
negative weights in network training. Then, we deploy a long-short-term-memory
(LSTM) to fetch the preliminary results, which will be further corrected by a
neural network (NN) involving the meteorological index as well as other
pollutants concentrations. The data-set we applied for forecasting is from
January 1st, 2016 to December 31st, 2016. According to the results, our PTC
achieves an excellent performance compared with the baseline model (CMAQ
prediction, GRU, DNN and etc.). This joint model-based data-driven method for
air quality prediction can be easily deployed on stations without extra
adjustment, providing results with high-time-resolution information for
vulnerable members to prevent heavy air pollution ahead.","['Haolin Fei', 'Xiaofeng Wu', 'Chunbo Luo']","['stat.AP', 'cs.LG', 'eess.SP', 'stat.ML']",2019-12-06 08:24:11+00:00
http://arxiv.org/abs/1912.03015v3,Learning to Correspond Dynamical Systems,"Many dynamical systems exhibit similar structure, as often captured by
hand-designed simplified models that can be used for analysis and control. We
develop a method for learning to correspond pairs of dynamical systems via a
learned latent dynamical system. Given trajectory data from two dynamical
systems, we learn a shared latent state space and a shared latent dynamics
model, along with an encoder-decoder pair for each of the original systems.
With the learned correspondences in place, we can use a simulation of one
system to produce an imagined motion of its counterpart. We can also simulate
in the learned latent dynamics and synthesize the motions of both corresponding
systems, as a form of bisimulation. We demonstrate the approach using pairs of
controlled bipedal walkers, as well as by pairing a walker with a controlled
pendulum.","['Nam Hee Kim', 'Zhaoming Xie', 'Michiel van de Panne']","['cs.LG', 'cs.RO', 'stat.ML']",2019-12-06 08:21:49+00:00
http://arxiv.org/abs/1912.03011v3,A priori generalization error for two-layer ReLU neural network through minimum norm solution,"We focus on estimating \emph{a priori} generalization error of two-layer ReLU
neural networks (NNs) trained by mean squared error, which only depends on
initial parameters and the target function, through the following research
line. We first estimate \emph{a priori} generalization error of finite-width
two-layer ReLU NN with constraint of minimal norm solution, which is proved by
\cite{zhang2019type} to be an equivalent solution of a linearized (w.r.t.
parameter) finite-width two-layer NN. As the width goes to infinity, the
linearized NN converges to the NN in Neural Tangent Kernel (NTK) regime
\citep{jacot2018neural}. Thus, we can derive the \emph{a priori} generalization
error of two-layer ReLU NN in NTK regime. The distance between NN in a NTK
regime and a finite-width NN with gradient training is estimated by
\cite{arora2019exact}. Based on the results in \cite{arora2019exact}, our work
proves an \emph{a priori} generalization error bound of two-layer ReLU NNs.
This estimate uses the intrinsic implicit bias of the minimum norm solution
without requiring extra regularity in the loss function. This \emph{a priori}
estimate also implies that NN does not suffer from curse of dimensionality, and
a small generalization error can be achieved without requiring exponentially
large number of neurons. In addition the research line proposed in this paper
can also be used to study other properties of the finite-width network, such as
the posterior generalization error.","['Zhi-Qin John Xu', 'Jiwei Zhang', 'Yaoyu Zhang', 'Chengchao Zhao']","['cs.LG', 'stat.ML', '68Q32, 68T01', 'I.2.6']",2019-12-06 08:04:02+00:00
http://arxiv.org/abs/1912.02997v3,Improved Analysis of Spectral Algorithm for Clustering,"Spectral algorithms are graph partitioning algorithms that partition a node
set of a graph into groups by using a spectral embedding map. Clustering
techniques based on the algorithms are referred to as spectral clustering and
are widely used in data analysis. To gain a better understanding of why
spectral clustering is successful, Peng et al. (2015) and Kolev and Mehlhorn
(2016) studied the behavior of a certain type of spectral algorithm for a class
of graphs, called well-clustered graphs. Specifically, they put an assumption
on graphs and showed the performance guarantee of the spectral algorithm under
it. The algorithm they studied used the spectral embedding map developed by Shi
and Malic (2000). In this paper, we improve on their results, giving a better
performance guarantee under a weaker assumption. We also evaluate the
performance of the spectral algorithm with the spectral embedding map developed
by Ng et al. (2001).",['Tomohiko Mizutani'],"['cs.LG', 'stat.ML']",2019-12-06 06:32:14+00:00
http://arxiv.org/abs/1912.02992v1,Sampling-Free Learning of Bayesian Quantized Neural Networks,"Bayesian learning of model parameters in neural networks is important in
scenarios where estimates with well-calibrated uncertainty are important. In
this paper, we propose Bayesian quantized networks (BQNs), quantized neural
networks (QNNs) for which we learn a posterior distribution over their discrete
parameters. We provide a set of efficient algorithms for learning and
prediction in BQNs without the need to sample from their parameters or
activations, which not only allows for differentiable learning in QNNs, but
also reduces the variance in gradients. We evaluate BQNs on MNIST,
Fashion-MNIST, KMNIST and CIFAR10 image classification datasets, compared
against bootstrap ensemble of QNNs (E-QNN). We demonstrate BQNs achieve both
lower predictive errors and better-calibrated uncertainties than E-QNN (with
less than 20% of the negative log-likelihood).","['Jiahao Su', 'Milan Cvitkovic', 'Furong Huang']","['cs.LG', 'stat.ML']",2019-12-06 06:27:06+00:00
http://arxiv.org/abs/1912.02989v1,Influenza Modeling Based on Massive Feature Engineering and International Flow Deconvolution,"In this article, we focus on the analysis of the potential factors driving
the spread of influenza, and possible policies to mitigate the adverse effects
of the disease. To be precise, we first invoke discrete Fourier transform (DFT)
to conclude a yearly periodic regional structure in the influenza activity,
thus safely restricting ourselves to the analysis of the yearly influenza
behavior. Then we collect a massive number of possible region-wise indicators
contributing to the influenza mortality, such as consumption, immunization,
sanitation, water quality, and other indicators from external data, with $1170$
dimensions in total. We extract significant features from the high dimensional
indicators using a combination of data analysis techniques, including matrix
completion, support vector machines (SVM), autoencoders, and principal
component analysis (PCA). Furthermore, we model the international flow of
migration and trade as a convolution on regional influenza activity, and solve
the deconvolution problem as higher-order perturbations to the linear
regression, thus separating regional and international factors related to the
influenza mortality. Finally, both the original model and the perturbed model
are tested on regional examples, as validations of our models. Pertaining to
the policy, we make a proposal based on the connectivity data along with the
previously extracted significant features to alleviate the impact of influenza,
as well as efficiently propagate and carry out the policies. We conclude that
environmental features and economic features are of significance to the
influenza mortality. The model can be easily adapted to model other types of
infectious diseases.","['Ziming Liu', 'Yixuan Wang', 'Zizhao Han', 'Dian Wu']","['cs.LG', 'stat.AP', 'stat.ML']",2019-12-06 06:11:31+00:00
http://arxiv.org/abs/1912.02986v2,How Does an Approximate Model Help in Reinforcement Learning?,"One of the key approaches to save samples in reinforcement learning (RL) is
to use knowledge from an approximate model such as its simulator. However, how
much does an approximate model help to learn a near-optimal policy of the true
unknown model? Despite numerous empirical studies of transfer reinforcement
learning, an answer to this question is still elusive. In this paper, we study
the sample complexity of RL while an approximate model of the environment is
provided. For an unknown Markov decision process (MDP), we show that the
approximate model can effectively reduce the complexity by eliminating
sub-optimal actions from the policy searching space. In particular, we provide
an algorithm that uses $\widetilde{O}(N/(1-\gamma)^3/\varepsilon^2)$ samples in
a generative model to learn an $\varepsilon$-optimal policy, where $\gamma$ is
the discount factor and $N$ is the number of near-optimal actions in the
approximate model. This can be much smaller than the learning-from-scratch
complexity $\widetilde{\Theta}(SA/(1-\gamma)^3/\varepsilon^2)$, where $S$ and
$A$ are the sizes of state and action spaces respectively. We also provide a
lower bound showing that the above upper bound is nearly-tight if the value gap
between near-optimal actions and sub-optimal actions in the approximate model
is sufficiently large. Our results provide a very precise characterization of
how an approximate model helps reinforcement learning when no additional
assumption on the model is posed.","['Fei Feng', 'Wotao Yin', 'Lin F. Yang']","['cs.LG', 'cs.AI', 'stat.ML']",2019-12-06 06:05:59+00:00
http://arxiv.org/abs/1912.02983v1,DeepEthnic: Multi-Label Ethnic Classification from Face Images,"Ethnic group classification is a well-researched problem, which has been
pursued mainly during the past two decades via traditional approaches of image
processing and machine learning. In this paper, we propose a method of
classifying an image face into an ethnic group by applying transfer learning
from a previously trained classification network for large-scale data
recognition. Our proposed method yields state-of-the-art success rates of
99.02%, 99.76%, 99.2%, and 96.7%, respectively, for the four ethnic groups:
African, Asian, Caucasian, and Indian.","['Katia Huri', 'Eli David', 'Nathan S. Netanyahu']","['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']",2019-12-06 05:59:16+00:00
http://arxiv.org/abs/1912.02975v2,Observational Overfitting in Reinforcement Learning,"A major component of overfitting in model-free reinforcement learning (RL)
involves the case where the agent may mistakenly correlate reward with certain
spurious features from the observations generated by the Markov Decision
Process (MDP). We provide a general framework for analyzing this scenario,
which we use to design multiple synthetic benchmarks from only modifying the
observation space of an MDP. When an agent overfits to different observation
spaces even if the underlying MDP dynamics is fixed, we term this observational
overfitting. Our experiments expose intriguing properties especially with
regards to implicit regularization, and also corroborate results from previous
works in RL generalization and supervised learning (SL).","['Xingyou Song', 'Yiding Jiang', 'Stephen Tu', 'Yilun Du', 'Behnam Neyshabur']","['cs.LG', 'cs.AI', 'stat.ML']",2019-12-06 04:52:16+00:00
http://arxiv.org/abs/1912.02968v1,Physics-Informed Neural Networks for Multiphysics Data Assimilation with Application to Subsurface Transport,"Data assimilation for parameter and state estimation in subsurface transport
problems remains a significant challenge due to the sparsity of measurements,
the heterogeneity of porous media, and the high computational cost of forward
numerical models. We present a physics-informed deep neural networks (DNNs)
machine learning method for estimating space-dependent hydraulic conductivity,
hydraulic head, and concentration fields from sparse measurements. In this
approach, we employ individual DNNs to approximate the unknown parameters
(e.g., hydraulic conductivity) and states (e.g., hydraulic head and
concentration) of a physical system, and jointly train these DNNs by minimizing
the loss function that consists of the governing equations residuals in
addition to the error with respect to measurement data. We apply this approach
to assimilate conductivity, hydraulic head, and concentration measurements for
joint inversion of the conductivity, hydraulic head, and concentration fields
in a steady-state advection--dispersion problem. We study the accuracy of the
physics-informed DNN approach with respect to data size, number of variables
(conductivity and head versus conductivity, head, and concentration), DNNs
size, and DNN initialization during training. We demonstrate that the
physics-informed DNNs are significantly more accurate than standard data-driven
DNNs when the training set consists of sparse data. We also show that the
accuracy of parameter estimation increases as additional variables are inverted
jointly.","['QiZhi He', 'David Brajas-Solano', 'Guzel Tartakovsky', 'Alexandre M. Tartakovsky']","['cs.LG', 'physics.comp-ph', 'stat.ML']",2019-12-06 03:33:25+00:00
http://arxiv.org/abs/1912.02955v1,Hybrid Kronecker Product Decomposition and Approximation,"Discovering the underlying low dimensional structure of high dimensional data
has attracted a significant amount of researches recently and has shown to have
a wide range of applications. As an effective dimension reduction tool,
singular value decomposition is often used to analyze high dimensional
matrices, which are traditionally assumed to have a low rank matrix
approximation. In this paper, we propose a new approach. We assume a high
dimensional matrix can be approximated by a sum of a small number of Kronecker
products of matrices with potentially different configurations, named as a
hybird Kronecker outer Product Approximation (hKoPA). It provides an extremely
flexible way of dimension reduction compared to the low-rank matrix
approximation. Challenges arise in estimating a hKoPA when the configurations
of component Kronecker products are different or unknown. We propose an
estimation procedure when the set of configurations are given and a joint
configuration determination and component estimation procedure when the
configurations are unknown. Specifically, a least squares backfitting algorithm
is used when the configuration is given. When the configuration is unknown, an
iterative greedy algorithm is used. Both simulation and real image examples
show that the proposed algorithms have promising performances. The hybrid
Kronecker product approximation may have potentially wider applications in low
dimensional representation of high dimensional data","['Chencheng Cai', 'Rong Chen', 'Han Xiao']","['stat.ME', 'stat.ML']",2019-12-06 02:57:31+00:00
http://arxiv.org/abs/1912.02945v1,A pedestrian path-planning model in accordance with obstacle's danger with reinforcement learning,"Most microscopic pedestrian navigation models use the concept of ""forces""
applied to the pedestrian agents to replicate the navigation environment. While
the approach could provide believable results in regular situations, it does
not always resemble natural pedestrian navigation behaviour in many typical
settings. In our research, we proposed a novel approach using reinforcement
learning for simulation of pedestrian agent path planning and collision
avoidance problem. The primary focus of this approach is using human perception
of the environment and danger awareness of interferences. The implementation of
our model has shown that the path planned by the agent shares many similarities
with a human pedestrian in several aspects such as following common walking
conventions and human behaviours.","['Thanh-Trung Trinh', 'Dinh-Minh Vu', 'Masaomi Kimura']","['cs.LG', 'cs.MA', 'cs.RO', 'stat.ML']",2019-12-06 01:40:43+00:00
http://arxiv.org/abs/1912.02933v1,Risk-Aware MMSE Estimation,"Despite the simplicity and intuitive interpretation of Minimum Mean Squared
Error (MMSE) estimators, their effectiveness in certain scenarios is
questionable. Indeed, minimizing squared errors on average does not provide any
form of stability, as the volatility of the estimation error is left
unconstrained. When this volatility is statistically significant, the
difference between the average and realized performance of the MMSE estimator
can be drastically different. To address this issue, we introduce a new
risk-aware MMSE formulation which trades between mean performance and risk by
explicitly constraining the expected predictive variance of the involved
squared error. We show that, under mild moment boundedness conditions, the
corresponding risk-aware optimal solution can be evaluated explicitly, and has
the form of an appropriately biased nonlinear MMSE estimator. We further
illustrate the effectiveness of our approach via several numerical examples,
which also showcase the advantages of risk-aware MMSE estimation against
risk-neutral MMSE estimation, especially in models involving skewed,
heavy-tailed distributions.","['Dionysios S. Kalogerias', 'Luiz F. O. Chamon', 'George J. Pappas', 'Alejandro Ribeiro']","['math.OC', 'cs.IT', 'cs.SY', 'eess.SP', 'eess.SY', 'math.IT', 'stat.ML']",2019-12-06 00:33:33+00:00
http://arxiv.org/abs/1912.02928v4,"Bregman dynamics, contact transformations and convex optimization","Recent research on accelerated gradient methods of use in optimization has
demonstrated that these methods can be derived as discretizations of dynamical
systems. This, in turn, has provided a basis for more systematic
investigations, especially into the geometric structure of those dynamical
systems and their structure--preserving discretizations. In this work, we
introduce dynamical systems defined through a contact geometry which are not
only naturally suited to the optimization goal but also subsume all previous
methods based on geometric dynamical systems. As a consequence, all the
deterministic flows used in optimization share an extremely interesting
geometric property: they are invariant under contact transformations. In our
main result, we exploit this observation to show that the celebrated Bregman
Hamiltonian system can always be transformed into an equivalent but separable
Hamiltonian by means of a contact transformation. This in turn enables the
development of fast and robust discretizations through geometric contact
splitting integrators. As an illustration, we propose the Relativistic Bregman
algorithm, and show in some paradigmatic examples that it compares favorably
with respect to standard optimization algorithms such as classical momentum and
Nesterov's accelerated gradient.","['Alessandro Bravetti', 'Maria L. Daza-Torres', 'Hugo Flores-Arguedas', 'Michael Betancourt']","['math.OC', 'stat.ML']",2019-12-06 00:14:47+00:00
http://arxiv.org/abs/1912.03310v1,Geometric Capsule Autoencoders for 3D Point Clouds,"We propose a method to learn object representations from 3D point clouds
using bundles of geometrically interpretable hidden units, which we call
geometric capsules. Each geometric capsule represents a visual entity, such as
an object or a part, and consists of two components: a pose and a feature. The
pose encodes where the entity is, while the feature encodes what it is. We use
these capsules to construct a Geometric Capsule Autoencoder that learns to
group 3D points into parts (small local surfaces), and these parts into the
whole object, in an unsupervised manner. Our novel Multi-View Agreement voting
mechanism is used to discover an object's canonical pose and its pose-invariant
feature vector. Using the ShapeNet and ModelNet40 datasets, we analyze the
properties of the learned representations and show the benefits of having
multiple votes agree. We perform alignment and retrieval of arbitrarily rotated
objects -- tasks that evaluate our model's object identification and canonical
pose recovery capabilities -- and obtained insightful results.","['Nitish Srivastava', 'Hanlin Goh', 'Ruslan Salakhutdinov']","['cs.LG', 'cs.CV', 'cs.GR', 'cs.NE', 'stat.ML']",2019-12-06 00:10:14+00:00
http://arxiv.org/abs/1912.02919v4,An Empirical Study on the Intrinsic Privacy of SGD,"Introducing noise in the training of machine learning systems is a powerful
way to protect individual privacy via differential privacy guarantees, but
comes at a cost to utility. This work looks at whether the inherent randomness
of stochastic gradient descent (SGD) could contribute to privacy, effectively
reducing the amount of \emph{additional} noise required to achieve a given
privacy guarantee. We conduct a large-scale empirical study to examine this
question. Training a grid of over 120,000 models across four datasets (tabular
and images) on convex and non-convex objectives, we demonstrate that the random
seed has a larger impact on model weights than any individual training example.
We test the distribution over weights induced by the seed, finding that the
simple convex case can be modelled with a multivariate Gaussian posterior,
while neural networks exhibit multi-modal and non-Gaussian weight
distributions. By casting convex SGD as a Gaussian mechanism, we then estimate
an `intrinsic' data-dependent $\epsilon_i(\mathcal{D})$, finding values as low
as 6.3, dropping to 1.9 using empirical estimates. We use a membership
inference attack to estimate $\epsilon$ for non-convex SGD and demonstrate that
hiding the random seed from the adversary results in a statistically
significant reduction in attack performance, corresponding to a reduction in
the effective $\epsilon$. These results provide empirical evidence that SGD
exhibits appreciable variability relative to its dataset sensitivity, and this
`intrinsic noise' has the potential to be leveraged to improve the utility of
privacy-preserving machine learning.","['Stephanie L. Hyland', 'Shruti Tople']","['cs.LG', 'cs.CR', 'stat.ML']",2019-12-05 23:28:05+00:00
http://arxiv.org/abs/1912.02915v1,A Clustering Approach to Edge Controller Placement in Software Defined Networks with Cost Balancing,"In this work we introduce two novel deterministic annealing based clustering
algorithms to address the problem of Edge Controller Placement (ECP) in
wireless edge networks. These networks lie at the core of the fifth generation
(5G) wireless systems and beyond. These algorithms, ECP-LL and ECP-LB, address
the dominant leader-less and leader-based controller placement topologies and
have linear computational complexity in terms of network size, maximum number
of clusters and dimensionality of data. Each algorithm tries to place
controllers close to edge node clusters and not far away from other controllers
to maintain a reasonable balance between synchronization and delay costs. While
the ECP problem can be conveniently expressed as a multi-objective mixed
integer non-linear program (MINLP), our algorithms outperform state of art
MINLP solver, BARON both in terms of accuracy and speed. Our proposed
algorithms have the competitive edge of avoiding poor local minima through a
Shannon entropy term in the clustering objective function. Most ECP algorithms
are highly susceptible to poor local minima and greatly depend on
initialization.","['Reza Soleymanifar', 'Amber Srivastava', 'Carolyn Beck', 'Srinivasa Salapaka']","['cs.LG', 'cs.NI', 'stat.ML']",2019-12-05 23:07:35+00:00
http://arxiv.org/abs/1912.04042v1,Element Level Differential Privacy: The Right Granularity of Privacy,"Differential Privacy (DP) provides strong guarantees on the risk of
compromising a user's data in statistical learning applications, though these
strong protections make learning challenging and may be too stringent for some
use cases. To address this, we propose element level differential privacy,
which extends differential privacy to provide protection against leaking
information about any particular ""element"" a user has, allowing better utility
and more robust results than classical DP. By carefully choosing these
""elements,"" it is possible to provide privacy protections at a desired
granularity. We provide definitions, associated privacy guarantees, and
analysis to identify the tradeoffs with the new definition; we also develop
several private estimation and learning methodologies, providing careful
examples for item frequency and M-estimation (empirical risk minimization) with
concomitant privacy and utility analysis. We complement our theoretical and
methodological advances with several real-world applications, estimating
histograms and fitting several large-scale prediction models, including deep
networks.","['Hilal Asi', 'John Duchi', 'Omid Javidbakht']","['cs.LG', 'cs.CR', 'stat.ML']",2019-12-05 23:05:54+00:00
http://arxiv.org/abs/1912.02911v4,Deep learning with noisy labels: exploring techniques and remedies in medical image analysis,"Supervised training of deep learning models requires large labeled datasets.
There is a growing interest in obtaining such datasets for medical image
analysis applications. However, the impact of label noise has not received
sufficient attention. Recent studies have shown that label noise can
significantly impact the performance of deep learning models in many machine
learning and computer vision applications. This is especially concerning for
medical applications, where datasets are typically small, labeling requires
domain expertise and suffers from high inter- and intra-observer variability,
and erroneous predictions may influence decisions that directly impact human
health. In this paper, we first review the state-of-the-art in handling label
noise in deep learning. Then, we review studies that have dealt with label
noise in deep learning for medical image analysis. Our review shows that recent
progress on handling label noise in deep learning has gone largely unnoticed by
the medical image analysis community. To help achieve a better understanding of
the extent of the problem and its potential remedies, we conducted experiments
with three medical imaging datasets with different types of label noise, where
we investigated several existing strategies and developed new methods to combat
the negative effect of label noise. Based on the results of these experiments
and our review of the literature, we have made recommendations on methods that
can be used to alleviate the effects of different types of label noise on deep
models trained for medical image analysis. We hope that this article helps the
medical image analysis researchers and developers in choosing and devising new
techniques that effectively handle label noise in deep learning.","['Davood Karimi', 'Haoran Dou', 'Simon K. Warfield', 'Ali Gholipour']","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",2019-12-05 22:58:55+00:00
http://arxiv.org/abs/1912.02893v1,Learning undirected models via query training,"Typical amortized inference in variational autoencoders is specialized for a
single probabilistic query. Here we propose an inference network architecture
that generalizes to unseen probabilistic queries. Instead of an encoder-decoder
pair, we can train a single inference network directly from data, using a cost
function that is stochastic not only over samples, but also over queries. We
can use this network to perform the same inference tasks as we would in an
undirected graphical model with hidden variables, without having to deal with
the intractable partition function. The results can be mapped to the learning
of an actual undirected model, which is a notoriously hard problem. Our network
also marginalizes nuisance variables as required. We show that our approach
generalizes to unseen probabilistic queries on also unseen test data, providing
fast and flexible inference. Experiments show that this approach outperforms or
matches PCD and AdVIL on 9 benchmark datasets.","['Miguel Lazaro-Gredilla', 'Wolfgang Lehrach', 'Dileep George']","['cs.LG', 'stat.ML']",2019-12-05 21:42:52+00:00
http://arxiv.org/abs/1912.02864v1,Transfer Learning from an Auxiliary Discriminative Task for Unsupervised Anomaly Detection,"Unsupervised anomaly detection from high dimensional data like mobility
networks is a challenging task. Study of different approaches of feature
engineering from such high dimensional data have been a focus of research in
this field. This study aims to investigate the transferability of features
learned by network classification to unsupervised anomaly detection. We propose
use of an auxiliary classification task to extract features from unlabelled
data by supervised learning, which can be used for unsupervised anomaly
detection. We validate this approach by designing experiments to detect
anomalies in mobility network data from New York and Taipei, and compare the
results to traditional unsupervised feature learning approaches of PCA and
autoencoders. We find that our feature learning approach yields best anomaly
detection performance for both datasets, outperforming other studied
approaches. This establishes the utility of this approach to feature
engineering, which can be applied to other problems of similar nature.","['Urwa Muaz', 'Stanislav Sobolevsky']","['cs.LG', 'physics.soc-ph', 'stat.ML', '68T10']",2019-12-05 20:26:21+00:00
http://arxiv.org/abs/1912.03306v1,Asymptotic Unbiasedness of the Permutation Importance Measure in Random Forest Models,"Variable selection in sparse regression models is an important task as
applications ranging from biomedical research to econometrics have shown.
Especially for higher dimensional regression problems, for which the link
function between response and covariates cannot be directly detected, the
selection of informative variables is challenging. Under these circumstances,
the Random Forest method is a helpful tool to predict new outcomes while
delivering measures for variable selection. One common approach is the usage of
the permutation importance. Due to its intuitive idea and flexible usage, it is
important to explore circumstances, for which the permutation importance based
on Random Forest correctly indicates informative covariates. Regarding the
latter, we deliver theoretical guarantees for the validity of the permutation
importance measure under specific assumptions and prove its (asymptotic)
unbiasedness. An extensive simulation study verifies our findings.","['Burim Ramosaj', 'Markus Pauly']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2019-12-05 19:11:32+00:00
http://arxiv.org/abs/1912.02807v2,Combining Q-Learning and Search with Amortized Value Estimates,"We introduce ""Search with Amortized Value Estimates"" (SAVE), an approach for
combining model-free Q-learning with model-based Monte-Carlo Tree Search
(MCTS). In SAVE, a learned prior over state-action values is used to guide
MCTS, which estimates an improved set of state-action values. The new
Q-estimates are then used in combination with real experience to update the
prior. This effectively amortizes the value computation performed by MCTS,
resulting in a cooperative relationship between model-free learning and
model-based search. SAVE can be implemented on top of any Q-learning agent with
access to a model, which we demonstrate by incorporating it into agents that
perform challenging physical reasoning tasks and Atari. SAVE consistently
achieves higher rewards with fewer training steps, and---in contrast to typical
model-based search approaches---yields strong performance with very small
search budgets. By combining real experience with information computed during
search, SAVE demonstrates that it is possible to improve on both the
performance of model-free learning and the computational cost of planning.","['Jessica B. Hamrick', 'Victor Bapst', 'Alvaro Sanchez-Gonzalez', 'Tobias Pfaff', 'Theophane Weber', 'Lars Buesing', 'Peter W. Battaglia']","['cs.LG', 'stat.ML']",2019-12-05 18:54:23+00:00
http://arxiv.org/abs/1912.02803v1,Neural Tangents: Fast and Easy Infinite Neural Networks in Python,"Neural Tangents is a library designed to enable research into infinite-width
neural networks. It provides a high-level API for specifying complex and
hierarchical neural network architectures. These networks can then be trained
and evaluated either at finite-width as usual or in their infinite-width limit.
Infinite-width networks can be trained analytically using exact Bayesian
inference or using gradient descent via the Neural Tangent Kernel.
Additionally, Neural Tangents provides tools to study gradient descent training
dynamics of wide but finite networks in either function space or weight space.
  The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations
can be automatically distributed over multiple accelerators with near-linear
scaling in the number of devices. Neural Tangents is available at
www.github.com/google/neural-tangents. We also provide an accompanying
interactive Colab notebook.","['Roman Novak', 'Lechao Xiao', 'Jiri Hron', 'Jaehoon Lee', 'Alexander A. Alemi', 'Jascha Sohl-Dickstein', 'Samuel S. Schoenholz']","['stat.ML', 'cs.LG']",2019-12-05 18:51:57+00:00
http://arxiv.org/abs/1912.03120v1,A Study into Echocardiography View Conversion,"Transthoracic echo is one of the most common means of cardiac studies in the
clinical routines. During the echo exam, the sonographer captures a set of
standard cross sections (echo views) of the heart. Each 2D echo view cuts
through the 3D cardiac geometry via a unique plane. Consequently, different
views share some limited information. In this work, we investigate the
feasibility of generating a 2D echo view using another view based on
adversarial generative models. The objective optimized to train the
view-conversion model is based on the ideas introduced by LSGAN, PatchGAN and
Conditional GAN (cGAN). The size and length of the left ventricle in the
generated target echo view is compared against that of the target ground-truth
to assess the validity of the echo view conversion. Results show that there is
a correlation of 0.50 between the LV areas and 0.49 between the LV lengths of
the generated target frames and the real target frames.","['Amir H. Abdi', 'Mohammad H. Jafari', 'Sidney Fels', 'Theresa Tsang', 'Purang Abolmaesumi']","['eess.IV', 'cs.LG', 'stat.ML']",2019-12-05 18:44:59+00:00
http://arxiv.org/abs/1912.02794v2,Adversarial Risk via Optimal Transport and Optimal Couplings,"Modern machine learning algorithms perform poorly on adversarially
manipulated data. Adversarial risk quantifies the error of classifiers in
adversarial settings; adversarial classifiers minimize adversarial risk. In
this paper, we analyze adversarial risk and adversarial classifiers from an
optimal transport perspective. We show that the optimal adversarial risk for
binary classification with 0-1 loss is determined by an optimal transport cost
between the probability distributions of the two classes. We develop optimal
transport plans (probabilistic couplings) for univariate distributions such as
the normal, the uniform, and the triangular distribution. We also derive
optimal adversarial classifiers in these settings. Our analysis leads to
algorithm-independent fundamental limits on adversarial risk, which we
calculate for several real-world datasets. We extend our results to general
loss functions under convexity and smoothness assumptions.","['Muni Sreenivas Pydi', 'Varun Jog']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2019-12-05 18:39:07+00:00
http://arxiv.org/abs/1912.04211v1,Learning to run a power network challenge for training topology controllers,"For power grid operations, a large body of research focuses on using
generation redispatching, load shedding or demand side management
flexibilities. However, a less costly and potentially more flexible option
would be grid topology reconfiguration, as already partially exploited by
Coreso (European RSC) and RTE (French TSO) operations. Beyond previous work on
branch switching, bus reconfigurations are a broader class of action and could
provide some substantial benefits to route electricity and optimize the grid
capacity to keep it within safety margins. Because of its non-linear and
combinatorial nature, no existing optimal power flow solver can yet tackle this
problem. We here propose a new framework to learn topology controllers through
imitation and reinforcement learning. We present the design and the results of
the first ""Learning to Run a Power Network"" challenge released with this
framework. We finally develop a method providing performance upper-bounds
(oracle), which highlights remaining unsolved challenges and suggests future
directions of improvement.","['Antoine Marot', 'Benjamin Donnot', 'Camilo Romero', 'Luca Veyrin-Forrer', 'Marvin Lerousseau', 'Balthazar Donon', 'Isabelle Guyon']","['eess.SP', 'cs.LG', 'stat.ML']",2019-12-05 18:35:57+00:00
http://arxiv.org/abs/1912.05652v2,Learning Human Objectives by Evaluating Hypothetical Behavior,"We seek to align agent behavior with a user's objectives in a reinforcement
learning setting with unknown dynamics, an unknown reward function, and unknown
unsafe states. The user knows the rewards and unsafe states, but querying the
user is expensive. To address this challenge, we propose an algorithm that
safely and interactively learns a model of the user's reward function. We start
with a generative model of initial states and a forward dynamics model trained
on off-policy data. Our method uses these models to synthesize hypothetical
behaviors, asks the user to label the behaviors with rewards, and trains a
neural network to predict the rewards. The key idea is to actively synthesize
the hypothetical behaviors from scratch by maximizing tractable proxies for the
value of information, without interacting with the environment. We call this
method reward query synthesis via trajectory optimization (ReQueST). We
evaluate ReQueST with simulated users on a state-based 2D navigation task and
the image-based Car Racing video game. The results show that ReQueST
significantly outperforms prior methods in learning reward models that transfer
to new environments with different initial state distributions. Moreover,
ReQueST safely trains the reward model to detect unsafe states, and corrects
reward hacking before deploying the agent.","['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine', 'Shane Legg', 'Jan Leike']","['cs.CY', 'cs.LG', 'stat.ML']",2019-12-05 18:25:48+00:00
http://arxiv.org/abs/1912.02781v2,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,"Modern deep neural networks can achieve high accuracy when the training
distribution and test distribution are identically distributed, but this
assumption is frequently violated in practice. When the train and test
distributions are mismatched, accuracy can plummet. Currently there are few
techniques that improve robustness to unforeseen data shifts encountered during
deployment. In this work, we propose a technique to improve the robustness and
uncertainty estimates of image classifiers. We propose AugMix, a data
processing technique that is simple to implement, adds limited computational
overhead, and helps models withstand unforeseen corruptions. AugMix
significantly improves robustness and uncertainty measures on challenging image
classification benchmarks, closing the gap between previous methods and the
best possible performance in some cases by more than half.","['Dan Hendrycks', 'Norman Mu', 'Ekin D. Cubuk', 'Barret Zoph', 'Justin Gilmer', 'Balaji Lakshminarayanan']","['stat.ML', 'cs.CV', 'cs.LG']",2019-12-05 18:18:10+00:00
http://arxiv.org/abs/1912.02771v2,Label-Consistent Backdoor Attacks,"Deep neural networks have been demonstrated to be vulnerable to backdoor
attacks. Specifically, by injecting a small number of maliciously constructed
inputs into the training set, an adversary is able to plant a backdoor into the
trained model. This backdoor can then be activated during inference by a
backdoor trigger to fully control the model's behavior. While such attacks are
very effective, they crucially rely on the adversary injecting arbitrary inputs
that are---often blatantly---mislabeled. Such samples would raise suspicion
upon human inspection, potentially revealing the attack. Thus, for backdoor
attacks to remain undetected, it is crucial that they maintain
label-consistency---the condition that injected inputs are consistent with
their labels. In this work, we leverage adversarial perturbations and
generative models to execute efficient, yet label-consistent, backdoor attacks.
Our approach is based on injecting inputs that appear plausible, yet are hard
to classify, hence causing the model to rely on the (easier-to-learn) backdoor
trigger.","['Alexander Turner', 'Dimitris Tsipras', 'Aleksander Madry']","['stat.ML', 'cs.CR', 'cs.LG']",2019-12-05 18:05:59+00:00
http://arxiv.org/abs/1912.02765v2,On the Sample Complexity of Learning Sum-Product Networks,"Sum-Product Networks (SPNs) can be regarded as a form of deep graphical
models that compactly represent deeply factored and mixed distributions. An SPN
is a rooted directed acyclic graph (DAG) consisting of a set of leaves
(corresponding to base distributions), a set of sum nodes (which represent
mixtures of their children distributions) and a set of product nodes
(representing the products of its children distributions).
  In this work, we initiate the study of the sample complexity of PAC-learning
the set of distributions that correspond to SPNs. We show that the sample
complexity of learning tree structured SPNs with the usual type of leaves
(i.e., Gaussian or discrete) grows at most linearly (up to logarithmic factors)
with the number of parameters of the SPN. More specifically, we show that the
class of distributions that corresponds to tree structured Gaussian SPNs with
$k$ mixing weights and $e$ ($d$-dimensional Gaussian) leaves can be learned
within Total Variation error $\epsilon$ using at most
$\widetilde{O}(\frac{ed^2+k}{\epsilon^2})$ samples. A similar result holds for
tree structured SPNs with discrete leaves.
  We obtain the upper bounds based on the recently proposed notion of
distribution compression schemes. More specifically, we show that if a (base)
class of distributions $\mathcal{F}$ admits an ""efficient"" compression, then
the class of tree structured SPNs with leaves from $\mathcal{F}$ also admits an
efficient compression.","['Ishaq Aden-Ali', 'Hassan Ashtiani']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2019-12-05 17:57:58+00:00
http://arxiv.org/abs/1912.02762v2,Normalizing Flows for Probabilistic Modeling and Inference,"Normalizing flows provide a general mechanism for defining expressive
probability distributions, only requiring the specification of a (usually
simple) base distribution and a series of bijective transformations. There has
been much recent work on normalizing flows, ranging from improving their
expressive power to expanding their application. We believe the field has now
matured and is in need of a unified perspective. In this review, we attempt to
provide such a perspective by describing flows through the lens of
probabilistic modeling and inference. We place special emphasis on the
fundamental principles of flow design, and discuss foundational topics such as
expressive power and computational trade-offs. We also broaden the conceptual
framing of flows by relating them to more general probability transformations.
Lastly, we summarize the use of flows for tasks such as generative modeling,
approximate inference, and supervised learning.","['George Papamakarios', 'Eric Nalisnick', 'Danilo Jimenez Rezende', 'Shakir Mohamed', 'Balaji Lakshminarayanan']","['stat.ML', 'cs.LG']",2019-12-05 17:55:27+00:00
http://arxiv.org/abs/1912.02757v2,Deep Ensembles: A Loss Landscape Perspective,"Deep ensembles have been empirically shown to be a promising approach for
improving accuracy, uncertainty and out-of-distribution robustness of deep
learning models. While deep ensembles were theoretically motivated by the
bootstrap, non-bootstrap ensembles trained with just random initialization also
perform well in practice, which suggests that there could be other explanations
for why deep ensembles work well. Bayesian neural networks, which learn
distributions over the parameters of the network, are theoretically
well-motivated by Bayesian principles, but do not perform as well as deep
ensembles in practice, particularly under dataset shift. One possible
explanation for this gap between theory and practice is that popular scalable
variational Bayesian methods tend to focus on a single mode, whereas deep
ensembles tend to explore diverse modes in function space. We investigate this
hypothesis by building on recent work on understanding the loss landscape of
neural networks and adding our own exploration to measure the similarity of
functions in the space of predictions. Our results show that random
initializations explore entirely different modes, while functions along an
optimization trajectory or sampled from the subspace thereof cluster within a
single mode predictions-wise, while often deviating significantly in the weight
space. Developing the concept of the diversity--accuracy plane, we show that
the decorrelation power of random initializations is unmatched by popular
subspace sampling methods. Finally, we evaluate the relative effects of
ensembling, subspace based methods and ensembles of subspace based methods, and
the experimental results validate our hypothesis.","['Stanislav Fort', 'Huiyi Hu', 'Balaji Lakshminarayanan']","['stat.ML', 'cs.LG']",2019-12-05 17:48:18+00:00
http://arxiv.org/abs/1912.02738v4,MetaFun: Meta-Learning with Iterative Functional Updates,"We develop a functional encoder-decoder approach to supervised meta-learning,
where labeled data is encoded into an infinite-dimensional functional
representation rather than a finite-dimensional one. Furthermore, rather than
directly producing the representation, we learn a neural update rule resembling
functional gradient descent which iteratively improves the representation. The
final representation is used to condition the decoder to make predictions on
unlabeled data. Our approach is the first to demonstrates the success of
encoder-decoder style meta-learning methods like conditional neural processes
on large-scale few-shot classification benchmarks such as miniImageNet and
tieredImageNet, where it achieves state-of-the-art performance.","['Jin Xu', 'Jean-Francois Ton', 'Hyunjik Kim', 'Adam R. Kosiorek', 'Yee Whye Teh']","['stat.ML', 'cs.LG']",2019-12-05 17:25:13+00:00
http://arxiv.org/abs/1912.02729v2,Rademacher complexity and spin glasses: A link between the replica and statistical theories of learning,"Statistical learning theory provides bounds of the generalization gap, using
in particular the Vapnik-Chervonenkis dimension and the Rademacher complexity.
An alternative approach, mainly studied in the statistical physics literature,
is the study of generalization in simple synthetic-data models. Here we discuss
the connections between these approaches and focus on the link between the
Rademacher complexity in statistical learning and the theories of
generalization for typical-case synthetic models from statistical physics,
involving quantities known as Gardner capacity and ground state energy. We show
that in these models the Rademacher complexity is closely related to the ground
state energy computed by replica theories. Using this connection, one may
reinterpret many results of the literature as rigorous Rademacher bounds in a
variety of models in the high-dimensional statistics limit. Somewhat
surprisingly, we also show that statistical learning theory provides
predictions for the behavior of the ground-state energies in some full replica
symmetry breaking models.","['Alia Abbara', 'Benjamin Aubin', 'Florent Krzakala', 'Lenka ZdeborovÃ¡']","['cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.LG', 'stat.ML']",2019-12-05 17:09:17+00:00
http://arxiv.org/abs/1912.02724v1,Causal structure based root cause analysis of outliers,"We describe a formal approach to identify 'root causes' of outliers observed
in $n$ variables $X_1,\dots,X_n$ in a scenario where the causal relation
between the variables is a known directed acyclic graph (DAG). To this end, we
first introduce a systematic way to define outlier scores. Further, we
introduce the concept of 'conditional outlier score' which measures whether a
value of some variable is unexpected *given the value of its parents* in the
DAG, if one were to assume that the causal structure and the corresponding
conditional distributions are also valid for the anomaly. Finally, we quantify
to what extent the high outlier score of some target variable can be attributed
to outliers of its ancestors. This quantification is defined via Shapley values
from cooperative game theory.","['Dominik Janzing', 'Kailash Budhathoki', 'Lenon Minorics', 'Patrick BlÃ¶baum']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2019-12-05 17:00:20+00:00
http://arxiv.org/abs/1912.04212v9,Solving Bayesian Inverse Problems via Variational Autoencoders,"In recent years, the field of machine learning has made phenomenal progress
in the pursuit of simulating real-world data generation processes. One notable
example of such success is the variational autoencoder (VAE). In this work,
with a small shift in perspective, we leverage and adapt VAEs for a different
purpose: uncertainty quantification in scientific inverse problems. We
introduce UQ-VAE: a flexible, adaptive, hybrid data/model-informed framework
for training neural networks capable of rapid modelling of the posterior
distribution representing the unknown parameter of interest. Specifically, from
divergence-based variational inference, our framework is derived such that most
of the information usually present in scientific inverse problems is fully
utilized in the training procedure. Additionally, this framework includes an
adjustable hyperparameter that allows selection of the notion of distance
between the posterior model and the target distribution. This introduces more
flexibility in controlling how optimization directs the learning of the
posterior model. Further, this framework possesses an inherent adaptive
optimization property that emerges through the learning of the posterior
uncertainty.","['Hwan Goh', 'Sheroze Sheriffdeen', 'Jonathan Wittmer', 'Tan Bui-Thanh']","['stat.ML', 'cs.LG', 'eess.IV']",2019-12-05 16:33:32+00:00
http://arxiv.org/abs/1912.02703v1,Self-Supervised Contextual Language Representation of Radiology Reports to Improve the Identification of Communication Urgency,"Machine learning methods have recently achieved high-performance in
biomedical text analysis. However, a major bottleneck in the widespread
application of these methods is obtaining the required large amounts of
annotated training data, which is resource intensive and time consuming. Recent
progress in self-supervised learning has shown promise in leveraging large text
corpora without explicit annotations. In this work, we built a self-supervised
contextual language representation model using BERT, a deep bidirectional
transformer architecture, to identify radiology reports requiring prompt
communication to the referring physicians. We pre-trained the BERT model on a
large unlabeled corpus of radiology reports and used the resulting contextual
representations in a final text classifier for communication urgency. Our model
achieved a precision of 97.0%, recall of 93.3%, and F-measure of 95.1% on an
independent test set in identifying radiology reports for prompt communication,
and significantly outperformed the previous state-of-the-art model based on
word2vec representations.","['Xing Meng', 'Craig H. Ganoe', 'Ryan T. Sieberg', 'Yvonne Y. Cheung', 'Saeed Hassanpour']","['cs.LG', 'cs.CL', 'stat.ML']",2019-12-05 16:33:23+00:00
http://arxiv.org/abs/1912.08922v1,Real-time 2019 Portuguese Parliament Election Results Dataset,"This paper presents a data set describing the evolution of results in the
Portuguese Parliamentary Elections of October 6$^{th}$ 2019. The data spans a
time interval of 4 hours and 25 minutes, in intervals of 5 minutes, concerning
the results of the 27 parties involved in the electoral event. The data set is
tailored for predictive modelling tasks, mostly focused on numerical
forecasting tasks. Regardless, it allows for other tasks such as ordinal
regression or learn-to-rank.",['Nuno Moniz'],"['cs.SI', 'cs.LG', 'stat.ML']",2019-12-05 15:52:31+00:00
http://arxiv.org/abs/1912.02644v1,Representing Closed Transformation Paths in Encoded Network Latent Space,"Deep generative networks have been widely used for learning mappings from a
low-dimensional latent space to a high-dimensional data space. In many cases,
data transformations are defined by linear paths in this latent space. However,
the Euclidean structure of the latent space may be a poor match for the
underlying latent structure in the data. In this work, we incorporate a
generative manifold model into the latent space of an autoencoder in order to
learn the low-dimensional manifold structure from the data and adapt the latent
space to accommodate this structure. In particular, we focus on applications in
which the data has closed transformation paths which extend from a starting
point and return to nearly the same point. Through experiments on data with
natural closed transformation paths, we show that this model introduces the
ability to learn the latent dynamics of complex systems, generate
transformation paths, and classify samples that belong on the same
transformation path.","['Marissa Connor', 'Christopher Rozell']","['stat.ML', 'cs.LG']",2019-12-05 15:27:26+00:00
http://arxiv.org/abs/1912.02641v1,Scalable Variational Bayesian Kernel Selection for Sparse Gaussian Process Regression,"This paper presents a variational Bayesian kernel selection (VBKS) algorithm
for sparse Gaussian process regression (SGPR) models. In contrast to existing
GP kernel selection algorithms that aim to select only one kernel with the
highest model evidence, our proposed VBKS algorithm considers the kernel as a
random variable and learns its belief from data such that the uncertainty of
the kernel can be interpreted and exploited to avoid overconfident GP
predictions. To achieve this, we represent the probabilistic kernel as an
additional variational variable in a variational inference (VI) framework for
SGPR models where its posterior belief is learned together with that of the
other variational variables (i.e., inducing variables and kernel
hyperparameters). In particular, we transform the discrete kernel belief into a
continuous parametric distribution via reparameterization in order to apply VI.
Though it is computationally challenging to jointly optimize a large number of
hyperparameters due to many kernels being evaluated simultaneously by our VBKS
algorithm, we show that the variational lower bound of the log-marginal
likelihood can be decomposed into an additive form such that each additive term
depends only on a disjoint subset of the variational variables and can thus be
optimized independently. Stochastic optimization is then used to maximize the
variational lower bound by iteratively improving the variational approximation
of the exact posterior belief via stochastic gradient ascent, which incurs
constant time per iteration and hence scales to big data. We empirically
evaluate the performance of our VBKS algorithm on synthetic and massive
real-world datasets.","['Tong Teng', 'Jie Chen', 'Yehong Zhang', 'Kian Hsiang Low']","['cs.LG', 'stat.ML']",2019-12-05 15:23:10+00:00
http://arxiv.org/abs/1912.02631v2,Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning,"Machine learning has started to be deployed in fields such as healthcare and
finance, which propelled the need for and growth of privacy-preserving machine
learning (PPML). We propose an actively secure four-party protocol (4PC), and a
framework for PPML, showcasing its applications on four of the most
widely-known machine learning algorithms -- Linear Regression, Logistic
Regression, Neural Networks, and Convolutional Neural Networks. Our 4PC
protocol tolerating at most one malicious corruption is practically efficient
as compared to the existing works. We use the protocol to build an efficient
mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and
Garbled worlds. Our framework operates in the offline-online paradigm over
rings and is instantiated in an outsourced setting for machine learning. Also,
we propose conversions especially relevant to privacy-preserving machine
learning. The highlights of our framework include using a minimal number of
expensive circuits overall as compared to ABY3. This can be seen in our
technique for truncation, which does not affect the online cost of
multiplication and removes the need for any circuits in the offline phase. Our
B2A conversion has an improvement of $\mathbf{7} \times$ in rounds and
$\mathbf{18} \times$ in the communication complexity. The practicality of our
framework is argued through improvements in the benchmarking of the
aforementioned algorithms when compared with ABY3. All the protocols are
implemented over a 64-bit ring in both LAN and WAN settings. Our improvements
go up to $\mathbf{187} \times$ for the training phase and $\mathbf{158} \times$
for the prediction phase when observed over LAN and WAN.","['Harsh Chaudhari', 'Rahul Rachuri', 'Ajith Suresh']","['cs.LG', 'cs.CR', 'stat.ML']",2019-12-05 15:06:39+00:00
http://arxiv.org/abs/1912.02605v2,Towards Understanding Residual and Dilated Dense Neural Networks via Convolutional Sparse Coding,"Convolutional neural network (CNN) and its variants have led to many
state-of-art results in various fields. However, a clear theoretical
understanding about them is still lacking. Recently, multi-layer convolutional
sparse coding (ML-CSC) has been proposed and proved to equal such simply
stacked networks (plain networks). Here, we think three factors in each layer
of it including the initialization, the dictionary design and the number of
iterations greatly affect the performance of ML-CSC. Inspired by these
considerations, we propose two novel multi-layer models--residual convolutional
sparse coding model (Res-CSC) and mixed-scale dense convolutional sparse coding
model (MSD-CSC), which have close relationship with the residual neural network
(ResNet) and mixed-scale (dilated) dense neural network (MSDNet), respectively.
Mathematically, we derive the shortcut connection in ResNet as a special case
of a new forward propagation rule on ML-CSC. We find a theoretical
interpretation of the dilated convolution and dense connection in MSDNet by
analyzing MSD-CSC, which gives a clear mathematical understanding about them.
We implement the iterative soft thresholding algorithm (ISTA) and its fast
version to solve Res-CSC and MSD-CSC, which can employ the unfolding operation
for further improvements. At last, extensive numerical experiments and
comparison with competing methods demonstrate their effectiveness using three
typical datasets.","['Zhiyang Zhang', 'Shihua Zhang']","['cs.LG', 'cs.CV', 'stat.ML']",2019-12-05 14:46:01+00:00
http://arxiv.org/abs/1912.02588v1,Tensor Recovery from Noisy and Multi-Level Quantized Measurements,"Higher-order tensors can represent scores in a rating system, frames in a
video, and images of the same subject. In practice, the measurements are often
highly quantized due to the sampling strategies or the quality of devices.
Existing works on tensor recovery have focused on data losses and random
noises. Only a few works consider tensor recovery from quantized measurements
but are restricted to binary measurements. This paper, for the first time,
addresses the problem of tensor recovery from multi-level quantized
measurements. Leveraging the low-rank property of the tensor, this paper
proposes a nonconvex optimization problem for tensor recovery. We provide a
theoretical upper bound of the recovery error, which diminishes to zero when
the sizes of dimensions increase to infinity. Our error bound significantly
improves over the existing results in one-bit tensor recovery and quantized
matrix recovery. A tensor-based alternating proximal gradient descent algorithm
with a convergence guarantee is proposed to solve the nonconvex problem. Our
recovery method can handle data losses and do not need the information of the
quantization rule. The method is validated on synthetic data, image datasets,
and music recommender datasets.","['Ren Wang', 'Meng Wang', 'Jinjun Xiong']","['cs.LG', 'eess.SP', 'stat.ML']",2019-12-05 14:27:25+00:00
http://arxiv.org/abs/1912.02580v2,Collective Learning,"In this paper, we introduce the concept of collective learning (CL) which
exploits the notion of collective intelligence in the field of distributed
semi-supervised learning. The proposed framework draws inspiration from the
learning behavior of human beings, who alternate phases involving
collaboration, confrontation and exchange of views with other consisting of
studying and learning on their own. On this regard, CL comprises two main
phases: a self-training phase in which learning is performed on local private
(labeled) data only and a collective training phase in which proxy-labels are
assigned to shared (unlabeled) data by means of a consensus-based algorithm. In
the considered framework, heterogeneous systems can be connected over the same
network, each with different computational capabilities and resources and
everyone in the network may take advantage of the cooperation and will
eventually reach higher performance with respect to those it can reach on its
own. An extensive experimental campaign on an image classification problem
emphasizes the properties of CL by analyzing the performance achieved by the
cooperating agents.",['Francesco Farina'],"['cs.LG', 'cs.MA', 'math.OC', 'stat.ML']",2019-12-05 14:08:34+00:00
http://arxiv.org/abs/1912.02572v3,Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning: A Field Experiment,"In this paper we present an end-to-end framework for addressing the problem
of dynamic pricing (DP) on E-commerce platform using methods based on deep
reinforcement learning (DRL). By using four groups of different business data
to represent the states of each time period, we model the dynamic pricing
problem as a Markov Decision Process (MDP). Compared with the state-of-the-art
DRL-based dynamic pricing algorithms, our approaches make the following three
contributions. First, we extend the discrete set problem to the continuous
price set. Second, instead of using revenue as the reward function directly, we
define a new function named difference of revenue conversion rates (DRCR).
Third, the cold-start problem of MDP is tackled by pre-training and evaluation
using some carefully chosen historical sales data. Our approaches are evaluated
by both offline evaluation method using real dataset of Alibaba Inc., and
online field experiments starting from July 2018 with thousands of items,
lasting for months on Tmall.com. To our knowledge, there is no other DP field
experiment using DRL before. Field experiment results suggest that DRCR is a
more appropriate reward function than revenue, which is widely used by current
literature. Also, continuous price sets have better performance than discrete
sets and our approaches significantly outperformed the manual pricing by
operation experts.","['Jiaxi Liu', 'Yidong Zhang', 'Xiaoqing Wang', 'Yuming Deng', 'Xingyu Wu']","['cs.LG', 'stat.ML']",2019-12-05 13:41:03+00:00
http://arxiv.org/abs/1912.02566v3,Screening Data Points in Empirical Risk Minimization via Ellipsoidal Regions and Safe Loss Functions,"We design simple screening tests to automatically discard data samples in
empirical risk minimization without losing optimization guarantees. We derive
loss functions that produce dual objectives with a sparse solution. We also
show how to regularize convex losses to ensure such a dual sparsity-inducing
property, and propose a general method to design screening tests for
classification or regression based on ellipsoidal approximations of the optimal
set. In addition to producing computational gains, our approach also allows us
to compress a dataset into a subset of representative points.","['GrÃ©goire Mialon', ""Alexandre d'Aspremont"", 'Julien Mairal']","['cs.LG', 'stat.ML']",2019-12-05 13:30:01+00:00
http://arxiv.org/abs/1912.10162v1,Design and implementation of an open source Greek POS Tagger and Entity Recognizer using spaCy,"This paper proposes a machine learning approach to part-of-speech tagging and
named entity recognition for Greek, focusing on the extraction of morphological
features and classification of tokens into a small set of classes for named
entities. The architecture model that was used is introduced. The greek version
of the spaCy platform was added into the source code, a feature that did not
exist before our contribution, and was used for building the models.
Additionally, a part of speech tagger was trained that can detect the
morphology of the tokens and performs higher than the state-of-the-art results
when classifying only the part of speech. For named entity recognition using
spaCy, a model that extends the standard ENAMEX type (organization, location,
person) was built. Certain experiments that were conducted indicate the need
for flexibility in out-of-vocabulary words and there is an effort for resolving
this issue. Finally, the evaluation results are discussed.","['Eleni Partalidou', 'Eleftherios Spyromitros-Xioufis', 'Stavros Doropoulos', 'Stavros Vologiannidis', 'Konstantinos I. Diamantaras']","['cs.CL', 'cs.IR', 'cs.LG', 'stat.ML']",2019-12-05 13:29:27+00:00
http://arxiv.org/abs/1912.02532v1,Iterative Policy-Space Expansion in Reinforcement Learning,"Humans and animals solve a difficult problem much more easily when they are
presented with a sequence of problems that starts simple and slowly increases
in difficulty. We explore this idea in the context of reinforcement learning.
Rather than providing the agent with an externally provided curriculum of
progressively more difficult tasks, the agent solves a single task utilizing a
decreasingly constrained policy space. The algorithm we propose first learns to
categorize features into positive and negative before gradually learning a more
refined policy. Experimental results in Tetris demonstrate superior learning
rate of our approach when compared to existing algorithms.","['Jan Malte Lichtenberg', 'ÃzgÃ¼r ÅimÅek']","['cs.LG', 'cs.AI', 'stat.ML']",2019-12-05 12:32:15+00:00
http://arxiv.org/abs/1912.02527v1,Warped Input Gaussian Processes for Time Series Forecasting,"We introduce a Gaussian process-based model for handling of non-stationarity.
The warping is achieved non-parametrically, through imposing a prior on the
relative change of distance between subsequent observation inputs. The model
allows the use of general gradient optimization algorithms for training and
incurs only a small computational overhead on training and prediction. The
model finds its applications in forecasting in non-stationary time series with
either gradually varying volatility, presence of change points, or a
combination thereof. We evaluate the model on synthetic and real-world time
series data comparing against both baseline and known state-of-the-art
approaches and show that the model exhibits state-of-the-art forecasting
performance at a lower implementation and computation cost.",['David Tolpin'],"['stat.ML', 'cs.LG']",2019-12-05 12:11:54+00:00
http://arxiv.org/abs/1912.02522v1,VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge,"The VoxCeleb Speaker Recognition Challenge 2019 aimed to assess how well
current speaker recognition technology is able to identify speakers in
unconstrained or `in the wild' data. It consisted of: (i) a publicly available
speaker recognition dataset from YouTube videos together with ground truth
annotation and standardised evaluation software; and (ii) a public challenge
and workshop held at Interspeech 2019 in Graz, Austria. This paper outlines the
challenge and provides its baselines, results and discussions.","['Joon Son Chung', 'Arsha Nagrani', 'Ernesto Coto', 'Weidi Xie', 'Mitchell McLaren', 'Douglas A Reynolds', 'Andrew Zisserman']","['cs.SD', 'cs.LG', 'eess.AS', 'stat.ML']",2019-12-05 12:00:45+00:00
http://arxiv.org/abs/1912.02503v1,Hindsight Credit Assignment,"We consider the problem of efficient credit assignment in reinforcement
learning. In order to efficiently and meaningfully utilize new data, we propose
to explicitly assign credit to past decisions based on the likelihood of them
having led to the observed outcome. This approach uses new information in
hindsight, rather than employing foresight. Somewhat surprisingly, we show that
value functions can be rewritten through this lens, yielding a new family of
algorithms. We study the properties of these algorithms, and empirically show
that they successfully address important credit assignment challenges, through
a set of illustrative tasks.","['Anna Harutyunyan', 'Will Dabney', 'Thomas Mesnard', 'Mohammad Azar', 'Bilal Piot', 'Nicolas Heess', 'Hado van Hasselt', 'Greg Wayne', 'Satinder Singh', 'Doina Precup', 'Remi Munos']","['cs.LG', 'stat.ML']",2019-12-05 11:05:27+00:00
http://arxiv.org/abs/1912.02494v2,MetalGAN: Multi-Domain Label-Less Image Synthesis Using cGANs and Meta-Learning,"Image synthesis is currently one of the most addressed image processing topic
in computer vision and deep learning fields of study. Researchers have tackled
this problem focusing their efforts on its several challenging problems, e.g.
image quality and size, domain and pose changing, architecture of the networks,
and so on. Above all, producing images belonging to different domains by using
a single architecture is a very relevant goal for image generation. In fact, a
single multi-domain network would allow greater flexibility and robustness in
the image synthesis task than other approaches. This paper proposes a novel
architecture and a training algorithm, which are able to produce multi-domain
outputs using a single network. A small portion of a dataset is intentionally
used, and there are no hard-coded labels (or classes). This is achieved by
combining a conditional Generative Adversarial Network (cGAN) for image
generation and a Meta-Learning algorithm for domain switch, and we called our
approach MetalGAN. The approach has proved to be appropriate for solving the
multi-domain problem and it is validated on facial attribute transfer, using
CelebA dataset.","['Tomaso Fontanini', 'Eleonora Iotti', 'Luca Donati', 'Andrea Prati']","['cs.LG', 'cs.CV', 'stat.ML']",2019-12-05 10:47:08+00:00
http://arxiv.org/abs/1912.02493v1,Ordinal Bayesian Optimisation,"Bayesian optimisation is a powerful tool to solve expensive black-box
problems, but fails when the stationary assumption made on the objective
function is strongly violated, which is the case in particular for
ill-conditioned or discontinuous objectives. We tackle this problem by
proposing a new Bayesian optimisation framework that only considers the
ordering of variables, both in the input and output spaces, to fit a Gaussian
process in a latent space. By doing so, our approach is agnostic to the
original metrics on the original spaces. We propose two algorithms,
respectively based on an optimistic strategy and on Thompson sampling. For the
optimistic strategy we prove an optimal performance under the measure of regret
in the latent space. We illustrate the capability of our framework on several
challenging toy problems.","['Victor Picheny', 'Sattar Vakili', 'Artem Artemev']","['stat.ML', 'cs.LG', 'math.OC']",2019-12-05 10:46:06+00:00
http://arxiv.org/abs/1912.02427v2,Analysis of the Optimization Landscapes for Overcomplete Representation Learning,"We study nonconvex optimization landscapes for learning overcomplete
representations, including learning (i) sparsely used overcomplete dictionaries
and (ii) convolutional dictionaries, where these unsupervised learning problems
find many applications in high-dimensional data analysis. Despite the empirical
success of simple nonconvex algorithms, theoretical justifications of why these
methods work so well are far from satisfactory. In this work, we show these
problems can be formulated as $\ell^4$-norm optimization problems with
spherical constraint, and study the geometric properties of their nonconvex
optimization landscapes. For both problems, we show the nonconvex objectives
have benign (global) geometric structures, in the sense that every local
minimizer is close to one of the target solutions and every saddle point
exhibits negative curvature. This discovery enables the development of
guaranteed global optimization methods using simple initializations. For both
problems, we show the nonconvex objectives have benign geometric structures --
every local minimizer is close to one of the target solutions and every saddle
point exhibits negative curvature -- either in the entire space or within a
sufficiently large region. This discovery ensures local search algorithms (such
as Riemannian gradient descent) with simple initializations approximately find
the target solutions. Finally, numerical experiments justify our theoretical
discoveries.","['Qing Qu', 'Yuexiang Zhai', 'Xiao Li', 'Yuqian Zhang', 'Zhihui Zhu']","['cs.LG', 'cs.IT', 'eess.SP', 'math.IT', 'math.OC', 'stat.ML']",2019-12-05 08:14:24+00:00
http://arxiv.org/abs/1912.04106v2,Towards countering hate speech against journalists on social media,"The damaging effects of hate speech on social media are evident during the
last few years, and several organizations, researchers and social media
platforms tried to harness them in various ways. Despite these efforts, social
media users are still affected by hate speech. The problem is even more
apparent to social groups that promote public discourse, such as journalists.
In this work, we focus on countering hate speech that is targeted to
journalistic social media accounts. To accomplish this, a group of journalists
assembled a definition of hate speech, taking into account the journalistic
point of view and the types of hate speech that are usually targeted against
journalists. We then compile a large pool of tweets referring to
journalism-related accounts in multiple languages. In order to annotate the
pool of unlabeled tweets according to the definition, we follow a concise
annotation strategy that involves active learning annotation stages. The
outcome of this paper is a novel, publicly available collection of Twitter
datasets in five different languages. Additionally, we experiment with
state-of-the-art deep learning architectures for hate speech detection and use
our annotated datasets to train and evaluate them. Finally, we propose an
ensemble detection model that outperforms all individual models.","['Polychronis Charitidis', 'Stavros Doropoulos', 'Stavros Vologiannidis', 'Ioannis Papastergiou', 'Sophia Karakeva']","['cs.IR', 'cs.LG', 'cs.SI', 'stat.ML']",2019-12-05 07:51:23+00:00
http://arxiv.org/abs/1912.02405v1,Clustering Time-Series by a Novel Slope-Based Similarity Measure Considering Particle Swarm Optimization,"Recently there has been an increase in the studies on time-series data mining
specifically time-series clustering due to the vast existence of time-series in
various domains. The large volume of data in the form of time-series makes it
necessary to employ various techniques such as clustering to understand the
data and to extract information and hidden patterns. In the field of clustering
specifically, time-series clustering, the most important aspects are the
similarity measure used and the algorithm employed to conduct the clustering.
In this paper, a new similarity measure for time-series clustering is developed
based on a combination of a simple representation of time-series, slope of each
segment of time-series, Euclidean distance and the so-called dynamic time
warping. It is proved in this paper that the proposed distance measure is
metric and thus indexing can be applied. For the task of clustering, the
Particle Swarm Optimization algorithm is employed. The proposed similarity
measure is compared to three existing measures in terms of various criteria
used for the evaluation of clustering algorithms. The results indicate that the
proposed similarity measure outperforms the rest in almost every dataset used
in this paper.","['Hossein Kamalzadeh', 'Abbas Ahmadi', 'Saeed Mansour']","['cs.LG', 'cs.NE', 'stat.ML']",2019-12-05 06:22:04+00:00
http://arxiv.org/abs/1912.02400v2,Covariance Matrix Adaptation for the Rapid Illumination of Behavior Space,"We focus on the challenge of finding a diverse collection of quality
solutions on complex continuous domains. While quality diver-sity (QD)
algorithms like Novelty Search with Local Competition (NSLC) and MAP-Elites are
designed to generate a diverse range of solutions, these algorithms require a
large number of evaluations for exploration of continuous spaces. Meanwhile,
variants of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) are
among the best-performing derivative-free optimizers in single-objective
continuous domains. This paper proposes a new QD algorithm called Covariance
Matrix Adaptation MAP-Elites (CMA-ME). Our new algorithm combines the
self-adaptation techniques of CMA-ES with archiving and mapping techniques for
maintaining diversity in QD. Results from experiments based on standard
continuous optimization benchmarks show that CMA-ME finds better-quality
solutions than MAP-Elites; similarly, results on the strategic game Hearthstone
show that CMA-ME finds both a higher overall quality and broader diversity of
strategies than both CMA-ES and MAP-Elites. Overall, CMA-ME more than doubles
the performance of MAP-Elites using standard QD performance metrics. These
results suggest that QD algorithms augmented by operators from state-of-the-art
optimization algorithms can yield high-performing methods for simultaneously
exploring and optimizing continuous search spaces, with significant
applications to design, testing, and reinforcement learning among other
domains.","['Matthew C. Fontaine', 'Julian Togelius', 'Stefanos Nikolaidis', 'Amy K. Hoover']","['cs.LG', 'stat.ML']",2019-12-05 06:06:42+00:00
http://arxiv.org/abs/1912.02399v2,A sparse negative binomial mixture model for clustering RNA-seq count data,"Clustering with variable selection is a challenging yet critical task for
modern small-n-large-p data. Existing methods based on sparse Gaussian mixture
models or sparse K-means provide solutions to continuous data. With the
prevalence of RNA-seq technology and lack of count data modeling for
clustering, the current practice is to normalize count expression data into
continuous measures and apply existing models with Gaussian assumption. In this
paper, we develop a negative binomial mixture model with lasso or fused lasso
gene regularization to cluster samples (small n) with high-dimensional gene
features (large p). EM algorithm and Bayesian information criterion are used
for inference and determining tuning parameters. The method is compared with
existing methods using extensive simulations and two real transcriptomic
applications in rat brain and breast cancer studies. The result shows superior
performance of the proposed count data model in clustering accuracy, feature
selection and biological interpretation in pathways.","['Tanbin Rahman', 'Yujia Li', 'Tianzhou Ma', 'Lu Tang', 'George Tseng']","['stat.ML', 'cs.LG']",2019-12-05 05:55:36+00:00
http://arxiv.org/abs/1912.02392v3,KoPA: Automated Kronecker Product Approximation,"We consider the problem of matrix approximation and denoising induced by the
Kronecker product decomposition. Specifically, we propose to approximate a
given matrix by the sum of a few Kronecker products of matrices, which we refer
to as the Kronecker product approximation (KoPA). Because the Kronecker product
is an extension of the outer product from vectors to matrices, KoPA extends the
low rank matrix approximation, and includes it as a special case. Comparing
with the latter, KoPA also offers a greater flexibility, since it allows the
user to choose the configuration, which are the dimensions of the two smaller
matrices forming the Kronecker product. On the other hand, the configuration to
be used is usually unknown, and needs to be determined from the data in order
to achieve the optimal balance between accuracy and parsimony. We propose to
use extended information criteria to select the configuration. Under the
paradigm of high dimensional analysis, we show that the proposed procedure is
able to select the true configuration with probability tending to one, under
suitable conditions on the signal-to-noise ratio. We demonstrate the
superiority of KoPA over the low rank approximations through numerical studies,
and several benchmark image examples.","['Chencheng Cai', 'Rong Chen', 'Han Xiao']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2019-12-05 05:27:01+00:00
http://arxiv.org/abs/1912.02390v1,Towards Robust Relational Causal Discovery,"We consider the problem of learning causal relationships from relational
data. Existing approaches rely on queries to a relational conditional
independence (RCI) oracle to establish and orient causal relations in such a
setting. In practice, queries to a RCI oracle have to be replaced by reliable
tests for RCI against available data. Relational data present several unique
challenges in testing for RCI. We study the conditions under which traditional
iid-based conditional independence (CI) tests yield reliable answers to RCI
queries against relational data. We show how to conduct CI tests against
relational data to robustly recover the underlying relational causal structure.
Results of our experiments demonstrate the effectiveness of our proposed
approach.","['Sanghack Lee', 'Vasant Honavar']","['cs.LG', 'cs.AI', 'stat.ML']",2019-12-05 05:13:22+00:00
http://arxiv.org/abs/1912.02386v1,"The Search for Sparse, Robust Neural Networks","Recent work on deep neural network pruning has shown there exist sparse
subnetworks that achieve equal or improved accuracy, training time, and loss
using fewer network parameters when compared to their dense counterparts.
Orthogonal to pruning literature, deep neural networks are known to be
susceptible to adversarial examples, which may pose risks in security- or
safety-critical applications. Intuition suggests that there is an inherent
trade-off between sparsity and robustness such that these characteristics could
not co-exist. We perform an extensive empirical evaluation and analysis testing
the Lottery Ticket Hypothesis with adversarial training and show this approach
enables us to find sparse, robust neural networks. Code for reproducing
experiments is available here:
https://github.com/justincosentino/robust-sparse-networks.","['Justin Cosentino', 'Federico Zaiter', 'Dan Pei', 'Jun Zhu']","['cs.LG', 'stat.ML']",2019-12-05 05:06:35+00:00
http://arxiv.org/abs/1912.02379v2,Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline,"Prior work in visual dialog has focused on training deep neural models on
VisDial in isolation. Instead, we present an approach to leverage pretraining
on related vision-language datasets before transferring to visual dialog. We
adapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn
visually-grounded conversations. Our model is pretrained on the Conceptual
Captions and Visual Question Answering datasets, and finetuned on VisDial. Our
best single model outperforms prior published work (including model ensembles)
by more than 1% absolute on NDCG and MRR. Next, we find that additional
finetuning using ""dense"" annotations in VisDial leads to even higher NDCG --
more than 10% over our base model -- but hurts MRR -- more than 17% below our
base model! This highlights a trade-off between the two primary metrics -- NDCG
and MRR -- which we find is due to dense annotations not correlating well with
the original ground-truth answers to questions.","['Vishvak Murahari', 'Dhruv Batra', 'Devi Parikh', 'Abhishek Das']","['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']",2019-12-05 04:51:11+00:00
http://arxiv.org/abs/1912.03133v1,Why Should we Combine Training and Post-Training Methods for Out-of-Distribution Detection?,"Deep neural networks are known to achieve superior results in classification
tasks. However, it has been recently shown that they are incapable to detect
examples that are generated by a distribution which is different than the one
they have been trained on since they are making overconfident prediction for
Out-Of-Distribution (OOD) examples. OOD detection has attracted a lot of
attention recently. In this paper, we review some of the most seminal recent
algorithms in the OOD detection field, we divide those methods into training
and post-training and we experimentally show how the combination of the former
with the latter can achieve state-of-the-art results in the OOD detection task.","['Aristotelis-Angelos Papadopoulos', 'Nazim Shaikh', 'Mohammad Reza Rajati']","['cs.LG', 'cs.CV', 'stat.ML']",2019-12-05 04:24:14+00:00
http://arxiv.org/abs/1912.02373v1,Modeling and Prediction of Iran's Steel Consumption Based on Economic Activity Using Support Vector Machines,"The steel industry has great impacts on the economy and the environment of
both developed and underdeveloped countries. The importance of this industry
and these impacts have led many researchers to investigate the relationship
between a country's steel consumption and its economic activity resulting in
the so-called intensity of use model. This paper investigates the validity of
the intensity of use model for the case of Iran's steel consumption and extends
this hypothesis by using the indexes of economic activity to model the steel
consumption. We use the proposed model to train support vector machines and
predict the future values for Iran's steel consumption. The paper provides
detailed correlation tests for the factors used in the model to check for their
relationships with the steel consumption. The results indicate that Iran's
steel consumption is strongly correlated with its economic activity following
the same pattern as the economy has been in the last four decades.","['Hossein Kamalzadeh', 'Saeid Nassim Sobhan', 'Azam Boskabadi', 'Mohsen Hatami', 'Amin Gharehyakheh']","['econ.GN', 'cs.LG', 'q-fin.EC', 'stat.ML']",2019-12-05 04:13:35+00:00
http://arxiv.org/abs/1912.02368v3,Inter-Level Cooperation in Hierarchical Reinforcement Learning,"Hierarchies of temporally decoupled policies present a promising approach for
enabling structured exploration in complex long-term planning problems. To
fully achieve this approach an end-to-end training paradigm is needed. However,
training these multi-level policies has had limited success due to challenges
arising from interactions between the goal-assigning and goal-achieving levels
within a hierarchy. In this article, we consider the policy optimization
process as a multi-agent process. This allows us to draw on connections between
communication and cooperation in multi-agent RL, and demonstrate the benefits
of increased cooperation between sub-policies on the training performance of
the overall policy. We introduce a simple yet effective technique for inducing
inter-level cooperation by modifying the objective function and subsequent
gradients of higher-level policies. Experimental results on a wide variety of
simulated robotics and traffic control tasks demonstrate that inducing
cooperation results in stronger performing policies and increased sample
efficiency on a set of difficult long time horizon tasks. We also find that
goal-conditioned policies trained using our method display better transfer to
new tasks, highlighting the benefits of our method in learning task-agnostic
lower-level behaviors. Videos and code are available at:
https://sites.google.com/berkeley.edu/cooperative-hrl.","['Abdul Rahman Kreidieh', 'Glen Berseth', 'Brandon Trabucco', 'Samyak Parajuli', 'Sergey Levine', 'Alexandre M. Bayen']","['cs.LG', 'cs.AI', 'stat.ML']",2019-12-05 03:56:44+00:00
http://arxiv.org/abs/1912.03132v2,New Potential-Based Bounds for the Geometric-Stopping Version of Prediction with Expert Advice,"This work addresses the classic machine learning problem of online prediction
with expert advice. A new potential-based framework for the fixed horizon
version of this problem has been recently developed using verification
arguments from optimal control theory. This paper extends this framework to the
random (geometric) stopping version. To obtain explicit bounds, we construct
potentials for the geometric version from potentials used for the fixed horizon
version of the problem. This construction leads to new explicit lower and upper
bounds associated with specific adversary and player strategies. While there
are several known lower bounds in the fixed horizon setting, our lower bounds
appear to be the first such results in the geometric stopping setting with an
arbitrary number of experts. Our framework also leads in some cases to improved
upper bounds. For two and three experts, our bounds are optimal to leading
order.","['Vladimir A. Kobzar', 'Robert V. Kohn', 'Zhilei Wang']","['cs.LG', 'cs.GT', 'math.AP', 'math.OC', 'stat.ML', '35Q93, 35Q68, 49L20, 68W27, 91A05, 93C20', 'I.2.8']",2019-12-05 03:53:55+00:00
http://arxiv.org/abs/1912.02365v2,Lower Bounds for Non-Convex Stochastic Optimization,"We lower bound the complexity of finding $\epsilon$-stationary points (with
gradient norm at most $\epsilon$) using stochastic first-order methods. In a
well-studied model where algorithms access smooth, potentially non-convex
functions through queries to an unbiased stochastic gradient oracle with
bounded variance, we prove that (in the worst case) any algorithm requires at
least $\epsilon^{-4}$ queries to find an $\epsilon$ stationary point. The lower
bound is tight, and establishes that stochastic gradient descent is minimax
optimal in this model. In a more restrictive model where the noisy gradient
estimates satisfy a mean-squared smoothness property, we prove a lower bound of
$\epsilon^{-3}$ queries, establishing the optimality of recently proposed
variance reduction techniques.","['Yossi Arjevani', 'Yair Carmon', 'John C. Duchi', 'Dylan J. Foster', 'Nathan Srebro', 'Blake Woodworth']","['math.OC', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2019-12-05 03:37:44+00:00
http://arxiv.org/abs/1912.02351v1,Probabilistically-autoencoded horseshoe-disentangled multidomain item-response theory models,"Item response theory (IRT) is a non-linear generative probabilistic paradigm
for using exams to identify, quantify, and compare latent traits of
individuals, relative to their peers, within a population of interest. In
pre-existing multidimensional IRT methods, one requires a factorization of the
test items. For this task, linear exploratory factor analysis is used, making
IRT a posthoc model. We propose skipping the initial factor analysis by using a
sparsity-promoting horseshoe prior to perform factorization directly within the
IRT model so that all training occurs in a single self-consistent step. Being a
hierarchical Bayesian model, we adapt the WAIC to the problem of dimensionality
selection. IRT models are analogous to probabilistic autoencoders. By binding
the generative IRT model to a Bayesian neural network (forming a probabilistic
autoencoder), one obtains a scoring algorithm consistent with the interpretable
Bayesian model. In some IRT applications the black-box nature of a neural
network scoring machine is desirable. In this manuscript, we demonstrate
within-IRT factorization and comment on scoring approaches.","['Joshua C. Chang', 'Shashaank Vattikuti', 'Carson C. Chow']","['cs.LG', 'stat.ML']",2019-12-05 02:28:47+00:00
http://arxiv.org/abs/1912.02338v1,RoNGBa: A Robustly Optimized Natural Gradient Boosting Training Approach with Leaf Number Clipping,"Natural gradient has been recently introduced to the field of boosting to
enable the generic probabilistic predication capability. Natural gradient
boosting shows promising performance improvements on small datasets due to
better training dynamics, but it suffers from slow training speed overhead
especially for large datasets. We present a replication study of NGBoost(Duan
et al., 2019) training that carefully examines the impacts of key
hyper-parameters under the circumstance of best-first decision tree learning.
We find that with the regularization of leaf number clipping, the performance
of NGBoost can be largely improved via a better choice of hyperparameters.
Experiments show that our approach significantly beats the state-of-the-art
performance on various kinds of datasets from the UCI Machine Learning
Repository while still has up to 4.85x speed up compared with the original
approach of NGBoost.","['Liliang Ren', 'Gen Sun', 'Jiaman Wu']","['cs.LG', 'stat.ML']",2019-12-05 01:38:34+00:00
http://arxiv.org/abs/1912.02292v1,Deep Double Descent: Where Bigger Models and More Data Hurt,"We show that a variety of modern deep learning tasks exhibit a
""double-descent"" phenomenon where, as we increase model size, performance first
gets worse and then gets better. Moreover, we show that double descent occurs
not just as a function of model size, but also as a function of the number of
training epochs. We unify the above phenomena by defining a new complexity
measure we call the effective model complexity and conjecture a generalized
double descent with respect to this measure. Furthermore, our notion of model
complexity allows us to identify certain regimes where increasing (even
quadrupling) the number of train samples actually hurts test performance.","['Preetum Nakkiran', 'Gal Kaplun', 'Yamini Bansal', 'Tristan Yang', 'Boaz Barak', 'Ilya Sutskever']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2019-12-04 22:47:31+00:00
http://arxiv.org/abs/1912.02290v5,Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning,"We place an Indian Buffet process (IBP) prior over the structure of a
Bayesian Neural Network (BNN), thus allowing the complexity of the BNN to
increase and decrease automatically. We further extend this model such that the
prior on the structure of each hidden layer is shared globally across all
layers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of
resource allocation in Continual Learning (CL) where new tasks occur and the
network requires extra resources. Our model uses online variational inference
with reparameterisation of the Bernoulli and Beta distributions, which
constitute the IBP and H-IBP priors. As we automatically learn the number of
weights in each layer of the BNN, overfitting and underfitting problems are
largely overcome. We show empirically that our approach offers a competitive
edge over existing methods in CL.","['Samuel Kessler', 'Vu Nguyen', 'Stefan Zohren', 'Stephen Roberts']","['stat.ML', 'cs.LG']",2019-12-04 22:43:31+00:00
http://arxiv.org/abs/2001.01647v1,Are skip connections necessary for biologically plausible learning rules?,"Backpropagation is the workhorse of deep learning, however, several other
biologically-motivated learning rules have been introduced, such as random
feedback alignment and difference target propagation. None of these methods
have produced a competitive performance against backpropagation. In this paper,
we show that biologically-motivated learning rules with skip connections
between intermediate layers can perform as well as backpropagation on the MNIST
dataset and are robust to various sets of hyper-parameters.","['Daniel Jiwoong Im', 'Rutuja Patil', 'Kristin Branson']","['cs.NE', 'cs.LG', 'stat.ML']",2019-12-04 22:21:16+00:00
http://arxiv.org/abs/1912.02280v2,Natural Alpha Embeddings,"Learning an embedding for a large collection of items is a popular approach
to overcome the computational limitations associated to one-hot encodings. The
aim of item embedding is to learn a low dimensional space for the
representations, able to capture with its geometry relevant features or
relationships for the data at hand. This can be achieved for example by
exploiting adjacencies among items in large sets of unlabelled data. In this
paper we interpret in an Information Geometric framework the item embeddings
obtained from conditional models. By exploiting the $\alpha$-geometry of the
exponential family, first introduced by Amari, we introduce a family of natural
$\alpha$-embeddings represented by vectors in the tangent space of the
probability simplex, which includes as a special case standard approaches
available in the literature. A typical example is given by word embeddings,
commonly used in natural language processing, such as Word2Vec and GloVe. In
our analysis, we show how the $\alpha$-deformation parameter can impact on
standard evaluation tasks.","['Riccardo Volpi', 'Luigi MalagÃ²']","['cs.LG', 'stat.ML']",2019-12-04 22:13:16+00:00
http://arxiv.org/abs/1912.02279v4,Angular Visual Hardness,"Recent convolutional neural networks (CNNs) have led to impressive
performance but often suffer from poor calibration. They tend to be
overconfident, with the model confidence not always reflecting the underlying
true ambiguity and hardness. In this paper, we propose angular visual hardness
(AVH), a score given by the normalized angular distance between the sample
feature embedding and the target classifier to measure sample hardness. We
validate this score with an in-depth and extensive scientific study, and
observe that CNN models with the highest accuracy also have the best AVH
scores. This agrees with an earlier finding that state-of-art models improve on
the classification of harder examples. We observe that the training dynamics of
AVH is vastly different compared to the training loss. Specifically, AVH
quickly reaches a plateau for all samples even though the training loss keeps
improving. This suggests the need for designing better loss functions that can
target harder examples more effectively. We also find that AVH has a
statistically significant correlation with human visual hardness. Finally, we
demonstrate the benefit of AVH to a variety of applications such as
self-training for domain adaptation and domain generalization.","['Beidi Chen', 'Weiyang Liu', 'Zhiding Yu', 'Jan Kautz', 'Anshumali Shrivastava', 'Animesh Garg', 'Anima Anandkumar']","['cs.LG', 'cs.CV', 'stat.ML']",2019-12-04 22:12:42+00:00
http://arxiv.org/abs/1912.02276v1,Enhancing Stratospheric Weather Analyses and Forecasts by Deploying Sensors from a Weather Balloon,"The ability to analyze and forecast stratospheric weather conditions is
fundamental to addressing climate change. However, our capacity to collect data
in the stratosphere is limited by sparsely deployed weather balloons. We
propose a framework to collect stratospheric data by releasing a contrail of
tiny sensor devices as a weather balloon ascends. The key machine learning
challenges are determining when and how to deploy a finite collection of
sensors to produce a useful data set. We decide when to release sensors by
modeling the deviation of a forecast from actual stratospheric conditions as a
Gaussian process. We then implement a novel hardware system that is capable of
optimally releasing sensors from a rising weather balloon. We show that this
data engineering framework is effective through real weather balloon flights,
as well as simulations.","['Kiwan Maeng', 'Iskender Kushan', 'Brandon Lucia', 'Ashish Kapoor']","['cs.LG', 'stat.ML']",2019-12-04 22:07:05+00:00
http://arxiv.org/abs/1912.02260v1,The effect of task and training on intermediate representations in convolutional neural networks revealed with modified RV similarity analysis,"Centered Kernel Alignment (CKA) was recently proposed as a similarity metric
for comparing activation patterns in deep networks. Here we experiment with the
modified RV-coefficient (RV2), which has very similar properties as CKA while
being less sensitive to dataset size. We compare the representations of
networks that received varying amounts of training on different layers: a
standard trained network (all parameters updated at every step), a freeze
trained network (layers gradually frozen during training), random networks
(only some layers trained), and a completely untrained network. We found that
RV2 was able to recover expected similarity patterns and provide interpretable
similarity matrices that suggested hypotheses about how representations are
affected by different training recipes. We propose that the superior
performance achieved by freeze training can be attributed to representational
differences in the penultimate layer. Our comparisons of random networks
suggest that the inputs and targets serve as anchors on the representations in
the lowest and highest layers.","['Jessica A. F. Thompson', 'Yoshua Bengio', 'Marc Schoenwiesner']","['cs.LG', 'stat.ML']",2019-12-04 21:43:57+00:00
http://arxiv.org/abs/1912.02258v1,A Survey of Game Theoretic Approaches for Adversarial Machine Learning in Cybersecurity Tasks,"Machine learning techniques are currently used extensively for automating
various cybersecurity tasks. Most of these techniques utilize supervised
learning algorithms that rely on training the algorithm to classify incoming
data into different categories, using data encountered in the relevant domain.
A critical vulnerability of these algorithms is that they are susceptible to
adversarial attacks where a malicious entity called an adversary deliberately
alters the training data to misguide the learning algorithm into making
classification errors. Adversarial attacks could render the learning algorithm
unsuitable to use and leave critical systems vulnerable to cybersecurity
attacks. Our paper provides a detailed survey of the state-of-the-art
techniques that are used to make a machine learning algorithm robust against
adversarial attacks using the computational framework of game theory. We also
discuss open problems and challenges and possible directions for further
research that would make deep machine learning-based systems more robust and
reliable for cybersecurity tasks.","['Prithviraj Dasgupta', 'Joseph B. Collins']","['cs.CR', 'cs.AI', 'cs.LG', 'stat.ML', '68T05']",2019-12-04 21:42:15+00:00
