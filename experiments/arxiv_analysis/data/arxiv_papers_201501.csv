id,title,abstract,authors,categories,date
http://arxiv.org/abs/1503.00778v1,"Simple, Efficient, and Neural Algorithms for Sparse Coding","Sparse coding is a basic task in many fields including signal processing,
neuroscience and machine learning where the goal is to learn a basis that
enables a sparse representation of a given set of data, if one exists. Its
standard formulation is as a non-convex optimization problem which is solved in
practice by heuristics based on alternating minimization. Re- cent work has
resulted in several algorithms for sparse coding with provable guarantees, but
somewhat surprisingly these are outperformed by the simple alternating
minimization heuristics. Here we give a general framework for understanding
alternating minimization which we leverage to analyze existing heuristics and
to design new ones also with provable guarantees. Some of these algorithms seem
implementable on simple neural architectures, which was the original motivation
of Olshausen and Field (1997a) in introducing sparse coding. We also give the
first efficient algorithm for sparse coding that works almost up to the
information theoretic limit for sparse recovery on incoherent dictionaries. All
previous algorithms that approached or surpassed this limit run in time
exponential in some natural parameter. Finally, our algorithms improve upon the
sample complexity of existing approaches. We believe that our analysis
framework will have applications in other settings where simple iterative
algorithms are used.","['Sanjeev Arora', 'Rong Ge', 'Tengyu Ma', 'Ankur Moitra']","['cs.LG', 'cs.DS', 'cs.NE', 'stat.ML']",2015-03-02 23:02:56+00:00
http://arxiv.org/abs/1503.00759v3,A Review of Relational Machine Learning for Knowledge Graphs,"Relational machine learning studies methods for the statistical analysis of
relational, or graph-structured, data. In this paper, we provide a review of
how such statistical models can be ""trained"" on large knowledge graphs, and
then used to predict new facts about the world (which is equivalent to
predicting new edges in the graph). In particular, we discuss two fundamentally
different kinds of statistical relational models, both of which can scale to
massive datasets. The first is based on latent feature models such as tensor
factorization and multiway neural networks. The second is based on mining
observable patterns in the graph. We also show how to combine these latent and
observable models to get improved modeling power at decreased computational
cost. Finally, we discuss how such statistical models of graphs can be combined
with text-based information extraction methods for automatically constructing
knowledge graphs from the Web. To this end, we also discuss Google's Knowledge
Vault project as an example of such combination.","['Maximilian Nickel', 'Kevin Murphy', 'Volker Tresp', 'Evgeniy Gabrilovich']","['stat.ML', 'cs.LG']",2015-03-02 21:35:41+00:00
http://arxiv.org/abs/1503.00693v1,Bayesian Optimization of Text Representations,"When applying machine learning to problems in NLP, there are many choices to
make about how to represent input texts. These choices can have a big effect on
performance, but they are often uninteresting to researchers or practitioners
who simply need a module that performs well. We propose an approach to
optimizing over this space of choices, formulating the problem as global
optimization. We apply a sequential model-based optimization technique and show
that our method makes standard linear models competitive with more
sophisticated, expensive state-of-the-art methods based on latent variable
models or neural networks on various topic classification and sentiment
analysis problems. Our approach is a first step towards black-box NLP systems
that work with raw text and do not require manual tuning.","['Dani Yogatama', 'Noah A. Smith']","['cs.CL', 'cs.LG', 'stat.ML']",2015-03-02 20:23:18+00:00
http://arxiv.org/abs/1503.00690v2,A Hebbian/Anti-Hebbian Network for Online Sparse Dictionary Learning Derived from Symmetric Matrix Factorization,"Olshausen and Field (OF) proposed that neural computations in the primary
visual cortex (V1) can be partially modeled by sparse dictionary learning. By
minimizing the regularized representation error they derived an online
algorithm, which learns Gabor-filter receptive fields from a natural image
ensemble in agreement with physiological experiments. Whereas the OF algorithm
can be mapped onto the dynamics and synaptic plasticity in a single-layer
neural network, the derived learning rule is nonlocal - the synaptic weight
update depends on the activity of neurons other than just pre- and postsynaptic
ones - and hence biologically implausible. Here, to overcome this problem, we
derive sparse dictionary learning from a novel cost-function - a regularized
error of the symmetric factorization of the input's similarity matrix. Our
algorithm maps onto a neural network of the same architecture as OF but using
only biologically plausible local learning rules. When trained on natural
images our network learns Gabor-filter receptive fields and reproduces the
correlation among synaptic weights hard-wired in the OF network. Therefore,
online symmetric matrix factorization may serve as an algorithmic theory of
neural computation.","['Tao Hu', 'Cengiz Pehlevan', 'Dmitri B. Chklovskii']","['q-bio.NC', 'cs.NE', 'stat.ML']",2015-03-02 20:16:19+00:00
http://arxiv.org/abs/1503.00687v1,A review of mean-shift algorithms for clustering,"A natural way to characterize the cluster structure of a dataset is by
finding regions containing a high density of data. This can be done in a
nonparametric way with a kernel density estimate, whose modes and hence
clusters can be found using mean-shift algorithms. We describe the theory and
practice behind clustering based on kernel density estimates and mean-shift
algorithms. We discuss the blurring and non-blurring versions of mean-shift;
theoretical results about mean-shift algorithms and Gaussian mixtures;
relations with scale-space theory, spectral clustering and other algorithms;
extensions to tracking, to manifold and graph data, and to manifold denoising;
K-modes and Laplacian K-modes algorithms; acceleration strategies for large
datasets; and applications to image segmentation, manifold denoising and
multivalued regression.",['Miguel Á. Carreira-Perpiñán'],"['cs.LG', 'cs.CV', 'stat.ML']",2015-03-02 20:09:14+00:00
http://arxiv.org/abs/1503.00680v1,A Hebbian/Anti-Hebbian Network Derived from Online Non-Negative Matrix Factorization Can Cluster and Discover Sparse Features,"Despite our extensive knowledge of biophysical properties of neurons, there
is no commonly accepted algorithmic theory of neuronal function. Here we
explore the hypothesis that single-layer neuronal networks perform online
symmetric nonnegative matrix factorization (SNMF) of the similarity matrix of
the streamed data. By starting with the SNMF cost function we derive an online
algorithm, which can be implemented by a biologically plausible network with
local learning rules. We demonstrate that such network performs soft clustering
of the data as well as sparse feature discovery. The derived algorithm
replicates many known aspects of sensory anatomy and biophysical properties of
neurons including unipolar nature of neuronal activity and synaptic weights,
local synaptic plasticity rules and the dependence of learning rate on
cumulative neuronal activity. Thus, we make a step towards an algorithmic
theory of neuronal function, which should facilitate large-scale neural circuit
simulations and biologically inspired artificial intelligence.","['Cengiz Pehlevan', 'Dmitri B. Chklovskii']","['q-bio.NC', 'cs.NE', 'stat.ML']",2015-03-02 19:57:28+00:00
http://arxiv.org/abs/1503.00669v1,A Hebbian/Anti-Hebbian Neural Network for Linear Subspace Learning: A Derivation from Multidimensional Scaling of Streaming Data,"Neural network models of early sensory processing typically reduce the
dimensionality of streaming input data. Such networks learn the principal
subspace, in the sense of principal component analysis (PCA), by adjusting
synaptic weights according to activity-dependent learning rules. When derived
from a principled cost function these rules are nonlocal and hence biologically
implausible. At the same time, biologically plausible local rules have been
postulated rather than derived from a principled cost function. Here, to bridge
this gap, we derive a biologically plausible network for subspace learning on
streaming data by minimizing a principled cost function. In a departure from
previous work, where cost was quantified by the representation, or
reconstruction, error, we adopt a multidimensional scaling (MDS) cost function
for streaming data. The resulting algorithm relies only on biologically
plausible Hebbian and anti-Hebbian local learning rules. In a stochastic
setting, synaptic weights converge to a stationary state which projects the
input data onto the principal subspace. If the data are generated by a
nonstationary distribution, the network can track the principal subspace. Thus,
our result makes a step towards an algorithmic theory of neural computation.","['Cengiz Pehlevan', 'Tao Hu', 'Dmitri B. Chklovskii']","['q-bio.NC', 'cs.NE', 'stat.ML']",2015-03-02 19:39:33+00:00
http://arxiv.org/abs/1503.00623v2,Unregularized Online Learning Algorithms with General Loss Functions,"In this paper, we consider unregularized online learning algorithms in a
Reproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicit
convergence rates of the unregularized online learning algorithms for
classification associated with a general gamma-activating loss (see Definition
1 in the paper). Our results extend and refine the results in Ying and Pontil
(2008) for the least-square loss and the recent result in Bach and Moulines
(2011) for the loss function with a Lipschitz-continuous gradient. Moreover, we
establish a very general condition on the step sizes which guarantees the
convergence of the last iterate of such algorithms. Secondly, we establish, for
the first time, the convergence of the unregularized pairwise learning
algorithm with a general loss function and derive explicit rates under the
assumption of polynomially decaying step sizes. Concrete examples are used to
illustrate our main results. The main techniques are tools from convex
analysis, refined inequalities of Gaussian averages, and an induction approach.","['Yiming Ying', 'Ding-Xuan Zhou']","['cs.LG', 'stat.ML']",2015-03-02 17:21:23+00:00
http://arxiv.org/abs/1503.00547v1,"Recovering PCA from Hybrid-$(\ell_1,\ell_2)$ Sparse Sampling of Data Elements","This paper addresses how well we can recover a data matrix when only given a
few of its elements. We present a randomized algorithm that element-wise
sparsifies the data, retaining only a few its elements. Our new algorithm
independently samples the data using sampling probabilities that depend on both
the squares ($\ell_2$ sampling) and absolute values ($\ell_1$ sampling) of the
entries. We prove that the hybrid algorithm recovers a near-PCA reconstruction
of the data from a sublinear sample-size: hybrid-($\ell_1,\ell_2$) inherits the
$\ell_2$-ability to sample the important elements as well as the regularization
properties of $\ell_1$ sampling, and gives strictly better performance than
either $\ell_1$ or $\ell_2$ on their own. We also give a one-pass version of
our algorithm and show experiments to corroborate the theory.","['Abhisek Kundu', 'Petros Drineas', 'Malik Magdon-Ismail']","['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2015-03-02 14:34:48+00:00
http://arxiv.org/abs/1503.00338v1,Phase Transitions in Sparse PCA,"We study optimal estimation for sparse principal component analysis when the
number of non-zero elements is small but on the same order as the dimension of
the data. We employ approximate message passing (AMP) algorithm and its state
evolution to analyze what is the information theoretically minimal mean-squared
error and the one achieved by AMP in the limit of large sizes. For a special
case of rank one and large enough density of non-zeros Deshpande and Montanari
[1] proved that AMP is asymptotically optimal. We show that both for low
density and for large rank the problem undergoes a series of phase transitions
suggesting existence of a region of parameters where estimation is information
theoretically possible, but AMP (and presumably every other polynomial
algorithm) fails. The analysis of the large rank limit is particularly
instructive.","['Thibault Lesieur', 'Florent Krzakala', 'Lenka Zdeborova']","['cs.IT', 'cond-mat.stat-mech', 'math.IT', 'stat.ML']",2015-03-01 19:26:39+00:00
http://arxiv.org/abs/1503.00332v3,JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes,"Markov jump processes (MJPs) are used to model a wide range of phenomena from
disease progression to RNA path folding. However, maximum likelihood estimation
of parametric models leads to degenerate trajectories and inferential
performance is poor in nonparametric models. We take a small-variance
asymptotics (SVA) approach to overcome these limitations. We derive the
small-variance asymptotics for parametric and nonparametric MJPs for both
directly observed and hidden state models. In the parametric case we obtain a
novel objective function which leads to non-degenerate trajectories. To derive
the nonparametric version we introduce the gamma-gamma process, a novel
extension to the gamma-exponential process. We propose algorithms for each of
these formulations, which we call \emph{JUMP-means}. Our experiments
demonstrate that JUMP-means is competitive with or outperforms widely used MJP
inference approaches in terms of both speed and reconstruction accuracy.","['Jonathan H. Huggins', 'Karthik Narasimhan', 'Ardavan Saeedi', 'Vikash K. Mansinghka']","['stat.ML', 'cs.LG']",2015-03-01 18:59:12+00:00
http://arxiv.org/abs/1503.00323v1,Sparse Approximation of a Kernel Mean,"Kernel means are frequently used to represent probability distributions in
machine learning problems. In particular, the well known kernel density
estimator and the kernel mean embedding both have the form of a kernel mean.
Unfortunately, kernel means are faced with scalability issues. A single point
evaluation of the kernel density estimator, for example, requires a computation
time linear in the training sample size. To address this challenge, we present
a method to efficiently construct a sparse approximation of a kernel mean. We
do so by first establishing an incoherence-based bound on the approximation
error, and then noticing that, for the case of radial kernels, the bound can be
minimized by solving the $k$-center problem. The outcome is a linear time
construction of a sparse kernel mean, which also lends itself naturally to an
automatic sparsity selection scheme. We show the computational gains of our
method by looking at three problems involving kernel means: Euclidean embedding
of distributions, class proportion estimation, and clustering using the
mean-shift algorithm.","['E. Cruz Cortés', 'C. Scott']","['stat.ML', 'cs.LG']",2015-03-01 18:30:07+00:00
http://arxiv.org/abs/1503.00282v1,Constructive sparse trigonometric approximation for functions with small mixed smoothness,"The paper gives a constructive method, based on greedy algorithms, that
provides for the classes of functions with small mixed smoothness the best
possible in the sense of order approximation error for the $m$-term
approximation with respect to the trigonometric system.",['V. N. Temlyakov'],"['math.NA', 'stat.ML', 'primary: 41A65, secondary: 42A10, 46B20']",2015-03-01 14:13:41+00:00
http://arxiv.org/abs/1503.00269v2,Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification,"Improvement guarantees for semi-supervised classifiers can currently only be
given under restrictive conditions on the data. We propose a general way to
perform semi-supervised parameter estimation for likelihood-based classifiers
for which, on the full training set, the estimates are never worse than the
supervised solution in terms of the log-likelihood. We argue, moreover, that we
may expect these solutions to really improve upon the supervised classifier in
particular cases. In a worked-out example for LDA, we take it one step further
and essentially prove that its semi-supervised version is strictly better than
its supervised counterpart. The two new concepts that form the core of our
estimation principle are contrast and pessimism. The former refers to the fact
that our objective function takes the supervised estimates into account,
enabling the semi-supervised solution to explicitly control the potential
improvements over this estimate. The latter refers to the fact that our
estimates are conservative and therefore resilient to whatever form the true
labeling of the unlabeled data takes on. Experiments demonstrate the
improvements in terms of both the log-likelihood and the classification error
rate on independent test sets.",['Marco Loog'],"['stat.ML', 'cs.LG', 'stat.ME', '68T10 (Primary), 62Fxx (Secondary), 62H12, 97K80, 68T05', 'I.2.6; I.5.1']",2015-03-01 13:16:43+00:00
http://arxiv.org/abs/1503.00214v3,Matrix Completion with Noisy Entries and Outliers,"This paper considers the problem of matrix completion when the observed
entries are noisy and contain outliers. It begins with introducing a new
optimization criterion for which the recovered matrix is defined as its
solution. This criterion uses the celebrated Huber function from the robust
statistics literature to downweigh the effects of outliers. A practical
algorithm is developed to solve the optimization involved. This algorithm is
fast, straightforward to implement, and monotonic convergent. Furthermore, the
proposed methodology is theoretically shown to be stable in a well defined
sense. Its promising empirical performance is demonstrated via a sequence of
simulation experiments, including image inpainting.","['Raymond K. W. Wong', 'Thomas C. M. Lee']",['stat.ML'],2015-03-01 04:24:42+00:00
http://arxiv.org/abs/1503.00173v6,Signal Processing on Graphs: Causal Modeling of Unstructured Data,"Many applications collect a large number of time series, for example, the
financial data of companies quoted in a stock exchange, the health care data of
all patients that visit the emergency room of a hospital, or the temperature
sequences continuously measured by weather stations across the US. These data
are often referred to as unstructured. A first task in its analytics is to
derive a low dimensional representation, a graph or discrete manifold, that
describes well the interrelations among the time series and their
intrarelations across time. This paper presents a computationally tractable
algorithm for estimating this graph that structures the data. The resulting
graph is directed and weighted, possibly capturing causal relations, not just
reciprocal correlations as in many existing approaches in the literature. A
convergence analysis is carried out. The algorithm is demonstrated on random
graph datasets and real network time series datasets, and its performance is
compared to that of related methods. The adjacency matrices estimated with the
new method are close to the true graph in the simulated data and consistent
with prior physical knowledge in the real dataset tested.","['Jonathan Mei', 'José M. F. Moura']","['cs.IT', 'math.IT', 'stat.ML']",2015-02-28 20:28:05+00:00
http://arxiv.org/abs/1503.00164v2,Analysis of Crowdsourced Sampling Strategies for HodgeRank with Sparse Random Graphs,"Crowdsourcing platforms are now extensively used for conducting subjective
pairwise comparison studies. In this setting, a pairwise comparison dataset is
typically gathered via random sampling, either \emph{with} or \emph{without}
replacement. In this paper, we use tools from random graph theory to analyze
these two random sampling methods for the HodgeRank estimator. Using the
Fiedler value of the graph as a measurement for estimator stability
(informativeness), we provide a new estimate of the Fiedler value for these two
random graph models. In the asymptotic limit as the number of vertices tends to
infinity, we prove the validity of the estimate. Based on our findings, for a
small number of items to be compared, we recommend a two-stage sampling
strategy where a greedy sampling method is used initially and random sampling
\emph{without} replacement is used in the second stage. When a large number of
items is to be compared, we recommend random sampling with replacement as this
is computationally inexpensive and trivially parallelizable. Experiments on
synthetic and real-world datasets support our analysis.","['Braxton Osting', 'Jiechao Xiong', 'Qianqian Xu', 'Yuan Yao']","['stat.ML', 'cs.LG']",2015-02-28 18:32:45+00:00
http://arxiv.org/abs/1503.00135v1,Supervised learning sets benchmark for robust spike detection from calcium imaging signals,"A fundamental challenge in calcium imaging has been to infer the timing of
action potentials from the measured noisy calcium fluorescence traces. We
systematically evaluate a range of spike inference algorithms on a large
benchmark dataset recorded from varying neural tissue (V1 and retina) using
different calcium indicators (OGB-1 and GCamp6). We show that a new algorithm
based on supervised learning in flexible probabilistic models outperforms all
previously published techniques, setting a new standard for spike inference
from calcium signals. Importantly, it performs better than other algorithms
even on datasets not seen during training. Future data acquired in new
experimental conditions can easily be used to further improve its spike
prediction accuracy and generalization performance. Finally, we show that
comparing algorithms on artificial data is not informative about performance on
real population imaging data, suggesting that a benchmark dataset may greatly
facilitate future algorithmic developments.","['Lucas Theis', 'Philipp Berens', 'Emmanouil Froudarakis', 'Jacob Reimer', 'Miroslav Román Rosón', 'Tom Baden', 'Thomas Euler', 'Andreas Tolias', 'Matthias Bethge']","['stat.ML', 'stat.AP']",2015-02-28 14:52:33+00:00
http://arxiv.org/abs/1503.00038v1,Sequential Feature Explanations for Anomaly Detection,"In many applications, an anomaly detection system presents the most anomalous
data instance to a human analyst, who then must determine whether the instance
is truly of interest (e.g. a threat in a security setting). Unfortunately, most
anomaly detectors provide no explanation about why an instance was considered
anomalous, leaving the analyst with no guidance about where to begin the
investigation. To address this issue, we study the problems of computing and
evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE
of an anomaly is a sequence of features, which are presented to the analyst one
at a time (in order) until the information contained in the highlighted
features is enough for the analyst to make a confident judgement about the
anomaly. Since analyst effort is related to the amount of information that they
consider in an investigation, an explanation's quality is related to the number
of features that must be revealed to attain confidence. One of our main
contributions is to present a novel framework for large scale quantitative
evaluations of SFEs, where the quality measure is based on analyst effort. To
do this we construct anomaly detection benchmarks from real data sets along
with artificial experts that can be simulated for evaluation. Our second
contribution is to evaluate several novel explanation approaches within the
framework and on traditional anomaly detection benchmarks, offering several
insights into the approaches.","['Md Amran Siddiqui', 'Alan Fern', 'Thomas G. Dietterich', 'Weng-Keen Wong']","['cs.AI', 'cs.LG', 'stat.ML']",2015-02-28 00:04:11+00:00
http://arxiv.org/abs/1503.00036v2,Norm-Based Capacity Control in Neural Networks,"We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.","['Behnam Neyshabur', 'Ryota Tomioka', 'Nathan Srebro']","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",2015-02-27 23:50:22+00:00
http://arxiv.org/abs/1503.00024v4,Influence Maximization with Bandits,"We consider the problem of \emph{influence maximization}, the problem of
maximizing the number of people that become aware of a product by finding the
`best' set of `seed' users to expose the product to. Most prior work on this
topic assumes that we know the probability of each user influencing each other
user, or we have data that lets us estimate these influences. However, this
information is typically not initially available or is difficult to obtain. To
avoid this assumption, we adopt a combinatorial multi-armed bandit paradigm
that estimates the influence probabilities as we sequentially try different
seed sets. We establish bounds on the performance of this procedure under the
existing edge-level feedback as well as a novel and more realistic node-level
feedback. Beyond our theoretical results, we describe a practical
implementation and experimentally demonstrate its efficiency and effectiveness
on four real datasets.","['Sharan Vaswani', 'Laks. V. S. Lakshmanan', 'Mark Schmidt']","['cs.SI', 'cs.LG', 'stat.ML']",2015-02-27 21:59:08+00:00
http://arxiv.org/abs/1502.08053v1,Stochastic Dual Coordinate Ascent with Adaptive Probabilities,"This paper introduces AdaSDCA: an adaptive variant of stochastic dual
coordinate ascent (SDCA) for solving the regularized empirical risk
minimization problems. Our modification consists in allowing the method
adaptively change the probability distribution over the dual variables
throughout the iterative process. AdaSDCA achieves provably better complexity
bound than SDCA with the best fixed probability distribution, known as
importance sampling. However, it is of a theoretical character as it is
expensive to implement. We also propose AdaSDCA+: a practical variant which in
our experiments outperforms existing non-adaptive methods.","['Dominik Csiba', 'Zheng Qu', 'Peter Richtárik']","['math.OC', 'cs.LG', 'stat.ML']",2015-02-27 20:54:03+00:00
http://arxiv.org/abs/1502.08029v5,Describing Videos by Exploiting Temporal Structure,"Recent progress in using recurrent neural networks (RNNs) for image
description has motivated the exploration of their application for video
description. However, while images are static, working with videos requires
modeling their dynamic temporal structure and then properly integrating that
information into a natural language description. In this context, we propose an
approach that successfully takes into account both the local and global
temporal structure of videos to produce descriptions. First, our approach
incorporates a spatial temporal 3-D convolutional neural network (3-D CNN)
representation of the short temporal dynamics. The 3-D CNN representation is
trained on video action recognition tasks, so as to produce a representation
that is tuned to human motion and behavior. Second we propose a temporal
attention mechanism that allows to go beyond local temporal modeling and learns
to automatically select the most relevant temporal segments given the
text-generating RNN. Our approach exceeds the current state-of-art for both
BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on
a new, larger and more challenging dataset of paired video and natural language
descriptions.","['Li Yao', 'Atousa Torabi', 'Kyunghyun Cho', 'Nicolas Ballas', 'Christopher Pal', 'Hugo Larochelle', 'Aaron Courville']","['stat.ML', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.LG']",2015-02-27 19:30:40+00:00
http://arxiv.org/abs/1502.08009v1,Second-order Quantile Methods for Experts and Combinatorial Games,"We aim to design strategies for sequential decision making that adjust to the
difficulty of the learning problem. We study this question both in the setting
of prediction with expert advice, and for more general combinatorial decision
tasks. We are not satisfied with just guaranteeing minimax regret rates, but we
want our algorithms to perform significantly better on easy data. Two popular
ways to formalize such adaptivity are second-order regret bounds and quantile
bounds. The underlying notions of 'easy data', which may be paraphrased as ""the
learning problem has small variance"" and ""multiple decisions are useful"", are
synergetic. But even though there are sophisticated algorithms that exploit one
of the two, no existing algorithm is able to adapt to both.
  In this paper we outline a new method for obtaining such adaptive algorithms,
based on a potential function that aggregates a range of learning rates (which
are essential tuning parameters). By choosing the right prior we construct
efficient algorithms and show that they reap both benefits by proving the first
bounds that are both second-order and incorporate quantiles.","['Wouter M. Koolen', 'Tim van Erven']","['cs.LG', 'stat.ML']",2015-02-27 18:56:45+00:00
http://arxiv.org/abs/1502.07943v1,Non-stochastic Best Arm Identification and Hyperparameter Optimization,"Motivated by the task of hyperparameter optimization, we introduce the
non-stochastic best-arm identification problem. Within the multi-armed bandit
literature, the cumulative regret objective enjoys algorithms and analyses for
both the non-stochastic and stochastic settings while to the best of our
knowledge, the best-arm identification framework has only been considered in
the stochastic setting. We introduce the non-stochastic setting under this
framework, identify a known algorithm that is well-suited for this setting, and
analyze its behavior. Next, by leveraging the iterative nature of standard
machine learning algorithms, we cast hyperparameter optimization as an instance
of non-stochastic best-arm identification, and empirically evaluate our
proposed algorithm on this task. Our empirical results show that, by allocating
more resources to promising hyperparameter settings, we typically achieve
comparable test accuracies an order of magnitude faster than baseline methods.","['Kevin Jamieson', 'Ameet Talwalkar']","['cs.LG', 'stat.ML']",2015-02-27 15:58:45+00:00
http://arxiv.org/abs/1502.07813v1,Minimum message length estimation of mixtures of multivariate Gaussian and von Mises-Fisher distributions,"Mixture modelling involves explaining some observed evidence using a
combination of probability distributions. The crux of the problem is the
inference of an optimal number of mixture components and their corresponding
parameters. This paper discusses unsupervised learning of mixture models using
the Bayesian Minimum Message Length (MML) criterion. To demonstrate the
effectiveness of search and inference of mixture parameters using the proposed
approach, we select two key probability distributions, each handling
fundamentally different types of data: the multivariate Gaussian distribution
to address mixture modelling of data distributed in Euclidean space, and the
multivariate von Mises-Fisher (vMF) distribution to address mixture modelling
of directional data distributed on a unit hypersphere. The key contributions of
this paper, in addition to the general search and inference methodology,
include the derivation of MML expressions for encoding the data using
multivariate Gaussian and von Mises-Fisher distributions, and the analytical
derivation of the MML estimates of the parameters of the two distributions. Our
approach is tested on simulated and real world data sets. For instance, we
infer vMF mixtures that concisely explain experimentally determined
three-dimensional protein conformations, providing an effective null model
description of protein structures that is central to many inference problems in
structural bioinformatics. The experimental results demonstrate that the
performance of our proposed search and inference method along with the encoding
schemes improve on the state of the art mixture modelling techniques.","['Parthan Kasarapu', 'Lloyd Allison']","['cs.LG', 'stat.ML']",2015-02-27 03:32:49+00:00
http://arxiv.org/abs/1502.07738v3,Achieving Exact Cluster Recovery Threshold via Semidefinite Programming: Extensions,"Resolving a conjecture of Abbe, Bandeira and Hall, the authors have recently
shown that the semidefinite programming (SDP) relaxation of the maximum
likelihood estimator achieves the sharp threshold for exactly recovering the
community structure under the binary stochastic block model of two equal-sized
clusters. The same was shown for the case of a single cluster and outliers.
Extending the proof techniques, in this paper it is shown that SDP relaxations
also achieve the sharp recovery threshold in the following cases: (1) Binary
stochastic block model with two clusters of sizes proportional to network size
but not necessarily equal; (2) Stochastic block model with a fixed number of
equal-sized clusters; (3) Binary censored block model with the background graph
being Erd\H{o}s-R\'enyi. Furthermore, a sufficient condition is given for an
SDP procedure to achieve exact recovery for the general case of a fixed number
of clusters plus outliers. These results demonstrate the versatility of SDP
relaxation as a simple, general purpose, computationally feasible methodology
for community detection.","['Bruce Hajek', 'Yihong Wu', 'Jiaming Xu']","['stat.ML', 'cs.SI', 'math.PR']",2015-02-26 20:56:48+00:00
http://arxiv.org/abs/1502.07697v2,A Chaining Algorithm for Online Nonparametric Regression,"We consider the problem of online nonparametric regression with arbitrary
deterministic sequences. Using ideas from the chaining technique, we design an
algorithm that achieves a Dudley-type regret bound similar to the one obtained
in a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret bound
is expressed in terms of the metric entropy in the sup norm, which yields
optimal guarantees when the metric and sequential entropies are of the same
order of magnitude. In particular our algorithm is the first one that achieves
optimal rates for online regression over H{\""o}lder balls. In addition we show
for this example how to adapt our chaining algorithm to get a reasonable
computational efficiency with similar regret guarantees (up to a log factor).","['Pierre Gaillard', 'Sébastien Gerchinovitz']","['stat.ML', 'cs.LG']",2015-02-26 19:47:41+00:00
http://arxiv.org/abs/1502.07685v1,Covariance Matrices and Influence Scores for Mean Field Variational Bayes,"Mean field variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is that it underestimates the uncertainty of
model variables (sometimes severely) and provides no information about model
variable covariance. We develop a fast, general methodology for exponential
families that augments MFVB to deliver accurate uncertainty estimates for model
variables -- both for individual variables and coherently across variables.
MFVB for exponential families defines a fixed-point equation in the means of
the approximating posterior, and our approach yields a covariance estimate by
perturbing this fixed point. Inspired by linear response theory, we call our
method linear response variational Bayes (LRVB). We also show how LRVB can be
used to quickly calculate a measure of the influence of individual data points
on parameter point estimates. We demonstrate the accuracy and scalability of
our method by learning Gaussian mixture models for both simulated and real
data.","['Ryan Giordano', 'Tamara Broderick']","['stat.ML', 'stat.ME']",2015-02-26 19:08:39+00:00
http://arxiv.org/abs/1502.07645v2,Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo,"We consider the problem of Bayesian learning on sensitive datasets and
present two simple but somewhat surprising results that connect Bayesian
learning to ""differential privacy:, a cryptographic approach to protect
individual-level privacy while permiting database-level utility. Specifically,
we show that that under standard assumptions, getting one single sample from a
posterior distribution is differentially private ""for free"". We will see that
estimator is statistically consistent, near optimal and computationally
tractable whenever the Bayesian model of interest is consistent, optimal and
tractable. Similarly but separately, we show that a recent line of works that
use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve
differentially privacy with minor or no modifications of the algorithmic
procedure at all, these observations lead to an ""anytime"" algorithm for
Bayesian learning under privacy constraint. We demonstrate that it performs
much better than the state-of-the-art differential private methods on synthetic
and real datasets.","['Yu-Xiang Wang', 'Stephen E. Fienberg', 'Alex Smola']","['stat.ML', 'cs.LG']",2015-02-26 17:38:47+00:00
http://arxiv.org/abs/1502.07334v5,Sparse Multivariate Factor Regression,"We consider the problem of multivariate regression in a setting where the
relevant predictors could be shared among different responses. We propose an
algorithm which decomposes the coefficient matrix into the product of a long
matrix and a wide matrix, with an elastic net penalty on the former and an
$\ell_1$ penalty on the latter. The first matrix linearly transforms the
predictors to a set of latent factors, and the second one regresses the
responses on these factors. Our algorithm simultaneously performs dimension
reduction and coefficient estimation and automatically estimates the number of
latent factors from the data. Our formulation results in a non-convex
optimization problem, which despite its flexibility to impose effective
low-dimensional structure, is difficult, or even impossible, to solve exactly
in a reasonable time. We specify an optimization algorithm based on alternating
minimization with three different sets of updates to solve this non-convex
problem and provide theoretical results on its convergence and optimality.
Finally, we demonstrate the effectiveness of our algorithm via experiments on
simulated and real data.","['Milad Kharratzadeh', 'Mark Coates']",['stat.ML'],2015-02-25 20:40:30+00:00
http://arxiv.org/abs/1502.07229v1,Online Pairwise Learning Algorithms with Kernels,"Pairwise learning usually refers to a learning task which involves a loss
function depending on pairs of examples, among which most notable ones include
ranking, metric learning and AUC maximization. In this paper, we study an
online algorithm for pairwise learning with a least-square loss function in an
unconstrained setting of a reproducing kernel Hilbert space (RKHS), which we
refer to as the Online Pairwise lEaRning Algorithm (OPERA). In contrast to
existing works \cite{Kar,Wang} which require that the iterates are restricted
to a bounded domain or the loss function is strongly-convex, OPERA is
associated with a non-strongly convex objective function and learns the target
function in an unconstrained RKHS. Specifically, we establish a general theorem
which guarantees the almost surely convergence for the last iterate of OPERA
without any assumptions on the underlying distribution. Explicit convergence
rates are derived under the condition of polynomially decaying step sizes. We
also establish an interesting property for a family of widely-used kernels in
the setting of pairwise learning and illustrate the above convergence results
using such kernels. Our methodology mainly depends on the characterization of
RKHSs using its associated integral operators and probability inequalities for
random variables with values in a Hilbert space.","['Yiming Ying', 'Ding-Xuan Zhou']","['stat.ML', 'cs.LG']",2015-02-25 16:26:33+00:00
http://arxiv.org/abs/1502.07190v3,Topic-adjusted visibility metric for scientific articles,"Measuring the impact of scientific articles is important for evaluating the
research output of individual scientists, academic institutions and journals.
While citations are raw data for constructing impact measures, there exist
biases and potential issues if factors affecting citation patterns are not
properly accounted for. In this work, we address the problem of field variation
and introduce an article level metric useful for evaluating individual
articles' visibility. This measure derives from joint probabilistic modeling of
the content in the articles and the citations amongst them using latent
Dirichlet allocation (LDA) and the mixed membership stochastic blockmodel
(MMSB). Our proposed model provides a visibility metric for individual articles
adjusted for field variation in citation rates, a structural understanding of
citation behavior in different fields, and article recommendations which take
into account article visibility and citation patterns. We develop an efficient
algorithm for model fitting using variational methods. To scale up to large
networks, we develop an online variant using stochastic gradient methods and
case-control likelihood approximation. We apply our methods to the benchmark
KDD Cup 2003 dataset with approximately 30,000 high energy physics papers.","['Linda S. L. Tan', 'Aik Hui Chan', 'Tian Zheng']","['stat.ML', 'cs.LG']",2015-02-25 14:57:55+00:00
http://arxiv.org/abs/1502.07104v1,A Note on the Kullback-Leibler Divergence for the von Mises-Fisher distribution,"We present a derivation of the Kullback Leibler (KL)-Divergence (also known
as Relative Entropy) for the von Mises Fisher (VMF) Distribution in
$d$-dimensions.",['Tom Diethe'],['stat.ML'],2015-02-25 10:08:34+00:00
http://arxiv.org/abs/1502.07097v1,On aggregation for heavy-tailed classes,"We introduce an alternative to the notion of `fast rate' in Learning Theory,
which coincides with the optimal error rate when the given class happens to be
convex and regular in some sense. While it is well known that such a rate
cannot always be attained by a learning procedure (i.e., a procedure that
selects a function in the given class), we introduce an aggregation procedure
that attains that rate under rather minimal assumptions -- for example, that
the $L_q$ and $L_2$ norms are equivalent on the linear span of the class for
some $q>2$, and the target random variable is square-integrable.",['Shahar Mendelson'],"['math.ST', 'stat.ML', 'stat.TH', 'I.2.6']",2015-02-25 09:33:49+00:00
http://arxiv.org/abs/1502.07017v1,On Convolutional Approximations to Linear Dimensionality Reduction Operators for Large Scale Data Processing,"In this paper, we examine the problem of approximating a general linear
dimensionality reduction (LDR) operator, represented as a matrix $A \in
\mathbb{R}^{m \times n}$ with $m < n$, by a partial circulant matrix with rows
related by circular shifts. Partial circulant matrices admit fast
implementations via Fourier transform methods and subsampling operations; our
investigation here is motivated by a desire to leverage these potential
computational improvements in large-scale data processing tasks. We establish a
fundamental result, that most large LDR matrices (whose row spaces are
uniformly distributed) in fact cannot be well approximated by partial circulant
matrices. Then, we propose a natural generalization of the partial circulant
approximation framework that entails approximating the range space of a given
LDR operator $A$ over a restricted domain of inputs, using a matrix formed as a
product of a partial circulant matrix having $m '> m$ rows and a $m \times k$
'post processing' matrix. We introduce a novel algorithmic technique, based on
sparse matrix factorization, for identifying the factors comprising such
approximations, and provide preliminary evidence to demonstrate the potential
of this approach.","['Swayambhoo Jain', 'Jarvis Haupt']",['stat.ML'],2015-02-25 00:45:41+00:00
http://arxiv.org/abs/1502.06952v4,Phase Transitions for High Dimensional Clustering and Related Problems,"Consider a two-class clustering problem where we observe $X_i = \ell_i \mu +
Z_i$, $Z_i \stackrel{iid}{\sim} N(0, I_p)$, $1 \leq i \leq n$. The feature
vector $\mu\in R^p$ is unknown but is presumably sparse. The class labels
$\ell_i\in\{-1, 1\}$ are also unknown and the main interest is to estimate
them.
  We are interested in the statistical limits. In the two-dimensional phase
space calibrating the rarity and strengths of useful features, we find the
precise demarcation for the Region of Impossibility and Region of Possibility.
In the former, useful features are too rare/weak for successful clustering. In
the latter, useful features are strong enough to allow successful clustering.
The results are extended to the case of colored noise using Le Cam's idea on
comparison of experiments.
  We also extend the study on statistical limits for clustering to that for
signal recovery and that for hypothesis testing. We compare the statistical
limits for three problems and expose some interesting insight.
  We propose classical PCA and Important Features PCA (IF-PCA) for clustering.
For a threshold $t > 0$, IF-PCA clusters by applying classical PCA to all
columns of $X$ with an $L^2$-norm larger than $t$. We also propose two
aggregation methods. For any parameter in the Region of Possibility, some of
these methods yield successful clustering. We find an interesting phase
transition for IF-PCA.
  Our results require delicate analysis, especially on post-selection Random
Matrix Theory and on lower bound arguments.","['Jiashun Jin', 'Zheng Tracy Ke', 'Wanjie Wang']","['math.ST', 'stat.ML', 'stat.TH', '62H30, 62H25 (Primary) 62G05, 62G10 (Secondary)']",2015-02-24 20:58:44+00:00
http://arxiv.org/abs/1502.06930v3,Tensor decomposition with generalized lasso penalties,"We present an approach for penalized tensor decomposition (PTD) that
estimates smoothly varying latent factors in multi-way data. This generalizes
existing work on sparse tensor decomposition and penalized matrix
decompositions, in a manner parallel to the generalized lasso for regression
and smoothing problems. Our approach presents many nontrivial challenges at the
intersection of modeling and computation, which are studied in detail. An
efficient coordinate-wise optimization algorithm for (PTD) is presented, and
its convergence properties are characterized. The method is applied both to
simulated data and real data on flu hospitalizations in Texas. These results
show that our penalized tensor decomposition can offer major improvements on
existing methods for analyzing multi-way data that exhibit smooth spatial or
temporal features.","['Oscar Hernan Madrid Padilla', 'James G. Scott']","['stat.ME', 'stat.CO', 'stat.ML']",2015-02-24 20:03:05+00:00
http://arxiv.org/abs/1502.06919v2,Low Rank Matrix Completion with Exponential Family Noise,"The matrix completion problem consists in reconstructing a matrix from a
sample of entries, possibly observed with noise. A popular class of estimator,
known as nuclear norm penalized estimators, are based on minimizing the sum of
a data fitting term and a nuclear norm penalization. Here, we investigate the
case where the noise distribution belongs to the exponential family and is
sub-exponential. Our framework alllows for a general sampling scheme. We first
consider an estimator defined as the minimizer of the sum of a log-likelihood
term and a nuclear norm penalization and prove an upper bound on the Frobenius
prediction risk. The rate obtained improves on previous works on matrix
completion for exponential family. When the sampling distribution is known, we
propose another estimator and prove an oracle inequality w.r.t. the
Kullback-Leibler prediction risk, which translates immediatly into an upper
bound on the Frobenius prediction risk. Finally, we show that all the rates
obtained are minimax optimal up to a logarithmic factor.",['Jean Lafond'],"['math.ST', 'stat.ML', 'stat.TH']",2015-02-24 19:27:06+00:00
http://arxiv.org/abs/1502.06895v3,On the consistency theory of high dimensional variable screening,"Variable screening is a fast dimension reduction technique for assisting high
dimensional feature selection. As a preselection method, it selects a moderate
size subset of candidate variables for further refining via feature selection
to produce the final model. The performance of variable screening depends on
both computational efficiency and the ability to dramatically reduce the number
of variables without discarding the important ones. When the data dimension $p$
is substantially larger than the sample size $n$, variable screening becomes
crucial as 1) Faster feature selection algorithms are needed; 2) Conditions
guaranteeing selection consistency might fail to hold. This article studies a
class of linear screening methods and establishes consistency theory for this
special class. In particular, we prove the restricted diagonally dominant (RDD)
condition is a necessary and sufficient condition for strong screening
consistency. As concrete examples, we show two screening methods $SIS$ and
$HOLP$ are both strong screening consistent (subject to additional constraints)
with large probability if $n > O((\rho s + \sigma/\tau)^2\log p)$ under random
designs. In addition, we relate the RDD condition to the irrepresentable
condition, and highlight limitations of $SIS$.","['Xiangyu Wang', 'Chenlei Leng', 'David B. Dunson']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2015-02-24 17:52:20+00:00
http://arxiv.org/abs/1502.06811v1,A Review of Audio Features and Statistical Models Exploited for Voice Pattern Design,"Audio fingerprinting, also named as audio hashing, has been well-known as a
powerful technique to perform audio identification and synchronization. It
basically involves two major steps: fingerprint (voice pattern) design and
matching search. While the first step concerns the derivation of a robust and
compact audio signature, the second step usually requires knowledge about
database and quick-search algorithms. Though this technique offers a wide range
of real-world applications, to the best of the authors' knowledge, a
comprehensive survey of existing algorithms appeared more than eight years ago.
Thus, in this paper, we present a more up-to-date review and, for emphasizing
on the audio signal processing aspect, we focus our state-of-the-art survey on
the fingerprint design step for which various audio features and their
tractable statistical models are discussed.","['Ngoc Q. K. Duong', 'Hien-Thanh Duong']","['cs.SD', 'stat.ML']",2015-02-24 13:47:45+00:00
http://arxiv.org/abs/1502.06800v2,On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions,"We show that kernel-based quadrature rules for computing integrals can be
seen as a special case of random feature expansions for positive definite
kernels, for a particular decomposition that always exists for such kernels. We
provide a theoretical analysis of the number of required samples for a given
approximation error, leading to both upper and lower bounds that are based
solely on the eigenvalues of the associated integral operator and match up to
logarithmic terms. In particular, we show that the upper bound may be obtained
from independent and identically distributed samples from a specific
non-uniform distribution, while the lower bound if valid for any set of points.
Applying our results to kernel-based quadrature, while our results are fairly
general, we recover known upper and lower bounds for the special cases of
Sobolev spaces. Moreover, our results extend to the more general problem of
full function approximations (beyond simply computing an integral), with
results in L2- and L$\infty$-norm that match known results for special cases.
Applying our results to random features, we show an improvement of the number
of random features needed to preserve the generalization guarantees for
learning with Lipschitz-continuous losses.",['Francis Bach'],"['cs.LG', 'math.NA', 'stat.ML']",2015-02-24 13:12:51+00:00
http://arxiv.org/abs/1502.06689v1,1-Bit Matrix Completion under Exact Low-Rank Constraint,"We consider the problem of noisy 1-bit matrix completion under an exact rank
constraint on the true underlying matrix $M^*$. Instead of observing a subset
of the noisy continuous-valued entries of a matrix $M^*$, we observe a subset
of noisy 1-bit (or binary) measurements generated according to a probabilistic
model. We consider constrained maximum likelihood estimation of $M^*$, under a
constraint on the entry-wise infinity-norm of $M^*$ and an exact rank
constraint. This is in contrast to previous work which has used convex
relaxations for the rank. We provide an upper bound on the matrix estimation
error under this model. Compared to the existing results, our bound has faster
convergence rate with matrix dimensions when the fraction of revealed 1-bit
observations is fixed, independent of the matrix dimensions. We also propose an
iterative algorithm for solving our nonconvex optimization with a certificate
of global optimality of the limiting point. This algorithm is based on low rank
factorization of $M^*$. We validate the method on synthetic and real data with
improved performance over existing methods.","['Sonia Bhaskar', 'Adel Javanmard']",['stat.ML'],2015-02-24 05:38:31+00:00
http://arxiv.org/abs/1502.06644v2,On The Identifiability of Mixture Models from Grouped Samples,"Finite mixture models are statistical models which appear in many problems in
statistics and machine learning. In such models it is assumed that data are
drawn from random probability measures, called mixture components, which are
themselves drawn from a probability measure P over probability measures. When
estimating mixture models, it is common to make assumptions on the mixture
components, such as parametric assumptions. In this paper, we make no
assumption on the mixture components, and instead assume that observations from
the mixture model are grouped, such that observations in the same group are
known to be drawn from the same component. We show that any mixture of m
probability measures can be uniquely identified provided there are 2m-1
observations per group. Moreover we show that, for any m, there exists a
mixture of m probability measures that cannot be uniquely identified when
groups have 2m-2 observations. Our results hold for any sample space with more
than one element.","['Robert A. Vandermeulen', 'Clayton D. Scott']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2015-02-23 22:24:26+00:00
http://arxiv.org/abs/1502.06626v1,Optimal Sparse Linear Auto-Encoders and Sparse PCA,"Principal components analysis (PCA) is the optimal linear auto-encoder of
data, and it is often used to construct features. Enforcing sparsity on the
principal components can promote better generalization, while improving the
interpretability of the features. We study the problem of constructing optimal
sparse linear auto-encoders. Two natural questions in such a setting are: i)
Given a level of sparsity, what is the best approximation to PCA that can be
achieved? ii) Are there low-order polynomial-time algorithms which can
asymptotically achieve this optimal tradeoff between the sparsity and the
approximation quality?
  In this work, we answer both questions by giving efficient low-order
polynomial-time algorithms for constructing asymptotically \emph{optimal}
linear auto-encoders (in particular, sparse features with near-PCA
reconstruction error) and demonstrate the performance of our algorithms on real
data.","['Malik Magdon-Ismail', 'Christos Boutsidis']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.CO', 'stat.ML']",2015-02-23 21:06:39+00:00
http://arxiv.org/abs/1502.06590v1,Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems,"Given a large data matrix $A\in\mathbb{R}^{n\times n}$, we consider the
problem of determining whether its entries are i.i.d. with some known marginal
distribution $A_{ij}\sim P_0$, or instead $A$ contains a principal submatrix
$A_{{\sf Q},{\sf Q}}$ whose entries have marginal distribution $A_{ij}\sim
P_1\neq P_0$. As a special case, the hidden (or planted) clique problem
requires to find a planted clique in an otherwise uniformly random graph.
  Assuming unbounded computational resources, this hypothesis testing problem
is statistically solvable provided $|{\sf Q}|\ge C \log n$ for a suitable
constant $C$. However, despite substantial effort, no polynomial time algorithm
is known that succeeds with high probability when $|{\sf Q}| = o(\sqrt{n})$.
Recently Meka and Wigderson \cite{meka2013association}, proposed a method to
establish lower bounds within the Sum of Squares (SOS) semidefinite hierarchy.
  Here we consider the degree-$4$ SOS relaxation, and study the construction of
\cite{meka2013association} to prove that SOS fails unless $k\ge C\,
n^{1/3}/\log n$. An argument presented by Barak implies that this lower bound
cannot be substantially improved unless the witness construction is changed in
the proof. Our proof uses the moments method to bound the spectrum of a certain
random association scheme, i.e. a symmetric random matrix whose rows and
columns are indexed by the edges of an Erd\""os-Renyi random graph.","['Yash Deshpande', 'Andrea Montanari']","['cs.CC', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2015-02-23 20:45:11+00:00
http://arxiv.org/abs/1502.06557v2,Iteratively reweighted adaptive lasso for conditional heteroscedastic time series with applications to AR-ARCH type processes,"Shrinkage algorithms are of great importance in almost every area of
statistics due to the increasing impact of big data. Especially time series
analysis benefits from efficient and rapid estimation techniques such as the
lasso. However, currently lasso type estimators for autoregressive time series
models still focus on models with homoscedastic residuals. Therefore, an
iteratively reweighted adaptive lasso algorithm for the estimation of time
series models under conditional heteroscedasticity is presented in a
high-dimensional setting. The asymptotic behaviour of the resulting estimator
is analysed. It is found that the proposed estimation procedure performs
substantially better than its homoscedastic counterpart. A special case of the
algorithm is suitable to compute the estimated multivariate AR-ARCH type models
efficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCH
or ARMA-GARCH are discussed. Finally, different simulation results and
applications to electricity market data and returns of metal prices are shown.",['Florian Ziel'],"['stat.ME', 'q-fin.CP', 'stat.AP', 'stat.CO', 'stat.ML']",2015-02-23 19:14:39+00:00
http://arxiv.org/abs/1502.06531v2,Scalable Variational Inference in Log-supermodular Models,"We consider the problem of approximate Bayesian inference in log-supermodular
models. These models encompass regular pairwise MRFs with binary variables, but
allow to capture high-order interactions, which are intractable for existing
approximate inference techniques such as belief propagation, mean field, and
variants. We show that a recently proposed variational approach to inference in
log-supermodular models -L-FIELD- reduces to the widely-studied minimum norm
problem for submodular minimization. This insight allows to leverage powerful
existing tools, and hence to solve the variational problem orders of magnitude
more efficiently than previously possible. We then provide another natural
interpretation of L-FIELD, demonstrating that it exactly minimizes a specific
type of R\'enyi divergence measure. This insight sheds light on the nature of
the variational approximations produced by L-FIELD. Furthermore, we show how to
perform parallel inference as message passing in a suitable factor graph at a
linear convergence rate, without having to sum up over all the configurations
of the factor. Finally, we apply our approach to a challenging image
segmentation task. Our experiments confirm scalability of our approach, high
quality of the marginals, and the benefit of incorporating higher-order
potentials.","['Josip Djolonga', 'Andreas Krause']","['cs.LG', 'stat.ML']",2015-02-23 18:08:07+00:00
http://arxiv.org/abs/1502.06470v3,Approximate Message Passing with Restricted Boltzmann Machine Priors,"Approximate Message Passing (AMP) has been shown to be an excellent
statistical approach to signal inference and compressed sensing problem. The
AMP framework provides modularity in the choice of signal prior; here we
propose a hierarchical form of the Gauss-Bernouilli prior which utilizes a
Restricted Boltzmann Machine (RBM) trained on the signal support to push
reconstruction performance beyond that of simple iid priors for signals whose
support can be well represented by a trained binary RBM. We present and analyze
two methods of RBM factorization and demonstrate how these affect signal
reconstruction performance within our proposed algorithm. Finally, using the
MNIST handwritten digit dataset, we show experimentally that using an RBM
allows AMP to approach oracle-support performance.","['Eric W. Tramel', 'Angélique Drémeau', 'Florent Krzakala']","['cs.IT', 'cond-mat.dis-nn', 'math.IT', 'physics.data-an', 'stat.ML']",2015-02-23 15:51:07+00:00
http://arxiv.org/abs/1502.06464v2,Rectified Factor Networks,"We propose rectified factor networks (RFNs) to efficiently construct very
sparse, non-linear, high-dimensional representations of the input. RFN models
identify rare and small events in the input, have a low interference between
code units, have a small reconstruction error, and explain the data covariance
structure. RFN learning is a generalized alternating minimization algorithm
derived from the posterior regularization method which enforces non-negative
and normalized posterior means. We proof convergence and correctness of the RFN
learning algorithm. On benchmarks, RFNs are compared to other unsupervised
methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to
previous sparse coding methods, RFNs yield sparser codes, capture the data's
covariance structure more precisely, and have a significantly smaller
reconstruction error. We test RFNs as pretraining technique for deep networks
on different vision datasets, where RFNs were superior to RBMs and
autoencoders. On gene expression data from two pharmaceutical drug discovery
studies, RFNs detected small and rare gene modules that revealed highly
relevant new biological insights which were so far missed by other unsupervised
methods.","['Djork-Arné Clevert', 'Andreas Mayr', 'Thomas Unterthiner', 'Sepp Hochreiter']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2015-02-23 15:44:37+00:00
http://arxiv.org/abs/1502.06354v2,First-order regret bounds for combinatorial semi-bandits,"We consider the problem of online combinatorial optimization under
semi-bandit feedback, where a learner has to repeatedly pick actions from a
combinatorial decision set in order to minimize the total losses associated
with its decisions. After making each decision, the learner observes the losses
associated with its action, but not other losses. For this problem, there are
several learning algorithms that guarantee that the learner's expected regret
grows as $\widetilde{O}(\sqrt{T})$ with the number of rounds $T$. In this
paper, we propose an algorithm that improves this scaling to
$\widetilde{O}(\sqrt{{L_T^*}})$, where $L_T^*$ is the total loss of the best
action. Our algorithm is among the first to achieve such guarantees in a
partial-feedback scheme, and the first one to do so in a combinatorial setting.",['Gergely Neu'],"['cs.LG', 'stat.ML']",2015-02-23 09:12:26+00:00
http://arxiv.org/abs/1502.06309v3,"Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle","While machine learning has proven to be a powerful data-driven solution to
many real-life problems, its use in sensitive domains has been limited due to
privacy concerns. A popular approach known as **differential privacy** offers
provable privacy guarantees, but it is often observed in practice that it could
substantially hamper learning accuracy. In this paper we study the learnability
(whether a problem can be learned by any algorithm) under Vapnik's general
learning setting with differential privacy constraint, and reveal some
intricate relationships between privacy, stability and learnability.
  In particular, we show that a problem is privately learnable **if an only
if** there is a private algorithm that asymptotically minimizes the empirical
risk (AERM). In contrast, for non-private learning AERM alone is not sufficient
for learnability. This result suggests that when searching for private learning
algorithms, we can restrict the search to algorithms that are AERM. In light of
this, we propose a conceptual procedure that always finds a universally
consistent algorithm whenever the problem is learnable under privacy
constraint. We also propose a generic and practical algorithm and show that
under very general conditions it privately learns a wide class of learning
problems. Lastly, we extend some of the results to the more practical
$(\epsilon,\delta)$-differential privacy and establish the existence of a
phase-transition on the class of problems that are approximately privately
learnable with respect to how small $\delta$ needs to be.","['Yu-Xiang Wang', 'Jing Lei', 'Stephen E. Fienberg']","['stat.ML', 'cs.CR', 'cs.LG']",2015-02-23 03:52:08+00:00
http://arxiv.org/abs/1502.06189v2,"Two-stage Sampling, Prediction and Adaptive Regression via Correlation Screening (SPARCS)","This paper proposes a general adaptive procedure for budget-limited predictor
design in high dimensions called two-stage Sampling, Prediction and Adaptive
Regression via Correlation Screening (SPARCS). SPARCS can be applied to high
dimensional prediction problems in experimental science, medicine, finance, and
engineering, as illustrated by the following. Suppose one wishes to run a
sequence of experiments to learn a sparse multivariate predictor of a dependent
variable $Y$ (disease prognosis for instance) based on a $p$ dimensional set of
independent variables $\mathbf X=[X_1,\ldots, X_p]^T$ (assayed biomarkers).
Assume that the cost of acquiring the full set of variables $\mathbf X$
increases linearly in its dimension. SPARCS breaks the data collection into two
stages in order to achieve an optimal tradeoff between sampling cost and
predictor performance. In the first stage we collect a few ($n$) expensive
samples $\{y_i,\mathbf x_i\}_{i=1}^n$, at the full dimension $p\gg n$ of
$\mathbf X$, winnowing the number of variables down to a smaller dimension $l <
p$ using a type of cross-correlation or regression coefficient screening. In
the second stage we collect a larger number $(t-n)$ of cheaper samples of the
$l$ variables that passed the screening of the first stage. At the second
stage, a low dimensional predictor is constructed by solving the standard
regression problem using all $t$ samples of the selected variables. SPARCS is
an adaptive online algorithm that implements false positive control on the
selected variables, is well suited to small sample sizes, and is scalable to
high dimensions. We establish asymptotic bounds for the Familywise Error Rate
(FWER), specify high dimensional convergence rates for support recovery, and
establish optimal sample allocation rules to the first and second stages.","['Hamed Firouzi', 'Alfred Hero', 'Bala Rajaratnam']","['stat.ML', 'cs.LG']",2015-02-22 06:44:18+00:00
http://arxiv.org/abs/1502.06161v1,Using NLP to measure democracy,"This paper uses natural language processing to create the first machine-coded
democracy index, which I call Automated Democracy Scores (ADS). The ADS are
based on 42 million news articles from 6,043 different sources and cover all
independent countries in the 1993-2012 period. Unlike the democracy indices we
have today the ADS are replicable and have standard errors small enough to
actually distinguish between cases.
  The ADS are produced with supervised learning. Three approaches are tried: a)
a combination of Latent Semantic Analysis and tree-based regression methods; b)
a combination of Latent Dirichlet Allocation and tree-based regression methods;
and c) the Wordscores algorithm. The Wordscores algorithm outperforms the
alternatives, so it is the one on which the ADS are based.
  There is a web application where anyone can change the training set and see
how the results change: democracy-scores.org",['Thiago Marzagão'],"['cs.CL', 'cs.IR', 'cs.LG', 'stat.ML']",2015-02-22 01:30:32+00:00
http://arxiv.org/abs/1502.06134v3,Learning with Square Loss: Localization through Offset Rademacher Complexity,"We consider regression with square loss and general classes of functions
without the boundedness assumption. We introduce a notion of offset Rademacher
complexity that provides a transparent way to study localization both in
expectation and in high probability. For any (possibly non-convex) class, the
excess loss of a two-step estimator is shown to be upper bounded by this offset
complexity through a novel geometric inequality. In the convex case, the
estimator reduces to an empirical risk minimizer. The method recovers the
results of \citep{RakSriTsy15} for the bounded case while also providing
guarantees without the boundedness assumption.","['Tengyuan Liang', 'Alexander Rakhlin', 'Karthik Sridharan']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2015-02-21 19:20:44+00:00
http://arxiv.org/abs/1502.06064v1,MILJS : Brand New JavaScript Libraries for Matrix Calculation and Machine Learning,"MILJS is a collection of state-of-the-art, platform-independent, scalable,
fast JavaScript libraries for matrix calculation and machine learning. Our core
library offering a matrix calculation is called Sushi, which exhibits far
better performance than any other leading machine learning libraries written in
JavaScript. Especially, our matrix multiplication is 177 times faster than the
fastest JavaScript benchmark. Based on Sushi, a machine learning library called
Tempura is provided, which supports various algorithms widely used in machine
learning research. We also provide Soba as a visualization library. The
implementations of our libraries are clearly written, properly documented and
thus can are easy to get started with, as long as there is a web browser. These
libraries are available from http://mil-tokyo.github.io/ under the MIT license.","['Ken Miura', 'Tetsuaki Mano', 'Atsushi Kanehira', 'Yuichiro Tsuchiya', 'Tatsuya Harada']","['stat.ML', 'cs.LG', 'cs.MS']",2015-02-21 04:29:41+00:00
http://arxiv.org/abs/1502.05925v1,Feature-Budgeted Random Forest,"We seek decision rules for prediction-time cost reduction, where complete
data is available for training, but during prediction-time, each feature can
only be acquired for an additional cost. We propose a novel random forest
algorithm to minimize prediction error for a user-specified {\it average}
feature acquisition budget. While random forests yield strong generalization
performance, they do not explicitly account for feature costs and furthermore
require low correlation among trees, which amplifies costs. Our random forest
grows trees with low acquisition cost and high strength based on greedy minimax
cost-weighted-impurity splits. Theoretically, we establish near-optimal
acquisition cost guarantees for our algorithm. Empirically, on a number of
benchmark datasets we demonstrate superior accuracy-cost curves against
state-of-the-art prediction-time algorithms.","['Feng Nan', 'Joseph Wang', 'Venkatesh Saligrama']","['stat.ML', 'cs.LG']",2015-02-20 16:42:40+00:00
http://arxiv.org/abs/1502.05890v4,Contextual Semibandits via Supervised Learning Oracles,"We study an online decision making problem where on each round a learner
chooses a list of items based on some side information, receives a scalar
feedback value for each individual item, and a reward that is linearly related
to this feedback. These problems, known as contextual semibandits, arise in
crowdsourcing, recommendation, and many other domains. This paper reduces
contextual semibandits to supervised learning, allowing us to leverage powerful
supervised learning methods in this partial-feedback setting. Our first
reduction applies when the mapping from feedback to reward is known and leads
to a computationally efficient algorithm with near-optimal regret. We show that
this algorithm outperforms state-of-the-art approaches on real-world
learning-to-rank datasets, demonstrating the advantage of oracle-based
algorithms. Our second reduction applies to the previously unstudied setting
when the linear mapping from feedback to reward is unknown. Our regret
guarantees are superior to prior techniques that ignore the feedback.","['Akshay Krishnamurthy', 'Alekh Agarwal', 'Miroslav Dudik']","['cs.LG', 'stat.ML']",2015-02-20 14:55:41+00:00
http://arxiv.org/abs/1502.05774v2,Low-Cost Learning via Active Data Procurement,"We design mechanisms for online procurement of data held by strategic agents
for machine learning tasks. The challenge is to use past data to actively price
future data and give learning guarantees even when an agent's cost for
revealing her data may depend arbitrarily on the data itself. We achieve this
goal by showing how to convert a large class of no-regret algorithms into
online posted-price and learning mechanisms. Our results in a sense parallel
classic sample complexity guarantees, but with the key resource being money
rather than quantity of data: With a budget constraint $B$, we give robust risk
(predictive error) bounds on the order of $1/\sqrt{B}$. Because we use an
active approach, we can often guarantee to do significantly better by
leveraging correlations between costs and data.
  Our algorithms and analysis go through a model of no-regret learning with $T$
arriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds
for this model are on the order of $T/\sqrt{B}$ and we give lower bounds on the
same order.","['Jacob Abernethy', 'Yiling Chen', 'Chien-Ju Ho', 'Bo Waggoner']","['cs.GT', 'cs.AI', 'cs.LG', 'stat.ML', 'J.4; I.2.6']",2015-02-20 05:11:44+00:00
http://arxiv.org/abs/1502.05767v4,Automatic differentiation in machine learning: a survey,"Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in
machine learning. Automatic differentiation (AD), also called algorithmic
differentiation or simply ""autodiff"", is a family of techniques similar to but
more general than backpropagation for efficiently and accurately evaluating
derivatives of numeric functions expressed as computer programs. AD is a small
but established field with applications in areas including computational fluid
dynamics, atmospheric sciences, and engineering design optimization. Until very
recently, the fields of machine learning and AD have largely been unaware of
each other and, in some cases, have independently discovered each other's
results. Despite its relevance, general-purpose AD has been missing from the
machine learning toolbox, a situation slowly changing with its ongoing adoption
under the names ""dynamic computational graphs"" and ""differentiable
programming"". We survey the intersection of AD and machine learning, cover
applications where AD has direct relevance, and address the main implementation
techniques. By precisely defining the main differentiation techniques and their
interrelationships, we aim to bring clarity to the usage of the terms
""autodiff"", ""automatic differentiation"", and ""symbolic differentiation"" as
these are encountered more and more in machine learning settings.","['Atilim Gunes Baydin', 'Barak A. Pearlmutter', 'Alexey Andreyevich Radul', 'Jeffrey Mark Siskind']","['cs.SC', 'cs.LG', 'stat.ML', '68W30, 65D25, 68T05', 'G.1.4; I.2.6']",2015-02-20 04:20:47+00:00
http://arxiv.org/abs/1502.05752v1,Pairwise Constraint Propagation: A Survey,"As one of the most important types of (weaker) supervised information in
machine learning and pattern recognition, pairwise constraint, which specifies
whether a pair of data points occur together, has recently received significant
attention, especially the problem of pairwise constraint propagation. At least
two reasons account for this trend: the first is that compared to the data
label, pairwise constraints are more general and easily to collect, and the
second is that since the available pairwise constraints are usually limited,
the constraint propagation problem is thus important.
  This paper provides an up-to-date critical survey of pairwise constraint
propagation research. There are two underlying motivations for us to write this
survey paper: the first is to provide an up-to-date review of the existing
literature, and the second is to offer some insights into the studies of
pairwise constraint propagation. To provide a comprehensive survey, we not only
categorize existing propagation techniques but also present detailed
descriptions of representative methods within each category.","['Zhenyong Fu', 'Zhiwu Lu']","['cs.CV', 'cs.LG', 'stat.ML']",2015-02-19 23:59:48+00:00
http://arxiv.org/abs/1502.05700v2,Scalable Bayesian Optimization Using Deep Neural Networks,"Bayesian optimization is an effective methodology for the global optimization
of functions with expensive evaluations. It relies on querying a distribution
over functions defined by a relatively cheap surrogate model. An accurate model
for this distribution over functions is critical to the effectiveness of the
approach, and is typically fit using Gaussian processes (GPs). However, since
GPs scale cubically with the number of observations, it has been challenging to
handle objectives whose optimization requires many evaluations, and as such,
massively parallelizing the optimization.
  In this work, we explore the use of neural networks as an alternative to GPs
to model distributions over functions. We show that performing adaptive basis
function regression with a neural network as the parametric form performs
competitively with state-of-the-art GP-based approaches, but scales linearly
with the number of data rather than cubically. This allows us to achieve a
previously intractable degree of parallelism, which we apply to large scale
hyperparameter optimization, rapidly finding competitive models on benchmark
object recognition tasks using convolutional networks, and image caption
generation using neural language models.","['Jasper Snoek', 'Oren Rippel', 'Kevin Swersky', 'Ryan Kiros', 'Nadathur Satish', 'Narayanan Sundaram', 'Md. Mostofa Ali Patwary', 'Prabhat', 'Ryan P. Adams']",['stat.ML'],2015-02-19 20:51:27+00:00
http://arxiv.org/abs/1502.05698v10,Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks,"One long-term goal of machine learning research is to produce methods that
are applicable to reasoning and natural language, in particular building an
intelligent dialogue agent. To measure progress towards that goal, we argue for
the usefulness of a set of proxy tasks that evaluate reading comprehension via
question answering. Our tasks measure understanding in several ways: whether a
system is able to answer questions via chaining facts, simple induction,
deduction and many more. The tasks are designed to be prerequisites for any
system that aims to be capable of conversing with a human. We believe many
existing learning systems can currently not solve them, and hence our aim is to
classify these tasks into skill sets, so that researchers can identify (and
then rectify) the failings of their systems. We also extend and improve the
recently introduced Memory Networks model, and show it is able to solve some,
but not all, of the tasks.","['Jason Weston', 'Antoine Bordes', 'Sumit Chopra', 'Alexander M. Rush', 'Bart van Merriënboer', 'Armand Joulin', 'Tomas Mikolov']","['cs.AI', 'cs.CL', 'stat.ML']",2015-02-19 20:46:10+00:00
http://arxiv.org/abs/1502.05680v2,Finding One Community in a Sparse Graph,"We consider a random sparse graph with bounded average degree, in which a
subset of vertices has higher connectivity than the background. In particular,
the average degree inside this subset of vertices is larger than outside (but
still bounded). Given a realization of such graph, we aim at identifying the
hidden subset of vertices. This can be regarded as a model for the problem of
finding a tightly knitted community in a social network, or a cluster in a
relational dataset.
  In this paper we present two sets of contributions: $(i)$ We use the cavity
method from spin glass theory to derive an exact phase diagram for the
reconstruction problem. In particular, as the difference in edge probability
increases, the problem undergoes two phase transitions, a static phase
transition and a dynamic one. $(ii)$ We establish rigorous bounds on the
dynamic phase transition and prove that, above a certain threshold, a local
algorithm (belief propagation) correctly identify most of the hidden set. Below
the same threshold \emph{no local algorithm} can achieve this goal. However, in
this regime the subset can be identified by exhaustive search.
  For small hidden sets and large average degree, the phase transition for
local algorithms takes an intriguingly simple form. Local algorithms succeed
with high probability for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} >
\sqrt{{\rm deg}_{\rm out}/e}$ and fail for ${\rm deg}_{\rm in} - {\rm deg}_{\rm
out} < \sqrt{{\rm deg}_{\rm out}/e}$ (with ${\rm deg}_{\rm in}$, ${\rm
deg}_{\rm out}$ the average degrees inside and outside the community). We argue
that spectral algorithms are also ineffective in the latter regime.
  It is an open problem whether any polynomial time algorithms might succeed
for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} < \sqrt{{\rm deg}_{\rm out}/e}$.",['Andrea Montanari'],"['stat.ML', 'cond-mat.stat-mech', 'cs.SI']",2015-02-19 19:50:09+00:00
http://arxiv.org/abs/1502.05675v2,NP-Hardness and Inapproximability of Sparse PCA,"We give a reduction from {\sc clique} to establish that sparse PCA is
NP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse
PCA (unless P=NP). Under weaker complexity assumptions, we also exclude
polynomial constant-factor approximation algorithms.",['Malik Magdon-Ismail'],"['cs.LG', 'cs.CC', 'cs.DS', 'math.CO', 'stat.ML']",2015-02-19 19:30:46+00:00
http://arxiv.org/abs/1502.05571v1,Finding Dantzig selectors with a proximity operator based fixed-point algorithm,"In this paper, we study a simple iterative method for finding the Dantzig
selector, which was designed for linear regression problems. The method
consists of two main stages. The first stage is to approximate the Dantzig
selector through a fixed-point formulation of solutions to the Dantzig selector
problem. The second stage is to construct a new estimator by regressing data
onto the support of the approximated Dantzig selector. We compare our method to
an alternating direction method, and present the results of numerical
simulations using both the proposed method and the alternating direction method
on synthetic and real data sets. The numerical simulations demonstrate that the
two methods produce results of similar quality, however the proposed method
tends to be significantly faster.","['Ashley Prater', 'Lixin Shen', 'Bruce W. Suter']","['math.NA', 'stat.ML']",2015-02-19 13:50:18+00:00
http://arxiv.org/abs/1502.05556v2,Just Sort It! A Simple and Effective Approach to Active Preference Learning,"We address the problem of learning a ranking by using adaptively chosen
pairwise comparisons. Our goal is to recover the ranking accurately but to
sample the comparisons sparingly. If all comparison outcomes are consistent
with the ranking, the optimal solution is to use an efficient sorting
algorithm, such as Quicksort. But how do sorting algorithms behave if some
comparison outcomes are inconsistent with the ranking? We give favorable
guarantees for Quicksort for the popular Bradley-Terry model, under natural
assumptions on the parameters. Furthermore, we empirically demonstrate that
sorting algorithms lead to a very simple and effective active learning
strategy: repeatedly sort the items. This strategy performs as well as
state-of-the-art methods (and much better than random sampling) at a minuscule
fraction of the computational cost.","['Lucas Maystre', 'Matthias Grossglauser']","['stat.ML', 'cs.LG']",2015-02-19 12:50:13+00:00
http://arxiv.org/abs/1502.05503v1,Classification and Bayesian Optimization for Likelihood-Free Inference,"Some statistical models are specified via a data generating process for which
the likelihood function cannot be computed in closed form. Standard
likelihood-based inference is then not feasible but the model parameters can be
inferred by finding the values which yield simulated data that resemble the
observed data. This approach faces at least two major difficulties: The first
difficulty is the choice of the discrepancy measure which is used to judge
whether the simulated data resemble the observed data. The second difficulty is
the computationally efficient identification of regions in the parameter space
where the discrepancy is low. We give here an introduction to our recent work
where we tackle the two difficulties through classification and Bayesian
optimization.","['Michael U. Gutmann', 'Jukka Corander', 'Ritabrata Dutta', 'Samuel Kaski']","['stat.CO', 'stat.ME', 'stat.ML']",2015-02-19 09:09:27+00:00
http://arxiv.org/abs/1502.05336v2,Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks,"Large multilayer neural networks trained with backpropagation have recently
achieved state-of-the-art results in a wide range of problems. However, using
backprop for neural net learning still has some disadvantages, e.g., having to
tune a large number of hyperparameters to the data, lack of calibrated
probabilistic predictions, and a tendency to overfit the training data. In
principle, the Bayesian approach to learning neural networks does not have
these problems. However, existing Bayesian techniques lack scalability to large
dataset and network sizes. In this work we present a novel scalable method for
learning Bayesian neural networks, called probabilistic backpropagation (PBP).
Similar to classical backpropagation, PBP works by computing a forward
propagation of probabilities through the network and then doing a backward
computation of gradients. A series of experiments on ten real-world datasets
show that PBP is significantly faster than other techniques, while offering
competitive predictive abilities. Our experiments also show that PBP provides
accurate estimates of the posterior variance on the network weights.","['José Miguel Hernández-Lobato', 'Ryan P. Adams']",['stat.ML'],2015-02-18 18:45:17+00:00
http://arxiv.org/abs/1502.05313v2,Variational Optimization of Annealing Schedules,"Annealed importance sampling (AIS) is a common algorithm to estimate
partition functions of useful stochastic models. One important problem for
obtaining accurate AIS estimates is the selection of an annealing schedule.
Conventionally, an annealing schedule is often determined heuristically or is
simply set as a linearly increasing sequence. In this paper, we propose an
algorithm for the optimal schedule by deriving a functional that dominates the
AIS estimation error and by numerically minimizing this functional. We
experimentally demonstrate that the proposed algorithm mostly outperforms
conventional scheduling schemes with large quantization numbers.",['Taichi Kiwaki'],['stat.ML'],2015-02-18 17:45:44+00:00
http://arxiv.org/abs/1502.05312v2,Predictive Entropy Search for Bayesian Optimization with Unknown Constraints,"Unknown constraints arise in many types of expensive black-box optimization
problems. Several methods have been proposed recently for performing Bayesian
optimization with constraints, based on the expected improvement (EI)
heuristic. However, EI can lead to pathologies when used with constraints. For
example, in the case of decoupled constraints---i.e., when one can
independently evaluate the objective or the constraints---EI can encounter a
pathology that prevents exploration. Additionally, computing EI requires a
current best solution, which may not exist if none of the data collected so far
satisfy the constraints. By contrast, information-based approaches do not
suffer from these failure modes. In this paper, we present a new
information-based method called Predictive Entropy Search with Constraints
(PESC). We analyze the performance of PESC and show that it compares favorably
to EI-based approaches on synthetic and benchmark problems, as well as several
real-world examples. We demonstrate that PESC is an effective algorithm that
provides a promising direction towards a unified solution for constrained
Bayesian optimization.","['José Miguel Hernández-Lobato', 'Michael A. Gelbart', 'Matthew W. Hoffman', 'Ryan P. Adams', 'Zoubin Ghahramani']",['stat.ML'],2015-02-18 17:39:30+00:00
http://arxiv.org/abs/1502.05023v2,A New Sampling Technique for Tensors,"In this paper we propose new techniques to sample arbitrary third-order
tensors, with an objective of speeding up tensor algorithms that have recently
gained popularity in machine learning. Our main contribution is a new way to
select, in a biased random way, only $O(n^{1.5}/\epsilon^2)$ of the possible
$n^3$ elements while still achieving each of the three goals: \\ {\em (a)
tensor sparsification}: for a tensor that has to be formed from arbitrary
samples, compute very few elements to get a good spectral approximation, and
for arbitrary orthogonal tensors {\em (b) tensor completion:} recover an
exactly low-rank tensor from a small number of samples via alternating least
squares, or {\em (c) tensor factorization:} approximating factors of a low-rank
tensor corrupted by noise. \\ Our sampling can be used along with existing
tensor-based algorithms to speed them up, removing the computational bottleneck
in these methods.","['Srinadh Bhojanapalli', 'Sujay Sanghavi']","['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']",2015-02-17 20:23:13+00:00
http://arxiv.org/abs/1502.04993v2,Reconstruction of recurrent synaptic connectivity of thousands of neurons from simulated spiking activity,"Dynamics and function of neuronal networks are determined by their synaptic
connectivity. Current experimental methods to analyze synaptic network
structure on the cellular level, however, cover only small fractions of
functional neuronal circuits, typically without a simultaneous record of
neuronal spiking activity. Here we present a method for the reconstruction of
large recurrent neuronal networks from thousands of parallel spike train
recordings. We employ maximum likelihood estimation of a generalized linear
model of the spiking activity in continuous time. For this model the point
process likelihood is concave, such that a global optimum of the parameters can
be obtained by gradient ascent. Previous methods, including those of the same
class, did not allow recurrent networks of that order of magnitude to be
reconstructed due to prohibitive computational cost and numerical
instabilities. We describe a minimal model that is optimized for large networks
and an efficient scheme for its parallelized numerical optimization on generic
computing clusters. For a simulated balanced random network of 1000 neurons,
synaptic connectivity is recovered with a misclassification error rate of less
than 1% under ideal conditions. We show that the error rate remains low in a
series of example cases under progressively less ideal conditions. Finally, we
successfully reconstruct the connectivity of a hidden synfire chain that is
embedded in a random network, which requires clustering of the network
connectivity to reveal the synfire groups. Our results demonstrate how synaptic
connectivity could potentially be inferred from large-scale parallel spike
train recordings.","['Yury V. Zaytsev', 'Abigail Morrison', 'Moritz Deger']","['q-bio.NC', 'q-bio.QM', 'stat.ML']",2015-02-17 18:43:58+00:00
http://arxiv.org/abs/1502.04874v2,Regret bounds for Narendra-Shapiro bandit algorithms,"Narendra-Shapiro (NS) algorithms are bandit-type algorithms that have been
introduced in the sixties (with a view to applications in Psychology or
learning automata), whose convergence has been intensively studied in the
stochastic algorithm literature. In this paper, we adress the following
question: are the Narendra-Shapiro (NS) bandit algorithms competitive from a
\textit{regret} point of view? In our main result, we show that some
competitive bounds can be obtained for such algorithms in their penalized
version (introduced in \cite{Lamberton_Pages}). More precisely, up to an
over-penalization modification, the pseudo-regret $\bar{R}_n$ related to the
penalized two-armed bandit algorithm is uniformly bounded by $C \sqrt{n}$
(where $C$ is made explicit in the paper). \noindent We also generalize
existing convergence and rates of convergence results to the multi-armed case
of the over-penalized bandit algorithm, including the convergence toward the
invariant measure of a Piecewise Deterministic Markov Process (PDMP) after a
suitable renormalization. Finally, ergodic properties of this PDMP are given in
the multi-armed case.","['Sébastien Gadat', 'Fabien Panloup', 'Sofiane Saadane']","['math.PR', 'math.ST', 'stat.ML', 'stat.TH']",2015-02-17 12:49:01+00:00
http://arxiv.org/abs/1502.04868v2,Proper Complex Gaussian Processes for Regression,"Complex-valued signals are used in the modeling of many systems in
engineering and science, hence being of fundamental interest. Often, random
complex-valued signals are considered to be proper. A proper complex random
variable or process is uncorrelated with its complex conjugate. This assumption
is a good model of the underlying physics in many problems, and simplifies the
computations. While linear processing and neural networks have been widely
studied for these signals, the development of complex-valued nonlinear kernel
approaches remains an open problem. In this paper we propose Gaussian processes
for regression as a framework to develop 1) a solution for proper
complex-valued kernel regression and 2) the design of the reproducing kernel
for complex-valued inputs, using the convolutional approach for
cross-covariances. In this design we pay attention to preserve, in the complex
domain, the measure of similarity between near inputs. The hyperparameters of
the kernel are learned maximizing the marginal likelihood using Wirtinger
derivatives. Besides, the approach is connected to the multiple output learning
scenario. In the experiments included, we first solve a proper complex Gaussian
process where the cross-covariance does not cancel, a challenging scenario when
dealing with proper complex signals. Then we successfully use these novel
results to solve some problems previously proposed in the literature as
benchmarks, reporting a remarkable improvement in the estimation error.","['Rafael Boloix-Tortosa', 'F. Javier Payán-Somet', 'Eva Arias-de-Reyna', 'Juan José Murillo-Fuentes']","['cs.LG', 'stat.ML']",2015-02-17 11:59:44+00:00
http://arxiv.org/abs/1502.04837v2,Nonparametric Nearest Neighbor Descent Clustering based on Delaunay Triangulation,"In our physically inspired in-tree (IT) based clustering algorithm and the
series after it, there is only one free parameter involved in computing the
potential value of each point. In this work, based on the Delaunay
Triangulation or its dual Voronoi tessellation, we propose a nonparametric
process to compute potential values by the local information. This computation,
though nonparametric, is relatively very rough, and consequently, many local
extreme points will be generated. However, unlike those gradient-based methods,
our IT-based methods are generally insensitive to those local extremes. This
positively demonstrates the superiority of these parametric (previous) and
nonparametric (in this work) IT-based methods.","['Teng Qiu', 'Yongjie Li']","['stat.ML', 'cs.CV', 'cs.LG']",2015-02-17 09:27:03+00:00
http://arxiv.org/abs/1502.04742v1,On the Predictive Properties of Binary Link Functions,"This paper provides a theoretical and computational justification of the long
held claim that of the similarity of the probit and logit link functions often
used in binary classification. Despite this widespread recognition of the
strong similarities between these two link functions, very few (if any)
researchers have dedicated time to carry out a formal study aimed at
establishing and characterizing firmly all the aspects of the similarities and
differences. This paper proposes a definition of both structural and predictive
equivalence of link functions-based binary regression models, and explores the
various ways in which they are either similar or dissimilar. From a predictive
analytics perspective, it turns out that not only are probit and logit
perfectly predictively concordant, but the other link functions like cauchit
and complementary log log enjoy very high percentage of predictive equivalence.
Throughout this paper, simulated and real life examples demonstrate all the
equivalence results that we prove theoretically.","['Necla Gunduz', 'Ernest Fokoue']","['stat.ML', 'stat.ME']",2015-02-16 22:39:57+00:00
http://arxiv.org/abs/1502.04726v1,ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike and Slab Priors,"In this letter, we address sparse signal recovery using spike and slab
priors. In particular, we focus on a Bayesian framework where sparsity is
enforced on reconstruction coefficients via probabilistic priors. The
optimization resulting from spike and slab prior maximization is known to be a
hard non-convex problem, and existing solutions involve simplifying assumptions
and/or relaxations. We propose an approach called Iterative Convex Refinement
(ICR) that aims to solve the aforementioned optimization problem directly
allowing for greater generality in the sparse structure. Essentially, ICR
solves a sequence of convex optimization problems such that sequence of
solutions converges to a sub-optimal solution of the original hard optimization
problem. We propose two versions of our algorithm: a.) an unconstrained
version, and b.) with a non-negativity constraint on sparse coefficients, which
may be required in some real-world problems. Experimental validation is
performed on both synthetic data and for a real-world image recovery problem,
which illustrates merits of ICR over state of the art alternatives.","['Hojjat S. Mousavi', 'Vishal Monga', 'Trac D. Tran']","['stat.ML', 'cs.CV', 'math.OC']",2015-02-16 21:17:52+00:00
http://arxiv.org/abs/1502.04689v2,Exact tensor completion using t-SVD,"In this paper we focus on the problem of completion of multidimensional
arrays (also referred to as tensors) from limited sampling. Our approach is
based on a recently proposed tensor-Singular Value Decomposition (t-SVD) [1].
Using this factorization one can derive notion of tensor rank, referred to as
the tensor tubal rank, which has optimality properties similar to that of
matrix rank derived from SVD. As shown in [2] some multidimensional data, such
as panning video sequences exhibit low tensor tubal rank and we look at the
problem of completing such data under random sampling of the data cube. We show
that by solving a convex optimization problem, which minimizes the tensor
nuclear norm obtained as the convex relaxation of tensor tubal rank, one can
guarantee recovery with overwhelming probability as long as samples in
proportion to the degrees of freedom in t-SVD are observed. In this sense our
results are order-wise optimal. The conditions under which this result holds
are very similar to the incoherency conditions for the matrix completion,
albeit we define incoherency under the algebraic set-up of t-SVD. We show the
performance of the algorithm on some real data sets and compare it with other
existing approaches based on tensor flattening and Tucker decomposition.","['Zemin Zhang', 'Shuchin Aeron']","['cs.LG', 'cs.NA', 'stat.ML']",2015-02-16 20:37:35+00:00
http://arxiv.org/abs/1502.04635v2,Parameter estimation in softmax decision-making models with linear objective functions,"With an eye towards human-centered automation, we contribute to the
development of a systematic means to infer features of human decision-making
from behavioral data. Motivated by the common use of softmax selection in
models of human decision-making, we study the maximum likelihood parameter
estimation problem for softmax decision-making models with linear objective
functions. We present conditions under which the likelihood function is convex.
These allow us to provide sufficient conditions for convergence of the
resulting maximum likelihood estimator and to construct its asymptotic
distribution. In the case of models with nonlinear objective functions, we show
how the estimator can be applied by linearizing about a nominal parameter
value. We apply the estimator to fit the stochastic UCL (Upper Credible Limit)
model of human decision-making to human subject data. We show statistically
significant differences in behavior across related, but distinct, tasks.","['Paul Reverdy', 'Naomi E. Leonard']","['math.OC', 'cs.LG', 'stat.ML', '93E10']",2015-02-16 17:17:24+00:00
http://arxiv.org/abs/1502.04631v2,Clustering and Inference From Pairwise Comparisons,"Given a set of pairwise comparisons, the classical ranking problem computes a
single ranking that best represents the preferences of all users. In this
paper, we study the problem of inferring individual preferences, arising in the
context of making personalized recommendations. In particular, we assume that
there are $n$ users of $r$ types; users of the same type provide similar
pairwise comparisons for $m$ items according to the Bradley-Terry model. We
propose an efficient algorithm that accurately estimates the individual
preferences for almost all users, if there are $r \max \{m, n\}\log m \log^2 n$
pairwise comparisons per type, which is near optimal in sample complexity when
$r$ only grows logarithmically with $m$ or $n$. Our algorithm has three steps:
first, for each user, compute the \emph{net-win} vector which is a projection
of its $\binom{m}{2}$-dimensional vector of pairwise comparisons onto an
$m$-dimensional linear subspace; second, cluster the users based on the net-win
vectors; third, estimate a single preference for each cluster separately. The
net-win vectors are much less noisy than the high dimensional vectors of
pairwise comparisons and clustering is more accurate after the projection as
confirmed by numerical experiments. Moreover, we show that, when a cluster is
only approximately correct, the maximum likelihood estimation for the
Bradley-Terry model is still close to the true preference.","['Rui Wu', 'Jiaming Xu', 'R. Srikant', 'Laurent Massoulié', 'Marc Lelarge', 'Bruce Hajek']",['stat.ML'],2015-02-16 17:08:48+00:00
http://arxiv.org/abs/1502.04622v1,Particle Gibbs for Bayesian Additive Regression Trees,"Additive regression trees are flexible non-parametric models and popular
off-the-shelf tools for real-world non-linear regression. In application
domains, such as bioinformatics, where there is also demand for probabilistic
predictions with measures of uncertainty, the Bayesian additive regression
trees (BART) model, introduced by Chipman et al. (2010), is increasingly
popular. As data sets have grown in size, however, the standard
Metropolis-Hastings algorithms used to perform inference in BART are proving
inadequate. In particular, these Markov chains make local changes to the trees
and suffer from slow mixing when the data are high-dimensional or the best
fitting trees are more than a few layers deep. We present a novel sampler for
BART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a
top-down particle filtering algorithm for Bayesian decision trees
(Lakshminarayanan et al., 2013). Rather than making local changes to individual
trees, the PG sampler proposes a complete tree to fit the residual. Experiments
show that the PG sampler outperforms existing samplers in many settings.","['Balaji Lakshminarayanan', 'Daniel M. Roy', 'Yee Whye Teh']","['stat.ML', 'cs.LG', 'stat.CO']",2015-02-16 16:48:30+00:00
http://arxiv.org/abs/1502.04502v1,Clustering by Descending to the Nearest Neighbor in the Delaunay Graph Space,"In our previous works, we proposed a physically-inspired rule to organize the
data points into an in-tree (IT) structure, in which some undesired edges are
allowed to occur. By removing those undesired or redundant edges, this IT
structure is divided into several separate parts, each representing one
cluster. In this work, we seek to prevent the undesired edges from arising at
the source. Before using the physically-inspired rule, data points are at first
organized into a proximity graph which restricts each point to select the
optimal directed neighbor just among its neighbors. Consequently, separated
in-trees or clusters automatically arise, without redundant edges requiring to
be removed.","['Teng Qiu', 'Yongjie Li']","['stat.ML', 'cs.CV', 'cs.LG']",2015-02-16 11:50:42+00:00
http://arxiv.org/abs/1502.04434v3,Invariant backpropagation: how to train a transformation-invariant neural network,"In many classification problems a classifier should be robust to small
variations in the input vector. This is a desired property not only for
particular transformations, such as translation and rotation in image
classification problems, but also for all others for which the change is small
enough to retain the object perceptually indistinguishable. We propose two
extensions of the backpropagation algorithm that train a neural network to be
robust to variations in the feature vector. While the first of them enforces
robustness of the loss function to all variations, the second method trains the
predictions to be robust to a particular variation which changes the loss
function the most. The second methods demonstrates better results, but is
slightly slower. We analytically compare the proposed algorithm with two the
most similar approaches (Tangent BP and Adversarial Training), and propose
their fast versions. In the experimental part we perform comparison of all
algorithms in terms of classification accuracy and robustness to noise on MNIST
and CIFAR-10 datasets. Additionally we analyze how the performance of the
proposed algorithm depends on the dataset size and data augmentation.","['Sergey Demyanov', 'James Bailey', 'Ramamohanarao Kotagiri', 'Christopher Leckie']","['stat.ML', 'cs.LG', 'cs.NE']",2015-02-16 06:28:35+00:00
http://arxiv.org/abs/1502.04416v5,Random Subspace Learning Approach to High-Dimensional Outliers Detection,"We introduce and develop a novel approach to outlier detection based on
adaptation of random subspace learning. Our proposed method handles both
high-dimension low-sample size and traditional low-dimensional high-sample size
datasets. Essentially, we avoid the computational bottleneck of techniques like
minimum covariance determinant (MCD) by computing the needed determinants and
associated measures in much lower dimensional subspaces. Both theoretical and
computational development of our approach reveal that it is computationally
more efficient than the regularized methods in high-dimensional low-sample
size, and often competes favorably with existing methods as far as the
percentage of correct outlier detection is concerned.","['Bohan Liu', 'Ernest Fokoue']","['stat.ML', '62H25, 62H30']",2015-02-16 03:31:02+00:00
http://arxiv.org/abs/1502.04315v1,Fast and Memory-Efficient Significant Pattern Mining via Permutation Testing,"We present a novel algorithm, Westfall-Young light, for detecting patterns,
such as itemsets and subgraphs, which are statistically significantly enriched
in one of two classes. Our method corrects rigorously for multiple hypothesis
testing and correlations between patterns through the Westfall-Young
permutation procedure, which empirically estimates the null distribution of
pattern frequencies in each class via permutations. In our experiments,
Westfall-Young light dramatically outperforms the current state-of-the-art
approach in terms of both runtime and memory efficiency on popular real-world
benchmark datasets for pattern mining. The key to this efficiency is that
unlike all existing methods, our algorithm neither needs to solve the
underlying frequent itemset mining problem anew for each permutation nor needs
to store the occurrence list of all frequent patterns. Westfall-Young light
opens the door to significant pattern mining on large datasets that previously
led to prohibitive runtime or memory costs.","['Felipe Llinares López', 'Mahito Sugiyama', 'Laetitia Papaxanthos', 'Karsten M. Borgwardt']",['stat.ML'],2015-02-15 14:46:13+00:00
http://arxiv.org/abs/1502.04269v3,Supersparse Linear Integer Models for Optimized Medical Scoring Systems,"Scoring systems are linear classification models that only require users to
add, subtract and multiply a few small numbers in order to make a prediction.
These models are in widespread use by the medical community, but are difficult
to learn from data because they need to be accurate and sparse, have coprime
integer coefficients, and satisfy multiple operational constraints. We present
a new method for creating data-driven scoring systems called a Supersparse
Linear Integer Model (SLIM). SLIM scoring systems are built by solving an
integer program that directly encodes measures of accuracy (the 0-1 loss) and
sparsity (the $\ell_0$-seminorm) while restricting coefficients to coprime
integers. SLIM can seamlessly incorporate a wide range of operational
constraints related to accuracy and sparsity, and can produce highly tailored
models without parameter tuning. We provide bounds on the testing and training
accuracy of SLIM scoring systems, and present a new data reduction technique
that can improve scalability by eliminating a portion of the training data
beforehand. Our paper includes results from a collaboration with the
Massachusetts General Hospital Sleep Laboratory, where SLIM was used to create
a highly tailored scoring system for sleep apnea screening","['Berk Ustun', 'Cynthia Rudin']","['stat.ML', 'cs.DM', 'cs.LG', 'stat.AP', 'stat.ME']",2015-02-15 01:26:41+00:00
http://arxiv.org/abs/1502.04168v2,Nonparametric regression using needlet kernels for spherical data,"Needlets have been recognized as state-of-the-art tools to tackle spherical
data, due to their excellent localization properties in both spacial and
frequency domains.
  This paper considers developing kernel methods associated with the needlet
kernel for nonparametric regression problems whose predictor variables are
defined on a sphere. Due to the localization property in the frequency domain,
we prove that the regularization parameter of the kernel ridge regression
associated with the needlet kernel can decrease arbitrarily fast. A natural
consequence is that the regularization term for the kernel ridge regression is
not necessary in the sense of rate optimality. Based on the excellent
localization property in the spacial domain further, we also prove that all the
$l^{q}$ $(01\leq q < \infty)$ kernel regularization estimates associated with
the needlet kernel, including the kernel lasso estimate and the kernel bridge
estimate, possess almost the same generalization capability for a large range
of regularization parameters in the sense of rate optimality.
  This finding tentatively reveals that, if the needlet kernel is utilized,
then the choice of $q$ might not have a strong impact in terms of the
generalization capability in some modeling contexts. From this perspective, $q$
can be arbitrarily specified, or specified merely by other no generalization
criteria like smoothness, computational complexity, sparsity, etc..",['Shaobo Lin'],"['cs.LG', 'stat.ML', '68T05, 62J02', 'F.2.2']",2015-02-14 05:37:32+00:00
http://arxiv.org/abs/1502.04148v2,A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA,"Independent Component Analysis (ICA) is a popular model for blind signal
separation. The ICA model assumes that a number of independent source signals
are linearly mixed to form the observed signals. We propose a new algorithm,
PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for
ICA with Gaussian noise. The main technical innovation of the algorithm is to
use a fixed point iteration in a pseudo-Euclidean (indefinite ""inner product"")
space. The use of this indefinite ""inner product"" resolves technical issues
common to several existing algorithms for noisy ICA. This leads to an algorithm
which is conceptually simple, efficient and accurate in testing.
  Our second contribution is combining PEGI with the analysis of objectives for
optimal recovery in the noisy ICA model. It has been observed that the direct
approach of demixing with the inverse of the mixing matrix is suboptimal for
signal recovery in terms of the natural Signal to Interference plus Noise Ratio
(SINR) criterion. There have been several partial solutions proposed in the ICA
literature. It turns out that any solution to the mixing matrix reconstruction
problem can be used to construct an SINR-optimal ICA demixing, despite the fact
that SINR itself cannot be computed from data. That allows us to obtain a
practical and provably SINR-optimal recovery method for ICA with arbitrary
Gaussian noise.","['James Voss', 'Mikhail Belkin', 'Luis Rademacher']","['cs.LG', 'stat.ML']",2015-02-13 23:18:35+00:00
http://arxiv.org/abs/1502.04081v2,A Linear Dynamical System Model for Text,"Low dimensional representations of words allow accurate NLP models to be
trained on limited annotated data. While most representations ignore words'
local context, a natural way to induce context-dependent representations is to
perform inference in a probabilistic latent-variable sequence model. Given the
recent success of continuous vector space word representations, we provide such
an inference procedure for continuous states, where words' representations are
given by the posterior mean of a linear dynamical system. Here, efficient
inference can be performed using Kalman filtering. Our learning algorithm is
extremely scalable, operating on simple cooccurrence counts for both parameter
initialization using the method of moments and subsequent iterations of EM. In
our experiments, we employ our inferred word embeddings as features in standard
tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman
filter updates can be seen as a linear recurrent neural network. We demonstrate
that using the parameters of our model to initialize a non-linear recurrent
neural network language model reduces its training time by a day and yields
lower perplexity.","['David Belanger', 'Sham Kakade']","['stat.ML', 'cs.CL', 'cs.LG']",2015-02-13 18:39:29+00:00
http://arxiv.org/abs/1502.04033v2,The Responsibility Weighted Mahalanobis Kernel for Semi-Supervised Training of Support Vector Machines for Classification,"Kernel functions in support vector machines (SVM) are needed to assess the
similarity of input samples in order to classify these samples, for instance.
Besides standard kernels such as Gaussian (i.e., radial basis function, RBF) or
polynomial kernels, there are also specific kernels tailored to consider
structure in the data for similarity assessment. In this article, we will
capture structure in data by means of probabilistic mixture density models, for
example Gaussian mixtures in the case of real-valued input spaces. From the
distance measures that are inherently contained in these models, e.g.,
Mahalanobis distances in the case of Gaussian mixtures, we derive a new kernel,
the responsibility weighted Mahalanobis (RWM) kernel. Basically, this kernel
emphasizes the influence of model components from which any two samples that
are compared are assumed to originate (that is, the ""responsible"" model
components). We will see that this kernel outperforms the RBF kernel and other
kernels capturing structure in data (such as the LAP kernel in Laplacian SVM)
in many applications where partially labeled data are available, i.e., for
semi-supervised training of SVM. Other key advantages are that the RWM kernel
can easily be used with standard SVM implementations and training algorithms
such as sequential minimal optimization, and heuristics known for the
parametrization of RBF kernels in a C-SVM can easily be transferred to this new
kernel. Properties of the RWM kernel are demonstrated with 20 benchmark data
sets and an increasing percentage of labeled samples in the training data.","['Tobias Reitmaier', 'Bernhard Sick']","['cs.LG', 'stat.ML']",2015-02-13 15:48:00+00:00
http://arxiv.org/abs/1502.03939v1,Polynomial-Chaos-based Kriging,"Computer simulation has become the standard tool in many engineering fields
for designing and optimizing systems, as well as for assessing their
reliability. To cope with demanding analysis such as optimization and
reliability, surrogate models (a.k.a meta-models) have been increasingly
investigated in the last decade. Polynomial Chaos Expansions (PCE) and Kriging
are two popular non-intrusive meta-modelling techniques. PCE surrogates the
computational model with a series of orthonormal polynomials in the input
variables where polynomials are chosen in coherency with the probability
distributions of those input variables. On the other hand, Kriging assumes that
the computer model behaves as a realization of a Gaussian random process whose
parameters are estimated from the available computer runs, i.e. input vectors
and response values. These two techniques have been developed more or less in
parallel so far with little interaction between the researchers in the two
fields. In this paper, PC-Kriging is derived as a new non-intrusive
meta-modeling approach combining PCE and Kriging. A sparse set of orthonormal
polynomials (PCE) approximates the global behavior of the computational model
whereas Kriging manages the local variability of the model output. An adaptive
algorithm similar to the least angle regression algorithm determines the
optimal sparse set of polynomials. PC-Kriging is validated on various benchmark
analytical functions which are easy to sample for reference results. From the
numerical investigations it is concluded that PC-Kriging performs better than
or at least as good as the two distinct meta-modeling techniques. A larger gain
in accuracy is obtained when the experimental design has a limited size, which
is an asset when dealing with demanding computational models.","['R. Schoebi', 'B. Sudret', 'J. Wiart']","['stat.CO', 'stat.ME', 'stat.ML']",2015-02-13 10:53:52+00:00
http://arxiv.org/abs/1502.03919v2,Policy Gradient for Coherent Risk Measures,"Several authors have recently developed risk-sensitive policy gradient
methods that augment the standard expected cost minimization problem with a
measure of variability in cost. These studies have focused on specific
risk-measures, such as the variance or conditional value at risk (CVaR). In
this work, we extend the policy gradient method to the whole class of coherent
risk measures, which is widely accepted in finance and operations research,
among other fields. We consider both static and time-consistent dynamic risk
measures. For static risk measures, our approach is in the spirit of policy
gradient algorithms and combines a standard sampling approach with convex
programming. For dynamic risk measures, our approach is actor-critic style and
involves explicit approximation of value function. Most importantly, our
contribution presents a unified approach to risk-sensitive reinforcement
learning that generalizes and extends previous results.","['Aviv Tamar', 'Yinlam Chow', 'Mohammad Ghavamzadeh', 'Shie Mannor']","['cs.AI', 'cs.LG', 'stat.ML']",2015-02-13 09:16:24+00:00
http://arxiv.org/abs/1502.03696v7,Monte Carlo Planning method estimates planning horizons during interactive social exchange,"Reciprocating interactions represent a central feature of all human
exchanges. They have been the target of various recent experiments, with
healthy participants and psychiatric populations engaging as dyads in
multi-round exchanges such as a repeated trust task. Behaviour in such
exchanges involves complexities related to each agent's preference for equity
with their partner, beliefs about the partner's appetite for equity, beliefs
about the partner's model of their partner, and so on. Agents may also plan
different numbers of steps into the future. Providing a computationally precise
account of the behaviour is an essential step towards understanding what
underlies choices. A natural framework for this is that of an interactive
partially observable Markov decision process (IPOMDP). However, the various
complexities make IPOMDPs inordinately computationally challenging. Here, we
show how to approximate the solution for the multi-round trust task using a
variant of the Monte-Carlo tree search algorithm. We demonstrate that the
algorithm is efficient and effective, and therefore can be used to invert
observations of behavioural choices. We use generated behaviour to elucidate
the richness and sophistication of interactive inference.","['Andreas Hula', 'P. Read Montague', 'Peter Dayan']",['stat.ML'],2015-02-12 15:18:43+00:00
http://arxiv.org/abs/1502.03656v2,Quasi-Newton particle Metropolis-Hastings,"Particle Metropolis-Hastings enables Bayesian parameter inference in general
nonlinear state space models (SSMs). However, in many implementations a random
walk proposal is used and this can result in poor mixing if not tuned correctly
using tedious pilot runs. Therefore, we consider a new proposal inspired by
quasi-Newton algorithms that may achieve similar (or better) mixing with less
tuning. An advantage compared to other Hessian based proposals, is that it only
requires estimates of the gradient of the log-posterior. A possible application
is parameter inference in the challenging class of SSMs with intractable
likelihoods. We exemplify this application and the benefits of the new proposal
by modelling log-returns of future contracts on coffee by a stochastic
volatility model with $\alpha$-stable observations.","['Johan Dahlin', 'Fredrik Lindsten', 'Thomas B. Schön']","['stat.CO', 'q-fin.CP', 'stat.ML']",2015-02-12 13:48:01+00:00
http://arxiv.org/abs/1502.03655v2,Newton-based maximum likelihood estimation in nonlinear state space models,"Maximum likelihood (ML) estimation using Newton's method in nonlinear state
space models (SSMs) is a challenging problem due to the analytical
intractability of the log-likelihood and its gradient and Hessian. We estimate
the gradient and Hessian using Fisher's identity in combination with a
smoothing algorithm. We explore two approximations of the log-likelihood and of
the solution of the smoothing problem. The first is a linearization
approximation which is computationally cheap, but the accuracy typically varies
between models. The second is a sampling approximation which is asymptotically
valid for any SSM but is more computationally costly. We demonstrate our
approach for ML parameter estimation on simulated data from two different SSMs
with encouraging results.","['Manon Kok', 'Johan Dahlin', 'Thomas B. Schön', 'Adrian Wills']","['stat.CO', 'stat.ML']",2015-02-12 13:47:23+00:00
http://arxiv.org/abs/1502.03571v5,Weighted SGD for $\ell_p$ Regression with Randomized Preconditioning,"In recent years, stochastic gradient descent (SGD) methods and randomized
linear algebra (RLA) algorithms have been applied to many large-scale problems
in machine learning and data analysis. We aim to bridge the gap between these
two methods in solving constrained overdetermined linear regression
problems---e.g., $\ell_2$ and $\ell_1$ regression problems. We propose a hybrid
algorithm named pwSGD that uses RLA techniques for preconditioning and
constructing an importance sampling distribution, and then performs an SGD-like
iterative process with weighted sampling on the preconditioned system. We prove
that pwSGD inherits faster convergence rates that only depend on the lower
dimension of the linear system, while maintaining low computation complexity.
Particularly, when solving $\ell_1$ regression with size $n$ by $d$, pwSGD
returns an approximate solution with $\epsilon$ relative error in the objective
value in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)/\epsilon^2)$
time. This complexity is uniformly better than that of RLA methods in terms of
both $\epsilon$ and $d$ when the problem is unconstrained. For $\ell_2$
regression, pwSGD returns an approximate solution with $\epsilon$ relative
error in the objective value and the solution vector measured in prediction
norm in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)
\log(1/\epsilon) /\epsilon)$ time. We also provide lower bounds on the coreset
complexity for more general regression problems, indicating that still new
ideas will be needed to extend similar RLA preconditioning ideas to weighted
SGD algorithms for more general regression problems. Finally, the effectiveness
of such algorithms is illustrated numerically on both synthetic and real
datasets.","['Jiyan Yang', 'Yin-Lam Chow', 'Christopher Ré', 'Michael W. Mahoney']","['math.OC', 'stat.ML']",2015-02-12 09:11:58+00:00
http://arxiv.org/abs/1502.03536v1,Speeding up Permutation Testing in Neuroimaging,"Multiple hypothesis testing is a significant problem in nearly all
neuroimaging studies. In order to correct for this phenomena, we require a
reliable estimate of the Family-Wise Error Rate (FWER). The well known
Bonferroni correction method, while simple to implement, is quite conservative,
and can substantially under-power a study because it ignores dependencies
between test statistics. Permutation testing, on the other hand, is an exact,
non-parametric method of estimating the FWER for a given $\alpha$-threshold,
but for acceptably low thresholds the computational burden can be prohibitive.
In this paper, we show that permutation testing in fact amounts to populating
the columns of a very large matrix ${\bf P}$. By analyzing the spectrum of this
matrix, under certain conditions, we see that ${\bf P}$ has a low-rank plus a
low-variance residual decomposition which makes it suitable for highly
sub--sampled --- on the order of $0.5\%$ --- matrix completion methods. Based
on this observation, we propose a novel permutation testing methodology which
offers a large speedup, without sacrificing the fidelity of the estimated FWER.
Our evaluations on four different neuroimaging datasets show that a
computational speedup factor of roughly $50\times$ can be achieved while
recovering the FWER distribution up to very high accuracy. Further, we show
that the estimated $\alpha$-threshold is also recovered faithfully, and is
stable.","['Chris Hinrichs', 'Vamsi K Ithapu', 'Qinyuan Sun', 'Sterling C Johnson', 'Vikas Singh']","['stat.CO', 'cs.AI', 'stat.ML']",2015-02-12 04:30:06+00:00
http://arxiv.org/abs/1502.03520v8,A Latent Variable Model Approach to PMI-based Word Embeddings,"Semantic word embeddings represent the meaning of a word via a vector, and
are created by diverse methods. Many use nonlinear operations on co-occurrence
statistics, and have hand-tuned hyperparameters and reweighting methods.
  This paper proposes a new generative model, a dynamic version of the
log-linear topic model of~\citet{mnih2007three}. The methodological novelty is
to use the prior to compute closed form expressions for word statistics. This
provides a theoretical justification for nonlinear models like PMI, word2vec,
and GloVe, as well as some hyperparameter choices. It also helps explain why
low-dimensional semantic embeddings contain linear algebraic structure that
allows solution of word analogies, as shown by~\citet{mikolov2013efficient} and
many subsequent papers.
  Experimental support is provided for the generative model assumptions, the
most important of which is that latent word vectors are fairly uniformly
dispersed in space.","['Sanjeev Arora', 'Yuanzhi Li', 'Yingyu Liang', 'Tengyu Ma', 'Andrej Risteski']","['cs.LG', 'cs.CL', 'stat.ML']",2015-02-12 02:50:08+00:00
http://arxiv.org/abs/1502.03509v2,MADE: Masked Autoencoder for Distribution Estimation,"There has been a lot of recent interest in designing neural network models to
estimate a distribution from a set of examples. We introduce a simple
modification for autoencoder neural networks that yields powerful generative
models. Our method masks the autoencoder's parameters to respect autoregressive
constraints: each input is reconstructed only from previous inputs in a given
ordering. Constrained this way, the autoencoder outputs can be interpreted as a
set of conditional probabilities, and their product, the full joint
probability. We can also train a single network that can decompose the joint
probability in multiple different orderings. Our simple framework can be
applied to multiple architectures, including deep ones. Vectorized
implementations, such as on GPUs, are simple and fast. Experiments demonstrate
that this approach is competitive with state-of-the-art tractable distribution
estimators. At test time, the method is significantly faster and scales better
than other autoregressive estimators.","['Mathieu Germain', 'Karol Gregor', 'Iain Murray', 'Hugo Larochelle']","['cs.LG', 'cs.NE', 'stat.ML']",2015-02-12 02:06:07+00:00
