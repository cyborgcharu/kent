id,title,abstract,authors,categories,date
http://arxiv.org/abs/2308.09122v1,RTB Formulation Using Point Process,"We propose a general stochastic framework for modelling repeated auctions in
the Real Time Bidding (RTB) ecosystem using point processes. The flexibility of
the framework allows a variety of auction scenarios including configuration of
information provided to player, determination of auction winner and
quantification of utility gained from each auctions. We propose theoretical
results on how this formulation of process can be approximated to a Poisson
point process, which enables the analyzer to take advantage of well-established
properties. Under this framework, we specify the player's optimal strategy
under various scenarios. We also emphasize that it is critical to consider the
joint distribution of utility and market condition instead of estimating the
marginal distributions independently.","['Seong Jin Lee', 'Bumsik Kim']","['stat.ML', 'cs.GT', 'cs.LG']",2023-08-17 17:57:59+00:00
http://arxiv.org/abs/2308.09113v3,Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage,"Deep learning-based surrogate models have been widely applied in geological
carbon storage (GCS) problems to accelerate the prediction of reservoir
pressure and CO2 plume migration. Large amounts of data from physics-based
numerical simulators are required to train a model to accurately predict the
complex physical behaviors associated with this process. In practice, the
available training data are always limited in large-scale 3D problems due to
the high computational cost. Therefore, we propose to use a multi-fidelity
Fourier neural operator (FNO) to solve large-scale GCS problems with more
affordable multi-fidelity training datasets. FNO has a desirable grid-invariant
property, which simplifies the transfer learning procedure between datasets
with different discretization. We first test the model efficacy on a GCS
reservoir model being discretized into 110k grid cells. The multi-fidelity
model can predict with accuracy comparable to a high-fidelity model trained
with the same amount of high-fidelity data with 81% less data generation costs.
We further test the generalizability of the multi-fidelity model on a same
reservoir model with a finer discretization of 1 million grid cells. This case
was made more challenging by employing high-fidelity and low-fidelity datasets
generated by different geostatistical models and reservoir simulators. We
observe that the multi-fidelity FNO model can predict pressure fields with
reasonable accuracy even when the high-fidelity data are extremely limited. The
findings of this study can help for better understanding of the transferability
of multi-fidelity deep learning surrogate models.","['Hewei Tang', 'Qingkai Kong', 'Joseph P. Morris']","['stat.ML', 'cs.LG']",2023-08-17 17:44:59+00:00
http://arxiv.org/abs/2308.09108v1,Spectral information criterion for automatic elbow detection,"We introduce a generalized information criterion that contains other
well-known information criteria, such as Bayesian information Criterion (BIC)
and Akaike information criterion (AIC), as special cases. Furthermore, the
proposed spectral information criterion (SIC) is also more general than the
other information criteria, e.g., since the knowledge of a likelihood function
is not strictly required. SIC extracts geometric features of the error curve
and, as a consequence, it can be considered an automatic elbow detector. SIC
provides a subset of all possible models, with a cardinality that often is much
smaller than the total number of possible models. The elements of this subset
are elbows of the error curve. A practical rule for selecting a unique model
within the sets of elbows is suggested as well. Theoretical invariance
properties of SIC are analyzed. Moreover, we test SIC in ideal scenarios where
provides always the optimal expected results. We also test SIC in several
numerical experiments: some involving synthetic data, and two experiments
involving real datasets. They are all real-world applications such as
clustering, variable selection, or polynomial order selection, to name a few.
The results show the benefits of the proposed scheme. Matlab code related to
the experiments is also provided. Possible future research lines are finally
discussed.","['L. Martino', 'R. San Millan-Castillo', 'E. Morgado']","['stat.ME', 'cs.AI', 'eess.SP', 'stat.ML']",2023-08-17 17:18:45+00:00
http://arxiv.org/abs/2308.09104v2,Spike-and-slab shrinkage priors for structurally sparse Bayesian neural networks,"Network complexity and computational efficiency have become increasingly
significant aspects of deep learning. Sparse deep learning addresses these
challenges by recovering a sparse representation of the underlying target
function by reducing heavily over-parameterized deep neural networks.
Specifically, deep neural architectures compressed via structured sparsity
(e.g. node sparsity) provide low latency inference, higher data throughput, and
reduced energy consumption. In this paper, we explore two well-established
shrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian
neural networks. To this end, we propose structurally sparse Bayesian neural
networks which systematically prune excessive nodes with (i) Spike-and-Slab
Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors,
and develop computationally tractable variational inference including
continuous relaxation of Bernoulli variables. We establish the contraction
rates of the variational posterior of our proposed models as a function of the
network topology, layer-wise node cardinalities, and bounds on the network
weights. We empirically demonstrate the competitive performance of our models
compared to the baseline models in prediction accuracy, model compression, and
inference latency.","['Sanket Jantre', 'Shrijita Bhattacharya', 'Tapabrata Maiti']","['stat.ML', 'cs.LG', 'stat.ME']",2023-08-17 17:14:18+00:00
http://arxiv.org/abs/2308.09078v2,Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling,"Conditional sampling of variational autoencoders (VAEs) is needed in various
applications, such as missing data imputation, but is computationally
intractable. A principled choice for asymptotically exact conditional sampling
is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs
to learn a structured latent space, a commonly desired property, can cause the
MWG sampler to get ""stuck"" far from the target distribution. This paper
mitigates the limitations of MWG: we systematically outline the pitfalls in the
context of VAEs, propose two original methods that address these pitfalls, and
demonstrate an improved performance of the proposed methods on a set of
sampling tasks.","['Vaidotas Simkus', 'Michael U. Gutmann']","['cs.LG', 'stat.ML', '62D10', 'G.3']",2023-08-17 16:08:18+00:00
http://arxiv.org/abs/2308.09065v2,Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression,"Uncertainty quantification is critical for deploying deep neural networks
(DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE)
is one of the most effective means to estimate the uncertainty of the main task
prediction without modifying the main task model. To be considered robust, an
AuxUE must be capable of maintaining its performance and triggering higher
uncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to
provide robust aleatoric and epistemic uncertainty. However, for vision
regression tasks, current AuxUE designs are mainly adopted for aleatoric
uncertainty estimates, and AuxUE robustness has not been explored. In this
work, we propose a generalized AuxUE scheme for more robust uncertainty
quantification on regression tasks. Concretely, to achieve a more robust
aleatoric uncertainty estimation, different distribution assumptions are
considered for heteroscedastic noise, and Laplace distribution is finally
chosen to approximate the prediction error. For epistemic uncertainty, we
propose a novel solution named Discretization-Induced Dirichlet pOsterior
(DIDO), which models the Dirichlet posterior on the discretized prediction
error. Extensive experiments on age estimation, monocular depth estimation, and
super-resolution tasks show that our proposed method can provide robust
uncertainty estimates in the face of noisy inputs and that it can be scalable
to both image-level and pixel-wise tasks. Code is available at
https://github.com/ENSTA-U2IS/DIDO .","['Xuanlong Yu', 'Gianni Franchi', 'Jindong Gu', 'Emanuel Aldea']","['cs.CV', 'cs.LG', 'stat.ML']",2023-08-17 15:54:11+00:00
http://arxiv.org/abs/2308.09043v2,Kernel-Based Tests for Likelihood-Free Hypothesis Testing,"Given $n$ observations from two balanced classes, consider the task of
labeling an additional $m$ inputs that are known to all belong to \emph{one} of
the two classes. Special cases of this problem are well-known: with complete
knowledge of class distributions ($n=\infty$) the problem is solved optimally
by the likelihood-ratio test; when $m=1$ it corresponds to binary
classification; and when $m\approx n$ it is equivalent to two-sample testing.
The intermediate settings occur in the field of likelihood-free inference,
where labeled samples are obtained by running forward simulations and the
unlabeled sample is collected experimentally. In recent work it was discovered
that there is a fundamental trade-off between $m$ and $n$: increasing the data
sample $m$ reduces the amount $n$ of training/simulation data needed. In this
work we (a) introduce a generalization where unlabeled samples come from a
mixture of the two classes -- a case often encountered in practice; (b) study
the minimax sample complexity for non-parametric classes of densities under
\textit{maximum mean discrepancy} (MMD) separation; and (c) investigate the
empirical performance of kernels parameterized by neural networks on two tasks:
detection of the Higgs boson and detection of planted DDPM generated images
amidst CIFAR-10 images. For both problems we confirm the existence of the
theoretically predicted asymmetric $m$ vs $n$ trade-off.","['Patrik Róbert Gerber', 'Tianze Jiang', 'Yury Polyanskiy', 'Rui Sun']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2023-08-17 15:24:03+00:00
http://arxiv.org/abs/2308.08977v1,Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models,"We analyze the dynamics of streaming stochastic gradient descent (SGD) in the
high-dimensional limit when applied to generalized linear models and
multi-index models (e.g. logistic regression, phase retrieval) with general
data-covariance. In particular, we demonstrate a deterministic equivalent of
SGD in the form of a system of ordinary differential equations that describes a
wide class of statistics, such as the risk and other measures of
sub-optimality. This equivalence holds with overwhelming probability when the
model parameter count grows proportionally to the number of data. This
framework allows us to obtain learning rate thresholds for stability of SGD as
well as convergence guarantees. In addition to the deterministic equivalent, we
introduce an SDE with a simplified diffusion coefficient (homogenized SGD)
which allows us to analyze the dynamics of general statistics of SGD iterates.
Finally, we illustrate this theory on some standard examples and show numerical
simulations which give an excellent match to the theory.","['Elizabeth Collins-Woodfin', 'Courtney Paquette', 'Elliot Paquette', 'Inbar Seroussi']","['math.OC', 'cs.LG', 'math.PR', 'math.ST', 'stat.ML', 'stat.TH']",2023-08-17 13:33:02+00:00
http://arxiv.org/abs/2308.08858v2,Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games,"The problem of two-player zero-sum Markov games has recently attracted
increasing interests in theoretical studies of multi-agent reinforcement
learning (RL). In particular, for finite-horizon episodic Markov decision
processes (MDPs), it has been shown that model-based algorithms can find an
$\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of
$O(H^3SAB/\epsilon^2)$, which is optimal in the dependence of the horizon $H$
and the number of states $S$ (where $A$ and $B$ denote the number of actions of
the two players, respectively). However, none of the existing model-free
algorithms can achieve such an optimality. In this work, we propose a
model-free stage-based Q-learning algorithm and show that it achieves the same
sample complexity as the best model-based algorithm, and hence for the first
time demonstrate that model-free algorithms can enjoy the same optimality in
the $H$ dependence as model-based algorithms. The main improvement of the
dependency on $H$ arises by leveraging the popular variance reduction technique
based on the reference-advantage decomposition previously used only for
single-agent RL. However, such a technique relies on a critical monotonicity
property of the value function, which does not hold in Markov games due to the
update of the policy via the coarse correlated equilibrium (CCE) oracle. Thus,
to extend such a technique to Markov games, our algorithm features a key novel
design of updating the reference value functions as the pair of optimistic and
pessimistic value functions whose value difference is the smallest in the
history in order to achieve the desired improvement in the sample efficiency.","['Songtao Feng', 'Ming Yin', 'Yu-Xiang Wang', 'Jing Yang', 'Yingbin Liang']","['cs.LG', 'cs.AI', 'cs.GT', 'stat.ML']",2023-08-17 08:34:58+00:00
http://arxiv.org/abs/2308.08852v1,Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm,"Graphical models have exhibited their performance in numerous tasks ranging
from biological analysis to recommender systems. However, graphical models with
hub nodes are computationally difficult to fit, particularly when the dimension
of the data is large. To efficiently estimate the hub graphical models, we
introduce a two-phase algorithm. The proposed algorithm first generates a good
initial point via a dual alternating direction method of multipliers (ADMM),
and then warm starts a semismooth Newton (SSN) based augmented Lagrangian
method (ALM) to compute a solution that is accurate enough for practical tasks.
The sparsity structure of the generalized Jacobian ensures that the algorithm
can obtain a nice solution very efficiently. Comprehensive experiments on both
synthetic data and real data show that it obviously outperforms the existing
state-of-the-art algorithms. In particular, in some high dimensional tasks, it
can save more than 70\% of the execution time, meanwhile still achieves a
high-quality estimation.","['Chengjing Wang', 'Peipei Tang', 'Wenling He', 'Meixia Lin']","['math.OC', 'cs.LG', 'cs.NA', 'math.NA', 'stat.CO', 'stat.ML', '65K05, 90C06, 90C25, 90C90']",2023-08-17 08:24:28+00:00
http://arxiv.org/abs/2308.08427v1,Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning,"This paper proposes a novel framework for identifying an agent's risk
aversion using interactive questioning. Our study is conducted in two
scenarios: a one-period case and an infinite horizon case. In the one-period
case, we assume that the agent's risk aversion is characterized by a cost
function of the state and a distortion risk measure. In the infinite horizon
case, we model risk aversion with an additional component, a discount factor.
Assuming the access to a finite set of candidates containing the agent's true
risk aversion, we show that asking the agent to demonstrate her optimal
policies in various environment, which may depend on their previous answers, is
an effective means of identifying the agent's risk aversion. Specifically, we
prove that the agent's risk aversion can be identified as the number of
questions tends to infinity, and the questions are randomly designed. We also
develop an algorithm for designing optimal questions and provide empirical
evidence that our method learns risk aversion significantly faster than
randomly designed questions in simulations. Our framework has important
applications in robo-advising and provides a new approach for identifying an
agent's risk preferences.","['Ziteng Cheng', 'Anthony Coache', 'Sebastian Jaimungal']","['stat.ML', 'cs.LG']",2023-08-16 15:17:57+00:00
http://arxiv.org/abs/2308.08387v2,Continuous Sweep for Binary Quantification Learning,"A quantifier is a supervised machine learning algorithm, focused on
estimating the class prevalence in a dataset rather than labeling its
individual observations. We introduce Continuous Sweep, a new parametric binary
quantifier inspired by the well-performing Median Sweep, which is an ensemble
method based on Adjusted Count estimators. We modified two aspects of Median
Sweep: 1) using parametric class distributions instead of empirical
distributions for the true and false positive rate; 2) using the mean instead
of the median of a set of Adjusted Count estimates. These two modifications
allow for a theoretical analysis of the bias and variance of Continuous Sweep.
Furthermore, the expressions of bias and variance can be used to define optimal
decision boundaries of the set of Adjusted count estimates to be used in the
ensemble. We show in three simulation studies that Continuous Sweep outperforms
the quantifiers in the group Classify, Count, and Correct, including Median
Sweep, and is competitive with the two best quantifiers from the group
Distribution Matchers. Also an empirical data set is analysed with these
quantifiers showing similar performances.","['Kevin Kloos', 'Julian D. Karch', 'Quinten A. Meertens', 'Mark de Rooij']","['stat.ML', 'cs.LG', '68U99']",2023-08-16 14:18:31+00:00
http://arxiv.org/abs/2308.08358v1,Convergence of Two-Layer Regression with Nonlinear Units,"Large language models (LLMs), such as ChatGPT and GPT4, have shown
outstanding performance in many human life task. Attention computation plays an
important role in training LLMs. Softmax unit and ReLU unit are the key
structure in attention computation. Inspired by them, we put forward a softmax
ReLU regression problem. Generally speaking, our goal is to find an optimal
solution to the regression problem involving the ReLU unit. In this work, we
calculate a close form representation for the Hessian of the loss function.
Under certain assumptions, we prove the Lipschitz continuous and the PSDness of
the Hessian. Then, we introduce an greedy algorithm based on approximate Newton
method, which converges in the sense of the distance to optimal solution. Last,
We relax the Lipschitz condition and prove the convergence in the sense of loss
value.","['Yichuan Deng', 'Zhao Song', 'Shenghao Xie']","['cs.LG', 'stat.ML']",2023-08-16 13:30:45+00:00
http://arxiv.org/abs/2308.08305v2,Warped geometric information on the optimisation of Euclidean functions,"We consider the fundamental task of optimising a real-valued function defined
in a potentially high-dimensional Euclidean space, such as the loss function in
many machine-learning tasks or the logarithm of the probability distribution in
statistical inference. We use Riemannian geometry notions to redefine the
optimisation problem of a function on the Euclidean space to a Riemannian
manifold with a warped metric, and then find the function's optimum along this
manifold. The warped metric chosen for the search domain induces a
computational friendly metric-tensor for which optimal search directions
associated with geodesic curves on the manifold becomes easier to compute.
Performing optimization along geodesics is known to be generally infeasible,
yet we show that in this specific manifold we can analytically derive Taylor
approximations up to third-order. In general these approximations to the
geodesic curve will not lie on the manifold, however we construct suitable
retraction maps to pull them back onto the manifold. Therefore, we can
efficiently optimize along the approximate geodesic curves. We cover the
related theory, describe a practical optimization algorithm and empirically
evaluate it on a collection of challenging optimisation benchmarks. Our
proposed algorithm, using 3rd-order approximation of geodesics, tends to
outperform standard Euclidean gradient-based counterparts in term of number of
iterations until convergence.","['Marcelo Hartmann', 'Bernardo Williams', 'Hanlin Yu', 'Mark Girolami', 'Alessandro Barp', 'Arto Klami']","['stat.ML', 'cs.LG']",2023-08-16 12:08:50+00:00
http://arxiv.org/abs/2308.08247v1,Two Phases of Scaling Laws for Nearest Neighbor Classifiers,"A scaling law refers to the observation that the test performance of a model
improves as the number of training data increases. A fast scaling law implies
that one can solve machine learning problems by simply boosting the data and
the model sizes. Yet, in many cases, the benefit of adding more data can be
negligible. In this work, we study the rate of scaling laws of nearest neighbor
classifiers. We show that a scaling law can have two phases: in the first
phase, the generalization error depends polynomially on the data dimension and
decreases fast; whereas in the second phase, the error depends exponentially on
the data dimension and decreases slowly. Our analysis highlights the complexity
of the data distribution in determining the generalization error. When the data
distributes benignly, our result suggests that nearest neighbor classifier can
achieve a generalization error that depends polynomially, instead of
exponentially, on the data dimension.","['Pengkun Yang', 'Jingzhao Zhang']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2023-08-16 09:28:55+00:00
http://arxiv.org/abs/2308.08165v2,Stochastic Controlled Averaging for Federated Learning with Communication Compression,"Communication compression, a technique aiming to reduce the information
volume to be transmitted over the air, has gained great interests in Federated
Learning (FL) for the potential of alleviating its communication overhead.
However, communication compression brings forth new challenges in FL due to the
interplay of compression-incurred information distortion and inherent
characteristics of FL such as partial participation and data heterogeneity.
Despite the recent development, the performance of compressed FL approaches has
not been fully exploited. The existing approaches either cannot accommodate
arbitrary data heterogeneity or partial participation, or require stringent
conditions on compression.
  In this paper, we revisit the seminal stochastic controlled averaging method
by proposing an equivalent but more efficient/simplified formulation with
halved uplink communication costs. Building upon this implementation, we
propose two compressed FL algorithms, SCALLION and SCAFCOM, to support unbiased
and biased compression, respectively. Both the proposed methods outperform the
existing compressed FL methods in terms of communication and computation
complexities. Moreover, SCALLION and SCAFCOM accommodates arbitrary data
heterogeneity and do not make any additional assumptions on compression errors.
Experiments show that SCALLION and SCAFCOM can match the performance of
corresponding full-precision FL approaches with substantially reduced uplink
communication, and outperform recent compressed FL methods under the same
communication budget.","['Xinmeng Huang', 'Ping Li', 'Xiaoyun Li']","['math.OC', 'cs.DC', 'cs.LG', 'stat.ML']",2023-08-16 06:35:36+00:00
http://arxiv.org/abs/2308.08158v1,Deep Generative Imputation Model for Missing Not At Random Data,"Data analysis usually suffers from the Missing Not At Random (MNAR) problem,
where the cause of the value missing is not fully observed. Compared to the
naive Missing Completely At Random (MCAR) problem, it is more in line with the
realistic scenario whereas more complex and challenging. Existing statistical
methods model the MNAR mechanism by different decomposition of the joint
distribution of the complete data and the missing mask. But we empirically find
that directly incorporating these statistical methods into deep generative
models is sub-optimal. Specifically, it would neglect the confidence of the
reconstructed mask during the MNAR imputation process, which leads to
insufficient information extraction and less-guaranteed imputation quality. In
this paper, we revisit the MNAR problem from a novel perspective that the
complete data and missing mask are two modalities of incomplete data on an
equal footing. Along with this line, we put forward a generative-model-specific
joint probability decomposition method, conjunction model, to represent the
distributions of two modalities in parallel and extract sufficient information
from both complete data and missing mask. Taking a step further, we exploit a
deep generative imputation model, namely GNR, to process the real-world missing
mechanism in the latent space and concurrently impute the incomplete data and
reconstruct the missing mask. The experimental results show that our GNR
surpasses state-of-the-art MNAR baselines with significant margins (averagely
improved from 9.9% to 18.8% in RMSE) and always gives a better mask
reconstruction accuracy which makes the imputation more principle.","['Jialei Chen', 'Yuanbo Xu', 'Pengyang Wang', 'Yongjian Yang']","['cs.LG', 'stat.ML']",2023-08-16 06:01:12+00:00
http://arxiv.org/abs/2308.08070v1,Max-affine regression via first-order methods,"We consider regression of a max-affine model that produces a piecewise linear
model by combining affine models via the max function. The max-affine model
ubiquitously arises in applications in signal processing and statistics
including multiclass classification, auction problems, and convex regression.
It also generalizes phase retrieval and learning rectifier linear unit
activation functions. We present a non-asymptotic convergence analysis of
gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for
max-affine regression when the model is observed at random locations following
the sub-Gaussianity and an anti-concentration with additive sub-Gaussian noise.
Under these assumptions, a suitably initialized GD and SGD converge linearly to
a neighborhood of the ground truth specified by the corresponding error bound.
We provide numerical results that corroborate the theoretical finding.
Importantly, SGD not only converges faster in run time with fewer observations
than alternating minimization and GD in the noiseless scenario but also
outperforms them in low-sample scenarios with noise.","['Seonho Kim', 'Kiryung Lee']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",2023-08-15 23:46:44+00:00
http://arxiv.org/abs/2308.08060v1,Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation,"Tensor factorizations (TF) are powerful tools for the efficient
representation and analysis of multidimensional data. However, classic TF
methods based on maximum likelihood estimation underperform when applied to
zero-inflated count data, such as single-cell RNA sequencing (scRNA-seq) data.
Additionally, the stochasticity inherent in TFs results in factors that vary
across repeated runs, making interpretation and reproducibility of the results
challenging. In this paper, we introduce Zero Inflated Poisson Tensor
Factorization (ZIPTF), a novel approach for the factorization of
high-dimensional count data with excess zeros. To address the challenge of
stochasticity, we introduce Consensus Zero Inflated Poisson Tensor
Factorization (C-ZIPTF), which combines ZIPTF with a consensus-based
meta-analysis. We evaluate our proposed ZIPTF and C-ZIPTF on synthetic
zero-inflated count data and synthetic and real scRNA-seq data. ZIPTF
consistently outperforms baseline matrix and tensor factorization methods in
terms of reconstruction accuracy for zero-inflated data. When the probability
of excess zeros is high, ZIPTF achieves up to $2.4\times$ better accuracy.
Additionally, C-ZIPTF significantly improves the consistency and accuracy of
the factorization. When tested on both synthetic and real scRNA-seq data, ZIPTF
and C-ZIPTF consistently recover known and biologically meaningful gene
expression programs.","['Daniel Chafamo', 'Vignesh Shanmugam', 'Neriman Tokcan']","['stat.ML', 'cs.LG', 'math.AG', 'q-bio.GN', 'stat.AP', '92Bxx, 62F15, 68-XX, 68Txx, 11E76, 11P05, 12D15, 14N10']",2023-08-15 22:25:15+00:00
http://arxiv.org/abs/2308.08055v2,Simple online learning with consistent oracle,"We consider online learning in the model where a learning algorithm can
access the class only via the \emph{consistent oracle} -- an oracle, that, at
any moment, can give a function from the class that agrees with all examples
seen so far. This model was recently considered by Assos et al.~(COLT'23). It
is motivated by the fact that standard methods of online learning rely on
computing the Littlestone dimension of subclasses, a computationally
intractable problem.
  Assos et al.~gave an online learning algorithm in this model that makes at
most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute
unspecified constant $C > 0$. We give a novel algorithm that makes at most
$O(256^d)$ mistakes. Our proof is significantly simpler and uses only very
basic properties of the Littlestone dimension. We also show that there exists
no algorithm in this model that makes less than $3^d$ mistakes.","['Alexander Kozachinskiy', 'Tomasz Steifer']","['cs.LG', 'cs.AI', 'stat.ML']",2023-08-15 21:50:40+00:00
http://arxiv.org/abs/2308.08046v1,Regret Lower Bounds in Multi-agent Multi-armed Bandit,"Multi-armed Bandit motivates methods with provable upper bounds on regret and
also the counterpart lower bounds have been extensively studied in this
context. Recently, Multi-agent Multi-armed Bandit has gained significant
traction in various domains, where individual clients face bandit problems in a
distributed manner and the objective is the overall system performance,
typically measured by regret. While efficient algorithms with regret upper
bounds have emerged, limited attention has been given to the corresponding
regret lower bounds, except for a recent lower bound for adversarial settings,
which, however, has a gap with let known upper bounds. To this end, we herein
provide the first comprehensive study on regret lower bounds across different
settings and establish their tightness. Specifically, when the graphs exhibit
good connectivity properties and the rewards are stochastically distributed, we
demonstrate a lower bound of order $O(\log T)$ for instance-dependent bounds
and $\sqrt{T}$ for mean-gap independent bounds which are tight. Assuming
adversarial rewards, we establish a lower bound $O(T^{\frac{2}{3}})$ for
connected graphs, thereby bridging the gap between the lower and upper bound in
the prior work. We also show a linear regret lower bound when the graph is
disconnected. While previous works have explored these settings with upper
bounds, we provide a thorough study on tight lower bounds.","['Mengfan Xu', 'Diego Klabjan']","['cs.LG', 'stat.ML']",2023-08-15 21:20:24+00:00
http://arxiv.org/abs/2308.08030v1,Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks,"This paper studies the binary classification of unbounded data from ${\mathbb
R}^d$ generated under Gaussian Mixture Models (GMMs) using deep ReLU neural
networks. We obtain $\unicode{x2013}$ for the first time $\unicode{x2013}$
non-asymptotic upper bounds and convergence rates of the excess risk (excess
misclassification error) for the classification without restrictions on model
parameters. The convergence rates we derive do not depend on dimension $d$,
demonstrating that deep ReLU networks can overcome the curse of dimensionality
in classification. While the majority of existing generalization analysis of
classification algorithms relies on a bounded domain, we consider an unbounded
domain by leveraging the analyticity and fast decay of Gaussian distributions.
To facilitate our analysis, we give a novel approximation error bound for
general analytic functions using ReLU networks, which may be of independent
interest. Gaussian distributions can be adopted nicely to model data arising in
applications, e.g., speeches, images, and texts; our results provide a
theoretical verification of the observed efficiency of deep neural networks in
practical classification problems.","['Tian-Yi Zhou', 'Xiaoming Huo']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2023-08-15 20:40:42+00:00
http://arxiv.org/abs/2308.08025v1,Potential Energy Advantage of Quantum Economy,"Energy cost is increasingly crucial in the modern computing industry with the
wide deployment of large-scale machine learning models and language models. For
the firms that provide computing services, low energy consumption is important
both from the perspective of their own market growth and the government's
regulations. In this paper, we study the energy benefits of quantum computing
vis-a-vis classical computing. Deviating from the conventional notion of
quantum advantage based solely on computational complexity, we redefine
advantage in an energy efficiency context. Through a Cournot competition model
constrained by energy usage, we demonstrate quantum computing firms can
outperform classical counterparts in both profitability and energy efficiency
at Nash equilibrium. Therefore quantum computing may represent a more
sustainable pathway for the computing industry. Moreover, we discover that the
energy benefits of quantum computing economies are contingent on large-scale
computation. Based on real physical parameters, we further illustrate the scale
of operation necessary for realizing this energy efficiency advantage.","['Junyu Liu', 'Hansheng Jiang', 'Zuo-Jun Max Shen']","['quant-ph', 'cs.AI', 'cs.ET', 'cs.LG', 'stat.ML']",2023-08-15 20:30:52+00:00
http://arxiv.org/abs/2308.07983v2,Monte Carlo guided Diffusion for Bayesian linear inverse problems,"Ill-posed linear inverse problems arise frequently in various applications,
from computational photography to medical imaging. A recent line of research
exploits Bayesian inference with informative priors to handle the ill-posedness
of such problems. Amongst such priors, score-based generative models (SGM) have
recently been successfully applied to several different inverse problems. In
this study, we exploit the particular structure of the prior defined by the SGM
to define a sequence of intermediate linear inverse problems. As the noise
level decreases, the posteriors of these inverse problems get closer to the
target posterior of the original inverse problem. To sample from this sequence
of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The
proposed algorithm, MCGDiff, is shown to be theoretically grounded and we
provide numerical simulations showing that it outperforms competing baselines
when dealing with ill-posed inverse problems in a Bayesian setting.","['Gabriel Cardoso', 'Yazid Janati El Idrissi', 'Sylvain Le Corff', 'Eric Moulines']","['stat.ML', 'cs.LG', 'stat.ME']",2023-08-15 18:32:00+00:00
http://arxiv.org/abs/2308.07896v3,SciRE-Solver: Accelerating Diffusion Models Sampling by Score-integrand Solver with Recursive Difference,"Diffusion models (DMs) have made significant progress in the fields of image,
audio, and video generation. One downside of DMs is their slow iterative
process. Recent algorithms for fast sampling are designed from the perspective
of differential equations. However, in higher-order algorithms based on Taylor
expansion, estimating the derivative of the score function becomes intractable
due to the complexity of large-scale, well-trained neural networks. Driven by
this motivation, in this work, we introduce the recursive difference (RD)
method to calculate the derivative of the score function in the realm of DMs.
Based on the RD method and the truncated Taylor expansion of score-integrand,
we propose SciRE-Solver with the convergence order guarantee for accelerating
sampling of DMs. To further investigate the effectiveness of the RD method, we
also propose a variant named SciREI-Solver based on the RD method and
exponential integrator. Our proposed sampling algorithms with RD method attain
state-of-the-art (SOTA) FIDs in comparison to existing training-free sampling
algorithms, across both discrete-time and continuous-time pre-trained DMs,
under various number of score function evaluations (NFE). Remarkably,
SciRE-Solver using a small NFEs demonstrates promising potential to surpass the
FID achieved by some pre-trained models in their original papers using no fewer
than $1000$ NFEs. For example, we reach SOTA value of $2.40$ FID with $100$ NFE
for continuous-time DM and of $3.15$ FID with $84$ NFE for discrete-time DM on
CIFAR-10, as well as of $2.17$ (2.02) FID with $18$ (50) NFE for discrete-time
DM on CelebA 64$\times$64.","['Shigui Li', 'Wei Chen', 'Delu Zeng']","['stat.ML', 'cs.LG', 'math.DS', 'stat.CO']",2023-08-15 17:37:44+00:00
http://arxiv.org/abs/2308.07887v1,On regularized Radon-Nikodym differentiation,"We discuss the problem of estimating Radon-Nikodym derivatives. This problem
appears in various applications, such as covariate shift adaptation,
likelihood-ratio testing, mutual information estimation, and conditional
probability estimation. To address the above problem, we employ the general
regularization scheme in reproducing kernel Hilbert spaces. The convergence
rate of the corresponding regularized algorithm is established by taking into
account both the smoothness of the derivative and the capacity of the space in
which it is estimated. This is done in terms of general source conditions and
the regularized Christoffel functions. We also find that the reconstruction of
Radon-Nikodym derivatives at any particular point can be done with high order
of accuracy. Our theoretical results are illustrated by numerical simulations.","['Duc Hoan Nguyen', 'Werner Zellinger', 'Sergei V. Pereverzyev']","['math.ST', 'cs.LG', 'cs.NA', 'math.NA', 'stat.ML', 'stat.TH', '68T05, 68Q32']",2023-08-15 17:27:16+00:00
http://arxiv.org/abs/2308.07843v6,Dyadic Reinforcement Learning,"Mobile health aims to enhance health outcomes by delivering interventions to
individuals as they go about their daily life. The involvement of care partners
and social support networks often proves crucial in helping individuals
managing burdensome medical conditions. This presents opportunities in mobile
health to design interventions that target the dyadic relationship -- the
relationship between a target person and their care partner -- with the aim of
enhancing social support. In this paper, we develop dyadic RL, an online
reinforcement learning algorithm designed to personalize intervention delivery
based on contextual factors and past responses of a target person and their
care partner. Here, multiple sets of interventions impact the dyad across
multiple time intervals. The developed dyadic RL is Bayesian and hierarchical.
We formally introduce the problem setup, develop dyadic RL and establish a
regret bound. We demonstrate dyadic RL's empirical performance through
simulation studies on both toy scenarios and on a realistic test bed
constructed from data collected in a mobile health study.","['Shuangning Li', 'Lluis Salvat Niell', 'Sung Won Choi', 'Inbal Nahum-Shani', 'Guy Shani', 'Susan Murphy']","['cs.LG', 'stat.AP', 'stat.ML']",2023-08-15 15:43:12+00:00
http://arxiv.org/abs/2308.07673v1,A Review of Adversarial Attacks in Computer Vision,"Deep neural networks have been widely used in various downstream tasks,
especially those safety-critical scenario such as autonomous driving, but deep
networks are often threatened by adversarial samples. Such adversarial attacks
can be invisible to human eyes, but can lead to DNN misclassification, and
often exhibits transferability between deep learning and machine learning
models and real-world achievability. Adversarial attacks can be divided into
white-box attacks, for which the attacker knows the parameters and gradient of
the model, and black-box attacks, for the latter, the attacker can only obtain
the input and output of the model. In terms of the attacker's purpose, it can
be divided into targeted attacks and non-targeted attacks, which means that the
attacker wants the model to misclassify the original sample into the specified
class, which is more practical, while the non-targeted attack just needs to
make the model misclassify the sample. The black box setting is a scenario we
will encounter in practice.","['Yutong Zhang', 'Yao Li', 'Yin Li', 'Zhichang Guo']","['cs.CV', 'stat.CO', 'stat.ML']",2023-08-15 09:43:10+00:00
http://arxiv.org/abs/2308.07536v1,Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem,"In this paper, we study a class of stochastic bilevel optimization problems,
also known as stochastic simple bilevel optimization, where we minimize a
smooth stochastic objective function over the optimal solution set of another
stochastic convex optimization problem. We introduce novel stochastic bilevel
optimization methods that locally approximate the solution set of the
lower-level problem via a stochastic cutting plane, and then run a conditional
gradient update with variance reduction techniques to control the error induced
by using stochastic gradients. For the case that the upper-level function is
convex, our method requires
$\tilde{\mathcal{O}}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\}) $ stochastic
oracle queries to obtain a solution that is $\epsilon_f$-optimal for the
upper-level and $\epsilon_g$-optimal for the lower-level. This guarantee
improves the previous best-known complexity of
$\mathcal{O}(\max\{1/\epsilon_f^{4},1/\epsilon_g^{4}\})$. Moreover, for the
case that the upper-level function is non-convex, our method requires at most
$\tilde{\mathcal{O}}(\max\{1/\epsilon_f^{3},1/\epsilon_g^{3}\}) $ stochastic
oracle queries to find an $(\epsilon_f, \epsilon_g)$-stationary point. In the
finite-sum setting, we show that the number of stochastic oracle calls required
by our method are $\tilde{\mathcal{O}}(\sqrt{n}/\epsilon)$ and
$\tilde{\mathcal{O}}(\sqrt{n}/\epsilon^{2})$ for the convex and non-convex
settings, respectively, where $\epsilon=\min \{\epsilon_f,\epsilon_g\}$.","['Jincheng Cao', 'Ruichen Jiang', 'Nazanin Abolfazli', 'Erfan Yazdandoost Hamedani', 'Aryan Mokhtari']","['math.OC', 'cs.LG', 'stat.ML']",2023-08-15 02:37:11+00:00
http://arxiv.org/abs/2308.07523v2,Deep Neural Operator Driven Real Time Inference for Nuclear Systems to Enable Digital Twin Solutions,"This paper focuses on the feasibility of Deep Neural Operator (DeepONet) as a
robust surrogate modeling method within the context of digital twin (DT) for
nuclear energy systems. Through benchmarking and evaluation, this study
showcases the generalizability and computational efficiency of DeepONet in
solving a challenging particle transport problem. DeepONet also exhibits
remarkable prediction accuracy and speed, outperforming traditional ML methods,
making it a suitable algorithm for real-time DT inference. However, the
application of DeepONet also reveals challenges related to optimal sensor
placement and model evaluation, critical aspects of real-world implementation.
Addressing these challenges will further enhance the method's practicality and
reliability. Overall, DeepONet presents a promising and transformative nuclear
engineering research and applications tool. Its accurate prediction and
computational efficiency capabilities can revolutionize DT systems, advancing
nuclear engineering research. This study marks an important step towards
harnessing the power of surrogate modeling techniques in critical engineering
domains.","['Kazuma Kobayashi', 'Syed Bahauddin Alam']","['stat.ML', 'cs.LG', 'stat.CO']",2023-08-15 01:25:35+00:00
http://arxiv.org/abs/2308.07520v2,"Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning","The goal of Causal Discovery is to find automated search methods for learning
causal structures from observational data. In some cases all variables of the
interested causal mechanism are measured, and the task is to predict the
effects one measured variable has on another. In contrast, sometimes the
variables of primary interest are not directly observable but instead inferred
from their manifestations in the data. These are referred to as latent
variables. One commonly known example is the psychological construct of
intelligence, which cannot directly measured so researchers try to assess
through various indicators such as IQ tests. In this case, casual discovery
algorithms can uncover underlying patterns and structures to reveal the causal
connections between the latent variables and between the latent and observed
variables. This thesis focuses on two questions in causal discovery: providing
an alternative definition of k-Triangle Faithfulness that (i) is weaker than
strong faithfulness when applied to the Gaussian family of distributions, (ii)
can be applied to non-Gaussian families of distributions, and (iii) under the
assumption that the modified version of Strong Faithfulness holds, can be used
to show the uniform consistency of a modified causal discovery algorithm;
relaxing the sufficiency assumption to learn causal structures with latent
variables. Given the importance of inferring cause-and-effect relationships for
understanding and forecasting complex systems, the work in this thesis of
relaxing various simplification assumptions is expected to extend the causal
discovery method to be applicable in a wider range with diversified causal
mechanism and statistical phenomena.",['Shuyan Wang'],"['stat.ML', 'cs.AI', 'cs.LG']",2023-08-15 01:23:42+00:00
http://arxiv.org/abs/2308.07424v2,Addressing Distribution Shift in RTB Markets via Exponential Tilting,"In machine learning applications, distribution shifts between training and
target environments can lead to significant drops in model performance. This
study investigates the impact of such shifts on binary classification models
within the Real-Time Bidding (RTB) market context, where selection bias
contributes to these shifts. To address this challenge, we apply the
Exponential Tilt Reweighting Alignment (ExTRA) algorithm, proposed by Maity et
al. (2023). This algorithm estimates importance weights for the empirical risk
by considering both covariate and label distributions, without requiring target
label information, by assuming a specific weight structure. The goal of this
study is to estimate weights that correct for the distribution shifts in RTB
model and to evaluate the efficiency of the proposed model using simulated
real-world data.","['Minji Kim', 'Seong Jin Lee', 'Bumsik Kim']","['stat.ML', 'cs.LG']",2023-08-14 19:31:58+00:00
http://arxiv.org/abs/2308.07418v2,Locally Adaptive and Differentiable Regression,"Over-parameterized models like deep nets and random forests have become very
popular in machine learning. However, the natural goals of continuity and
differentiability, common in regression models, are now often ignored in modern
overparametrized, locally-adaptive models. We propose a general framework to
construct a global continuous and differentiable model based on a weighted
average of locally learned models in corresponding local regions. This model is
competitive in dealing with data with different densities or scales of function
values in different local regions. We demonstrate that when we mix kernel ridge
and polynomial regression terms in the local models, and stitch them together
continuously, we achieve faster statistical convergence in theory and improved
performance in various practical settings.","['Mingxuan Han', 'Varun Shankar', 'Jeff M Phillips', 'Chenglong Ye']","['cs.LG', 'stat.ML']",2023-08-14 19:12:40+00:00
http://arxiv.org/abs/2308.07247v1,Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI,"The Rash\=omon effect poses challenges for deriving reliable knowledge from
machine learning models. This study examined the influence of sample size on
explanations from models in a Rash\=omon set using SHAP. Experiments on 5
public datasets showed that explanations gradually converged as the sample size
increased. Explanations from <128 samples exhibited high variability, limiting
reliable knowledge extraction. However, agreement between models improved with
more data, allowing for consensus. Bagging ensembles often had higher
agreement. The results provide guidance on sufficient data to trust
explanations. Variability at low samples suggests that conclusions may be
unreliable without validation. Further work is needed with more model types,
data domains, and explanation methods. Testing convergence in neural networks
and with model-specific explanation methods would be impactful. The approaches
explored here point towards principled techniques for eliciting knowledge from
ambiguous models.","['Clement Poiret', 'Antoine Grigis', 'Justin Thomas', 'Marion Noulhiane']","['cs.LG', 'cs.AI', 'stat.ML', 'H.1.1; I.6.4; I.2.1']",2023-08-14 16:32:24+00:00
http://arxiv.org/abs/2308.07126v2,A Time-aware tensor decomposition for tracking evolving patterns,"Time-evolving data sets can often be arranged as a higher-order tensor with
one of the modes being the time mode. While tensor factorizations have been
successfully used to capture the underlying patterns in such higher-order data
sets, the temporal aspect is often ignored, allowing for the reordering of time
points. In recent studies, temporal regularizers are incorporated in the time
mode to tackle this issue. Nevertheless, existing approaches still do not allow
underlying patterns to change in time (e.g., spatial changes in the brain,
contextual changes in topics). In this paper, we propose temporal PARAFAC2
(tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal
regularization to extract gradually evolving patterns from temporal data.
Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2
can capture the underlying evolving patterns accurately performing better than
PARAFAC2 and coupled matrix factorization with temporal smoothness
regularization.","['Christos Chatzis', 'Max Pfeffer', 'Pedro Lind', 'Evrim Acar']","['cs.LG', 'cs.CV', 'stat.ML']",2023-08-14 13:13:50+00:00
http://arxiv.org/abs/2308.07117v1,iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN,"The inverse short-time Fourier transform network (iSTFTNet) has garnered
attention owing to its fast, lightweight, and high-fidelity speech synthesis.
It obtains these characteristics using a fast and lightweight 1D CNN as the
backbone and replacing some neural processes with iSTFT. Owing to the
difficulty of a 1D CNN to model high-dimensional spectrograms, the frequency
dimension is reduced via temporal upsampling. However, this strategy
compromises the potential to enhance the speed. Therefore, we propose
iSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and
2D CNNs to model temporal and spectrogram structures, respectively. We designed
a 2D CNN that performs frequency upsampling after conversion in a few-frequency
space. This design facilitates the modeling of high-dimensional spectrograms
without compromising the speed. The results demonstrated that iSTFTNet2 made
iSTFTNet faster and more lightweight with comparable speech quality. Audio
samples are available at
https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.","['Takuhiro Kaneko', 'Hirokazu Kameoka', 'Kou Tanaka', 'Shogo Seki']","['cs.SD', 'cs.LG', 'eess.AS', 'stat.ML']",2023-08-14 12:56:31+00:00
http://arxiv.org/abs/2308.07047v1,No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning,"Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a
sample. In reality, it is always laborious to obtain complete degrees, giving
birth to the Incomplete LDL (InLDL). However, InLDL often suffers from
performance degeneration. To remedy it, existing methods need one or more
explicit regularizations, leading to burdensome parameter tuning and extra
computation. We argue that label distribution itself may provide useful prior,
when used appropriately, the InLDL problem can be solved without any explicit
regularization. In this paper, we offer a rational alternative to use such a
prior. Our intuition is that large degrees are likely to get more concern, the
small ones are easily overlooked, whereas the missing degrees are completely
neglected in InLDL. To learn an accurate label distribution, it is crucial not
to ignore the small observed degrees but to give them properly large weights,
while gradually increasing the weights of the missing degrees. To this end, we
first define a weighted empirical risk and derive upper bounds between the
expected risk and the weighted empirical risk, which reveals in principle that
weighting plays an implicit regularization role. Then, by using the prior of
degrees, we design a weighted scheme and verify its effectiveness. To sum up,
our model has four advantages, it is 1) model selection free, as no explicit
regularization is imposed; 2) with closed form solution (sub-problem) and
easy-to-implement (a few lines of codes); 3) with linear computational
complexity in the number of samples, thus scalable to large datasets; 4)
competitive with state-of-the-arts even without any explicit regularization.","['Xiang Li', 'Songcan Chen']","['cs.LG', 'stat.ML']",2023-08-14 10:16:12+00:00
http://arxiv.org/abs/2308.07012v1,Greedy online change point detection,"Standard online change point detection (CPD) methods tend to have large false
discovery rates as their detections are sensitive to outliers. To overcome this
drawback, we propose Greedy Online Change Point Detection (GOCPD), a
computationally appealing method which finds change points by maximizing the
probability of the data coming from the (temporal) concatenation of two
independent models. We show that, for time series with a single change point,
this objective is unimodal and thus CPD can be accelerated via ternary search
with logarithmic complexity. We demonstrate the effectiveness of GOCPD on
synthetic data and validate our findings on real-world univariate and
multivariate settings.","['Jou-Hui Ho', 'Felipe Tobar']","['eess.SP', 'cs.LG', 'stat.ML']",2023-08-14 08:59:59+00:00
http://arxiv.org/abs/2308.06869v1,Shape-Graph Matching Network (SGM-net): Registration for Statistical Shape Analysis,"This paper focuses on the statistical analysis of shapes of data objects
called shape graphs, a set of nodes connected by articulated curves with
arbitrary shapes. A critical need here is a constrained registration of points
(nodes to nodes, edges to edges) across objects. This, in turn, requires
optimization over the permutation group, made challenging by differences in
nodes (in terms of numbers, locations) and edges (in terms of shapes,
placements, and sizes) across objects. This paper tackles this registration
problem using a novel neural-network architecture and involves an unsupervised
loss function developed using the elastic shape metric for curves. This
architecture results in (1) state-of-the-art matching performance and (2) an
order of magnitude reduction in the computational cost relative to baseline
approaches. We demonstrate the effectiveness of the proposed approach using
both simulated data and real-world 2D and 3D shape graphs. Code and data will
be made publicly available after review to foster research.","['Shenyuan Liang', 'Mauricio Pamplona Segundo', 'Sathyanarayanan N. Aakur', 'Sudeep Sarkar', 'Anuj Srivastava']","['cs.CV', 'stat.ML']",2023-08-14 00:42:03+00:00
http://arxiv.org/abs/2308.06769v2,Fréchet Statistics Based Change Point Detection in Multivariate Hawkes Process,"This paper proposes a new approach for change point detection in causal
networks of multivariate Hawkes processes using Frechet statistics. Our method
splits the point process into overlapping windows, estimates kernel matrices in
each window, and reconstructs the signed Laplacians by treating the kernel
matrices as the adjacency matrices of the causal network. We demonstrate the
effectiveness of our method through experiments on both simulated and
real-world cryptocurrency datasets. Our results show that our method is capable
of accurately detecting and characterizing changes in the causal structure of
multivariate Hawkes processes, and may have potential applications in fields
such as finance and neuroscience. The proposed method is an extension of
previous work on Frechet statistics in point process settings and represents an
important contribution to the field of change point detection in multivariate
point processes.","['Rui Luo', 'Vikram Krishnamurthy']","['stat.ML', 'cs.SI', 'eess.SP']",2023-08-13 13:46:38+00:00
http://arxiv.org/abs/2308.06740v1,Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection,"Sparse Partial Least Squares (sPLS) is a common dimensionality reduction
technique for data fusion, which projects data samples from two views by
seeking linear combinations with a small number of variables with the maximum
variance. However, sPLS extracts the combinations between two data sets with
all data samples so that it cannot detect latent subsets of samples. To extend
the application of sPLS by identifying a specific subset of samples and remove
outliers, we propose an $\ell_\infty/\ell_0$-norm constrained weighted sparse
PLS ($\ell_\infty/\ell_0$-wsPLS) method for joint sample and feature selection,
where the $\ell_\infty/\ell_0$-norm constrains are used to select a subset of
samples. We prove that the $\ell_\infty/\ell_0$-norm constrains have the
Kurdyka-\L{ojasiewicz}~property so that a globally convergent algorithm is
developed to solve it. Moreover, multi-view data with a same set of samples can
be available in various real problems. To this end, we extend the
$\ell_\infty/\ell_0$-wsPLS model and propose two multi-view wsPLS models for
multi-view data fusion. We develop an efficient iterative algorithm for each
multi-view wsPLS model and show its convergence property. As well as numerical
and biomedical data experiments demonstrate the efficiency of the proposed
methods.","['Wenwen Min', 'Taosheng Xu', 'Chris Ding']","['cs.LG', 'stat.ML']",2023-08-13 10:09:25+00:00
http://arxiv.org/abs/2308.06717v1,Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards,"In practice, incentive providers (i.e., principals) often cannot observe the
reward realizations of incentivized agents, which is in contrast to many
principal-agent models that have been previously studied. This information
asymmetry challenges the principal to consistently estimate the agent's unknown
rewards by solely watching the agent's decisions, which becomes even more
challenging when the agent has to learn its own rewards. This complex setting
is observed in various real-life scenarios ranging from renewable energy
storage contracts to personalized healthcare incentives. Hence, it offers not
only interesting theoretical questions but also wide practical relevance. This
paper explores a repeated adverse selection game between a self-interested
learning agent and a learning principal. The agent tackles a multi-armed bandit
(MAB) problem to maximize their expected reward plus incentive. On top of the
agent's learning, the principal trains a parallel algorithm and faces a
trade-off between consistently estimating the agent's unknown rewards and
maximizing their own utility by offering adaptive incentives to lead the agent.
For a non-parametric model, we introduce an estimator whose only input is the
history of principal's incentives and agent's choices. We unite this estimator
with a proposed data-driven incentive policy within a MAB framework. Without
restricting the type of the agent's algorithm, we prove finite-sample
consistency of the estimator and a rigorous regret bound for the principal by
considering the sequential externality imposed by the agent. Lastly, our
theoretical results are reinforced by simulations justifying applicability of
our framework to green energy aggregator contracts.","['Ilgin Dogan', 'Zuo-Jun Max Shen', 'Anil Aswani']","['cs.LG', 'cs.AI', 'cs.GT', 'stat.ML']",2023-08-13 08:12:01+00:00
http://arxiv.org/abs/2308.06671v1,Law of Balance and Stationary Distribution of Stochastic Gradient Descent,"The stochastic gradient descent (SGD) algorithm is the algorithm we use to
train neural networks. However, it remains poorly understood how the SGD
navigates the highly nonlinear and degenerate loss landscape of a neural
network. In this work, we prove that the minibatch noise of SGD regularizes the
solution towards a balanced solution whenever the loss function contains a
rescaling symmetry. Because the difference between a simple diffusion process
and SGD dynamics is the most significant when symmetries are present, our
theory implies that the loss function symmetries constitute an essential probe
of how SGD works. We then apply this result to derive the stationary
distribution of stochastic gradient flow for a diagonal linear network with
arbitrary depth and width. The stationary distribution exhibits complicated
nonlinear phenomena such as phase transitions, broken ergodicity, and
fluctuation inversion. These phenomena are shown to exist uniquely in deep
networks, implying a fundamental difference between deep and shallow models.","['Liu Ziyin', 'Hongchao Li', 'Masahito Ueda']","['cs.LG', 'cs.AI', 'stat.ML']",2023-08-13 03:13:03+00:00
http://arxiv.org/abs/2308.06424v2,Multiclass Learnability Does Not Imply Sample Compression,"A hypothesis class admits a sample compression scheme, if for every sample
labeled by a hypothesis from the class, it is possible to retain only a small
subsample, using which the labels on the entire sample can be inferred. The
size of the compression scheme is an upper bound on the size of the subsample
produced. Every learnable binary hypothesis class (which must necessarily have
finite VC dimension) admits a sample compression scheme of size only a finite
function of its VC dimension, independent of the sample size. For multiclass
hypothesis classes, the analog of VC dimension is the DS dimension. We show
that the analogous statement pertaining to sample compression is not true for
multiclass hypothesis classes: every learnable multiclass hypothesis class,
which must necessarily have finite DS dimension, does not admit a sample
compression scheme of size only a finite function of its DS dimension.",['Chirag Pabbaraju'],"['cs.LG', 'stat.ML']",2023-08-12 00:26:08+00:00
http://arxiv.org/abs/2308.06422v3,Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation,"As the complexity and computational demands of deep learning models rise, the
need for effective optimization methods for neural network designs becomes
paramount. This work introduces an innovative search mechanism for
automatically selecting the best bit-width and layer-width for individual
neural network layers. This leads to a marked enhancement in deep neural
network efficiency. The search domain is strategically reduced by leveraging
Hessian-based pruning, ensuring the removal of non-crucial parameters.
Subsequently, we detail the development of surrogate models for favorable and
unfavorable outcomes by employing a cluster-based tree-structured Parzen
estimator. This strategy allows for a streamlined exploration of architectural
possibilities and swift pinpointing of top-performing designs. Through rigorous
testing on well-known datasets, our method proves its distinct advantage over
existing methods. Compared to leading compression strategies, our approach
records an impressive 20% decrease in model size without compromising accuracy.
Additionally, our method boasts a 12x reduction in search time relative to the
best search-focused strategies currently available. As a result, our proposed
method represents a leap forward in neural network design optimization, paving
the way for quick model design and implementation in settings with limited
resources, thereby propelling the potential of scalable deep learning
solutions.","['Seyedarmin Azizi', 'Mahdi Nazemi', 'Arash Fayyazi', 'Massoud Pedram']","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",2023-08-12 00:16:51+00:00
http://arxiv.org/abs/2308.06399v5,Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering,"Maize, a crucial crop globally cultivated across vast regions, especially in
sub-Saharan Africa, Asia, and Latin America, occupies 197 million hectares as
of 2021. Various statistical and machine learning models, including
mixed-effect models, random coefficients models, random forests, and deep
learning architectures, have been devised to predict maize yield. These models
consider factors such as genotype, environment, genotype-environment
interaction, and field management. However, the existing models often fall
short of fully exploiting the complex network of causal relationships among
these factors and the hierarchical structure inherent in agronomic data. This
study introduces an innovative approach integrating random effects into
Bayesian networks (BNs), leveraging their capacity to model causal and
probabilistic relationships through directed acyclic graphs. Rooted in the
linear mixed-effects models framework and tailored for hierarchical data, this
novel approach demonstrates enhanced BN learning. Application to a real-world
agronomic trial produces a model with improved interpretability, unveiling new
causal connections. Notably, the proposed method significantly reduces the
error rate in maize yield prediction from 28% to 17%. These results advocate
for the preference of BNs in constructing practical decision support tools for
hierarchical agronomic data, facilitating causal inference.","['Lorenzo Valleggi', 'Marco Scutari', 'Federico Mattia Stefanini']","['stat.ML', 'cs.LG', 'stat.AP']",2023-08-11 21:46:45+00:00
http://arxiv.org/abs/2308.06239v2,Private Distribution Learning with Public Data: The View from Sample Compression,"We study the problem of private distribution learning with access to public
data. In this setup, which we refer to as public-private learning, the learner
is given public and private samples drawn from an unknown distribution $p$
belonging to a class $\mathcal Q$, with the goal of outputting an estimate of
$p$ while adhering to privacy constraints (here, pure differential privacy)
only with respect to the private samples.
  We show that the public-private learnability of a class $\mathcal Q$ is
connected to the existence of a sample compression scheme for $\mathcal Q$, as
well as to an intermediate notion we refer to as list learning. Leveraging this
connection: (1) approximately recovers previous results on Gaussians over
$\mathbb R^d$; and (2) leads to new ones, including sample complexity upper
bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for
agnostic and distribution-shift resistant learners, as well as closure
properties for public-private learnability under taking mixtures and products
of distributions. Finally, via the connection to list learning, we show that
for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for
private learnability, which is close to the known upper bound of $d+1$ public
samples.","['Shai Ben-David', 'Alex Bie', 'Clément L. Canonne', 'Gautam Kamath', 'Vikrant Singhal']","['cs.LG', 'cs.CR', 'stat.ML']",2023-08-11 17:15:12+00:00
http://arxiv.org/abs/2308.06220v2,Nonlinear Permuted Granger Causality,"Granger causal inference is a contentious but widespread method used in
fields ranging from economics to neuroscience. The original definition
addresses the notion of causality in time series by establishing functional
dependence conditional on a specified model. Adaptation of Granger causality to
nonlinear data remains challenging, and many methods apply in-sample tests that
do not incorporate out-of-sample predictability, leading to concerns of model
overfitting. To allow for out-of-sample comparison, a measure of functional
connectivity is explicitly defined using permutations of the covariate set.
Artificial neural networks serve as featurizers of the data to approximate any
arbitrary, nonlinear relationship, and consistent estimation of the variance
for each permutation is shown under certain conditions on the featurization
process and the model residuals. Performance of the permutation method is
compared to penalized variable selection, naive replacement, and omission
techniques via simulation, and it is applied to neuronal responses of acoustic
stimuli in the auditory cortex of anesthetized rats. Targeted use of the
Granger causal framework, when prior knowledge of the causal mechanisms in a
dataset are limited, can help to reveal potential predictive relationships
between sets of variables that warrant further study.","['Noah D. Gade', 'Jordan Rodu']","['stat.ME', 'stat.ML']",2023-08-11 16:44:16+00:00
http://arxiv.org/abs/2308.06213v2,Change Point Detection with Conceptors,"Offline change point detection retrospectively locates change points in a
time series. Many nonparametric methods that target i.i.d. mean and variance
changes fail in the presence of nonlinear temporal dependence, and model based
methods require a known, rigid structure. For the at most one change point
problem, we propose use of a conceptor matrix to learn the characteristic
dynamics of a baseline training window with arbitrary dependence structure. The
associated echo state network acts as a featurizer of the data, and change
points are identified from the nature of the interactions between the features
and their relationship to the baseline state. This model agnostic method can
suggest potential locations of interest that warrant further study. We prove
that, under mild assumptions, the method provides a consistent estimate of the
true change point, and quantile estimates are produced via a moving block
bootstrap of the original data. The method is evaluated with clustering metrics
and Type 1 error control on simulated data, and applied to publicly available
neural data from rats experiencing bouts of non-REM sleep prior to exploration
of a radial maze. With sufficient spacing, the framework provides a simple
extension to the sparse, multiple change point problem.","['Noah D. Gade', 'Jordan Rodu']","['stat.ML', 'cs.LG', 'stat.ME']",2023-08-11 16:32:00+00:00
http://arxiv.org/abs/2308.06203v2,"Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks","Uncertainties in the real world mean that is impossible for system designers
to anticipate and explicitly design for all scenarios that a robot might
encounter. Thus, robots designed like this are fragile and fail outside of
highly-controlled environments. Causal models provide a principled framework to
encode formal knowledge of the causal relationships that govern the robot's
interaction with its environment, in addition to probabilistic representations
of noise and uncertainty typically encountered by real-world robots. Combined
with causal inference, these models permit an autonomous agent to understand,
reason about, and explain its environment. In this work, we focus on the
problem of a robot block-stacking task due to the fundamental perception and
manipulation capabilities it demonstrates, required by many applications
including warehouse logistics and domestic human support robotics. We propose a
novel causal probabilistic framework to embed a physics simulation capability
into a structural causal model to permit robots to perceive and assess the
current state of a block-stacking task, reason about the next-best action from
placement candidates, and generate post-hoc counterfactual explanations. We
provide exemplar next-best action selection results and outline planned
experimentation in simulated and real-world robot block-stacking tasks.","['Ricardo Cannizzaro', 'Jonathan Routley', 'Lars Kunze']","['cs.RO', 'cs.AI', 'cs.LG', 'stat.AP', 'stat.ML', 'I.2.9; I.2.8; I.2.3; G.3; I.6.8']",2023-08-11 15:58:15+00:00
http://arxiv.org/abs/2308.06149v1,Gaussian Process Regression for Maximum Entropy Distribution,"Maximum-Entropy Distributions offer an attractive family of probability
densities suitable for moment closure problems. Yet finding the Lagrange
multipliers which parametrize these distributions, turns out to be a
computational bottleneck for practical closure settings. Motivated by recent
success of Gaussian processes, we investigate the suitability of Gaussian
priors to approximate the Lagrange multipliers as a map of a given set of
moments. Examining various kernel functions, the hyperparameters are optimized
by maximizing the log-likelihood. The performance of the devised data-driven
Maximum-Entropy closure is studied for couple of test cases including
relaxation of non-equilibrium distributions governed by Bhatnagar-Gross-Krook
and Boltzmann kinetic equations.","['Mohsen Sadr', 'Manuel Torrilhon', 'M. Hossein Gorji']","['stat.ML', 'cs.LG', 'math-ph', 'math.MP', 'physics.data-an']",2023-08-11 14:26:29+00:00
http://arxiv.org/abs/2308.06138v1,"Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling","Machine Learning (ML) is a powerful tool for material science applications.
Artificial Neural Network (ANN) is a machine learning technique that can
provide high prediction accuracy. This study aimed to develop an ANN model to
predict the cake moisture of the pressure filtration process of zinc
production. The cake moisture was influenced by seven parameters: temperature
(35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and
5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm),
pressure, and filtration time. The study conducted 288 tests using two types of
fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by
the Coefficient of determination (R2), the Mean Square Error (MSE), and the
Mean Absolute Error (MAE) metrics for both datasets. The results showed R2
values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE
values of 0.00056 and 0.00088 for S1 and S2, respectively. These results
indicated that the ANN model could predict the cake moisture of pressure
filtration in the zinc leaching process with high accuracy.","['Masoume Kazemi', 'Davood Moradkhani', 'Alireza A. Alipour']","['cs.LG', 'cs.AI', 'stat.ML']",2023-08-11 13:58:42+00:00
http://arxiv.org/abs/2308.06129v1,Uncertainty Quantification for Image-based Traffic Prediction across Cities,"Despite the strong predictive performance of deep learning models for traffic
prediction, their widespread deployment in real-world intelligent
transportation systems has been restrained by a lack of interpretability.
Uncertainty quantification (UQ) methods provide an approach to induce
probabilistic reasoning, improve decision-making and enhance model deployment
potential. To gain a comprehensive picture of the usefulness of existing UQ
methods for traffic prediction and the relation between obtained uncertainties
and city-wide traffic dynamics, we investigate their application to a
large-scale image-based traffic dataset spanning multiple cities and time
periods. We compare two epistemic and two aleatoric UQ methods on both temporal
and spatio-temporal transfer tasks, and find that meaningful uncertainty
estimates can be recovered. We further demonstrate how uncertainty estimates
can be employed for unsupervised outlier detection on changes in city traffic
dynamics. We find that our approach can capture both temporal and spatial
effects on traffic behaviour in a representative case study for the city of
Moscow. Our work presents a further step towards boosting uncertainty awareness
in traffic prediction tasks, and aims to highlight the value contribution of UQ
methods to a better understanding of city traffic dynamics.","['Alexander Timans', 'Nina Wiedemann', 'Nishant Kumar', 'Ye Hong', 'Martin Raubal']","['cs.CV', 'cs.LG', 'stat.ML', 'I.4.9; I.2.6; I.5.4; J.2']",2023-08-11 13:35:52+00:00
http://arxiv.org/abs/2308.06106v1,Hawkes Processes with Delayed Granger Causality,"We aim to explicitly model the delayed Granger causal effects based on
multivariate Hawkes processes. The idea is inspired by the fact that a causal
event usually takes some time to exert an effect. Studying this time lag itself
is of interest. Given the proposed model, we first prove the identifiability of
the delay parameter under mild conditions. We further investigate a model
estimation method under a complex setting, where we want to infer the posterior
distribution of the time lags and understand how this distribution varies
across different scenarios. We treat the time lags as latent variables and
formulate a Variational Auto-Encoder (VAE) algorithm to approximate the
posterior distribution of the time lags. By explicitly modeling the time lags
in Hawkes processes, we add flexibility to the model. The inferred time-lag
posterior distributions are of scientific meaning and help trace the original
causal time that supports the root cause analysis. We empirically evaluate our
model's event prediction and time-lag inference accuracy on synthetic and real
data, achieving promising results.","['Chao Yang', 'Hengyuan Miao', 'Shuang Li']","['cs.LG', 'stat.ML']",2023-08-11 12:43:43+00:00
http://arxiv.org/abs/2308.06058v2,Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction,"The recently proposed stochastic Polyak stepsize (SPS) and stochastic
line-search (SLS) for SGD have shown remarkable effectiveness when training
over-parameterized models. However, in non-interpolation settings, both
algorithms only guarantee convergence to a neighborhood of a solution which may
result in a worse output than the initial guess. While artificially decreasing
the adaptive stepsize has been proposed to address this issue (Orvieto et al.
[2022]), this approach results in slower convergence rates for convex and
over-parameterized models. In this work, we make two contributions: Firstly, we
propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which
guarantee convergence in non-interpolation settings and maintain sub-linear and
linear convergence rates for convex and strongly convex functions when training
over-parameterized models. AdaSLS requires no knowledge of problem-dependent
parameters, and AdaSPS requires only a lower bound of the optimal function
value as input. Secondly, we equip AdaSPS and AdaSLS with a novel variance
reduction technique and obtain algorithms that require
$\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve
an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves
upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without
variance reduction in the non-interpolation regimes. Moreover, our result
matches the fast rates of AdaSVRG but removes the inner-outer-loop structure,
which is easier to implement and analyze. Finally, numerical experiments on
synthetic and real datasets validate our theory and demonstrate the
effectiveness and robustness of our algorithms.","['Xiaowen Jiang', 'Sebastian U. Stich']","['cs.LG', 'math.OC', 'stat.ML']",2023-08-11 10:17:29+00:00
http://arxiv.org/abs/2308.05969v2,Learning nonparametric DAGs with incremental information via high-order HSIC,"Score-based methods for learning Bayesain networks(BN) aim to maximizing the
global score functions. However, if local variables have direct and indirect
dependence simultaneously, the global optimization on score functions misses
edges between variables with indirect dependent relationship, of which scores
are smaller than those with direct dependent relationship. In this paper, we
present an identifiability condition based on a determined subset of parents to
identify the underlying DAG. By the identifiability condition, we develop a
two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the
global optimization. In the optimal phase, an optimization problem based on
first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated
skeleton as the initial determined parents subset. In the tuning phase, the
skeleton is locally tuned by deletion, addition and DAG-formalization
strategies using the theoretically proved incremental properties of high-order
HSIC. Numerical experiments for different synthetic datasets and real-world
datasets show that the OT algorithm outperforms existing methods. Especially in
Sigmoid Mix model with the size of the graph being ${\rm\bf d=40}$, the
structure intervention distance (SID) of the OT algorithm is 329.7 smaller than
the one obtained by CAM, which indicates that the graph estimated by the OT
algorithm misses fewer edges compared with CAM.Source code of the OT algorithm
is available at https://github.com/YafeiannWang/optimal-tune-algorithm.","['Yafei Wang', 'Jianguo Liu']","['cs.LG', 'stat.ML']",2023-08-11 07:07:21+00:00
http://arxiv.org/abs/2308.05957v1,Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights,"Representing nodes in a network as dense vectors node embeddings is important
for understanding a given network and solving many downstream tasks. In
particular, for weighted homophilous graphs where similar nodes are connected
with larger edge weights, we desire node embeddings where node pairs with
strong weights have closer embeddings. Although random walk based node
embedding methods like node2vec and node2vec+ do work for weighted networks via
including edge weights in the walk transition probabilities, our experiments
show that the embedding result does not adequately reflect edge weights. In
this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge
Weights), a novel augmentation method for random walks that expands the corpus
in such a way that nodes with larger edge weights end up with closer
embeddings. ARGEW can work with any random walk based node embedding method,
because it is independent of the random sampling strategy itself and works on
top of the already-performed walks. With several real-world networks, we
demonstrate that with ARGEW, compared to not using it, the desired pattern that
node pairs with larger edge weights have closer embeddings is much clearer. We
also examine ARGEW's performance in node classification: node2vec with ARGEW
outperforms pure node2vec and is not sensitive to hyperparameters (i.e.
consistently good). In fact, it achieves similarly good results as supervised
GCN, even without any node feature or label information during training.
Finally, we explain why ARGEW works consistently well by exploring the
coappearance distributions using a synthetic graph with clear structural roles.","['Jun Hee Kim', 'Jaeman Son', 'Hyunsoo Kim', 'Eunjo Lee']","['cs.SI', 'cs.LG', 'stat.ML']",2023-08-11 06:19:23+00:00
http://arxiv.org/abs/2308.05903v1,Comparing the quality of neural network uncertainty estimates for classification problems,"Traditional deep learning (DL) models are powerful classifiers, but many
approaches do not provide uncertainties for their estimates. Uncertainty
quantification (UQ) methods for DL models have received increased attention in
the literature due to their usefulness in decision making, particularly for
high-consequence decisions. However, there has been little research done on how
to evaluate the quality of such methods. We use statistical methods of
frequentist interval coverage and interval width to evaluate the quality of
credible intervals, and expected calibration error to evaluate classification
predicted confidence. These metrics are evaluated on Bayesian neural networks
(BNN) fit using Markov Chain Monte Carlo (MCMC) and variational inference (VI),
bootstrapped neural networks (NN), Deep Ensembles (DE), and Monte Carlo (MC)
dropout. We apply these different UQ for DL methods to a hyperspectral image
target detection problem and show the inconsistency of the different methods'
results and the necessity of a UQ quality metric. To reconcile these
differences and choose a UQ method that appropriately quantifies the
uncertainty, we create a simulated data set with fully parameterized
probability distribution for a two-class classification problem. The gold
standard MCMC performs the best overall, and the bootstrapped NN is a close
second, requiring the same computational expense as DE. Through this
comparison, we demonstrate that, for a given data set, different models can
produce uncertainty estimates of markedly different quality. This in turn
points to a great need for principled assessment methods of UQ quality in DL
applications.","['Daniel Ries', 'Joshua Michalenko', 'Tyler Ganter', 'Rashad Imad-Fayez Baiyasi', 'Jason Adams']","['cs.LG', 'stat.ML']",2023-08-11 01:55:14+00:00
http://arxiv.org/abs/2308.05883v1,Empirical Bayes Estimation with Side Information: A Nonparametric Integrative Tweedie Approach,"We investigate the problem of compound estimation of normal means while
accounting for the presence of side information. Leveraging the empirical Bayes
framework, we develop a nonparametric integrative Tweedie (NIT) approach that
incorporates structural knowledge encoded in multivariate auxiliary data to
enhance the precision of compound estimation. Our approach employs convex
optimization tools to estimate the gradient of the log-density directly,
enabling the incorporation of structural constraints. We conduct theoretical
analyses of the asymptotic risk of NIT and establish the rate at which NIT
converges to the oracle estimator. As the dimension of the auxiliary data
increases, we accurately quantify the improvements in estimation risk and the
associated deterioration in convergence rate. The numerical performance of NIT
is illustrated through the analysis of both simulated and real data,
demonstrating its superiority over existing methods.","['Jiajun Luo', 'Trambak Banerjee', 'Gourab Mukherjee', 'Wenguang Sun']","['stat.ME', 'stat.ML']",2023-08-11 00:24:45+00:00
http://arxiv.org/abs/2308.05665v1,Exploring Deep Learning Approaches to Predict Person and Vehicle Trips: An Analysis of NHTS Data,"Modern transportation planning relies heavily on accurate predictions of
person and vehicle trips. However, traditional planning models often fail to
account for the intricacies and dynamics of travel behavior, leading to
less-than-optimal accuracy in these predictions. This study explores the
potential of deep learning techniques to transform the way we approach trip
predictions, and ultimately, transportation planning. Utilizing a comprehensive
dataset from the National Household Travel Survey (NHTS), we developed and
trained a deep learning model for predicting person and vehicle trips. The
proposed model leverages the vast amount of information in the NHTS data,
capturing complex, non-linear relationships that were previously overlooked by
traditional models. As a result, our deep learning model achieved an impressive
accuracy of 98% for person trip prediction and 96% for vehicle trip estimation.
This represents a significant improvement over the performances of traditional
transportation planning models, thereby demonstrating the power of deep
learning in this domain. The implications of this study extend beyond just more
accurate predictions. By enhancing the accuracy and reliability of trip
prediction models, planners can formulate more effective, data-driven
transportation policies, infrastructure, and services. As such, our research
underscores the need for the transportation planning field to embrace advanced
techniques like deep learning. The detailed methodology, along with a thorough
discussion of the results and their implications, are presented in the
subsequent sections of this paper.","['Kojo Adu-Gyamfi', 'Sharma Anuj']","['cs.AI', 'stat.ML']",2023-08-10 16:06:10+00:00
http://arxiv.org/abs/2308.05658v1,Automatic Extraction of Relevant Road Infrastructure using Connected vehicle data and Deep Learning Model,"In today's rapidly evolving urban landscapes, efficient and accurate mapping
of road infrastructure is critical for optimizing transportation systems,
enhancing road safety, and improving the overall mobility experience for
drivers and commuters. Yet, a formidable bottleneck obstructs progress - the
laborious and time-intensive manual identification of intersections. Simply
considering the shear number of intersections that need to be identified, and
the labor hours required per intersection, the need for an automated solution
becomes undeniable. To address this challenge, we propose a novel approach that
leverages connected vehicle data and cutting-edge deep learning techniques. By
employing geohashing to segment vehicle trajectories and then generating image
representations of road segments, we utilize the YOLOv5 (You Only Look Once
version 5) algorithm for accurate classification of both straight road segments
and intersections. Experimental results demonstrate an impressive overall
classification accuracy of 95%, with straight roads achieving a remarkable 97%
F1 score and intersections reaching a 90% F1 score. This approach not only
saves time and resources but also enables more frequent updates and a
comprehensive understanding of the road network. Our research showcases the
potential impact on traffic management, urban planning, and autonomous vehicle
navigation systems. The fusion of connected vehicle data and deep learning
models holds promise for a transformative shift in road infrastructure mapping,
propelling us towards a smarter, safer, and more connected transportation
ecosystem.","['Adu-Gyamfi Kojo', 'Kandiboina Raghupathi', 'Ravichandra-Mouli Varsha', 'Knickerbocker Skylar', 'Hans Zachary N', 'Hawkins', 'Neal R', 'Sharma Anuj']","['cs.AI', 'stat.ML']",2023-08-10 15:57:47+00:00
http://arxiv.org/abs/2308.05621v1,Normalized Gradients for All,"In this short note, I show how to adapt to H\""{o}lder smoothness using
normalized gradients in a black-box way. Moreover, the bound will depend on a
novel notion of local H\""{o}lder smoothness. The main idea directly comes from
Levy [2017].",['Francesco Orabona'],"['cs.LG', 'math.OC', 'stat.ML']",2023-08-10 15:10:08+00:00
http://arxiv.org/abs/2308.05619v1,Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance,"As data shift or new data become available, updating clinical machine
learning models may be necessary to maintain or improve performance over time.
However, updating a model can introduce compatibility issues when the behavior
of the updated model does not align with user expectations, resulting in poor
user-model team performance. Existing compatibility measures depend on model
decision thresholds, limiting their applicability in settings where models are
used to generate rankings based on estimated risk. To address this limitation,
we propose a novel rank-based compatibility measure, $C^R$, and a new loss
function that aims to optimize discriminative performance while encouraging
good compatibility. Applied to a case study in mortality risk stratification
leveraging data from MIMIC, our approach yields more compatible models while
maintaining discriminative performance compared to existing model selection
techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval:
$0.005$, $0.035$). This work provides new tools to analyze and update risk
stratification models used in clinical care.","['Erkin Ötleş', 'Brian T. Denton', 'Jenna Wiens']","['stat.ML', 'cs.AI', 'cs.LG']",2023-08-10 15:08:13+00:00
http://arxiv.org/abs/2308.05583v1,Generative Diffusion Models for Radio Wireless Channel Modelling and Sampling,"Channel modelling is essential to designing modern wireless communication
systems. The increasing complexity of channel modelling and the cost of
collecting high-quality wireless channel data have become major challenges. In
this paper, we propose a diffusion model based channel sampling approach for
rapidly synthesizing channel realizations from limited data. We use a diffusion
model with a U Net based architecture operating in the frequency space domain.
To evaluate how well the proposed model reproduces the true distribution of
channels in the training dataset, two evaluation metrics are used: $i)$ the
approximate $2$-Wasserstein distance between real and generated distributions
of the normalized power spectrum in the antenna and frequency domains and $ii)$
precision and recall metric for distributions. We show that, compared to
existing GAN based approaches which suffer from mode collapse and unstable
training, our diffusion based approach trains stably and generates diverse and
high-fidelity samples from the true channel distribution. We also show that we
can pretrain the model on a simulated urban macro-cellular channel dataset and
fine-tune it on a smaller, out-of-distribution urban micro-cellular dataset,
therefore showing that it is feasible to model real world channels using
limited data with this approach.","['Ushnish Sengupta', 'Chinkuo Jao', 'Alberto Bernacchia', 'Sattar Vakili', 'Da-shan Shiu']","['cs.AI', 'cs.CE', 'cs.NI', 'stat.ML']",2023-08-10 13:49:26+00:00
http://arxiv.org/abs/2308.05422v1,TSLiNGAM: DirectLiNGAM under heavy tails,"One of the established approaches to causal discovery consists of combining
directed acyclic graphs (DAGs) with structural causal models (SCMs) to describe
the functional dependencies of effects on their causes. Possible
identifiability of SCMs given data depends on assumptions made on the noise
variables and the functional classes in the SCM. For instance, in the LiNGAM
model, the functional class is restricted to linear functions and the
disturbances have to be non-Gaussian.
  In this work, we propose TSLiNGAM, a new method for identifying the DAG of a
causal model based on observational data. TSLiNGAM builds on DirectLiNGAM, a
popular algorithm which uses simple OLS regression for identifying causal
directions between variables. TSLiNGAM leverages the non-Gaussianity assumption
of the error terms in the LiNGAM model to obtain more efficient and robust
estimation of the causal structure. TSLiNGAM is justified theoretically and is
studied empirically in an extensive simulation study. It performs significantly
better on heavy-tailed and skewed data and demonstrates a high small-sample
efficiency. In addition, TSLiNGAM also shows better robustness properties as it
is more resilient to contamination.","['Sarah Leyder', 'Jakob Raymaekers', 'Tim Verdonck']","['stat.ME', 'stat.ML']",2023-08-10 08:34:46+00:00
http://arxiv.org/abs/2308.05414v1,Unifying Distributionally Robust Optimization via Optimal Transport Theory,"In the past few years, there has been considerable interest in two prominent
approaches for Distributionally Robust Optimization (DRO): Divergence-based and
Wasserstein-based methods. The divergence approach models misspecification in
terms of likelihood ratios, while the latter models it through a measure of
distance or cost in actual outcomes. Building upon these advances, this paper
introduces a novel approach that unifies these methods into a single framework
based on optimal transport (OT) with conditional moment constraints. Our
proposed approach, for example, makes it possible for optimal adversarial
distributions to simultaneously perturb likelihood and outcomes, while
producing an optimal (in an optimal transport sense) coupling between the
baseline model and the adversarial model.Additionally, the paper investigates
several duality results and presents tractable reformulations that enhance the
practical applicability of this unified framework.","['Jose Blanchet', 'Daniel Kuhn', 'Jiajin Li', 'Bahar Taskesen']","['math.OC', 'stat.ML']",2023-08-10 08:17:55+00:00
http://arxiv.org/abs/2308.05061v4,Fine-Tune Language Models as Multi-Modal Differential Equation Solvers,"In the growing domain of scientific machine learning, in-context operator
learning has shown notable potential in building foundation models, as in this
framework the model is trained to learn operators and solve differential
equations using prompted data, during the inference stage without weight
updates. However, the current model's overdependence on function data overlooks
the invaluable human insight into the operator. To address this, we present a
transformation of in-context operator learning into a multi-modal paradigm. In
particular, we take inspiration from the recent success of large language
models, and propose using ""captions"" to integrate human knowledge about the
operator, expressed through natural language descriptions and equations. Also,
we introduce a novel approach to train a language-model-like architecture, or
directly fine-tune existing language models, for in-context operator learning.
We beat the baseline on single-modal learning tasks, and also demonstrated the
effectiveness of multi-modal learning in enhancing performance and reducing
function data requirements. The proposed method not only significantly enhanced
the development of the in-context operator learning paradigm, but also created
a new path for the application of language models.","['Liu Yang', 'Siting Liu', 'Stanley J. Osher']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2023-08-09 16:44:25+00:00
http://arxiv.org/abs/2308.04870v1,Decorrelating neurons using persistence,"We propose a novel way to improve the generalisation capacity of deep
learning models by reducing high correlations between neurons. For this, we
present two regularisation terms computed from the weights of a minimum
spanning tree of the clique whose vertices are the neurons of a given network
(or a sample of those), where weights on edges are correlation dissimilarities.
We provide an extensive set of experiments to validate the effectiveness of our
terms, showing that they outperform popular ones. Also, we demonstrate that
naive minimisation of all correlations between neurons obtains lower accuracies
than our regularisation terms, suggesting that redundancies play a significant
role in artificial neural networks, as evidenced by some studies in
neuroscience for real networks. We include a proof of differentiability of our
regularisers, thus developing the first effective topological persistence-based
regularisation terms that consider the whole set of neurons and that can be
applied to a feedforward architecture in any deep learning task such as
classification, data generation, or regression.","['Rubén Ballester', 'Carles Casacuberta', 'Sergio Escalera']","['cs.LG', 'math.AT', 'stat.ML', '55N31, 68T07', 'I.2.6']",2023-08-09 11:09:14+00:00
http://arxiv.org/abs/2308.04620v3,Multiclass Online Learnability under Bandit Feedback,"We study online multiclass classification under bandit feedback. We extend
the results of Daniely and Helbertal [2013] by showing that the finiteness of
the Bandit Littlestone dimension is necessary and sufficient for bandit online
learnability even when the label space is unbounded. Moreover, we show that,
unlike the full-information setting, sequential uniform convergence is
necessary but not sufficient for bandit online learnability. Our result
complements the recent work by Hanneke, Moran, Raman, Subedi, and Tewari [2023]
who show that the Littlestone dimension characterizes online multiclass
learnability in the full-information setting even when the label space is
unbounded.","['Ananth Raman', 'Vinod Raman', 'Unique Subedi', 'Idan Mehalel', 'Ambuj Tewari']","['cs.LG', 'stat.ML']",2023-08-08 22:54:47+00:00
http://arxiv.org/abs/2308.04585v3,Kernel Single Proxy Control for Deterministic Confounding,"We consider the problem of causal effect estimation with an unobserved
confounder, where we observe a proxy variable that is associated with the
confounder. Although Proxy causal learning (PCL) uses two proxy variables to
recover the true causal effect, we show that a single proxy variable is
sufficient for causal estimation if the outcome is generated deterministically,
generalizing Control Outcome Calibration Approach (COCA). We propose two
kernel-based methods for this setting: the first based on the two-stage
regression approach, and the second based on a maximum moment restriction
approach. We prove that both approaches can consistently estimate the causal
effect, and we empirically demonstrate that we can successfully recover the
causal effect on challenging synthetic benchmarks.","['Liyuan Xu', 'Arthur Gretton']","['stat.ML', 'cs.LG']",2023-08-08 21:11:06+00:00
http://arxiv.org/abs/2308.04561v1,Spectral Regularized Kernel Goodness-of-Fit Tests,"Maximum mean discrepancy (MMD) has enjoyed a lot of success in many machine
learning and statistical applications, including non-parametric hypothesis
testing, because of its ability to handle non-Euclidean data. Recently, it has
been demonstrated in Balasubramanian et al.(2021) that the goodness-of-fit test
based on MMD is not minimax optimal while a Tikhonov regularized version of it
is, for an appropriate choice of the regularization parameter. However, the
results in Balasubramanian et al. (2021) are obtained under the restrictive
assumptions of the mean element being zero, and the uniform boundedness
condition on the eigenfunctions of the integral operator. Moreover, the test
proposed in Balasubramanian et al. (2021) is not practical as it is not
computable for many kernels. In this paper, we address these shortcomings and
extend the results to general spectral regularizers that include Tikhonov
regularization.","['Omar Hagrass', 'Bharath K. Sriperumbudur', 'Bing Li']","['math.ST', 'stat.ML', 'stat.TH', '62G10 (Primary), 65J20, 65J22, 46E22, 47A52 (Secondary)']",2023-08-08 20:18:45+00:00
http://arxiv.org/abs/2308.04428v4,Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data,"A powerful concept behind much of the recent progress in machine learning is
the extraction of common features across data from heterogeneous sources or
tasks. Intuitively, using all of one's data to learn a common representation
function benefits both computational effort and statistical generalization by
leaving a smaller number of parameters to fine-tune on a given task. Toward
theoretically grounding these merits, we propose a general setting of
recovering linear operators $M$ from noisy vector measurements $y = Mx + w$,
where the covariates $x$ may be both non-i.i.d. and non-isotropic. We
demonstrate that existing isotropy-agnostic representation learning approaches
incur biases on the representation update, which causes the scaling of the
noise terms to lose favorable dependence on the number of source tasks. This in
turn can cause the sample complexity of representation learning to be
bottlenecked by the single-task data size. We introduce an adaptation,
$\texttt{De-bias & Feature-Whiten}$ ($\texttt{DFW}$), of the popular
alternating minimization-descent scheme proposed independently in Collins et
al., (2021) and Nayer and Vaswani (2022), and establish linear convergence to
the optimal representation with noise level scaling down with the
$\textit{total}$ source data size. This leads to generalization bounds on the
same order as an oracle empirical risk minimizer. We verify the vital
importance of $\texttt{DFW}$ on various numerical simulations. In particular,
we show that vanilla alternating-minimization descent fails catastrophically
even for iid, but mildly non-isotropic data. Our analysis unifies and
generalizes prior work, and provides a flexible framework for a wider range of
applications, such as in controls and dynamical systems.","['Thomas T. C. K. Zhang', 'Leonardo F. Toso', 'James Anderson', 'Nikolai Matni']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY']",2023-08-08 17:56:20+00:00
http://arxiv.org/abs/2308.04365v6,SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling,"Causal inference is a crucial goal of science, enabling researchers to arrive
at meaningful conclusions regarding the predictions of hypothetical
interventions using observational data. Path models, Structural Equation Models
(SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to
unambiguously specify assumptions regarding the causal structure underlying a
phenomenon. Unlike DAGs, which make very few assumptions about the functional
and parametric form, SEM assumes linearity. This can result in functional
misspecification which prevents researchers from undertaking reliable effect
size estimation. In contrast, we propose Super Learner Equation Modeling, a
path modeling technique integrating machine learning Super Learner ensembles.
We empirically demonstrate its ability to provide consistent and unbiased
estimates of causal effects, its competitive performance for linear models when
compared with SEM, and highlight its superiority over SEM when dealing with
non-linear relationships. We provide open-source code, and a tutorial notebook
with example usage, accentuating the easy-to-use nature of the method.",['Matthew J. Vowels'],"['stat.ML', 'cs.LG', 'stat.AP']",2023-08-08 16:04:42+00:00
http://arxiv.org/abs/2308.04314v1,Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs,"Recently, there has been extensive study of cooperative multi-agent
multi-armed bandits where a set of distributed agents cooperatively play the
same multi-armed bandit game. The goal is to develop bandit algorithms with the
optimal group and individual regrets and low communication between agents. The
prior work tackled this problem using two paradigms: leader-follower and fully
distributed algorithms. Prior algorithms in both paradigms achieve the optimal
group regret. The leader-follower algorithms achieve constant communication
costs but fail to achieve optimal individual regrets. The state-of-the-art
fully distributed algorithms achieve optimal individual regrets but fail to
achieve constant communication costs. This paper presents a simple yet
effective communication policy and integrates it into a learning algorithm for
cooperative bandits. Our algorithm achieves the best of both paradigms: optimal
individual regret and constant communication costs.","['Lin Yang', 'Xuchuang Wang', 'Mohammad Hajiesmaili', 'Lijun Zhang', 'John C. S. Lui', 'Don Towsley']","['cs.LG', 'cs.AI', 'stat.ML']",2023-08-08 15:02:50+00:00
http://arxiv.org/abs/2308.04212v1,Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study,"Health outcomes, such as body mass index and cholesterol levels, are known to
be dependent on age and exhibit varying effects with their associated risk
factors. In this paper, we propose a novel framework for dynamic modeling of
the associations between health outcomes and risk factors using
varying-coefficients (VC) regional quantile regression via K-nearest neighbors
(KNN) fused Lasso, which captures the time-varying effects of age. The proposed
method has strong theoretical properties, including a tight estimation error
bound and the ability to detect exact clustered patterns under certain
regularity conditions. To efficiently solve the resulting optimization problem,
we develop an alternating direction method of multipliers (ADMM) algorithm. Our
empirical results demonstrate the efficacy of the proposed method in capturing
the complex age-dependent associations between health outcomes and their risk
factors.","['Seyoung Park', 'Eun Ryung Lee', 'Hyokyoung G. Hong']","['stat.ML', 'cs.LG', 'stat.ME']",2023-08-08 12:22:09+00:00
http://arxiv.org/abs/2308.04106v1,Parallel Learning by Multitasking Neural Networks,"A modern challenge of Artificial Intelligence is learning multiple patterns
at once (i.e.parallel learning). While this can not be accomplished by standard
Hebbian associative neural networks, in this paper we show how the Multitasking
Hebbian Network (a variation on theme of the Hopfield model working on sparse
data-sets) is naturally able to perform this complex task. We focus on systems
processing in parallel a finite (up to logarithmic growth in the size of the
network) amount of patterns, mirroring the low-storage level of standard
associative neural networks at work with pattern recognition. For mild dilution
in the patterns, the network handles them hierarchically, distributing the
amplitudes of their signals as power-laws w.r.t. their information content
(hierarchical regime), while, for strong dilution, all the signals pertaining
to all the patterns are raised with the same strength (parallel regime).
Further, confined to the low-storage setting (i.e., far from the spin glass
limit), the presence of a teacher neither alters the multitasking performances
nor changes the thresholds for learning: the latter are the same whatever the
training protocol is supervised or unsupervised. Results obtained through
statistical mechanics, signal-to-noise technique and Monte Carlo simulations
are overall in perfect agreement and carry interesting insights on multiple
learning at once: for instance, whenever the cost-function of the model is
minimized in parallel on several patterns (in its description via Statistical
Mechanics), the same happens to the standard sum-squared error Loss function
(typically used in Machine Learning).","['Elena Agliari', 'Andrea Alessandrelli', 'Adriano Barra', 'Federico Ricci-Tersenghi']","['cond-mat.dis-nn', 'physics.bio-ph', 'stat.ML']",2023-08-08 07:43:31+00:00
http://arxiv.org/abs/2308.04060v1,Toward Improving Predictive Risk Modelling for New Zealand's Child Welfare System Using Clustering Methods,"The combination of clinical judgement and predictive risk models crucially
assist social workers to segregate children at risk of maltreatment and decide
when authorities should intervene. Predictive risk modelling to address this
matter has been initiated by several governmental welfare authorities worldwide
involving administrative data and machine learning algorithms. While previous
studies have investigated risk factors relating to child maltreatment, several
gaps remain as to understanding how such risk factors interact and whether
predictive risk models perform differently for children with different
features. By integrating Principal Component Analysis and K-Means clustering,
this paper presents initial findings of our work on the identification of such
features as well as their potential effect on current risk modelling
frameworks. This approach allows examining existent, unidentified yet, clusters
of New Zealand (NZ) children reported with care and protection concerns, as
well as to analyse their inner structure, and evaluate the performance of
prediction models trained cluster wise. We aim to discover the extent of
clustering degree required as an early step in the development of predictive
risk models for child maltreatment and so enhance the accuracy of such models
intended for use by child protection authorities. The results from testing
LASSO logistic regression models trained on identified clusters revealed no
significant difference in their performance. The models, however, performed
slightly better for two clusters including younger children. our results
suggest that separate models might need to be developed for children of certain
age to gain additional control over the error rates and to improve model
accuracy. While results are promising, more evidence is needed to draw
definitive conclusions, and further investigation is necessary.","['Sahar Barmomanesh', 'Victor Miranda-Soberanis']","['stat.ML', 'cs.LG']",2023-08-08 05:46:03+00:00
http://arxiv.org/abs/2308.04051v2,Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization,"Our work presents a novel approach to shape optimization, with the twofold
objective to improve the efficiency of global optimization algorithms while
promoting the generation of high-quality designs during the optimization
process free of geometrical anomalies. This is accomplished by reducing the
number of the original design variables defining a new reduced subspace where
the geometrical variance is maximized and modeling the underlying generative
process of the data via probabilistic linear latent variable models such as
factor analysis and probabilistic principal component analysis. We show that
the data follows approximately a Gaussian distribution when the shape
modification method is linear and the design variables are sampled uniformly at
random, due to the direct application of the central limit theorem. The degree
of anomalousness is measured in terms of Mahalanobis distance, and the paper
demonstrates that abnormal designs tend to exhibit a high value of this metric.
This enables the definition of a new optimization model where anomalous
geometries are penalized and consequently avoided during the optimization loop.
The procedure is demonstrated for hull shape optimization of the DTMB 5415
model, extensively used as an international benchmark for shape optimization
problems. The global optimization routine is carried out using Bayesian
optimization and the DIRECT algorithm. From the numerical results, the new
framework improves the convergence of global optimization algorithms, while
only designs with high-quality geometrical features are generated through the
optimization routine thereby avoiding the wastage of precious computationally
expensive simulations.","[""Danny D'Agostino""]","['stat.ML', 'cs.LG', 'math.OC', 'physics.data-an', 'physics.flu-dyn']",2023-08-08 04:57:58+00:00
http://arxiv.org/abs/2308.03970v3,Dependent Cluster Mapping (DCMAP): Optimal clustering of directed acyclic graphs for statistical inference,"A Directed Acyclic Graph (DAG) can be partitioned or mapped into clusters to
support and make inference more computationally efficient in Bayesian Network
(BN), Markov process and other models. However, optimal partitioning with an
arbitrary cost function is challenging, especially in statistical inference as
the local cluster cost is dependent on both nodes within a cluster, and the
mapping of clusters connected via parent and/or child nodes, which we call
dependent clusters. We propose a novel algorithm called DCMAP for optimal
cluster mapping with dependent clusters. Given an arbitrarily defined, positive
cost function based on the DAG, we show that DCMAP converges to find all
optimal clusters, and returns near-optimal solutions along the way.
Empirically, we find that the algorithm is time-efficient for a Dynamic BN
(DBN) model of a seagrass complex system using a computation cost function. For
a 25 and 50-node DBN, the search space size was $9.91\times 10^9$ and
$1.51\times10^{21}$ possible cluster mappings, and the first optimal solution
was found at iteration 934 $(\text{95\% CI } 926,971)$, and 2256 $(2150,2271)$
with a cost that was 4\% and 0.2\% of the naive heuristic cost, respectively.","['Paul Pao-Yen Wu', 'Fabrizio Ruggeri', 'Kerrie Mengersen']","['cs.DS', 'stat.ML']",2023-08-08 01:01:37+00:00
http://arxiv.org/abs/2308.03882v2,Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations,"Offline reinforcement learning (RL) methods strike a balance between
exploration and exploitation by conservative value estimation -- penalizing
values of unseen states and actions. Model-free methods penalize values at all
unseen actions, while model-based methods are able to further exploit unseen
states via model rollouts. However, such methods are handicapped in their
ability to find unseen states far away from the available offline data due to
two factors -- (a) very short rollout horizons in models due to cascading model
errors, and (b) model rollouts originating solely from states observed in
offline data. We relax the second assumption and present a novel unseen state
augmentation strategy to allow exploitation of unseen states where the learned
model and value estimates generalize. Our strategy finds unseen states by
value-informed perturbations of seen states followed by filtering out states
with epistemic uncertainty estimates too high (high error) or too low (too
similar to seen data). We observe improved performance in several offline RL
tasks and find that our augmentation strategy consistently leads to overall
lower average dataset Q-value estimates i.e. more conservative Q-value
estimates than a baseline.","['Nirbhay Modhe', 'Qiaozi Gao', 'Ashwin Kalyan', 'Dhruv Batra', 'Govind Thattai', 'Gaurav Sukhatme']","['cs.LG', 'cs.AI', 'stat.ML']",2023-08-07 19:24:47+00:00
http://arxiv.org/abs/2308.03730v1,SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator,"An explanation method called SurvBeX is proposed to interpret predictions of
the machine learning survival black-box models. The main idea behind the method
is to use the modified Beran estimator as the surrogate explanation model.
Coefficients, incorporated into Beran estimator, can be regarded as values of
the feature impacts on the black-box model prediction. Following the well-known
LIME method, many points are generated in a local area around an example of
interest. For every generated example, the survival function of the black-box
model is computed, and the survival function of the surrogate model (the Beran
estimator) is constructed as a function of the explanation coefficients. In
order to find the explanation coefficients, it is proposed to minimize the mean
distance between the survival functions of the black-box model and the Beran
estimator produced by the generated examples. Many numerical experiments with
synthetic and real survival data demonstrate the SurvBeX efficiency and compare
the method with the well-known method SurvLIME. The method is also compared
with the method SurvSHAP. The code implementing SurvBeX is available at:
https://github.com/DanilaEremenko/SurvBeX","['Lev V. Utkin', 'Danila Y. Eremenko', 'Andrei V. Konstantinov']","['cs.LG', 'cs.AI', 'stat.ML']",2023-08-07 17:18:37+00:00
http://arxiv.org/abs/2308.03686v3,Nearly $d$-Linear Convergence Bounds for Diffusion Models via Stochastic Localization,"Denoising diffusions are a powerful method to generate approximate samples
from high-dimensional data distributions. Recent results provide polynomial
bounds on their convergence rate, assuming $L^2$-accurate scores. Until now,
the tightest bounds were either superlinear in the data dimension or required
strong smoothness assumptions. We provide the first convergence bounds which
are linear in the data dimension (up to logarithmic factors) assuming only
finite second moments of the data distribution. We show that diffusion models
require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to
approximate an arbitrary distribution on $\mathbb{R}^d$ corrupted with Gaussian
noise of variance $\delta$ to within $\varepsilon^2$ in KL divergence. Our
proof extends the Girsanov-based methods of previous works. We introduce a
refined treatment of the error from discretizing the reverse SDE inspired by
stochastic localization.","['Joe Benton', 'Valentin De Bortoli', 'Arnaud Doucet', 'George Deligiannidis']","['stat.ML', 'cs.LG']",2023-08-07 16:01:14+00:00
http://arxiv.org/abs/2308.03669v4,Diffusion Model in Causal Inference with Unmeasured Confounders,"We study how to extend the use of the diffusion model to answer the causal
question from the observational data under the existence of unmeasured
confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to
capture the causal intervention, a Diffusion-based Causal Model (DCM) was
proposed incorporating the diffusion model to answer the causal questions more
accurately, assuming that all of the confounders are observed. However,
unmeasured confounders in practice exist, which hinders DCM from being
applicable. To alleviate this limitation of DCM, we propose an extended model
called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the
Backdoor criterion to find the variables in DAG to be included in the decoding
process of the diffusion model so that we can extend DCM to the case with
unmeasured confounders. Synthetic data experiment demonstrates that our
proposed model captures the counterfactual distribution more precisely than DCM
under the unmeasured confounders.",['Tatsuhiro Shimizu'],"['cs.LG', 'cs.AI', 'stat.ML']",2023-08-07 15:40:34+00:00
http://arxiv.org/abs/2308.03666v4,"Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness","As researchers strive to narrow the gap between machine intelligence and
human through the development of artificial intelligence technologies, it is
imperative that we recognize the critical importance of trustworthiness in
open-world, which has become ubiquitous in all aspects of daily life for
everyone. However, several challenges may create a crisis of trust in current
artificial intelligence systems that need to be bridged: 1) Insufficient
explanation of predictive results; 2) Inadequate generalization for learning
models; 3) Poor adaptability to uncertain environments. Consequently, we
explore a neural program to bridge trustworthiness and open-world learning,
extending from single-modal to multi-modal scenarios for readers. 1) To enhance
design-level interpretability, we first customize trustworthy networks with
specific physical meanings; 2) We then design environmental well-being
task-interfaces via flexible learning regularizers for improving the
generalization of trustworthy learning; 3) We propose to increase the
robustness of trustworthy learning by integrating open-world recognition losses
with agent mechanisms. Eventually, we enhance various trustworthy properties
through the establishment of design-level explainability, environmental
well-being task-interfaces and open-world recognition programs. These designed
open-world protocols are applicable across a wide range of surroundings, under
open-world multimedia recognition scenarios with significant performance
improvements observed.","['Shide Du', 'Zihan Fang', 'Shiyang Lan', 'Yanchao Tan', 'Manuel Günther', 'Shiping Wang', 'Wenzhong Guo']","['stat.ML', 'cs.LG']",2023-08-07 15:35:32+00:00
http://arxiv.org/abs/2308.03574v2,Generalized Early Stopping in Evolutionary Direct Policy Search,"Lengthy evaluation times are common in many optimization problems such as
direct policy search tasks, especially when they involve conducting evaluations
in the physical world, e.g. in robotics applications. Often when evaluating
solution over a fixed time period it becomes clear that the objective value
will not increase with additional computation time (for example when a two
wheeled robot continuously spins on the spot). In such cases, it makes sense to
stop the evaluation early to save computation time. However, most approaches to
stop the evaluation are problem specific and need to be specifically designed
for the task at hand. Therefore, we propose an early stopping method for direct
policy search. The proposed method only looks at the objective value at each
time step and requires no problem specific knowledge. We test the introduced
stopping criterion in five direct policy search environments drawn from games,
robotics and classic control domains, and show that it can save up to 75% of
the computation time. We also compare it with problem specific stopping
criteria and show that it performs comparably, while being more generally
applicable.","['Etor Arza', 'Leni K. Le Goff', 'Emma Hart']","['stat.ML', 'cs.LG', 'cs.NE', 'cs.RO']",2023-08-07 13:25:48+00:00
http://arxiv.org/abs/2308.03570v1,Partial identification of kernel based two sample tests with mismeasured data,"Nonparametric two-sample tests such as the Maximum Mean Discrepancy (MMD) are
often used to detect differences between two distributions in machine learning
applications. However, the majority of existing literature assumes that
error-free samples from the two distributions of interest are available.We
relax this assumption and study the estimation of the MMD under
$\epsilon$-contamination, where a possibly non-random $\epsilon$ proportion of
one distribution is erroneously grouped with the other. We show that under
$\epsilon$-contamination, the typical estimate of the MMD is unreliable.
Instead, we study partial identification of the MMD, and characterize sharp
upper and lower bounds that contain the true, unknown MMD. We propose a method
to estimate these bounds, and show that it gives estimates that converge to the
sharpest possible bounds on the MMD as sample size increases, with a
convergence rate that is faster than alternative approaches. Using three
datasets, we empirically validate that our approach is superior to the
alternatives: it gives tight bounds with a low false coverage rate.","['Ron Nafshi', 'Maggie Makar']","['stat.ML', 'cs.LG']",2023-08-07 13:21:58+00:00
http://arxiv.org/abs/2308.03443v3,Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces,"We study Off-Policy Evaluation (OPE) in contextual bandit settings with large
action spaces. The benchmark estimators suffer from severe bias and variance
tradeoffs. Parametric approaches suffer from bias due to difficulty specifying
the correct model, whereas ones with importance weight suffer from variance. To
overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was
proposed to mitigate the estimator's variance via embeddings of an action.
Nevertheless, MIPS is unbiased under the no direct effect, which assumes that
the action embedding completely mediates the effect of an action on a reward.
To overcome the dependency on these unrealistic assumptions, we propose a
Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the
proposed estimator is unbiased under weaker assumptions than MIPS while
reducing the variance against MIPS. The empirical experiment verifies the
supremacy of MDR against existing estimators with large action spaces.","['Tatsuhiro Shimizu', 'Laura Forastiere']","['stat.ML', 'cs.AI', 'cs.IR', 'cs.LG']",2023-08-07 10:00:07+00:00
http://arxiv.org/abs/2308.03369v1,Variable importance for causal forests: breaking down the heterogeneity of treatment effects,"Causal random forests provide efficient estimates of heterogeneous treatment
effects. However, forest algorithms are also well-known for their black-box
nature, and therefore, do not characterize how input variables are involved in
treatment effect heterogeneity, which is a strong practical limitation. In this
article, we develop a new importance variable algorithm for causal forests, to
quantify the impact of each input on the heterogeneity of treatment effects.
The proposed approach is inspired from the drop and relearn principle, widely
used for regression problems. Importantly, we show how to handle the forest
retrain without a confounding variable. If the confounder is not involved in
the treatment effect heterogeneity, the local centering step enforces
consistency of the importance measure. Otherwise, when a confounder also
impacts heterogeneity, we introduce a corrective term in the retrained causal
forest to recover consistency. Additionally, experiments on simulated,
semi-synthetic, and real data show the good performance of our importance
measure, which outperforms competitors on several test cases. Experiments also
show that our approach can be efficiently extended to groups of variables,
providing key insights in practice.","['Clément Bénard', 'Julie Josse']",['stat.ML'],2023-08-07 07:43:42+00:00
http://arxiv.org/abs/2308.03296v1,Studying Large Language Model Generalization with Influence Functions,"When trying to gain better visibility into a machine learning model in order
to understand and mitigate the associated risks, a potentially valuable source
of evidence is: which training examples most contribute to a given behavior?
Influence functions aim to answer a counterfactual: how would the model's
parameters (and hence its outputs) change if a given sequence were added to the
training set? While influence functions have produced insights for small
models, they are difficult to scale to large language models (LLMs) due to the
difficulty of computing an inverse-Hessian-vector product (IHVP). We use the
Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)
approximation to scale influence functions up to LLMs with up to 52 billion
parameters. In our experiments, EK-FAC achieves similar accuracy to traditional
influence function estimators despite the IHVP computation being orders of
magnitude faster. We investigate two algorithmic techniques to reduce the cost
of computing gradients of candidate training sequences: TF-IDF filtering and
query batching. We use influence functions to investigate the generalization
patterns of LLMs, including the sparsity of the influence patterns, increasing
abstraction with scale, math and programming abilities, cross-lingual
generalization, and role-playing behavior. Despite many apparently
sophisticated forms of generalization, we identify a surprising limitation:
influences decay to near-zero when the order of key phrases is flipped.
Overall, influence functions give us a powerful new tool for studying the
generalization properties of LLMs.","['Roger Grosse', 'Juhan Bae', 'Cem Anil', 'Nelson Elhage', 'Alex Tamkin', 'Amirhossein Tajdini', 'Benoit Steiner', 'Dustin Li', 'Esin Durmus', 'Ethan Perez', 'Evan Hubinger', 'Kamilė Lukošiūtė', 'Karina Nguyen', 'Nicholas Joseph', 'Sam McCandlish', 'Jared Kaplan', 'Samuel R. Bowman']","['cs.LG', 'cs.CL', 'stat.ML']",2023-08-07 04:47:42+00:00
http://arxiv.org/abs/2308.03215v1,"The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning","In this work, we investigate the dynamics of stochastic gradient descent
(SGD) when training a single-neuron autoencoder with linear or ReLU activation
on orthogonal data. We show that for this non-convex problem, randomly
initialized SGD with a constant step size successfully finds a global minimum
for any batch size choice. However, the particular global minimum found depends
upon the batch size. In the full-batch setting, we show that the solution is
dense (i.e., not sparse) and is highly aligned with its initialized direction,
showing that relatively little feature learning occurs. On the other hand, for
any batch size strictly smaller than the number of samples, SGD finds a global
minimum which is sparse and nearly orthogonal to its initialization, showing
that the randomness of stochastic gradients induces a qualitatively different
type of ""feature selection"" in this setting. Moreover, if we measure the
sharpness of the minimum by the trace of the Hessian, the minima found with
full batch gradient descent are flatter than those found with strictly smaller
batch sizes, in contrast to previous works which suggest that large batches
lead to sharper minima. To prove convergence of SGD with a constant step size,
we introduce a powerful tool from the theory of non-homogeneous random walks
which may be of independent interest.","['Nikhil Ghosh', 'Spencer Frei', 'Wooseok Ha', 'Bin Yu']","['stat.ML', 'cs.LG']",2023-08-06 21:54:07+00:00
http://arxiv.org/abs/2308.04457v1,A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems,"Machine learning has emerged as a powerful tool in various fields, including
computer vision, natural language processing, and speech recognition. It can
unravel hidden patterns within large data sets and reveal unparalleled
insights, revolutionizing many industries and disciplines. However, machine and
deep learning models lack interpretability and limited domain-specific
knowledge, especially in applications such as physics and engineering.
Alternatively, physics-informed machine learning (PIML) techniques integrate
physics principles into data-driven models. By combining deep learning with
domain knowledge, PIML improves the generalization of the model, abidance by
the governing physical laws, and interpretability. This paper comprehensively
reviews PIML applications related to subsurface energy systems, mainly in the
oil and gas industry. The review highlights the successful utilization of PIML
for tasks such as seismic applications, reservoir simulation, hydrocarbons
production forecasting, and intelligent decision-making in the exploration and
production stages. Additionally, it demonstrates PIML's capabilities to
revolutionize the oil and gas industry and other emerging areas of interest,
such as carbon and hydrogen storage; and geothermal systems by providing more
accurate and reliable predictions for resource management and operational
efficiency.","['Abdeldjalil Latrach', 'Mohamed Lamine Malki', 'Misael Morales', 'Mohamed Mehana', 'Minou Rabiei']","['cs.LG', 'stat.ML']",2023-08-06 18:20:24+00:00
http://arxiv.org/abs/2308.03171v1,Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques,"Anomaly Detection in multivariate time series is a major problem in many
fields. Due to their nature, anomalies sparsely occur in real data, thus making
the task of anomaly detection a challenging problem for classification
algorithms to solve. Methods that are based on Deep Neural Networks such as
LSTM, Autoencoders, Convolutional Autoencoders etc., have shown positive
results in such imbalanced data. However, the major challenge that algorithms
face when applied to multivariate time series is that the anomaly can arise
from a small subset of the feature set. To boost the performance of these base
models, we propose a feature-bagging technique that considers only a subset of
features at a time, and we further apply a transformation that is based on
nested rotation computed from Principal Component Analysis (PCA) to improve the
effectiveness and generalization of the approach. To further enhance the
prediction performance, we propose an ensemble technique that combines multiple
base models toward the final decision. In addition, a semi-supervised approach
using a Logistic Regressor to combine the base models' outputs is proposed. The
proposed methodology is applied to the Skoltech Anomaly Benchmark (SKAB)
dataset, which contains time series data related to the flow of water in a
closed circuit, and the experimental results show that the proposed ensemble
technique outperforms the basic algorithms. More specifically, the performance
improvement in terms of anomaly detection accuracy reaches 2% for the
unsupervised and at least 10% for the semi-supervised models.","['Anastasios Iliopoulos', 'John Violos', 'Christos Diou', 'Iraklis Varlamis']","['cs.LG', 'stat.ML']",2023-08-06 17:51:22+00:00
http://arxiv.org/abs/2308.03142v1,Self-Directed Linear Classification,"In online classification, a learner is presented with a sequence of examples
and aims to predict their labels in an online fashion so as to minimize the
total number of mistakes. In the self-directed variant, the learner knows in
advance the pool of examples and can adaptively choose the order in which
predictions are made. Here we study the power of choosing the prediction order
and establish the first strong separation between worst-order and random-order
learning for the fundamental task of linear classification. Prior to our work,
such a separation was known only for very restricted concept classes, e.g.,
one-dimensional thresholds or axis-aligned rectangles.
  We present two main results. If $X$ is a dataset of $n$ points drawn
uniformly at random from the $d$-dimensional unit sphere, we design an
efficient self-directed learner that makes $O(d \log \log(n))$ mistakes and
classifies the entire dataset. If $X$ is an arbitrary $d$-dimensional dataset
of size $n$, we design an efficient self-directed learner that predicts the
labels of $99\%$ of the points in $X$ with mistake bound independent of $n$. In
contrast, under a worst- or random-ordering, the number of mistakes must be at
least $\Omega(d \log n)$, even when the points are drawn uniformly from the
unit sphere and the learner only needs to predict the labels for $1\%$ of them.","['Ilias Diakonikolas', 'Vasilis Kontonis', 'Christos Tzamos', 'Nikos Zarifis']","['cs.LG', 'cs.DS', 'math.ST', 'stat.ML', 'stat.TH']",2023-08-06 15:38:44+00:00
http://arxiv.org/abs/2308.03128v1,Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis,"This thesis delves into the intricate world of Deep Neural Networks (DNNs),
focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The
LTH posits that within extensive DNNs, smaller, trainable subnetworks termed
""winning tickets"", can achieve performance comparable to the full model. A key
process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates
minimal weights, emulating stepwise learning in DNNs. Once we identify these
winning tickets, we further investigate their ""universality"". In other words,
we check if a winning ticket that works well for one specific problem could
also work well for other, similar problems. We also bridge the divide between
the IMP and the Renormalisation Group (RG) theory in physics, promoting a more
rigorous understanding of IMP.",['Abu-Al Hassan'],"['cs.LG', 'stat.ML']",2023-08-06 14:36:57+00:00
http://arxiv.org/abs/2308.03079v1,Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data,"We present an overview of taxicab correspondence analysis, a robust variant
of correspondence analysis, for visualization of extremely sparse ontingency
tables. In particular we visualize an extremely sparse textual data set of size
590 by 8265 concerning fragments of 8 sacred books recently introduced by Sah
and Fokou\'e (2019) and studied quite in detail by (12 + 1) dimension reduction
methods (t-SNE, UMAP, PHATE,...) by Ma, Sun and Zou (2022).","['V. Choulakian', 'J. Allard']","['stat.ML', 'cs.LG', '62H25, 62H30']",2023-08-06 10:14:19+00:00
http://arxiv.org/abs/2308.02966v1,Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory,"In supervised learning, it is quite frequent to be confronted with real
imbalanced datasets. This situation leads to a learning difficulty for standard
algorithms. Research and solutions in imbalanced learning have mainly focused
on classification tasks. Despite its importance, very few solutions exist for
imbalanced regression. In this paper, we propose a data augmentation procedure,
the GOLIATH algorithm, based on kernel density estimates which can be used in
classification and regression. This general approach encompasses two large
families of synthetic oversampling: those based on perturbations, such as
Gaussian Noise, and those based on interpolations, such as SMOTE. It also
provides an explicit form of these machine learning algorithms and an
expression of their conditional densities, in particular for SMOTE. New
synthetic data generators are deduced. We apply GOLIATH in imbalanced
regression combining such generator procedures with a wild-bootstrap resampling
technique for the target values. We evaluate the performance of the GOLIATH
algorithm in imbalanced regression situations. We empirically evaluate and
compare our approach and demonstrate significant improvement over existing
state-of-the-art techniques.","['Samuel Stocksieker', 'Denys Pommeret', 'Arthur Charpentier']","['stat.ML', 'cs.LG']",2023-08-05 23:08:08+00:00
http://arxiv.org/abs/2308.02922v1,Structured Low-Rank Tensors for Generalized Linear Models,"Recent works have shown that imposing tensor structures on the coefficient
tensor in regression problems can lead to more reliable parameter estimation
and lower sample complexity compared to vector-based methods. This work
investigates a new low-rank tensor model, called Low Separation Rank (LSR), in
Generalized Linear Model (GLM) problems. The LSR model -- which generalizes the
well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of
the Block Tensor Decomposition (BTD) model -- is imposed onto the coefficient
tensor in the GLM model. This work proposes a block coordinate descent
algorithm for parameter estimation in LSR-structured tensor GLMs. Most
importantly, it derives a minimax lower bound on the error threshold on
estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound
is proportional to the intrinsic degrees of freedom in the LSR tensor GLM
problem, suggesting that its sample complexity may be significantly lower than
that of vectorized GLMs. This result can also be specialised to lower bound the
estimation error in CP and Tucker-structured GLMs. The derived bounds are
comparable to tight bounds in the literature for Tucker linear regression, and
the tightness of the minimax lower bound is further assessed numerically.
Finally, numerical experiments on synthetic datasets demonstrate the efficacy
of the proposed LSR tensor model for three regression types (linear, logistic
and Poisson). Experiments on a collection of medical imaging datasets
demonstrate the usefulness of the LSR model over other tensor models (Tucker
and CP) on real, imbalanced data with limited available samples.","['Batoul Taki', 'Anand D. Sarwate', 'Waheed U. Bajwa']","['stat.ML', 'cs.LG', 'eess.SP', 'math.ST', 'stat.TH']",2023-08-05 17:20:41+00:00
http://arxiv.org/abs/2308.02918v3,Spectral Ranking Inferences based on General Multiway Comparisons,"This paper studies the performance of the spectral method in the estimation
and uncertainty quantification of the unobserved preference scores of compared
entities in a general and more realistic setup. Specifically, the comparison
graph consists of hyper-edges of possible heterogeneous sizes, and the number
of comparisons can be as low as one for a given hyper-edge. Such a setting is
pervasive in real applications, circumventing the need to specify the graph
randomness and the restrictive homogeneous sampling assumption imposed in the
commonly used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models.
Furthermore, in scenarios where the BTL or PL models are appropriate, we
unravel the relationship between the spectral estimator and the Maximum
Likelihood Estimator (MLE). We discover that a two-step spectral method, where
we apply the optimal weighting estimated from the equal weighting vanilla
spectral method, can achieve the same asymptotic efficiency as the MLE. Given
the asymptotic distributions of the estimated preference scores, we also
introduce a comprehensive framework to carry out both one-sample and two-sample
ranking inferences, applicable to both fixed and random graph settings. It is
noteworthy that this is the first time effective two-sample rank testing
methods have been proposed. Finally, we substantiate our findings via
comprehensive numerical simulations and subsequently apply our developed
methodologies to perform statistical inferences for statistical journals and
movie rankings.","['Jianqing Fan', 'Zhipeng Lou', 'Weichen Wang', 'Mengxin Yu']","['stat.ME', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2023-08-05 16:31:32+00:00
http://arxiv.org/abs/2308.02894v1,Physics-informed Gaussian process model for Euler-Bernoulli beam elements,"A physics-informed machine learning model, in the form of a multi-output
Gaussian process, is formulated using the Euler-Bernoulli beam equation. Given
appropriate datasets, the model can be used to regress the analytical value of
the structure's bending stiffness, interpolate responses, and make
probabilistic inferences on latent physical quantities. The developed model is
applied on a numerically simulated cantilever beam, where the regressed bending
stiffness is evaluated and the influence measurement noise on the prediction
quality is investigated. Further, the regressed probabilistic stiffness
distribution is used in a structural health monitoring context, where the
Mahalanobis distance is employed to reason about the possible location and
extent of damage in the structural system. To validate the developed framework,
an experiment is conducted and measured heterogeneous datasets are used to
update the assumed analytical structural model.","['Gledson Rodrigo Tondo', 'Sebastian Rau', 'Igor Kavrakov', 'Guido Morgenthal']","['stat.ML', 'cs.LG']",2023-08-05 14:48:37+00:00
http://arxiv.org/abs/2308.02836v1,Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks,"We investigate to what extent it is possible to solve linear inverse problems
with $ReLu$ networks. Due to the scaling invariance arising from the linearity,
an optimal reconstruction function $f$ for such a problem is positive
homogeneous, i.e., satisfies $f(\lambda x) = \lambda f(x)$ for all non-negative
$\lambda$. In a $ReLu$ network, this condition translates to considering
networks without bias terms. We first consider recovery of sparse vectors from
few linear measurements. We prove that $ReLu$- networks with only one hidden
layer cannot even recover $1$-sparse vectors, not even approximately, and
regardless of the width of the network. However, with two hidden layers,
approximate recovery with arbitrary precision and arbitrary sparsity level $s$
is possible in a stable way. We then extend our results to a wider class of
recovery problems including low-rank matrix recovery and phase retrieval.
Furthermore, we also consider the approximation of general positive homogeneous
functions with neural networks. Extending previous work, we establish new
results explaining under which conditions such functions can be approximated
with neural networks. Our results also shed some light on the seeming
contradiction between previous works showing that neural networks for inverse
problems typically have very large Lipschitz constants, but still perform very
well also for adversarial noise. Namely, the error bounds in our expressivity
results include a combination of a small constant term and a term that is
linear in the noise level, indicating that robustness issues may occur only for
very small noise levels.","['Stefan Bamberger', 'Reinhard Heckel', 'Felix Krahmer']","['cs.LG', 'cs.NE', 'stat.ML', '41A30, 68T07']",2023-08-05 10:17:04+00:00
