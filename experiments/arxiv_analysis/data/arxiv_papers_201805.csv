id,title,abstract,authors,categories,date
http://arxiv.org/abs/1806.05768v1,Hardware Trojan Attacks on Neural Networks,"With the rising popularity of machine learning and the ever increasing demand
for computational power, there is a growing need for hardware optimized
implementations of neural networks and other machine learning models. As the
technology evolves, it is also plausible that machine learning or artificial
intelligence will soon become consumer electronic products and military
equipment, in the form of well-trained models. Unfortunately, the modern
fabless business model of manufacturing hardware, while economic, leads to
deficiencies in security through the supply chain. In this paper, we illuminate
these security issues by introducing hardware Trojan attacks on neural
networks, expanding the current taxonomy of neural network security to
incorporate attacks of this nature. To aid in this, we develop a novel
framework for inserting malicious hardware Trojans in the implementation of a
neural network classifier. We evaluate the capabilities of the adversary in
this setting by implementing the attack algorithm on convolutional neural
networks while controlling a variety of parameters available to the adversary.
Our experimental results show that the proposed algorithm could effectively
classify a selected input trigger as a specified class on the MNIST dataset by
injecting hardware Trojans into $0.03\%$, on average, of neurons in the 5th
hidden layer of arbitrary 7-layer convolutional neural networks, while
undetectable under the test data. Finally, we discuss the potential defenses to
protect neural networks against hardware Trojan attacks.","['Joseph Clements', 'Yingjie Lao']","['cs.LG', 'cs.CR', 'stat.ML']",2018-06-14 23:49:55+00:00
http://arxiv.org/abs/1806.05767v2,Motion Planning Networks,"Fast and efficient motion planning algorithms are crucial for many
state-of-the-art robotics applications such as self-driving cars. Existing
motion planning methods become ineffective as their computational complexity
increases exponentially with the dimensionality of the motion planning problem.
To address this issue, we present Motion Planning Networks (MPNet), a neural
network-based novel planning algorithm. The proposed method encodes the given
workspaces directly from a point cloud measurement and generates the end-to-end
collision-free paths for the given start and goal configurations. We evaluate
MPNet on various 2D and 3D environments including the planning of a 7 DOF
Baxter robot manipulator. The results show that MPNet is not only consistently
computationally efficient in all environments but also generalizes to
completely unseen environments. The results also show that the computation time
of MPNet consistently remains less than 1 second in all presented experiments,
which is significantly lower than existing state-of-the-art motion planning
algorithms.","['Ahmed H. Qureshi', 'Anthony Simeonov', 'Mayur J. Bency', 'Michael C. Yip']","['cs.RO', 'cs.AI', 'stat.ML']",2018-06-14 23:48:08+00:00
http://arxiv.org/abs/1806.05759v3,Insights on representational similarity in neural networks with canonical correlation,"Comparing different neural network representations and determining how
representations evolve over time remain challenging open questions in our
understanding of the function of neural networks. Comparing representations in
neural networks is fundamentally difficult as the structure of representations
varies greatly, even across groups of networks trained on identical tasks, and
over the course of training. Here, we develop projection weighted CCA
(Canonical Correlation Analysis) as a tool for understanding neural networks,
building off of SVCCA, a recently proposed method (Raghu et al., 2017). We
first improve the core method, showing how to differentiate between signal and
noise, and then apply this technique to compare across a group of CNNs,
demonstrating that networks which generalize converge to more similar
representations than networks which memorize, that wider networks converge to
more similar solutions than narrow networks, and that trained networks with
identical topology but different learning rates converge to distinct clusters
with diverse representations. We also investigate the representational dynamics
of RNNs, across both training and sequential timesteps, finding that RNNs
converge in a bottom-up pattern over the course of training and that the hidden
state is highly variable over the course of a sequence, even when accounting
for linear transforms. Together, these results provide new insights into the
function of CNNs and RNNs, and demonstrate the utility of using CCA to
understand representations.","['Ari S. Morcos', 'Maithra Raghu', 'Samy Bengio']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.NE']",2018-06-14 22:34:11+00:00
http://arxiv.org/abs/1806.05738v1,Efficient sampling for Gaussian linear regression with arbitrary priors,"This paper develops a slice sampler for Bayesian linear regression models
with arbitrary priors. The new sampler has two advantages over current
approaches. One, it is faster than many custom implementations that rely on
auxiliary latent variables, if the number of regressors is large. Two, it can
be used with any prior with a density function that can be evaluated up to a
normalizing constant, making it ideal for investigating the properties of new
shrinkage priors without having to develop custom sampling algorithms. The new
sampler takes advantage of the special structure of the linear regression
likelihood, allowing it to produce better effective sample size per second than
common alternative approaches.","['P. Richard Hahn', 'Jingyu He', 'Hedibert Lopes']","['stat.CO', 'stat.ML']",2018-06-14 20:39:06+00:00
http://arxiv.org/abs/1806.05730v2,Learning Influence-Receptivity Network Structure with Guarantee,"Traditional works on community detection from observations of information
cascade assume that a single adjacency matrix parametrizes all the observed
cascades. However, in reality the connection structure usually does not stay
the same across cascades. For example, different people have different topics
of interest, therefore the connection structure depends on the
information/topic content of the cascade. In this paper we consider the case
where we observe a sequence of noisy adjacency matrices triggered by
information/event with different topic distributions. We propose a novel latent
model using the intuition that a connection is more likely to exist between two
nodes if they are interested in similar topics, which are common with the
information/event. Specifically, we endow each node with two node-topic
vectors: an influence vector that measures how influential/authoritative they
are on each topic; and a receptivity vector that measures how
receptive/susceptible they are to each topic. We show how these two node-topic
structures can be estimated from observed adjacency matrices with theoretical
guarantee on estimation error, in cases where the topic distributions of the
information/event are known, as well as when they are unknown. Experiments on
synthetic and real data demonstrate the effectiveness of our model and superior
performance compared to state-of-the-art methods.","['Ming Yu', 'Varun Gupta', 'Mladen Kolar']","['stat.ML', 'cs.LG']",2018-06-14 20:24:16+00:00
http://arxiv.org/abs/1806.05722v2,Non-asymptotic Identification of LTI Systems from a Single Trajectory,"We consider the problem of learning a realization for a linear time-invariant
(LTI) dynamical system from input/output data. Given a single input/output
trajectory, we provide finite time analysis for learning the system's Markov
parameters, from which a balanced realization is obtained using the classical
Ho-Kalman algorithm. By proving a stability result for the Ho-Kalman algorithm
and combining it with the sample complexity results for Markov parameters, we
show how much data is needed to learn a balanced realization of the system up
to a desired accuracy with high probability.","['Samet Oymak', 'Necmiye Ozay']","['cs.LG', 'cs.SY', 'math.OC', 'stat.ML']",2018-06-14 20:05:25+00:00
http://arxiv.org/abs/1806.06790v3,Towards Distributed Energy Services: Decentralizing Optimal Power Flow with Machine Learning,"The implementation of optimal power flow (OPF) methods to perform voltage and
power flow regulation in electric networks is generally believed to require
extensive communication. We consider distribution systems with multiple
controllable Distributed Energy Resources (DERs) and present a data-driven
approach to learn control policies for each DER to reconstruct and mimic the
solution to a centralized OPF problem from solely locally available
information. Collectively, all local controllers closely match the centralized
OPF solution, providing near optimal performance and satisfaction of system
constraints. A rate distortion framework enables the analysis of how well the
resulting fully decentralized control policies are able to reconstruct the OPF
solution. The methodology provides a natural extension to decide what nodes a
DER should communicate with to improve the reconstruction of its individual
policy. The method is applied on both single- and three-phase test feeder
networks using data from real loads and distributed generators, focusing on
DERs that do not exhibit inter-temporal dependencies. It provides a framework
for Distribution System Operators to efficiently plan and operate the
contributions of DERs to achieve Distributed Energy Services in distribution
networks.","['Roel Dobbe', 'Oscar Sondermeijer', 'David Fridovich-Keil', 'Daniel Arnold', 'Duncan Callaway', 'Claire Tomlin']","['cs.LG', 'cs.AI', 'cs.IT', 'cs.SY', 'math.IT', 'math.OC', 'stat.ML']",2018-06-14 19:46:37+00:00
http://arxiv.org/abs/1806.05703v3,Multilevel Artificial Neural Network Training for Spatially Correlated Learning,"Multigrid modeling algorithms are a technique used to accelerate relaxation
models running on a hierarchy of similar graphlike structures. We introduce and
demonstrate a new method for training neural networks which uses multilevel
methods. Using an objective function derived from a graph-distance metric, we
perform orthogonally-constrained optimization to find optimal prolongation and
restriction maps between graphs. We compare and contrast several methods for
performing this numerical optimization, and additionally present some new
theoretical results on upper bounds of this type of objective function. Once
calculated, these optimal maps between graphs form the core of Multiscale
Artificial Neural Network (MsANN) training, a new procedure we present which
simultaneously trains a hierarchy of neural network models of varying spatial
resolution. Parameter information is passed between members of this hierarchy
according to standard coarsening and refinement schedules from the multiscale
modelling literature. In our machine learning experiments, these models are
able to learn faster than default training, achieving a comparable level of
error in an order of magnitude fewer training examples.","['C. B. Scott', 'Eric Mjolsness']","['cs.LG', 'stat.ML']",2018-06-14 18:50:47+00:00
http://arxiv.org/abs/1806.05694v1,Discovering Latent Patterns of Urban Cultural Interactions in WeChat for Modern City Planning,"Cultural activity is an inherent aspect of urban life and the success of a
modern city is largely determined by its capacity to offer generous cultural
entertainment to its citizens. To this end, the optimal allocation of cultural
establishments and related resources across urban regions becomes of vital
importance, as it can reduce financial costs in terms of planning and improve
quality of life in the city, more generally. In this paper, we make use of a
large longitudinal dataset of user location check-ins from the online social
network WeChat to develop a data-driven framework for cultural planning in the
city of Beijing. We exploit rich spatio-temporal representations on user
activity at cultural venues and use a novel extended version of the traditional
latent Dirichlet allocation model that incorporates temporal information to
identify latent patterns of urban cultural interactions. Using the
characteristic typologies of mobile user cultural activities emitted by the
model, we determine the levels of demand for different types of cultural
resources across urban areas. We then compare those with the corresponding
levels of supply as driven by the presence and spatial reach of cultural venues
in local areas to obtain high resolution maps that indicate urban regions with
lack of cultural resources, and thus give suggestions for further urban
cultural planning and investment optimisation.","['Xiao Zhou', 'Anastasios Noulas', 'Cecilia Mascoloo', 'Zhongxiang Zhao']","['cs.SI', 'physics.data-an', 'stat.ML']",2018-06-14 18:07:15+00:00
http://arxiv.org/abs/1806.05662v3,GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations,"Modern deep transfer learning approaches have mainly focused on learning
generic feature vectors from one task that are transferable to other tasks,
such as word embeddings in language and pretrained convolutional features in
vision. However, these approaches usually transfer unary features and largely
ignore more structured graphical representations. This work explores the
possibility of learning generic latent relational graphs that capture
dependencies between pairs of data units (e.g., words or pixels) from
large-scale unlabeled data and transferring the graphs to downstream tasks. Our
proposed transfer learning framework improves performance on various tasks
including question answering, natural language inference, sentiment analysis,
and image classification. We also show that the learned graphs are generic
enough to be transferred to different embeddings on which the graphs have not
been trained (including GloVe embeddings, ELMo embeddings, and task-specific
RNN hidden unit), or embedding-free units such as image pixels.","['Zhilin Yang', 'Jake Zhao', 'Bhuwan Dhingra', 'Kaiming He', 'William W. Cohen', 'Ruslan Salakhutdinov', 'Yann LeCun']","['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']",2018-06-14 17:41:19+00:00
http://arxiv.org/abs/1806.05635v1,Self-Imitation Learning,"This paper proposes Self-Imitation Learning (SIL), a simple off-policy
actor-critic algorithm that learns to reproduce the agent's past good
decisions. This algorithm is designed to verify our hypothesis that exploiting
past good experiences can indirectly drive deep exploration. Our empirical
results show that SIL significantly improves advantage actor-critic (A2C) on
several hard exploration Atari games and is competitive to the state-of-the-art
count-based exploration methods. We also show that SIL improves proximal policy
optimization (PPO) on MuJoCo tasks.","['Junhyuk Oh', 'Yijie Guo', 'Satinder Singh', 'Honglak Lee']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-14 16:25:55+00:00
http://arxiv.org/abs/1806.05618v1,Stochastic Variance-Reduced Policy Gradient,"In this paper, we propose a novel reinforcement- learning algorithm
consisting in a stochastic variance-reduced version of policy gradient for
solving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient
(SVRG) methods have proven to be very successful in supervised learning.
However, their adaptation to policy gradient is not straightforward and needs
to account for I) a non-concave objective func- tion; II) approximations in the
full gradient com- putation; and III) a non-stationary sampling pro- cess. The
result is SVRPG, a stochastic variance- reduced policy gradient algorithm that
leverages on importance weights to preserve the unbiased- ness of the gradient
estimate. Under standard as- sumptions on the MDP, we provide convergence
guarantees for SVRPG with a convergence rate that is linear under increasing
batch sizes. Finally, we suggest practical variants of SVRPG, and we
empirically evaluate them on continuous MDPs.","['Matteo Papini', 'Damiano Binaghi', 'Giuseppe Canonaco', 'Matteo Pirotta', 'Marcello Restelli']","['cs.LG', 'stat.ML']",2018-06-14 15:49:13+00:00
http://arxiv.org/abs/1806.05594v3,There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average,"Presently the most successful approaches to semi-supervised learning are
based on consistency regularization, whereby a model is trained to be robust to
small perturbations of its inputs and parameters. To understand consistency
regularization, we conceptually explore how loss geometry interacts with
training procedures. The consistency loss dramatically improves generalization
performance over supervised-only training; however, we show that SGD struggles
to converge on the consistency loss and continues to make large steps that lead
to changes in predictions on the test data. Motivated by these observations, we
propose to train consistency-based methods with Stochastic Weight Averaging
(SWA), a recent approach which averages weights along the trajectory of SGD
with a modified learning rate schedule. We also propose fast-SWA, which further
accelerates convergence by averaging multiple points within each cycle of a
cyclical learning rate schedule. With weight averaging, we achieve the best
known semi-supervised results on CIFAR-10 and CIFAR-100, over many different
quantities of labeled training data. For example, we achieve 5.0% error on
CIFAR-10 with only 4000 labels, compared to the previous best result in the
literature of 6.3%.","['Ben Athiwaratkun', 'Marc Finzi', 'Pavel Izmailov', 'Andrew Gordon Wilson']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2018-06-14 14:58:36+00:00
http://arxiv.org/abs/1806.05575v1,Autoregressive Quantile Networks for Generative Modeling,"We introduce autoregressive implicit quantile networks (AIQN), a
fundamentally different approach to generative modeling than those commonly
used, that implicitly captures the distribution using quantile regression. AIQN
is able to achieve superior perceptual quality and improvements in evaluation
metrics, without incurring a loss of sample diversity. The method can be
applied to many existing models and architectures. In this work we extend the
PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using
Inception score, FID, non-cherry-picked samples, and inpainting results. We
consistently observe that AIQN yields a highly stable algorithm that improves
perceptual quality while maintaining a highly diverse distribution.","['Georg Ostrovski', 'Will Dabney', 'Rémi Munos']","['cs.LG', 'stat.ML']",2018-06-14 14:29:18+00:00
http://arxiv.org/abs/1806.06923v1,Implicit Quantile Networks for Distributional Reinforcement Learning,"In this work, we build on recent advances in distributional reinforcement
learning to give a generally applicable, flexible, and state-of-the-art
distributional variant of DQN. We achieve this by using quantile regression to
approximate the full quantile function for the state-action return
distribution. By reparameterizing a distribution over the sample space, this
yields an implicitly defined return distribution and gives rise to a large
class of risk-sensitive policies. We demonstrate improved performance on the 57
Atari 2600 games in the ALE, and use our algorithm's implicitly defined
distributions to study the effects of risk-sensitive policies in Atari games.","['Will Dabney', 'Georg Ostrovski', 'David Silver', 'Rémi Munos']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-14 14:28:37+00:00
http://arxiv.org/abs/1806.05514v5,The Exact Equivalence of Distance and Kernel Methods for Hypothesis Testing,"Distance-based tests, also called ""energy statistics"", are leading methods
for two-sample and independence tests from the statistics community.
Kernel-based tests, developed from ""kernel mean embeddings"", are leading
methods for two-sample and independence tests from the machine learning
community. A fixed-point transformation was previously proposed to connect the
distance methods and kernel methods for the population statistics. In this
paper, we propose a new bijective transformation between metrics and kernels.
It simplifies the fixed-point transformation, inherits similar theoretical
properties, allows distance methods to be exactly the same as kernel methods
for sample statistics and p-value, and better preserves the data structure upon
transformation. Our results further advance the understanding in distance and
kernel-based tests, streamline the code base for implementing these tests, and
enable a rich literature of distance-based and kernel-based methodologies to
directly communicate with each other.","['Cencheng Shen', 'Joshua T. Vogelstein']","['stat.ML', 'cs.LG']",2018-06-14 12:57:57+00:00
http://arxiv.org/abs/1806.05512v2,NetScore: Towards Universal Metrics for Large-scale Performance Analysis of Deep Neural Networks for Practical On-Device Edge Usage,"Much of the focus in the design of deep neural networks has been on improving
accuracy, leading to more powerful yet highly complex network architectures
that are difficult to deploy in practical scenarios, particularly on edge
devices such as mobile and other consumer devices given their high
computational and memory requirements. As a result, there has been a recent
interest in the design of quantitative metrics for evaluating deep neural
networks that accounts for more than just model accuracy as the sole indicator
of network performance. In this study, we continue the conversation towards
universal metrics for evaluating the performance of deep neural networks for
practical on-device edge usage. In particular, we propose a new balanced metric
called NetScore, which is designed specifically to provide a quantitative
assessment of the balance between accuracy, computational complexity, and
network architecture complexity of a deep neural network, which is important
for on-device edge operation. In what is one of the largest comparative
analysis between deep neural networks in literature, the NetScore metric, the
top-1 accuracy metric, and the popular information density metric were compared
across a diverse set of 60 different deep convolutional neural networks for
image classification on the ImageNet Large Scale Visual Recognition Challenge
(ILSVRC 2012) dataset. The evaluation results across these three metrics for
this diverse set of networks are presented in this study to act as a reference
guide for practitioners in the field. The proposed NetScore metric, along with
the other tested metrics, are by no means perfect, but the hope is to push the
conversation towards better universal metrics for evaluating deep neural
networks for use in practical on-device edge scenarios to help guide
practitioners in model design for such scenarios.",['Alexander Wong'],"['cs.CV', 'cs.LG', 'stat.ML']",2018-06-14 12:55:35+00:00
http://arxiv.org/abs/1806.06920v1,Maximum a Posteriori Policy Optimisation,"We introduce a new algorithm for reinforcement learning called Maximum
aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative
entropy objective. We show that several existing methods can directly be
related to our derivation. We develop two off-policy algorithms and demonstrate
that they are competitive with the state-of-the-art in deep reinforcement
learning. In particular, for continuous control, our method outperforms
existing methods with respect to sample efficiency, premature convergence and
robustness to hyperparameter settings while achieving similar or better final
performance.","['Abbas Abdolmaleki', 'Jost Tobias Springenberg', 'Yuval Tassa', 'Remi Munos', 'Nicolas Heess', 'Martin Riedmiller']","['cs.LG', 'cs.AI', 'cs.IT', 'cs.RO', 'math.IT', 'stat.ML']",2018-06-14 12:46:23+00:00
http://arxiv.org/abs/1806.05502v5,Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes,"Visually predicting the stability of block towers is a popular task in the
domain of intuitive physics. While previous work focusses on prediction
accuracy, a one-dimensional performance measure, we provide a broader analysis
of the learned physical understanding of the final model and how the learning
process can be guided. To this end, we introduce neural stethoscopes as a
general purpose framework for quantifying the degree of importance of specific
factors of influence in deep neural networks as well as for actively promoting
and suppressing information as appropriate. In doing so, we unify concepts from
multitask learning as well as training with auxiliary and adversarial losses.
We apply neural stethoscopes to analyse the state-of-the-art neural network for
stability prediction. We show that the baseline model is susceptible to being
misled by incorrect visual cues. This leads to a performance breakdown to the
level of random guessing when training on scenarios where visual cues are
inversely correlated with stability. Using stethoscopes to promote meaningful
feature extraction increases performance from 51% to 90% prediction accuracy.
Conversely, training on an easy dataset where visual cues are positively
correlated with stability, the baseline model learns a bias leading to poor
performance on a harder dataset. Using an adversarial stethoscope, the network
is successfully de-biased, leading to a performance increase from 66% to 88%.","['Fabian B. Fuchs', 'Oliver Groth', 'Adam R. Kosiorek', 'Alex Bewley', 'Markus Wulfmeier', 'Andrea Vedaldi', 'Ingmar Posner']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']",2018-06-14 12:35:50+00:00
http://arxiv.org/abs/1806.06953v3,Qualitative Measurements of Policy Discrepancy for Return-Based Deep Q-Network,"The deep Q-network (DQN) and return-based reinforcement learning are two
promising algorithms proposed in recent years. DQN brings advances to complex
sequential decision problems, while return-based algorithms have advantages in
making use of sample trajectories. In this paper, we propose a general
framework to combine DQN and most of the return-based reinforcement learning
algorithms, named R-DQN. We show the performance of traditional DQN can be
improved effectively by introducing return-based reinforcement learning. In
order to further improve the R-DQN, we design a strategy with two measurements
which can qualitatively measure the policy discrepancy. Moreover, we give the
two measurements' bounds in the proposed R-DQN framework. We show that
algorithms with our strategy can accurately express the trace coefficient and
achieve a better approximation to return. The experiments, conducted on several
representative tasks from the OpenAI Gym library, validate the effectiveness of
the proposed measurements. The results also show that the algorithms with our
strategy outperform the state-of-the-art methods.","['Wenjia Meng', 'Qian Zheng', 'Long Yang', 'Pengfei Li', 'Gang Pan']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-14 12:12:18+00:00
http://arxiv.org/abs/1806.05490v3,Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo,"Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian
Processes that combine well calibrated uncertainty estimates with the high
flexibility of multilayer models. One of the biggest challenges with these
models is that exact inference is intractable. The current state-of-the-art
inference method, Variational Inference (VI), employs a Gaussian approximation
to the posterior distribution. This can be a potentially poor unimodal
approximation of the generally multimodal posterior. In this work, we provide
evidence for the non-Gaussian nature of the posterior and we apply the
Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To
efficiently optimize the hyperparameters, we introduce the Moving Window MCEM
algorithm. This results in significantly better predictions at a lower
computational cost than its VI counterpart. Thus our method establishes a new
state-of-the-art for inference in DGPs.","['Marton Havasi', 'José Miguel Hernández-Lobato', 'Juan José Murillo-Fuentes']","['stat.ML', 'cs.LG']",2018-06-14 12:00:38+00:00
http://arxiv.org/abs/1806.05476v1,Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data,"In the past few years, Convolutional Neural Networks (CNNs) have been
achieving state-of-the-art performance on a variety of problems. Many companies
employ resources and money to generate these models and provide them as an API,
therefore it is in their best interest to protect them, i.e., to avoid that
someone else copies them. Recent studies revealed that state-of-the-art CNNs
are vulnerable to adversarial examples attacks, and this weakness indicates
that CNNs do not need to operate in the problem domain (PD). Therefore, we
hypothesize that they also do not need to be trained with examples of the PD in
order to operate in it.
  Given these facts, in this paper, we investigate if a target black-box CNN
can be copied by persuading it to confess its knowledge through random
non-labeled data. The copy is two-fold: i) the target network is queried with
random data and its predictions are used to create a fake dataset with the
knowledge of the network; and ii) a copycat network is trained with the fake
dataset and should be able to achieve similar performance as the target
network.
  This hypothesis was evaluated locally in three problems (facial expression,
object, and crosswalk classification) and against a cloud-based API. In the
copy attacks, images from both non-problem domain and PD were used. All copycat
networks achieved at least 93.7% of the performance of the original models with
non-problem domain data, and at least 98.6% using additional data from the PD.
Additionally, the copycat CNN successfully copied at least 97.3% of the
performance of the Microsoft Azure Emotion API. Our results show that it is
possible to create a copycat CNN by simply querying a target network as
black-box with random non-labeled data.","['Jacson Rodrigues Correia-Silva', 'Rodrigo F. Berriel', 'Claudine Badue', 'Alberto F. de Souza', 'Thiago Oliveira-Santos']","['cs.CV', 'stat.ML']",2018-06-14 11:32:27+00:00
http://arxiv.org/abs/1806.05454v1,Low-rank geometric mean metric learning,"We propose a low-rank approach to learning a Mahalanobis metric from data.
Inspired by the recent geometric mean metric learning (GMML) algorithm, we
propose a low-rank variant of the algorithm. This allows to jointly learn a
low-dimensional subspace where the data reside and the Mahalanobis metric that
appropriately fits the data. Our results show that we compete effectively with
GMML at lower ranks.","['Mukul Bhutani', 'Pratik Jawanpuria', 'Hiroyuki Kasai', 'Bamdev Mishra']","['cs.LG', 'stat.ML']",2018-06-14 10:35:21+00:00
http://arxiv.org/abs/1806.05451v3,The committee machine: Computational to statistical gaps in learning a two-layers neural network,"Heuristic tools from statistical physics have been used in the past to locate
the phase transitions and compute the optimal learning and generalization
errors in the teacher-student scenario in multi-layer neural networks. In this
contribution, we provide a rigorous justification of these approaches for a
two-layers neural network model called the committee machine. We also introduce
a version of the approximate message passing (AMP) algorithm for the committee
machine that allows to perform optimal learning in polynomial time for a large
set of parameters. We find that there are regimes in which a low generalization
error is information-theoretically achievable while the AMP algorithm fails to
deliver it, strongly suggesting that no efficient algorithm exists for those
cases, and unveiling a large computational gap.","['Benjamin Aubin', 'Antoine Maillard', 'Jean Barbier', 'Florent Krzakala', 'Nicolas Macris', 'Lenka Zdeborová']","['cs.LG', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'physics.comp-ph', 'stat.ML']",2018-06-14 10:22:04+00:00
http://arxiv.org/abs/1806.05438v4,Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors,"We consider stochastic gradient descent and its averaging variant for binary
classification problems in a reproducing kernel Hilbert space. In the
traditional analysis using a consistency property of loss functions, it is
known that the expected classification error converges more slowly than the
expected risk even when assuming a low-noise condition on the conditional label
probabilities. Consequently, the resulting rate is sublinear. Therefore, it is
important to consider whether much faster convergence of the expected
classification error can be achieved. In recent research, an exponential
convergence rate for stochastic gradient descent was shown under a strong
low-noise condition but provided theoretical analysis was limited to the
squared loss function, which is somewhat inadequate for binary classification
tasks. In this paper, we show an exponential convergence of the expected
classification error in the final phase of the stochastic gradient descent for
a wide class of differentiable convex loss functions under similar assumptions.
As for the averaged stochastic gradient descent, we show that the same
convergence rate holds from the early phase of training. In experiments, we
verify our analyses on the $L_2$-regularized logistic regression.","['Atsushi Nitanda', 'Taiji Suzuki']","['stat.ML', 'cs.LG', 'math.OC']",2018-06-14 09:55:42+00:00
http://arxiv.org/abs/1806.05437v3,ServeNet: A Deep Neural Network for Web Services Classification,"Automated service classification plays a crucial role in service discovery,
selection, and composition. Machine learning has been widely used for service
classification in recent years. However, the performance of conventional
machine learning methods highly depends on the quality of manual feature
engineering. In this paper, we present a novel deep neural network to
automatically abstract low-level representation of both service name and
service description to high-level merged features without feature engineering
and the length limitation, and then predict service classification on 50
service categories. To demonstrate the effectiveness of our approach, we
conduct a comprehensive experimental study by comparing 10 machine learning
methods on 10,000 real-world web services. The result shows that the proposed
deep neural network can achieve higher accuracy in classification and more
robust than other machine learning methods.","['Yilong Yang', 'Nafees Qamar', 'Peng Liu', 'Katarina Grolinger', 'Weiru Wang', 'Zhi Li', 'Zhifang Liao']","['cs.LG', 'cs.SE', 'stat.ML']",2018-06-14 09:53:56+00:00
http://arxiv.org/abs/1806.05421v5,Selfless Sequential Learning,"Sequential learning, also called lifelong learning, studies the problem of
learning tasks in a sequence with access restricted to only the data of the
current task. In this paper we look at a scenario with fixed model capacity,
and postulate that the learning process should not be selfish, i.e. it should
account for future tasks to be added and thus leave enough capacity for them.
To achieve Selfless Sequential Learning we study different regularization
strategies and activation functions. We find that imposing sparsity at the
level of the representation (i.e.~neuron activations) is more beneficial for
sequential learning than encouraging parameter sparsity. In particular, we
propose a novel regularizer, that encourages representation sparsity by means
of neural inhibition. It results in few active neurons which in turn leaves
more free neurons to be utilized by upcoming tasks. As neural inhibition over
an entire layer can be too drastic, especially for complex tasks requiring
strong representations, our regularizer only inhibits other neurons in a local
neighbourhood, inspired by lateral inhibition processes in the brain. We
combine our novel regularizer, with state-of-the-art lifelong learning methods
that penalize changes to important previously learned parts of the network. We
show that our new regularizer leads to increased sparsity which translates in
consistent performance improvement %over alternative regularizers we studied on
diverse datasets.","['Rahaf Aljundi', 'Marcus Rohrbach', 'Tinne Tuytelaars']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']",2018-06-14 09:06:10+00:00
http://arxiv.org/abs/1806.05419v1,Ranking Recovery from Limited Comparisons using Low-Rank Matrix Completion,"This paper proposes a new method for solving the well-known rank aggregation
problem from pairwise comparisons using the method of low-rank matrix
completion. The partial and noisy data of pairwise comparisons is transformed
into a matrix form. We then use tools from matrix completion, which has served
as a major component in the low-rank completion solution of the Netflix
challenge, to construct the preference of the different objects. In our
approach, the data of multiple comparisons is used to create an estimate of the
probability of object i to win (or be chosen) over object j, where only a
partial set of comparisons between N objects is known. The data is then
transformed into a matrix form for which the noiseless solution has a known
rank of one. An alternating minimization algorithm, in which the target matrix
takes a bilinear form, is then used in combination with maximum likelihood
estimation for both factors. The reconstructed matrix is used to obtain the
true underlying preference intensity. This work demonstrates the improvement of
our proposed algorithm over the current state-of-the-art in both simulated
scenarios and real data.","['Tal Levy', 'Alireza Vahid', 'Raja Giryes']","['stat.ML', 'cs.LG', 'cs.NA', 'math.ST', 'stat.TH']",2018-06-14 09:01:46+00:00
http://arxiv.org/abs/1806.09679v1,On the Resilience of RTL NN Accelerators: Fault Characterization and Mitigation,"Machine Learning (ML) is making a strong resurgence in tune with the massive
generation of unstructured data which in turn requires massive computational
resources. Due to the inherently compute- and power-intensive structure of
Neural Networks (NNs), hardware accelerators emerge as a promising solution.
However, with technology node scaling below 10nm, hardware accelerators become
more susceptible to faults, which in turn can impact the NN accuracy. In this
paper, we study the resilience aspects of Register-Transfer Level (RTL) model
of NN accelerators, in particular, fault characterization and mitigation. By
following a High-Level Synthesis (HLS) approach, first, we characterize the
vulnerability of various components of RTL NN. We observed that the severity of
faults depends on both i) application-level specifications, i.e., NN data
(inputs, weights, or intermediate), NN layers, and NN activation functions, and
ii) architectural-level specifications, i.e., data representation model and the
parallelism degree of the underlying accelerator. Second, motivated by
characterization results, we present a low-overhead fault mitigation technique
that can efficiently correct bit flips, by 47.3% better than state-of-the-art
methods.","['Behzad Salami', 'Osman Unsal', 'Adrian Cristal']","['cs.LG', 'cs.AR', 'stat.ML', '68T01']",2018-06-14 08:52:18+00:00
http://arxiv.org/abs/1806.05413v2,Learning Dynamics of Linear Denoising Autoencoders,"Denoising autoencoders (DAEs) have proven useful for unsupervised
representation learning, but a thorough theoretical understanding is still
lacking of how the input noise influences learning. Here we develop theory for
how noise influences learning in DAEs. By focusing on linear DAEs, we are able
to derive analytic expressions that exactly describe their learning dynamics.
We verify our theoretical predictions with simulations as well as experiments
on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise
allows DAEs to ignore low variance directions in the inputs while learning to
reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs
to standard regularised autoencoders, we show that noise has a similar
regularisation effect to weight decay, but with faster training dynamics. We
also show that our theoretical predictions approximate learning dynamics on
real-world data and qualitatively match observed dynamics in nonlinear DAEs.","['Arnu Pretorius', 'Steve Kroon', 'Herman Kamper']","['stat.ML', 'cs.LG']",2018-06-14 08:46:36+00:00
http://arxiv.org/abs/1806.05403v1,On the Perceptron's Compression,"We study and provide exposition to several phenomena that are related to the
perceptron's compression. One theme concerns modifications of the perceptron
algorithm that yield better guarantees on the margin of the hyperplane it
outputs. These modifications can be useful in training neural networks as well,
and we demonstrate them with some experimental data. In a second theme, we
deduce conclusions from the perceptron's compression in various contexts.","['Shay Moran', 'Ido Nachum', 'Itai Panasoff', 'Amir Yehudayoff']","['cs.LG', 'stat.ML']",2018-06-14 08:00:45+00:00
http://arxiv.org/abs/1806.05394v2,Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks,"Recurrent neural networks have gained widespread use in modeling sequence
data across various domains. While many successful recurrent architectures
employ a notion of gating, the exact mechanism that enables such remarkable
performance is not well understood. We develop a theory for signal propagation
in recurrent networks after random initialization using a combination of mean
field theory and random matrix theory. To simplify our discussion, we introduce
a new RNN cell with a simple gating mechanism that we call the minimalRNN and
compare it with vanilla RNNs. Our theory allows us to define a maximum
timescale over which RNNs can remember an input. We show that this theory
predicts trainability for both recurrent architectures. We show that gated
recurrent networks feature a much broader, more robust, trainable region than
vanilla RNNs, which corroborates recent experimental findings. Finally, we
develop a closed-form critical initialization scheme that achieves dynamical
isometry in both vanilla RNNs and minimalRNNs. We show that this results in
significantly improvement in training dynamics. Finally, we demonstrate that
the minimalRNN achieves comparable performance to its more complex
counterparts, such as LSTMs or GRUs, on a language modeling task.","['Minmin Chen', 'Jeffrey Pennington', 'Samuel S. Schoenholz']","['stat.ML', 'cs.LG']",2018-06-14 07:04:31+00:00
http://arxiv.org/abs/1806.05393v2,"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks","In recent years, state-of-the-art methods in computer vision have utilized
increasingly deep convolutional neural network architectures (CNNs), with some
of the most successful models employing hundreds or even thousands of layers. A
variety of pathologies such as vanishing/exploding gradients make training such
deep networks challenging. While residual connections and batch normalization
do enable training at these depths, it has remained unclear whether such
specialized architecture designs are truly necessary to train deep CNNs. In
this work, we demonstrate that it is possible to train vanilla CNNs with ten
thousand layers or more simply by using an appropriate initialization scheme.
We derive this initialization scheme theoretically by developing a mean field
theory for signal propagation and by characterizing the conditions for
dynamical isometry, the equilibration of singular values of the input-output
Jacobian matrix. These conditions require that the convolution operator be an
orthogonal transformation in the sense that it is norm-preserving. We present
an algorithm for generating such random initial orthogonal convolution kernels
and demonstrate empirically that they enable efficient training of extremely
deep architectures.","['Lechao Xiao', 'Yasaman Bahri', 'Jascha Sohl-Dickstein', 'Samuel S. Schoenholz', 'Jeffrey Pennington']","['stat.ML', 'cs.LG']",2018-06-14 07:04:15+00:00
http://arxiv.org/abs/1806.05387v1,Parameter Learning and Change Detection Using a Particle Filter With Accelerated Adaptation,"This paper presents the construction of a particle filter, which incorporates
elements inspired by genetic algorithms, in order to achieve accelerated
adaptation of the estimated posterior distribution to changes in model
parameters. Specifically, the filter is designed for the situation where the
subsequent data in online sequential filtering does not match the model
posterior filtered based on data up to a current point in time. The examples
considered encompass parameter regime shifts and stochastic volatility. The
filter adapts to regime shifts extremely rapidly and delivers a clear heuristic
for distinguishing between regime shifts and stochastic volatility, even though
the model dynamics assumed by the filter exhibit neither of those features.","['Karol Gellert', 'Erik Schlögl']","['stat.ML', 'cs.CE', 'cs.LG', 'cs.NE', 'q-fin.ST']",2018-06-14 06:41:10+00:00
http://arxiv.org/abs/1806.05382v3,PCAS: Pruning Channels with Attention Statistics for Deep Network Compression,"Compression techniques for deep neural networks are important for
implementing them on small embedded devices. In particular, channel-pruning is
a useful technique for realizing compact networks. However, many conventional
methods require manual setting of compression ratios in each layer. It is
difficult to analyze the relationships between all layers, especially for
deeper models. To address these issues, we propose a simple channel-pruning
technique based on attention statistics that enables to evaluate the importance
of channels. We improved the method by means of a criterion for automatic
channel selection, using a single compression ratio for the entire model in
place of per-layer model analysis. The proposed approach achieved superior
performance over conventional methods with respect to accuracy and the
computational costs for various models and datasets. We provide analysis
results for behavior of the proposed criterion on different datasets to
demonstrate its favorable properties for channel pruning.","['Kohei Yamamoto', 'Kurato Maeno']","['stat.ML', 'cs.CV', 'cs.LG']",2018-06-14 06:28:59+00:00
http://arxiv.org/abs/1806.05358v4,Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning,"We study robust distributed learning that involves minimizing a non-convex
loss function with saddle points. We consider the Byzantine setting where some
worker machines have abnormal or even arbitrary and adversarial behavior. In
this setting, the Byzantine machines may create fake local minima near a saddle
point that is far away from any true local minimum, even when robust gradient
estimators are used. We develop ByzantinePGD, a robust first-order algorithm
that can provably escape saddle points and fake local minima, and converge to
an approximate true local minimizer with low iteration complexity. As a
by-product, we give a simpler algorithm and analysis for escaping saddle points
in the usual non-Byzantine setting. We further discuss three robust gradient
estimators that can be used in ByzantinePGD, including median, trimmed mean,
and iterative filtering. We characterize their performance in concrete
statistical settings, and argue for their near-optimality in low and high
dimensional regimes.","['Dong Yin', 'Yudong Chen', 'Kannan Ramchandran', 'Peter Bartlett']","['cs.LG', 'cs.CR', 'cs.DC', 'math.OC', 'stat.ML']",2018-06-14 04:15:59+00:00
http://arxiv.org/abs/1806.05357v1,Deep Multi-Output Forecasting: Learning to Accurately Predict Blood Glucose Trajectories,"In many forecasting applications, it is valuable to predict not only the
value of a signal at a certain time point in the future, but also the values
leading up to that point. This is especially true in clinical applications,
where the future state of the patient can be less important than the patient's
overall trajectory. This requires multi-step forecasting, a forecasting variant
where one aims to predict multiple values in the future simultaneously.
Standard methods to accomplish this can propagate error from prediction to
prediction, reducing quality over the long term. In light of these challenges,
we propose multi-output deep architectures for multi-step forecasting in which
we explicitly model the distribution of future values of the signal over a
prediction horizon. We apply these techniques to the challenging and clinically
relevant task of blood glucose forecasting. Through a series of experiments on
a real-world dataset consisting of 550K blood glucose measurements, we
demonstrate the effectiveness of our proposed approaches in capturing the
underlying signal dynamics. Compared to existing shallow and deep methods, we
find that our proposed approaches improve performance individually and capture
complementary information, leading to a large improvement over the baseline
when combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the
results suggest the efficacy of our proposed approach in predicting blood
glucose level and multi-step forecasting more generally.","['Ian Fox', 'Lynn Ang', 'Mamta Jaiswal', 'Rodica Pop-Busui', 'Jenna Wiens']","['cs.LG', 'stat.ML']",2018-06-14 04:06:00+00:00
http://arxiv.org/abs/1806.05356v1,Finding GEMS: Multi-Scale Dictionaries for High-Dimensional Graph Signals,"Modern data introduces new challenges to classic signal processing
approaches, leading to a growing interest in the field of graph signal
processing. A powerful and well established model for real world signals in
various domains is sparse representation over a dictionary, combined with the
ability to train the dictionary from signal examples. This model has been
successfully applied to graph signals as well by integrating the underlying
graph topology into the learned dictionary. Nonetheless, dictionary learning
methods for graph signals are typically restricted to small dimensions due to
the computational constraints that the dictionary learning problem entails, and
due to the direct use of the graph Laplacian matrix. In this paper, we propose
a dictionary learning algorithm that applies to a broader class of graph
signals, and is capable of handling much higher dimensional data. We
incorporate the underlying graph topology both implicitly, by forcing the
learned dictionary atoms to be sparse combinations of graph-wavelet functions,
and explicitly, by adding direct graph constraints to promote smoothness in
both the feature and manifold domains. The resulting atoms are thus adapted to
the data of interest while adhering to the underlying graph structure and
possessing a desired multi-scale property. Experimental results on several
datasets, representing both synthetic and real network data of different
nature, demonstrate the effectiveness of the proposed algorithm for graph
signal processing even in high dimensions.","['Yael Yankelevsky', 'Michael Elad']","['cs.LG', 'stat.ML']",2018-06-14 04:04:24+00:00
http://arxiv.org/abs/1806.05355v1,Scalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization,"We propose a simple and easy to implement neural network compression
algorithm that achieves results competitive with more complicated
state-of-the-art methods. The key idea is to modify the original optimization
problem by adding K independent Gaussian priors (corresponding to the k-means
objective) over the network parameters to achieve parameter quantization, as
well as an L1 penalty to achieve pruning. Unlike many existing
quantization-based methods, our method uses hard clustering assignments of
network parameters, which adds minimal change or overhead to standard network
training. We also demonstrate experimentally that tying neural network
parameters provides less gain in generalization performance than changing
network architecture and connectivity patterns entirely.","['Yibo Yang', 'Nicholas Ruozzi', 'Vibhav Gogate']","['stat.ML', 'cs.LG']",2018-06-14 03:59:37+00:00
http://arxiv.org/abs/1806.05337v2,Hierarchical interpretations for neural network predictions,"Deep neural networks (DNNs) have achieved impressive predictive performance
due to their ability to learn complex, non-linear relationships between
variables. However, the inability to effectively visualize these relationships
has led to DNNs being characterized as black boxes and consequently limited
their applications. To ameliorate this problem, we introduce the use of
hierarchical interpretations to explain DNN predictions through our proposed
method, agglomerative contextual decomposition (ACD). Given a prediction from a
trained DNN, ACD produces a hierarchical clustering of the input features,
along with the contribution of each cluster to the final prediction. This
hierarchy is optimized to identify clusters of features that the DNN learned
are predictive. Using examples from Stanford Sentiment Treebank and ImageNet,
we show that ACD is effective at diagnosing incorrect predictions and
identifying dataset bias. Through human experiments, we demonstrate that ACD
enables users both to identify the more accurate of two DNNs and to better
trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to
adversarial perturbations, implying that it captures fundamental aspects of the
input and ignores spurious noise.","['Chandan Singh', 'W. James Murdoch', 'Bin Yu']","['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'stat.ML']",2018-06-14 02:41:03+00:00
http://arxiv.org/abs/1806.05310v1,Deep Reinforcement Learning for Dynamic Urban Transportation Problems,"We explore the use of deep learning and deep reinforcement learning for
optimization problems in transportation. Many transportation system analysis
tasks are formulated as an optimization problem - such as optimal control
problems in intelligent transportation systems and long term urban planning.
Often transportation models used to represent dynamics of a transportation
system involve large data sets with complex input-output interactions and are
difficult to use in the context of optimization. Use of deep learning
metamodels can produce a lower dimensional representation of those relations
and allow to implement optimization and reinforcement learning algorithms in an
efficient manner. In particular, we develop deep learning models for
calibrating transportation simulators and for reinforcement learning to solve
the problem of optimal scheduling of travelers on the network.","['Laura Schultz', 'Vadim Sokolov']","['stat.ML', 'cs.LG']",2018-06-14 00:24:49+00:00
http://arxiv.org/abs/1806.05297v1,Pattern Dependence Detection using n-TARP Clustering,"Consider an experiment involving a potentially small number of subjects. Some
random variables are observed on each subject: a high-dimensional one called
the ""observed"" random variable, and a one-dimensional one called the ""outcome""
random variable. We are interested in the dependencies between the observed
random variable and the outcome random variable. We propose a method to
quantify and validate the dependencies of the outcome random variable on the
various patterns contained in the observed random variable. Different degrees
of relationship are explored (linear, quadratic, cubic, ...). This work is
motivated by the need to analyze educational data, which often involves
high-dimensional data representing a small number of students. Thus our
implementation is designed for a small number of subjects; however, it can be
easily modified to handle a very large dataset. As an illustration, the
proposed method is used to study the influence of certain skills on the course
grade of students in a signal processing class. A valid dependency of the grade
on the different skill patterns is observed in the data.","['Tarun Yellamraju', 'Mireille Boutin']","['stat.ML', 'cs.LG', 'stat.ME']",2018-06-13 23:14:45+00:00
http://arxiv.org/abs/1806.05272v1,Benchmarks for Image Classification and Other High-dimensional Pattern Recognition Problems,"A good classification method should yield more accurate results than simple
heuristics. But there are classification problems, especially high-dimensional
ones like the ones based on image/video data, for which simple heuristics can
work quite accurately; the structure of the data in such problems is easy to
uncover without any sophisticated or computationally expensive method. On the
other hand, some problems have a structure that can only be found with
sophisticated pattern recognition methods. We are interested in quantifying the
difficulty of a given high-dimensional pattern recognition problem. We consider
the case where the patterns come from two pre-determined classes and where the
objects are represented by points in a high-dimensional vector space. However,
the framework we propose is extendable to an arbitrarily large number of
classes. We propose classification benchmarks based on simple random projection
heuristics. Our benchmarks are 2D curves parameterized by the classification
error and computational cost of these simple heuristics. Each curve divides the
plane into a ""positive- gain"" and a ""negative-gain"" region. The latter contains
methods that are ill-suited for the given classification problem. The former is
divided into two by the curve asymptote; methods that lie in the small region
under the curve but right of the asymptote merely provide a computational gain
but no structural advantage over the random heuristics. We prove that the curve
asymptotes are optimal (i.e. at Bayes error) in some cases, and thus no
sophisticated method can provide a structural advantage over the random
heuristics. Such classification problems, an example of which we present in our
numerical experiments, provide poor ground for testing new pattern
classification methods.","['Tarun Yellamraju', 'Jonas Hepp', 'Mireille Boutin']","['stat.ML', 'cs.CV', 'cs.LG']",2018-06-13 21:22:30+00:00
http://arxiv.org/abs/1806.05975v2,Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors,"Bayesian Neural Networks (BNNs) have recently received increasing attention
for their ability to provide well-calibrated posterior uncertainties. However,
model selection---even choosing the number of nodes---remains an open question.
Recent work has proposed the use of a horseshoe prior over node pre-activations
of a Bayesian neural network, which effectively turns off nodes that do not
help explain the data. In this work, we propose several modeling and inference
advances that consistently improve the compactness of the model learned while
maintaining predictive performance, especially in smaller-sample settings
including reinforcement learning.","['Soumya Ghosh', 'Jiayu Yao', 'Finale Doshi-Velez']","['stat.ML', 'cs.LG']",2018-06-13 21:20:43+00:00
http://arxiv.org/abs/1806.05250v1,What About Applied Fairness?,"Machine learning practitioners are often ambivalent about the ethical aspects
of their products. We believe anything that gets us from that current state to
one in which our systems are achieving some degree of fairness is an
improvement that should be welcomed. This is true even when that progress does
not get us 100% of the way to the goal of ""complete"" fairness or perfectly
align with our personal belief on which measure of fairness is used. Some
measure of fairness being built would still put us in a better position than
the status quo. Impediments to getting fairness and ethical concerns applied in
real applications, whether they are abstruse philosophical debates or technical
overhead such as the introduction of ever more hyper-parameters, should be
avoided. In this paper we further elaborate on our argument for this viewpoint
and its importance.","['Jared Sylvester', 'Edward Raff']","['cs.AI', 'cs.LG', 'stat.ML']",2018-06-13 20:15:28+00:00
http://arxiv.org/abs/1806.05236v7,Manifold Mixup: Better Representations by Interpolating Hidden States,"Deep neural networks excel at learning the training data, but often provide
incorrect and confident predictions when evaluated on slightly different test
examples. This includes distribution shifts, outliers, and adversarial
examples. To address these issues, we propose Manifold Mixup, a simple
regularizer that encourages neural networks to predict less confidently on
interpolations of hidden representations. Manifold Mixup leverages semantic
interpolations as additional training signal, obtaining neural networks with
smoother decision boundaries at multiple levels of representation. As a result,
neural networks trained with Manifold Mixup learn class-representations with
fewer directions of variance. We prove theory on why this flattening happens
under ideal conditions, validate it on practical situations, and connect it to
previous works on information theory and generalization. In spite of incurring
no significant computation and being implemented in a few lines of code,
Manifold Mixup improves strong baselines in supervised learning, robustness to
single-step adversarial attacks, and test log-likelihood.","['Vikas Verma', 'Alex Lamb', 'Christopher Beckham', 'Amir Najafi', 'Ioannis Mitliagkas', 'Aaron Courville', 'David Lopez-Paz', 'Yoshua Bengio']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.NE']",2018-06-13 19:32:59+00:00
http://arxiv.org/abs/1806.05161v3,Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate,"Many modern machine learning models are trained to achieve zero or near-zero
training error in order to obtain near-optimal (but non-zero) test error. This
phenomenon of strong generalization performance for ""overfitted"" / interpolated
classifiers appears to be ubiquitous in high-dimensional data, having been
observed in deep networks, kernel machines, boosting and random forests. Their
performance is consistently robust even when the data contain large amounts of
label noise.
  Very little theory is available to explain these observations. The vast
majority of theoretical analyses of generalization allows for interpolation
only when there is little or no label noise. This paper takes a step toward a
theoretical foundation for interpolated classifiers by analyzing local
interpolating schemes, including geometric simplicial interpolation algorithm
and singularly weighted $k$-nearest neighbor schemes. Consistency or
near-consistency is proved for these schemes in classification and regression
problems. Moreover, the nearest neighbor schemes exhibit optimal rates under
some standard statistical assumptions.
  Finally, this paper suggests a way to explain the phenomenon of adversarial
examples, which are seemingly ubiquitous in modern machine learning, and also
discusses some connections to kernel machines and random forests in the
interpolated regime.","['Mikhail Belkin', 'Daniel Hsu', 'Partha Mitra']","['stat.ML', 'cond-mat.stat-mech', 'cs.LG']",2018-06-13 17:37:55+00:00
http://arxiv.org/abs/1806.05159v4,"On Tighter Generalization Bound for Deep Neural Networks: CNNs, ResNets, and Beyond","We establish a margin based data dependent generalization error bound for a
general family of deep neural networks in terms of the depth and width, as well
as the Jacobian of the networks. Through introducing a new characterization of
the Lipschitz properties of neural network family, we achieve significantly
tighter generalization bounds than existing results. Moreover, we show that the
generalization bound can be further improved for bounded losses. Aside from the
general feedforward deep neural networks, our results can be applied to derive
new bounds for popular architectures, including convolutional neural networks
(CNNs) and residual networks (ResNets). When achieving same generalization
errors with previous arts, our bounds allow for the choice of larger parameter
spaces of weight matrices, inducing potentially stronger expressive ability for
neural networks. Numerical evaluation is also provided to support our theory.","['Xingguo Li', 'Junwei Lu', 'Zhaoran Wang', 'Jarvis Haupt', 'Tuo Zhao']","['cs.LG', 'stat.ML']",2018-06-13 17:35:55+00:00
http://arxiv.org/abs/1806.05151v3,On Landscape of Lagrangian Functions and Stochastic Search for Constrained Nonconvex Optimization,"We study constrained nonconvex optimization problems in machine learning,
signal processing, and stochastic control. It is well-known that these problems
can be rewritten to a minimax problem in a Lagrangian form. However, due to the
lack of convexity, their landscape is not well understood and how to find the
stable equilibria of the Lagrangian function is still unknown. To bridge the
gap, we study the landscape of the Lagrangian function. Further, we define a
special class of Lagrangian functions. They enjoy two properties: 1.Equilibria
are either stable or unstable (Formal definition in Section 2); 2.Stable
equilibria correspond to the global optima of the original problem. We show
that a generalized eigenvalue (GEV) problem, including canonical correlation
analysis and other problems, belongs to the class. Specifically, we
characterize its stable and unstable equilibria by leveraging an invariant
group and symmetric property (more details in Section 3). Motivated by these
neat geometric structures, we propose a simple, efficient, and stochastic
primal-dual algorithm solving the online GEV problem. Theoretically, we provide
sufficient conditions, based on which we establish an asymptotic convergence
rate and obtain the first sample complexity result for the online GEV problem
by diffusion approximations, which are widely used in applied probability and
stochastic control. Numerical results are provided to support our theory.","['Zhehui Chen', 'Xingguo Li', 'Lin F. Yang', 'Jarvis Haupt', 'Tuo Zhao']","['cs.LG', 'math.OC', 'stat.ML']",2018-06-13 17:19:22+00:00
http://arxiv.org/abs/1806.05139v2,High-Dimensional Inference for Cluster-Based Graphical Models,"Motivated by modern applications in which one constructs graphical models
based on a very large number of features, this paper introduces a new class of
cluster-based graphical models, in which variable clustering is applied as an
initial step for reducing the dimension of the feature space. We employ model
assisted clustering, in which the clusters contain features that are similar to
the same unobserved latent variable. Two different cluster-based Gaussian
graphical models are considered: the latent variable graph, corresponding to
the graphical model associated with the unobserved latent variables, and the
cluster-average graph, corresponding to the vector of features averaged over
clusters. Our study reveals that likelihood based inference for the latent
graph, not analyzed previously, is analytically intractable. Our main
contribution is the development and analysis of alternative estimation and
inference strategies, for the precision matrix of an unobservable latent vector
$Z$. We replace the likelihood of the data by an appropriate class of empirical
risk functions, that can be specialized to the latent graphical model and to
the simpler, but under-analyzed, cluster-average graphical model. The
estimators thus derived can be used for inference on the graph structure, for
instance on edge strength or pattern recovery. Inference is based on the
asymptotic limits of the entry-wise estimates of the precision matrices
associated with the conditional independence graphs under consideration. While
taking the uncertainty induced by the clustering step into account, we
establish Berry-Esseen central limit theorems for the proposed estimators. It
is noteworthy that, although the clusters are estimated adaptively from the
data, the central limit theorems regarding the entries of the estimated graphs
are proved under the same conditions one would use if the clusters were
known....","['Carson Eisenach', 'Florentina Bunea', 'Yang Ning', 'Claudiu Dinicu']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2018-06-13 16:37:34+00:00
http://arxiv.org/abs/1806.05138v1,Generative Neural Machine Translation,"We introduce Generative Neural Machine Translation (GNMT), a latent variable
architecture which is designed to model the semantics of the source and target
sentences. We modify an encoder-decoder translation model by adding a latent
variable as a language agnostic representation which is encouraged to learn the
meaning of the sentence. GNMT achieves competitive BLEU scores on pure
translation tasks, and is superior when there are missing words in the source
sentence. We augment the model to facilitate multilingual translation and
semi-supervised learning without adding parameters. This framework
significantly reduces overfitting when there is limited paired data available,
and is effective for translating between pairs of languages not seen during
training.","['Harshil Shah', 'David Barber']","['cs.CL', 'cs.LG', 'stat.ML']",2018-06-13 16:35:32+00:00
http://arxiv.org/abs/1806.05134v3,Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications,"Many complex domains, such as robotics control and real-time strategy (RTS)
games, require an agent to learn a continuous control. In the former, an agent
learns a policy over $\mathbb{R}^d$ and in the latter, over a discrete set of
actions each of which is parametrized by a continuous parameter. Such problems
are naturally solved using policy based reinforcement learning (RL) methods,
but unfortunately these often suffer from high variance leading to instability
and slow convergence. Unnecessary variance is introduced whenever policies over
bounded action spaces are modeled using distributions with unbounded support by
applying a transformation $T$ to the sampled action before execution in the
environment. Recently, the variance reduced clipped action policy gradient
(CAPG) was introduced for actions in bounded intervals, but to date no variance
reduced methods exist when the action is a direction, something often seen in
RTS games. To this end we introduce the angular policy gradient (APG), a
stochastic policy gradient method for directional control. With the marginal
policy gradients family of estimators we present a unified analysis of the
variance reduction properties of APG and CAPG; our results provide a stronger
guarantee than existing analyses for CAPG. Experimental results on a popular
RTS game and a navigation task show that the APG estimator offers a substantial
improvement over the standard policy gradient.","['Carson Eisenach', 'Haichuan Yang', 'Ji Liu', 'Han Liu']","['cs.LG', 'stat.ML']",2018-06-13 16:32:27+00:00
http://arxiv.org/abs/1806.05112v1,Comparing Fairness Criteria Based on Social Outcome,"Fairness in algorithmic decision-making processes is attracting increasing
concern. When an algorithm is applied to human-related decision-making an
estimator solely optimizing its predictive power can learn biases on the
existing data, which motivates us the notion of fairness in machine learning.
while several different notions are studied in the literature, little studies
are done on how these notions affect the individuals. We demonstrate such a
comparison between several policies induced by well-known fairness criteria,
including the color-blind (CB), the demographic parity (DP), and the equalized
odds (EO). We show that the EO is the only criterion among them that removes
group-level disparity. Empirical studies on the social welfare and disparity of
these policies are conducted.","['Junpei Komiyama', 'Hajime Shimao']","['cs.AI', 'cs.LG', 'stat.ML']",2018-06-13 15:34:13+00:00
http://arxiv.org/abs/1806.05096v2,Introducing user-prescribed constraints in Markov chains for nonlinear dimensionality reduction,"Stochastic kernel based dimensionality reduction approaches have become
popular in the last decade. The central component of many of these methods is a
symmetric kernel that quantifies the vicinity between pairs of data points and
a kernel-induced Markov chain on the data. Typically, the Markov chain is fully
specified by the kernel through row normalization. However, in many cases, it
is desirable to impose user-specified stationary-state and dynamical
constraints on the Markov chain. Unfortunately, no systematic framework exists
to impose such user-defined constraints. Here, we introduce a path entropy
maximization based approach to derive the transition probabilities of Markov
chains using a kernel and additional user-specified constraints. We illustrate
the usefulness of these Markov chains with examples.",['Purushottam D. Dixit'],"['cs.LG', 'stat.ML']",2018-06-13 14:58:49+00:00
http://arxiv.org/abs/1806.05085v2,"Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in Ratings","Cardinal scores (numeric ratings) collected from people are well known to
suffer from miscalibrations. A popular approach to address this issue is to
assume simplistic models of miscalibration (such as linear biases) to de-bias
the scores. This approach, however, often fares poorly because people's
miscalibrations are typically far more complex and not well understood. In the
absence of simplifying assumptions on the miscalibration, it is widely believed
by the crowdsourcing community that the only useful information in the cardinal
scores is the induced ranking. In this paper, inspired by the framework of
Stein's shrinkage, empirical Bayes, and the classic two-envelope problem, we
contest this widespread belief. Specifically, we consider cardinal scores with
arbitrary (or even adversarially chosen) miscalibrations which are only
required to be consistent with the induced ranking. We design estimators which
despite making no assumptions on the miscalibration, strictly and uniformly
outperform all possible estimators that rely on only the ranking. Our
estimators are flexible in that they can be used as a plug-in for a variety of
applications, and we provide a proof-of-concept for A/B testing and ranking.
Our results thus provide novel insights in the eternal debate between cardinal
and ordinal data.","['Jingyan Wang', 'Nihar B. Shah']","['stat.ML', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT']",2018-06-13 14:28:41+00:00
http://arxiv.org/abs/1806.05049v2,MAP inference via Block-Coordinate Frank-Wolfe Algorithm,"We present a new proximal bundle method for Maximum-A-Posteriori (MAP)
inference in structured energy minimization problems. The method optimizes a
Lagrangean relaxation of the original energy minimization problem using a multi
plane block-coordinate Frank-Wolfe method that takes advantage of the specific
structure of the Lagrangean decomposition. We show empirically that our method
outperforms state-of-the-art Lagrangean decomposition based algorithms on some
challenging Markov Random Field, multi-label discrete tomography and graph
matching problems.","['Paul Swoboda', 'Vladimir Kolmogorov']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-13 14:03:38+00:00
http://arxiv.org/abs/1806.05559v2,Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation,"Parallel sentence extraction is a task addressing the data sparsity problem
found in multilingual natural language processing applications. We propose a
bidirectional recurrent neural network based approach to extract parallel
sentences from collections of multilingual texts. Our experiments with noisy
parallel corpora show that we can achieve promising results against a
competitive baseline by removing the need of specific feature engineering or
additional external resources. To justify the utility of our approach, we
extract sentence pairs from Wikipedia articles to train machine translation
systems and show significant improvements in translation performance.","['Francis Grégoire', 'Philippe Langlais']","['cs.CL', 'cs.LG', 'stat.ML']",2018-06-13 13:57:13+00:00
http://arxiv.org/abs/1806.05034v4,A Probabilistic U-Net for Segmentation of Ambiguous Images,"Many real-world vision problems suffer from inherent ambiguities. In clinical
applications for example, it might not be clear from a CT scan alone which
particular region is cancer tissue. Therefore a group of graders typically
produces a set of diverse but plausible segmentations. We consider the task of
learning a distribution over segmentations given an input. To this end we
propose a generative segmentation model based on a combination of a U-Net with
a conditional variational autoencoder that is capable of efficiently producing
an unlimited number of plausible hypotheses. We show on a lung abnormalities
segmentation task and on a Cityscapes segmentation task that our model
reproduces the possible segmentation variants as well as the frequencies with
which they occur, doing so significantly better than published approaches.
These models could have a high impact in real-world applications, such as being
used as clinical decision-making algorithms accounting for multiple plausible
semantic segmentation hypotheses to provide possible diagnoses and recommend
further actions to resolve the present ambiguities.","['Simon A. A. Kohl', 'Bernardino Romera-Paredes', 'Clemens Meyer', 'Jeffrey De Fauw', 'Joseph R. Ledsam', 'Klaus H. Maier-Hein', 'S. M. Ali Eslami', 'Danilo Jimenez Rezende', 'Olaf Ronneberger']","['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']",2018-06-13 13:47:04+00:00
http://arxiv.org/abs/1806.05017v2,Brain-Computer Interface with Corrupted EEG Data: A Tensor Completion Approach,"One of the current issues in Brain-Computer Interface is how to deal with
noisy Electroencephalography measurements organized as multidimensional
datasets. On the other hand, recently, significant advances have been made in
multidimensional signal completion algorithms that exploit tensor decomposition
models to capture the intricate relationship among entries in a
multidimensional signal. We propose to use tensor completion applied to EEG
data for improving the classification performance in a motor imagery BCI system
with corrupted measurements. Noisy measurements are considered as unknowns that
are inferred from a tensor decomposition model. We evaluate the performance of
four recently proposed tensor completion algorithms plus a simple interpolation
strategy, first with random missing entries and then with missing samples
constrained to have a specific structure (random missing channels), which is a
more realistic assumption in BCI Applications. We measured the ability of these
algorithms to reconstruct the tensor from observed data. Then, we tested the
classification accuracy of imagined movement in a BCI experiment with missing
samples. We show that for random missing entries, all tensor completion
algorithms can recover missing samples increasing the classification
performance compared to a simple interpolation approach. For the random missing
channels case, we show that tensor completion algorithms help to reconstruct
missing channels, significantly improving the accuracy in the classification of
motor imagery, however, not at the same level as clean data. Tensor completion
algorithms are useful in real BCI applications. The proposed strategy could
allow using motor imagery BCI systems even when EEG data is highly affected by
missing channels and/or samples, avoiding the need of new acquisitions in the
calibration stage.","['Jordi Sole-Casals', 'Cesar F. Caiafa', 'Qibin Zhao', 'Adrzej Cichocki']","['q-bio.QM', 'eess.SP', 'stat.ML']",2018-06-13 13:16:28+00:00
http://arxiv.org/abs/1806.05009v3,Tree Edit Distance Learning via Adaptive Symbol Embeddings,"Metric learning has the aim to improve classification accuracy by learning a
distance measure which brings data points from the same class closer together
and pushes data points from different classes further apart. Recent research
has demonstrated that metric learning approaches can also be applied to trees,
such as molecular structures, abstract syntax trees of computer programs, or
syntax trees of natural language, by learning the cost function of an edit
distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.
However, learning such costs directly may yield an edit distance which violates
metric axioms, is challenging to interpret, and may not generalize well. In
this contribution, we propose a novel metric learning approach for trees which
we call embedding edit distance learning (BEDL) and which learns an edit
distance indirectly by embedding the tree nodes as vectors, such that the
Euclidean distance between those vectors supports class discrimination. We
learn such embeddings by reducing the distance to prototypical trees from the
same class and increasing the distance to prototypical trees from different
classes. In our experiments, we show that BEDL improves upon the
state-of-the-art in metric learning for trees on six benchmark data sets,
ranging from computer science over biomedical data to a natural-language
processing data set containing over 300,000 nodes.","['Benjamin Paaßen', 'Claudio Gallicchio', 'Alessio Micheli', 'Barbara Hammer']","['cs.LG', 'stat.ML']",2018-06-13 13:08:16+00:00
http://arxiv.org/abs/1806.05178v1,Generating Sentences Using a Dynamic Canvas,"We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word
level generative model for natural language. It uses a recurrent neural network
with a dynamic attention and canvas memory mechanism to iteratively construct
sentences. By viewing the state of the memory at intermediate stages and where
the model is placing its attention, we gain insight into how it constructs
sentences. We demonstrate that AUTR learns a meaningful latent representation
for each sentence, and achieves competitive log-likelihood lower bounds whilst
being computationally efficient. It is effective at generating and
reconstructing sentences, as well as imputing missing words.","['Harshil Shah', 'Bowen Zheng', 'David Barber']","['cs.CL', 'cs.LG', 'stat.ML']",2018-06-13 12:57:19+00:00
http://arxiv.org/abs/1806.04994v3,Only Bayes should learn a manifold (on the estimation of differential geometric structure from data),"We investigate learning of the differential geometric structure of a data
manifold embedded in a high-dimensional Euclidean space. We first analyze
kernel-based algorithms and show that under the usual regularizations,
non-probabilistic methods cannot recover the differential geometric structure,
but instead find mostly linear manifolds or spaces equipped with teleports. To
properly learn the differential geometric structure, non-probabilistic methods
must apply regularizations that enforce large gradients, which go against
common wisdom. We repeat the analysis for probabilistic methods and find that
under reasonable priors, the geometric structure can be recovered. Fully
exploiting the recovered structure, however, requires the development of
stochastic extensions to classic Riemannian geometry. We take early steps in
that regard. Finally, we partly extend the analysis to modern models based on
neural networks, thereby highlighting geometric and probabilistic shortcomings
of current deep generative models.",['Søren Hauberg'],"['stat.ML', 'cs.LG']",2018-06-13 12:50:33+00:00
http://arxiv.org/abs/1806.04965v2,The streaming rollout of deep networks - towards fully model-parallel execution,"Deep neural networks, and in particular recurrent networks, are promising
candidates to control autonomous agents that interact in real-time with the
physical world. However, this requires a seamless integration of temporal
features into the network's architecture. For the training of and inference
with recurrent neural networks, they are usually rolled out over time, and
different rollouts exist. Conventionally during inference, the layers of a
network are computed in a sequential manner resulting in sparse temporal
integration of information and long response times. In this study, we present a
theoretical framework to describe rollouts, the level of model-parallelization
they induce, and demonstrate differences in solving specific tasks. We prove
that certain rollouts, also for networks with only skip and no recurrent
connections, enable earlier and more frequent responses, and show empirically
that these early responses have better performance. The streaming rollout
maximizes these properties and enables a fully parallel execution of the
network reducing runtime on massively parallel devices. Finally, we provide an
open-source toolbox to design, train, evaluate, and interact with streaming
rollouts.","['Volker Fischer', 'Jan Köhler', 'Thomas Pfeil']","['stat.ML', 'cs.LG']",2018-06-13 11:53:23+00:00
http://arxiv.org/abs/1806.04941v1,Far-HO: A Bilevel Programming Package for Hyperparameter Optimization and Meta-Learning,"In (Franceschi et al., 2018) we proposed a unified mathematical framework,
grounded on bilevel programming, that encompasses gradient-based hyperparameter
optimization and meta-learning. We formulated an approximate version of the
problem where the inner objective is solved iteratively, and gave sufficient
conditions ensuring convergence to the exact problem. In this work we show how
to optimize learning rates, automatically weight the loss of single examples
and learn hyper-representations with Far-HO, a software package based on the
popular deep learning framework TensorFlow that allows to seamlessly tackle
both HO and ML problems.","['Luca Franceschi', 'Riccardo Grazzi', 'Massimiliano Pontil', 'Saverio Salzo', 'Paolo Frasconi']","['cs.MS', 'cs.LG', 'stat.ML']",2018-06-13 10:46:32+00:00
http://arxiv.org/abs/1806.04931v1,An image representation based convolutional network for DNA classification,"The folding structure of the DNA molecule combined with helper molecules,
also referred to as the chromatin, is highly relevant for the functional
properties of DNA. The chromatin structure is largely determined by the
underlying primary DNA sequence, though the interaction is not yet fully
understood. In this paper we develop a convolutional neural network that takes
an image-representation of primary DNA sequence as its input, and predicts key
determinants of chromatin structure. The method is developed such that it is
capable of detecting interactions between distal elements in the DNA sequence,
which are known to be highly relevant. Our experiments show that the method
outperforms several existing methods both in terms of prediction accuracy and
training time.","['Bojian Yin', 'Marleen Balvert', 'Davide Zambrano', 'Alexander Schönhuth', 'Sander Bohte']","['cs.LG', 'stat.ML']",2018-06-13 10:27:44+00:00
http://arxiv.org/abs/1806.04910v2,Bilevel Programming for Hyperparameter Optimization and Meta-Learning,"We introduce a framework based on bilevel programming that unifies
gradient-based hyperparameter optimization and meta-learning. We show that an
approximate version of the bilevel problem can be solved by taking into
explicit account the optimization dynamics for the inner objective. Depending
on the specific setting, the outer variables take either the meaning of
hyperparameters in a supervised learning problem or parameters of a
meta-learner. We provide sufficient conditions under which solutions of the
approximate problem converge to those of the exact problem. We instantiate our
approach for meta-learning in the case of deep learning where representation
layers are treated as hyperparameters shared across a set of training episodes.
In experiments, we confirm our theoretical findings, present encouraging
results for few-shot learning and contrast the bilevel approach against
classical approaches for learning-to-learn.","['Luca Franceschi', 'Paolo Frasconi', 'Saverio Salzo', 'Riccardo Grazzi', 'Massimilano Pontil']","['stat.ML', 'cs.LG']",2018-06-13 09:21:42+00:00
http://arxiv.org/abs/1806.04900v2,A Machine-Learning Item Recommendation System for Video Games,"Video-game players generate huge amounts of data, as everything they do
within a game is recorded. In particular, among all the stored actions and
behaviors, there is information on the in-game purchases of virtual products.
Such information is of critical importance in modern free-to-play titles, where
gamers can select or buy a profusion of items during the game in order to
progress and fully enjoy their experience.
  To try to maximize these kind of purchases, one can use a recommendation
system so as to present players with items that might be interesting for them.
Such systems can better achieve their goal by employing machine learning
algorithms that are able to predict the rating of an item or product by a
particular user. In this paper we evaluate and compare two of these algorithms,
an ensemble-based model (extremely randomized trees) and a deep neural network,
both of which are promising candidates for operational video-game recommender
engines.
  Item recommenders can help developers improve the game. But, more
importantly, it should be possible to integrate them into the game, so that
users automatically get personalized recommendations while playing. The
presented models are not only able to meet this challenge, providing accurate
predictions of the items that a particular player will find attractive, but
also sufficiently fast and robust to be used in operational settings.","['Paul Bertens', 'Anna Guitart', 'Pei Pei Chen', 'África Periáñez']","['stat.ML', 'cs.LG']",2018-06-13 09:00:36+00:00
http://arxiv.org/abs/1806.04899v3,Ensemble Pruning based on Objection Maximization with a General Distributed Framework,"Ensemble pruning, selecting a subset of individual learners from an original
ensemble, alleviates the deficiencies of ensemble learning on the cost of time
and space. Accuracy and diversity serve as two crucial factors while they
usually conflict with each other. To balance both of them, we formalize the
ensemble pruning problem as an objection maximization problem based on
information entropy. Then we propose an ensemble pruning method including a
centralized version and a distributed version, in which the latter is to speed
up the former. At last, we extract a general distributed framework for ensemble
pruning, which can be widely suitable for most of the existing ensemble pruning
methods and achieve less time consuming without much accuracy degradation.
Experimental results validate the efficiency of our framework and methods,
particularly concerning a remarkable improvement of the execution speed,
accompanied by gratifying accuracy performance.","['Yijun Bian', 'Yijun Wang', 'Yaqiang Yao', 'Huanhuan Chen']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-13 08:58:49+00:00
http://arxiv.org/abs/1806.08647v2,Matrix Completion and Performance Guarantees for Single Individual Haplotyping,"Single individual haplotyping is an NP-hard problem that emerges when
attempting to reconstruct an organism's inherited genetic variations using data
typically generated by high-throughput DNA sequencing platforms. Genomes of
diploid organisms, including humans, are organized into homologous pairs of
chromosomes that differ from each other in a relatively small number of variant
positions. Haplotypes are ordered sequences of the nucleotides in the variant
positions of the chromosomes in a homologous pair; for diploids, haplotypes
associated with a pair of chromosomes may be conveniently represented by means
of complementary binary sequences. In this paper, we consider a binary matrix
factorization formulation of the single individual haplotyping problem and
efficiently solve it by means of alternating minimization. We analyze the
convergence properties of the alternating minimization algorithm and establish
theoretical bounds for the achievable haplotype reconstruction error. The
proposed technique is shown to outperform existing methods when applied to
synthetic as well as real-world Fosmid-based HapMap NA12878 datasets.","['Somsubhra Barik', 'Haris Vikalo']","['cs.LG', 'cs.IT', 'math.IT', 'q-bio.QM', 'stat.ML']",2018-06-13 08:50:25+00:00
http://arxiv.org/abs/1806.04884v3,Spurious Local Minima of Deep ReLU Neural Networks in the Neural Tangent Kernel Regime,"In this paper, we theoretically prove that the deep ReLU neural networks do
not lie in spurious local minima in the loss landscape under the Neural Tangent
Kernel (NTK) regime, that is, in the gradient descent training dynamics of the
deep ReLU neural networks whose parameters are initialized by a normal
distribution in the limit as the widths of the hidden layers tend to infinity.",['Tohru Nitta'],"['stat.ML', 'cs.LG']",2018-06-13 08:18:00+00:00
http://arxiv.org/abs/1806.04863v1,Cell Identity Codes: Understanding Cell Identity from Gene Expression Profiles using Deep Neural Networks,"Understanding cell identity is an important task in many biomedical areas.
Expression patterns of specific marker genes have been used to characterize
some limited cell types, but exclusive markers are not available for many cell
types. A second approach is to use machine learning to discriminate cell types
based on the whole gene expression profiles (GEPs). The accuracies of simple
classification algorithms such as linear discriminators or support vector
machines are limited due to the complexity of biological systems. We used deep
neural networks to analyze 1040 GEPs from 16 different human tissues and cell
types. After comparing different architectures, we identified a specific
structure of deep autoencoders that can encode a GEP into a vector of 30
numeric values, which we call the cell identity code (CIC). The original GEP
can be reproduced from the CIC with an accuracy comparable to technical
replicates of the same experiment. Although we use an unsupervised approach to
train the autoencoder, we show different values of the CIC are connected to
different biological aspects of the cell, such as different pathways or
biological processes. This network can use CIC to reproduce the GEP of the cell
types it has never seen during the training. It also can resist some noise in
the measurement of the GEP. Furthermore, we introduce classifier autoencoder,
an architecture that can accurately identify cell type based on the GEP or the
CIC.","['Farzad Abdolhosseini', 'Behrooz Azarkhalili', 'Abbas Maazallahi', 'Aryan Kamal', 'Seyed Abolfazl Motahari', 'Ali Sharifi-Zarchi', 'Hamidreza Chitsaz']","['q-bio.GN', 'cs.LG', 'stat.ML']",2018-06-13 06:42:44+00:00
http://arxiv.org/abs/1806.06913v1,Deep Learning based Estimation of Weaving Target Maneuvers,"In target tracking, the estimation of an unknown weaving target frequency is
crucial for improving the miss distance. The estimation process is commonly
carried out in a Kalman framework. The objective of this paper is to examine
the potential of using neural networks in target tracking applications. To that
end, we propose estimating the weaving frequency using deep neural networks,
instead of classical Kalman framework based estimation. Particularly, we focus
on the case where a set of possible constant target frequencies is known.
Several neural network architectures, requiring low computational resources
were designed to estimate the unknown frequency out of the known set of
frequencies. The proposed approach performance is compared with the multiple
model adaptive estimation algorithm. Simulation results show that in the
examined scenarios, deep neural network outperforms multiple model adaptive
estimation in terms of accuracy and the amount of required measurements to
convergence.","['Vitaly Shalumov', 'Itzik Klein']","['cs.LG', 'stat.ML']",2018-06-13 06:16:14+00:00
http://arxiv.org/abs/1806.04854v3,Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam,"Uncertainty computation in deep learning is essential to design robust and
reliable systems. Variational inference (VI) is a promising approach for such
computation, but requires more effort to implement and execute compared to
maximum-likelihood methods. In this paper, we propose new natural-gradient
algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms
can be implemented within the Adam optimizer by perturbing the network weights
during gradient evaluations, and uncertainty estimates can be cheaply obtained
by using the vector that adapts the learning rate. This requires lower memory,
computation, and implementation effort than existing VI methods, while
obtaining uncertainty estimates of comparable quality. Our empirical results
confirm this and further suggest that the weight-perturbation in our algorithm
could be useful for exploration in reinforcement learning and stochastic
optimization.","['Mohammad Emtiyaz Khan', 'Didrik Nielsen', 'Voot Tangkaratt', 'Wu Lin', 'Yarin Gal', 'Akash Srivastava']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.CO']",2018-06-13 05:45:22+00:00
http://arxiv.org/abs/1806.06850v3,Polynomial Regression As an Alternative to Neural Nets,"Despite the success of neural networks (NNs), there is still a concern among
many over their ""black box"" nature. Why do they work? Here we present a simple
analytic argument that NNs are in fact essentially polynomial regression
models. This view will have various implications for NNs, e.g. providing an
explanation for why convergence problems arise in NNs, and it gives rough
guidance on avoiding overfitting. In addition, we use this phenomenon to
predict and confirm a multicollinearity property of NNs not previously reported
in the literature. Most importantly, given this loose correspondence, one may
choose to routinely use polynomial models instead of NNs, thus avoiding some
major problems of the latter, such as having to set many tuning parameters and
dealing with convergence issues. We present a number of empirical results; in
each case, the accuracy of the polynomial approach matches or exceeds that of
NN approaches. A many-featured, open-source software package, polyreg, is
available.","['Xi Cheng', 'Bohdan Khomtchouk', 'Norman Matloff', 'Pete Mohanty']","['cs.LG', 'stat.ML']",2018-06-13 05:06:43+00:00
http://arxiv.org/abs/1806.06931v1,Reinforcement Learning with Function-Valued Action Spaces for Partial Differential Equation Control,"Recent work has shown that reinforcement learning (RL) is a promising
approach to control dynamical systems described by partial differential
equations (PDE). This paper shows how to use RL to tackle more general PDE
control problems that have continuous high-dimensional action spaces with
spatial relationship among action dimensions. In particular, we propose the
concept of action descriptors, which encode regularities among
spatially-extended action dimensions and enable the agent to control
high-dimensional action PDEs. We provide theoretical evidence suggesting that
this approach can be more sample efficient compared to a conventional approach
that treats each action dimension separately and does not explicitly exploit
the spatial regularity of the action space. The action descriptor approach is
then used within the deep deterministic policy gradient algorithm. Experiments
on two PDE control problems, with up to 256-dimensional continuous actions,
show the advantage of the proposed approach over the conventional one.","['Yangchen Pan', 'Amir-massoud Farahmand', 'Martha White', 'Saleh Nabi', 'Piyush Grover', 'Daniel Nikovski']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-13 03:47:12+00:00
http://arxiv.org/abs/1806.04838v1,Partial AUC Maximization via Nonlinear Scoring Functions,"We propose a method for maximizing a partial area under a receiver operating
characteristic (ROC) curve (pAUC) for binary classification tasks. In binary
classification tasks, accuracy is the most commonly used as a measure of
classifier performance. In some applications such as anomaly detection and
diagnostic testing, accuracy is not an appropriate measure since prior
probabilties are often greatly biased. Although in such cases the pAUC has been
utilized as a performance measure, few methods have been proposed for directly
maximizing the pAUC. This optimization is achieved by using a scoring function.
The conventional approach utilizes a linear function as the scoring function.
In contrast we newly introduce nonlinear scoring functions for this purpose.
Specifically, we present two types of nonlinear scoring functions based on
generative models and deep neural networks. We show experimentally that
nonlinear scoring fucntions improve the conventional methods through the
application of a binary classification of real and bogus objects obtained with
the Hyper Suprime-Cam on the Subaru telescope.","['Naonori Ueda', 'Akinori Fujino']","['stat.ML', 'cs.LG']",2018-06-13 03:26:27+00:00
http://arxiv.org/abs/1806.04823v8,Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models,"This paper proposes a Lasso-type estimator for a high-dimensional sparse
parameter identified by a single index conditional moment restriction (CMR). In
addition to this parameter, the moment function can also depend on a nuisance
function, such as the propensity score or the conditional choice probability,
which we estimate by modern machine learning tools. We first adjust the moment
function so that the gradient of the future loss function is insensitive
(formally, Neyman-orthogonal) with respect to the first-stage regularization
bias, preserving the single index property. We then take the loss function to
be an indefinite integral of the adjusted moment function with respect to the
single index. The proposed Lasso estimator converges at the oracle rate, where
the oracle knows the nuisance function and solves only the parametric problem.
We demonstrate our method by estimating the short-term heterogeneous impact of
Connecticut's Jobs First welfare reform experiment on women's welfare
participation decision.","['Denis Nekipelov', 'Vira Semenova', 'Vasilis Syrgkanis']","['math.ST', 'cs.LG', 'econ.EM', 'stat.ML', 'stat.TH']",2018-06-13 02:22:51+00:00
http://arxiv.org/abs/1806.04819v5,Integral Privacy for Sampling,"Differential privacy is a leading protection setting, focused by design on
individual privacy. Many applications, in medical / pharmaceutical domains or
social networks, rather posit privacy at a group level, a setting we call
integral privacy. We aim for the strongest form of privacy: the group size is
in particular not known in advance. We study a problem with related
applications in domains cited above that have recently met with substantial
recent press: sampling.
  Keeping correct utility levels in such a strong model of statistical
indistinguishability looks difficult to be achieved with the usual differential
privacy toolbox because it would typically scale in the worst case the
sensitivity by the sample size and so the noise variance by up to its square.
We introduce a trick specific to sampling that bypasses the sensitivity
analysis. Privacy enforces an information theoretic barrier on approximation,
and we show how to reach this barrier with guarantees on the approximation of
the target non private density. We do so using a recent approach to non private
density estimation relying on the original boosting theory, learning the
sufficient statistics of an exponential family with classifiers. Approximation
guarantees cover the mode capture problem. In the context of learning, the
sampling problem is particularly important: because integral privacy enjoys the
same closure under post-processing as differential privacy does, any algorithm
using integrally privacy sampled data would result in an output equally
integrally private. We also show that this brings fairness guarantees on
post-processing that would eventually elude classical differential privacy: any
decision process has bounded data-dependent bias when the data is integrally
privately sampled. Experimental results against private kernel density
estimation and private GANs displays the quality of our results.","['Hisham Husain', 'Zac Cranko', 'Richard Nock']","['stat.ML', 'cs.LG']",2018-06-13 02:01:22+00:00
http://arxiv.org/abs/1806.04808v1,Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection,"Learning expressive low-dimensional representations of ultrahigh-dimensional
data, e.g., data with thousands/millions of features, has been a major way to
enable learning methods to address the curse of dimensionality. However,
existing unsupervised representation learning methods mainly focus on
preserving the data regularity information and learning the representations
independently of subsequent outlier detection methods, which can result in
suboptimal and unstable performance of detecting irregularities (i.e.,
outliers).
  This paper introduces a ranking model-based framework, called RAMODO, to
address this issue. RAMODO unifies representation learning and outlier
detection to learn low-dimensional representations that are tailored for a
state-of-the-art outlier detection approach - the random distance-based
approach. This customized learning yields more optimal and stable
representations for the targeted outlier detectors. Additionally, RAMODO can
leverage little labeled data as prior knowledge to learn more expressive and
application-relevant representations. We instantiate RAMODO to an efficient
method called REPEN to demonstrate the performance of RAMODO.
  Extensive empirical results on eight real-world ultrahigh dimensional data
sets show that REPEN (i) enables a random distance-based detector to obtain
significantly better AUC performance and two orders of magnitude speedup; (ii)
performs substantially better and more stably than four state-of-the-art
representation learning methods; and (iii) leverages less than 1% labeled data
to achieve up to 32% AUC improvement.","['Guansong Pang', 'Longbing Cao', 'Ling Chen', 'Huan Liu']","['cs.LG', 'cs.AI', 'cs.DB', 'stat.ML']",2018-06-13 00:53:56+00:00
http://arxiv.org/abs/1806.04798v1,Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning,"Active learning (AL) aims to enable training high performance classifiers
with low annotation cost by predicting which subset of unlabelled instances
would be most beneficial to label. The importance of AL has motivated extensive
research, proposing a wide variety of manually designed AL algorithms with
diverse theoretical and intuitive motivations. In contrast to this body of
research, we propose to treat active learning algorithm design as a
meta-learning problem and learn the best criterion from data. We model an
active learning algorithm as a deep neural network that inputs the base learner
state and the unlabelled point set and predicts the best point to annotate
next. Training this active query policy network with reinforcement learning,
produces the best non-myopic policy for a given dataset. The key challenge in
achieving a general solution to AL then becomes that of learner generalisation,
particularly across heterogeneous datasets. We propose a multi-task
dataset-embedding approach that allows dataset-agnostic active learners to be
trained. Our evaluation shows that AL algorithms trained in this way can
directly generalise across diverse problems.","['Kunkun Pang', 'Mingzhi Dong', 'Yang Wu', 'Timothy Hospedales']","['cs.LG', 'stat.ML']",2018-06-12 23:52:08+00:00
http://arxiv.org/abs/1806.04795v1,Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data,"With automobiles becoming increasingly reliant on sensors to perform various
driving tasks, it is important to encode the relevant CAN bus sensor data in a
way that captures the general state of the vehicle in a compact form. In this
paper, we develop a deep learning-based method, called Drive2Vec, for embedding
such sensor data in a low-dimensional yet actionable form. Our method is based
on stacked gated recurrent units (GRUs). It accepts a short interval of
automobile sensor data as input and computes a low-dimensional representation
of that data, which can then be used to accurately solve a range of tasks. With
this representation, we (1) predict the exact values of the sensors in the
short term (up to three seconds in the future), (2) forecast the long-term
average values of these same sensors, (3) infer additional contextual
information that is not encoded in the data, including the identity of the
driver behind the wheel, and (4) build a knowledge base that can be used to
auto-label data and identify risky states. We evaluate our approach on a
dataset collected by Audi, which equipped a fleet of test vehicles with data
loggers to store all sensor readings on 2,098 hours of driving on real roads.
We show in several experiments that our method outperforms other baselines by
up to 90%, and we further demonstrate how these embeddings of sensor data can
be used to solve a variety of real-world automotive applications.","['David Hallac', 'Suvrat Bhooshan', 'Michael Chen', 'Kacem Abida', 'Rok Sosic', 'Jure Leskovec']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-12 23:19:54+00:00
http://arxiv.org/abs/1806.04773v1,Static Malware Detection & Subterfuge: Quantifying the Robustness of Machine Learning and Current Anti-Virus,"As machine-learning (ML) based systems for malware detection become more
prevalent, it becomes necessary to quantify the benefits compared to the more
traditional anti-virus (AV) systems widely used today. It is not practical to
build an agreed upon test set to benchmark malware detection systems on pure
classification performance. Instead we tackle the problem by creating a new
testing methodology, where we evaluate the change in performance on a set of
known benign & malicious files as adversarial modifications are performed. The
change in performance combined with the evasion techniques then quantifies a
system's robustness against that approach. Through these experiments we are
able to show in a quantifiable way how purely ML based systems can be more
robust than AV products at detecting malware that attempts evasion through
modification, but may be slower to adapt in the face of significantly novel
attacks.","['William Fleshman', 'Edward Raff', 'Richard Zak', 'Mark McLean', 'Charles Nicholas']","['cs.CR', 'cs.LG', 'stat.ML']",2018-06-12 21:35:56+00:00
http://arxiv.org/abs/1806.04743v2,INFERNO: Inference-Aware Neural Optimisation,"Complex computer simulations are commonly required for accurate data
modelling in many scientific disciplines, making statistical inference
challenging due to the intractability of the likelihood evaluation for the
observed data. Furthermore, sometimes one is interested on inference drawn over
a subset of the generative model parameters while taking into account model
uncertainty or misspecification on the remaining nuisance parameters. In this
work, we show how non-linear summary statistics can be constructed by
minimising inference-motivated losses via stochastic gradient descent such they
provided the smallest uncertainty for the parameters of interest. As a use
case, the problem of confidence interval estimation for the mixture coefficient
in a multi-dimensional two-component mixture model (i.e. signal vs background)
is considered, where the proposed technique clearly outperforms summary
statistics based on probabilistic classification, which are a commonly used
alternative but do not account for the presence of nuisance parameters.","['Pablo de Castro', 'Tommaso Dorigo']","['stat.ML', 'cs.LG', 'hep-ex', 'physics.data-an', 'stat.ME']",2018-06-12 20:08:53+00:00
http://arxiv.org/abs/1806.04731v3,Deep learning to represent sub-grid processes in climate models,"The representation of nonlinear sub-grid processes, especially clouds, has
been a major source of uncertainty in climate models for decades.
Cloud-resolving models better represent many of these processes and can now be
run globally but only for short-term simulations of at most a few years because
of computational limitations. Here we demonstrate that deep learning can be
used to capture many advantages of cloud-resolving modeling at a fraction of
the computational cost. We train a deep neural network to represent all
atmospheric sub-grid processes in a climate model by learning from a
multi-scale model in which convection is treated explicitly. The trained neural
network then replaces the traditional sub-grid parameterizations in a global
general circulation model in which it freely interacts with the resolved
dynamics and the surface-flux scheme. The prognostic multi-year simulations are
stable and closely reproduce not only the mean climate of the cloud-resolving
simulation but also key aspects of variability, including precipitation
extremes and the equatorial wave spectrum. Furthermore, the neural network
approximately conserves energy despite not being explicitly instructed to.
Finally, we show that the neural network parameterization generalizes to new
surface forcing patterns but struggles to cope with temperatures far outside
its training manifold. Our results show the feasibility of using deep learning
for climate model parameterization. In a broader context, we anticipate that
data-driven Earth System Model development could play a key role in reducing
climate prediction uncertainty in the coming decade.","['Stephan Rasp', 'Michael S. Pritchard', 'Pierre Gentine']","['physics.ao-ph', 'cs.LG', 'stat.ML']",2018-06-12 19:29:25+00:00
http://arxiv.org/abs/1806.04655v2,FigureNet: A Deep Learning model for Question-Answering on Scientific Plots,"Deep Learning has managed to push boundaries in a wide variety of tasks. One
area of interest is to tackle problems in reasoning and understanding, with an
aim to emulate human intelligence. In this work, we describe a deep learning
model that addresses the reasoning task of question-answering on categorical
plots. We introduce a novel architecture FigureNet, that learns to identify
various plot elements, quantify the represented values and determine a relative
ordering of these statistical values. We test our model on the FigureQA dataset
which provides images and accompanying questions for scientific plots like bar
graphs and pie charts, augmented with rich annotations. Our approach
outperforms the state-of-the-art Relation Networks baseline by approximately
$7\%$ on this dataset, with a training time that is over an order of magnitude
lesser.","['Revanth Reddy', 'Rahul Ramesh', 'Ameet Deshpande', 'Mitesh M. Khapra']","['cs.LG', 'stat.ML']",2018-06-12 17:31:23+00:00
http://arxiv.org/abs/1806.04642v4,Accelerating Imitation Learning with Predictive Models,"Sample efficiency is critical in solving real-world reinforcement learning
problems, where agent-environment interactions can be costly. Imitation
learning from expert advice has proved to be an effective strategy for reducing
the number of interactions required to train a policy. Online imitation
learning, which interleaves policy evaluation and policy optimization, is a
particularly effective technique with provable performance guarantees. In this
work, we seek to further accelerate the convergence rate of online imitation
learning, thereby making it more sample efficient. We propose two model-based
algorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based
on solving variational inequalities and MoBIL-Prox based on stochastic
first-order updates. These two methods leverage a model to predict future
gradients to speed up policy learning. When the model oracle is learned online,
these algorithms can provably accelerate the best known convergence rate up to
an order. Our algorithms can be viewed as a generalization of stochastic
Mirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style
analysis of performance.","['Ching-An Cheng', 'Xinyan Yan', 'Evangelos A. Theodorou', 'Byron Boots']","['cs.LG', 'stat.ML']",2018-06-12 16:55:06+00:00
http://arxiv.org/abs/1806.04640v3,Unsupervised Meta-Learning for Reinforcement Learning,"Meta-learning algorithms use past experience to learn to quickly solve new
tasks. In the context of reinforcement learning, meta-learning algorithms
acquire reinforcement learning procedures to solve new problems more
efficiently by utilizing experience from prior tasks. The performance of
meta-learning algorithms depends on the tasks available for meta-training: in
the same way that supervised learning generalizes best to test points drawn
from the same distribution as the training points, meta-learning methods
generalize best to tasks from the same distribution as the meta-training tasks.
In effect, meta-reinforcement learning offloads the design burden from
algorithm design to task design. If we can automate the process of task design
as well, we can devise a meta-learning algorithm that is truly automated. In
this work, we take a step in this direction, proposing a family of unsupervised
meta-learning algorithms for reinforcement learning. We motivate and describe a
general recipe for unsupervised meta-reinforcement learning, and present an
instantiation of this approach. Our conceptual and theoretical contributions
consist of formulating the unsupervised meta-reinforcement learning problem and
describing how task proposals based on mutual information can be used to train
optimal meta-learners. Our experimental results indicate that unsupervised
meta-reinforcement learning effectively acquires accelerated reinforcement
learning procedures without the need for manual task design and these
procedures exceed the performance of learning from scratch.","['Abhishek Gupta', 'Benjamin Eysenbach', 'Chelsea Finn', 'Sergey Levine']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-12 16:48:52+00:00
http://arxiv.org/abs/1806.04613v1,Improving Regression Performance with Distributional Losses,"There is growing evidence that converting targets to soft targets in
supervised learning can provide considerable gains in performance. Much of this
work has considered classification, converting hard zero-one values to soft
labels---such as by adding label noise, incorporating label ambiguity or using
distillation. In parallel, there is some evidence from a regression setting in
reinforcement learning that learning distributions can improve performance. In
this work, we investigate the reasons for this improvement, in a regression
setting. We introduce a novel distributional regression loss, and similarly
find it significantly improves prediction accuracy. We investigate several
common hypotheses, around reducing overfitting and improved representations. We
instead find evidence for an alternative hypothesis: this loss is easier to
optimize, with better behaved gradients, resulting in improved generalization.
We provide theoretical support for this alternative hypothesis, by
characterizing the norm of the gradients of this loss.","['Ehsan Imani', 'Martha White']","['stat.ML', 'cs.LG']",2018-06-12 15:42:25+00:00
http://arxiv.org/abs/1806.04610v1,A Novel Bayesian Approach for Latent Variable Modeling from Mixed Data with Missing Values,"We consider the problem of learning parameters of latent variable models from
mixed (continuous and ordinal) data with missing values. We propose a novel
Bayesian Gaussian copula factor (BGCF) approach that is consistent under
certain conditions and that is quite robust to the violations of these
conditions. In simulations, BGCF substantially outperforms two state-of-the-art
alternative approaches. An illustration on the `Holzinger & Swineford 1939'
dataset indicates that BGCF is favorable over the so-called robust maximum
likelihood (MLR) even if the data match the assumptions of MLR.","['Ruifei Cui', 'Ioan Gabriel Bucur', 'Perry Groot', 'Tom Heskes']","['stat.ML', 'cs.LG']",2018-06-12 15:38:10+00:00
http://arxiv.org/abs/1806.04609v1,Streaming PCA and Subspace Tracking: The Missing Data Case,"For many modern applications in science and engineering, data are collected
in a streaming fashion carrying time-varying information, and practitioners
need to process them with a limited amount of memory and computational
resources in a timely manner for decision making. This often is coupled with
the missing data problem, such that only a small fraction of data attributes
are observed. These complications impose significant, and unconventional,
constraints on the problem of streaming Principal Component Analysis (PCA) and
subspace tracking, which is an essential building block for many inference
tasks in signal processing and machine learning. This survey article reviews a
variety of classical and recent algorithms for solving this problem with low
computational and memory complexities, particularly those applicable in the big
data regime with missing data. We illustrate that streaming PCA and subspace
tracking algorithms can be understood through algebraic and geometric
perspectives, and they need to be adjusted carefully to handle missing data.
Both asymptotic and non-asymptotic convergence guarantees are reviewed.
Finally, we benchmark the performance of several competitive algorithms in the
presence of missing data for both well-conditioned and ill-conditioned systems.","['Laura Balzano', 'Yuejie Chi', 'Yue M. Lu']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2018-06-12 15:32:17+00:00
http://arxiv.org/abs/1806.04594v5,Exponential Weights on the Hypercube in Polynomial Time,"We study a general online linear optimization problem(OLO). At each round, a
subset of objects from a fixed universe of $n$ objects is chosen, and a linear
cost associated with the chosen subset is incurred. To measure the performance
of our algorithms, we use the notion of regret which is the difference between
the total cost incurred over all iterations and the cost of the best fixed
subset in hindsight. We consider Full Information and Bandit feedback for this
problem. This problem is equivalent to OLO on the $\{0,1\}^n$ hypercube. The
Exp2 algorithm and its bandit variant are commonly used strategies for this
problem. It was previously unknown if it is possible to run Exp2 on the
hypercube in polynomial time.
  In this paper, we present a polynomial time algorithm called PolyExp for OLO
on the hypercube. We show that our algorithm is equivalent Exp2 on $\{0,1\}^n$,
Online Mirror Descent(OMD), Follow The Regularized Leader(FTRL) and Follow The
Perturbed Leader(FTPL) algorithms. We show PolyExp achieves expected regret
bound that is a factor of $\sqrt{n}$ better than Exp2 in the full information
setting under $L_\infty$ adversarial losses. Because of the equivalence of
these algorithms, this implies an improvement on Exp2's regret bound in full
information. We also show matching regret lower bounds. Finally, we show how to
use PolyExp on the $\{-1,+1\}^n$ hypercube, solving an open problem in Bubeck
et al (COLT 2012).","['Sudeep Raja Putta', 'Abhishek Shetty']","['stat.ML', 'cs.LG']",2018-06-12 15:12:48+00:00
http://arxiv.org/abs/1806.04577v1,Using Inherent Structures to design Lean 2-layer RBMs,"Understanding the representational power of Restricted Boltzmann Machines
(RBMs) with multiple layers is an ill-understood problem and is an area of
active research. Motivated from the approach of \emph{Inherent Structure
formalism} (Stillinger & Weber, 1982), extensively used in analysing Spin
Glasses, we propose a novel measure called \emph{Inherent Structure Capacity}
(ISC), which characterizes the representation capacity of a fixed architecture
RBM by the expected number of modes of distributions emanating from the RBM
with parameters drawn from a prior distribution. Though ISC is intractable, we
show that for a single layer RBM architecture ISC approaches a finite constant
as number of hidden units are increased and to further improve the ISC, one
needs to add a second layer. Furthermore, we introduce \emph{Lean} RBMs, which
are multi-layer RBMs where each layer can have at-most $O(n)$ units with the
number of visible units being n. We show that for every single layer RBM with
$\Omega(n^{2+r}), r \ge 0$, hidden units there exists a two-layered \emph{lean}
RBM with $\Theta(n^2)$ parameters with the same ISC, establishing that 2 layer
RBMs can achieve the same representational power as single-layer RBMs but using
far fewer number of parameters. To the best of our knowledge, this is the first
result which quantitatively establishes the need for layering.","['Abhishek Bansal', 'Abhinav Anand', 'Chiranjib Bhattacharyya']","['stat.ML', 'cs.LG']",2018-06-12 14:55:42+00:00
http://arxiv.org/abs/1806.06915v1,A One-Sided Classification Toolkit with Applications in the Analysis of Spectroscopy Data,"This dissertation investigates the use of one-sided classification algorithms
in the application of separating hazardous chlorinated solvents from other
materials, based on their Raman spectra. The experimentation is carried out
using a new one-sided classification toolkit that was designed and developed
from the ground up. In the one-sided classification paradigm, the objective is
to separate elements of the target class from all outliers. These one-sided
classifiers are generally chosen, in practice, when there is a deficiency of
some sort in the training examples. Sometimes outlier examples can be rare,
expensive to label, or even entirely absent. However, this author would like to
note that they can be equally applicable when outlier examples are plentiful
but nonetheless not statistically representative of the complete outlier
concept. It is this scenario that is explicitly dealt with in this research
work. In these circumstances, one-sided classifiers have been found to be more
robust that conventional multi-class classifiers. The term ""unexpected""
outliers is introduced to represent outlier examples, encountered in the test
set, that have been taken from a different distribution to the training set
examples. These are examples that are a result of an inadequate representation
of all possible outliers in the training set. It can often be impossible to
fully characterise outlier examples given the fact that they can represent the
immeasurable quantity of ""everything else"" that is not a target. The findings
from this research have shown the potential drawbacks of using conventional
multi-class classification algorithms when the test data come from a completely
different distribution to that of the training samples.",['Frank G. Glavin'],"['cs.LG', 'stat.ML']",2018-06-12 14:43:21+00:00
http://arxiv.org/abs/1806.04562v2,Multi-Agent Deep Reinforcement Learning with Human Strategies,"Deep learning has enabled traditional reinforcement learning methods to deal
with high-dimensional problems. However, one of the disadvantages of deep
reinforcement learning methods is the limited exploration capacity of learning
agents. In this paper, we introduce an approach that integrates human
strategies to increase the exploration capacity of multiple deep reinforcement
learning agents. We also report the development of our own multi-agent
environment called Multiple Tank Defence to simulate the proposed approach. The
results show the significant performance improvement of multiple agents that
have learned cooperatively with human strategies. This implies that there is a
critical need for human intellect teamed with machines to solve complex
problems. In addition, the success of this simulation indicates that our
multi-agent environment can be used as a testbed platform to develop and
validate other multi-agent control algorithms.","['Thanh Nguyen', 'Ngoc Duy Nguyen', 'Saeid Nahavandi']","['cs.LG', 'cs.AI', 'stat.ML']",2018-06-12 14:40:24+00:00
http://arxiv.org/abs/1806.04561v1,An Extension of Averaged-Operator-Based Algorithms,"Many of the algorithms used to solve minimization problems with
sparsity-inducing regularizers are generic in the sense that they do not take
into account the sparsity of the solution in any particular way. However,
algorithms known as semismooth Newton are able to take advantage of this
sparsity to accelerate their convergence. We show how to extend these
algorithms in different directions, and study the convergence of the resulting
algorithms by showing that they are a particular case of an extension of the
well-known Krasnosel'ski\u{\i}--Mann scheme.","['Miguel Simões', 'José Bioucas-Dias', 'Luis B. Almeida']","['math.OC', 'cs.CV', 'eess.SP', 'stat.ML']",2018-06-12 14:35:40+00:00
http://arxiv.org/abs/1806.04555v1,Logistic Ensemble Models,"Predictive models that are developed in a regulated industry or a regulated
application, like determination of credit worthiness, must be interpretable and
rational (e.g., meaningful improvements in basic credit behavior must result in
improved credit worthiness scores). Machine Learning technologies provide very
good performance with minimal analyst intervention, making them well suited to
a high volume analytic environment, but the majority are black box tools that
provide very limited insight or interpretability into key drivers of model
performance or predicted model output values. This paper presents a methodology
that blends one of the most popular predictive statistical modeling methods for
binary classification with a core model enhancement strategy found in machine
learning. The resulting prediction methodology provides solid performance, from
minimal analyst effort, while providing the interpretability and rationality
required in regulated industries, as well as in other environments where
interpretation of model parameters is required (e.g. businesses that require
interpretation of models, to take action on them).","['Bob Vanderheyden', 'Jennifer Priestley']","['stat.ML', 'cs.LG']",2018-06-12 14:27:21+00:00
http://arxiv.org/abs/1806.04552v1,Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed Exploration,"Q-Ensembles are a model-free approach where input images are fed into
different Q-networks and exploration is driven by the assumption that
uncertainty is proportional to the variance of the output Q-values obtained.
They have been shown to perform relatively well compared to other exploration
strategies. Further, model-based approaches, such as encoder-decoder models
have been used successfully for next frame prediction given previous frames.
This paper proposes to integrate the model-free Q-ensembles and model-based
approaches with the hope of compounding the benefits of both and achieving
superior exploration as a result. Results show that a model-based trajectory
memory approach when combined with Q-ensembles produces superior performance
when compared to only using Q-ensembles.","['Sreecharan Sankaranarayanan', 'Raghuram Mandyam Annasamy', 'Katia Sycara', 'Carolyn Penstein Rosé']","['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE', 'stat.ML']",2018-06-12 14:24:02+00:00
http://arxiv.org/abs/1806.04550v2,Deep State Space Models for Unconditional Word Generation,"Autoregressive feedback is considered a necessity for successful
unconditional text generation using stochastic sequence models. However, such
feedback is known to introduce systematic biases into the training process and
it obscures a principle of generation: committing to global information and
forgetting local nuances. We show that a non-autoregressive deep state space
model with a clear separation of global and local uncertainty can be built from
only two ingredients: An independent noise source and a deterministic
transition function. Recent advances on flow-based variational inference can be
used to train an evidence lower-bound without resorting to annealing, auxiliary
losses or similar measures. The result is a highly interpretable generative
model on par with comparable auto-regressive models on the task of word
generation.","['Florian Schmidt', 'Thomas Hofmann']","['cs.LG', 'cs.CL', 'stat.ML']",2018-06-12 14:19:48+00:00
http://arxiv.org/abs/1806.04549v1,Early Seizure Detection with an Energy-Efficient Convolutional Neural Network on an Implantable Microcontroller,"Implantable, closed-loop devices for automated early detection and
stimulation of epileptic seizures are promising treatment options for patients
with severe epilepsy that cannot be treated with traditional means. Most
approaches for early seizure detection in the literature are, however, not
optimized for implementation on ultra-low power microcontrollers required for
long-term implantation. In this paper we present a convolutional neural network
for the early detection of seizures from intracranial EEG signals, designed
specifically for this purpose. In addition, we investigate approximations to
comply with hardware limits while preserving accuracy. We compare our approach
to three previously proposed convolutional neural networks and a feature-based
SVM classifier with respect to detection accuracy, latency and computational
needs. Evaluation is based on a comprehensive database with long-term EEG
recordings. The proposed method outperforms the other detectors with a median
sensitivity of 0.96, false detection rate of 10.1 per hour and median detection
delay of 3.7 seconds, while being the only approach suited to be realized on a
low power microcontroller due to its parsimonious use of computational and
memory resources.","['Maria Hügle', 'Simon Heller', 'Manuel Watter', 'Manuel Blum', 'Farrokh Manzouri', 'Matthias Dümpelmann', 'Andreas Schulze-Bonhage', 'Peter Woias', 'Joschka Boedecker']","['stat.ML', 'cs.LG', 'stat.AP']",2018-06-12 14:15:27+00:00
http://arxiv.org/abs/1806.04542v1,Approximate inference with Wasserstein gradient flows,"We present a novel approximate inference method for diffusion processes,
based on the Wasserstein gradient flow formulation of the diffusion. In this
formulation, the time-dependent density of the diffusion is derived as the
limit of implicit Euler steps that follow the gradients of a particular free
energy functional. Existing methods for computing Wasserstein gradient flows
rely on discretization of the domain of the diffusion, prohibiting their
application to domains in more than several dimensions. We propose instead a
discretization-free inference method that computes the Wasserstein gradient
flow directly in a space of continuous functions. We characterize approximation
properties of the proposed method and evaluate it on a nonlinear filtering
task, finding performance comparable to the state-of-the-art for filtering
diffusions.","['Charlie Frogner', 'Tomaso Poggio']","['stat.ML', 'cs.LG']",2018-06-12 14:08:16+00:00
