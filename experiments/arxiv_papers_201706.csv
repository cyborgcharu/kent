id,title,abstract,authors,categories,date
http://arxiv.org/abs/1707.06315v9,FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference,"A classical problem in causal inference is that of matching, where treatment
units need to be matched to control units based on covariate information. In
this work, we propose a method that computes high quality almost-exact matches
for high-dimensional categorical datasets. This method, called FLAME (Fast
Large-scale Almost Matching Exactly), learns a distance metric for matching
using a hold-out training data set. In order to perform matching efficiently
for large datasets, FLAME leverages techniques that are natural for query
processing in the area of database management, and two implementations of FLAME
are provided: the first uses SQL queries and the second uses bit-vector
techniques. The algorithm starts by constructing matches of the highest quality
(exact matches on all covariates), and successively eliminates variables in
order to match exactly on as many variables as possible, while still
maintaining interpretable high-quality matches and balance between treatment
and control groups. We leverage these high quality matches to estimate
conditional average treatment effects (CATEs). Our experiments show that FLAME
scales to huge datasets with millions of observations where existing
state-of-the-art methods fail, and that it achieves significantly better
performance than other matching methods.","['Tianyu Wang', 'Marco Morucci', 'M. Usaid Awan', 'Yameng Liu', 'Sudeepa Roy', 'Cynthia Rudin', 'Alexander Volfovsky']","['stat.ML', 'cs.DB']",2017-07-19 22:35:40+00:00
http://arxiv.org/abs/1707.06299v1,Reward-Balancing for Statistical Spoken Dialogue Systems using Multi-objective Reinforcement Learning,"Reinforcement learning is widely used for dialogue policy optimization where
the reward function often consists of more than one component, e.g., the
dialogue success and the dialogue length. In this work, we propose a structured
method for finding a good balance between these components by searching for the
optimal reward component weighting. To render this search feasible, we use
multi-objective reinforcement learning to significantly reduce the number of
training dialogues required. We apply our proposed method to find optimized
component weights for six domains and compare them to a default baseline.","['Stefan Ultes', 'Paweł Budzianowski', 'Iñigo Casanueva', 'Nikola Mrkšić', 'Lina Rojas-Barahona', 'Pei-Hao Su', 'Tsung-Hsien Wen', 'Milica Gašić', 'Steve Young']","['cs.CL', 'stat.ML']",2017-07-19 21:21:03+00:00
http://arxiv.org/abs/1707.06261v2,Non-Asymptotic Uniform Rates of Consistency for k-NN Regression,"We derive high-probability finite-sample uniform rates of consistency for
$k$-NN regression that are optimal up to logarithmic factors under mild
assumptions. We moreover show that $k$-NN regression adapts to an unknown lower
intrinsic dimension automatically. We then apply the $k$-NN regression rates to
establish new results about estimating the level sets and global maxima of a
function from noisy observations.",['Heinrich Jiang'],"['stat.ML', 'cs.LG']",2017-07-19 18:56:04+00:00
http://arxiv.org/abs/1707.06219v1,Acceleration and Averaging in Stochastic Mirror Descent Dynamics,"We formulate and study a general family of (continuous-time) stochastic
dynamics for accelerated first-order minimization of smooth convex functions.
Building on an averaging formulation of accelerated mirror descent, we propose
a stochastic variant in which the gradient is contaminated by noise, and study
the resulting stochastic differential equation. We prove a bound on the rate of
change of an energy function associated with the problem, then use it to derive
estimates of convergence rates of the function values, (a.s. and in
expectation) both for persistent and asymptotically vanishing noise. We discuss
the interaction between the parameters of the dynamics (learning rate and
averaging weights) and the covariation of the noise process, and show, in
particular, how the asymptotic rate of covariation affects the choice of
parameters and, ultimately, the convergence rate.","['Walid Krichene', 'Peter L. Bartlett']","['math.OC', 'stat.ML']",2017-07-19 17:50:53+00:00
http://arxiv.org/abs/1707.06217v1,Worst-case vs Average-case Design for Estimation from Fixed Pairwise Comparisons,"Pairwise comparison data arises in many domains, including tournament
rankings, web search, and preference elicitation. Given noisy comparisons of a
fixed subset of pairs of items, we study the problem of estimating the
underlying comparison probabilities under the assumption of strong stochastic
transitivity (SST). We also consider the noisy sorting subclass of the SST
model. We show that when the assignment of items to the topology is arbitrary,
these permutation-based models, unlike their parametric counterparts, do not
admit consistent estimation for most comparison topologies used in practice. We
then demonstrate that consistent estimation is possible when the assignment of
items to the topology is randomized, thus establishing a dichotomy between
worst-case and average-case designs. We propose two estimators in the
average-case setting and analyze their risk, showing that it depends on the
comparison topology only through the degree sequence of the topology. The rates
achieved by these estimators are shown to be optimal for a large class of
graphs. Our results are corroborated by simulations on multiple comparison
topologies.","['Ashwin Pananjady', 'Cheng Mao', 'Vidya Muthukumar', 'Martin J. Wainwright', 'Thomas A. Courtade']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",2017-07-19 17:47:05+00:00
http://arxiv.org/abs/1707.06213v2,Analysis of $p$-Laplacian Regularization in Semi-Supervised Learning,"We investigate a family of regression problems in a semi-supervised setting.
The task is to assign real-valued labels to a set of $n$ sample points,
provided a small training subset of $N$ labeled points. A goal of
semi-supervised learning is to take advantage of the (geometric) structure
provided by the large number of unlabeled data when assigning labels. We
consider random geometric graphs, with connection radius $\epsilon(n)$, to
represent the geometry of the data set. Functionals which model the task reward
the regularity of the estimator function and impose or reward the agreement
with the training data. Here we consider the discrete $p$-Laplacian
regularization.
  We investigate asymptotic behavior when the number of unlabeled points
increases, while the number of training points remains fixed. We uncover a
delicate interplay between the regularizing nature of the functionals
considered and the nonlocality inherent to the graph constructions. We
rigorously obtain almost optimal ranges on the scaling of $\epsilon(n)$ for the
asymptotic consistency to hold. We prove that the minimizers of the discrete
functionals in random setting converge uniformly to the desired continuum
limit. Furthermore we discover that for the standard model used there is a
restrictive upper bound on how quickly $\epsilon(n)$ must converge to zero as
$n \to \infty$. We introduce a new model which is as simple as the original
model, but overcomes this restriction.","['Dejan Slepčev', 'Matthew Thorpe']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH', '49J55, 49J45, 62G20, 35J20, 65N12']",2017-07-19 17:31:14+00:00
http://arxiv.org/abs/1707.06209v1,Crowdsourcing Multiple Choice Science Questions,"We present a novel method for obtaining high-quality, domain-targeted
multiple choice questions from crowd workers. Generating these questions can be
difficult without trading away originality, relevance or diversity in the
answer options. Our method addresses these problems by leveraging a large
corpus of domain-specific text and a small set of existing questions. It
produces model suggestions for document selection and answer distractor choice
which aid the human question generation process. With this method we have
assembled SciQ, a dataset of 13.7K multiple choice science exam questions
(Dataset available at http://allenai.org/data.html). We demonstrate that the
method produces in-domain questions by providing an analysis of this new
dataset and by showing that humans cannot distinguish the crowdsourced
questions from original questions. When using SciQ as additional training data
to existing questions, we observe accuracy improvements on real science exams.","['Johannes Welbl', 'Nelson F. Liu', 'Matt Gardner']","['cs.HC', 'cs.AI', 'cs.CL', 'stat.ML']",2017-07-19 17:28:46+00:00
http://arxiv.org/abs/1707.06203v2,Imagination-Augmented Agents for Deep Reinforcement Learning,"We introduce Imagination-Augmented Agents (I2As), a novel architecture for
deep reinforcement learning combining model-free and model-based aspects. In
contrast to most existing model-based reinforcement learning and planning
methods, which prescribe how a model should be used to arrive at a policy, I2As
learn to interpret predictions from a learned environment model to construct
implicit plans in arbitrary ways, by using the predictions as additional
context in deep policy networks. I2As show improved data efficiency,
performance, and robustness to model misspecification compared to several
baselines.","['Théophane Weber', 'Sébastien Racanière', 'David P. Reichert', 'Lars Buesing', 'Arthur Guez', 'Danilo Jimenez Rezende', 'Adria Puigdomènech Badia', 'Oriol Vinyals', 'Nicolas Heess', 'Yujia Li', 'Razvan Pascanu', 'Peter Battaglia', 'Demis Hassabis', 'David Silver', 'Daan Wierstra']","['cs.LG', 'cs.AI', 'stat.ML']",2017-07-19 17:12:56+00:00
http://arxiv.org/abs/1707.06197v1,Can GAN Learn Topological Features of a Graph?,"This paper is first-line research expanding GANs into graph topology
analysis. By leveraging the hierarchical connectivity structure of a graph, we
have demonstrated that generative adversarial networks (GANs) can successfully
capture topological features of any arbitrary graph, and rank edge sets by
different stages according to their contribution to topology reconstruction.
Moreover, in addition to acting as an indicator of graph reconstruction, we
find that these stages can also preserve important topological features in a
graph.","['Weiyi Liu', 'Pin-Yu Chen', 'Hal Cooper', 'Min Hwan Oh', 'Sailung Yeung', 'Toyotaro Suzumura']","['cs.LG', 'stat.ML']",2017-07-19 17:06:21+00:00
http://arxiv.org/abs/1707.06194v1,Entropy-based Pruning for Learning Bayesian Networks using BIC,"For decomposable score-based structure learning of Bayesian networks,
existing approaches first compute a collection of candidate parent sets for
each variable and then optimize over this collection by choosing one parent set
for each variable without creating directed cycles while maximizing the total
score. We target the task of constructing the collection of candidate parent
sets when the score of choice is the Bayesian Information Criterion (BIC). We
provide new non-trivial results that can be used to prune the search space of
candidate parent sets of each node. We analyze how these new results relate to
previous ideas in the literature both theoretically and empirically. We show in
experiments with UCI data sets that gains can be significant. Since the new
pruning rules are easy to implement and have low computational costs, they can
be promptly integrated into all state-of-the-art methods for structure learning
of Bayesian networks.","['Cassio P. de Campos', 'Mauro Scanagatta', 'Giorgio Corani', 'Marco Zaffalon']","['cs.AI', 'stat.ML']",2017-07-19 17:03:14+00:00
http://arxiv.org/abs/1707.06170v1,Learning model-based planning from scratch,"Conventional wisdom holds that model-based planning is a powerful approach to
sequential decision-making. It is often very challenging in practice, however,
because while a model can be used to evaluate a plan, it does not prescribe how
to construct a plan. Here we introduce the ""Imagination-based Planner"", the
first model-based, sequential decision-making agent that can learn to
construct, evaluate, and execute plans. Before any action, it can perform a
variable number of imagination steps, which involve proposing an imagined
action and evaluating it with its model-based imagination. All imagined actions
and outcomes are aggregated, iteratively, into a ""plan context"" which
conditions future real and imagined actions. The agent can even decide how to
imagine: testing out alternative imagined actions, chaining sequences of
actions together, or building a more complex ""imagination tree"" by navigating
flexibly among the previously imagined states using a learned policy. And our
agent can learn to plan economically, jointly optimizing for external rewards
and computational costs associated with using its imagination. We show that our
architecture can learn to solve a challenging continuous control problem, and
also learn elaborate planning strategies in a discrete maze-solving task. Our
work opens a new direction toward learning the components of a model-based
planning system and how to use them.","['Razvan Pascanu', 'Yujia Li', 'Oriol Vinyals', 'Nicolas Heess', 'Lars Buesing', 'Sebastien Racanière', 'David Reichert', 'Théophane Weber', 'Daan Wierstra', 'Peter Battaglia']","['cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']",2017-07-19 15:52:35+00:00
http://arxiv.org/abs/1707.06145v1,Self-paced Convolutional Neural Network for Computer Aided Detection in Medical Imaging Analysis,"Tissue characterization has long been an important component of Computer
Aided Diagnosis (CAD) systems for automatic lesion detection and further
clinical planning. Motivated by the superior performance of deep learning
methods on various computer vision problems, there has been increasing work
applying deep learning to medical image analysis. However, the development of a
robust and reliable deep learning model for computer-aided diagnosis is still
highly challenging due to the combination of the high heterogeneity in the
medical images and the relative lack of training samples. Specifically,
annotation and labeling of the medical images is much more expensive and
time-consuming than other applications and often involves manual labor from
multiple domain experts. In this work, we propose a multi-stage, self-paced
learning framework utilizing a convolutional neural network (CNN) to classify
Computed Tomography (CT) image patches. The key contribution of this approach
is that we augment the size of training samples by refining the unlabeled
instances with a self-paced learning CNN. By implementing the framework on high
performance computing servers including the NVIDIA DGX1 machine, we obtained
the experimental result, showing that the self-pace boosted network
consistently outperformed the original network even with very scarce manual
labels. The performance gain indicates that applications with limited training
samples such as medical image analysis can benefit from using the proposed
framework.","['Xiang Li', 'Aoxiao Zhong', 'Ming Lin', 'Ning Guo', 'Mu Sun', 'Arkadiusz Sitek', 'Jieping Ye', 'James Thrall', 'Quanzheng Li']","['cs.CV', 'cs.LG', 'stat.ML']",2017-07-19 15:15:36+00:00
http://arxiv.org/abs/1707.06017v1,EnzyNet: enzyme classification using 3D convolutional neural networks on spatial representation,"During the past decade, with the significant progress of computational power
as well as ever-rising data availability, deep learning techniques became
increasingly popular due to their excellent performance on computer vision
problems. The size of the Protein Data Bank has increased more than 15 fold
since 1999, which enabled the expansion of models that aim at predicting
enzymatic function via their amino acid composition. Amino acid sequence
however is less conserved in nature than protein structure and therefore
considered a less reliable predictor of protein function. This paper presents
EnzyNet, a novel 3D-convolutional neural networks classifier that predicts the
Enzyme Commission number of enzymes based only on their voxel-based spatial
structure. The spatial distribution of biochemical properties was also examined
as complementary information. The 2-layer architecture was investigated on a
large dataset of 63,558 enzymes from the Protein Data Bank and achieved an
accuracy of 78.4% by exploiting only the binary representation of the protein
shape. Code and datasets are available at https://github.com/shervinea/enzynet.","['Afshine Amidi', 'Shervine Amidi', 'Dimitrios Vlachakis', 'Vasileios Megalooikonomou', 'Nikos Paragios', 'Evangelia I. Zacharaki']","['q-bio.QM', 'cs.CV', 'stat.ML']",2017-07-19 10:59:29+00:00
http://arxiv.org/abs/1707.05961v1,Multidimensional classification of hippocampal shape features discriminates Alzheimer's disease and mild cognitive impairment from normal aging,"We describe a new method to automatically discriminate between patients with
Alzheimer's disease (AD) or mild cognitive impairment (MCI) and elderly
controls, based on multidimensional classification of hippocampal shape
features. This approach uses spherical harmonics (SPHARM) coefficients to model
the shape of the hippocampi, which are segmented from magnetic resonance images
(MRI) using a fully automatic method that we previously developed. SPHARM
coefficients are used as features in a classification procedure based on
support vector machines (SVM). The most relevant features for classification
are selected using a bagging strategy. We evaluate the accuracy of our method
in a group of 23 patients with AD (10 males, 13 females, age $\pm$
standard-deviation (SD) = 73 $\pm$ 6 years, mini-mental score (MMS) = 24.4
$\pm$ 2.8), 23 patients with amnestic MCI (10 males, 13 females, age $\pm$ SD =
74 $\pm$ 8 years, MMS = 27.3 $\pm$ 1.4) and 25 elderly healthy controls (13
males, 12 females, age $\pm$ SD = 64 $\pm$ 8 years), using leave-one-out
cross-validation. For AD vs controls, we obtain a correct classification rate
of 94%, a sensitivity of 96%, and a specificity of 92%. For MCI vs controls, we
obtain a classification rate of 83%, a sensitivity of 83%, and a specificity of
84%. This accuracy is superior to that of hippocampal volumetry and is
comparable to recently published SVM-based whole-brain classification methods,
which relied on a different strategy. This new method may become a useful tool
to assist in the diagnosis of Alzheimer's disease.","['Emilie Gerardin', 'Gaël Chételat', 'Marie Chupin', 'Rémi Cuingnet', 'Béatrice Desgranges', 'Ho-Sung Kim', 'Marc Niethammer', 'Bruno Dubois', 'Stéphane Lehéricy', 'Line Garnero', 'Francis Eustache', 'Olivier Colliot']","['cs.CV', 'q-bio.NC', 'stat.ML']",2017-07-19 07:33:02+00:00
http://arxiv.org/abs/1707.05947v1,Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints,"Algorithm-dependent generalization error bounds are central to statistical
learning theory. A learning algorithm may use a large hypothesis space, but the
limited number of iterations controls its model capacity and generalization
error. The impacts of stochastic gradient methods on generalization error for
non-convex learning problems not only have important theoretical consequences,
but are also critical to generalization errors of deep learning.
  In this paper, we study the generalization errors of Stochastic Gradient
Langevin Dynamics (SGLD) with non-convex objectives. Two theories are proposed
with non-asymptotic discrete-time analysis, using Stability and PAC-Bayesian
results respectively. The stability-based theory obtains a bound of
$O\left(\frac{1}{n}L\sqrt{\beta T_k}\right)$, where $L$ is uniform Lipschitz
parameter, $\beta$ is inverse temperature, and $T_k$ is aggregated step sizes.
For PAC-Bayesian theory, though the bound has a slower $O(1/\sqrt{n})$ rate,
the contribution of each step is shown with an exponentially decaying factor by
imposing $\ell^2$ regularization, and the uniform Lipschitz constant is also
replaced by actual norms of gradients along trajectory. Our bounds have no
implicit dependence on dimensions, norms or other capacity measures of
parameter, which elegantly characterizes the phenomenon of ""Fast Training
Guarantees Generalization"" in non-convex settings. This is the first
algorithm-dependent result with reasonable dependence on aggregated step sizes
for non-convex learning, and has important implications to statistical learning
aspects of stochastic gradient methods in complicated models such as deep
learning.","['Wenlong Mou', 'Liwei Wang', 'Xiyu Zhai', 'Kai Zheng']","['cs.LG', 'math.OC', 'stat.ML']",2017-07-19 06:17:57+00:00
http://arxiv.org/abs/1707.05922v1,Improving Output Uncertainty Estimation and Generalization in Deep Learning via Neural Network Gaussian Processes,"We propose a simple method that combines neural networks and Gaussian
processes. The proposed method can estimate the uncertainty of outputs and
flexibly adjust target functions where training data exist, which are
advantages of Gaussian processes. The proposed method can also achieve high
generalization performance for unseen input configurations, which is an
advantage of neural networks. With the proposed method, neural networks are
used for the mean functions of Gaussian processes. We present a scalable
stochastic inference procedure, where sparse Gaussian processes are inferred by
stochastic variational inference, and the parameters of neural networks and
kernels are estimated by stochastic gradient descent methods, simultaneously.
We use two real-world spatio-temporal data sets to demonstrate experimentally
that the proposed method achieves better uncertainty estimation and
generalization performance than neural networks and Gaussian processes.","['Tomoharu Iwata', 'Zoubin Ghahramani']",['stat.ML'],2017-07-19 02:29:49+00:00
http://arxiv.org/abs/1707.05909v1,Recovering Latent Signals from a Mixture of Measurements using a Gaussian Process Prior,"In sensing applications, sensors cannot always measure the latent quantity of
interest at the required resolution, sometimes they can only acquire a blurred
version of it due the sensor's transfer function. To recover latent signals
when only noisy mixed measurements of the signal are available, we propose the
Gaussian process mixture of measurements (GPMM), which models the latent signal
as a Gaussian process (GP) and allows us to perform Bayesian inference on such
signal conditional to a set of noisy mixture of measurements. We describe how
to train GPMM, that is, to find the hyperparameters of the GP and the mixing
weights, and how to perform inference on the latent signal under GPMM;
additionally, we identify the solution to the underdetermined linear system
resulting from a sensing application as a particular case of GPMM. The proposed
model is validated in the recovery of three signals: a smooth synthetic signal,
a real-world heart-rate time series and a step function, where GPMM
outperformed the standard GP in terms of estimation error, uncertainty
representation and recovery of the spectral content of the latent signal.","['Felipe Tobar', 'Gonzalo Rios', 'Tomás Valdivia', 'Pablo Guerrero']","['stat.ML', 'cs.LG']",2017-07-19 00:56:11+00:00
http://arxiv.org/abs/1707.05861v1,On Adaptive Propensity Score Truncation in Causal Inference,"The positivity assumption, or the experimental treatment assignment (ETA)
assumption, is important for identifiability in causal inference. Even if the
positivity assumption holds, practical violations of this assumption may
jeopardize the finite sample performance of the causal estimator. One of the
consequences of practical violations of the positivity assumption is extreme
values in the estimated propensity score (PS). A common practice to address
this issue is truncating the PS estimate when constructing PS-based estimators.
In this study, we propose a novel adaptive truncation method,
Positivity-C-TMLE, based on the collaborative targeted maximum likelihood
estimation (C-TMLE) methodology. We demonstrate the outstanding performance of
our novel approach in a variety of simulations by comparing it with other
commonly studied estimators. Results show that by adaptively truncating the
estimated PS with a more targeted objective function, the Positivity-C-TMLE
estimator achieves the best performance for both point estimation and
confidence interval coverage among all estimators considered.","['Cheng Ju', 'Joshua Schwab', 'Mark J. van der Laan']","['stat.ME', 'stat.CO', 'stat.ML']",2017-07-18 21:17:55+00:00
http://arxiv.org/abs/1707.05841v1,Linear Time Complexity Deep Fourier Scattering Network and Extension to Nonlinear Invariants,"In this paper we propose a scalable version of a state-of-the-art
deterministic time-invariant feature extraction approach based on consecutive
changes of basis and nonlinearities, namely, the scattering network. The first
focus of the paper is to extend the scattering network to allow the use of
higher order nonlinearities as well as extracting nonlinear and Fourier based
statistics leading to the required invariants of any inherently structured
input. In order to reach fast convolutions and to leverage the intrinsic
structure of wavelets, we derive our complete model in the Fourier domain. In
addition of providing fast computations, we are now able to exploit sparse
matrices due to extremely high sparsity well localized in the Fourier domain.
As a result, we are able to reach a true linear time complexity with inputs in
the Fourier domain allowing fast and energy efficient solutions to machine
learning tasks. Validation of the features and computational results will be
presented through the use of these invariant coefficients to perform
classification on audio recordings of bird songs captured in multiple different
soundscapes. In the end, the applicability of the presented solutions to deep
artificial neural networks is discussed.","['Randall Balestriero', 'Herve Glotin']","['stat.ML', 'cs.LG']",2017-07-18 20:04:35+00:00
http://arxiv.org/abs/1707.05840v1,Multiscale Residual Mixture of PCA: Dynamic Dictionaries for Optimal Basis Learning,"In this paper we are interested in the problem of learning an over-complete
basis and a methodology such that the reconstruction or inverse problem does
not need optimization. We analyze the optimality of the presented approaches,
their link to popular already known techniques s.a. Artificial Neural
Networks,k-means or Oja's learning rule. Finally, we will see that one approach
to reach the optimal dictionary is a factorial and hierarchical approach. The
derived approach lead to a formulation of a Deep Oja Network. We present
results on different tasks and present the resulting very efficient learning
algorithm which brings a new vision on the training of deep nets. Finally, the
theoretical work shows that deep frameworks are one way to efficiently have
over-complete (combinatorially large) dictionary yet allowing easy
reconstruction. We thus present the Deep Residual Oja Network (DRON). We
demonstrate that a recursive deep approach working on the residuals allow
exponential decrease of the error w.r.t. the depth.",['Randall Balestriero'],"['stat.ML', 'cs.LG']",2017-07-18 19:56:05+00:00
http://arxiv.org/abs/1707.05807v1,Improving Gibbs Sampler Scan Quality with DoGS,"The pairwise influence matrix of Dobrushin has long been used as an
analytical tool to bound the rate of convergence of Gibbs sampling. In this
work, we use Dobrushin influence as the basis of a practical tool to certify
and efficiently improve the quality of a discrete Gibbs sampler. Our
Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection
orders for a given sampling budget and variable subset of interest, explicit
bounds on total variation distance to stationarity, and certifiable
improvements over the standard systematic and uniform random scan Gibbs
samplers. In our experiments with joint image segmentation and object
recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising
model inference, DoGS consistently deliver higher-quality inferences with
significantly smaller sampling budgets than standard Gibbs samplers.","['Ioannis Mitliagkas', 'Lester Mackey']","['stat.ML', 'cs.LG', 'math.PR', 'stat.ME']",2017-07-18 18:17:55+00:00
http://arxiv.org/abs/1707.05776v2,Optimizing the Latent Space of Generative Networks,"Generative Adversarial Networks (GANs) have achieved remarkable results in
the task of generating realistic natural images. In most successful
applications, GAN models share two common aspects: solving a challenging saddle
point optimization problem, interpreted as an adversarial game between a
generator and a discriminator functions; and parameterizing the generator and
the discriminator as deep convolutional neural networks. The goal of this paper
is to disentangle the contribution of these two factors to the success of GANs.
In particular, we introduce Generative Latent Optimization (GLO), a framework
to train deep convolutional generators using simple reconstruction losses.
Throughout a variety of experiments, we show that GLO enjoys many of the
desirable properties of GANs: synthesizing visually-appealing samples,
interpolating meaningfully between samples, and performing linear arithmetic
with noise vectors; all of this without the adversarial optimization scheme.","['Piotr Bojanowski', 'Armand Joulin', 'David Lopez-Paz', 'Arthur Szlam']","['stat.ML', 'cs.CV', 'cs.LG']",2017-07-18 17:58:34+00:00
http://arxiv.org/abs/1707.05729v1,Robust Bayesian Optimization with Student-t Likelihood,"Bayesian optimization has recently attracted the attention of the automatic
machine learning community for its excellent results in hyperparameter tuning.
BO is characterized by the sample efficiency with which it can optimize
expensive black-box functions. The efficiency is achieved in a similar fashion
to the learning to learn methods: surrogate models (typically in the form of
Gaussian processes) learn the target function and perform intelligent sampling.
This surrogate model can be applied even in the presence of noise; however, as
with most regression methods, it is very sensitive to outlier data. This can
result in erroneous predictions and, in the case of BO, biased and inefficient
exploration. In this work, we present a GP model that is robust to outliers
which uses a Student-t likelihood to segregate outliers and robustly conduct
Bayesian optimization. We present numerical results evaluating the proposed
method in both artificial functions and real problems.","['Ruben Martinez-Cantin', 'Michael McCourt', 'Kevin Tee']","['cs.LG', 'cs.AI', 'stat.ML']",2017-07-18 16:22:07+00:00
http://arxiv.org/abs/1707.05697v1,An Iterative BP-CNN Architecture for Channel Decoding,"Inspired by recent advances in deep learning, we propose a novel iterative
BP-CNN architecture for channel decoding under correlated noise. This
architecture concatenates a trained convolutional neural network (CNN) with a
standard belief-propagation (BP) decoder. The standard BP decoder is used to
estimate the coded bits, followed by a CNN to remove the estimation errors of
the BP decoder and obtain a more accurate estimation of the channel noise.
Iterating between BP and CNN will gradually improve the decoding SNR and hence
result in better decoding performance. To train a well-behaved CNN model, we
define a new loss function which involves not only the accuracy of the noise
estimation but also the normality test for the estimation errors, i.e., to
measure how likely the estimation errors follow a Gaussian distribution. The
introduction of the normality test to the CNN training shapes the residual
noise distribution and further reduces the BER of the iterative decoding,
compared to using the standard quadratic loss function. We carry out extensive
experiments to analyze and verify the proposed framework. The iterative BP-CNN
decoder has better BER performance with lower complexity, is suitable for
parallel implementation, does not rely on any specific channel model or
encoding method, and is robust against training mismatches. All of these
features make it a good candidate for decoding modern channel codes.","['Fei Liang', 'Cong Shen', 'Feng Wu']","['stat.ML', 'cs.IT', 'math.IT']",2017-07-18 15:41:49+00:00
http://arxiv.org/abs/1707.05609v2,Solving $\ell^p\!$-norm regularization with tensor kernels,"In this paper, we discuss how a suitable family of tensor kernels can be used
to efficiently solve nonparametric extensions of $\ell^p$ regularized learning
methods. Our main contribution is proposing a fast dual algorithm, and showing
that it allows to solve the problem efficiently. Our results contrast recent
findings suggesting kernel methods cannot be extended beyond Hilbert setting.
Numerical experiments confirm the effectiveness of the method.","['Saverio Salzo', 'Johan A. K. Suykens', 'Lorenzo Rosasco']","['stat.ML', 'math.OC']",2017-07-18 13:48:22+00:00
http://arxiv.org/abs/1707.05587v1,Graph learning under sparsity priors,"Graph signals offer a very generic and natural representation for data that
lives on networks or irregular structures. The actual data structure is however
often unknown a priori but can sometimes be estimated from the knowledge of the
application domain. If this is not possible, the data structure has to be
inferred from the mere signal observations. This is exactly the problem that we
address in this paper, under the assumption that the graph signals can be
represented as a sparse linear combination of a few atoms of a structured graph
dictionary. The dictionary is constructed on polynomials of the graph
Laplacian, which can sparsely represent a general class of graph signals
composed of localized patterns on the graph. We formulate a graph learning
problem, whose solution provides an ideal fit between the signal observations
and the sparse graph signal model. As the problem is non-convex, we propose to
solve it by alternating between a signal sparse coding and a graph update step.
We provide experimental results that outline the good graph recovery
performance of our method, which generally compares favourably to other recent
network inference algorithms.","['Hermina Petric Maretic', 'Dorina Thanou', 'Pascal Frossard']","['cs.LG', 'cs.SI', 'stat.ML']",2017-07-18 12:31:53+00:00
http://arxiv.org/abs/1707.05562v1,One-Shot Learning in Discriminative Neural Networks,"We consider the task of one-shot learning of visual categories. In this paper
we explore a Bayesian procedure for updating a pretrained convnet to classify a
novel image category for which data is limited. We decompose this convnet into
a fixed feature extractor and softmax classifier. We assume that the target
weights for the new task come from the same distribution as the pretrained
softmax weights, which we model as a multivariate Gaussian. By using this as a
prior for the new weights, we demonstrate competitive performance with
state-of-the-art methods whilst also being consistent with 'normal' methods for
training deep networks on large data.","['Jordan Burgess', 'James Robert Lloyd', 'Zoubin Ghahramani']","['stat.ML', 'cs.LG']",2017-07-18 11:17:22+00:00
http://arxiv.org/abs/1707.07539v1,Exploring Outliers in Crowdsourced Ranking for QoE,"Outlier detection is a crucial part of robust evaluation for crowdsourceable
assessment of Quality of Experience (QoE) and has attracted much attention in
recent years. In this paper, we propose some simple and fast algorithms for
outlier detection and robust QoE evaluation based on the nonconvex optimization
principle. Several iterative procedures are designed with or without knowing
the number of outliers in samples. Theoretical analysis is given to show that
such procedures can reach statistically good estimates under mild conditions.
Finally, experimental results with simulated and real-world crowdsourcing
datasets show that the proposed algorithms could produce similar performance to
Huber-LASSO approach in robust ranking, yet with nearly 8 or 90 times speed-up,
without or with a prior knowledge on the sparsity size of outliers,
respectively. Therefore the proposed methodology provides us a set of helpful
tools for robust QoE evaluation with crowdsourcing data.","['Qianqian Xu', 'Ming Yan', 'Chendi Huang', 'Jiechao Xiong', 'Qingming Huang', 'Yuan Yao']","['stat.ML', 'cs.LG']",2017-07-18 10:34:13+00:00
http://arxiv.org/abs/1707.05534v2,Latent Gaussian Process Regression,"We introduce Latent Gaussian Process Regression which is a latent variable
extension allowing modelling of non-stationary multi-modal processes using GPs.
The approach is built on extending the input space of a regression problem with
a latent variable that is used to modulate the covariance function over the
training data. We show how our approach can be used to model multi-modal and
non-stationary processes. We exemplify the approach on a set of synthetic data
and provide results on real data from motion capture and geostatistics.","['Erik Bodin', 'Neill D. F. Campbell', 'Carl Henrik Ek']","['stat.ML', 'cs.LG']",2017-07-18 09:19:20+00:00
http://arxiv.org/abs/1707.05533v3,Global optimization for low-dimensional switching linear regression and bounded-error estimation,"The paper provides global optimization algorithms for two particularly
difficult nonconvex problems raised by hybrid system identification: switching
linear regression and bounded-error estimation. While most works focus on local
optimization heuristics without global optimality guarantees or with guarantees
valid only under restrictive conditions, the proposed approach always yields a
solution with a certificate of global optimality. This approach relies on a
branch-and-bound strategy for which we devise lower bounds that can be
efficiently computed. In order to obtain scalable algorithms with respect to
the number of data, we directly optimize the model parameters in a continuous
optimization setting without involving integer variables. Numerical experiments
show that the proposed algorithms offer a higher accuracy than convex
relaxations with a reasonable computational burden for hybrid system
identification. In addition, we discuss how bounded-error estimation is related
to robust estimation in the presence of outliers and exact recovery under
sparse noise, for which we also obtain promising numerical results.",['Fabien Lauer'],"['cs.LG', 'stat.ML']",2017-07-18 09:18:22+00:00
http://arxiv.org/abs/1707.05532v1,Bayesian Nonlinear Support Vector Machines for Big Data,"We propose a fast inference method for Bayesian nonlinear support vector
machines that leverages stochastic variational inference and inducing points.
Our experiments show that the proposed method is faster than competing Bayesian
approaches and scales easily to millions of data points. It provides additional
features over frequentist competitors such as accurate predictive uncertainty
estimates and automatic hyperparameter search.","['Florian Wenzel', 'Theo Galy-Fajou', 'Matthaeus Deutsch', 'Marius Kloft']","['stat.ML', 'cs.LG']",2017-07-18 09:16:50+00:00
http://arxiv.org/abs/1707.05499v1,A Machine Learning Approach for Evaluating Creative Artifacts,"Much work has been done in understanding human creativity and defining
measures to evaluate creativity. This is necessary mainly for the reason of
having an objective and automatic way of quantifying creative artifacts. In
this work, we propose a regression-based learning framework which takes into
account quantitatively the essential criteria for creativity like novelty,
influence, value and unexpectedness. As it is often the case with most creative
domains, there is no clear ground truth available for creativity. Our proposed
learning framework is applicable to all creative domains; yet we evaluate it on
a dataset of movies created from IMDb and Rotten Tomatoes due to availability
of audience and critic scores, which can be used as proxy ground truth labels
for creativity. We report promising results and observations from our
experiments in the following ways : 1) Correlation of creative criteria with
critic scores, 2) Improvement in movie rating prediction with inclusion of
various creative criteria, and 3) Identification of creative movies.","['Disha Shrivastava', 'Saneem Ahmed CG', 'Anirban Laha', 'Karthik Sankaranarayanan']","['cs.LG', 'cs.AI', 'stat.ML']",2017-07-18 06:59:45+00:00
http://arxiv.org/abs/1707.05497v1,Differentially Private Identity and Closeness Testing of Discrete Distributions,"We investigate the problems of identity and closeness testing over a discrete
population from random samples. Our goal is to develop efficient testers while
guaranteeing Differential Privacy to the individuals of the population. We
describe an approach that yields sample-efficient differentially private
testers for these problems. Our theoretical results show that there exist
private identity and closeness testers that are nearly as sample-efficient as
their non-private counterparts. We perform an experimental evaluation of our
algorithms on synthetic data. Our experiments illustrate that our private
testers achieve small type I and type II errors with sample size sublinear in
the domain size of the underlying distributions.","['Maryam Aliakbarpour', 'Ilias Diakonikolas', 'Ronitt Rubinfeld']","['cs.LG', 'cs.DS', 'cs.IT', 'math.IT', 'stat.ML']",2017-07-18 06:51:31+00:00
http://arxiv.org/abs/1707.05470v2,DeepProbe: Information Directed Sequence Understanding and Chatbot Design via Recurrent Neural Networks,"Information extraction and user intention identification are central topics
in modern query understanding and recommendation systems. In this paper, we
propose DeepProbe, a generic information-directed interaction framework which
is built around an attention-based sequence to sequence (seq2seq) recurrent
neural network. DeepProbe can rephrase, evaluate, and even actively ask
questions, leveraging the generative ability and likelihood estimation made
possible by seq2seq models. DeepProbe makes decisions based on a derived
uncertainty (entropy) measure conditioned on user inputs, possibly with
multiple rounds of interactions. Three applications, namely a rewritter, a
relevance scorer and a chatbot for ad recommendation, were built around
DeepProbe, with the first two serving as precursory building blocks for the
third. We first use the seq2seq model in DeepProbe to rewrite a user query into
one of standard query form, which is submitted to an ordinary recommendation
system. Secondly, we evaluate DeepProbe's seq2seq model-based relevance
scoring. Finally, we build a chatbot prototype capable of making active user
interactions, which can ask questions that maximize information gain, allowing
for a more efficient user intention idenfication process. We evaluate first two
applications by 1) comparing with baselines by BLEU and AUC, and 2) human judge
evaluation. Both demonstrate significant improvements compared with current
state-of-the-art systems, proving their values as useful tools on their own,
and at the same time laying a good foundation for the ongoing chatbot
application.","['Zi Yin', 'Keng-hao Chang', 'Ruofei Zhang']","['stat.ML', 'cs.LG']",2017-07-18 05:12:09+00:00
http://arxiv.org/abs/1707.05420v1,Cooperative Hierarchical Dirichlet Processes: Superposition vs. Maximization,"The cooperative hierarchical structure is a common and significant data
structure observed in, or adopted by, many research areas, such as: text mining
(author-paper-word) and multi-label classification (label-instance-feature).
Renowned Bayesian approaches for cooperative hierarchical structure modeling
are mostly based on topic models. However, these approaches suffer from a
serious issue in that the number of hidden topics/factors needs to be fixed in
advance and an inappropriate number may lead to overfitting or underfitting.
One elegant way to resolve this issue is Bayesian nonparametric learning, but
existing work in this area still cannot be applied to cooperative hierarchical
structure modeling.
  In this paper, we propose a cooperative hierarchical Dirichlet process (CHDP)
to fill this gap. Each node in a cooperative hierarchical structure is assigned
a Dirichlet process to model its weights on the infinite hidden factors/topics.
Together with measure inheritance from hierarchical Dirichlet process, two
kinds of measure cooperation, i.e., superposition and maximization, are defined
to capture the many-to-many relationships in the cooperative hierarchical
structure. Furthermore, two constructive representations for CHDP, i.e.,
stick-breaking and international restaurant process, are designed to facilitate
the model inference. Experiments on synthetic and real-world data with
cooperative hierarchical structures demonstrate the properties and the ability
of CHDP for cooperative hierarchical structure modeling and its potential for
practical application scenarios.","['Junyu Xuan', 'Jie Lu', 'Guangquan Zhang', 'Richard Yi Da Xu']","['cs.LG', 'stat.ML']",2017-07-18 00:42:10+00:00
http://arxiv.org/abs/1707.05373v1,Houdini: Fooling Deep Structured Prediction Models,"Generating adversarial examples is a critical step for evaluating and
improving the robustness of learning machines. So far, most existing methods
only work for classification and are not designed to alter the true performance
measure of the problem at hand. We introduce a novel flexible approach named
Houdini for generating adversarial examples specifically tailored for the final
performance measure of the task considered, be it combinatorial and
non-decomposable. We successfully apply Houdini to a range of applications such
as speech recognition, pose estimation and semantic segmentation. In all cases,
the attacks based on Houdini achieve higher success rate than those based on
the traditional surrogates used to train the models while using a less
perceptible adversarial perturbation.","['Moustapha Cisse', 'Yossi Adi', 'Natalia Neverova', 'Joseph Keshet']","['stat.ML', 'cs.AI', 'cs.CR', 'cs.CV', 'cs.LG']",2017-07-17 19:11:08+00:00
http://arxiv.org/abs/1707.05342v3,An optimal unrestricted learning procedure,"We study learning problems involving arbitrary classes of functions $F$,
distributions $X$ and targets $Y$. Because proper learning procedures, i.e.,
procedures that are only allowed to select functions in $F$, tend to perform
poorly unless the problem satisfies some additional structural property (e.g.,
that $F$ is convex), we consider unrestricted learning procedures that are free
to choose functions outside the given class.
  We present a new unrestricted procedure that is optimal in a very strong
sense: the required sample complexity is essentially the best one can hope for,
and the estimate holds for (almost) any problem, including heavy-tailed
situations. Moreover, the sample complexity coincides with the what one would
expect if $F$ were convex, even when $F$ is not. And if $F$ is convex, the
procedure turns out to be proper. Thus, the unrestricted procedure is actually
optimal in both realms, for convex classes as a proper procedure and for
arbitrary classes as an unrestricted procedure.",['Shahar Mendelson'],['stat.ML'],2017-07-17 18:05:59+00:00
http://arxiv.org/abs/1707.05167v2,Cosmological model discrimination with Deep Learning,"We demonstrate the potential of Deep Learning methods for measurements of
cosmological parameters from density fields, focusing on the extraction of
non-Gaussian information. We consider weak lensing mass maps as our dataset. We
aim for our method to be able to distinguish between five models, which were
chosen to lie along the $\sigma_8$ - $\Omega_m$ degeneracy, and have nearly the
same two-point statistics. We design and implement a Deep Convolutional Neural
Network (DCNN) which learns the relation between five cosmological models and
the mass maps they generate. We develop a new training strategy which ensures
the good performance of the network for high levels of noise. We compare the
performance of this approach to commonly used non-Gaussian statistics, namely
the skewness and kurtosis of the convergence maps. We find that our
implementation of DCNN outperforms the skewness and kurtosis statistics,
especially for high noise levels. The network maintains the mean discrimination
efficiency greater than $85\%$ even for noise levels corresponding to ground
based lensing observations, while the other statistics perform worse in this
setting, achieving efficiency less than $70\%$. This demonstrates the ability
of CNN-based methods to efficiently break the $\sigma_8$ - $\Omega_m$
degeneracy with weak lensing mass maps alone. We discuss the potential of this
method to be applied to the analysis of real weak lensing data and other
datasets.","['Jorit Schmelzle', 'Aurelien Lucchi', 'Tomasz Kacprzak', 'Adam Amara', 'Raphael Sgier', 'Alexandre Réfrégier', 'Thomas Hofmann']","['astro-ph.CO', 'stat.ML']",2017-07-17 13:58:18+00:00
http://arxiv.org/abs/1707.05316v1,Current-mode Memristor Crossbars for Neuromemristive Systems,"Motivated by advantages of current-mode design, this brief contribution
explores the implementation of weight matrices in neuromemristive systems via
current-mode memristor crossbar circuits. After deriving theoretical results
for the range and distribution of weights in the current-mode design, it is
shown that any weight matrix based on voltage-mode crossbars can be mapped to a
current-mode crossbar if the voltage-mode weights are carefully bounded. Then,
a modified gradient descent rule is derived for the current-mode design that
can be used to perform backpropagation training. Behavioral simulations on the
MNIST dataset indicate that both voltage and current-mode designs are able to
achieve similar accuracy and have similar defect tolerance. However, analysis
of trained weight distributions reveals that current-mode and voltage-mode
designs may use different feature representations.",['Cory Merkel'],"['stat.ML', 'cs.ET']",2017-07-17 13:10:13+00:00
http://arxiv.org/abs/1707.05712v3,PAC-Bayes and Domain Adaptation,"We provide two main contributions in PAC-Bayesian theory for domain
adaptation where the objective is to learn, from a source distribution, a
well-performing majority vote on a different, but related, target distribution.
Firstly, we propose an improvement of the previous approach we proposed in
Germain et al. (2013), which relies on a novel distribution pseudodistance
based on a disagreement averaging, allowing us to derive a new tighter domain
adaptation bound for the target risk. While this bound stands in the spirit of
common domain adaptation works, we derive a second bound (introduced in Germain
et al., 2016) that brings a new perspective on domain adaptation by deriving an
upper bound on the target risk where the distributions' divergence-expressed as
a ratio-controls the trade-off between a source error measure and the target
voters' disagreement. We discuss and compare both results, from which we obtain
PAC-Bayesian generalization bounds. Furthermore, from the PAC-Bayesian
specialization to linear classifiers, we infer two learning algorithms, and we
evaluate them on real data.","['Pascal Germain', 'Amaury Habrard', 'François Laviolette', 'Emilie Morvant']",['stat.ML'],2017-07-17 11:34:53+00:00
http://arxiv.org/abs/1707.05101v2,On consistency of optimal pricing algorithms in repeated posted-price auctions with strategic buyer,"We study revenue optimization learning algorithms for repeated posted-price
auctions where a seller interacts with a single strategic buyer that holds a
fixed private valuation for a good and seeks to maximize his cumulative
discounted surplus. For this setting, first, we propose a novel algorithm that
never decreases offered prices and has a tight strategic regret bound in
$\Theta(\log\log T)$ under some mild assumptions on the buyer surplus
discounting. This result closes the open research question on the existence of
a no-regret horizon-independent weakly consistent pricing. The proposed
algorithm is inspired by our observation that a double decrease of offered
prices in a weakly consistent algorithm is enough to cause a linear regret.
This motivates us to construct a novel transformation that maps a
right-consistent algorithm to a weakly consistent one that never decreases
offered prices.
  Second, we outperform the previously known strategic regret upper bound of
the algorithm PRRFES, where the improvement is achieved by means of a finer
constant factor $C$ of the principal term $C\log\log T$ in this upper bound.
Finally, we generalize results on strategic regret previously known for
geometric discounting of the buyer's surplus to discounting of other types,
namely: the optimality of the pricing PRRFES to the case of geometrically
concave decreasing discounting; and linear lower bound on the strategic regret
of a wide range of horizon-independent weakly consistent algorithms to the case
of arbitrary discounts.",['Alexey Drutsa'],"['cs.GT', 'cs.AI', 'cs.LG', 'stat.ML', '91A20 (Primary), 68T05 (Secondary), 68W27 (Secondary), 91A05\n  (Secondary), 91A26 (Secondary)', 'I.2.6; F.2.2']",2017-07-17 11:29:14+00:00
http://arxiv.org/abs/1707.05010v1,Deep Learning to Attend to Risk in ICU,"Modeling physiological time-series in ICU is of high clinical importance.
However, data collected within ICU are irregular in time and often contain
missing measurements. Since absence of a measure would signify its lack of
importance, the missingness is indeed informative and might reflect the
decision making by the clinician. Here we propose a deep learning architecture
that can effectively handle these challenges for predicting ICU mortality
outcomes. The model is based on Long Short-Term Memory, and has layered
attention mechanisms. At the sensing layer, the model decides whether to
observe and incorporate parts of the current measurements. At the reasoning
layer, evidences across time steps are weighted and combined. The model is
evaluated on the PhysioNet 2012 dataset showing competitive and interpretable
results.","['Phuoc Nguyen', 'Truyen Tran', 'Svetha Venkatesh']","['cs.LG', 'stat.ML']",2017-07-17 06:23:20+00:00
http://arxiv.org/abs/1707.04958v1,An Ensemble Boosting Model for Predicting Transfer to the Pediatric Intensive Care Unit,"Our work focuses on the problem of predicting the transfer of pediatric
patients from the general ward of a hospital to the pediatric intensive care
unit. Using data collected over 5.5 years from the electronic health records of
two medical facilities, we develop classifiers based on adaptive boosting and
gradient tree boosting. We further combine these learned classifiers into an
ensemble model and compare its performance to a modified pediatric early
warning score (PEWS) baseline that relies on expert defined guidelines. To
gauge model generalizability, we perform an inter-facility evaluation where we
train our algorithm on data from one facility and perform evaluation on a
hidden test dataset from a separate facility. We show that improvements are
witnessed over the PEWS baseline in accuracy (0.77 vs. 0.69), sensitivity (0.80
vs. 0.68), specificity (0.74 vs. 0.70) and AUROC (0.85 vs. 0.73).","['Jonathan Rubin', 'Cristhian Potes', 'Minnan Xu-Wilson', 'Junzi Dong', 'Asif Rahman', 'Hiep Nguyen', 'David Moromisato']","['cs.LG', 'stat.AP', 'stat.ML']",2017-07-16 23:01:35+00:00
http://arxiv.org/abs/1707.04926v3,Theoretical insights into the optimization landscape of over-parameterized shallow neural networks,"In this paper we study the problem of learning a shallow artificial neural
network that best fits a training data set. We study this problem in the
over-parameterized regime where the number of observations are fewer than the
number of parameters in the model. We show that with quadratic activations the
optimization landscape of training such shallow neural networks has certain
favorable characteristics that allow globally optimal models to be found
efficiently using a variety of local search heuristics. This result holds for
an arbitrary training data of input/output pairs. For differentiable activation
functions we also show that gradient descent, when suitably initialized,
converges at a linear rate to a globally optimal model. This result focuses on
a realizable model where the inputs are chosen i.i.d. from a Gaussian
distribution and the labels are generated according to planted weight
coefficients.","['Mahdi Soltanolkotabi', 'Adel Javanmard', 'Jason D. Lee']","['cs.LG', 'cs.IT', 'math.IT', 'math.OC', 'stat.ML']",2017-07-16 18:13:51+00:00
http://arxiv.org/abs/1707.04692v1,On the Performance of Forecasting Models in the Presence of Input Uncertainty,"Nowadays, with the unprecedented penetration of renewable distributed energy
resources (DERs), the necessity of an efficient energy forecasting model is
more demanding than before. Generally, forecasting models are trained using
observed weather data while the trained models are applied for energy
forecasting using forecasted weather data. In this study, the performance of
several commonly used forecasting methods in the presence of weather predictors
with uncertainty is assessed and compared. Accordingly, both observed and
forecasted weather data are collected, then the influential predictors for
solar PV generation forecasting model are selected using several measures.
Using observed and forecasted weather data, an analysis on the uncertainty of
weather variables is represented by MAE and bootstrapping. The energy
forecasting model is trained using observed weather data, and finally, the
performance of several commonly used forecasting methods in solar energy
forecasting is simulated and compared for a real case study.","['Hossein Sangrody', 'Morteza Sarailoo', 'Ning Zhou', 'Ahmad Shokrollahi', 'Elham Foruzan']",['stat.ML'],2017-07-15 06:16:21+00:00
http://arxiv.org/abs/1707.04673v1,Learning linear structural equation models in polynomial time and sample complexity,"The problem of learning structural equation models (SEMs) from data is a
fundamental problem in causal inference. We develop a new algorithm --- which
is computationally and statistically efficient and works in the
high-dimensional regime --- for learning linear SEMs from purely observational
data with arbitrary noise distribution. We consider three aspects of the
problem: identifiability, computational efficiency, and statistical efficiency.
We show that when data is generated from a linear SEM over $p$ nodes and
maximum degree $d$, our algorithm recovers the directed acyclic graph (DAG)
structure of the SEM under an identifiability condition that is more general
than those considered in the literature, and without faithfulness assumptions.
In the population setting, our algorithm recovers the DAG structure in
$\mathcal{O}(p(d^2 + \log p))$ operations. In the finite sample setting, if the
estimated precision matrix is sparse, our algorithm has a smoothed complexity
of $\widetilde{\mathcal{O}}(p^3 + pd^7)$, while if the estimated precision
matrix is dense, our algorithm has a smoothed complexity of
$\widetilde{\mathcal{O}}(p^5)$. For sub-Gaussian noise, we show that our
algorithm has a sample complexity of $\mathcal{O}(\frac{d^8}{\varepsilon^2}
\log (\frac{p}{\sqrt{\delta}}))$ to achieve $\varepsilon$ element-wise additive
error with respect to the true autoregression matrix with probability at most
$1 - \delta$, while for noise with bounded $(4m)$-th moment, with $m$ being a
positive integer, our algorithm has a sample complexity of
$\mathcal{O}(\frac{d^8}{\varepsilon^2} (\frac{p^2}{\delta})^{1/m})$.","['Asish Ghoshal', 'Jean Honorio']","['cs.LG', 'stat.ML']",2017-07-15 01:10:57+00:00
http://arxiv.org/abs/1707.04659v3,Variational approach for learning Markov processes from time series data,"Inference, prediction and control of complex dynamical systems from time
series is important in many areas, including financial markets, power grid
management, climate and weather modeling, or molecular dynamics. The analysis
of such highly nonlinear dynamical systems is facilitated by the fact that we
can often find a (generally nonlinear) transformation of the system coordinates
to features in which the dynamics can be excellently approximated by a linear
Markovian model. Moreover, the large number of system variables often change
collectively on large time- and length-scales, facilitating a low-dimensional
analysis in feature space. In this paper, we introduce a variational approach
for Markov processes (VAMP) that allows us to find optimal feature mappings and
optimal Markovian models of the dynamics from given time series data. The key
insight is that the best linear model can be obtained from the top singular
components of the Koopman operator. This leads to the definition of a family of
score functions called VAMP-r which can be calculated from data, and can be
employed to optimize a Markovian model. In addition, based on the relationship
between the variational scores and approximation errors of Koopman operators,
we propose a new VAMP-E score, which can be applied to cross-validation for
hyper-parameter optimization and model selection in VAMP. VAMP is valid for
both reversible and nonreversible processes and for stationary and
non-stationary processes or realizations.","['Hao Wu', 'Frank Noé']","['stat.ML', 'math.DS']",2017-07-14 23:07:32+00:00
http://arxiv.org/abs/1707.04638v1,Predicting multicellular function through multi-layer tissue networks,"Motivation: Understanding functions of proteins in specific human tissues is
essential for insights into disease diagnostics and therapeutics, yet
prediction of tissue-specific cellular function remains a critical challenge
for biomedicine.
  Results: Here we present OhmNet, a hierarchy-aware unsupervised node feature
learning approach for multi-layer networks. We build a multi-layer network,
where each layer represents molecular interactions in a different human tissue.
OhmNet then automatically learns a mapping of proteins, represented as nodes,
to a neural embedding based low-dimensional space of features. OhmNet
encourages sharing of similar features among proteins with similar network
neighborhoods and among proteins activated in similar tissues. The algorithm
generalizes prior work, which generally ignores relationships between tissues,
by modeling tissue organization with a rich multiscale tissue hierarchy. We use
OhmNet to study multicellular function in a multi-layer protein interaction
network of 107 human tissues. In 48 tissues with known tissue-specific cellular
functions, OhmNet provides more accurate predictions of cellular function than
alternative approaches, and also generates more accurate hypotheses about
tissue-specific protein actions. We show that taking into account the tissue
hierarchy leads to improved predictive power. Remarkably, we also demonstrate
that it is possible to leverage the tissue hierarchy in order to effectively
transfer cellular functions to a functionally uncharacterized tissue. Overall,
OhmNet moves from flat networks to multiscale models able to predict a range of
phenotypes spanning cellular subsystems","['Marinka Zitnik', 'Jure Leskovec']","['cs.LG', 'cs.SI', 'q-bio.MN', 'stat.ML']",2017-07-14 21:03:53+00:00
http://arxiv.org/abs/1707.04588v1,GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures,"VAEs (Variational AutoEncoders) have proved to be powerful in the context of
density modeling and have been used in a variety of contexts for creative
purposes. In many settings, the data we model possesses continuous attributes
that we would like to take into account at generation time. We propose in this
paper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational
AutoEncoder architecture and its generalizations which allows a fine control on
the embedding of the data into the latent space. When augmenting the VAE loss
with this regularization, changes in the learned latent space reflects changes
of the attributes of the data. This deeper understanding of the VAE latent
space structure offers the possibility to modulate the attributes of the
generated data in a continuous way. We demonstrate its efficiency on a
monophonic music generation task where we manage to generate variations of
discrete sequences in an intended and playful way.","['Gaëtan Hadjeres', 'Frank Nielsen', 'François Pachet']","['cs.LG', 'cs.AI', 'stat.ML']",2017-07-14 12:28:25+00:00
http://arxiv.org/abs/1707.04476v5,Collaborative Nested Sampling: Big Data vs. complex physical models,"The data torrent unleashed by current and upcoming astronomical surveys
demands scalable analysis methods. Many machine learning approaches scale well,
but separating the instrument measurement from the physical effects of
interest, dealing with variable errors, and deriving parameter uncertainties is
often an after-thought. Classic forward-folding analyses with Markov Chain
Monte Carlo or Nested Sampling enable parameter estimation and model
comparison, even for complex and slow-to-evaluate physical models. However,
these approaches require independent runs for each data set, implying an
unfeasible number of model evaluations in the Big Data regime. Here I present a
new algorithm, collaborative nested sampling, for deriving parameter
probability distributions for each observation. Importantly, the number of
physical model evaluations scales sub-linearly with the number of data sets,
and no assumptions about homogeneous errors, Gaussianity, the form of the model
or heterogeneity/completeness of the observations need to be made.
Collaborative nested sampling has immediate application in speeding up analyses
of large surveys, integral-field-unit observations, and Monte Carlo
simulations.",['Johannes Buchner'],"['stat.CO', 'astro-ph.IM', 'physics.data-an', 'stat.ML']",2017-07-14 12:07:22+00:00
http://arxiv.org/abs/1707.04385v1,f-GANs in an Information Geometric Nutshell,"Nowozin \textit{et al} showed last year how to extend the GAN
\textit{principle} to all $f$-divergences. The approach is elegant but falls
short of a full description of the supervised game, and says little about the
key player, the generator: for example, what does the generator actually
converge to if solving the GAN game means convergence in some space of
parameters? How does that provide hints on the generator's design and compare
to the flourishing but almost exclusively experimental literature on the
subject?
  In this paper, we unveil a broad class of distributions for which such
convergence happens --- namely, deformed exponential families, a wide superset
of exponential families --- and show tight connections with the three other key
GAN parameters: loss, game and architecture. In particular, we show that
current deep architectures are able to factorize a very large number of such
densities using an especially compact design, hence displaying the power of
deep architectures and their concinnity in the $f$-GAN game. This result holds
given a sufficient condition on \textit{activation functions} --- which turns
out to be satisfied by popular choices. The key to our results is a variational
generalization of an old theorem that relates the KL divergence between regular
exponential families and divergences between their natural parameters. We
complete this picture with additional results and experimental insights on how
these results may be used to ground further improvements of GAN architectures,
via (i) a principled design of the activation functions in the generator and
(ii) an explicit integration of proper composite losses' link function in the
discriminator.","['Richard Nock', 'Zac Cranko', 'Aditya Krishna Menon', 'Lizhen Qu', 'Robert C. Williamson']","['cs.LG', 'stat.ML', 'I.2.6; I.5.1']",2017-07-14 05:07:52+00:00
http://arxiv.org/abs/1707.04368v1,"Kernel Method for Detecting Higher Order Interactions in multi-view Data: An Application to Imaging, Genetics, and Epigenetics","In this study, we tested the interaction effect of multimodal datasets using
a novel method called the kernel method for detecting higher order interactions
among biologically relevant mulit-view data. Using a semiparametric method on a
reproducing kernel Hilbert space (RKHS), we used a standard mixed-effects
linear model and derived a score-based variance component statistic that tests
for higher order interactions between multi-view data. The proposed method
offers an intangible framework for the identification of higher order
interaction effects (e.g., three way interaction) between genetics, brain
imaging, and epigenetic data. Extensive numerical simulation studies were first
conducted to evaluate the performance of this method. Finally, this method was
evaluated using data from the Mind Clinical Imaging Consortium (MCIC) including
single nucleotide polymorphism (SNP) data, functional magnetic resonance
imaging (fMRI) scans, and deoxyribonucleic acid (DNA) methylation data,
respectfully, in schizophrenia patients and healthy controls. We treated each
gene-derived SNPs, region of interest (ROI) and gene-derived DNA methylation as
a single testing unit, which are combined into triplets for evaluation. In
addition, cardiovascular disease risk factors such as age, gender, and body
mass index were assessed as covariates on hippocampal volume and compared
between triplets. Our method identified $13$-triplets ($p$-values $\leq 0.001$)
that included $6$ gene-derived SNPs, $10$ ROIs, and $6$ gene-derived DNA
methylations that correlated with changes in hippocampal volume, suggesting
that these triplets may be important in explaining schizophrenia-related
neurodegeneration. With strong evidence ($p$-values $\leq 0.000001$), the
triplet ({\bf MAGI2, CRBLCrus1.L, FBXO28}) has the potential to distinguish
schizophrenia patients from the healthy control variations.","['Md. Ashad Alam', 'Hui-Yi Lin', 'Vince Calhoun', 'Yu-Ping Wang']",['stat.ML'],2017-07-14 02:13:12+00:00
http://arxiv.org/abs/1707.04347v1,Weakly Submodular Maximization Beyond Cardinality Constraints: Does Randomization Help Greedy?,"Submodular functions are a broad class of set functions, which naturally
arise in diverse areas. Many algorithms have been suggested for the
maximization of these functions. Unfortunately, once the function deviates from
submodularity, the known algorithms may perform arbitrarily poorly. Amending
this issue, by obtaining approximation results for set functions generalizing
submodular functions, has been the focus of recent works.
  One such class, known as weakly submodular functions, has received a lot of
attention. A key result proved by Das and Kempe (2011) showed that the
approximation ratio of the greedy algorithm for weakly submodular maximization
subject to a cardinality constraint degrades smoothly with the distance from
submodularity. However, no results have been obtained for maximization subject
to constraints beyond cardinality. In particular, it is not known whether the
greedy algorithm achieves any non-trivial approximation ratio for such
constraints.
  In this paper, we prove that a randomized version of the greedy algorithm
(previously used by Buchbinder et al. (2014) for a different problem) achieves
an approximation ratio of $(1 + 1/\gamma)^{-2}$ for the maximization of a
weakly submodular function subject to a general matroid constraint, where
$\gamma$ is a parameter measuring the distance of the function from
submodularity. Moreover, we also experimentally compare the performance of this
version of the greedy algorithm on real world problems against natural
benchmarks, and show that the algorithm we study performs well also in
practice. To the best of our knowledge, this is the first algorithm with a
non-trivial approximation guarantee for maximizing a weakly submodular function
subject to a constraint other than the simple cardinality constraint. In
particular, it is the first algorithm with such a guarantee for the important
and broad class of matroid constraints.","['Lin Chen', 'Moran Feldman', 'Amin Karbasi']","['cs.DM', 'cs.AI', 'cs.DS', 'cs.LG', 'stat.ML']",2017-07-13 22:48:43+00:00
http://arxiv.org/abs/1707.04327v1,Human-Level Intelligence or Animal-Like Abilities?,"The vision systems of the eagle and the snake outperform everything that we
can make in the laboratory, but snakes and eagles cannot build an eyeglass or a
telescope or a microscope. (Judea Pearl)",['Adnan Darwiche'],"['cs.AI', 'cs.CY', 'cs.LG', 'stat.ML']",2017-07-13 21:17:14+00:00
http://arxiv.org/abs/1707.04319v1,"Model compression as constrained optimization, with application to neural nets. Part II: quantization","We consider the problem of deep neural net compression by quantization: given
a large, reference net, we want to quantize its real-valued weights using a
codebook with $K$ entries so that the training loss of the quantized net is
minimal. The codebook can be optimally learned jointly with the net, or fixed,
as for binarization or ternarization approaches. Previous work has quantized
the weights of the reference net, or incorporated rounding operations in the
backpropagation algorithm, but this has no guarantee of converging to a
loss-optimal, quantized net. We describe a new approach based on the recently
proposed framework of model compression as constrained optimization
\citep{Carreir17a}. This results in a simple iterative ""learning-compression""
algorithm, which alternates a step that learns a net of continuous weights with
a step that quantizes (or binarizes/ternarizes) the weights, and is guaranteed
to converge to local optimum of the loss for quantized nets. We develop
algorithms for an adaptive codebook or a (partially) fixed codebook. The latter
includes binarization, ternarization, powers-of-two and other important
particular cases. We show experimentally that we can achieve much higher
compression rates than previous quantization work (even using just 1 bit per
weight) with negligible loss degradation.","['Miguel Á. Carreira-Perpiñán', 'Yerlan Idelbayev']","['cs.LG', 'cs.NE', 'math.OC', 'stat.ML']",2017-07-13 20:58:40+00:00
http://arxiv.org/abs/1707.04314v1,Bayesian Optimization for Probabilistic Programs,"We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.","['Tom Rainforth', 'Tuan Anh Le', 'Jan-Willem van de Meent', 'Michael A. Osborne', 'Frank Wood']","['stat.ML', 'cs.AI', 'cs.PL', 'stat.CO']",2017-07-13 20:49:29+00:00
http://arxiv.org/abs/1707.05147v1,Comparative Study of Inference Methods for Bayesian Nonnegative Matrix Factorisation,"In this paper, we study the trade-offs of different inference approaches for
Bayesian matrix factorisation methods, which are commonly used for predicting
missing values, and for finding patterns in the data. In particular, we
consider Bayesian nonnegative variants of matrix factorisation and
tri-factorisation, and compare non-probabilistic inference, Gibbs sampling,
variational Bayesian inference, and a maximum-a-posteriori approach. The
variational approach is new for the Bayesian nonnegative models. We compare
their convergence, and robustness to noise and sparsity of the data, on both
synthetic and real-world datasets. Furthermore, we extend the models with the
Bayesian automatic relevance determination prior, allowing the models to
perform automatic model selection, and demonstrate its efficiency.","['Thomas Brouwer', 'Jes Frellsen', 'Pietro Lió']","['stat.ML', 'cs.LG']",2017-07-13 18:24:55+00:00
http://arxiv.org/abs/1707.04236v1,Improving Sparsity in Kernel Adaptive Filters Using a Unit-Norm Dictionary,"Kernel adaptive filters, a class of adaptive nonlinear time-series models,
are known by their ability to learn expressive autoregressive patterns from
sequential data. However, for trivial monotonic signals, they struggle to
perform accurate predictions and at the same time keep computational complexity
within desired boundaries. This is because new observations are incorporated to
the dictionary when they are far from what the algorithm has seen in the past.
We propose a novel approach to kernel adaptive filtering that compares new
observations against dictionary samples in terms of their unit-norm
(normalised) versions, meaning that new observations that look like previous
samples but have a different magnitude are not added to the dictionary. We
achieve this by proposing the unit-norm Gaussian kernel and define a
sparsification criterion for this novel kernel. This new methodology is
validated on two real-world datasets against standard KAF in terms of the
normalised mean square error and the dictionary size.",['Felipe Tobar'],"['stat.ML', 'cs.LG']",2017-07-13 17:37:46+00:00
http://arxiv.org/abs/1707.04218v1,Learning Features from Co-occurrences: A Theoretical Analysis,"Representing a word by its co-occurrences with other words in context is an
effective way to capture the meaning of the word. However, the theory behind
remains a challenge. In this work, taking the example of a word classification
task, we give a theoretical analysis of the approaches that represent a word X
by a function f(P(C|X)), where C is a context feature, P(C|X) is the
conditional probability estimated from a text corpus, and the function f maps
the co-occurrence measure to a prediction score. We investigate the impact of
context feature C and the function f. We also explain the reasons why using the
co-occurrences with multiple context features may be better than just using a
single one. In addition, some of the results shed light on the theory of
feature learning and machine learning in general.",['Yanpeng Li'],"['cs.CL', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2017-07-13 16:46:50+00:00
http://arxiv.org/abs/1707.04191v4,Distributionally Ambiguous Optimization Techniques for Batch Bayesian Optimization,"We propose a novel, theoretically-grounded, acquisition function for Batch
Bayesian optimization informed by insights from distributionally ambiguous
optimization. Our acquisition function is a lower bound on the well-known
Expected Improvement function, which requires evaluation of a Gaussian
Expectation over a multivariate piecewise affine function. Our bound is
computed instead by evaluating the best-case expectation over all probability
distributions consistent with the same mean and variance as the original
Gaussian distribution. Unlike alternative approaches, including Expected
Improvement, our proposed acquisition function avoids multi-dimensional
integrations entirely, and can be computed exactly - even on large batch sizes
- as the solution of a tractable convex optimization problem. Our suggested
acquisition function can also be optimized efficiently, since first and second
derivative information can be calculated inexpensively as by-products of the
acquisition function calculation itself. We derive various novel theorems that
ground our work theoretically and we demonstrate superior performance via
simple motivating examples, benchmark functions and real-world problems.","['Nikitas Rontsis', 'Michael A. Osborne', 'Paul J. Goulart']",['stat.ML'],2017-07-13 16:04:14+00:00
http://arxiv.org/abs/1707.04175v1,Distral: Robust Multitask Reinforcement Learning,"Most deep reinforcement learning algorithms are data inefficient in complex
and rich environments, limiting their applicability to many scenarios. One
direction for improving data efficiency is multitask learning with shared
neural network parameters, where efficiency may be improved through transfer
across related tasks. In practice, however, this is not usually observed,
because gradients from different tasks can interfere negatively, making
learning unstable and sometimes even less data efficient. Another issue is the
different reward schemes between tasks, which can easily lead to one task
dominating the learning of a shared model. We propose a new approach for joint
training of multiple tasks, which we refer to as Distral (Distill & transfer
learning). Instead of sharing parameters between the different workers, we
propose to share a ""distilled"" policy that captures common behaviour across
tasks. Each worker is trained to solve its own task while constrained to stay
close to the shared policy, while the shared policy is trained by distillation
to be the centroid of all task policies. Both aspects of the learning process
are derived by optimizing a joint objective function. We show that our approach
supports efficient transfer on complex 3D environments, outperforming several
related methods. Moreover, the proposed learning process is more robust and
more stable---attributes that are critical in deep reinforcement learning.","['Yee Whye Teh', 'Victor Bapst', 'Wojciech Marian Czarnecki', 'John Quan', 'James Kirkpatrick', 'Raia Hadsell', 'Nicolas Heess', 'Razvan Pascanu']","['cs.LG', 'stat.ML']",2017-07-13 15:24:20+00:00
http://arxiv.org/abs/1707.04131v3,Foolbox: A Python toolbox to benchmark the robustness of machine learning models,"Even todays most advanced machine learning models are easily fooled by almost
imperceptible perturbations of their inputs. Foolbox is a new Python package to
generate such adversarial perturbations and to quantify and compare the
robustness of machine learning models. It is build around the idea that the
most comparable robustness measure is the minimum perturbation needed to craft
an adversarial example. To this end, Foolbox provides reference implementations
of most published adversarial attack methods alongside some new ones, all of
which perform internal hyperparameter tuning to find the minimum adversarial
perturbation. Additionally, Foolbox interfaces with most popular deep learning
frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows
different adversarial criteria such as targeted misclassification and top-k
misclassification as well as different distance measures. The code is licensed
under the MIT license and is openly available at
https://github.com/bethgelab/foolbox . The most up-to-date documentation can be
found at http://foolbox.readthedocs.io .","['Jonas Rauber', 'Wieland Brendel', 'Matthias Bethge']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2017-07-13 13:59:15+00:00
http://arxiv.org/abs/1707.04114v3,Inferring the parameters of a Markov process from snapshots of the steady state,"We seek to infer the parameters of an ergodic Markov process from samples
taken independently from the steady state. Our focus is on non-equilibrium
processes, where the steady state is not described by the Boltzmann measure,
but is generally unknown and hard to compute, which prevents the application of
established equilibrium inference methods. We propose a quantity we call
propagator likelihood, which takes on the role of the likelihood in equilibrium
processes. This propagator likelihood is based on fictitious transitions
between those configurations of the system which occur in the samples. The
propagator likelihood can be derived by minimising the relative entropy between
the empirical distribution and a distribution generated by propagating the
empirical distribution forward in time. Maximising the propagator likelihood
leads to an efficient reconstruction of the parameters of the underlying model
in different systems, both with discrete configurations and with continuous
configurations. We apply the method to non-equilibrium models from statistical
physics and theoretical biology, including the asymmetric simple exclusion
process (ASEP), the kinetic Ising model, and replicator dynamics.","['Simon Lee Dettmer', 'Johannes Berg']","['cond-mat.stat-mech', 'cond-mat.dis-nn', 'math.PR', 'physics.data-an', 'stat.ML']",2017-07-13 13:31:20+00:00
http://arxiv.org/abs/1707.04067v1,Automation of Feature Engineering for IoT Analytics,"This paper presents an approach for automation of interpretable feature
selection for Internet Of Things Analytics (IoTA) using machine learning (ML)
techniques. Authors have conducted a survey over different people involved in
different IoTA based application development tasks. The survey reveals that
feature selection is the most time consuming and niche skill demanding part of
the entire workflow. This paper shows how feature selection is successfully
automated without sacrificing the decision making accuracy and thereby reducing
the project completion time and cost of hiring expensive resources. Several
pattern recognition principles and state of art (SoA) ML techniques are
followed to design the overall approach for the proposed automation. Three data
sets are considered to establish the proof-of-concept. Experimental results
show that the proposed automation is able to reduce the time for feature
selection to $2$ days instead of $4-6$ months which would have been required in
absence of the automation. This reduction in time is achieved without any
sacrifice in the accuracy of the decision making process. Proposed method is
also compared against Multi Layer Perceptron (MLP) model as most of the state
of the art works on IoTA uses MLP based Deep Learning. Moreover the feature
selection method is compared against SoA feature reduction technique namely
Principal Component Analysis (PCA) and its variants. The results obtained show
that the proposed method is effective.","['Snehasis Banerjee', 'Tanushyam Chattopadhyay', 'Arpan Pal', 'Utpal Garain']",['stat.ML'],2017-07-13 11:14:54+00:00
http://arxiv.org/abs/1707.04035v2,Kafnets: kernel-based non-parametric activation functions for neural networks,"Neural networks are generally built by interleaving (adaptable) linear layers
with (fixed) nonlinear activation functions. To increase their flexibility,
several authors have proposed methods for adapting the activation functions
themselves, endowing them with varying degrees of flexibility. None of these
approaches, however, have gained wide acceptance in practice, and research in
this topic remains open. In this paper, we introduce a novel family of flexible
activation functions that are based on an inexpensive kernel expansion at every
neuron. Leveraging over several properties of kernel-based models, we propose
multiple variations for designing and initializing these kernel activation
functions (KAFs), including a multidimensional scheme allowing to nonlinearly
combine information from different paths in the network. The resulting KAFs can
approximate any mapping defined over a subset of the real line, either convex
or nonconvex. Furthermore, they are smooth over their entire domain, linear in
their parameters, and they can be regularized using any known scheme, including
the use of $\ell_1$ penalties to enforce sparseness. To the best of our
knowledge, no other known model satisfies all these properties simultaneously.
In addition, we provide a relatively complete overview on alternative
techniques for adapting the activation functions, which is currently lacking in
the literature. A large set of experiments validates our proposal.","['Simone Scardapane', 'Steven Van Vaerenbergh', 'Simone Totaro', 'Aurelio Uncini']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.NE']",2017-07-13 09:22:01+00:00
http://arxiv.org/abs/1707.04025v1,"On Measuring and Quantifying Performance: Error Rates, Surrogate Loss, and an Example in SSL","In various approaches to learning, notably in domain adaptation, active
learning, learning under covariate shift, semi-supervised learning, learning
with concept drift, and the like, one often wants to compare a baseline
classifier to one or more advanced (or at least different) strategies. In this
chapter, we basically argue that if such classifiers, in their respective
training phases, optimize a so-called surrogate loss that it may also be
valuable to compare the behavior of this loss on the test set, next to the
regular classification error rate. It can provide us with an additional view on
the classifiers' relative performances that error rates cannot capture. As an
example, limited but convincing empirical results demonstrates that we may be
able to find semi-supervised learning strategies that can guarantee performance
improvements with increasing numbers of unlabeled data in terms of
log-likelihood. In contrast, the latter may be impossible to guarantee for the
classification error rate.","['Marco Loog', 'Jesse H. Krijthe', 'Are C. Jensen']","['cs.LG', 'cs.CV', 'stat.ML']",2017-07-13 08:29:00+00:00
http://arxiv.org/abs/1707.03916v1,Large Scale Variable Fidelity Surrogate Modeling,"Engineers widely use Gaussian process regression framework to construct
surrogate models aimed to replace computationally expensive physical models
while exploring design space. Thanks to Gaussian process properties we can use
both samples generated by a high fidelity function (an expensive and accurate
representation of a physical phenomenon) and a low fidelity function (a cheap
and coarse approximation of the same physical phenomenon) while constructing a
surrogate model. However, if samples sizes are more than few thousands of
points, computational costs of the Gaussian process regression become
prohibitive both in case of learning and in case of prediction calculation. We
propose two approaches to circumvent this computational burden: one approach is
based on the Nystr\""om approximation of sample covariance matrices and another
is based on an intelligent usage of a blackbox that can evaluate a~low fidelity
function on the fly at any point of a design space. We examine performance of
the proposed approaches using a number of artificial and real problems,
including engineering optimization of a rotating disk shape.","['Evgeny Burnaev', 'Alexey Zaytsev']","['stat.ML', 'stat.AP', 'stat.ME']",2017-07-12 21:21:32+00:00
http://arxiv.org/abs/1707.03909v1,Model Selection for Anomaly Detection,"Anomaly detection based on one-class classification algorithms is broadly
used in many applied domains like image processing (e.g. detection of whether a
patient is ""cancerous"" or ""healthy"" from mammography image), network intrusion
detection, etc. Performance of an anomaly detection algorithm crucially depends
on a kernel, used to measure similarity in a feature space. The standard
approaches (e.g. cross-validation) for kernel selection, used in two-class
classification problems, can not be used directly due to the specific nature of
a data (absence of a second, abnormal, class data). In this paper we generalize
several kernel selection methods from binary-class case to the case of
one-class classification and perform extensive comparison of these approaches
using both synthetic and real-world data.","['Evgeny Burnaev', 'Pavel Erofeev', 'Dmitry Smolyakov']","['stat.ML', 'cs.LG', 'stat.AP']",2017-07-12 21:03:36+00:00
http://arxiv.org/abs/1707.03905v1,Influence of Resampling on Accuracy of Imbalanced Classification,"In many real-world binary classification tasks (e.g. detection of certain
objects from images), an available dataset is imbalanced, i.e., it has much
less representatives of a one class (a minor class), than of another.
Generally, accurate prediction of the minor class is crucial but it's hard to
achieve since there is not much information about the minor class. One approach
to deal with this problem is to preliminarily resample the dataset, i.e., add
new elements to the dataset or remove existing ones. Resampling can be done in
various ways which raises the problem of choosing the most appropriate one. In
this paper we experimentally investigate impact of resampling on classification
accuracy, compare resampling methods and highlight key points and difficulties
of resampling.","['Evgeny Burnaev', 'Pavel Erofeev', 'Artem Papanov']","['stat.ML', 'cs.LG', 'stat.AP']",2017-07-12 20:55:22+00:00
http://arxiv.org/abs/1707.03897v2,ClustGeo: an R package for hierarchical clustering with spatial constraints,"In this paper, we propose a Ward-like hierarchical clustering algorithm
including spatial/geographical constraints. Two dissimilarity matrices $D_0$
and $D_1$ are inputted, along with a mixing parameter $\alpha \in [0,1]$. The
dissimilarities can be non-Euclidean and the weights of the observations can be
non-uniform. The first matrix gives the dissimilarities in the ""feature space""
and the second matrix gives the dissimilarities in the ""constraint space"". The
criterion minimized at each stage is a convex combination of the homogeneity
criterion calculated with $D_0$ and the homogeneity criterion calculated with
$D_1$. The idea is then to determine a value of $\alpha$ which increases the
spatial contiguity without deteriorating too much the quality of the solution
based on the variables of interest i.e. those of the feature space. This
procedure is illustrated on a real dataset using the R package ClustGeo.","['Marie Chavent', 'Vanessa Kuentz-Simonet', 'Amaury Labenne', 'Jérôme Saracco']","['stat.CO', 'stat.ML']",2017-07-12 20:29:59+00:00
http://arxiv.org/abs/1707.03858v3,Gradient Coding from Cyclic MDS Codes and Expander Graphs,"Gradient coding is a technique for straggler mitigation in distributed
learning. In this paper we design novel gradient codes using tools from
classical coding theory, namely, cyclic MDS codes, which compare favorably with
existing solutions, both in the applicable range of parameters and in the
complexity of the involved algorithms. Second, we introduce an approximate
variant of the gradient coding problem, in which we settle for approximate
gradient computation instead of the exact one. This approach enables graceful
degradation, i.e., the $\ell_2$ error of the approximate gradient is a
decreasing function of the number of stragglers. Our main result is that
normalized adjacency matrices of expander graphs yield excellent approximate
gradient codes, which enable significantly less computation compared to exact
gradient coding, and guarantee faster convergence than trivial solutions under
standard assumptions. We experimentally test our approach on Amazon EC2, and
show that the generalization error of approximate gradient coding is very close
to the full gradient while requiring significantly less computation from the
workers.","['Netanel Raviv', 'Itzhak Tamo', 'Rashish Tandon', 'Alexandros G. Dimakis']","['cs.IT', 'math.IT', 'stat.ML']",2017-07-12 18:38:54+00:00
http://arxiv.org/abs/1707.03854v1,Estimating the unseen from multiple populations,"Given samples from a distribution, how many new elements should we expect to
find if we continue sampling this distribution? This is an important and
actively studied problem, with many applications ranging from unseen species
estimation to genomics. We generalize this extrapolation and related unseen
estimation problems to the multiple population setting, where population $j$
has an unknown distribution $D_j$ from which we observe $n_j$ samples. We
derive an optimal estimator for the total number of elements we expect to find
among new samples across the populations. Surprisingly, we prove that our
estimator's accuracy is independent of the number of populations. We also
develop an efficient optimization algorithm to solve the more general problem
of estimating multi-population frequency distributions. We validate our methods
and theory through extensive experiments. Finally, on a real dataset of human
genomes across multiple ancestries, we demonstrate how our approach for unseen
estimation can enable cohort designs that can discover interesting mutations
with greater efficiency.","['Aditi Raghunathan', 'Greg Valiant', 'James Zou']","['cs.LG', 'stat.ML']",2017-07-12 18:26:19+00:00
http://arxiv.org/abs/1707.03815v4,Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking,"Methods that learn representations of nodes in a graph play a critical role
in network analysis since they enable many downstream learning tasks. We
propose Graph2Gauss - an approach that can efficiently learn versatile node
embeddings on large scale (attributed) graphs that show strong performance on
tasks such as link prediction and node classification. Unlike most approaches
that represent nodes as point vectors in a low-dimensional continuous space, we
embed each node as a Gaussian distribution, allowing us to capture uncertainty
about the representation. Furthermore, we propose an unsupervised method that
handles inductive learning scenarios and is applicable to different types of
graphs: plain/attributed, directed/undirected. By leveraging both the network
structure and the associated node attributes, we are able to generalize to
unseen nodes without additional training. To learn the embeddings we adopt a
personalized ranking formulation w.r.t. the node distances that exploits the
natural ordering of the nodes imposed by the network structure. Experiments on
real world networks demonstrate the high performance of our approach,
outperforming state-of-the-art network embedding methods on several different
tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by
analyzing it we can estimate neighborhood diversity and detect the intrinsic
latent dimensionality of a graph.","['Aleksandar Bojchevski', 'Stephan Günnemann']","['stat.ML', 'cs.LG', 'cs.SI']",2017-07-12 17:54:04+00:00
http://arxiv.org/abs/1707.03821v1,Process Monitoring on Sequences of System Call Count Vectors,"We introduce a methodology for efficient monitoring of processes running on
hosts in a corporate network. The methodology is based on collecting streams of
system calls produced by all or selected processes on the hosts, and sending
them over the network to a monitoring server, where machine learning algorithms
are used to identify changes in process behavior due to malicious activity,
hardware failures, or software errors. The methodology uses a sequence of
system call count vectors as the data format which can handle large and varying
volumes of data.
  Unlike previous approaches, the methodology introduced in this paper is
suitable for distributed collection and processing of data in large corporate
networks. We evaluate the methodology both in a laboratory setting on a
real-life setup and provide statistics characterizing performance and accuracy
of the methodology.","['Michael Dymshits', 'Ben Myara', 'David Tolpin']","['cs.CR', 'cs.LG', 'stat.ML']",2017-07-12 13:14:43+00:00
http://arxiv.org/abs/1707.03663v7,Underdamped Langevin MCMC: A non-asymptotic analysis,"We study the underdamped Langevin diffusion when the log of the target
distribution is smooth and strongly concave. We present a MCMC algorithm based
on its discretization and show that it achieves $\varepsilon$ error (in
2-Wasserstein distance) in $\mathcal{O}(\sqrt{d}/\varepsilon)$ steps. This is a
significant improvement over the best known rate for overdamped Langevin MCMC,
which is $\mathcal{O}(d/\varepsilon^2)$ steps under the same
smoothness/concavity assumptions.
  The underdamped Langevin MCMC scheme can be viewed as a version of
Hamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped
Langevin MCMC methods in a number of application areas. We provide quantitative
rates that support this empirical wisdom.","['Xiang Cheng', 'Niladri S. Chatterji', 'Peter L. Bartlett', 'Michael I. Jordan']","['stat.ML', 'cs.LG', 'stat.CO']",2017-07-12 12:08:55+00:00
http://arxiv.org/abs/1707.03538v1,An Introduction to the Practical and Theoretical Aspects of Mixture-of-Experts Modeling,"Mixture-of-experts (MoE) models are a powerful paradigm for modeling of data
arising from complex data generating processes (DGPs). In this article, we
demonstrate how different MoE models can be constructed to approximate the
underlying DGPs of arbitrary types of data. Due to the probabilistic nature of
MoE models, we propose the maximum quasi-likelihood (MQL) estimator as a method
for estimating MoE model parameters from data, and we provide conditions under
which MQL estimators are consistent and asymptotically normal. The blockwise
minorization-maximizatoin (blockwise-MM) algorithm framework is proposed as an
all-purpose method for constructing algorithms for obtaining MQL estimators. An
example derivation of a blockwise-MM algorithm is provided. We then present a
method for constructing information criteria for estimating the number of
components in MoE models and provide justification for the classic Bayesian
information criterion (BIC). We explain how MoE models can be used to conduct
classification, clustering, and regression and we illustrate these applications
via a pair of worked examples.","['Hien D. Nguyen', 'Faicel Chamroukhi']","['stat.ML', 'cs.LG']",2017-07-12 04:44:14+00:00
http://arxiv.org/abs/1707.03530v2,A Cluster Elastic Net for Multivariate Regression,"We propose a method for estimating coefficients in multivariate regression
when there is a clustering structure to the response variables. The proposed
method includes a fusion penalty, to shrink the difference in fitted values
from responses in the same cluster, and an L1 penalty for simultaneous variable
selection and estimation. The method can be used when the grouping structure of
the response variables is known or unknown. When the clustering structure is
unknown the method will simultaneously estimate the clusters of the response
and the regression coefficients. Theoretical results are presented for the
penalized least squares case, including asymptotic results allowing for p >> n.
We extend our method to the setting where the responses are binomial variables.
We propose a coordinate descent algorithm for both the normal and binomial
likelihood, which can easily be extended to other generalized linear model
(GLM) settings. Simulations and data examples from business operations and
genomics are presented to show the merits of both the least squares and
binomial methods.","['Bradley S. Price', 'Ben Sherwood']",['stat.ML'],2017-07-12 03:50:38+00:00
http://arxiv.org/abs/1707.03494v2,Unsupervised robust nonparametric learning of hidden community properties,"We consider learning of fundamental properties of communities in large noisy
networks, in the prototypical situation where the nodes or users are split into
two classes according to a binary property, e.g., according to their opinions
or preferences on a topic. For learning these properties, we propose a
nonparametric, unsupervised, and scalable graph scan procedure that is, in
addition, robust against a class of powerful adversaries. In our setup, one of
the communities can fall under the influence of a knowledgeable adversarial
leader, who knows the full network structure, has unlimited computational
resources and can completely foresee our planned actions on the network. We
prove strong consistency of our results in this setup with minimal assumptions.
In particular, the learning procedure estimates the baseline activity of normal
users asymptotically correctly with probability 1; the only assumption being
the existence of a single implicit community of asymptotically negligible
logarithmic size. We provide experiments on real and synthetic data to
illustrate the performance of our method, including examples with adversaries.","['Mikhail A. Langovoy', 'Akhilesh Gotmare', 'Martin Jaggi']","['stat.ML', 'cs.SI', 'stat.CO', 'stat.ME']",2017-07-11 23:28:52+00:00
http://arxiv.org/abs/1707.03490v1,Detecting Policy Preferences and Dynamics in the UN General Debate with Neural Word Embeddings,"Foreign policy analysis has been struggling to find ways to measure policy
preferences and paradigm shifts in international political systems. This paper
presents a novel, potential solution to this challenge, through the application
of a neural word embedding (Word2vec) model on a dataset featuring speeches by
heads of state or government in the United Nations General Debate. The paper
provides three key contributions based on the output of the Word2vec model.
First, it presents a set of policy attention indices, synthesizing the semantic
proximity of political speeches to specific policy themes. Second, it
introduces country-specific semantic centrality indices, based on topological
analyses of countries' semantic positions with respect to each other. Third, it
tests the hypothesis that there exists a statistical relation between the
semantic content of political speeches and UN voting behavior, falsifying it
and suggesting that political speeches contain information of different nature
then the one behind voting outcomes. The paper concludes with a discussion of
the practical use of its results and consequences for foreign policy analysis,
public accountability, and transparency.","['Stefano Gurciullo', 'Slava Mikhaylov']","['cs.CL', 'cs.AI', 'stat.ML']",2017-07-11 23:16:20+00:00
http://arxiv.org/abs/1707.03450v1,Initialising Kernel Adaptive Filters via Probabilistic Inference,"We present a probabilistic framework for both (i) determining the initial
settings of kernel adaptive filters (KAFs) and (ii) constructing fully-adaptive
KAFs whereby in addition to weights and dictionaries, kernel parameters are
learnt sequentially. This is achieved by formulating the estimator as a
probabilistic model and defining dedicated prior distributions over the kernel
parameters, weights and dictionary, enforcing desired properties such as
sparsity. The model can then be trained using a subset of data to initialise
standard KAFs or updated sequentially each time a new observation becomes
available. Due to the nonlinear/non-Gaussian properties of the model, learning
and inference is achieved using gradient-based maximum-a-posteriori
optimisation and Markov chain Monte Carlo methods, and can be confidently used
to compute predictions. The proposed framework was validated on nonlinear time
series of both synthetic and real-world nature, where it outperformed standard
KAFs in terms of mean square error and the sparsity of the learnt dictionaries.","['Iván Castro', 'Cristóbal Silva', 'Felipe Tobar']","['stat.ML', 'cs.LG']",2017-07-11 20:03:31+00:00
http://arxiv.org/abs/1707.03426v1,Multi-Task Learning Using Neighborhood Kernels,"This paper introduces a new and effective algorithm for learning kernels in a
Multi-Task Learning (MTL) setting. Although, we consider a MTL scenario here,
our approach can be easily applied to standard single task learning, as well.
As shown by our empirical results, our algorithm consistently outperforms the
traditional kernel learning algorithms such as uniform combination solution,
convex combinations of base kernels as well as some kernel alignment-based
models, which have been proven to give promising results in the past. We
present a Rademacher complexity bound based on which a new Multi-Task Multiple
Kernel Learning (MT-MKL) model is derived. In particular, we propose a Support
Vector Machine-regularized model in which, for each task, an optimal kernel is
learned based on a neighborhood-defining kernel that is not restricted to be
positive semi-definite. Comparative experimental results are showcased that
underline the merits of our neighborhood-defining framework in both
classification and regression problems.","['Niloofar Yousefi', 'Cong Li', 'Mansooreh Mollaghasemi', 'Georgios Anagnostopoulos', 'Michael Georgiopoulos']","['cs.LG', 'stat.ML']",2017-07-11 18:43:41+00:00
http://arxiv.org/abs/1707.03389v3,SCAN: Learning Hierarchical Compositional Visual Concepts,"The seemingly infinite diversity of the natural world arises from a
relatively small set of coherent rules, such as the laws of physics or
chemistry. We conjecture that these rules give rise to regularities that can be
discovered through primarily unsupervised experiences and represented as
abstract concepts. If such representations are compositional and hierarchical,
they can be recombined into an exponentially large set of new concepts. This
paper describes SCAN (Symbol-Concept Association Network), a new framework for
learning such abstractions in the visual domain. SCAN learns concepts through
fast symbol association, grounding them in disentangled visual primitives that
are discovered in an unsupervised manner. Unlike state of the art multimodal
generative model baselines, our approach requires very few pairings between
symbols and images and makes no assumptions about the form of symbol
representations. Once trained, SCAN is capable of multimodal bi-directional
inference, generating a diverse set of image samples from symbolic descriptions
and vice versa. It also allows for traversal and manipulation of the implicit
hierarchy of visual concepts through symbolic instructions and learnt logical
recombination operations. Such manipulations enable SCAN to break away from its
training data distribution and imagine novel visual concepts through
symbolically instructed recombination of previously learnt concepts.","['Irina Higgins', 'Nicolas Sonnerat', 'Loic Matthey', 'Arka Pal', 'Christopher P Burgess', 'Matko Bosnjak', 'Murray Shanahan', 'Matthew Botvinick', 'Demis Hassabis', 'Alexander Lerchner']","['stat.ML', 'cs.LG']",2017-07-11 17:58:45+00:00
http://arxiv.org/abs/1707.03386v1,DeepCodec: Adaptive Sensing and Recovery via Deep Convolutional Neural Networks,"In this paper we develop a novel computational sensing framework for sensing
and recovering structured signals. When trained on a set of representative
signals, our framework learns to take undersampled measurements and recover
signals from them using a deep convolutional neural network. In other words, it
learns a transformation from the original signals to a near-optimal number of
undersampled measurements and the inverse transformation from measurements to
signals. This is in contrast to traditional compressive sensing (CS) systems
that use random linear measurements and convex optimization or iterative
algorithms for signal recovery. We compare our new framework with
$\ell_1$-minimization from the phase transition point of view and demonstrate
that it outperforms $\ell_1$-minimization in the regions of phase transition
plot where $\ell_1$-minimization cannot recover the exact solution. In
addition, we experimentally demonstrate how learning measurements enhances the
overall recovery performance, speeds up training of recovery framework, and
leads to having fewer parameters to learn.","['Ali Mousavi', 'Gautam Dasarathy', 'Richard G. Baraniuk']","['stat.ML', 'cs.LG']",2017-07-11 17:49:20+00:00
http://arxiv.org/abs/1707.03384v1,Deep Learning-Based Communication Over the Air,"End-to-end learning of communications systems is a fascinating novel concept
that has so far only been validated by simulations for block-based
transmissions. It allows learning of transmitter and receiver implementations
as deep neural networks (NNs) that are optimized for an arbitrary
differentiable end-to-end performance metric, e.g., block error rate (BLER). In
this paper, we demonstrate that over-the-air transmissions are possible: We
build, train, and run a complete communications system solely composed of NNs
using unsynchronized off-the-shelf software-defined radios (SDRs) and
open-source deep learning (DL) software libraries. We extend the existing ideas
towards continuous data transmission which eases their current restriction to
short block lengths but also entails the issue of receiver synchronization. We
overcome this problem by introducing a frame synchronization module based on
another NN. A comparison of the BLER performance of the ""learned"" system with
that of a practical baseline shows competitive performance close to 1 dB, even
without extensive hyperparameter tuning. We identify several practical
challenges of training such a system over actual channels, in particular the
missing channel gradient, and propose a two-step learning procedure based on
the idea of transfer learning that circumvents this issue.","['Sebastian Dörner', 'Sebastian Cammerer', 'Jakob Hoydis', 'Stephan ten Brink']","['stat.ML', 'cs.IT', 'math.IT']",2017-07-11 17:47:23+00:00
http://arxiv.org/abs/1707.03383v1,A step towards procedural terrain generation with GANs,"Procedural terrain generation for video games has been traditionally been
done with smartly designed but handcrafted algorithms that generate heightmaps.
We propose a first step toward the learning and synthesis of these using recent
advances in deep generative modelling with openly available satellite imagery
from NASA.","['Christopher Beckham', 'Christopher Pal']","['stat.ML', 'cs.CV']",2017-07-11 17:44:20+00:00
http://arxiv.org/abs/1707.03372v1,Fast Amortized Inference and Learning in Log-linear Models with Randomly Perturbed Nearest Neighbor Search,"Inference in log-linear models scales linearly in the size of output space in
the worst-case. This is often a bottleneck in natural language processing and
computer vision tasks when the output space is feasibly enumerable but very
large. We propose a method to perform inference in log-linear models with
sublinear amortized cost. Our idea hinges on using Gumbel random variable
perturbations and a pre-computed Maximum Inner Product Search data structure to
access the most-likely elements in sublinear amortized time. Our method yields
provable runtime and accuracy guarantees. Further, we present empirical
experiments on ImageNet and Word Embeddings showing significant speedups for
sampling, inference, and learning in log-linear models.","['Stephen Mussmann', 'Daniel Levy', 'Stefano Ermon']","['cs.LG', 'stat.ML']",2017-07-11 17:23:10+00:00
http://arxiv.org/abs/1707.03360v3,Unsupervised identification of rat behavioral motifs across timescales,"Behaviors of several laboratory animals can be modeled as sequences of
stereotyped behaviors, or behavioral motifs. However, identifying such motifs
is a challenging problem. Behaviors have a multi-scale structure: the animal
can be simultaneously performing a small-scale motif and a large-scale one
(e.g. \textit{chewing} and \textit{feeding}). Motifs are compositional: a
large-scale motif is a chain of smaller-scale ones, folded in (some behavioral)
space in a specific manner. We demonstrate an approach which captures these
structures, using rat locomotor data as an example. From the same dataset, we
used a preprocessing procedure to create different versions, each describing
motifs of a different scale. We then trained several Hidden Markov Models
(HMMs) in parallel, one for each dataset version. This approach essentially
forced each HMM to learn motifs on a different scale, allowing us to capture
behavioral structures lost in previous approaches. By comparing HMMs with
models representing different null hypotheses, we found that rat locomotion was
composed of distinct motifs from second scale to minute scale. We found that
transitions between motifs were modulated by rats' location in the environment,
leading to non-Markovian transitions. To test the ethological relevance of
motifs we discovered, we compared their usage between rats with differences in
a high-level trait, prosociality. We found that these rats had distinct motif
repertoires, suggesting that motif usage statistics can be used to infer
internal states of rats. Our method is therefore an efficient way to discover
multi-scale, compositional structures in animal behaviors. It may also be
applied as a sensitive assay for internal states.","['Haozhe Shan', 'Peggy Mason']","['q-bio.QM', 'stat.ML']",2017-07-11 16:55:48+00:00
http://arxiv.org/abs/1707.03324v2,Dynamic Stochastic Approximation for Multi-stage Stochastic Optimization,"In this paper, we consider multi-stage stochastic optimization problems with
convex objectives and conic constraints at each stage. We present a new
stochastic first-order method, namely the dynamic stochastic approximation
(DSA) algorithm, for solving these types of stochastic optimization problems.
We show that DSA can achieve an optimal ${\cal O}(1/\epsilon^4)$ rate of
convergence in terms of the total number of required scenarios when applied to
a three-stage stochastic optimization problem. We further show that this rate
of convergence can be improved to ${\cal O}(1/\epsilon^2)$ when the objective
function is strongly convex. We also discuss variants of DSA for solving more
general multi-stage stochastic optimization problems with the number of stages
$T > 3$. The developed DSA algorithms only need to go through the scenario tree
once in order to compute an $\epsilon$-solution of the multi-stage stochastic
optimization problem. As a result, the memory required by DSA only grows
linearly with respect to the number of stages. To the best of our knowledge,
this is the first time that stochastic approximation type methods are
generalized for multi-stage stochastic optimization with $T \ge 3$.","['Guanghui Lan', 'Zhiqiang Zhou']","['math.OC', 'cs.CC', 'cs.LG', 'stat.ML']",2017-07-11 15:29:55+00:00
http://arxiv.org/abs/1707.03194v3,Sensitivity Analysis for Mirror-Stratifiable Convex Functions,"This paper provides a set of sensitivity analysis and activity identification
results for a class of convex functions with a strong geometric structure, that
we coined ""mirror-stratifiable"". These functions are such that there is a
bijection between a primal and a dual stratification of the space into
partitioning sets, called strata. This pairing is crucial to track the strata
that are identifiable by solutions of parametrized optimization problems or by
iterates of optimization algorithms. This class of functions encompasses all
regularizers routinely used in signal and image processing, machine learning,
and statistics. We show that this ""mirror-stratifiable"" structure enjoys a nice
sensitivity theory, allowing us to study stability of solutions of optimization
problems to small perturbations, as well as activity identification of
first-order proximal splitting-type algorithms. Existing results in the
literature typically assume that, under a non-degeneracy condition, the active
set associated to a minimizer is stable to small perturbations and is
identified in finite time by optimization schemes. In contrast, our results do
not require any non-degeneracy assumption: in consequence, the optimal active
set is not necessarily stable anymore, but we are able to track precisely the
set of identifiable strata.We show that these results have crucial implications
when solving challenging ill-posed inverse problems via regularization, a
typical scenario where the non-degeneracy condition is not fulfilled. Our
theoretical results, illustrated by numerical simulations, allow to
characterize the instability behaviour of the regularized solutions, by
locating the set of all low-dimensional strata that can be potentially
identified by these solutions.","['Jalal Fadili', 'Jérôme Malick', 'Gabriel Peyré']","['math.OC', 'cs.CV', 'stat.ML', '65K05, 65K10, 90C25, 90C31']",2017-07-11 09:35:14+00:00
http://arxiv.org/abs/1707.03190v1,Accelerated Variance Reduced Stochastic ADMM,"Recently, many variance reduced stochastic alternating direction method of
multipliers (ADMM) methods (e.g.\ SAG-ADMM, SDCA-ADMM and SVRG-ADMM) have made
exciting progress such as linear convergence rates for strongly convex
problems. However, the best known convergence rate for general convex problems
is O(1/T) as opposed to O(1/T^2) of accelerated batch algorithms, where $T$ is
the number of iterations. Thus, there still remains a gap in convergence rates
between existing stochastic ADMM and batch algorithms. To bridge this gap, we
introduce the momentum acceleration trick for batch optimization into the
stochastic variance reduced gradient based ADMM (SVRG-ADMM), which leads to an
accelerated (ASVRG-ADMM) method. Then we design two different momentum term
update rules for strongly convex and general convex cases. We prove that
ASVRG-ADMM converges linearly for strongly convex problems. Besides having a
low per-iteration complexity as existing stochastic ADMM methods, ASVRG-ADMM
improves the convergence rate on general convex problems from O(1/T) to
O(1/T^2). Our experimental results show the effectiveness of ASVRG-ADMM.","['Yuanyuan Liu', 'Fanhua Shang', 'James Cheng']","['cs.LG', 'stat.ML']",2017-07-11 09:29:46+00:00
http://arxiv.org/abs/1707.03157v1,Efficient mixture model for clustering of sparse high dimensional binary data,"In this paper we propose a mixture model, SparseMix, for clustering of sparse
high dimensional binary data, which connects model-based with centroid-based
clustering. Every group is described by a representative and a probability
distribution modeling dispersion from this representative. In contrast to
classical mixture models based on EM algorithm, SparseMix:
  -is especially designed for the processing of sparse data,
  -can be efficiently realized by an on-line Hartigan optimization algorithm,
  -is able to automatically reduce unnecessary clusters.
  We perform extensive experimental studies on various types of data, which
confirm that SparseMix builds partitions with higher compatibility with
reference grouping than related methods. Moreover, constructed representatives
often better reveal the internal structure of data.","['Marek Śmieja', 'Krzysztof Hajto', 'Jacek Tabor']","['cs.LG', 'stat.ML']",2017-07-11 07:48:57+00:00
http://arxiv.org/abs/1707.03141v3,A Simple Neural Attentive Meta-Learner,"Deep neural networks excel in regimes with large amounts of data, but tend to
struggle when data is scarce or when they need to adapt quickly to changes in
the task. In response, recent work in meta-learning proposes training a
meta-learner on a distribution of similar tasks, in the hopes of generalization
to novel but related tasks by learning a high-level strategy that captures the
essence of the problem it is asked to solve. However, many recent meta-learning
approaches are extensively hand-designed, either using architectures
specialized to a particular application, or hard-coding algorithmic components
that constrain how the meta-learner solves the task. We propose a class of
simple and generic meta-learner architectures that use a novel combination of
temporal convolutions and soft attention; the former to aggregate information
from past experience and the latter to pinpoint specific pieces of information.
In the most extensive set of meta-learning experiments to date, we evaluate the
resulting Simple Neural AttentIve Learner (or SNAIL) on several
heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement
learning, SNAIL attains state-of-the-art performance by significant margins.","['Nikhil Mishra', 'Mostafa Rohaninejad', 'Xi Chen', 'Pieter Abbeel']","['cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']",2017-07-11 06:21:31+00:00
http://arxiv.org/abs/1707.03134v1,Least Square Variational Bayesian Autoencoder with Regularization,"In recent years Variation Autoencoders have become one of the most popular
unsupervised learning of complicated distributions.Variational Autoencoder
(VAE) provides more efficient reconstructive performance over a traditional
autoencoder. Variational auto enocders make better approximaiton than MCMC. The
VAE defines a generative process in terms of ancestral sampling through a
cascade of hidden stochastic layers. They are a directed graphic models.
Variational autoencoder is trained to maximise the variational lower bound.
Here we are trying maximise the likelihood and also at the same time we are
trying to make a good approximation of the data. Its basically trading of the
data log-likelihood and the KL divergence from the true posterior. This paper
describes the scenario in which we wish to find a point-estimate to the
parameters $\theta$ of some parametric model in which we generate each
observations by first sampling a local latent variable and then sampling the
associated observation. Here we use least square loss function with
regularization in the the reconstruction of the image, the least square loss
function was found to give better reconstructed images and had a faster
training time.",['Gautam Ramachandra'],"['stat.ML', 'cs.LG']",2017-07-11 05:24:01+00:00
http://arxiv.org/abs/1707.03017v5,Learning Visual Reasoning Without Strong Priors,"Achieving artificial visual reasoning - the ability to answer image-related
questions which require a multi-step, high-level process - is an important step
towards artificial general intelligence. This multi-modal task requires
learning a question-dependent, structured reasoning process over images from
language. Standard deep learning approaches tend to exploit biases in the data
rather than learn this underlying structure, while leading methods learn to
visually reason successfully but are hand-crafted for reasoning. We show that a
general-purpose, Conditional Batch Normalization approach achieves
state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%
error rate. We outperform the next best end-to-end method (4.5%) and even
methods that use extra supervision (3.1%). We probe our model to shed light on
how it reasons, showing it has learned a question-dependent, multi-step
process. Previous work has operated under the assumption that visual reasoning
calls for a specialized architecture, but we show that a general architecture
with proper conditioning can learn to visually reason effectively.","['Ethan Perez', 'Harm de Vries', 'Florian Strub', 'Vincent Dumoulin', 'Aaron Courville']","['cs.CV', 'cs.AI', 'cs.CL', 'stat.ML']",2017-07-10 18:49:28+00:00
http://arxiv.org/abs/1707.03010v1,Sparse inference of the drift of a high-dimensional Ornstein-Uhlenbeck process,"Given the observation of a high-dimensional Ornstein-Uhlenbeck (OU) process
in continuous time, we proceed to the inference of the drift parameter under a
row-sparsity assumption. Towards that aim, we consider the negative
log-likelihood of the process, penalized by an $\ell^1$-penalization (Lasso and
Adaptive Lasso). We provide both non-asymptotic and asymptotic results for this
procedure, by means of a sharp oracle inequality, and a limit theorem in the
long-time asymptotics, including asymptotic consistency for variable selection.
As a by-product, we point out the fact that for the Ornstein-Uhlenbeck process,
one does not need an assumption of restricted eigenvalue type in order to
derive fast rates for the Lasso, while it is well-known to be mandatory for
linear regression for instance. Numerical results illustrate the benefits of
this penalized procedure compared to standard maximum likelihood approaches
both on simulations and real-world financial data.","['Stéphane Gaïffas', 'Gustaw Matulewicz']","['stat.ML', '60G15, 62H12, 62M99']",2017-07-10 18:33:02+00:00
http://arxiv.org/abs/1707.03003v2,"Tick: a Python library for statistical learning, with a particular emphasis on time-dependent modelling","Tick is a statistical learning library for Python~3, with a particular
emphasis on time-dependent models, such as point processes, and tools for
generalized linear models and survival analysis. The core of the library is an
optimization module providing model computational classes, solvers and proximal
operators for regularization. tick relies on a C++ implementation and
state-of-the-art optimization algorithms to provide very fast computations in a
single node multi-core setting. Source code and documentation can be downloaded
from https://github.com/X-DataInitiative/tick","['Emmanuel Bacry', 'Martin Bompaire', 'Stéphane Gaïffas', 'Soren Poulsen']",['stat.ML'],2017-07-10 18:18:24+00:00
http://arxiv.org/abs/1707.02963v5,An Interactive Greedy Approach to Group Sparsity in High Dimensions,"Sparsity learning with known grouping structure has received considerable
attention due to wide modern applications in high-dimensional data analysis.
Although advantages of using group information have been well-studied by
shrinkage-based approaches, benefits of group sparsity have not been
well-documented for greedy-type methods, which much limits our understanding
and use of this important class of methods. In this paper, generalizing from a
popular forward-backward greedy approach, we propose a new interactive greedy
algorithm for group sparsity learning and prove that the proposed greedy-type
algorithm attains the desired benefits of group sparsity under high dimensional
settings. An estimation error bound refining other existing methods and a
guarantee for group support recovery are also established simultaneously. In
addition, we incorporate a general M-estimation framework and introduce an
interactive feature to allow extra algorithm flexibility without compromise in
theoretical properties. The promising use of our proposal is demonstrated
through numerical evaluations including a real industrial application in human
activity recognition at home. Supplementary materials for this article are
available online.","['Wei Qian', 'Wending Li', 'Yasuhiro Sogawa', 'Ryohei Fujimaki', 'Xitong Yang', 'Ji Liu']",['stat.ML'],2017-07-10 17:48:28+00:00
http://arxiv.org/abs/1707.02914v1,Low Dose CT Image Reconstruction With Learned Sparsifying Transform,"A major challenge in computed tomography (CT) is to reduce X-ray dose to a
low or even ultra-low level while maintaining the high quality of reconstructed
images. We propose a new method for CT reconstruction that combines penalized
weighted-least squares reconstruction (PWLS) with regularization based on a
sparsifying transform (PWLS-ST) learned from a dataset of numerous CT images.
We adopt an alternating algorithm to optimize the PWLS-ST cost function that
alternates between a CT image update step and a sparse coding step. We adopt a
relaxed linearized augmented Lagrangian method with ordered-subsets (relaxed
OS-LALM) to accelerate the CT image update step by reducing the number of
forward and backward projections. Numerical experiments on the XCAT phantom
show that for low dose levels, the proposed PWLS-ST method dramatically
improves the quality of reconstructed images compared to PWLS reconstruction
with a nonadaptive edge-preserving regularizer (PWLS-EP).","['Xuehang Zheng', 'Zening Lu', 'Saiprasad Ravishankar', 'Yong Long', 'Jeffrey A. Fessler']","['stat.ML', 'cs.LG']",2017-07-10 15:42:05+00:00
http://arxiv.org/abs/1707.02796v2,Semi-Supervised Haptic Material Recognition for Robots using Generative Adversarial Networks,"Material recognition enables robots to incorporate knowledge of material
properties into their interactions with everyday objects. For example, material
recognition opens up opportunities for clearer communication with a robot, such
as ""bring me the metal coffee mug"", and recognizing plastic versus metal is
crucial when using a microwave or oven. However, collecting labeled training
data with a robot is often more difficult than unlabeled data. We present a
semi-supervised learning approach for material recognition that uses generative
adversarial networks (GANs) with haptic features such as force, temperature,
and vibration. Our approach achieves state-of-the-art results and enables a
robot to estimate the material class of household objects with ~90% accuracy
when 92% of the training data are unlabeled. We explore how well this approach
can recognize the material of new objects and we discuss challenges facing
generalization. To motivate learning from unlabeled training data, we also
compare results against several common supervised learning classifiers. In
addition, we have released the dataset used for this work which consists of
time-series haptic measurements from a robot that conducted thousands of
interactions with 72 household objects.","['Zackory Erickson', 'Sonia Chernova', 'Charles C. Kemp']","['cs.RO', 'cs.LG', 'stat.ML']",2017-07-10 11:04:42+00:00
http://arxiv.org/abs/1707.02780v1,Block modelling in dynamic networks with non-homogeneous Poisson processes and exact ICL,"We develop a model in which interactions between nodes of a dynamic network
are counted by non homogeneous Poisson processes. In a block modelling
perspective, nodes belong to hidden clusters (whose number is unknown) and the
intensity functions of the counting processes only depend on the clusters of
nodes. In order to make inference tractable we move to discrete time by
partitioning the entire time horizon in which interactions are observed in
fixed-length time sub-intervals. First, we derive an exact integrated
classification likelihood criterion and maximize it relying on a greedy search
approach. This allows to estimate the memberships to clusters and the number of
clusters simultaneously. Then a maximum-likelihood estimator is developed to
estimate non parametrically the integrated intensities. We discuss the
over-fitting problems of the model and propose a regularized version solving
these issues. Experiments on real and simulated data are carried out in order
to assess the proposed methodology.","['Marco Corneli', 'Pierre Latouche', 'Fabrice Rossi']",['stat.ML'],2017-07-10 09:44:50+00:00
