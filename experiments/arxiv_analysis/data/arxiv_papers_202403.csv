id,title,abstract,authors,categories,date
http://arxiv.org/abs/2404.09101v1,Mixture of Experts Soften the Curse of Dimensionality in Operator Learning,"In this paper, we construct a mixture of neural operators (MoNOs) between
function spaces whose complexity is distributed over a network of expert neural
operators (NOs), with each NO satisfying parameter scaling restrictions. Our
main result is a \textit{distributed} universal approximation theorem
guaranteeing that any Lipschitz non-linear operator between $L^2([0,1]^d)$
spaces can be approximated uniformly over the Sobolev unit ball therein, to any
given $\varepsilon>0$ accuracy, by an MoNO while satisfying the constraint
that: each expert NO has a depth, width, and rank of
$\mathcal{O}(\varepsilon^{-1})$. Naturally, our result implies that the
required number of experts must be large, however, each NO is guaranteed to be
small enough to be loadable into the active memory of most computers for
reasonable accuracies $\varepsilon$. During our analysis, we also obtain new
quantitative expression rates for classical NOs approximating uniformly
continuous non-linear operators uniformly on compact subsets of $L^2([0,1]^d)$.","['Anastasis Kratsios', 'Takashi Furuya', 'Jose Antonio Lara Benitez', 'Matti Lassas', 'Maarten de Hoop']","['cs.LG', 'cs.AI', 'cs.NA', 'math.NA', 'stat.ML']",2024-04-13 23:20:16+00:00
http://arxiv.org/abs/2404.09053v1,ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine Learning Insights,"This paper presents a new Python library called Automated Learning for
Insightful Comparison and Evaluation (ALICE), which merges conventional feature
selection and the concept of inter-rater agreeability in a simple,
user-friendly manner to seek insights into black box Machine Learning models.
The framework is proposed following an overview of the key concepts of
interpretability in ML. The entire architecture and intuition of the main
methods of the framework are also thoroughly discussed and results from initial
experiments on a customer churn predictive modeling task are presented,
alongside ideas for possible avenues to explore for the future. The full source
code for the framework and the experiment notebooks can be found at:
https://github.com/anasashb/aliceHU","['Bachana Anasashvili', 'Vahidin Jeleskovic']","['cs.LG', 'cs.HC', 'stat.AP', 'stat.ML']",2024-04-13 17:34:58+00:00
http://arxiv.org/abs/2404.12396v1,Optimized Dynamic Mode Decomposition for Reconstruction and Forecasting of Atmospheric Chemistry Data,"We introduce the optimized dynamic mode decomposition algorithm for
constructing an adaptive and computationally efficient reduced order model and
forecasting tool for global atmospheric chemistry dynamics. By exploiting a
low-dimensional set of global spatio-temporal modes, interpretable
characterizations of the underlying spatial and temporal scales can be
computed. Forecasting is also achieved with a linear model that uses a linear
superposition of the dominant spatio-temporal features. The DMD method is
demonstrated on three months of global chemistry dynamics data, showing its
significant performance in computational speed and interpretability. We show
that the presented decomposition method successfully extracts known major
features of atmospheric chemistry, such as summertime surface pollution and
biomass burning activities. Moreover, the DMD algorithm allows for rapid
reconstruction of the underlying linear model, which can then easily
accommodate non-stationary data and changes in the dynamics.","['Meghana Velegar', 'Christoph Keller', 'J. Nathan Kutz']","['cs.LG', 'math.DS', 'physics.ao-ph', 'stat.AP', 'stat.ML']",2024-04-13 15:44:12+00:00
http://arxiv.org/abs/2404.08980v1,Stability and Generalization in Free Adversarial Training,"While adversarial training methods have resulted in significant improvements
in the deep neural nets' robustness against norm-bounded adversarial
perturbations, their generalization performance from training samples to test
data has been shown to be considerably worse than standard empirical risk
minimization methods. Several recent studies seek to connect the generalization
behavior of adversarially trained classifiers to various gradient-based min-max
optimization algorithms used for their training. In this work, we study the
generalization performance of adversarial training methods using the
algorithmic stability framework. Specifically, our goal is to compare the
generalization performance of the vanilla adversarial training scheme fully
optimizing the perturbations at every iteration vs. the free adversarial
training simultaneously optimizing the norm-bounded perturbations and
classifier parameters. Our proven generalization bounds indicate that the free
adversarial training method could enjoy a lower generalization gap between
training and test samples due to the simultaneous nature of its min-max
optimization algorithm. We perform several numerical experiments to evaluate
the generalization performance of vanilla, fast, and free adversarial training
methods. Our empirical findings also show the improved generalization
performance of the free adversarial training method and further demonstrate
that the better generalization result could translate to greater robustness
against black-box attack schemes. The code is available at
https://github.com/Xiwei-Cheng/Stability_FreeAT.","['Xiwei Cheng', 'Kexin Fu', 'Farzan Farnia']","['cs.LG', 'stat.ML']",2024-04-13 12:07:20+00:00
http://arxiv.org/abs/2404.08969v1,Concentration properties of fractional posterior in 1-bit matrix completion,"The problem of estimating a matrix based on a set of its observed entries is
commonly referred to as the matrix completion problem. In this work, we
specifically address the scenario of binary observations, often termed as 1-bit
matrix completion. While numerous studies have explored Bayesian and
frequentist methods for real-value matrix completion, there has been a lack of
theoretical exploration regarding Bayesian approaches in 1-bit matrix
completion. We tackle this gap by considering a general, non-uniform sampling
scheme and providing theoretical assurances on the efficacy of the fractional
posterior. Our contributions include obtaining concentration results for the
fractional posterior and demonstrating its effectiveness in recovering the
underlying parameter matrix. We accomplish this using two distinct types of
prior distributions: low-rank factorization priors and a spectral scaled
Student prior, with the latter requiring fewer assumptions. Importantly, our
results exhibit an adaptive nature by not mandating prior knowledge of the rank
of the parameter matrix. Our findings are comparable to those found in the
frequentist literature, yet demand fewer restrictive assumptions.",['The Tien Mai'],"['stat.ML', 'cs.LG']",2024-04-13 11:22:53+00:00
http://arxiv.org/abs/2404.08913v1,On the best approximation by finite Gaussian mixtures,"We consider the problem of approximating a general Gaussian location mixture
by finite mixtures. The minimum order of finite mixtures that achieve a
prescribed accuracy (measured by various $f$-divergences) is determined within
constant factors for the family of mixing distributions with compactly support
or appropriate assumptions on the tail probability including subgaussian and
subexponential. While the upper bound is achieved using the technique of local
moment matching, the lower bound is established by relating the best
approximation error to the low-rank approximation of certain trigonometric
moment matrices, followed by a refined spectral analysis of their minimum
eigenvalue. In the case of Gaussian mixing distributions, this result corrects
a previous lower bound in [Allerton Conference 48 (2010) 620-628].","['Yun Ma', 'Yihong Wu', 'Pengkun Yang']","['math.ST', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML', 'stat.TH']",2024-04-13 06:57:44+00:00
http://arxiv.org/abs/2404.08839v4,Multiply-Robust Causal Change Attribution,"Comparing two samples of data, we observe a change in the distribution of an
outcome variable. In the presence of multiple explanatory variables, how much
of the change can be explained by each possible cause? We develop a new
estimation strategy that, given a causal model, combines regression and
re-weighting methods to quantify the contribution of each causal mechanism. Our
proposed methodology is multiply robust, meaning that it still recovers the
target parameter under partial misspecification. We prove that our estimator is
consistent and asymptotically normal. Moreover, it can be incorporated into
existing frameworks for causal attribution, such as Shapley values, which will
inherit the consistency and large-sample distribution properties. Our method
demonstrates excellent performance in Monte Carlo simulations, and we show its
usefulness in an empirical application. Our method is implemented as part of
the Python library DoWhy (arXiv:2011.04216, arXiv:2206.06821).","['Victor Quintas-Martinez', 'Mohammad Taha Bahadori', 'Eduardo Santiago', 'Jeff Mu', 'Dominik Janzing', 'David Heckerman']","['stat.ME', 'cs.LG', 'econ.EM', 'stat.ML']",2024-04-12 22:57:01+00:00
http://arxiv.org/abs/2404.08809v1,Leveraging viscous Hamilton-Jacobi PDEs for uncertainty quantification in scientific machine learning,"Uncertainty quantification (UQ) in scientific machine learning (SciML)
combines the powerful predictive power of SciML with methods for quantifying
the reliability of the learned models. However, two major challenges remain:
limited interpretability and expensive training procedures. We provide a new
interpretation for UQ problems by establishing a new theoretical connection
between some Bayesian inference problems arising in SciML and viscous
Hamilton-Jacobi partial differential equations (HJ PDEs). Namely, we show that
the posterior mean and covariance can be recovered from the spatial gradient
and Hessian of the solution to a viscous HJ PDE. As a first exploration of this
connection, we specialize to Bayesian inference problems with linear models,
Gaussian likelihoods, and Gaussian priors. In this case, the associated viscous
HJ PDEs can be solved using Riccati ODEs, and we develop a new Riccati-based
methodology that provides computational advantages when continuously updating
the model predictions. Specifically, our Riccati-based approach can efficiently
add or remove data points to the training set invariant to the order of the
data and continuously tune hyperparameters. Moreover, neither update requires
retraining on or access to previously incorporated data. We provide several
examples from SciML involving noisy data and \textit{epistemic uncertainty} to
illustrate the potential advantages of our approach. In particular, this
approach's amenability to data streaming applications demonstrates its
potential for real-time inferences, which, in turn, allows for applications in
which the predicted uncertainty is used to dynamically alter the learning
process.","['Zongren Zou', 'Tingwei Meng', 'Paula Chen', 'Jérôme Darbon', 'George Em Karniadakis']","['cs.LG', 'stat.ML', '35F21, 62F15, 65L99, 65N99, 68T05, 35B37']",2024-04-12 20:54:01+00:00
http://arxiv.org/abs/2404.08803v2,Random walks on simplicial complexes,"The notion of Laplacian of a graph can be generalized to simplicial complexes
and hypergraphs, and contains information on the topology of these structures.
Even for a graph, the consideration of associated simplicial complexes is
interesting to understand its shape. Whereas the Laplacian of a graph has a
simple probabilistic interpretation as the generator of a continuous time
Markov chain on the graph, things are not so direct when considering simplicial
complexes. We define here new Markov chains on simplicial complexes. For a
given order~$k$, the state space is the set of $k$-cycles that are chains of
$k$-simplexes with null boundary. This new framework is a natural
generalization of the canonical Markov chains on graphs. We show that the
generator of our Markov chain is the upper Laplacian defined in the context of
algebraic topology for discrete structure. We establish several key properties
of this new process: in particular, when the number of vertices is finite, the
Markov chain is positive recurrent. This result is not trivial, since the
cycles can loop over themselves an unbounded number of times. We study the
diffusive limits when the simplicial complexes under scrutiny are a sequence of
ever refining triangulations of the flat torus. Using the analogy between
singular and Hodge homologies, we express this limit as valued in the set of
currents. The proof of tightness and the identification of the limiting
martingale problem make use of the flat norm and carefully controls of the
error terms in the convergence of the generator. Uniqueness of the solution to
the martingale problem is left open. An application to hole detection is
carried.","['Thomas Bonis', 'Laurent Decreusefond', 'Viet Chi Tran', 'Zhihan Iris Zhang']","['math.PR', 'stat.ML', '60D05']",2024-04-12 20:37:34+00:00
http://arxiv.org/abs/2404.08792v1,Convergence of coordinate ascent variational inference for log-concave measures via optimal transport,"Mean field variational inference (VI) is the problem of finding the closest
product (factorized) measure, in the sense of relative entropy, to a given
high-dimensional probability measure $\rho$. The well known Coordinate Ascent
Variational Inference (CAVI) algorithm aims to approximate this product measure
by iteratively optimizing over one coordinate (factor) at a time, which can be
done explicitly. Despite its popularity, the convergence of CAVI remains poorly
understood. In this paper, we prove the convergence of CAVI for log-concave
densities $\rho$. If additionally $\log \rho$ has Lipschitz gradient, we find a
linear rate of convergence, and if also $\rho$ is strongly log-concave, we find
an exponential rate. Our analysis starts from the observation that mean field
VI, while notoriously non-convex in the usual sense, is in fact displacement
convex in the sense of optimal transport when $\rho$ is log-concave. This
allows us to adapt techniques from the optimization literature on coordinate
descent algorithms in Euclidean space.","['Manuel Arnese', 'Daniel Lacker']","['stat.ML', 'cs.LG', 'math.OC', 'math.PR', 'math.ST', 'stat.TH']",2024-04-12 19:43:54+00:00
http://arxiv.org/abs/2404.08747v1,Observation-specific explanations through scattered data approximation,"This work introduces the definition of observation-specific explanations to
assign a score to each data point proportional to its importance in the
definition of the prediction process. Such explanations involve the
identification of the most influential observations for the black-box model of
interest. The proposed method involves estimating these explanations by
constructing a surrogate model through scattered data approximation utilizing
the orthogonal matching pursuit algorithm. The proposed approach is validated
on both simulated and real-world datasets.","['Valentina Ghidini', 'Michael Multerer', 'Jacopo Quizi', 'Rohan Sen']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.NA', 'math.NA']",2024-04-12 18:20:26+00:00
http://arxiv.org/abs/2404.08613v1,Using Explainable AI and Transfer Learning to understand and predict the maintenance of Atlantic blocking with limited observational data,"Blocking events are an important cause of extreme weather, especially
long-lasting blocking events that trap weather systems in place. The duration
of blocking events is, however, underestimated in climate models. Explainable
Artificial Intelligence are a class of data analysis methods that can help
identify physical causes of prolonged blocking events and diagnose model
deficiencies. We demonstrate this approach on an idealized quasigeostrophic
model developed by Marshall and Molteni (1993). We train a convolutional neural
network (CNN), and subsequently, build a sparse predictive model for the
persistence of Atlantic blocking, conditioned on an initial high-pressure
anomaly. Shapley Additive ExPlanation (SHAP) analysis reveals that
high-pressure anomalies in the American Southeast and North Atlantic, separated
by a trough over Atlantic Canada, contribute significantly to prediction of
sustained blocking events in the Atlantic region. This agrees with previous
work that identified precursors in the same regions via wave train analysis.
When we apply the same CNN to blockings in the ERA5 atmospheric reanalysis,
there is insufficient data to accurately predict persistent blocks. We
partially overcome this limitation by pre-training the CNN on the plentiful
data of the Marshall-Molteni model, and then using Transfer Learning to achieve
better predictions than direct training. SHAP analysis before and after
transfer learning allows a comparison between the predictive features in the
reanalysis and the quasigeostrophic model, quantifying dynamical biases in the
idealized model. This work demonstrates the potential for machine learning
methods to extract meaningful precursors of extreme weather events and achieve
better prediction using limited observational data.","['Huan Zhang', 'Justin Finkel', 'Dorian S. Abbot', 'Edwin P. Gerber', 'Jonathan Weare']","['physics.ao-ph', 'stat.ML']",2024-04-12 17:22:29+00:00
http://arxiv.org/abs/2404.08602v2,Sliding down the stairs: how correlated latent variables accelerate learning with neural networks,"Neural networks extract features from data using stochastic gradient descent
(SGD). In particular, higher-order input cumulants (HOCs) are crucial for their
performance. However, extracting information from the $p$th cumulant of
$d$-dimensional inputs is computationally hard: the number of samples required
to recover a single direction from an order-$p$ tensor (tensor PCA) using
online SGD grows as $d^{p-1}$, which is prohibitive for high-dimensional
inputs. This result raises the question of how neural networks extract relevant
directions from the HOCs of their inputs efficiently. Here, we show that
correlations between latent variables along the directions encoded in different
input cumulants speed up learning from higher-order correlations. We show this
effect analytically by deriving nearly sharp thresholds for the number of
samples required by a single neuron to weakly-recover these directions using
online SGD from a random start in high dimensions. Our analytical results are
confirmed in simulations of two-layer neural networks and unveil a new
mechanism for hierarchical learning in neural networks.","['Lorenzo Bardone', 'Sebastian Goldt']","['stat.ML', 'cond-mat.stat-mech', 'cs.LG', 'math.PR', 'math.ST', 'stat.TH']",2024-04-12 17:01:25+00:00
http://arxiv.org/abs/2404.08476v1,Combining Statistical Depth and Fermat Distance for Uncertainty Quantification,"We measure the Out-of-domain uncertainty in the prediction of Neural Networks
using a statistical notion called ``Lens Depth'' (LD) combined with Fermat
Distance, which is able to capture precisely the ``depth'' of a point with
respect to a distribution in feature space, without any assumption about the
form of distribution. Our method has no trainable parameter. The method is
applicable to any classification model as it is applied directly in feature
space at test time and does not intervene in training process. As such, it does
not impact the performance of the original model. The proposed method gives
excellent qualitative result on toy datasets and can give competitive or better
uncertainty estimation on standard deep learning datasets compared to strong
baseline methods.","['Hai-Vy Nguyen', 'Fabrice Gamboa', 'Reda Chhaibi', 'Sixin Zhang', 'Serge Gratton', 'Thierry Giaccone']","['stat.ML', 'cs.AI', 'cs.LG', 'math.PR', 'stat.AP']",2024-04-12 13:54:21+00:00
http://arxiv.org/abs/2404.08472v2,TSLANet: Rethinking Transformers for Time Series Representation Learning,"Time series data, characterized by its intrinsic long and short-range
dependencies, poses a unique challenge across analytical applications. While
Transformer-based models excel at capturing long-range dependencies, they face
limitations in noise sensitivity, computational efficiency, and overfitting
with smaller datasets. In response, we introduce a novel Time Series
Lightweight Adaptive Network (TSLANet), as a universal convolutional model for
diverse time series tasks. Specifically, we propose an Adaptive Spectral Block,
harnessing Fourier analysis to enhance feature representation and to capture
both long-term and short-term interactions while mitigating noise via adaptive
thresholding. Additionally, we introduce an Interactive Convolution Block and
leverage self-supervised learning to refine the capacity of TSLANet for
decoding complex temporal patterns and improve its robustness on different
datasets. Our comprehensive experiments demonstrate that TSLANet outperforms
state-of-the-art models in various tasks spanning classification, forecasting,
and anomaly detection, showcasing its resilience and adaptability across a
spectrum of noise levels and data sizes. The code is available at
https://github.com/emadeldeen24/TSLANet.","['Emadeldeen Eldele', 'Mohamed Ragab', 'Zhenghua Chen', 'Min Wu', 'Xiaoli Li']","['cs.LG', 'stat.ML']",2024-04-12 13:41:29+00:00
http://arxiv.org/abs/2404.08458v2,On the Independence Assumption in Neurosymbolic Learning,"State-of-the-art neurosymbolic learning systems use probabilistic reasoning
to guide neural networks towards predictions that conform to logical
constraints over symbols. Many such systems assume that the probabilities of
the considered symbols are conditionally independent given the input to
simplify learning and reasoning. We study and criticise this assumption,
highlighting how it can hinder optimisation and prevent uncertainty
quantification. We prove that loss functions bias conditionally independent
neural networks to become overconfident in their predictions. As a result, they
are unable to represent uncertainty over multiple valid options. Furthermore,
we prove that these loss functions are difficult to optimise: they are
non-convex, and their minima are usually highly disconnected. Our theoretical
analysis gives the foundation for replacing the conditional independence
assumption and designing more expressive neurosymbolic probabilistic models.","['Emile van Krieken', 'Pasquale Minervini', 'Edoardo M. Ponti', 'Antonio Vergari']","['stat.ML', 'cs.AI', 'cs.LG']",2024-04-12 13:09:48+00:00
http://arxiv.org/abs/2404.08717v1,State-Space Systems as Dynamic Generative Models,"A probabilistic framework to study the dependence structure induced by
deterministic discrete-time state-space systems between input and output
processes is introduced. General sufficient conditions are formulated under
which output processes exist and are unique once an input process has been
fixed, a property that in the deterministic state-space literature is known as
the echo state property. When those conditions are satisfied, the given
state-space system becomes a generative model for probabilistic dependences
between two sequence spaces. Moreover, those conditions guarantee that the
output depends continuously on the input when using the Wasserstein metric. The
output processes whose existence is proved are shown to be causal in a specific
sense and to generalize those studied in purely deterministic situations. The
results in this paper constitute a significant stochastic generalization of
sufficient conditions for the deterministic echo state property to hold, in the
sense that the stochastic echo state property can be satisfied under
contractivity conditions that are strictly weaker than those in deterministic
situations. This means that state-space systems can induce a purely
probabilistic dependence structure between input and output sequence spaces
even when there is no functional relation between those two spaces.","['Juan-Pablo Ortega', 'Florian Rossmannek']","['stat.ML', 'cs.LG', 'math.DS', 'math.PR', 'math.ST', 'stat.TH', '37H05, 37N35, 62M10, 68T05']",2024-04-12 07:32:57+00:00
http://arxiv.org/abs/2404.08278v2,Minimax Optimal Goodness-of-Fit Testing with Kernel Stein Discrepancy,"We explore the minimax optimality of goodness-of-fit tests on general domains
using the kernelized Stein discrepancy (KSD). The KSD framework offers a
flexible approach for goodness-of-fit testing, avoiding strong distributional
assumptions, accommodating diverse data structures beyond Euclidean spaces, and
relying only on partial knowledge of the reference distribution, while
maintaining computational efficiency. We establish a general framework and an
operator-theoretic representation of the KSD, encompassing many existing KSD
tests in the literature, which vary depending on the domain. We reveal the
characteristics and limitations of KSD and demonstrate its non-optimality under
a certain alternative space, defined over general domains when considering
$\chi^2$-divergence as the separation metric. To address this issue of
non-optimality, we propose a modified, minimax optimal test by incorporating a
spectral regularizer, thereby overcoming the shortcomings of standard KSD
tests. Our results are established under a weak moment condition on the Stein
kernel, which relaxes the bounded kernel assumption required by prior work in
the analysis of kernel-based hypothesis testing. Additionally, we introduce an
adaptive test capable of achieving minimax optimality up to a logarithmic
factor by adapting to unknown parameters. Through numerical experiments, we
illustrate the superior performance of our proposed tests across various
domains compared to their unregularized counterparts.","['Omar Hagrass', 'Bharath Sriperumbudur', 'Krishnakumar Balasubramanian']","['math.ST', 'stat.ML', 'stat.TH', 'Primary: 62G10, Secondary: 65J20, 65J22, 46E22, 47A52']",2024-04-12 07:06:12+00:00
http://arxiv.org/abs/2404.08715v1,Differentially Private Log-Location-Scale Regression Using Functional Mechanism,"This article introduces differentially private log-location-scale (DP-LLS)
regression models, which incorporate differential privacy into LLS regression
through the functional mechanism. The proposed models are established by
injecting noise into the log-likelihood function of LLS regression for
perturbed parameter estimation. We will derive the sensitivities utilized to
determine the magnitude of the injected noise and prove that the proposed
DP-LLS models satisfy $\epsilon$-differential privacy. In addition, we will
conduct simulations and case studies to evaluate the performance of the
proposed models. The findings suggest that predictor dimension, training sample
size, and privacy budget are three key factors impacting the performance of the
proposed DP-LLS regression models. Moreover, the results indicate that a
sufficiently large training dataset is needed to simultaneously ensure decent
performance of the proposed models and achieve a satisfactory level of privacy
protection.","['Jiewen Sheng', 'Xiaolei Fang']","['stat.ML', 'cs.CR', 'cs.LG', 'stat.AP']",2024-04-12 04:14:08+00:00
http://arxiv.org/abs/2404.08168v1,Conformal Prediction via Regression-as-Classification,"Conformal prediction (CP) for regression can be challenging, especially when
the output distribution is heteroscedastic, multimodal, or skewed. Some of the
issues can be addressed by estimating a distribution over the output, but in
reality, such approaches can be sensitive to estimation error and yield
unstable intervals.~Here, we circumvent the challenges by converting regression
to a classification problem and then use CP for classification to obtain CP
sets for regression.~To preserve the ordering of the continuous-output space,
we design a new loss function and make necessary modifications to the CP
classification techniques.~Empirical results on many benchmarks shows that this
simple approach gives surprisingly good results on many practical problems.","['Etash Guha', 'Shlok Natarajan', 'Thomas Möllenhoff', 'Mohammad Emtiyaz Khan', 'Eugene Ndiaye']","['cs.LG', 'stat.ML']",2024-04-12 00:21:30+00:00
http://arxiv.org/abs/2404.08164v2,Language Model Prompt Selection via Simulation Optimization,"With the advancement in generative language models, the selection of prompts
has gained significant attention in recent years. A prompt is an instruction or
description provided by the user, serving as a guide for the generative
language model in content generation. Despite existing methods for prompt
selection that are based on human labor, we consider facilitating this
selection through simulation optimization, aiming to maximize a pre-defined
score for the selected prompt. Specifically, we propose a two-stage framework.
In the first stage, we determine a feasible set of prompts in sufficient
numbers, where each prompt is represented by a moderate-dimensional vector. In
the subsequent stage for evaluation and selection, we construct a surrogate
model of the score regarding the moderate-dimensional vectors that represent
the prompts. We propose sequentially selecting the prompt for evaluation based
on this constructed surrogate model. We prove the consistency of the sequential
evaluation procedure in our framework. We also conduct numerical experiments to
demonstrate the efficacy of our proposed framework, providing practical
instructions for implementation.","['Haoting Zhang', 'Jinghai He', 'Rhonda Righter', 'Zeyu Zheng']","['stat.ML', 'cs.AI', 'cs.CL', 'cs.LG']",2024-04-12 00:03:56+00:00
http://arxiv.org/abs/2404.08136v2,Exponentially Weighted Moving Models,"An exponentially weighted moving model (EWMM) for a vector time series fits a
new data model each time period, based on an exponentially fading loss function
on past observed data. The well known and widely used exponentially weighted
moving average (EWMA) is a special case that estimates the mean using a square
loss function. For quadratic loss functions EWMMs can be fit using a simple
recursion that updates the parameters of a quadratic function. For other loss
functions, the entire past history must be stored, and the fitting problem
grows in size as time increases. We propose a general method for computing an
approximation of EWMM, which requires storing only a window of a fixed number
of past samples, and uses an additional quadratic term to approximate the loss
associated with the data before the window. This approximate EWMM relies on
convex optimization, and solves problems that do not grow with time. We compare
the estimates produced by our approximation with the estimates from the exact
EWMM method.","['Eric Luxenberg', 'Stephen Boyd']","['stat.CO', 'eess.SP', 'math.OC', 'q-fin.CP', 'stat.ML']",2024-04-11 21:45:39+00:00
http://arxiv.org/abs/2404.08131v1,Frame Quantization of Neural Networks,"We present a post-training quantization algorithm with error estimates
relying on ideas originating from frame theory. Specifically, we use
first-order Sigma-Delta ($\Sigma\Delta$) quantization for finite unit-norm
tight frames to quantize weight matrices and biases in a neural network. In our
scenario, we derive an error bound between the original neural network and the
quantized neural network in terms of step size and the number of frame
elements. We also demonstrate how to leverage the redundancy of frames to
achieve a quantized neural network with higher accuracy.","['Wojciech Czaja', 'Sanghoon Na']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2024-04-11 21:24:38+00:00
http://arxiv.org/abs/2404.08073v1,Spurious Stationarity and Hardness Results for Mirror Descent,"Despite the considerable success of Bregman proximal-type algorithms, such as
mirror descent, in machine learning, a critical question remains: Can existing
stationarity measures, often based on Bregman divergence, reliably distinguish
between stationary and non-stationary points? In this paper, we present a
groundbreaking finding: All existing stationarity measures necessarily imply
the existence of spurious stationary points. We further establish an
algorithmic independent hardness result: Bregman proximal-type algorithms are
unable to escape from a spurious stationary point in finite steps when the
initial point is unfavorable, even for convex problems. Our hardness result
points out the inherent distinction between Euclidean and Bregman geometries,
and introduces both fundamental theoretical and numerical challenges to both
machine learning and optimization communities.","['He Chen', 'Jiajin Li', 'Anthony Man-Cho So']","['math.OC', 'cs.LG', 'stat.ML']",2024-04-11 18:28:01+00:00
http://arxiv.org/abs/2404.07937v2,Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method,"We study the quadratic prediction error method -- i.e., nonlinear least
squares -- for a class of time-varying parametric predictor models satisfying a
certain identifiability condition. While this method is known to asymptotically
achieve the optimal rate for a wide range of problems, there have been no
non-asymptotic results matching these optimal rates outside of a select few,
typically linear, model classes. By leveraging modern tools from learning with
dependent data, we provide the first rate-optimal non-asymptotic analysis of
this method for our more general setting of nonlinearly parametrized model
classes. Moreover, we show that our results can be applied to a particular
class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in
the first optimal non-asymptotic rates for identification of ARMA models.","['Charis Stamouli', 'Ingvar Ziemann', 'George J. Pappas']","['math.ST', 'cs.LG', 'cs.SY', 'eess.SY', 'stat.ML', 'stat.TH']",2024-04-11 17:36:28+00:00
http://arxiv.org/abs/2404.07864v2,Inferring Change Points in High-Dimensional Regression via Approximate Message Passing,"We consider the problem of localizing change points in a generalized linear
model (GLM), a model that covers many widely studied problems in statistical
learning including linear, logistic, and rectified linear regression. We
propose a novel and computationally efficient Approximate Message Passing (AMP)
algorithm for estimating both the signals and the change point locations, and
rigorously characterize its performance in the high-dimensional limit where the
number of parameters $p$ is proportional to the number of samples $n$. This
characterization is in terms of a state evolution recursion, which allows us to
precisely compute performance measures such as the asymptotic Hausdorff error
of our change point estimates, and allows us to tailor the algorithm to take
advantage of any prior structural information on the signals and change points.
Moreover, we show how our AMP iterates can be used to efficiently compute a
Bayesian posterior distribution over the change point locations in the
high-dimensional limit. We validate our theory via numerical experiments, and
demonstrate the favorable performance of our estimators on both synthetic and
real data in the settings of linear, logistic, and rectified linear regression.","['Gabriel Arpino', 'Xiaoqi Liu', 'Julia Gontarek', 'Ramji Venkataramanan']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2024-04-11 15:57:12+00:00
http://arxiv.org/abs/2404.07849v1,Overparameterized Multiple Linear Regression as Hyper-Curve Fitting,"The paper shows that the application of the fixed-effect multiple linear
regression model to an overparameterized dataset is equivalent to fitting the
data with a hyper-curve parameterized by a single scalar parameter. This
equivalence allows for a predictor-focused approach, where each predictor is
described by a function of the chosen parameter. It is proven that a linear
model will produce exact predictions even in the presence of nonlinear
dependencies that violate the model assumptions. Parameterization in terms of
the dependent variable and the monomial basis in the predictor function space
are applied here to both synthetic and experimental data. The hyper-curve
approach is especially suited for the regularization of problems with noise in
predictor variables and can be used to remove noisy and ""improper"" predictors
from the model.","['E. Atza', 'N. Budko']","['stat.ML', 'cs.LG']",2024-04-11 15:43:11+00:00
http://arxiv.org/abs/2404.07815v2,Post-Hoc Reversal: Are We Selecting Models Prematurely?,"Trained models are often composed with post-hoc transforms such as
temperature scaling (TS), ensembling and stochastic weight averaging (SWA) to
improve performance, robustness, uncertainty estimation, etc. However, such
transforms are typically applied only after the base models have already been
finalized by standard means. In this paper, we challenge this practice with an
extensive empirical study. In particular, we demonstrate a phenomenon that we
call post-hoc reversal, where performance trends are reversed after applying
post-hoc transforms. This phenomenon is especially prominent in high-noise
settings. For example, while base models overfit badly early in training, both
ensembling and SWA favor base models trained for more epochs. Post-hoc reversal
can also prevent the appearance of double descent and mitigate mismatches
between test loss and test error seen in base models. Preliminary analyses
suggest that these transforms induce reversal by suppressing the influence of
mislabeled examples, exploiting differences in their learning dynamics from
those of clean examples. Based on our findings, we propose post-hoc selection,
a simple technique whereby post-hoc metrics inform model development decisions
such as early stopping, checkpointing, and broader hyperparameter choices. Our
experiments span real-world vision, language, tabular and graph datasets. On an
LLM instruction tuning dataset, post-hoc selection results in >1.5x MMLU
improvement compared to naive selection.","['Rishabh Ranjan', 'Saurabh Garg', 'Mrigank Raman', 'Carlos Guestrin', 'Zachary Lipton']","['cs.LG', 'cs.AI', 'stat.ML']",2024-04-11 14:58:19+00:00
http://arxiv.org/abs/2404.07778v1,Quality check of a sample partition using multinomial distribution,"In this paper, we advocate a novel measure for the purpose of checking the
quality of a cluster partition for a sample into several distinct classes, and
thus, determine the unknown value for the true number of clusters prevailing
the provided set of data. Our objective leads us to the development of an
approach through applying the multinomial distribution to the distances of data
members, clustered in a group, from their respective cluster representatives.
This procedure is carried out independently for each of the clusters, and the
concerned statistics are combined together to design our targeted measure.
Individual clusters separately possess the category-wise probabilities which
correspond to different positions of its members in the cluster with respect to
a typical member, in the form of cluster-centroid, medoid or mode, referred to
as the corresponding cluster representative. Our method is robust in the sense
that it is distribution-free, since this is devised irrespective of the parent
distribution of the underlying sample. It fulfills one of the rare coveted
qualities, present in the existing cluster accuracy measures, of having the
capability to investigate whether the assigned sample owns any inherent
clusters other than a single group of all members or not. Our measure's simple
concept, easy algorithm, fast runtime, good performance, and wide usefulness,
demonstrated through extensive simulation and diverse case-studies, make it
appealing.",['Soumita Modak'],"['stat.AP', 'stat.ML', '62H30']",2024-04-11 14:14:58+00:00
http://arxiv.org/abs/2404.07771v1,"An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization","Diffusion models, a powerful and universal generative AI technology, have
achieved tremendous success in computer vision, audio, reinforcement learning,
and computational biology. In these applications, diffusion models provide
flexible high-dimensional data modeling, and act as a sampler for generating
new samples under active guidance towards task-desired properties. Despite the
significant empirical success, theory of diffusion models is very limited,
potentially slowing down principled methodological innovations for further
harnessing and improving diffusion models. In this paper, we review emerging
applications of diffusion models, understanding their sample generation under
various controls. Next, we overview the existing theories of diffusion models,
covering their statistical properties and sampling capabilities. We adopt a
progressive routine, beginning with unconditional diffusion models and
connecting to conditional counterparts. Further, we review a new avenue in
high-dimensional structured optimization through conditional diffusion models,
where searching for solutions is reformulated as a conditional sampling problem
and solved by diffusion models. Lastly, we discuss future directions about
diffusion models. The purpose of this paper is to provide a well-rounded
theoretical exposure for stimulating forward-looking theories and methods of
diffusion models.","['Minshuo Chen', 'Song Mei', 'Jianqing Fan', 'Mengdi Wang']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2024-04-11 14:07:25+00:00
http://arxiv.org/abs/2404.07724v2,Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models,"Guidance is a crucial technique for extracting the best performance out of
image-generating diffusion models. Traditionally, a constant guidance weight
has been applied throughout the sampling chain of an image. We show that
guidance is clearly harmful toward the beginning of the chain (high noise
levels), largely unnecessary toward the end (low noise levels), and only
beneficial in the middle. We thus restrict it to a specific range of noise
levels, improving both the inference speed and result quality. This limited
guidance interval improves the record FID in ImageNet-512 significantly, from
1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial
across different sampler parameters, network architectures, and datasets,
including the large-scale setting of Stable Diffusion XL. We thus suggest
exposing the guidance interval as a hyperparameter in all diffusion models that
use guidance.","['Tuomas Kynkäänniemi', 'Miika Aittala', 'Tero Karras', 'Samuli Laine', 'Timo Aila', 'Jaakko Lehtinen']","['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']",2024-04-11 13:16:47+00:00
http://arxiv.org/abs/2404.07662v1,PINNACLE: PINN Adaptive ColLocation and Experimental points selection,"Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft
constraints, train with a composite loss function that contains multiple
training point types: different types of collocation points chosen during
training to enforce each PDE and initial/boundary conditions, and experimental
points which are usually costly to obtain via experiments or simulations.
Training PINNs using this loss function is challenging as it typically requires
selecting large numbers of points of different types, each with different
training dynamics. Unlike past works that focused on the selection of either
collocation or experimental points, this work introduces PINN Adaptive
ColLocation and Experimental points selection (PINNACLE), the first algorithm
that jointly optimizes the selection of all training point types, while
automatically adjusting the proportion of collocation point types as training
progresses. PINNACLE uses information on the interaction among training point
types, which had not been considered before, based on an analysis of PINN
training dynamics via the Neural Tangent Kernel (NTK). We theoretically show
that the criterion used by PINNACLE is related to the PINN generalization
error, and empirically demonstrate that PINNACLE is able to outperform existing
point selection methods for forward, inverse, and transfer learning problems.","['Gregory Kang Ruey Lau', 'Apivich Hemachandra', 'See-Kiong Ng', 'Bryan Kian Hsiang Low']","['cs.LG', 'cs.AI', 'physics.comp-ph', 'physics.data-an', 'stat.ML']",2024-04-11 11:51:46+00:00
http://arxiv.org/abs/2404.07661v1,Robust performance metrics for imbalanced classification problems,"We show that established performance metrics in binary classification, such
as the F-score, the Jaccard similarity coefficient or Matthews' correlation
coefficient (MCC), are not robust to class imbalance in the sense that if the
proportion of the minority class tends to $0$, the true positive rate (TPR) of
the Bayes classifier under these metrics tends to $0$ as well. Thus, in
imbalanced classification problems, these metrics favour classifiers which
ignore the minority class. To alleviate this issue we introduce robust
modifications of the F-score and the MCC for which, even in strongly imbalanced
settings, the TPR is bounded away from $0$. We numerically illustrate the
behaviour of the various performance metrics in simulations as well as on a
credit default data set. We also discuss connections to the ROC and
precision-recall curves and give recommendations on how to combine their usage
with performance metrics.","['Hajo Holzmann', 'Bernhard Klar']","['stat.ML', 'cs.LG', 'stat.ME']",2024-04-11 11:50:05+00:00
http://arxiv.org/abs/2404.07593v2,Diffusion posterior sampling for simulation-based inference in tall data settings,"Determining which parameters of a non-linear model best describe a set of
experimental data is a fundamental problem in science and it has gained much
traction lately with the rise of complex large-scale simulators. The likelihood
of such models is typically intractable, which is why classical MCMC methods
can not be used. Simulation-based inference (SBI) stands out in this context by
only requiring a dataset of simulations to train deep generative models capable
of approximating the posterior distribution that relates input parameters to a
given observation. In this work, we consider a tall data extension in which
multiple observations are available to better infer the parameters of the
model. The proposed method is built upon recent developments from the
flourishing score-based diffusion literature and allows to estimate the tall
data posterior distribution, while simply using information from a score
network trained for a single context observation. We compare our method to
recently proposed competing approaches on various numerical experiments and
demonstrate its superiority in terms of numerical stability and computational
cost.","['Julia Linhart', 'Gabriel Victorino Cardoso', 'Alexandre Gramfort', 'Sylvain Le Corff', 'Pedro L. C. Rodrigues']","['stat.ML', 'cs.LG', 'stat.ME']",2024-04-11 09:23:36+00:00
http://arxiv.org/abs/2404.07559v1,Differentially Private Reinforcement Learning with Self-Play,"We study the problem of multi-agent reinforcement learning (multi-agent RL)
with differential privacy (DP) constraints. This is well-motivated by various
real-world applications involving sensitive data, where it is critical to
protect users' private information. We first extend the definitions of Joint DP
(JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where
both definitions ensure trajectory-wise privacy protection. Then we design a
provably efficient algorithm based on optimistic Nash value iteration and
privatization of Bernstein-type bonuses. The algorithm is able to satisfy JDP
and LDP requirements when instantiated with appropriate privacy mechanisms.
Furthermore, for both notions of DP, our regret bound generalizes the best
known result under the single-agent RL case, while our regret could also reduce
to the best known result for multi-agent RL without privacy constraints. To the
best of our knowledge, these are the first line of results towards
understanding trajectory-wise privacy protection in multi-agent RL.","['Dan Qiao', 'Yu-Xiang Wang']","['cs.LG', 'cs.AI', 'cs.CR', 'cs.MA', 'stat.ML']",2024-04-11 08:42:51+00:00
http://arxiv.org/abs/2404.15344v1,Adversarial Robustness of Distilled and Pruned Deep Learning-based Wireless Classifiers,"Data-driven deep learning (DL) techniques developed for automatic modulation
classification (AMC) of wireless signals are vulnerable to adversarial attacks.
This poses a severe security threat to the DL-based wireless systems,
specifically for edge applications of AMC. In this work, we address the joint
problem of developing optimized DL models that are also robust against
adversarial attacks. This enables efficient and reliable deployment of DL-based
AMC on edge devices. We first propose two optimized models using knowledge
distillation and network pruning, followed by a computationally efficient
adversarial training process to improve the robustness. Experimental results on
five white-box attacks show that the proposed optimized and adversarially
trained models can achieve better robustness than the standard (unoptimized)
model. The two optimized models also achieve higher accuracy on clean
(unattacked) samples, which is essential for the reliability of DL-based
solutions at edge applications.","['Nayan Moni Baishya', 'B. R. Manoj']","['eess.SP', 'cs.CR', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2024-04-11 06:15:01+00:00
http://arxiv.org/abs/2404.15343v1,Edge-Efficient Deep Learning Models for Automatic Modulation Classification: A Performance Analysis,"The recent advancement in deep learning (DL) for automatic modulation
classification (AMC) of wireless signals has encouraged numerous possible
applications on resource-constrained edge devices. However, developing
optimized DL models suitable for edge applications of wireless communications
is yet to be studied in depth. In this work, we perform a thorough
investigation of optimized convolutional neural networks (CNNs) developed for
AMC using the three most commonly used model optimization techniques: a)
pruning, b) quantization, and c) knowledge distillation. Furthermore, we have
proposed optimized models with the combinations of these techniques to fuse the
complementary optimization benefits. The performances of all the proposed
methods are evaluated in terms of sparsity, storage compression for network
parameters, and the effect on classification accuracy with a reduction in
parameters. The experimental results show that the proposed individual and
combined optimization techniques are highly effective for developing models
with significantly less complexity while maintaining or even improving
classification performance compared to the benchmark CNNs.","['Nayan Moni Baishya', 'B. R. Manoj', 'Prabin K. Bora']","['eess.SP', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2024-04-11 06:08:23+00:00
http://arxiv.org/abs/2404.07358v1,A solution for the mean parametrization of the von Mises-Fisher distribution,"The von Mises-Fisher distribution as an exponential family can be expressed
in terms of either its natural or its mean parameters. Unfortunately, however,
the normalization function for the distribution in terms of its mean parameters
is not available in closed form, limiting the practicality of the mean
parametrization and complicating maximum-likelihood estimation more generally.
We derive a second-order ordinary differential equation, the solution to which
yields the mean-parameter normalizer along with its first two derivatives, as
well as the variance function of the family. We also provide closed-form
approximations to the solution of the differential equation. This allows rapid
evaluation of both densities and natural parameters in terms of mean
parameters. We show applications to topic modeling with mixtures of von
Mises-Fisher distributions using Bregman Clustering.","['Marcel Nonnenmacher', 'Maneesh Sahani']","['stat.CO', 'stat.ML']",2024-04-10 21:28:54+00:00
http://arxiv.org/abs/2404.06993v1,Quiver Laplacians and Feature Selection,"The challenge of selecting the most relevant features of a given dataset
arises ubiquitously in data analysis and dimensionality reduction. However,
features found to be of high importance for the entire dataset may not be
relevant to subsets of interest, and vice versa. Given a feature selector and a
fixed decomposition of the data into subsets, we describe a method for
identifying selected features which are compatible with the decomposition into
subsets. We achieve this by re-framing the problem of finding compatible
features to one of finding sections of a suitable quiver representation. In
order to approximate such sections, we then introduce a Laplacian operator for
quiver representations valued in Hilbert spaces. We provide explicit bounds on
how the spectrum of a quiver Laplacian changes when the representation and the
underlying quiver are modified in certain natural ways. Finally, we apply this
machinery to the study of peak-calling algorithms which measure chromatin
accessibility in single-cell data. We demonstrate that eigenvectors of the
associated quiver Laplacian yield locally and globally compatible features.","['Otto Sumray', 'Heather A. Harrington', 'Vidit Nanda']","['stat.ML', 'cs.LG', 'math.CO', 'math.RT', 'math.ST', 'q-bio.QM', 'stat.TH', '16G20, 05C50, 62P05, 62H25']",2024-04-10 13:12:07+00:00
http://arxiv.org/abs/2404.06978v1,The CAST package for training and assessment of spatial prediction models in R,"One key task in environmental science is to map environmental variables
continuously in space or even in space and time. Machine learning algorithms
are frequently used to learn from local field observations to make spatial
predictions by estimating the value of the variable of interest in places where
it has not been measured. However, the application of machine learning
strategies for spatial mapping involves additional challenges compared to
""non-spatial"" prediction tasks that often originate from spatial
autocorrelation and from training data that are not independent and identically
distributed.
  In the past few years, we developed a number of methods to support the
application of machine learning for spatial data which involves the development
of suitable cross-validation strategies for performance assessment and model
selection, spatial feature selection, and methods to assess the area of
applicability of the trained models. The intention of the CAST package is to
support the application of machine learning strategies for predictive mapping
by implementing such methods and making them available for easy integration
into modelling workflows.
  Here we introduce the CAST package and its core functionalities. At the case
study of mapping plant species richness, we will go through the different steps
of the modelling workflow and show how CAST can be used to support more
reliable spatial predictions.","['Hanna Meyer', 'Marvin Ludwig', 'Carles Milà', 'Jan Linnenbrink', 'Fabian Schumacher']","['stat.ML', 'cs.LG', 'q-bio.QM']",2024-04-10 12:48:10+00:00
http://arxiv.org/abs/2404.06969v2,FiP: a Fixed-Point Approach for Causal Generative Modeling,"Modeling true world data-generating processes lies at the heart of empirical
science. Structural Causal Models (SCMs) and their associated Directed Acyclic
Graphs (DAGs) provide an increasingly popular answer to such problems by
defining the causal generative process that transforms random noise into
observations. However, learning them from observational data poses an ill-posed
and NP-hard inverse problem in general. In this work, we propose a new and
equivalent formalism that does not require DAGs to describe them, viewed as
fixed-point problems on the causally ordered variables, and we show three
important cases where they can be uniquely recovered given the topological
ordering (TO). To the best of our knowledge, we obtain the weakest conditions
for their recovery when TO is known. Based on this, we design a two-stage
causal generative model that first infers the causal order from observations in
a zero-shot manner, thus by-passing the search, and then learns the generative
fixed-point SCM on the ordered variables. To infer TOs from observations, we
propose to amortize the learning of TOs on generated datasets by sequentially
predicting the leaves of graphs seen during training. To learn fixed-point
SCMs, we design a transformer-based architecture that exploits a new attention
mechanism enabling the modeling of causal structures, and show that this
parameterization is consistent with our formalism. Finally, we conduct an
extensive evaluation of each method individually, and show that when combined,
our model outperforms various baselines on generated out-of-distribution
problems.","['Meyer Scetbon', 'Joel Jennings', 'Agrin Hilmkil', 'Cheng Zhang', 'Chao Ma']","['cs.LG', 'stat.ML']",2024-04-10 12:29:05+00:00
http://arxiv.org/abs/2404.06735v1,A Copula Graphical Model for Multi-Attribute Data using Optimal Transport,"Motivated by modern data forms such as images and multi-view data, the
multi-attribute graphical model aims to explore the conditional independence
structure among vectors. Under the Gaussian assumption, the conditional
independence between vectors is characterized by blockwise zeros in the
precision matrix. To relax the restrictive Gaussian assumption, in this paper,
we introduce a novel semiparametric multi-attribute graphical model based on a
new copula named Cyclically Monotone Copula. This new copula treats the
distribution of the node vectors as multivariate marginals and transforms them
into Gaussian distributions based on the optimal transport theory. Since the
model allows the node vectors to have arbitrary continuous distributions, it is
more flexible than the classical Gaussian copula method that performs
coordinatewise Gaussianization. We establish the concentration inequalities of
the estimated covariance matrices and provide sufficient conditions for
selection consistency of the group graphical lasso estimator. For the setting
with high-dimensional attributes, a {Projected Cyclically Monotone Copula}
model is proposed to address the curse of dimensionality issue that arises from
solving high-dimensional optimal transport problems. Numerical results based on
synthetic and real data show the efficiency and flexibility of our methods.","['Qi Zhang', 'Bing Li', 'Lingzhou Xue']","['stat.ML', 'cs.LG', 'math.ST', 'stat.AP', 'stat.ME', 'stat.TH']",2024-04-10 04:49:00+00:00
http://arxiv.org/abs/2404.06720v1,Gradient Descent is Pareto-Optimal in the Oracle Complexity and Memory Tradeoff for Feasibility Problems,"In this paper we provide oracle complexity lower bounds for finding a point
in a given set using a memory-constrained algorithm that has access to a
separation oracle. We assume that the set is contained within the unit
$d$-dimensional ball and contains a ball of known radius $\epsilon>0$. This
setup is commonly referred to as the feasibility problem. We show that to solve
feasibility problems with accuracy $\epsilon \geq e^{-d^{o(1)}}$, any
deterministic algorithm either uses $d^{1+\delta}$ bits of memory or must make
at least $1/(d^{0.01\delta }\epsilon^{2\frac{1-\delta}{1+1.01 \delta}-o(1)})$
oracle queries, for any $\delta\in[0,1]$. Additionally, we show that randomized
algorithms either use $d^{1+\delta}$ memory or make at least $1/(d^{2\delta}
\epsilon^{2(1-4\delta)-o(1)})$ queries for any $\delta\in[0,\frac{1}{4}]$.
Because gradient descent only uses linear memory $\mathcal O(d\ln 1/\epsilon)$
but makes $\Omega(1/\epsilon^2)$ queries, our results imply that it is
Pareto-optimal in the oracle complexity/memory tradeoff. Further, our results
show that the oracle complexity for deterministic algorithms is always
polynomial in $1/\epsilon$ if the algorithm has less than quadratic memory in
$d$. This reveals a sharp phase transition since with quadratic $\mathcal O(d^2
\ln1/\epsilon)$ memory, cutting plane methods only require $\mathcal O(d\ln
1/\epsilon)$ queries.",['Moise Blanchard'],"['math.OC', 'cs.CC', 'cs.DS', 'cs.LG', 'stat.ML']",2024-04-10 04:15:50+00:00
http://arxiv.org/abs/2404.06549v1,Variational Stochastic Gradient Descent for Deep Neural Networks,"Optimizing deep neural networks is one of the main tasks in successful deep
learning. Current state-of-the-art optimizers are adaptive gradient-based
optimization methods such as Adam. Recently, there has been an increasing
interest in formulating gradient-based optimizers in a probabilistic framework
for better estimation of gradients and modeling uncertainties. Here, we propose
to combine both approaches, resulting in the Variational Stochastic Gradient
Descent (VSGD) optimizer. We model gradient updates as a probabilistic model
and utilize stochastic variational inference (SVI) to derive an efficient and
effective update rule. Further, we show how our VSGD method relates to other
adaptive gradient-based optimizers like Adam. Lastly, we carry out experiments
on two image classification datasets and four deep neural network
architectures, where we show that VSGD outperforms Adam and SGD.","['Haotian Chen', 'Anna Kuzina', 'Babak Esmaeili', 'Jakub M Tomczak']","['cs.LG', 'stat.ML']",2024-04-09 18:02:01+00:00
http://arxiv.org/abs/2404.06498v1,Simultaneous linear connectivity of neural networks modulo permutation,"Neural networks typically exhibit permutation symmetries which contribute to
the non-convexity of the networks' loss landscapes, since linearly
interpolating between two permuted versions of a trained network tends to
encounter a high loss barrier. Recent work has argued that permutation
symmetries are the only sources of non-convexity, meaning there are essentially
no such barriers between trained networks if they are permuted appropriately.
In this work, we refine these arguments into three distinct claims of
increasing strength. We show that existing evidence only supports ""weak linear
connectivity""-that for each pair of networks belonging to a set of SGD
solutions, there exist (multiple) permutations that linearly connect it with
the other networks. In contrast, the claim ""strong linear connectivity""-that
for each network, there exists one permutation that simultaneously connects it
with the other networks-is both intuitively and practically more desirable.
This stronger claim would imply that the loss landscape is convex after
accounting for permutation, and enable linear interpolation between three or
more independently trained models without increased loss. In this work, we
introduce an intermediate claim-that for certain sequences of networks, there
exists one permutation that simultaneously aligns matching pairs of networks
from these sequences. Specifically, we discover that a single permutation
aligns sequences of iteratively trained as well as iteratively pruned networks,
meaning that two networks exhibit low loss barriers at each step of their
optimization and sparsification trajectories respectively. Finally, we provide
the first evidence that strong linear connectivity may be possible under
certain conditions, by showing that barriers decrease with increasing network
width when interpolating among three networks.","['Ekansh Sharma', 'Devin Kwok', 'Tom Denton', 'Daniel M. Roy', 'David Rolnick', 'Gintare Karolina Dziugaite']","['cs.LG', 'stat.ML']",2024-04-09 17:50:38+00:00
http://arxiv.org/abs/2404.06466v1,Hyperparameter Selection in Continual Learning,"In continual learning (CL) -- where a learner trains on a stream of data --
standard hyperparameter optimisation (HPO) cannot be applied, as a learner does
not have access to all of the data at the same time. This has prompted the
development of CL-specific HPO frameworks. The most popular way to tune
hyperparameters in CL is to repeatedly train over the whole data stream with
different hyperparameter settings. However, this end-of-training HPO is
unrealistic as in practice a learner can only see the stream once. Hence, there
is an open question: what HPO framework should a practitioner use for a CL
problem in reality? This paper answers this question by evaluating several
realistic HPO frameworks. We find that all the HPO frameworks considered,
including end-of-training HPO, perform similarly. We therefore advocate using
the realistic and most computationally efficient method: fitting the
hyperparameters on the first task and then fixing them throughout training.","['Thomas L. Lee', 'Sigrid Passano Hellan', 'Linus Ericsson', 'Elliot J. Crowley', 'Amos Storkey']","['cs.LG', 'stat.ML']",2024-04-09 17:14:41+00:00
http://arxiv.org/abs/2404.06391v1,Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity,"One of the most intriguing findings in the structure of neural network
landscape is the phenomenon of mode connectivity: For two typical global
minima, there exists a path connecting them without barrier. This concept of
mode connectivity has played a crucial role in understanding important
phenomena in deep learning.
  In this paper, we conduct a fine-grained analysis of this connectivity
phenomenon. First, we demonstrate that in the overparameterized case, the
connecting path can be as simple as a two-piece linear path, and the path
length can be nearly equal to the Euclidean distance. This finding suggests
that the landscape should be nearly convex in a certain sense. Second, we
uncover a surprising star-shaped connectivity: For a finite number of typical
minima, there exists a center on minima manifold that connects all of them
simultaneously via linear paths. These results are provably valid for linear
networks and two-layer ReLU networks under a teacher-student setup, and are
empirically supported by models trained on MNIST and CIFAR-10.","['Zhanran Lin', 'Puheng Li', 'Lei Wu']","['cs.LG', 'stat.ML']",2024-04-09 15:35:02+00:00
http://arxiv.org/abs/2404.06336v2,Quantum State Generation with Structure-Preserving Diffusion Model,"This article considers the generative modeling of the (mixed) states of
quantum systems, and an approach based on denoising diffusion model is
proposed. The key contribution is an algorithmic innovation that respects the
physical nature of quantum states. More precisely, the commonly used density
matrix representation of mixed-state has to be complex-valued Hermitian,
positive semi-definite, and trace one. Generic diffusion models, or other
generative methods, may not be able to generate data that strictly satisfy
these structural constraints, even if all training data do. To develop a
machine learning algorithm that has physics hard-wired in, we leverage mirror
diffusion and borrow the physical notion of von Neumann entropy to design a new
map, for enabling strict structure-preserving generation. Both unconditional
generation and conditional generation via classifier-free guidance are
experimentally demonstrated efficacious, the latter enabling the design of new
quantum states when generated on unseen labels.","['Yuchen Zhu', 'Tianrong Chen', 'Evangelos A. Theodorou', 'Xie Chen', 'Molei Tao']","['quant-ph', 'cs.LG', 'stat.ML']",2024-04-09 14:21:51+00:00
http://arxiv.org/abs/2404.06200v1,Further Understanding of a Local Gaussian Process Approximation: Characterising Convergence in the Finite Regime,"We show that common choices of kernel functions for a highly accurate and
massively scalable nearest-neighbour based GP regression model (GPnn:
\cite{GPnn}) exhibit gradual convergence to asymptotic behaviour as
dataset-size $n$ increases. For isotropic kernels such as Mat\'{e}rn and
squared-exponential, an upper bound on the predictive MSE can be obtained as
$O(n^{-\frac{p}{d}})$ for input dimension $d$, $p$ dictated by the kernel (and
$d>p$) and fixed number of nearest-neighbours $m$ with minimal assumptions on
the input distribution. Similar bounds can be found under model
misspecification and combined to give overall rates of convergence of both MSE
and an important calibration metric. We show that lower bounds on $n$ can be
given in terms of $m$, $l$, $p$, $d$, a tolerance $\varepsilon$ and a
probability $\delta$. When $m$ is chosen to be $O(n^{\frac{p}{p+d}})$ minimax
optimal rates of convergence are attained. Finally, we demonstrate empirical
performance and show that in many cases convergence occurs faster than the
upper bounds given here.","['Anthony Stephenson', 'Robert Allison', 'Edward Pyzer-Knapp']","['math.ST', 'stat.ML', 'stat.TH']",2024-04-09 10:47:01+00:00
http://arxiv.org/abs/2404.06023v2,Prelimit Coupling and Steady-State Convergence of Constant-stepsize Nonsmooth Contractive SA,"Motivated by Q-learning, we study nonsmooth contractive stochastic
approximation (SA) with constant stepsize. We focus on two important classes of
dynamics: 1) nonsmooth contractive SA with additive noise, and 2) synchronous
and asynchronous Q-learning, which features both additive and multiplicative
noise. For both dynamics, we establish weak convergence of the iterates to a
stationary limit distribution in Wasserstein distance. Furthermore, we propose
a prelimit coupling technique for establishing steady-state convergence and
characterize the limit of the stationary distribution as the stepsize goes to
zero. Using this result, we derive that the asymptotic bias of nonsmooth SA is
proportional to the square root of the stepsize, which stands in sharp contrast
to smooth SA. This bias characterization allows for the use of
Richardson-Romberg extrapolation for bias reduction in nonsmooth SA.","['Yixuan Zhang', 'Dongyan Huo', 'Yudong Chen', 'Qiaomin Xie']","['stat.ML', 'cs.LG', 'math.OC', 'math.PR']",2024-04-09 05:12:44+00:00
http://arxiv.org/abs/2404.06013v1,Feel-Good Thompson Sampling for Contextual Dueling Bandits,"Contextual dueling bandits, where a learner compares two options based on
context and receives feedback indicating which was preferred, extends classic
dueling bandits by incorporating contextual information for decision-making and
preference learning. Several algorithms based on the upper confidence bound
(UCB) have been proposed for linear contextual dueling bandits. However, no
algorithm based on posterior sampling has been developed in this setting,
despite the empirical success observed in traditional contextual bandits. In
this paper, we propose a Thompson sampling algorithm, named FGTS.CDB, for
linear contextual dueling bandits. At the core of our algorithm is a new
Feel-Good exploration term specifically tailored for dueling bandits. This term
leverages the independence of the two selected arms, thereby avoiding a cross
term in the analysis. We show that our algorithm achieves nearly
minimax-optimal regret, i.e., $\tilde{\mathcal{O}}(d\sqrt T)$, where $d$ is the
model dimension and $T$ is the time horizon. Finally, we evaluate our algorithm
on synthetic data and observe that FGTS.CDB outperforms existing algorithms by
a large margin.","['Xuheng Li', 'Heyang Zhao', 'Quanquan Gu']","['cs.LG', 'math.OC', 'stat.ML']",2024-04-09 04:45:18+00:00
http://arxiv.org/abs/2404.05991v1,Polynomial-time derivation of optimal k-tree topology from Markov networks,"Characterization of joint probability distribution for large networks of
random variables remains a challenging task in data science. Probabilistic
graph approximation with simple topologies has practically been resorted to;
typically the tree topology makes joint probability computation much simpler
and can be effective for statistical inference on insufficient data. However,
to characterize network components where multiple variables cooperate closely
to influence others, model topologies beyond a tree are needed, which
unfortunately are infeasible to acquire. In particular, our previous work has
related optimal approximation of Markov networks of tree-width k >=2 closely to
the graph-theoretic problem of finding maximum spanning k-tree (MSkT), which is
a provably intractable task.
  This paper investigates optimal approximation of Markov networks with k-tree
topology that retains some designated underlying subgraph. Such a subgraph may
encode certain background information that arises in scientific applications,
for example, about a known significant pathway in gene networks or the
indispensable backbone connectivity in the residue interaction graphs for a
biomolecule 3D structure. In particular, it is proved that the \beta-retaining
MSkT problem, for a number of classes \beta of graphs, admit O(n^{k+1})-time
algorithms for every fixed k>= 1. These \beta-retaining MSkT algorithms offer
efficient solutions for approximation of Markov networks with k-tree topology
in the situation where certain persistent information needs to be retained.","['Fereshteh R. Dastjerdi', 'Liming Cai']","['cs.DS', 'stat.ML']",2024-04-09 03:52:58+00:00
http://arxiv.org/abs/2404.05905v1,Computing Transition Pathways for the Study of Rare Events Using Deep Reinforcement Learning,"Understanding the transition events between metastable states in complex
systems is an important subject in the fields of computational physics,
chemistry and biology. The transition pathway plays an important role in
characterizing the mechanism underlying the transition, for example, in the
study of conformational changes of bio-molecules. In fact, computing the
transition pathway is a challenging task for complex and high-dimensional
systems. In this work, we formulate the path-finding task as a cost
minimization problem over a particular path space. The cost function is adapted
from the Freidlin-Wentzell action functional so that it is able to deal with
rough potential landscapes. The path-finding problem is then solved using a
actor-critic method based on the deep deterministic policy gradient algorithm
(DDPG). The method incorporates the potential force of the system in the policy
for generating episodes and combines physical properties of the system with the
learning process for molecular systems. The exploitation and exploration nature
of reinforcement learning enables the method to efficiently sample the
transition events and compute the globally optimal transition pathway. We
illustrate the effectiveness of the proposed method using three benchmark
systems including an extended Mueller system and the Lennard-Jones system of
seven particles.","['Bo Lin', 'Yangzheng Zhong', 'Weiqing Ren']","['physics.comp-ph', 'cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2024-04-08 23:30:15+00:00
http://arxiv.org/abs/2404.05868v2,Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning,"Large Language Models (LLMs) often memorize sensitive, private, or
copyrighted data during pre-training. LLM unlearning aims to eliminate the
influence of undesirable data from the pre-trained model while preserving the
model's utilities on other tasks. Several practical methods have recently been
proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss
of undesirable data. However, on certain unlearning tasks, these methods either
fail to effectively unlearn the target data or suffer from catastrophic
collapse -- a drastic degradation of the model's utilities.
  In this paper, we propose Negative Preference Optimization (NPO), a simple
alignment-inspired method that could efficiently and effectively unlearn a
target dataset. We theoretically show that the progression toward catastrophic
collapse by minimizing the NPO loss is exponentially slower than GA. Through
experiments on synthetic data and the benchmark TOFU dataset, we demonstrate
that NPO-based methods achieve a better balance between unlearning the
undesirable data and maintaining the model's utilities. We also observe that
NPO-based methods generate more sensible outputs than GA-based methods, whose
outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the
first to achieve reasonable unlearning results in forgetting 50% (or more) of
the training data, whereas existing methods already struggle with forgetting
10% of training data.","['Ruiqi Zhang', 'Licong Lin', 'Yu Bai', 'Song Mei']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2024-04-08 21:05:42+00:00
http://arxiv.org/abs/2404.05819v2,Just Wing It: Near-Optimal Estimation of Missing Mass in a Markovian Sequence,"We study the problem of estimating the stationary mass -- also called the
unigram mass -- that is missing from a single trajectory of a discrete-time,
ergodic Markov chain. This problem has several applications -- for example,
estimating the stationary missing mass is critical for accurately smoothing
probability estimates in sequence models. While the classical Good--Turing
estimator from the 1950s has appealing properties for i.i.d. data, it is known
to be biased in the Markovian setting, and other heuristic estimators do not
come equipped with guarantees. Operating in the general setting in which the
size of the state space may be much larger than the length $n$ of the
trajectory, we develop a linear-runtime estimator called Windowed Good--Turing
(WingIt) and show that its risk decays as $\widetilde{O}(\mathsf{T_{mix}}/n)$,
where $\mathsf{T_{mix}}$ denotes the mixing time of the chain in total
variation distance. Notably, this rate is independent of the size of the state
space and minimax-optimal up to a logarithmic factor in $n / \mathsf{T_{mix}}$.
We also present an upper bound on the variance of the missing mass random
variable, which may be of independent interest. We extend our estimator to
approximate the stationary mass placed on elements occurring with small
frequency in the trajectory. Finally, we demonstrate the efficacy of our
estimators both in simulations on canonical chains and on sequences constructed
from natural language text.","['Ashwin Pananjady', 'Vidya Muthukumar', 'Andrew Thangaraj']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.PR', 'math.ST', 'stat.TH']",2024-04-08 18:55:07+00:00
http://arxiv.org/abs/2404.05678v3,Flexible Fairness-Aware Learning via Inverse Conditional Permutation,"Equalized odds, as a popular notion of algorithmic fairness, aims to ensure
that sensitive variables, such as race and gender, do not unfairly influence
the algorithm's prediction when conditioning on the true outcome. Despite rapid
advancements, current research primarily focuses on equalized odds violations
caused by a single sensitive attribute, leaving the challenge of simultaneously
accounting for multiple attributes largely unaddressed. We bridge this gap by
introducing an in-processing fairness-aware learning approach, FairICP, which
integrates adversarial learning with a novel inverse conditional permutation
scheme. FairICP offers a theoretically justified, flexible, and efficient
scheme to promote equalized odds under fairness conditions described by complex
and multidimensional sensitive attributes. The efficacy and adaptability of our
method are demonstrated through both simulation studies and empirical analyses
of real-world datasets.","['Yuheng Lai', 'Leying Guan']","['stat.ML', 'cs.CY', 'cs.LG']",2024-04-08 16:57:44+00:00
http://arxiv.org/abs/2404.13056v1,Variational Bayesian Optimal Experimental Design with Normalizing Flows,"Bayesian optimal experimental design (OED) seeks experiments that maximize
the expected information gain (EIG) in model parameters. Directly estimating
the EIG using nested Monte Carlo is computationally expensive and requires an
explicit likelihood. Variational OED (vOED), in contrast, estimates a lower
bound of the EIG without likelihood evaluations by approximating the posterior
distributions with variational forms, and then tightens the bound by optimizing
its variational parameters. We introduce the use of normalizing flows (NFs) for
representing variational distributions in vOED; we call this approach vOED-NFs.
Specifically, we adopt NFs with a conditional invertible neural network
architecture built from compositions of coupling layers, and enhanced with a
summary network for data dimension reduction. We present Monte Carlo estimators
to the lower bound along with gradient expressions to enable a gradient-based
simultaneous optimization of the variational parameters and the design
variables. The vOED-NFs algorithm is then validated in two benchmark problems,
and demonstrated on a partial differential equation-governed application of
cathodic electrophoretic deposition and an implicit likelihood case with
stochastic modeling of aphid population. The findings suggest that a
composition of 4--5 coupling layers is able to achieve lower EIG estimation
bias, under a fixed budget of forward model runs, compared to previous
approaches. The resulting NFs produce approximate posteriors that agree well
with the true posteriors, able to capture non-Gaussian and multi-modal features
effectively.","['Jiayuan Dong', 'Christian Jacobsen', 'Mehdi Khalloufi', 'Maryam Akram', 'Wanjiao Liu', 'Karthik Duraisamy', 'Xun Huan']","['cs.LG', 'cs.CE', 'stat.CO', 'stat.ME', 'stat.ML', '62K05, 94A17, 62C10, 62F15']",2024-04-08 14:44:21+00:00
http://arxiv.org/abs/2404.05555v2,On the Convergence of Continual Learning with Adaptive Methods,"One of the objectives of continual learning is to prevent catastrophic
forgetting in learning multiple tasks sequentially, and the existing solutions
have been driven by the conceptualization of the plasticity-stability dilemma.
However, the convergence of continual learning for each sequential task is less
studied so far. In this paper, we provide a convergence analysis of
memory-based continual learning with stochastic gradient descent and empirical
evidence that training current tasks causes the cumulative degradation of
previous tasks. We propose an adaptive method for nonconvex continual learning
(NCCL), which adjusts step sizes of both previous and current tasks with the
gradients. The proposed method can achieve the same convergence rate as the SGD
method when the catastrophic forgetting term which we define in the paper is
suppressed at each iteration. Further, we demonstrate that the proposed
algorithm improves the performance of continual learning over existing methods
for several image classification tasks.","['Seungyub Han', 'Yeongmo Kim', 'Taehyun Cho', 'Jungwoo Lee']","['cs.LG', 'cs.AI', 'stat.ML']",2024-04-08 14:28:27+00:00
http://arxiv.org/abs/2404.05484v2,On Computational Modeling of Sleep-Wake Cycle,"Why do mammals need to sleep? Neuroscience treats sleep and wake as default
and perturbation modes of the brain. It is hypothesized that the brain
self-organizes neural activities without environmental inputs. This paper
presents a new computational model of the sleep-wake cycle (SWC) for learning
and memory. During the sleep mode, the memory consolidation by the
thalamocortical system is abstracted by a disentangling operator that maps
context-dependent representations (CDR) to context-independent representations
(CIR) for generalization. Such a disentangling operator can be mathematically
formalized by an integral transform that integrates the context variable from
CDR. During the wake mode, the memory formation by the hippocampal-neocortical
system is abstracted by an entangling operator from CIR to CDR where the
context is introduced by physical motion. When designed as inductive bias,
entangled CDR linearizes the problem of unsupervised learning for sensory
memory by direct-fit. The concatenation of disentangling and entangling
operators forms a disentangling-entangling cycle (DEC) as the building block
for sensorimotor learning. We also discuss the relationship of DEC and SWC to
the perception-action cycle (PAC) for internal model learning and perceptual
control theory for the ecological origin of natural languages.",['Xin Li'],"['cs.LG', 'stat.ML']",2024-04-08 13:06:23+00:00
http://arxiv.org/abs/2404.05209v1,Maximally Forward-Looking Core Inflation,"Timely monetary policy decision-making requires timely core inflation
measures. We create a new core inflation series that is explicitly designed to
succeed at that goal. Precisely, we introduce the Assemblage Regression, a
generalized nonnegative ridge regression problem that optimizes the price
index's subcomponent weights such that the aggregate is maximally predictive of
future headline inflation. Ordering subcomponents according to their rank in
each period switches the algorithm to be learning supervised trimmed inflation
- or, put differently, the maximally forward-looking summary statistic of the
realized price changes distribution. In an extensive out-of-sample forecasting
experiment for the US and the euro area, we find substantial improvements for
signaling medium-term inflation developments in both the pre- and post-Covid
years. Those coming from the supervised trimmed version are particularly
striking, and are attributable to a highly asymmetric trimming which contrasts
with conventional indicators. We also find that this metric was indicating
first upward pressures on inflation as early as mid-2020 and quickly captured
the turning point in 2022. We also consider extensions, like assembling
inflation from geographical regions, trimmed temporal aggregation, and building
core measures specialized for either upside or downside inflation risks.","['Philippe Goulet Coulombe', 'Karin Klieber', 'Christophe Barrette', 'Maximilian Goebel']","['econ.EM', 'stat.ML']",2024-04-08 05:39:41+00:00
http://arxiv.org/abs/2404.05185v1,Convergence analysis of controlled particle systems arising in deep learning: from finite to infinite sample size,"This paper deals with a class of neural SDEs and studies the limiting
behavior of the associated sampled optimal control problems as the sample size
grows to infinity. The neural SDEs with N samples can be linked to the
N-particle systems with centralized control. We analyze the
Hamilton--Jacobi--Bellman equation corresponding to the N-particle system and
establish regularity results which are uniform in N. The uniform regularity
estimates are obtained by the stochastic maximum principle and the analysis of
a backward stochastic Riccati equation. Using these uniform regularity results,
we show the convergence of the minima of objective functionals and optimal
parameters of the neural SDEs as the sample size N tends to infinity. The
limiting objects can be identified with suitable functions defined on the
Wasserstein space of Borel probability measures. Furthermore, quantitative
algebraic convergence rates are also obtained.","['Huafu Liao', 'Alpár R. Mészáros', 'Chenchen Mou', 'Chao Zhou']","['math.OC', 'cs.LG', 'math.PR', 'stat.ML', '49N80, 65C35, 49L12, 62M45']",2024-04-08 04:22:55+00:00
http://arxiv.org/abs/2404.05155v1,On the price of exact truthfulness in incentive-compatible online learning with bandit feedback: A regret lower bound for WSU-UX,"In one view of the classical game of prediction with expert advice with
binary outcomes, in each round, each expert maintains an adversarially chosen
belief and honestly reports this belief. We consider a recently introduced,
strategic variant of this problem with selfish (reputation-seeking) experts,
where each expert strategically reports in order to maximize their expected
future reputation based on their belief. In this work, our goal is to design an
algorithm for the selfish experts problem that is incentive-compatible (IC, or
\emph{truthful}), meaning each expert's best strategy is to report truthfully,
while also ensuring the algorithm enjoys sublinear regret with respect to the
expert with the best belief. Freeman et al. (2020) recently studied this
problem in the full information and bandit settings and obtained truthful,
no-regret algorithms by leveraging prior work on wagering mechanisms. While
their results under full information match the minimax rate for the classical
(""honest experts"") problem, the best-known regret for their bandit algorithm
WSU-UX is $O(T^{2/3})$, which does not match the minimax rate for the classical
(""honest bandits"") setting. It was unclear whether the higher regret was an
artifact of their analysis or a limitation of WSU-UX. We show, via explicit
construction of loss sequences, that the algorithm suffers a worst-case
$\Omega(T^{2/3})$ lower bound. Left open is the possibility that a different IC
algorithm obtains $O(\sqrt{T})$ regret. Yet, WSU-UX was a natural choice for
such an algorithm owing to the limited design room for IC algorithms in this
setting.","['Ali Mortazavi', 'Junhao Lin', 'Nishant A. Mehta']","['cs.LG', 'cs.GT', 'stat.ML']",2024-04-08 02:41:32+00:00
http://arxiv.org/abs/2404.05148v1,Generalized Criterion for Identifiability of Additive Noise Models Using Majorization,"The discovery of causal relationships from observational data is very
challenging. Many recent approaches rely on complexity or uncertainty concepts
to impose constraints on probability distributions, aiming to identify specific
classes of directed acyclic graph (DAG) models. In this paper, we introduce a
novel identifiability criterion for DAGs that places constraints on the
conditional variances of additive noise models. We demonstrate that this
criterion extends and generalizes existing identifiability criteria in the
literature that employ (conditional) variances as measures of uncertainty in
(conditional) distributions. For linear Structural Equation Models, we present
a new algorithm that leverages the concept of weak majorization applied to the
diagonal elements of the Cholesky factor of the covariance matrix to learn a
topological ordering of variables. Through extensive simulations and the
analysis of bank connectivity data, we provide evidence of the effectiveness of
our approach in successfully recovering DAGs. The code for reproducing the
results in this paper is available in Supplementary Materials.","['Aramayis Dallakyan', 'Yang Ni']","['stat.ME', 'stat.ML']",2024-04-08 02:18:57+00:00
http://arxiv.org/abs/2404.05062v2,New methods to compute the generalized chi-square distribution,"We present several new mathematical methods (ray-trace, inverse Fourier
transform and ellipse) and open-source software to compute the cdf, pdf and
inverse cdf of the generalized chi-square distribution. Some methods are geared
for speed, while others are designed to be accurate far into the tails, using
which we can also measure large values of the discriminability index d' between
multinormals. We characterize the performance and limitations of these and
previous methods, and recommend the best methods to use for each part of each
type of distribution. We also demonstrate the speed and accuracy of our new
methods against previous methods across a wide sample of distributions.",['Abhranil Das'],"['stat.CO', 'cs.LG', 'stat.ME', 'stat.ML']",2024-04-07 20:16:37+00:00
http://arxiv.org/abs/2404.05058v1,A robust assessment for invariant representations,"The performance of machine learning models can be impacted by changes in data
over time. A promising approach to address this challenge is invariant
learning, with a particular focus on a method known as invariant risk
minimization (IRM). This technique aims to identify a stable data
representation that remains effective with out-of-distribution (OOD) data.
While numerous studies have developed IRM-based methods adaptive to data
augmentation scenarios, there has been limited attention on directly assessing
how well these representations preserve their invariant performance under
varying conditions. In our paper, we propose a novel method to evaluate
invariant performance, specifically tailored for IRM-based methods. We
establish a bridge between the conditional expectation of an invariant
predictor across different environments through the likelihood ratio. Our
proposed criterion offers a robust basis for evaluating invariant performance.
We validate our approach with theoretical support and demonstrate its
effectiveness through extensive numerical studies.These experiments illustrate
how our method can assess the invariant performance of various representation
techniques.","['Wenlu Tang', 'Zicheng Liu']","['cs.LG', 'stat.ML']",2024-04-07 20:05:49+00:00
http://arxiv.org/abs/2405.04539v1,Some variation of COBRA in sequential learning setup,"This research paper introduces innovative approaches for multivariate time
series forecasting based on different variations of the combined regression
strategy. We use specific data preprocessing techniques which makes a radical
change in the behaviour of prediction. We compare the performance of the model
based on two types of hyper-parameter tuning Bayesian optimisation (BO) and
Usual Grid search. Our proposed methodologies outperform all state-of-the-art
comparative models. We illustrate the methodologies through eight time series
datasets from three categories: cryptocurrency, stock index, and short-term
load forecasting.","['Aryan Bhambu', 'Arabin Kumar Dey']","['stat.ML', 'cs.CE', 'cs.LG', 'eess.SP', 'q-fin.CP']",2024-04-07 17:41:02+00:00
http://arxiv.org/abs/2404.05768v2,Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A Multiobjective Hyperparameter and Architecture Optimization Approach,"Training an effective deep learning model to learn ocean processes involves
careful choices of various hyperparameters. We leverage the advanced search
algorithms for multiobjective optimization in DeepHyper, a scalable
hyperparameter optimization software, to streamline the development of neural
networks tailored for ocean modeling. The focus is on optimizing Fourier neural
operators (FNOs), a data-driven model capable of simulating complex ocean
behaviors. Selecting the correct model and tuning the hyperparameters are
challenging tasks, requiring much effort to ensure model accuracy. DeepHyper
allows efficient exploration of hyperparameters associated with data
preprocessing, FNO architecture-related hyperparameters, and various model
training strategies. We aim to obtain an optimal set of hyperparameters leading
to the most performant model. Moreover, on top of the commonly used mean
squared error for model training, we propose adopting the negative anomaly
correlation coefficient as the additional loss term to improve model
performance and investigate the potential trade-off between the two terms. The
experimental results show that the optimal set of hyperparameters enhanced
model performance in single timestepping forecasting and greatly exceeded the
baseline configuration in the autoregressive rollout for long-horizon
forecasting up to 30 days. Utilizing DeepHyper, we demonstrate an approach to
enhance the use of FNOs in ocean dynamics forecasting, offering a scalable
solution with improved precision.","['Yixuan Sun', 'Ololade Sowunmi', 'Romain Egele', 'Sri Hari Krishna Narayanan', 'Luke Van Roekel', 'Prasanna Balaprakash']","['cs.LG', 'physics.ao-ph', 'stat.ML']",2024-04-07 14:29:23+00:00
http://arxiv.org/abs/2404.08679v1,Your Finetuned Large Language Model is Already a Powerful Out-of-distribution Detector,"We revisit the likelihood ratio between a pretrained large language model
(LLM) and its finetuned variant as a criterion for out-of-distribution (OOD)
detection. The intuition behind such a criterion is that, the pretrained LLM
has the prior knowledge about OOD data due to its large amount of training
data, and once finetuned with the in-distribution data, the LLM has sufficient
knowledge to distinguish their difference. Leveraging the power of LLMs, we
show that, for the first time, the likelihood ratio can serve as an effective
OOD detector. Moreover, we apply the proposed LLM-based likelihood ratio to
detect OOD questions in question-answering (QA) systems, which can be used to
improve the performance of specialized LLMs for general questions. Given that
likelihood can be easily obtained by the loss functions within contemporary
neural network frameworks, it is straightforward to implement this approach in
practice. Since both the pretrained LLMs and its various finetuned models are
available, our proposed criterion can be effortlessly incorporated for OOD
detection without the need for further training. We conduct comprehensive
evaluation across on multiple settings, including far OOD, near OOD, spam
detection, and QA scenarios, to demonstrate the effectiveness of the method.","['Andi Zhang', 'Tim Z. Xiao', 'Weiyang Liu', 'Robert Bamler', 'Damon Wischik']","['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']",2024-04-07 10:32:49+00:00
http://arxiv.org/abs/2404.04865v1,On the Learnability of Out-of-distribution Detection,"Supervised learning aims to train a classifier under the assumption that
training and test data are from the same distribution. To ease the above
assumption, researchers have studied a more realistic setting:
out-of-distribution (OOD) detection, where test data may come from classes that
are unknown during training (i.e., OOD data). Due to the unavailability and
diversity of OOD data, good generalization ability is crucial for effective OOD
detection algorithms, and corresponding learning theory is still an open
problem. To study the generalization of OOD detection, this paper investigates
the probably approximately correct (PAC) learning theory of OOD detection that
fits the commonly used evaluation metrics in the literature. First, we find a
necessary condition for the learnability of OOD detection. Then, using this
condition, we prove several impossibility theorems for the learnability of OOD
detection under some scenarios. Although the impossibility theorems are
frustrating, we find that some conditions of these impossibility theorems may
not hold in some practical scenarios. Based on this observation, we next give
several necessary and sufficient conditions to characterize the learnability of
OOD detection in some practical scenarios. Lastly, we offer theoretical support
for representative OOD detection works based on our OOD theory.","['Zhen Fang', 'Yixuan Li', 'Feng Liu', 'Bo Han', 'Jie Lu']","['cs.LG', 'cs.CV', 'stat.ML']",2024-04-07 08:17:48+00:00
http://arxiv.org/abs/2404.04859v1,Demystifying Lazy Training of Neural Networks from a Macroscopic Viewpoint,"In this paper, we advance the understanding of neural network training
dynamics by examining the intricate interplay of various factors introduced by
weight parameters in the initialization process. Motivated by the foundational
work of Luo et al. (J. Mach. Learn. Res., Vol. 22, Iss. 1, No. 71, pp
3327-3373), we explore the gradient descent dynamics of neural networks through
the lens of macroscopic limits, where we analyze its behavior as width $m$
tends to infinity. Our study presents a unified approach with refined
techniques designed for multi-layer fully connected neural networks, which can
be readily extended to other neural network architectures. Our investigation
reveals that gradient descent can rapidly drive deep neural networks to zero
training loss, irrespective of the specific initialization schemes employed by
weight parameters, provided that the initial scale of the output function
$\kappa$ surpasses a certain threshold. This regime, characterized as the
theta-lazy area, accentuates the predominant influence of the initial scale
$\kappa$ over other factors on the training behavior of neural networks.
Furthermore, our approach draws inspiration from the Neural Tangent Kernel
(NTK) paradigm, and we expand its applicability. While NTK typically assumes
that $\lim_{m\to\infty}\frac{\log \kappa}{\log m}=\frac{1}{2}$, and imposes
each weight parameters to scale by the factor $\frac{1}{\sqrt{m}}$, in our
theta-lazy regime, we discard the factor and relax the conditions to
$\lim_{m\to\infty}\frac{\log \kappa}{\log m}>0$. Similar to NTK, the behavior
of overparameterized neural networks within the theta-lazy regime trained by
gradient descent can be effectively described by a specific kernel. Through
rigorous analysis, our investigation illuminates the pivotal role of $\kappa$
in governing the training dynamics of neural networks.","['Yuqing Li', 'Tao Luo', 'Qixuan Zhou']","['cs.LG', 'stat.ML']",2024-04-07 08:07:02+00:00
http://arxiv.org/abs/2404.04824v1,Mixup Domain Adaptations for Dynamic Remaining Useful Life Predictions,"Remaining Useful Life (RUL) predictions play vital role for asset planning
and maintenance leading to many benefits to industries such as reduced
downtime, low maintenance costs, etc. Although various efforts have been
devoted to study this topic, most existing works are restricted for i.i.d
conditions assuming the same condition of the training phase and the deployment
phase. This paper proposes a solution to this problem where a mix-up domain
adaptation (MDAN) is put forward. MDAN encompasses a three-staged mechanism
where the mix-up strategy is not only performed to regularize the source and
target domains but also applied to establish an intermediate mix-up domain
where the source and target domains are aligned. The self-supervised learning
strategy is implemented to prevent the supervision collapse problem. Rigorous
evaluations have been performed where MDAN is compared to recently published
works for dynamic RUL predictions. MDAN outperforms its counterparts with
substantial margins in 12 out of 12 cases. In addition, MDAN is evaluated with
the bearing machine dataset where it beats prior art with significant gaps in 8
of 12 cases. Source codes of MDAN are made publicly available in
\url{https://github.com/furqon3009/MDAN}.","['Muhammad Tanzil Furqon', 'Mahardhika Pratama', 'Lin Liu', 'Habibullah', 'Kutluyil Dogancay']","['cs.LG', 'cs.AI', 'stat.ML']",2024-04-07 06:23:18+00:00
http://arxiv.org/abs/2404.04800v1,Coordinated Sparse Recovery of Label Noise,"Label noise is a common issue in real-world datasets that inevitably impacts
the generalization of models. This study focuses on robust classification tasks
where the label noise is instance-dependent. Estimating the transition matrix
accurately in this task is challenging, and methods based on sample selection
often exhibit confirmation bias to varying degrees. Sparse over-parameterized
training (SOP) has been theoretically effective in estimating and recovering
label noise, offering a novel solution for noise-label learning. However, this
study empirically observes and verifies a technical flaw of SOP: the lack of
coordination between model predictions and noise recovery leads to increased
generalization error. To address this, we propose a method called Coordinated
Sparse Recovery (CSR). CSR introduces a collaboration matrix and confidence
weights to coordinate model predictions and noise recovery, reducing error
leakage. Based on CSR, this study designs a joint sample selection strategy and
constructs a comprehensive and powerful learning framework called CSR+. CSR+
significantly reduces confirmation bias, especially for datasets with more
classes and a high proportion of instance-specific noise. Experimental results
on simulated and real-world noisy datasets demonstrate that both CSR and CSR+
achieve outstanding performance compared to methods at the same level.","['Yukun Yang', 'Naihao Wang', 'Haixin Yang', 'Ruirui Li']","['cs.LG', 'cs.CV', 'stat.ML']",2024-04-07 03:41:45+00:00
http://arxiv.org/abs/2404.04738v1,BARMPy: Bayesian Additive Regression Models Python Package,"We make Bayesian Additive Regression Networks (BARN) available as a Python
package, \texttt{barmpy}, with documentation at
\url{https://dvbuntu.github.io/barmpy/} for general machine learning
practitioners. Our object-oriented design is compatible with SciKit-Learn,
allowing usage of their tools like cross-validation. To ease learning to use
\texttt{barmpy}, we produce a companion tutorial that expands on reference
information in the documentation. Any interested user can \texttt{pip install
barmpy} from the official PyPi repository. \texttt{barmpy} also serves as a
baseline Python library for generic Bayesian Additive Regression Models.",['Danielle Van Boxel'],"['stat.CO', 'stat.ML']",2024-04-06 21:51:53+00:00
http://arxiv.org/abs/2404.15328v1,Time topological analysis of EEG using signature theory,"Anomaly detection in multivariate signals is a task of paramount importance
in many disciplines (epidemiology, finance, cognitive sciences and
neurosciences, oncology, etc.). In this perspective, Topological Data Analysis
(TDA) offers a battery of ""shape"" invariants that can be exploited for the
implementation of an effective detection scheme. Our contribution consists of
extending the constructions presented in \cite{chretienleveraging} on the
construction of simplicial complexes from the Signatures of signals and their
predictive capacities, rather than the use of a generic distance as in
\cite{petri2014homological}. Signature theory is a new theme in Machine
Learning arXiv:1603.03788 stemming from recent work on the notions of Rough
Paths developed by Terry Lyons and his team \cite{lyons2002system} based on the
formalism introduced by Chen \cite{chen1957integration}. We explore in
particular the detection of changes in topology, based on tracking the
evolution of homological persistence and the Betti numbers associated with the
complex introduced in \cite{chretienleveraging}. We apply our tools for the
analysis of brain signals such as EEG to detect precursor phenomena to
epileptic seizures.","['Stéphane Chrétien', 'Ben Gao', 'Astrid Thebault-Guiochon', 'Rémi Vaucher']","['eess.SP', 'cs.LG', 'stat.ML']",2024-04-06 21:11:41+00:00
http://arxiv.org/abs/2404.04689v1,Multicalibration for Confidence Scoring in LLMs,"This paper proposes the use of ""multicalibration"" to yield interpretable and
reliable confidence scores for outputs generated by large language models
(LLMs). Multicalibration asks for calibration not just marginally, but
simultaneously across various intersecting groupings of the data. We show how
to form groupings for prompt/completion pairs that are correlated with the
probability of correctness via two techniques: clustering within an embedding
space, and ""self-annotation"" - querying the LLM by asking it various yes-or-no
questions about the prompt. We also develop novel variants of multicalibration
algorithms that offer performance improvements by reducing their tendency to
overfit. Through systematic benchmarking across various question answering
datasets and LLMs, we show how our techniques can yield confidence scores that
provide substantial improvements in fine-grained measures of both calibration
and accuracy compared to existing methods.","['Gianluca Detommaso', 'Martin Bertran', 'Riccardo Fogliato', 'Aaron Roth']","['stat.ML', 'cs.CL', 'cs.LG']",2024-04-06 17:33:37+00:00
http://arxiv.org/abs/2404.04612v2,Spectral Graph Pruning Against Over-Squashing and Over-Smoothing,"Message Passing Graph Neural Networks are known to suffer from two problems
that are sometimes believed to be diametrically opposed: over-squashing and
over-smoothing. The former results from topological bottlenecks that hamper the
information flow from distant nodes and are mitigated by spectral gap
maximization, primarily, by means of edge additions. However, such additions
often promote over-smoothing that renders nodes of different classes less
distinguishable. Inspired by the Braess phenomenon, we argue that deleting
edges can address over-squashing and over-smoothing simultaneously. This
insight explains how edge deletions can improve generalization, thus connecting
spectral gap optimization to a seemingly disconnected objective of reducing
computational resources by pruning graphs for lottery tickets. To this end, we
propose a more effective spectral gap optimization framework to add or delete
edges and demonstrate its effectiveness on large heterophilic datasets.","['Adarsh Jamadandi', 'Celia Rubio-Madrigal', 'Rebekka Burkholz']","['cs.LG', 'eess.SP', 'stat.ML']",2024-04-06 12:40:21+00:00
http://arxiv.org/abs/2404.04549v1,Efficient Learning Using Spiking Neural Networks Equipped With Affine Encoders and Decoders,"We study the learning problem associated with spiking neural networks.
Specifically, we consider hypothesis sets of spiking neural networks with
affine temporal encoders and decoders and simple spiking neurons having only
positive synaptic weights. We demonstrate that the positivity of the weights
continues to enable a wide range of expressivity results, including
rate-optimal approximation of smooth functions or approximation without the
curse of dimensionality. Moreover, positive-weight spiking neural networks are
shown to depend continuously on their parameters which facilitates classical
covering number-based generalization statements. Finally, we observe that from
a generalization perspective, contrary to feedforward neural networks or
previous results for general spiking neural networks, the depth has little to
no adverse effect on the generalization capabilities.","['A. Martina Neuman', 'Philipp Christian Petersen']","['cs.NE', 'cs.LG', 'math.FA', 'stat.ML']",2024-04-06 08:17:07+00:00
http://arxiv.org/abs/2404.04498v2,Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression,"The remarkable generalization performance of large-scale models has been
challenging the conventional wisdom of the statistical learning theory.
Although recent theoretical studies have shed light on this behavior in linear
models and nonlinear classifiers, a comprehensive understanding of
overparameterization in nonlinear regression models is still lacking. This
study explores the predictive properties of overparameterized nonlinear
regression within the Bayesian framework, extending the methodology of the
adaptive prior considering the intrinsic spectral structure of the data.
Posterior contraction is established for generalized linear and single-neuron
models with Lipschitz continuous activation functions, demonstrating the
consistency in the predictions of the proposed approach. Moreover, the Bayesian
framework enables uncertainty estimation of the predictions. The proposed
method was validated via numerical simulations and a real data application,
showing its ability to achieve accurate predictions and reliable uncertainty
estimates. This work provides a theoretical understanding of the advantages of
overparameterization and a principled Bayesian approach to large nonlinear
models.",['Tomoya Wakayama'],"['stat.ML', 'cs.LG', 'stat.ME']",2024-04-06 04:22:48+00:00
http://arxiv.org/abs/2404.04475v1,Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators,"LLM-based auto-annotators have become a key component of the LLM development
process due to their cost-effectiveness and scalability compared to human-based
evaluation. However, these auto-annotators can introduce complex biases that
are hard to remove. Even simple, known confounders such as preference for
longer outputs remain in existing automated evaluation metrics. We propose a
simple regression analysis approach for controlling biases in auto-evaluations.
As a real case study, we focus on reducing the length bias of AlpacaEval, a
fast and affordable benchmark for chat LLMs that uses LLMs to estimate response
quality. Despite being highly correlated with human preferences, AlpacaEval is
known to favor models that generate longer outputs. We introduce a
length-controlled AlpacaEval that aims to answer the counterfactual question:
""What would the preference be if the model's and baseline's output had the same
length?"". To achieve this, we first fit a generalized linear model to predict
the biased output of interest (auto-annotator preferences) based on the
mediators we want to control for (length difference) and other relevant
features. We then obtain length-controlled preferences by predicting
preferences while conditioning the GLM with a zero difference in lengths.
Length-controlling not only improves the robustness of the metric to
manipulations in model verbosity, we also find that it increases the Spearman
correlation with LMSYS' Chatbot Arena from 0.94 to 0.98. We release the code
and leaderboard at https://tatsu-lab.github.io/alpaca_eval/ .","['Yann Dubois', 'Balázs Galambosi', 'Percy Liang', 'Tatsunori B. Hashimoto']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2024-04-06 02:29:02+00:00
http://arxiv.org/abs/2404.04467v1,Demand Balancing in Primal-Dual Optimization for Blind Network Revenue Management,"This paper proposes a practically efficient algorithm with optimal
theoretical regret which solves the classical network revenue management (NRM)
problem with unknown, nonparametric demand. Over a time horizon of length $T$,
in each time period the retailer needs to decide prices of $N$ types of
products which are produced based on $M$ types of resources with
unreplenishable initial inventory. When demand is nonparametric with some mild
assumptions, Miao and Wang (2021) is the first paper which proposes an
algorithm with $O(\text{poly}(N,M,\ln(T))\sqrt{T})$ type of regret (in
particular, $\tilde O(N^{3.5}\sqrt{T})$ plus additional high-order terms that
are $o(\sqrt{T})$ with sufficiently large $T\gg N$). In this paper, we improve
the previous result by proposing a primal-dual optimization algorithm which is
not only more practical, but also with an improved regret of $\tilde
O(N^{3.25}\sqrt{T})$ free from additional high-order terms. A key technical
contribution of the proposed algorithm is the so-called demand balancing, which
pairs the primal solution (i.e., the price) in each time period with another
price to offset the violation of complementary slackness on resource inventory
constraints. Numerical experiments compared with several benchmark algorithms
further illustrate the effectiveness of our algorithm.","['Sentao Miao', 'Yining Wang']","['stat.ML', 'cs.LG']",2024-04-06 01:39:51+00:00
http://arxiv.org/abs/2404.04454v1,Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization,"Adam with decoupled weight decay, also known as AdamW, is widely acclaimed
for its superior performance in language modeling tasks, surpassing Adam with
$\ell_2$ regularization in terms of generalization and optimization. However,
this advantage is not theoretically well-understood. One challenge here is that
though intuitively Adam with $\ell_2$ regularization optimizes the $\ell_2$
regularized loss, it is not clear if AdamW optimizes a specific objective. In
this work, we make progress toward understanding the benefit of AdamW by
showing that it implicitly performs constrained optimization. More concretely,
we show in the full-batch setting, if AdamW converges with any non-increasing
learning rate schedule whose partial sum diverges, it must converge to a KKT
point of the original loss under the constraint that the $\ell_\infty$ norm of
the parameter is bounded by the inverse of the weight decay factor. This result
is built on the observation that Adam can be viewed as a smoothed version of
SignGD, which is the normalized steepest descent with respect to $\ell_\infty$
norm, and a surprising connection between normalized steepest descent with
weight decay and Frank-Wolfe.","['Shuo Xie', 'Zhiyuan Li']","['cs.LG', 'math.OC', 'stat.ML']",2024-04-05 23:56:50+00:00
http://arxiv.org/abs/2404.04425v1,Bayesian Additive Regression Networks,"We apply Bayesian Additive Regression Tree (BART) principles to training an
ensemble of small neural networks for regression tasks. Using Markov Chain
Monte Carlo, we sample from the posterior distribution of neural networks that
have a single hidden layer. To create an ensemble of these, we apply Gibbs
sampling to update each network against the residual target value (i.e.
subtracting the effect of the other networks). We demonstrate the effectiveness
of this technique on several benchmark regression problems, comparing it to
equivalent shallow neural networks, BART, and ordinary least squares. Our
Bayesian Additive Regression Networks (BARN) provide more consistent and often
more accurate results. On test data benchmarks, BARN averaged between 5 to 20
percent lower root mean square error. This error performance does come at the
cost, however, of greater computation time. BARN sometimes takes on the order
of a minute where competing methods take a second or less. But, BARN without
cross-validated hyperparameter tuning takes about the same amount of
computation time as tuned other methods. Yet BARN is still typically more
accurate.",['Danielle Van Boxel'],"['stat.ML', 'cs.LG']",2024-04-05 21:47:32+00:00
http://arxiv.org/abs/2404.04399v1,Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer,"We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep
LTMLE), a novel approach to estimate the counterfactual mean of outcome under
dynamic treatment policies in longitudinal problem settings. Our approach
utilizes a transformer architecture with heterogeneous type embedding trained
using temporal-difference learning. After obtaining an initial estimate using
the transformer, following the targeted minimum loss-based likelihood
estimation (TMLE) framework, we statistically corrected for the bias commonly
associated with machine learning algorithms. Furthermore, our method also
facilitates statistical inference by enabling the provision of 95% confidence
intervals grounded in asymptotic statistical theory. Simulation results
demonstrate our method's superior performance over existing approaches,
particularly in complex, long time-horizon scenarios. It remains effective in
small-sample, short-duration contexts, matching the performance of
asymptotically efficient estimators. To demonstrate our method in practice, we
applied our method to estimate counterfactual mean outcomes for standard versus
intensive blood pressure management strategies in a real-world cardiovascular
epidemiology cohort study.","['Toru Shirakawa', 'Yi Li', 'Yulun Wu', 'Sky Qiu', 'Yuxuan Li', 'Mingduo Zhao', 'Hiroyasu Iso', 'Mark van der Laan']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.AP', 'stat.ME']",2024-04-05 20:56:15+00:00
http://arxiv.org/abs/2404.04317v1,DeepLINK-T: deep learning inference for time series data using knockoffs and LSTM,"High-dimensional longitudinal time series data is prevalent across various
real-world applications. Many such applications can be modeled as regression
problems with high-dimensional time series covariates. Deep learning has been a
popular and powerful tool for fitting these regression models. Yet, the
development of interpretable and reproducible deep-learning models is
challenging and remains underexplored. This study introduces a novel method,
Deep Learning Inference using Knockoffs for Time series data (DeepLINK-T),
focusing on the selection of significant time series variables in regression
while controlling the false discovery rate (FDR) at a predetermined level.
DeepLINK-T combines deep learning with knockoff inference to control FDR in
feature selection for time series models, accommodating a wide variety of
feature distributions. It addresses dependencies across time and features by
leveraging a time-varying latent factor structure in time series covariates.
Three key ingredients for DeepLINK-T are 1) a Long Short-Term Memory (LSTM)
autoencoder for generating time series knockoff variables, 2) an LSTM
prediction network using both original and knockoff variables, and 3) the
application of the knockoffs framework for variable selection with FDR control.
Extensive simulation studies have been conducted to evaluate DeepLINK-T's
performance, showing its capability to control FDR effectively while
demonstrating superior feature selection power for high-dimensional
longitudinal time series data compared to its non-time series counterpart.
DeepLINK-T is further applied to three metagenomic data sets, validating its
practical utility and effectiveness, and underscoring its potential in
real-world applications.","['Wenxuan Zuo', 'Zifan Zhu', 'Yuxuan Du', 'Yi-Chun Yeh', 'Jed A. Fuhrman', 'Jinchi Lv', 'Yingying Fan', 'Fengzhu Sun']","['stat.ML', 'cs.LG', 'q-bio.QM']",2024-04-05 17:47:50+00:00
http://arxiv.org/abs/2404.04057v3,Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation,"We introduce Score identity Distillation (SiD), an innovative data-free
method that distills the generative capabilities of pretrained diffusion models
into a single-step generator. SiD not only facilitates an exponentially fast
reduction in Fr\'echet inception distance (FID) during distillation but also
approaches or even exceeds the FID performance of the original teacher
diffusion models. By reformulating forward diffusion processes as semi-implicit
distributions, we leverage three score-related identities to create an
innovative loss mechanism. This mechanism achieves rapid FID reduction by
training the generator using its own synthesized images, eliminating the need
for real data or reverse-diffusion-based generation, all accomplished within
significantly shortened generation time. Upon evaluation across four benchmark
datasets, the SiD algorithm demonstrates high iteration efficiency during
distillation and surpasses competing distillation approaches, whether they are
one-step or few-step, data-free, or dependent on training data, in terms of
generation quality. This achievement not only redefines the benchmarks for
efficiency and effectiveness in diffusion distillation but also in the broader
field of diffusion-based generation. The PyTorch implementation is available at
https://github.com/mingyuanzhou/SiD","['Mingyuan Zhou', 'Huangjie Zheng', 'Zhendong Wang', 'Mingzhang Yin', 'Hai Huang']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2024-04-05 12:30:19+00:00
http://arxiv.org/abs/2404.03916v2,Estimating mixed memberships in multi-layer networks,"Community detection in multi-layer networks has emerged as a crucial area of
modern network analysis. However, conventional approaches often assume that
nodes belong exclusively to a single community, which fails to capture the
complex structure of real-world networks where nodes may belong to multiple
communities simultaneously. To address this limitation, we propose novel
spectral methods to estimate the common mixed memberships in the multi-layer
mixed membership stochastic block model. The proposed methods leverage the
eigen-decomposition of three aggregate matrices: the sum of adjacency matrices,
the debiased sum of squared adjacency matrices, and the sum of squared
adjacency matrices. We establish rigorous theoretical guarantees for the
consistency of our methods. Specifically, we derive per-node error rates under
mild conditions on network sparsity, demonstrating their consistency as the
number of nodes and/or layers increases under the multi-layer mixed membership
stochastic block model. Our theoretical results reveal that the method
leveraging the sum of adjacency matrices generally performs poorer than the
other two methods for mixed membership estimation in multi-layer networks. We
conduct extensive numerical experiments to empirically validate our theoretical
findings. For real-world multi-layer networks with unknown community
information, we introduce two novel modularity metrics to quantify the quality
of mixed membership community detection. Finally, we demonstrate the practical
applications of our algorithms and modularity metrics by applying them to
real-world multi-layer networks, demonstrating their effectiveness in
extracting meaningful community structures.",['Huan Qing'],"['cs.SI', 'stat.ML']",2024-04-05 07:02:10+00:00
http://arxiv.org/abs/2404.03900v1,Nonparametric Modern Hopfield Models,"We present a nonparametric construction for deep learning compatible modern
Hopfield models and utilize this framework to debut an efficient variant. Our
key contribution stems from interpreting the memory storage and retrieval
processes in modern Hopfield models as a nonparametric regression problem
subject to a set of query-memory pairs. Crucially, our framework not only
recovers the known results from the original dense modern Hopfield model but
also fills the void in the literature regarding efficient modern Hopfield
models, by introducing \textit{sparse-structured} modern Hopfield models with
sub-quadratic complexity. We establish that this sparse model inherits the
appealing theoretical properties of its dense analogue -- connection with
transformer attention, fixed point convergence and exponential memory capacity
-- even without knowing details of the Hopfield energy function. Additionally,
we showcase the versatility of our framework by constructing a family of modern
Hopfield models as extensions, including linear, random masked, top-$K$ and
positive random feature modern Hopfield models. Empirically, we validate the
efficacy of our framework in both synthetic and realistic settings.","['Jerry Yao-Chieh Hu', 'Bo-Yu Chen', 'Dennis Wu', 'Feng Ruan', 'Han Liu']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.NE']",2024-04-05 05:46:20+00:00
http://arxiv.org/abs/2404.03878v2,Wasserstein F-tests for Fréchet regression on Bures-Wasserstein manifolds,"This paper considers the problem of regression analysis with random
covariance matrix as outcome and Euclidean covariates in the framework of
Fr\'echet regression on the Bures-Wasserstein manifold. Such regression
problems have many applications in single cell genomics and neuroscience, where
we have covariance matrix measured over a large set of samples. Fr\'echet
regression on the Bures-Wasserstein manifold is formulated as estimating the
conditional Fr\'echet mean given covariates $x$. A non-asymptotic
$\sqrt{n}$-rate of convergence (up to $\log n$ factors) is obtained for our
estimator $\hat{Q}_n(x)$ uniformly for $\left\|x\right\| \lesssim \sqrt{\log
n}$, which is crucial for deriving the asymptotic null distribution and power
of our proposed statistical test for the null hypothesis of no association. In
addition, a central limit theorem for the point estimate $\hat{Q}_n(x)$ is
obtained, giving insights to a test for covariate effects. The null
distribution of the test statistic is shown to converge to a weighted sum of
independent chi-squares, which implies that the proposed test has the desired
significance level asymptotically. Also, the power performance of the test is
demonstrated against a sequence of contiguous alternatives. Simulation results
show the accuracy of the asymptotic distributions. The proposed methods are
applied to a single cell gene expression data set that shows the change of gene
co-expression network as people age.","['Haoshu Xu', 'Hongzhe Li']","['stat.ME', 'stat.ML']",2024-04-05 04:01:51+00:00
http://arxiv.org/abs/2404.03867v1,Dimension-free Relaxation Times of Informed MCMC Samplers on Discrete Spaces,"Convergence analysis of Markov chain Monte Carlo methods in high-dimensional
statistical applications is increasingly recognized. In this paper, we develop
general mixing time bounds for Metropolis-Hastings algorithms on discrete
spaces by building upon and refining some recent theoretical advancements in
Bayesian model selection problems. We establish sufficient conditions for a
class of informed Metropolis-Hastings algorithms to attain relaxation times
that are independent of the problem dimension. These conditions are grounded in
high-dimensional statistical theory and allow for possibly multimodal posterior
distributions. We obtain our results through two independent techniques: the
multicommodity flow method and single-element drift condition analysis; we find
that the latter yields a tighter mixing time bound. Our results and proof
techniques are readily applicable to a broad spectrum of statistical problems
with discrete parameter spaces.","['Hyunwoong Chang', 'Quan Zhou']","['stat.CO', 'math.PR', 'stat.ML', '60J10, 60J20, 82M31, 62F15']",2024-04-05 02:40:45+00:00
http://arxiv.org/abs/2404.03842v2,The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs,"We study the algorithmic task of finding large independent sets in
Erdos-Renyi $r$-uniform hypergraphs on $n$ vertices having average degree $d$.
Krivelevich and Sudakov showed that the maximum independent set has density
$\left(\frac{r\log d}{(r-1)d}\right)^{1/(r-1)}$. We show that the class of
low-degree polynomial algorithms can find independent sets of density
$\left(\frac{\log d}{(r-1)d}\right)^{1/(r-1)}$ but no larger. This extends and
generalizes earlier results of Gamarnik and Sudan, Rahman and Virag, and Wein
on graphs, and answers a question of Bal and Bennett. We conjecture that this
statistical-computational gap holds for this problem.
  Additionally, we explore the universality of this gap by examining
$r$-partite hypergraphs. A hypergraph $H=(V,E)$ is $r$-partite if there is a
partition $V=V_1\cup\cdots\cup V_r$ such that each edge contains exactly one
vertex from each set $V_i$. We consider the problem of finding large balanced
independent sets (independent sets containing the same number of vertices in
each partition) in random $r$-partite hypergraphs with $n$ vertices in each
partition and average degree $d$. We prove that the maximum balanced
independent set has density $\left(\frac{r\log d}{(r-1)d}\right)^{1/(r-1)}$
asymptotically. Furthermore, we prove an analogous low-degree computational
threshold of $\left(\frac{\log d}{(r-1)d}\right)^{1/(r-1)}$. Our results
recover and generalize recent work of Perkins and the second author on
bipartite graphs.
  While the graph case has been extensively studied, this work is the first to
consider statistical-computational gaps of optimization problems on random
hypergraphs. Our results suggest that these gaps persist for larger
uniformities as well as across many models. A somewhat surprising aspect of the
gap for balanced independent sets is that the algorithm achieving the lower
bound is a simple degree-1 polynomial.","['Abhishek Dhawan', 'Yuzhou Wang']","['cs.CC', 'cs.DS', 'math.CO', 'math.PR', 'stat.ML']",2024-04-05 00:25:00+00:00
http://arxiv.org/abs/2404.03830v2,BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model,"We introduce the \textbf{B}i-Directional \textbf{S}parse \textbf{Hop}field
Network (\textbf{BiSHop}), a novel end-to-end framework for deep tabular
learning. BiSHop handles the two major challenges of deep tabular learning:
non-rotationally invariant data structure and feature sparsity in tabular data.
Our key motivation comes from the recent established connection between
associative memory and attention mechanisms. Consequently, BiSHop uses a
dual-component approach, sequentially processing data both column-wise and
row-wise through two interconnected directional learning modules.
Computationally, these modules house layers of generalized sparse modern
Hopfield layers, a sparse extension of the modern Hopfield model with adaptable
sparsity. Methodologically, BiSHop facilitates multi-scale representation
learning, capturing both intra-feature and inter-feature interactions, with
adaptive sparsity at each scale. Empirically, through experiments on diverse
real-world datasets, we demonstrate that BiSHop surpasses current SOTA methods
with significantly less HPO runs, marking it a robust solution for deep tabular
learning.","['Chenwei Xu', 'Yu-Chao Huang', 'Jerry Yao-Chieh Hu', 'Weijian Li', 'Ammar Gilani', 'Hsi-Sheng Goan', 'Han Liu']","['cs.LG', 'cs.AI', 'stat.ML']",2024-04-04 23:13:32+00:00
http://arxiv.org/abs/2404.03828v2,Outlier-Efficient Hopfield Layers for Large Transformer-Based Models,"We introduce an Outlier-Efficient Modern Hopfield Model (termed
$\mathrm{OutEffHop}$) and use it to address the outlier inefficiency problem of
{training} gigantic transformer-based models. Our main contribution is a novel
associative memory model facilitating \textit{outlier-efficient} associative
memory retrievals. Interestingly, this memory model manifests a model-based
interpretation of an outlier-efficient attention mechanism (${\rm Softmax}_1$):
it is an approximation of the memory retrieval process of $\mathrm{OutEffHop}$.
Methodologically, this allows us to introduce novel outlier-efficient Hopfield
layers as powerful alternatives to traditional attention mechanisms, with
superior post-quantization performance. Theoretically, the Outlier-Efficient
Modern Hopfield Model retains and improves the desirable properties of standard
modern Hopfield models, including fixed point convergence and exponential
storage capacity. Empirically, we demonstrate the efficacy of the proposed
model across large-scale transformer-based and Hopfield-based models (including
BERT, OPT, ViT, and STanHop-Net), benchmarking against state-of-the-art methods
like $\mathtt{Clipped\_Softmax}$ and $\mathtt{Gated\_Attention}$. Notably,
$\mathrm{OutEffHop}$ achieves an average reduction of 22+\% in average kurtosis
and 26+\% in the maximum infinity norm of model outputs across four models.
Code is available at \href{https://github.com/MAGICS-LAB/OutEffHop}{GitHub};
models are on
\href{https://huggingface.co/collections/magicslabnu/outeffhop-6610fcede8d2cda23009a98f}{Hugging
Face Hub}; future updates are on
\href{https://arxiv.org/abs/2404.03828}{arXiv}.","['Jerry Yao-Chieh Hu', 'Pei-Hsuan Chang', 'Robin Luo', 'Hong-Yu Chen', 'Weijian Li', 'Wei-Po Wang', 'Han Liu']","['cs.LG', 'cs.AI', 'stat.ML']",2024-04-04 23:08:43+00:00
http://arxiv.org/abs/2404.03827v3,Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models,"We propose a two-stage memory retrieval dynamics for modern Hopfield models,
termed $\mathtt{U\text{-}Hop}$, with enhanced memory capacity. Our key
contribution is a learnable feature map $\Phi$ which transforms the Hopfield
energy function into kernel space. This transformation ensures convergence
between the local minima of energy and the fixed points of retrieval dynamics
within the kernel space. Consequently, the kernel norm induced by $\Phi$ serves
as a novel similarity measure. It utilizes the stored memory patterns as
learning data to enhance memory capacity across all modern Hopfield models.
Specifically, we accomplish this by constructing a separation loss
$\mathcal{L}_\Phi$ that separates the local minima of kernelized energy by
separating stored memory patterns in kernel space. Methodologically,
$\mathtt{U\text{-}Hop}$ memory retrieval process consists of: (Stage I)
minimizing separation loss for a more uniform memory (local minimum)
distribution, followed by (Stage II) standard Hopfield energy minimization for
memory retrieval. This results in a significant reduction of possible
metastable states in the Hopfield energy function, thus enhancing memory
capacity by preventing memory confusion. Empirically, with real-world datasets,
we demonstrate that $\mathtt{U\text{-}Hop}$ outperforms all existing modern
Hopfield models and state-of-the-art similarity measures, achieving substantial
improvements in both associative memory retrieval and deep learning tasks. Code
is available at https://github.com/MAGICS-LAB/UHop ; future updates are on
arXiv:2404.03827","['Dennis Wu', 'Jerry Yao-Chieh Hu', 'Teng-Yun Hsiao', 'Han Liu']","['cs.LG', 'cs.AI', 'stat.ML']",2024-04-04 23:05:30+00:00
http://arxiv.org/abs/2404.03804v1,"TransformerLSR: Attentive Joint Model of Longitudinal Data, Survival, and Recurrent Events with Concurrent Latent Structure","In applications such as biomedical studies, epidemiology, and social
sciences, recurrent events often co-occur with longitudinal measurements and a
terminal event, such as death. Therefore, jointly modeling longitudinal
measurements, recurrent events, and survival data while accounting for their
dependencies is critical. While joint models for the three components exist in
statistical literature, many of these approaches are limited by heavy
parametric assumptions and scalability issues. Recently, incorporating deep
learning techniques into joint modeling has shown promising results. However,
current methods only address joint modeling of longitudinal measurements at
regularly-spaced observation times and survival events, neglecting recurrent
events. In this paper, we develop TransformerLSR, a flexible transformer-based
deep modeling and inference framework to jointly model all three components
simultaneously. TransformerLSR integrates deep temporal point processes into
the joint modeling framework, treating recurrent and terminal events as two
competing processes dependent on past longitudinal measurements and recurrent
event times. Additionally, TransformerLSR introduces a novel trajectory
representation and model architecture to potentially incorporate a priori
knowledge of known latent structures among concurrent longitudinal variables.
We demonstrate the effectiveness and necessity of TransformerLSR through
simulation studies and analyzing a real-world medical dataset on patients after
kidney transplantation.","['Zhiyue Zhang', 'Yao Zhao', 'Yanxun Xu']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.ME']",2024-04-04 20:51:37+00:00
http://arxiv.org/abs/2404.03586v1,Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning,"Effective verification and validation techniques for modern scientific
machine learning workflows are challenging to devise. Statistical methods are
abundant and easily deployed, but often rely on speculative assumptions about
the data and methods involved. Error bounds for classical interpolation
techniques can provide mathematically rigorous estimates of accuracy, but often
are difficult or impractical to determine computationally. In this work, we
present a best-of-both-worlds approach to verifiable scientific machine
learning by demonstrating that (1) multiple standard interpolation techniques
have informative error bounds that can be computed or estimated efficiently;
(2) comparative performance among distinct interpolants can aid in validation
goals; (3) deploying interpolation methods on latent spaces generated by deep
learning techniques enables some interpretability for black-box models. We
present a detailed case study of our approach for predicting lift-drag ratios
from airfoil images. Code developed for this work is available in a public
Github repository.","['Tyler Chang', 'Andrew Gillette', 'Romit Maulik']","['cs.LG', 'stat.ML']",2024-04-04 16:52:17+00:00
http://arxiv.org/abs/2404.03578v2,Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm,"The sim-to-real gap, which represents the disparity between training and
testing environments, poses a significant challenge in reinforcement learning
(RL). A promising approach to addressing this challenge is distributionally
robust RL, often framed as a robust Markov decision process (RMDP). In this
framework, the objective is to find a robust policy that achieves good
performance under the worst-case scenario among all environments within a
pre-specified uncertainty set centered around the training environment. Unlike
previous work, which relies on a generative model or a pre-collected offline
dataset enjoying good coverage of the deployment environment, we tackle robust
RL via interactive data collection, where the learner interacts with the
training environment only and refines the policy through trial and error. In
this robust RL paradigm, two main challenges emerge: managing distributional
robustness while striking a balance between exploration and exploitation during
data collection. Initially, we establish that sample-efficient learning without
additional assumptions is unattainable owing to the curse of support shift;
i.e., the potential disjointedness of the distributional supports between the
training and testing environments. To circumvent such a hardness result, we
introduce the vanishing minimal value assumption to RMDPs with a
total-variation (TV) distance robust set, postulating that the minimal value of
the optimal robust value function is zero. We prove that such an assumption
effectively eliminates the support shift issue for RMDPs with a TV distance
robust set, and present an algorithm with a provable sample complexity
guarantee. Our work makes the initial step to uncovering the inherent
difficulty of robust RL via interactive data collection and sufficient
conditions for designing a sample-efficient algorithm accompanied by sharp
sample complexity analysis.","['Miao Lu', 'Han Zhong', 'Tong Zhang', 'Jose Blanchet']","['cs.LG', 'stat.ML']",2024-04-04 16:40:22+00:00
http://arxiv.org/abs/2404.03524v1,Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data,"This work focuses on the challenges of non-IID data and stragglers/dropouts
in federated learning. We introduce and explore a privacy-flexible paradigm
that models parts of the clients' local data as non-private, offering a more
versatile and business-oriented perspective on privacy. Within this framework,
we propose a data-driven strategy for mitigating the effects of label
heterogeneity and client straggling on federated learning. Our solution
combines both offline data sharing and approximate gradient coding techniques.
Through numerical simulations using the MNIST dataset, we demonstrate that our
approach enables achieving a deliberate trade-off between privacy and utility,
leading to improved model convergence and accuracy while using an adaptable
portion of non-private data.","['Okko Makkonen', 'Sampo Niemelä', 'Camilla Hollanti', 'Serge Kas Hanna']","['cs.LG', 'cs.CR', 'cs.DC', 'cs.IT', 'math.IT', 'stat.ML']",2024-04-04 15:29:50+00:00
http://arxiv.org/abs/2404.03506v1,CountARFactuals -- Generating plausible model-agnostic counterfactual explanations with adversarial random forests,"Counterfactual explanations elucidate algorithmic decisions by pointing to
scenarios that would have led to an alternative, desired outcome. Giving
insight into the model's behavior, they hint users towards possible actions and
give grounds for contesting decisions. As a crucial factor in achieving these
goals, counterfactuals must be plausible, i.e., describing realistic
alternative scenarios within the data manifold. This paper leverages a recently
developed generative modeling technique -- adversarial random forests (ARFs) --
to efficiently generate plausible counterfactuals in a model-agnostic way. ARFs
can serve as a plausibility measure or directly generate counterfactual
explanations. Our ARF-based approach surpasses the limitations of existing
methods that aim to generate plausible counterfactual explanations: It is easy
to train and computationally highly efficient, handles continuous and
categorical data naturally, and allows integrating additional desiderata such
as sparsity in a straightforward manner.","['Susanne Dandl', 'Kristin Blesch', 'Timo Freiesleben', 'Gunnar König', 'Jan Kapar', 'Bernd Bischl', 'Marvin Wright']","['stat.ML', 'cs.LG']",2024-04-04 15:10:13+00:00
http://arxiv.org/abs/2404.03331v1,LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace,"Bilevel optimization, with broad applications in machine learning, has an
intricate hierarchical structure. Gradient-based methods have emerged as a
common approach to large-scale bilevel problems. However, the computation of
the hyper-gradient, which involves a Hessian inverse vector product, confines
the efficiency and is regarded as a bottleneck. To circumvent the inverse, we
construct a sequence of low-dimensional approximate Krylov subspaces with the
aid of the Lanczos process. As a result, the constructed subspace is able to
dynamically and incrementally approximate the Hessian inverse vector product
with less effort and thus leads to a favorable estimate of the hyper-gradient.
Moreover, we propose a~provable subspace-based framework for bilevel problems
where one central step is to solve a small-size tridiagonal linear system. To
the best of our knowledge, this is the first time that subspace techniques are
incorporated into bilevel optimization. This successful trial not only enjoys
$\mathcal{O}(\epsilon^{-1})$ convergence rate but also demonstrates efficiency
in a synthetic problem and two deep learning tasks.","['Bin Gao', 'Yan Yang', 'Ya-xiang Yuan']","['math.OC', 'cs.LG', 'stat.ML']",2024-04-04 09:57:29+00:00
http://arxiv.org/abs/2404.03329v2,DeepFunction: Deep Metric Learning-based Imbalanced Classification for Diagnosing Threaded Pipe Connection Defects using Functional Data,"In modern manufacturing, most of the product lines are conforming. Few
products are nonconforming but with different defect types. The identification
of defect types can help further root cause diagnosis of production lines. With
the sensing development, signals of process variables can be collected in high
resolution, which can be regarded as multichannel functional data. They have
abundant information to characterize the process and help identify the defect
types. Motivated by a real example from the pipe tightening process, we focus
on defect classification where each sample is a multichannel functional data.
However, the available samples for each defect type are limited and imbalanced.
Moreover, the functions are incomplete since the pre-tightening process before
the pipe tightening process is unobserved. To classify the defect samples based
on imbalanced, multichannel, and incomplete functional data is very important
but challenging. Thus, we propose an innovative classification framework based
on deep metric learning using functional data (DeepFunction). The framework
leverages the power of deep metric learning to train on imbalanced datasets. A
neural network specially crafted for processing functional data is also
proposed to handle multichannel and incomplete functional data. The results
from a real-world case study demonstrate the superior accuracy of our framework
when compared to existing benchmarks.","['Yukun Xie', 'Juan Du', 'Chen Zhang']","['cs.LG', 'eess.SP', 'stat.ML']",2024-04-04 09:55:11+00:00
