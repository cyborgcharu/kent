id,title,abstract,authors,categories,date
http://arxiv.org/abs/2205.12379v3,Gaussian Pre-Activations in Neural Networks: Myth or Reality?,"The study of feature propagation at initialization in neural networks lies at
the root of numerous initialization designs. An assumption very commonly made
in the field states that the pre-activations are Gaussian. Although this
convenient Gaussian hypothesis can be justified when the number of neurons per
layer tends to infinity, it is challenged by both theoretical and experimental
works for finite-width neural networks. Our major contribution is to construct
a family of pairs of activation functions and initialization distributions that
ensure that the pre-activations remain Gaussian throughout the network's depth,
even in narrow neural networks. In the process, we discover a set of
constraints that a neural network should fulfill to ensure Gaussian
pre-activations. Additionally, we provide a critical review of the claims of
the Edge of Chaos line of works and build an exact Edge of Chaos analysis. We
also propose a unified view on pre-activations propagation, encompassing the
framework of several well-known initialization procedures. Finally, our work
provides a principled framework for answering the much-debated question: is it
desirable to initialize the training of a neural network whose pre-activations
are ensured to be Gaussian?","['Pierre Wolinski', 'Julyan Arbel']","['cs.LG', 'stat.ML']",2022-05-24 21:52:23+00:00
http://arxiv.org/abs/2205.12365v2,"Low-rank Optimal Transport: Approximation, Statistics and Debiasing","The matching principles behind optimal transport (OT) play an increasingly
important role in machine learning, a trend which can be observed when OT is
used to disambiguate datasets in applications (e.g. single-cell genomics) or
used to improve more complex methods (e.g. balanced attention in transformers
or self-supervised learning). To scale to more challenging problems, there is a
growing consensus that OT requires solvers that can operate on millions, not
thousands, of points. The low-rank optimal transport (LOT) approach advocated
in \cite{scetbon2021lowrank} holds several promises in that regard, and was
shown to complement more established entropic regularization approaches, being
able to insert itself in more complex pipelines, such as quadratic OT. LOT
restricts the search for low-cost couplings to those that have a
low-nonnegative rank, yielding linear time algorithms in cases of interest.
However, these promises can only be fulfilled if the LOT approach is seen as a
legitimate contender to entropic regularization when compared on properties of
interest, where the scorecard typically includes theoretical properties
(statistical complexity and relation to other methods) or practical aspects
(debiasing, hyperparameter tuning, initialization). We target each of these
areas in this paper in order to cement the impact of low-rank approaches in
computational OT.","['Meyer Scetbon', 'Marco Cuturi']","['stat.ML', 'cs.LG']",2022-05-24 20:51:37+00:00
http://arxiv.org/abs/2205.12258v4,History Compression via Language Models in Reinforcement Learning,"In a partially observable Markov decision process (POMDP), an agent typically
uses a representation of the past to approximate the underlying MDP. We propose
to utilize a frozen Pretrained Language Transformer (PLT) for history
representation and compression to improve sample efficiency. To avoid training
of the Transformer, we introduce FrozenHopfield, which automatically associates
observations with pretrained token embeddings. To form these associations, a
modern Hopfield network stores these token embeddings, which are retrieved by
queries that are obtained by a random but fixed projection of observations. Our
new method, HELM, enables actor-critic network architectures that contain a
pretrained language Transformer for history representation as a memory module.
Since a representation of the past need not be learned, HELM is much more
sample efficient than competitors. On Minigrid and Procgen environments HELM
achieves new state-of-the-art results. Our code is available at
https://github.com/ml-jku/helm.","['Fabian Paischer', 'Thomas Adler', 'Vihang Patil', 'Angela Bitto-Nemling', 'Markus Holzleitner', 'Sebastian Lehner', 'Hamid Eghbal-zadeh', 'Sepp Hochreiter']","['cs.LG', 'cs.CL', 'stat.ML']",2022-05-24 17:59:29+00:00
http://arxiv.org/abs/2205.12243v1,"EBM Life Cycle: MCMC Strategies for Synthesis, Defense, and Density Modeling","This work presents strategies to learn an Energy-Based Model (EBM) according
to the desired length of its MCMC sampling trajectories. MCMC trajectories of
different lengths correspond to models with different purposes. Our experiments
cover three different trajectory magnitudes and learning outcomes: 1) shortrun
sampling for image generation; 2) midrun sampling for classifier-agnostic
adversarial defense; and 3) longrun sampling for principled modeling of image
probability densities. To achieve these outcomes, we introduce three novel
methods of MCMC initialization for negative samples used in Maximum Likelihood
(ML) learning. With standard network architectures and an unaltered ML
objective, our MCMC initialization methods alone enable significant performance
gains across the three applications that we investigate. Our results include
state-of-the-art FID scores for unnormalized image densities on the CIFAR-10
and ImageNet datasets; state-of-the-art adversarial defense on CIFAR-10 among
purification methods and the first EBM defense on ImageNet; and scalable
techniques for learning valid probability densities. Code for this project can
be found at https://github.com/point0bar1/ebm-life-cycle.","['Mitch Hill', 'Jonathan Mitchell', 'Chu Chen', 'Yuan Du', 'Mubarak Shah', 'Song-Chun Zhu']","['stat.ML', 'cs.LG']",2022-05-24 17:52:29+00:00
http://arxiv.org/abs/2205.12201v1,Forecasting Multilinear Data via Transform-Based Tensor Autoregression,"In the era of big data, there is an increasing demand for new methods for
analyzing and forecasting 2-dimensional data. The current research aims to
accomplish these goals through the combination of time-series modeling and
multilinear algebraic systems. We expand previous autoregressive techniques to
forecast multilinear data, aptly named the L-Transform Tensor autoregressive
(L-TAR for short). Tensor decompositions and multilinear tensor products have
allowed for this approach to be a feasible method of forecasting. We achieve
statistical independence between the columns of the observations through
invertible discrete linear transforms, enabling a divide and conquer approach.
We present an experimental validation of the proposed methods on datasets
containing image collections, video sequences, sea surface temperature
measurements, stock prices, and networks.","['Jackson Cates', 'Randy C. Hoover', 'Kyle Caudle', 'Cagri Ozdemir', 'Karen Braman', 'David Machette']","['cs.LG', 'stat.AP', 'stat.ML']",2022-05-24 16:56:35+00:00
http://arxiv.org/abs/2205.12184v2,Distributional Hamilton-Jacobi-Bellman Equations for Continuous-Time Reinforcement Learning,"Continuous-time reinforcement learning offers an appealing formalism for
describing control problems in which the passage of time is not naturally
divided into discrete increments. Here we consider the problem of predicting
the distribution of returns obtained by an agent interacting in a
continuous-time, stochastic environment. Accurate return predictions have
proven useful for determining optimal policies for risk-sensitive control,
learning state representations, multiagent coordination, and more. We begin by
establishing the distributional analogue of the Hamilton-Jacobi-Bellman (HJB)
equation for It\^o diffusions and the broader class of Feller-Dynkin processes.
We then specialize this equation to the setting in which the return
distribution is approximated by $N$ uniformly-weighted particles, a common
design choice in distributional algorithms. Our derivation highlights
additional terms due to statistical diffusivity which arise from the proper
handling of distributions in the continuous-time setting. Based on this, we
propose a tractable algorithm for approximately solving the distributional HJB
based on a JKO scheme, which can be implemented in an online control algorithm.
We demonstrate the effectiveness of such an algorithm in a synthetic control
problem.","['Harley Wiltzer', 'David Meger', 'Marc G. Bellemare']","['cs.LG', 'math.OC', 'stat.ML']",2022-05-24 16:33:54+00:00
http://arxiv.org/abs/2206.06090v1,"Regret-Aware Black-Box Optimization with Natural Gradients, Trust-Regions and Entropy Control","Most successful stochastic black-box optimizers, such as CMA-ES, use rankings
of the individual samples to obtain a new search distribution. Yet, the use of
rankings also introduces several issues such as the underlying optimization
objective is often unclear, i.e., we do not optimize the expected fitness.
Further, while these algorithms typically produce a high-quality mean estimate
of the search distribution, the produced samples can have poor quality as these
algorithms are ignorant of the regret. Lastly, noisy fitness function
evaluations may result in solutions that are highly sub-optimal on expectation.
In contrast, stochastic optimizers that are motivated by policy gradients, such
as the Model-based Relative Entropy Stochastic Search (MORE) algorithm,
directly optimize the expected fitness function without the use of rankings.
MORE can be derived by applying natural policy gradients and compatible
function approximation, and is using information theoretic constraints to
ensure the stability of the policy update. While MORE does not suffer from the
listed limitations, it often cannot achieve state of the art performance in
comparison to ranking based methods. We improve MORE by decoupling the update
of the mean and covariance of the search distribution allowing for more
aggressive updates on the mean while keeping the update on the covariance
conservative, an improved entropy scheduling technique based on an evolution
path which results in faster convergence and a simplified and more effective
model learning approach in comparison to the original paper. We compare our
algorithm to state of the art black-box optimization algorithms on standard
optimization tasks as well as on episodic RL tasks in robotics where it is also
crucial to have small regret. We obtain competitive results on benchmark
functions and clearly outperform ranking-based methods in terms of regret on
the RL tasks.","['Maximilian Hüttenrauch', 'Gerhard Neumann']","['stat.ML', 'cs.LG', 'cs.NE']",2022-05-24 16:25:15+00:00
http://arxiv.org/abs/2205.12156v2,"Not too little, not too much: a theoretical analysis of graph (over)smoothing","We analyze graph smoothing with \emph{mean aggregation}, where each node
successively receives the average of the features of its neighbors. Indeed, it
has quickly been observed that Graph Neural Networks (GNNs), which generally
follow some variant of Message-Passing (MP) with repeated aggregation, may be
subject to the oversmoothing phenomenon: by performing too many rounds of MP,
the node features tend to converge to a non-informative limit. In the case of
mean aggregation, for connected graphs, the node features become constant
across the whole graph. At the other end of the spectrum, it is intuitively
obvious that some MP rounds are necessary, but existing analyses do not exhibit
both phenomena at once: beneficial ``finite'' smoothing and oversmoothing in
the limit. In this paper, we consider simplified linear GNNs, and rigorously
analyze two examples for which a finite number of mean aggregation steps
provably improves the learning performance, before oversmoothing kicks in. We
consider a latent space random graph model, where node features are partial
observations of the latent variables and the graph contains pairwise
relationships between them. We show that graph smoothing restores some of the
lost information, up to a certain point, by two phenomenon: graph smoothing
shrinks non-principal directions in the data faster than principal ones, which
is useful for regression, and shrinks nodes within communities faster than they
collapse together, which improves classification.",['Nicolas Keriven'],"['stat.ML', 'cs.LG']",2022-05-24 15:39:31+00:00
http://arxiv.org/abs/2205.12141v2,One-Pixel Shortcut: on the Learning Preference of Deep Neural Networks,"Unlearnable examples (ULEs) aim to protect data from unauthorized usage for
training DNNs. Existing work adds $\ell_\infty$-bounded perturbations to the
original sample so that the trained model generalizes poorly. Such
perturbations, however, are easy to eliminate by adversarial training and data
augmentations. In this paper, we resolve this problem from a novel perspective
by perturbing only one pixel in each image. Interestingly, such a small
modification could effectively degrade model accuracy to almost an untrained
counterpart. Moreover, our produced \emph{One-Pixel Shortcut (OPS)} could not
be erased by adversarial training and strong augmentations. To generate OPS, we
perturb in-class images at the same position to the same target value that
could mostly and stably deviate from all the original images. Since such
generation is only based on images, OPS needs significantly less computation
cost than the previous methods using DNN generators. Based on OPS, we introduce
an unlearnable dataset called CIFAR-10-S, which is indistinguishable from
CIFAR-10 by humans but induces the trained model to extremely low accuracy.
Even under adversarial training, a ResNet-18 trained on CIFAR-10-S has only
10.61% accuracy, compared to 83.02% by the existing error-minimizing method.","['Shutong Wu', 'Sizhe Chen', 'Cihang Xie', 'Xiaolin Huang']","['cs.LG', 'stat.ML']",2022-05-24 15:17:52+00:00
http://arxiv.org/abs/2205.12112v2,Stereographic Markov Chain Monte Carlo,"High-dimensional distributions, especially those with heavy tails, are
notoriously difficult for off-the-shelf MCMC samplers: the combination of
unbounded state spaces, diminishing gradient information, and local moves
results in empirically observed ``stickiness'' and poor theoretical mixing
properties -- lack of geometric ergodicity. In this paper, we introduce a new
class of MCMC samplers that map the original high-dimensional problem in
Euclidean space onto a sphere and remedy these notorious mixing problems. In
particular, we develop random-walk Metropolis type algorithms as well as
versions of the Bouncy Particle Sampler that are uniformly ergodic for a large
class of light and heavy-tailed distributions and also empirically exhibit
rapid convergence in high dimensions. In the best scenario, the proposed
samplers can enjoy the ``blessings of dimensionality'' that the convergence is
faster in higher dimensions.","['Jun Yang', 'Krzysztof Łatuszyński', 'Gareth O. Roberts']","['stat.CO', 'stat.ME', 'stat.ML']",2022-05-24 14:43:23+00:00
http://arxiv.org/abs/2205.12086v3,Information-Directed Selection for Top-Two Algorithms,"We consider the best-k-arm identification problem for multi-armed bandits,
where the objective is to select the exact set of k arms with the highest mean
rewards by sequentially allocating measurement effort. We characterize the
necessary and sufficient conditions for the optimal allocation using dual
variables. Remarkably these optimality conditions lead to the extension of
top-two algorithm design principle (Russo, 2020), initially proposed for
best-arm identification. Furthermore, our optimality conditions induce a simple
and effective selection rule dubbed information-directed selection (IDS) that
selects one of the top-two candidates based on a measure of information gain.
As a theoretical guarantee, we prove that integrated with IDS, top-two Thompson
sampling is (asymptotically) optimal for Gaussian best-arm identification,
solving a glaring open problem in the pure exploration literature (Russo,
2020). As a by-product, we show that for k > 1, top-two algorithms cannot
achieve optimality even when the algorithm has access to the unknown ""optimal""
tuning parameter. Numerical experiments show the superior performance of the
proposed top-two algorithms with IDS and considerable improvement compared with
algorithms without adaptive selection.","['Wei You', 'Chao Qin', 'Zihao Wang', 'Shuoguang Yang']","['stat.ML', 'cs.LG']",2022-05-24 14:07:13+00:00
http://arxiv.org/abs/2205.11970v2,Weak Convergence of Approximate reflection coupling and its Application to Non-convex Optimization,"In this paper, we propose a weak approximation of the reflection coupling
(RC) for stochastic differential equations (SDEs), and prove it converges
weakly to the desired coupling. In contrast to the RC, the proposed approximate
reflection coupling (ARC) need not take the hitting time of processes to the
diagonal set into consideration and can be defined as the solution of some SDEs
on the whole time interval. Therefore, ARC can work effectively against SDEs
with different drift terms. As an application of ARC, an evaluation on the
effectiveness of the stochastic gradient descent in a non-convex setting is
also described. For the sample size $n$, the step size $\eta$, and the batch
size $B$, we derive uniform evaluations on the time with orders $n^{-1}$,
$\eta^{1/2}$, and $\sqrt{(n - B) / B (n - 1)}$, respectively.",['Keisuke Suzuki'],"['math.PR', 'stat.ML', 'Primary 60J20, Secondary 60H10']",2022-05-24 11:08:13+00:00
http://arxiv.org/abs/2205.11956v4,Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control,"Most machine learning methods require tuning of hyper-parameters. For kernel
ridge regression with the Gaussian kernel, the hyper-parameter is the
bandwidth. The bandwidth specifies the length scale of the kernel and has to be
carefully selected to obtain a model with good generalization. The default
methods for bandwidth selection, cross-validation and marginal likelihood
maximization, often yield good results, albeit at high computational costs.
Inspired by Jacobian regularization, we formulate an approximate expression for
how the derivatives of the functions inferred by kernel ridge regression with
the Gaussian kernel depend on the kernel bandwidth. We use this expression to
propose a closed-form, computationally feather-light, bandwidth selection
heuristic, based on controlling the Jacobian. In addition, the Jacobian
expression illuminates how the bandwidth selection is a trade-off between the
smoothness of the inferred function and the conditioning of the training data
kernel matrix. We show on real and synthetic data that compared to
cross-validation and marginal likelihood maximization, our method is on pair in
terms of model performance, but up to six orders of magnitude faster.","['Oskar Allerbo', 'Rebecka Jörnsten']","['stat.ML', 'cs.LG', 'stat.ME']",2022-05-24 10:36:05+00:00
http://arxiv.org/abs/2205.11894v3,Learning Interacting Dynamical Systems with Latent Gaussian Process ODEs,"We study time uncertainty-aware modeling of continuous-time dynamics of
interacting objects. We introduce a new model that decomposes independent
dynamics of single objects accurately from their interactions. By employing
latent Gaussian process ordinary differential equations, our model infers both
independent dynamics and their interactions with reliable uncertainty
estimates. In our formulation, each object is represented as a graph node and
interactions are modeled by accumulating the messages coming from neighboring
objects. We show that efficient inference of such a complex network of
variables is possible with modern variational sparse Gaussian process inference
techniques. We empirically demonstrate that our model improves the reliability
of long-term predictions over neural network based alternatives and it
successfully handles missing dynamic or static information. Furthermore, we
observe that only our model can successfully encapsulate independent dynamics
and interaction information in distinct functions and show the benefit from
this disentanglement in extrapolation scenarios.","['Çağatay Yıldız', 'Melih Kandemir', 'Barbara Rakitsch']","['cs.LG', 'stat.ML']",2022-05-24 08:36:25+00:00
http://arxiv.org/abs/2205.11890v2,A Quadrature Rule combining Control Variates and Adaptive Importance Sampling,"Driven by several successful applications such as in stochastic gradient
descent or in Bayesian computation, control variates have become a major tool
for Monte Carlo integration. However, standard methods do not allow the
distribution of the particles to evolve during the algorithm, as is the case in
sequential simulation methods. Within the standard adaptive importance sampling
framework, a simple weighted least squares approach is proposed to improve the
procedure with control variates. The procedure takes the form of a quadrature
rule with adapted quadrature weights to reflect the information brought in by
the control variates. The quadrature points and weights do not depend on the
integrand, a computational advantage in case of multiple integrands. Moreover,
the target density needs to be known only up to a multiplicative constant. Our
main result is a non-asymptotic bound on the probabilistic error of the
procedure. The bound proves that for improving the estimate's accuracy, the
benefits from adaptive importance sampling and control variates can be
combined. The good behavior of the method is illustrated empirically on
synthetic examples and real-world data for Bayesian linear regression.","['Rémi Leluc', 'François Portier', 'Johan Segers', 'Aigerim Zhuman']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-05-24 08:21:45+00:00
http://arxiv.org/abs/2205.11827v2,Advanced Manufacturing Configuration by Sample-efficient Batch Bayesian Optimization,"We propose a framework for the configuration and operation of
expensive-to-evaluate advanced manufacturing methods, based on Bayesian
optimization. The framework unifies a tailored acquisition function, a parallel
acquisition procedure, and the integration of process information providing
context to the optimization procedure. \cmtb{The novel acquisition function is
demonstrated, analyzed and compared on state-of-the-art benchmarking problems.
We apply the optimization approach to atmospheric plasma spraying and fused
deposition modeling.} Our results demonstrate that the proposed framework can
efficiently find input parameters that produce the desired outcome and minimize
the process cost.","['Xavier Guidetti', 'Alisa Rupenyan', 'Lutz Fassl', 'Majid Nabavi', 'John Lygeros']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2022-05-24 06:45:06+00:00
http://arxiv.org/abs/2205.11801v4,SepIt: Approaching a Single Channel Speech Separation Bound,"We present an upper bound for the Single Channel Speech Separation task,
which is based on an assumption regarding the nature of short segments of
speech. Using the bound, we are able to show that while the recent methods have
made significant progress for a few speakers, there is room for improvement for
five and ten speakers. We then introduce a Deep neural network, SepIt, that
iteratively improves the different speakers' estimation. At test time, SpeIt
has a varying number of iterations per test sample, based on a mutual
information criterion that arises from our analysis. In an extensive set of
experiments, SepIt outperforms the state-of-the-art neural networks for 2, 3,
5, and 10 speakers.","['Shahar Lutati', 'Eliya Nachmani', 'Lior Wolf']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2022-05-24 05:40:36+00:00
http://arxiv.org/abs/2205.11787v3,Quadratic models for understanding catapult dynamics of neural networks,"While neural networks can be approximated by linear models as their width
increases, certain properties of wide neural networks cannot be captured by
linear models. In this work we show that recently proposed Neural Quadratic
Models can exhibit the ""catapult phase"" [Lewkowycz et al. 2020] that arises
when training such models with large learning rates. We then empirically show
that the behaviour of neural quadratic models parallels that of neural networks
in generalization, especially in the catapult phase regime. Our analysis
further demonstrates that quadratic models can be an effective tool for
analysis of neural networks.","['Libin Zhu', 'Chaoyue Liu', 'Adityanarayanan Radhakrishnan', 'Mikhail Belkin']","['cs.LG', 'math.OC', 'stat.ML']",2022-05-24 05:03:06+00:00
http://arxiv.org/abs/2205.11786v2,Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture,"In this paper we show that feedforward neural networks corresponding to
arbitrary directed acyclic graphs undergo transition to linearity as their
""width"" approaches infinity. The width of these general networks is
characterized by the minimum in-degree of their neurons, except for the input
and first layers. Our results identify the mathematical structure underlying
transition to linearity and generalize a number of recent works aimed at
characterizing transition to linearity or constancy of the Neural Tangent
Kernel for standard architectures.","['Libin Zhu', 'Chaoyue Liu', 'Mikhail Belkin']","['cs.LG', 'math.OC', 'stat.ML']",2022-05-24 04:57:35+00:00
http://arxiv.org/abs/2205.11765v2,Byzantine-Robust Federated Learning with Optimal Statistical Rates and Privacy Guarantees,"We propose Byzantine-robust federated learning protocols with nearly optimal
statistical rates. In contrast to prior work, our proposed protocols improve
the dimension dependence and achieve a tight statistical rate in terms of all
the parameters for strongly convex losses. We benchmark against competing
protocols and show the empirical superiority of the proposed protocols.
Finally, we remark that our protocols with bucketing can be naturally combined
with privacy-guaranteeing procedures to introduce security against a
semi-honest server. The code for evaluation is provided in
https://github.com/wanglun1996/secure-robust-federated-learning.","['Banghua Zhu', 'Lun Wang', 'Qi Pang', 'Shuai Wang', 'Jiantao Jiao', 'Dawn Song', 'Michael I. Jordan']","['cs.LG', 'cs.AI', 'cs.CR', 'cs.DC', 'stat.ML']",2022-05-24 04:03:07+00:00
http://arxiv.org/abs/2205.11735v1,Soft-SVM Regression For Binary Classification,"The binomial deviance and the SVM hinge loss functions are two of the most
widely used loss functions in machine learning. While there are many
similarities between them, they also have their own strengths when dealing with
different types of data. In this work, we introduce a new exponential family
based on a convex relaxation of the hinge loss function using softness and
class-separation parameters. This new family, denoted Soft-SVM, allows us to
prescribe a generalized linear model that effectively bridges between logistic
regression and SVM classification. This new model is interpretable and avoids
data separability issues, attaining good fitting and predictive performance by
automatically adjusting for data label separability via the softness parameter.
These results are confirmed empirically through simulations and case studies as
we compare regularized logistic, SVM, and Soft-SVM regressions and conclude
that the proposed model performs well in terms of both classification and
prediction errors.","['Man Huang', 'Luis Carvalho']","['stat.ML', 'cs.LG']",2022-05-24 03:01:35+00:00
http://arxiv.org/abs/2205.11716v2,Randomly Initialized One-Layer Neural Networks Make Data Linearly Separable,"Recently, neural networks have demonstrated remarkable capabilities in
mapping two arbitrary sets to two linearly separable sets. The prospect of
achieving this with randomly initialized neural networks is particularly
appealing due to the computational efficiency compared to fully trained
networks. This paper contributes by establishing that, given sufficient width,
a randomly initialized one-layer neural network can, with high probability,
transform two sets into two linearly separable sets without any training.
Moreover, we furnish precise bounds on the necessary width of the neural
network for this phenomenon to occur. Our initial bound exhibits exponential
dependence on the input dimension while maintaining polynomial dependence on
all other parameters. In contrast, our second bound is independent of input
dimension, effectively surmounting the curse of dimensionality. The main tools
used in our proof heavily relies on a fusion of geometric principles and
concentration of random matrices.","['Promit Ghosal', 'Srinath Mahankali', 'Yihang Sun']","['cs.LG', 'math.PR', 'stat.ML']",2022-05-24 01:38:43+00:00
http://arxiv.org/abs/2205.11677v4,Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold,"The stochastic block model is a canonical random graph model for clustering
and community detection on network-structured data. Decades of extensive study
on the problem have established many profound results, among which the phase
transition at the Kesten-Stigum threshold is particularly interesting both from
a mathematical and an applied standpoint. It states that no estimator based on
the network topology can perform substantially better than chance on sparse
graphs if the model parameter is below a certain threshold. Nevertheless, if we
slightly extend the horizon to the ubiquitous semi-supervised setting, such a
fundamental limitation will disappear completely. We prove that with an
arbitrary fraction of the labels revealed, the detection problem is feasible
throughout the parameter domain. Moreover, we introduce two efficient
algorithms, one combinatorial and one based on optimization, to integrate label
information with graph structures. Our work brings a new perspective to the
stochastic model of networks and semidefinite program research.","['Junda Sheng', 'Thomas Strohmer']","['stat.ML', 'cs.LG', 'math.OC', 'math.PR', '60-08 (Primary) 90C35 (Secondary) 90C22', 'G.3; I.2.6']",2022-05-24 00:03:25+00:00
http://arxiv.org/abs/2205.11672v2,Why does Throwing Away Data Improve Worst-Group Error?,"When facing data with imbalanced classes or groups, practitioners follow an
intriguing strategy to achieve best results. They throw away examples until the
classes or groups are balanced in size, and then perform empirical risk
minimization on the reduced training set. This opposes common wisdom in
learning theory, where the expected error is supposed to decrease as the
dataset grows in size. In this work, we leverage extreme value theory to
address this apparent contradiction. Our results show that the tails of the
data distribution play an important role in determining the
worst-group-accuracy of linear classifiers. When learning on data with heavy
tails, throwing away data restores the geometric symmetry of the resulting
classifier, and therefore improves its worst-group generalization.","['Kamalika Chaudhuri', 'Kartik Ahuja', 'Martin Arjovsky', 'David Lopez-Paz']","['stat.ML', 'cs.LG']",2022-05-23 23:43:18+00:00
http://arxiv.org/abs/2205.11640v2,Generalization Gap in Amortized Inference,"The ability of likelihood-based probabilistic models to generalize to unseen
data is central to many machine learning applications such as lossless
compression. In this work, we study the generalization of a popular class of
probabilistic model - the Variational Auto-Encoder (VAE). We discuss the two
generalization gaps that affect VAEs and show that overfitting is usually
dominated by amortized inference. Based on this observation, we propose a new
training objective that improves the generalization of amortized inference. We
demonstrate how our method can improve performance in the context of image
modeling and lossless compression.","['Mingtian Zhang', 'Peter Hayes', 'David Barber']","['stat.ML', 'cs.LG']",2022-05-23 21:28:47+00:00
http://arxiv.org/abs/2205.11627v1,Identifying Patient-Specific Root Causes of Disease,"Complex diseases are caused by a multitude of factors that may differ between
patients. As a result, hypothesis tests comparing all patients to all healthy
controls can detect many significant variables with inconsequential effect
sizes. A few highly predictive root causes may nevertheless generate disease
within each patient. In this paper, we define patient-specific root causes as
variables subject to exogenous ""shocks"" which go on to perturb an otherwise
healthy system and induce disease. In other words, the variables are associated
with the exogenous errors of a structural equation model (SEM), and these
errors predict a downstream diagnostic label. We quantify predictivity using
sample-specific Shapley values. This derivation allows us to develop a fast
algorithm called Root Causal Inference for identifying patient-specific root
causes by extracting the error terms of a linear SEM and then computing the
Shapley value associated with each error. Experiments highlight considerable
improvements in accuracy because the method uncovers root causes that may have
large effect sizes at the individual level but clinically insignificant effect
sizes at the group level. An R implementation is available at
github.com/ericstrobl/RCI.","['Eric V. Strobl', 'Thomas A. Lasko']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.ME']",2022-05-23 20:54:24+00:00
http://arxiv.org/abs/2205.11610v2,uGLAD: Sparse graph recovery by optimizing deep unrolled networks,"Probabilistic Graphical Models (PGMs) are generative models of complex
systems. They rely on conditional independence assumptions between variables to
learn sparse representations which can be visualized in a form of a graph. Such
models are used for domain exploration and structure discovery in poorly
understood domains. This work introduces a novel technique to perform sparse
graph recovery by optimizing deep unrolled networks. Assuming that the input
data $X\in\mathbb{R}^{M\times D}$ comes from an underlying multivariate
Gaussian distribution, we apply a deep model on $X$ that outputs the precision
matrix $\hat{\Theta}$, which can also be interpreted as the adjacency matrix.
Our model, uGLAD, builds upon and extends the state-of-the-art model GLAD to
the unsupervised setting. The key benefits of our model are (1) uGLAD
automatically optimizes sparsity-related regularization parameters leading to
better performance than existing algorithms. (2) We introduce multi-task
learning based `consensus' strategy for robust handling of missing data in an
unsupervised setting. We evaluate model results on synthetic Gaussian data,
non-Gaussian data generated from Gene Regulatory Networks, and present a case
study in anaerobic digestion.","['Harsh Shrivastava', 'Urszula Chajewska', 'Robin Abraham', 'Xinshi Chen']","['cs.LG', 'stat.ML']",2022-05-23 20:20:27+00:00
http://arxiv.org/abs/2205.11568v3,Quasi Black-Box Variational Inference with Natural Gradients for Bayesian Learning,"We develop an optimization algorithm suitable for Bayesian learning in
complex models. Our approach relies on natural gradient updates within a
general black-box framework for efficient training with limited model-specific
derivations. It applies within the class of exponential-family variational
posterior distributions, for which we extensively discuss the Gaussian case for
which the updates have a rather simple form. Our Quasi Black-box Variational
Inference (QBVI) framework is readily applicable to a wide class of Bayesian
inference problems and is of simple implementation as the updates of the
variational posterior do not involve gradients with respect to the model
parameters, nor the prescription of the Fisher information matrix. We develop
QBVI under different hypotheses for the posterior covariance matrix, discuss
details about its robust and feasible implementation, and provide a number of
real-world applications to demonstrate its effectiveness.","['Martin Magris', 'Mostafa Shabani', 'Alexandros Iosifidis']","['stat.ML', 'cs.LG', 'econ.EM']",2022-05-23 18:54:27+00:00
http://arxiv.org/abs/2205.11508v3,Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods,"Self-Supervised Learning (SSL) surmises that inputs and pairwise positive
relationships are enough to learn meaningful representations. Although SSL has
recently reached a milestone: outperforming supervised methods in many
modalities\dots the theoretical foundations are limited, method-specific, and
fail to provide principled design guidelines to practitioners. In this paper,
we propose a unifying framework under the helm of spectral manifold learning to
address those limitations. Through the course of this study, we will rigorously
demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous
spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al.
  This unification will then allow us to obtain (i) the closed-form optimal
representation for each method, (ii) the closed-form optimal network parameters
in the linear regime for each method, (iii) the impact of the pairwise
relations used during training on each of those quantities and on downstream
task performances, and most importantly, (iv) the first theoretical bridge
between contrastive and non-contrastive methods towards global and local
spectral embedding methods respectively, hinting at the benefits and
limitations of each. For example, (i) if the pairwise relation is aligned with
the downstream task, any SSL method can be employed successfully and will
recover the supervised method, but in the low data regime, VICReg's invariance
hyper-parameter should be high; (ii) if the pairwise relation is misaligned
with the downstream task, VICReg with small invariance hyper-parameter should
be preferred over SimCLR or BarlowTwins.","['Randall Balestriero', 'Yann LeCun']","['cs.LG', 'cs.AI', 'cs.CV', 'math.SP', 'stat.ML']",2022-05-23 17:59:32+00:00
http://arxiv.org/abs/2205.11507v1,Computationally Efficient Horizon-Free Reinforcement Learning for Linear Mixture MDPs,"Recent studies have shown that episodic reinforcement learning (RL) is not
more difficult than contextual bandits, even with a long planning horizon and
unknown state transitions. However, these results are limited to either tabular
Markov decision processes (MDPs) or computationally inefficient algorithms for
linear mixture MDPs. In this paper, we propose the first computationally
efficient horizon-free algorithm for linear mixture MDPs, which achieves the
optimal $\tilde O(d\sqrt{K} +d^2)$ regret up to logarithmic factors. Our
algorithm adapts a weighted least square estimator for the unknown transitional
dynamic, where the weight is both \emph{variance-aware} and
\emph{uncertainty-aware}. When applying our weighted least square estimator to
heterogeneous linear bandits, we can obtain an $\tilde O(d\sqrt{\sum_{k=1}^K
\sigma_k^2} +d)$ regret in the first $K$ rounds, where $d$ is the dimension of
the context and $\sigma_k^2$ is the variance of the reward in the $k$-th round.
This also improves upon the best-known algorithms in this setting when
$\sigma_k^2$'s are known.","['Dongruo Zhou', 'Quanquan Gu']","['cs.LG', 'math.OC', 'stat.ML']",2022-05-23 17:59:18+00:00
http://arxiv.org/abs/2205.11486v2,Robust and Agnostic Learning of Conditional Distributional Treatment Effects,"The conditional average treatment effect (CATE) is the best measure of
individual causal effects given baseline covariates. However, the CATE only
captures the (conditional) average, and can overlook risks and tail events,
which are important to treatment choice. In aggregate analyses, this is usually
addressed by measuring the distributional treatment effect (DTE), such as
differences in quantiles or tail expectations between treatment groups.
Hypothetically, one can similarly fit conditional quantile regressions in each
treatment group and take their difference, but this would not be robust to
misspecification or provide agnostic best-in-class predictions. We provide a
new robust and model-agnostic methodology for learning the conditional DTE
(CDTE) for a class of problems that includes conditional quantile treatment
effects, conditional super-quantile treatment effects, and conditional
treatment effects on coherent risk measures given by $f$-divergences. Our
method is based on constructing a special pseudo-outcome and regressing it on
covariates using any regression learner. Our method is model-agnostic in that
it can provide the best projection of CDTE onto the regression model class. Our
method is robust in that even if we learn these nuisances nonparametrically at
very slow rates, we can still learn CDTEs at rates that depend on the class
complexity and even conduct inferences on linear projections of CDTEs. We
investigate the behavior of our proposal in simulations, as well as in a case
study of 401(k) eligibility effects on wealth.","['Nathan Kallus', 'Miruna Oprescu']","['stat.ML', 'cs.LG', 'econ.EM', 'stat.ME']",2022-05-23 17:40:31+00:00
http://arxiv.org/abs/2205.11474v2,"Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images","Due to the intractability of characterizing everything that looks unlike the
normal data, anomaly detection (AD) is traditionally treated as an unsupervised
problem utilizing only normal samples. However, it has recently been found that
unsupervised image AD can be drastically improved through the utilization of
huge corpora of random images to represent anomalousness; a technique which is
known as Outlier Exposure. In this paper we show that specialized AD learning
methods seem unnecessary for state-of-the-art performance, and furthermore one
can achieve strong performance with just a small collection of Outlier Exposure
data, contradicting common assumptions in the field of AD. We find that
standard classifiers and semi-supervised one-class methods trained to discern
between normal samples and relatively few random natural images are able to
outperform the current state of the art on an established AD benchmark with
ImageNet. Further experiments reveal that even one well-chosen outlier sample
is sufficient to achieve decent performance on this benchmark (79.3% AUC). We
investigate this phenomenon and find that one-class methods are more robust to
the choice of training outliers, indicating that there are scenarios where
these are still more useful than standard classifiers. Additionally, we include
experiments that delineate the scenarios where our results hold. Lastly, no
training samples are necessary when one uses the representations learned by
CLIP, a recent foundation model, which achieves state-of-the-art AD results on
CIFAR-10 and ImageNet in a zero-shot setting.","['Philipp Liznerski', 'Lukas Ruff', 'Robert A. Vandermeulen', 'Billy Joe Franks', 'Klaus-Robert Müller', 'Marius Kloft']","['cs.CV', 'cs.LG', 'stat.ML']",2022-05-23 17:23:15+00:00
http://arxiv.org/abs/2205.11473v1,Rethinking Streaming Machine Learning Evaluation,"While most work on evaluating machine learning (ML) models focuses on
computing accuracy on batches of data, tracking accuracy alone in a streaming
setting (i.e., unbounded, timestamp-ordered datasets) fails to appropriately
identify when models are performing unexpectedly. In this position paper, we
discuss how the nature of streaming ML problems introduces new real-world
challenges (e.g., delayed arrival of labels) and recommend additional metrics
to assess streaming ML performance.","['Shreya Shankar', 'Bernease Herman', 'Aditya G. Parameswaran']","['cs.LG', 'cs.AI', 'stat.ML']",2022-05-23 17:21:43+00:00
http://arxiv.org/abs/2205.11439v1,Probabilistic forecasting of German electricity imbalance prices,"The exponential growth of renewable energy capacity has brought much
uncertainty to electricity prices and to electricity generation. To address
this challenge, the energy exchanges have been developing further trading
possibilities, especially the intraday and balancing markets. For an energy
trader participating in both markets, the forecasting of imbalance prices is of
particular interest. Therefore, in this manuscript we conduct a very short-term
probabilistic forecasting of imbalance prices, contributing to the scarce
literature in this novel subject. The forecasting is performed 30 minutes
before the delivery, so that the trader might still choose the trading place.
The distribution of the imbalance prices is modelled and forecasted using
methods well-known in the electricity price forecasting literature: lasso with
bootstrap, gamlss, and probabilistic neural networks. The methods are compared
with a naive benchmark in a meaningful rolling window study. The results
provide evidence of the efficiency between the intraday and balancing markets
as the sophisticated methods do not substantially overperform the intraday
continuous price index. On the other hand, they significantly improve the
empirical coverage. The analysis was conducted on the German market, however it
could be easily applied to any other market of similar structure.",['Michał Narajewski'],"['q-fin.ST', 'econ.EM', 'stat.ML']",2022-05-23 16:32:20+00:00
http://arxiv.org/abs/2205.11404v1,Variable-Input Deep Operator Networks,"Existing architectures for operator learning require that the number and
locations of sensors (where the input functions are evaluated) remain the same
across all training and test samples, significantly restricting the range of
their applicability. We address this issue by proposing a novel operator
learning framework, termed Variable-Input Deep Operator Network (VIDON), which
allows for random sensors whose number and locations can vary across samples.
VIDON is invariant to permutations of sensor locations and is proved to be
universal in approximating a class of continuous operators. We also prove that
VIDON can efficiently approximate operators arising in PDEs. Numerical
experiments with a diverse set of PDEs are presented to illustrate the robust
performance of VIDON in learning operators.","['Michael Prasthofer', 'Tim De Ryck', 'Siddhartha Mishra']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2022-05-23 15:47:47+00:00
http://arxiv.org/abs/2205.11361v2,Chaotic Regularization and Heavy-Tailed Limits for Deterministic Gradient Descent,"Recent studies have shown that gradient descent (GD) can achieve improved
generalization when its dynamics exhibits a chaotic behavior. However, to
obtain the desired effect, the step-size should be chosen sufficiently large, a
task which is problem dependent and can be difficult in practice. In this
study, we incorporate a chaotic component to GD in a controlled manner, and
introduce multiscale perturbed GD (MPGD), a novel optimization framework where
the GD recursion is augmented with chaotic perturbations that evolve via an
independent dynamical system. We analyze MPGD from three different angles: (i)
By building up on recent advances in rough paths theory, we show that, under
appropriate assumptions, as the step-size decreases, the MPGD recursion
converges weakly to a stochastic differential equation (SDE) driven by a
heavy-tailed L\'evy-stable process. (ii) By making connections to recently
developed generalization bounds for heavy-tailed processes, we derive a
generalization bound for the limiting SDE and relate the worst-case
generalization error over the trajectories of the process to the parameters of
MPGD. (iii) We analyze the implicit regularization effect brought by the
dynamical regularization and show that, in the weak perturbation regime, MPGD
introduces terms that penalize the Hessian of the loss function. Empirical
results are provided to demonstrate the advantages of MPGD.","['Soon Hoe Lim', 'Yijun Wan', 'Umut Şimşekli']","['stat.ML', 'cs.LG', 'math.DS', 'math.PR']",2022-05-23 14:47:55+00:00
http://arxiv.org/abs/2205.11359v2,Towards Size-Independent Generalization Bounds for Deep Operator Nets,"In recent times machine learning methods have made significant advances in
becoming a useful tool for analyzing physical systems. A particularly active
area in this theme has been ""physics-informed machine learning"" which focuses
on using neural nets for numerically solving differential equations. In this
work, we aim to advance the theory of measuring out-of-sample error while
training DeepONets -- which is among the most versatile ways to solve PDE
systems in one-shot.
  Firstly, for a class of DeepONets, we prove a bound on their Rademacher
complexity which does not explicitly scale with the width of the nets involved.
Secondly, we use this to show how the Huber loss can be chosen so that for
these DeepONet classes generalization error bounds can be obtained that have no
explicit dependence on the size of the nets. We note that our theoretical
results apply to any PDE being targeted to be solved by DeepONets.","['Pulkit Gopalani', 'Sayar Karmakar', 'Dibyakanti Kumar', 'Anirbit Mukherjee']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2022-05-23 14:45:34+00:00
http://arxiv.org/abs/2205.11275v2,RL with KL penalties is better viewed as Bayesian inference,"Reinforcement learning (RL) is frequently employed in fine-tuning large
language models (LMs), such as GPT-3, to penalize them for undesirable features
of generated sequences, such as offensiveness, social bias, harmfulness or
falsehood. The RL formulation involves treating the LM as a policy and updating
it to maximise the expected value of a reward function which captures human
preferences, such as non-offensiveness. In this paper, we analyze challenges
associated with treating a language model as an RL policy and show how avoiding
those challenges requires moving beyond the RL paradigm. We start by observing
that the standard RL approach is flawed as an objective for fine-tuning LMs
because it leads to distribution collapse: turning the LM into a degenerate
distribution. Then, we analyze KL-regularised RL, a widely used recipe for
fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close
to its original distribution in terms of Kullback-Leibler (KL) divergence. We
show that KL-regularised RL is equivalent to variational inference:
approximating a Bayesian posterior which specifies how to update a prior LM to
conform with evidence provided by the reward function. We argue that this
Bayesian inference view of KL-regularised RL is more insightful than the
typically employed RL perspective. The Bayesian inference view explains how
KL-regularised RL avoids the distribution collapse problem and offers a
first-principles derivation for its objective. While this objective happens to
be equivalent to RL (with a particular choice of parametric reward), there
exist other objectives for fine-tuning LMs which are no longer equivalent to
RL. That observation leads to a more general point: RL is not an adequate
formal framework for problems such as fine-tuning language models. These
problems are best viewed as Bayesian inference: approximating a pre-defined
target distribution.","['Tomasz Korbak', 'Ethan Perez', 'Christopher L Buckley']","['cs.LG', 'stat.ML']",2022-05-23 12:47:13+00:00
http://arxiv.org/abs/2205.11168v4,Logarithmic regret bounds for continuous-time average-reward Markov decision processes,"We consider reinforcement learning for continuous-time Markov decision
processes (MDPs) in the infinite-horizon, average-reward setting. In contrast
to discrete-time MDPs, a continuous-time process moves to a state and stays
there for a random holding time after an action is taken. With unknown
transition probabilities and rates of exponential holding times, we derive
instance-dependent regret lower bounds that are logarithmic in the time
horizon. Moreover, we design a learning algorithm and establish a finite-time
regret bound that achieves the logarithmic growth rate. Our analysis builds
upon upper confidence reinforcement learning, a delicate estimation of the mean
holding times, and stochastic comparison of point processes.","['Xuefeng Gao', 'Xun Yu Zhou']","['cs.LG', 'math.OC', 'stat.ML']",2022-05-23 10:15:00+00:00
http://arxiv.org/abs/2205.11153v1,Decoupling multivariate functions using a nonparametric filtered tensor decomposition,"Multivariate functions emerge naturally in a wide variety of data-driven
models. Popular choices are expressions in the form of basis expansions or
neural networks. While highly effective, the resulting functions tend to be
hard to interpret, in part because of the large number of required parameters.
Decoupling techniques aim at providing an alternative representation of the
nonlinearity. The so-called decoupled form is often a more efficient
parameterisation of the relationship while being highly structured, favouring
interpretability. In this work two new algorithms, based on filtered tensor
decompositions of first order derivative information are introduced. The method
returns nonparametric estimates of smooth decoupled functions. Direct
applications are found in, i.a. the fields of nonlinear system identification
and machine learning.","['Jan Decuyper', 'Koen Tiels', 'Siep Weiland', 'Mark C. Runacres', 'Johan Schoukens']","['stat.ML', 'cs.SY', 'eess.SY']",2022-05-23 09:34:17+00:00
http://arxiv.org/abs/2205.11151v1,Split personalities in Bayesian Neural Networks: the case for full marginalisation,"The true posterior distribution of a Bayesian neural network is massively
multimodal. Whilst most of these modes are functionally equivalent, we
demonstrate that there remains a level of real multimodality that manifests in
even the simplest neural network setups. It is only by fully marginalising over
all posterior modes, using appropriate Bayesian sampling tools, that we can
capture the split personalities of the network. The ability of a network
trained in this manner to reason between multiple candidate solutions
dramatically improves the generalisability of the model, a feature we contend
is not consistently captured by alternative approaches to the training of
Bayesian neural networks. We provide a concise minimal example of this, which
can provide lessons and a future path forward for correctly utilising the
explainability and interpretability of Bayesian neural networks.","['David Yallup', 'Will Handley', 'Mike Hobson', 'Anthony Lasenby', 'Pablo Lemos']","['stat.ML', 'cs.LG']",2022-05-23 09:24:37+00:00
http://arxiv.org/abs/2205.11106v1,An improved neural network model for treatment effect estimation,"Nowadays, in many scientific and industrial fields there is an increasing
need for estimating treatment effects and answering causal questions. The key
for addressing these problems is the wealth of observational data and the
processes for leveraging this data. In this work, we propose a new model for
predicting the potential outcomes and the propensity score, which is based on a
neural network architecture. The proposed model exploits the covariates as well
as the outcomes of neighboring instances in training data. Numerical
experiments illustrate that the proposed model reports better treatment effect
estimation performance compared to state-of-the-art models.","['Niki Kiriakidou', 'Christos Diou']","['stat.ML', 'cs.LG']",2022-05-23 07:56:06+00:00
http://arxiv.org/abs/2205.11078v1,Beyond EM Algorithm on Over-specified Two-Component Location-Scale Gaussian Mixtures,"The Expectation-Maximization (EM) algorithm has been predominantly used to
approximate the maximum likelihood estimation of the location-scale Gaussian
mixtures. However, when the models are over-specified, namely, the chosen
number of components to fit the data is larger than the unknown true number of
components, EM needs a polynomial number of iterations in terms of the sample
size to reach the final statistical radius; this is computationally expensive
in practice. The slow convergence of EM is due to the missing of the locally
strong convexity with respect to the location parameter on the negative
population log-likelihood function, i.e., the limit of the negative sample
log-likelihood function when the sample size goes to infinity. To efficiently
explore the curvature of the negative log-likelihood functions, by specifically
considering two-component location-scale Gaussian mixtures, we develop the
Exponential Location Update (ELU) algorithm. The idea of the ELU algorithm is
that we first obtain the exact optimal solution for the scale parameter and
then perform an exponential step-size gradient descent for the location
parameter. We demonstrate theoretically and empirically that the ELU iterates
converge to the final statistical radius of the models after a logarithmic
number of iterations. To the best of our knowledge, it resolves the
long-standing open question in the literature about developing an optimization
algorithm that has optimal statistical and computational complexities for
solving parameter estimation even under some specific settings of the
over-specified Gaussian mixture models.","['Tongzheng Ren', 'Fuheng Cui', 'Sujay Sanghavi', 'Nhat Ho']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-05-23 06:49:55+00:00
http://arxiv.org/abs/2205.11061v1,Vegetation Mapping by UAV Visible Imagery and Machine Learning,"An experimental field cropped with sugar-beet with a wide spreading of weeds
has been used to test vegetation identification from drone visible imagery.
Expert masked and hue-filtered pictures have been used to train several Machine
Learning algorithms to develop a semi-automatic methodology for identification
and mapping species at high resolution. Results show that 5m altitude allows
for obtaining maps with an identification efficiency of more than 90%. Such a
method can be easily integrated to present VRHA, as much as tools to obtain
detailed maps of vegetation.",['Giuliano Vitali'],"['cs.CV', 'stat.ML', 'I.2; I.4; J.3']",2022-05-23 05:59:25+00:00
http://arxiv.org/abs/2205.11051v1,Flow-based Recurrent Belief State Learning for POMDPs,"Partially Observable Markov Decision Process (POMDP) provides a principled
and generic framework to model real world sequential decision making processes
but yet remains unsolved, especially for high dimensional continuous space and
unknown models. The main challenge lies in how to accurately obtain the belief
state, which is the probability distribution over the unobservable environment
states given historical information. Accurately calculating this belief state
is a precondition for obtaining an optimal policy of POMDPs. Recent advances in
deep learning techniques show great potential to learn good belief states.
However, existing methods can only learn approximated distribution with limited
flexibility. In this paper, we introduce the \textbf{F}l\textbf{O}w-based
\textbf{R}ecurrent \textbf{BE}lief \textbf{S}tate model (FORBES), which
incorporates normalizing flows into the variational inference to learn general
continuous belief states for POMDPs. Furthermore, we show that the learned
belief states can be plugged into downstream RL algorithms to improve
performance. In experiments, we show that our methods successfully capture the
complex belief states that enable multi-modal predictions as well as high
quality reconstructions, and results on challenging visual-motor control tasks
show that our method achieves superior performance and sample efficiency.","['Xiaoyu Chen', 'Yao Mu', 'Ping Luo', 'Shengbo Li', 'Jianyu Chen']","['cs.LG', 'stat.ML']",2022-05-23 05:29:55+00:00
http://arxiv.org/abs/2205.11025v2,Flexible and Hierarchical Prior for Bayesian Nonnegative Matrix Factorization,"In this paper, we introduce a probabilistic model for learning nonnegative
matrix factorization (NMF) that is commonly used for predicting missing values
and finding hidden patterns in the data, in which the matrix factors are latent
variables associated with each data dimension. The nonnegativity constraint for
the latent factors is handled by choosing priors with support on the
nonnegative subspace. Bayesian inference procedure based on Gibbs sampling is
employed. We evaluate the model on several real-world datasets including
MovieLens 100K and MovieLens 1M with different sizes and dimensions and show
that the proposed Bayesian NMF GRRN model leads to better predictions and
avoids overfitting compared to existing Bayesian NMF approaches.","['Jun Lu', 'Xuanyu Ye']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2022-05-23 03:51:55+00:00
http://arxiv.org/abs/2205.11019v1,Efficient Reinforcement Learning from Demonstration Using Local Ensemble and Reparameterization with Split and Merge of Expert Policies,"The current work on reinforcement learning (RL) from demonstrations often
assumes the demonstrations are samples from an optimal policy, an unrealistic
assumption in practice. When demonstrations are generated by sub-optimal
policies or have sparse state-action pairs, policy learned from sub-optimal
demonstrations may mislead an agent with incorrect or non-local action
decisions. We propose a new method called Local Ensemble and Reparameterization
with Split and Merge of expert policies (LEARN-SAM) to improve efficiency and
make better use of the sub-optimal demonstrations. First, LEARN-SAM employs a
new concept, the lambda-function, based on a discrepancy measure between the
current state to demonstrated states to ""localize"" the weights of the expert
policies during learning. Second, LEARN-SAM employs a split-and-merge (SAM)
mechanism by separating the helpful parts in each expert demonstration and
regrouping them into new expert policies to use the demonstrations selectively.
Both the lambda-function and SAM mechanism help boost the learning speed.
Theoretically, we prove the invariant property of reparameterized policy before
and after the SAM mechanism, providing theoretical guarantees for the
convergence of the employed policy gradient method. We demonstrate the
superiority of the LEARN-SAM method and its robustness with varying
demonstration quality and sparsity in six experiments on complex continuous
control problems of low to high dimensions, compared to existing methods on RL
from demonstration.","['Yu Wang', 'Fang Liu']","['cs.LG', 'stat.ML']",2022-05-23 03:36:24+00:00
http://arxiv.org/abs/2205.11006v1,Nonparametric learning of kernels in nonlocal operators,"Nonlocal operators with integral kernels have become a popular tool for
designing solution maps between function spaces, due to their efficiency in
representing long-range dependence and the attractive feature of being
resolution-invariant. In this work, we provide a rigorous identifiability
analysis and convergence study for the learning of kernels in nonlocal
operators. It is found that the kernel learning is an ill-posed or even
ill-defined inverse problem, leading to divergent estimators in the presence of
modeling errors or measurement noises. To resolve this issue, we propose a
nonparametric regression algorithm with a novel data adaptive RKHS Tikhonov
regularization method based on the function space of identifiability. The
method yields a noisy-robust convergent estimator of the kernel as the data
resolution refines, on both synthetic and real-world datasets. In particular,
the method successfully learns a homogenized model for the stress wave
propagation in a heterogeneous solid, revealing the unknown governing laws from
real-world data at microscale. Our regularization method outperforms baseline
methods in robustness, generalizability and accuracy.","['Fei Lu', 'Qingci An', 'Yue Yu']","['stat.ML', 'cs.LG', '62G35']",2022-05-23 02:47:55+00:00
http://arxiv.org/abs/2205.10947v2,Deep Direct Discriminative Decoders for High-dimensional Time-series Data Analysis,"The state-space models (SSMs) are widely utilized in the analysis of
time-series data. SSMs rely on an explicit definition of the state and
observation processes. Characterizing these processes is not always easy and
becomes a modeling challenge when the dimension of observed data grows or the
observed data distribution deviates from the normal distribution. Here, we
propose a new formulation of SSM for high-dimensional observation processes. We
call this solution the deep direct discriminative decoder (D4). The D4 brings
deep neural networks' expressiveness and scalability to the SSM formulation
letting us build a novel solution that efficiently estimates the underlying
state processes through high-dimensional observation signal. We demonstrate the
D4 solutions in simulated and real data such as Lorenz attractors, Langevin
dynamics, random walk dynamics, and rat hippocampus spiking neural data and
show that the D4 performs better than traditional SSMs and RNNs. The D4 can be
applied to a broader class of time-series data where the connection between
high-dimensional observation and the underlying latent process is hard to
characterize.","['Mohammad R. Rezaei', 'Milos R. Popovic', 'Milad Lankarany', 'Ali Yousefi']","['cs.LG', 'stat.ML']",2022-05-22 22:44:41+00:00
http://arxiv.org/abs/2205.10936v2,On Elimination Strategies for Bandit Fixed-Confidence Identification,"Elimination algorithms for bandit identification, which prune the plausible
correct answers sequentially until only one remains, are computationally
convenient since they reduce the problem size over time. However, existing
elimination strategies are often not fully adaptive (they update their sampling
rule infrequently) and are not easy to extend to combinatorial settings, where
the set of answers is exponentially large in the problem dimension. On the
other hand, most existing fully-adaptive strategies to tackle general
identification problems are computationally demanding since they repeatedly
test the correctness of every answer, without ever reducing the problem size.
We show that adaptive methods can be modified to use elimination in both their
stopping and sampling rules, hence obtaining the best of these two worlds: the
algorithms (1) remain fully adaptive, (2) suffer a sample complexity that is
never worse of their non-elimination counterpart, and (3) provably eliminate
certain wrong answers early. We confirm these benefits experimentally, where
elimination improves significantly the computational complexity of adaptive
methods on common tasks like best-arm identification in linear bandits.","['Andrea Tirinzoni', 'Rémy Degenne']","['cs.LG', 'stat.ML']",2022-05-22 21:41:57+00:00
http://arxiv.org/abs/2205.10927v2,Fast ABC-Boost: A Unified Framework for Selecting the Base Class in Multi-Class Classification,"The work in ICML'09 showed that the derivatives of the classical multi-class
logistic regression loss function could be re-written in terms of a pre-chosen
""base class"" and applied the new derivatives in the popular boosting framework.
In order to make use of the new derivatives, one must have a strategy to
identify/choose the base class at each boosting iteration. The idea of
""adaptive base class boost"" (ABC-Boost) in ICML'09, adopted a computationally
expensive ""exhaustive search"" strategy for the base class at each iteration. It
has been well demonstrated that ABC-Boost, when integrated with trees, can
achieve substantial improvements in many multi-class classification tasks.
Furthermore, the work in UAI'10 derived the explicit second-order tree split
gain formula which typically improved the classification accuracy considerably,
compared with using only the fist-order information for tree-splitting, for
both multi-class and binary-class classification tasks. In this paper, we
develop a unified framework for effectively selecting the base class by
introducing a series of ideas to improve the computational efficiency of
ABC-Boost. Our framework has parameters $(s,g,w)$. At each boosting iteration,
we only search for the ""$s$-worst classes"" (instead of all classes) to
determine the base class. We also allow a ""gap"" $g$ when conducting the search.
That is, we only search for the base class at every $g+1$ iterations. We
furthermore allow a ""warm up"" stage by only starting the search after $w$
boosting iterations. The parameters $s$, $g$, $w$, can be viewed as tunable
parameters and certain combinations of $(s,g,w)$ may even lead to better test
accuracy than the ""exhaustive search"" strategy. Overall, our proposed framework
provides a robust and reliable scheme for implementing ABC-Boost in practice.","['Ping Li', 'Weijie Zhao']","['cs.LG', 'stat.ML']",2022-05-22 20:42:26+00:00
http://arxiv.org/abs/2205.10914v3,Weisfeiler and Leman Go Walking: Random Walk Kernels Revisited,"Random walk kernels have been introduced in seminal work on graph learning
and were later largely superseded by kernels based on the Weisfeiler-Leman test
for graph isomorphism. We give a unified view on both classes of graph kernels.
We study walk-based node refinement methods and formally relate them to several
widely-used techniques, including Morgan's algorithm for molecule canonization
and the Weisfeiler-Leman test. We define corresponding walk-based kernels on
nodes that allow fine-grained parameterized neighborhood comparison, reach
Weisfeiler-Leman expressiveness, and are computed using the kernel trick. From
this we show that classical random walk kernels with only minor modifications
regarding definition and computation are as expressive as the widely-used
Weisfeiler-Leman subtree kernel but support non-strict neighborhood comparison.
We verify experimentally that walk-based kernels reach or even surpass the
accuracy of Weisfeiler-Leman kernels in real-world classification tasks.",['Nils M. Kriege'],"['cs.LG', 'cs.DS', 'stat.ML']",2022-05-22 19:41:25+00:00
http://arxiv.org/abs/2205.10907v1,Improved Modeling of Persistence Diagram,"High-dimensional reduction methods are powerful tools for describing the main
patterns in big data. One of these methods is the topological data analysis
(TDA), which modeling the shape of the data in terms of topological properties.
This method specifically translates the original data into two-dimensional
system, which is graphically represented via the 'persistence diagram'. The
outliers points on this diagram present the data pattern, whereas the other
points behave as a random noise. In order to determine which points are
significant outliers, replications of the original data set are needed. Once
only one original data is available, replications can be created by fitting a
model for the points on the persistence diagram, and then using the MCMC
methods. One of such model is the RST (Replicating Statistical Topology). In
this paper we suggest a modification of the RST model. Using a simulation
study, we show that the modified RST improves the performance of the RST in
terms of goodness of fit. We use the MCMC Metropolis-Hastings algorithm for
sampling according to the fitted model.",['Sarit Agami'],"['stat.ME', 'cs.LG', 'stat.ML']",2022-05-22 19:11:59+00:00
http://arxiv.org/abs/2205.10895v2,Contextual Information-Directed Sampling,"Information-directed sampling (IDS) has recently demonstrated its potential
as a data-efficient reinforcement learning algorithm. However, it is still
unclear what is the right form of information ratio to optimize when contextual
information is available. We investigate the IDS design through two contextual
bandit problems: contextual bandits with graph feedback and sparse linear
contextual bandits. We provably demonstrate the advantage of contextual IDS
over conditional IDS and emphasize the importance of considering the context
distribution. The main message is that an intelligent agent should invest more
on the actions that are beneficial for the future unseen contexts while the
conditional IDS can be myopic. We further propose a computationally-efficient
version of contextual IDS based on Actor-Critic and evaluate it empirically on
a neural network contextual bandit.","['Botao Hao', 'Tor Lattimore', 'Chao Qin']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2022-05-22 18:08:42+00:00
http://arxiv.org/abs/2205.10879v1,Fast Gaussian Process Posterior Mean Prediction via Local Cross Validation and Precomputation,"Gaussian processes (GPs) are Bayesian non-parametric models useful in a
myriad of applications. Despite their popularity, the cost of GP predictions
(quadratic storage and cubic complexity with respect to the number of training
points) remains a hurdle in applying GPs to large data. We present a fast
posterior mean prediction algorithm called FastMuyGPs to address this
shortcoming. FastMuyGPs is based upon the MuyGPs hyperparameter estimation
algorithm and utilizes a combination of leave-one-out cross-validation,
batching, nearest neighbors sparsification, and precomputation to provide
scalable, fast GP prediction. We demonstrate several benchmarks wherein
FastMuyGPs prediction attains superior accuracy and competitive or superior
runtime to both deep neural networks and state-of-the-art scalable GP
algorithms.","['Alec M. Dunton', 'Benjamin W. Priest', 'Amanda Muyskens']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML', '60G15', 'G.3']",2022-05-22 17:38:36+00:00
http://arxiv.org/abs/2205.10872v1,Fusion Subspace Clustering for Incomplete Data,"This paper introduces {\em fusion subspace clustering}, a novel method to
learn low-dimensional structures that approximate large scale yet highly
incomplete data. The main idea is to assign each datum to a subspace of its
own, and minimize the distance between the subspaces of all data, so that
subspaces of the same cluster get {\em fused} together. Our method allows low,
high, and even full-rank data; it directly accounts for noise, and its sample
complexity approaches the information-theoretic limit. In addition, our
approach provides a natural model selection {\em clusterpath}, and a direct
completion method. We give convergence guarantees, analyze computational
complexity, and show through extensive experiments on real and synthetic data
that our approach performs comparably to the state-of-the-art with complete
data, and dramatically better if data is missing.","['Usman Mahmood', 'Daniel Pimentel-Alarcón']","['cs.LG', 'stat.ML']",2022-05-22 17:23:41+00:00
http://arxiv.org/abs/2205.10864v2,Federated Learning Aggregation: New Robust Algorithms with Guarantees,"Federated Learning has been recently proposed for distributed model training
at the edge. The principle of this approach is to aggregate models learned on
distributed clients to obtain a new more general ""average"" model (FedAvg). The
resulting model is then redistributed to clients for further training. To date,
the most popular federated learning algorithm uses coordinate-wise averaging of
the model parameters for aggregation. In this paper, we carry out a complete
general mathematical convergence analysis to evaluate aggregation strategies in
a federated learning framework. From this, we derive novel aggregation
algorithms which are able to modify their model architecture by differentiating
client contributions according to the value of their losses. Moreover, we go
beyond the assumptions introduced in theory, by evaluating the performance of
these strategies and by comparing them with the one of FedAvg in classification
tasks in both the IID and the Non-IID framework without additional hypothesis.","['Adnan Ben Mansour', 'Gaia Carenini', 'Alexandre Duplessis', 'David Naccache']","['stat.ML', 'cs.LG']",2022-05-22 16:37:53+00:00
http://arxiv.org/abs/2205.10842v2,Addressing Strategic Manipulation Disparities in Fair Classification,"In real-world classification settings, such as loan application evaluation or
content moderation on online platforms, individuals respond to classifier
predictions by strategically updating their features to increase their
likelihood of receiving a particular (positive) decision (at a certain cost).
Yet, when different demographic groups have different feature distributions or
pay different update costs, prior work has shown that individuals from minority
groups often pay a higher cost to update their features. Fair classification
aims to address such classifier performance disparities by constraining the
classifiers to satisfy statistical fairness properties. However, we show that
standard fairness constraints do not guarantee that the constrained classifier
reduces the disparity in strategic manipulation cost. To address such biases in
strategic settings and provide equal opportunities for strategic manipulation,
we propose a constrained optimization framework that constructs classifiers
that lower the strategic manipulation cost for minority groups. We develop our
framework by studying theoretical connections between group-specific strategic
cost disparity and standard selection rate fairness metrics (e.g., statistical
rate and true positive rate). Empirically, we show the efficacy of this
approach over multiple real-world datasets.","['Vijay Keswani', 'L. Elisa Celis']","['cs.CY', 'cs.LG', 'stat.ML']",2022-05-22 14:59:40+00:00
http://arxiv.org/abs/2205.10798v2,PAC-Wrap: Semi-Supervised PAC Anomaly Detection,"Anomaly detection is essential for preventing hazardous outcomes for
safety-critical applications like autonomous driving. Given their
safety-criticality, these applications benefit from provable bounds on various
errors in anomaly detection. To achieve this goal in the semi-supervised
setting, we propose to provide Probably Approximately Correct (PAC) guarantees
on the false negative and false positive detection rates for anomaly detection
algorithms. Our method (PAC-Wrap) can wrap around virtually any existing
semi-supervised and unsupervised anomaly detection method, endowing it with
rigorous guarantees. Our experiments with various anomaly detectors and
datasets indicate that PAC-Wrap is broadly effective.","['Shuo Li', 'Xiayan Ji', 'Edgar Dobriban', 'Oleg Sokolsky', 'Insup Lee']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2022-05-22 11:05:49+00:00
http://arxiv.org/abs/2205.10772v2,Fast Instrument Learning with Faster Rates,"We investigate nonlinear instrumental variable (IV) regression given
high-dimensional instruments. We propose a simple algorithm which combines
kernelized IV methods and an arbitrary, adaptive regression algorithm, accessed
as a black box. Our algorithm enjoys faster-rate convergence and adapts to the
dimensionality of informative latent features, while avoiding an expensive
minimax optimization procedure, which has been necessary to establish similar
guarantees. It further brings the benefit of flexible machine learning models
to quasi-Bayesian uncertainty quantification, likelihood-based model selection,
and model averaging. Simulation studies demonstrate the competitive performance
of our method.","['Ziyu Wang', 'Yuhao Zhou', 'Jun Zhu']","['stat.ML', 'cs.LG', 'econ.EM']",2022-05-22 08:06:54+00:00
http://arxiv.org/abs/2205.10736v1,Should Models Be Accurate?,"Model-based Reinforcement Learning (MBRL) holds promise for data-efficiency
by planning with model-generated experience in addition to learning with
experience from the environment. However, in complex or changing environments,
models in MBRL will inevitably be imperfect, and their detrimental effects on
learning can be difficult to mitigate. In this work, we question whether the
objective of these models should be the accurate simulation of environment
dynamics at all. We focus our investigations on Dyna-style planning in a
prediction setting. First, we highlight and support three motivating points: a
perfectly accurate model of environment dynamics is not practically achievable,
is not necessary, and is not always the most useful anyways. Second, we
introduce a meta-learning algorithm for training models with a focus on their
usefulness to the learner instead of their accuracy in modelling the
environment. Our experiments show that in a simple non-stationary environment,
our algorithm enables faster learning than even using an accurate model built
with domain-specific knowledge of the non-stationarity.","[""Esra'a Saleh"", 'John D. Martin', 'Anna Koop', 'Arash Pourzarabi', 'Michael Bowling']","['cs.LG', 'cs.AI', 'stat.ML']",2022-05-22 04:23:54+00:00
http://arxiv.org/abs/2205.10732v2,Robust Flow-based Conformal Inference (FCI) with Statistical Guarantee,"Conformal prediction aims to determine precise levels of confidence in
predictions for new objects using past experience. However, the commonly used
exchangeable assumptions between the training data and testing data limit its
usage in dealing with contaminated testing sets. In this paper, we develop a
novel flow-based conformal inference (FCI) method to build predictive sets and
infer outliers for complex and high-dimensional data. We leverage ideas from
adversarial flow to transfer the input data to a random vector with known
distributions. Our roundtrip transformation can map the input data to a
low-dimensional space, meanwhile reserving the conditional distribution of
input data given each class label, which enables us to construct a
non-conformity score for uncertainty quantification. Our approach is applicable
and robust when the testing data is contaminated. We evaluate our method,
robust flow-based conformal inference, on benchmark datasets. We find that it
produces effective predictive sets and accurate outlier detection and is more
powerful relative to competing approaches.","['Youhui Ye', 'Meimei Liu', 'Xin Xing']","['stat.ML', 'cs.LG']",2022-05-22 04:17:30+00:00
http://arxiv.org/abs/2205.10697v6,Lassoed Tree Boosting,"Gradient boosting performs exceptionally in most prediction problems and
scales well to large datasets. In this paper we prove that a ``lassoed''
gradient boosted tree algorithm with early stopping achieves faster than
$n^{-1/4}$ L2 convergence in the large nonparametric space of cadlag functions
of bounded sectional variation. This rate is remarkable because it does not
depend on the dimension, sparsity, or smoothness. We use simulation and real
data to confirm our theory and demonstrate empirical performance and
scalability on par with standard boosting. Our convergence proofs are based on
a novel, general theorem on early stopping with empirical loss minimizers of
nested Donsker classes.","['Alejandro Schuler', 'Yi Li', 'Mark van der Laan']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-05-22 00:34:41+00:00
http://arxiv.org/abs/2205.10672v1,Multi-task Learning for Gaussian Graphical Regressions with High Dimensional Covariates,"Gaussian graphical regression is a powerful means that regresses the
precision matrix of a Gaussian graphical model on covariates, permitting the
numbers of the response variables and covariates to far exceed the sample size.
Model fitting is typically carried out via separate node-wise lasso
regressions, ignoring the network-induced structure among these regressions.
Consequently, the error rate is high, especially when the number of nodes is
large. We propose a multi-task learning estimator for fitting Gaussian
graphical regression models; we design a cross-task group sparsity penalty and
a within task element-wise sparsity penalty, which govern the sparsity of
active covariates and their effects on the graph, respectively. For
computation, we consider an efficient augmented Lagrangian algorithm, which
solves subproblems with a semi-smooth Newton method. For theory, we show that
the error rate of the multi-task learning based estimates has much improvement
over that of the separate node-wise lasso estimates, because the cross-task
penalty borrows information across tasks. To address the main challenge that
the tasks are entangled in a complicated correlation structure, we establish a
new tail probability bound for correlated heavy-tailed (sub-exponential)
variables with an arbitrary correlation structure, a useful theoretical result
in its own right. Finally, the utility of our method is demonstrated through
simulations as well as an application to a gene co-expression network study
with brain cancer patients.","['Jingfei Zhang', 'Yi Li']","['stat.ME', 'stat.ML']",2022-05-21 20:48:51+00:00
http://arxiv.org/abs/2205.10671v2,Pessimism for Offline Linear Contextual Bandits using $\ell_p$ Confidence Sets,"We present a family $\{\hat{\pi}\}_{p\ge 1}$ of pessimistic learning rules
for offline learning of linear contextual bandits, relying on confidence sets
with respect to different $\ell_p$ norms, where $\hat{\pi}_2$ corresponds to
Bellman-consistent pessimism (BCP), while $\hat{\pi}_\infty$ is a novel
generalization of lower confidence bound (LCB) to the linear setting. We show
that the novel $\hat{\pi}_\infty$ learning rule is, in a sense, adaptively
optimal, as it achieves the minimax performance (up to log factors) against all
$\ell_q$-constrained problems, and as such it strictly dominates all other
predictors in the family, including $\hat{\pi}_2$.","['Gene Li', 'Cong Ma', 'Nathan Srebro']","['cs.LG', 'stat.ML']",2022-05-21 20:42:15+00:00
http://arxiv.org/abs/2205.10662v2,Equivariant Mesh Attention Networks,"Equivariance to symmetries has proven to be a powerful inductive bias in deep
learning research. Recent works on mesh processing have concentrated on various
kinds of natural symmetries, including translations, rotations, scaling, node
permutations, and gauge transformations. To date, no existing architecture is
equivariant to all of these transformations. In this paper, we present an
attention-based architecture for mesh data that is provably equivariant to all
transformations mentioned above. Our pipeline relies on the use of relative
tangential features: a simple, effective, equivariance-friendly alternative to
raw node positions as inputs. Experiments on the FAUST and TOSCA datasets
confirm that our proposed architecture achieves improved performance on these
benchmarks and is indeed equivariant, and therefore robust, to a wide variety
of local/global transformations.","['Sourya Basu', 'Jose Gallego-Posada', 'Francesco Viganò', 'James Rowbottom', 'Taco Cohen']","['cs.LG', 'cs.CV', 'stat.ML']",2022-05-21 19:53:14+00:00
http://arxiv.org/abs/2205.10541v1,Neuroevolutionary Feature Representations for Causal Inference,"Within the field of causal inference, we consider the problem of estimating
heterogeneous treatment effects from data. We propose and validate a novel
approach for learning feature representations to aid the estimation of the
conditional average treatment effect or CATE. Our method focuses on an
intermediate layer in a neural network trained to predict the outcome from the
features. In contrast to previous approaches that encourage the distribution of
representations to be treatment-invariant, we leverage a genetic algorithm that
optimizes over representations useful for predicting the outcome to select
those less useful for predicting the treatment. This allows us to retain
information within the features useful for predicting outcome even if that
information may be related to treatment assignment. We validate our method on
synthetic examples and illustrate its use on a real life dataset.","['Michael C. Burkhart', 'Gabriel Ruiz']","['stat.ML', 'cs.LG', '62D20, 68T30, 68W50 (Primary) 68T20, 68T07 (Secondary)']",2022-05-21 09:13:04+00:00
http://arxiv.org/abs/2205.10490v2,Aligning Logits Generatively for Principled Black-Box Knowledge Distillation,"Black-Box Knowledge Distillation (B2KD) is a formulated problem for
cloud-to-edge model compression with invisible data and models hosted on the
server. B2KD faces challenges such as limited Internet exchange and edge-cloud
disparity of data distributions. In this paper, we formalize a two-step
workflow consisting of deprivatization and distillation, and theoretically
provide a new optimization direction from logits to cell boundary different
from direct logits alignment. With its guidance, we propose a new method
Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a
lightweight one. Our method does not differentiate between treating soft or
hard responses, and consists of: 1) deprivatization: emulating the inverse
mapping of the teacher function with a generator, and 2) distillation: aligning
low-dimensional logits of the teacher and student models by reducing the
distance of high-dimensional image points. For different teacher-student pairs,
our method yields inspiring distillation performance on various benchmarks, and
outperforms the previous state-of-the-art approaches.","['Jing Ma', 'Xiang Xiang', 'Ke Wang', 'Yuchuan Wu', 'Yongbin Li']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2022-05-21 02:38:16+00:00
http://arxiv.org/abs/2205.10457v1,Robust Sensible Adversarial Learning of Deep Neural Networks for Image Classification,"The idea of robustness is central and critical to modern statistical
analysis. However, despite the recent advances of deep neural networks (DNNs),
many studies have shown that DNNs are vulnerable to adversarial attacks. Making
imperceptible changes to an image can cause DNN models to make the wrong
classification with high confidence, such as classifying a benign mole as a
malignant tumor and a stop sign as a speed limit sign. The trade-off between
robustness and standard accuracy is common for DNN models. In this paper, we
introduce sensible adversarial learning and demonstrate the synergistic effect
between pursuits of standard natural accuracy and robustness. Specifically, we
define a sensible adversary which is useful for learning a robust model while
keeping high natural accuracy. We theoretically establish that the Bayes
classifier is the most robust multi-class classifier with the 0-1 loss under
sensible adversarial learning. We propose a novel and efficient algorithm that
trains a robust model using implicit loss truncation. We apply sensible
adversarial learning for large-scale image classification to a handwritten
digital image dataset called MNIST and an object recognition colored image
dataset called CIFAR10. We have performed an extensive comparative study to
compare our method with other competitive methods. Our experiments empirically
demonstrate that our method is not sensitive to its hyperparameter and does not
collapse even with a small model capacity while promoting robustness against
various attacks and keeping high natural accuracy.","['Jungeum Kim', 'Xiao Wang']","['cs.CR', 'cs.CV', 'cs.LG', 'stat.ML']",2022-05-20 22:57:44+00:00
http://arxiv.org/abs/2205.12004v2,Quantum Kerr Learning,"Quantum machine learning is a rapidly evolving field of research that could
facilitate important applications for quantum computing and also significantly
impact data-driven sciences. In our work, based on various arguments from
complexity theory and physics, we demonstrate that a single Kerr mode can
provide some ""quantum enhancements"" when dealing with kernel-based methods.
Using kernel properties, neural tangent kernel theory, first-order perturbation
theory of the Kerr non-linearity, and non-perturbative numerical simulations,
we show that quantum enhancements could happen in terms of convergence time and
generalization error. Furthermore, we make explicit indications on how
higher-dimensional input data could be considered. Finally, we propose an
experimental protocol, that we call \emph{quantum Kerr learning}, based on
circuit QED.","['Junyu Liu', 'Changchun Zhong', 'Matthew Otten', 'Anirban Chandra', 'Cristian L. Cortes', 'Chaoyang Ti', 'Stephen K Gray', 'Xu Han']","['quant-ph', 'cs.AI', 'cs.LG', 'stat.ML']",2022-05-20 21:45:29+00:00
http://arxiv.org/abs/2205.10390v2,EGR: Equivariant Graph Refinement and Assessment of 3D Protein Complex Structures,"Protein complexes are macromolecules essential to the functioning and
well-being of all living organisms. As the structure of a protein complex, in
particular its region of interaction between multiple protein subunits (i.e.,
chains), has a notable influence on the biological function of the complex,
computational methods that can quickly and effectively be used to refine and
assess the quality of a protein complex's 3D structure can directly be used
within a drug discovery pipeline to accelerate the development of new
therapeutics and improve the efficacy of future vaccines. In this work, we
introduce the Equivariant Graph Refiner (EGR), a novel E(3)-equivariant graph
neural network (GNN) for multi-task structure refinement and assessment of
protein complexes. Our experiments on new, diverse protein complex datasets,
all of which we make publicly available in this work, demonstrate the
state-of-the-art effectiveness of EGR for atomistic refinement and assessment
of protein complexes and outline directions for future work in the field. In
doing so, we establish a baseline for future studies in macromolecular
refinement and structure analysis.","['Alex Morehead', 'Xiao Chen', 'Tianqi Wu', 'Jian Liu', 'Jianlin Cheng']","['cs.LG', 'cs.AI', 'q-bio.BM', 'q-bio.QM', 'stat.ML', 'I.2.1; J.3']",2022-05-20 18:11:41+00:00
http://arxiv.org/abs/2205.10327v2,What's the Harm? Sharp Bounds on the Fraction Negatively Affected by Treatment,"The fundamental problem of causal inference -- that we never observe
counterfactuals -- prevents us from identifying how many might be negatively
affected by a proposed intervention. If, in an A/B test, half of users click
(or buy, or watch, or renew, etc.), whether exposed to the standard experience
A or a new one B, hypothetically it could be because the change affects no one,
because the change positively affects half the user population to go from
no-click to click while negatively affecting the other half, or something in
between. While unknowable, this impact is clearly of material importance to the
decision to implement a change or not, whether due to fairness, long-term,
systemic, or operational considerations. We therefore derive the
tightest-possible (i.e., sharp) bounds on the fraction negatively affected (and
other related estimands) given data with only factual observations, whether
experimental or observational. Naturally, the more we can stratify individuals
by observable covariates, the tighter the sharp bounds. Since these bounds
involve unknown functions that must be learned from data, we develop a robust
inference algorithm that is efficient almost regardless of how and how fast
these functions are learned, remains consistent when some are mislearned, and
still gives valid conservative bounds when most are mislearned. Our methodology
altogether therefore strongly supports credible conclusions: it avoids
spuriously point-identifying this unknowable impact, focusing on the best
bounds instead, and it permits exceedingly robust inference on these. We
demonstrate our method in simulation studies and in a case study of career
counseling for the unemployed.",['Nathan Kallus'],"['stat.ME', 'cs.LG', 'econ.EM', 'stat.ML']",2022-05-20 17:36:33+00:00
http://arxiv.org/abs/2205.10268v1,B-cos Networks: Alignment is All We Need for Interpretability,"We present a new direction for increasing the interpretability of deep neural
networks (DNNs) by promoting weight-input alignment during training. For this,
we propose to replace the linear transforms in DNNs by our B-cos transform. As
we show, a sequence (network) of such transforms induces a single linear
transform that faithfully summarises the full model computations. Moreover, the
B-cos transform introduces alignment pressure on the weights during
optimisation. As a result, those induced linear transforms become highly
interpretable and align with task-relevant features. Importantly, the B-cos
transform is designed to be compatible with existing architectures and we show
that it can easily be integrated into common models such as VGGs, ResNets,
InceptionNets, and DenseNets, whilst maintaining similar performance on
ImageNet. The resulting explanations are of high visual quality and perform
well under quantitative metrics for interpretability. Code available at
https://www.github.com/moboehle/B-cos.","['Moritz Böhle', 'Mario Fritz', 'Bernt Schiele']","['cs.CV', 'stat.ML']",2022-05-20 16:03:29+00:00
http://arxiv.org/abs/2206.02530v2,Persistent Homology of Coarse Grained State Space Networks,"This work is dedicated to the topological analysis of complex transitional
networks for dynamic state detection. Transitional networks are formed from
time series data and they leverage graph theory tools to reveal information
about the underlying dynamic system. However, traditional tools can fail to
summarize the complex topology present in such graphs. In this work, we
leverage persistent homology from topological data analysis to study the
structure of these networks. We contrast dynamic state detection from time
series using a coarse-grained state-space network (CGSSN) and topological data
analysis (TDA) to two state of the art approaches: ordinal partition networks
(OPNs) combined with TDA and the standard application of persistent homology to
the time-delay embedding of the signal. We show that the CGSSN captures rich
information about the dynamic state of the underlying dynamical system as
evidenced by a significant improvement in dynamic state detection and noise
robustness in comparison to OPNs. We also show that because the computational
time of CGSSN is not linearly dependent on the signal's length, it is more
computationally efficient than applying TDA to the time-delay embedding of the
time series.","['Audun D. Myers', 'Max M. Chumley', 'Firas A. Khasawneh', 'Elizabeth Munch']","['stat.ML', 'cs.LG', 'math.AT']",2022-05-20 15:29:29+00:00
http://arxiv.org/abs/2205.10242v1,EXODUS: Stable and Efficient Training of Spiking Neural Networks,"Spiking Neural Networks (SNNs) are gaining significant traction in machine
learning tasks where energy-efficiency is of utmost importance. Training such
networks using the state-of-the-art back-propagation through time (BPTT) is,
however, very time-consuming. Previous work by Shrestha and Orchard [2018]
employs an efficient GPU-accelerated back-propagation algorithm called SLAYER,
which speeds up training considerably. SLAYER, however, does not take into
account the neuron reset mechanism while computing the gradients, which we
argue to be the source of numerical instability. To counteract this, SLAYER
introduces a gradient scale hyperparameter across layers, which needs manual
tuning. In this paper, (i) we modify SLAYER and design an algorithm called
EXODUS, that accounts for the neuron reset mechanism and applies the Implicit
Function Theorem (IFT) to calculate the correct gradients (equivalent to those
computed by BPTT), (ii) we eliminate the need for ad-hoc scaling of gradients,
thus, reducing the training complexity tremendously, (iii) we demonstrate, via
computer simulations, that EXODUS is numerically stable and achieves a
comparable or better performance than SLAYER especially in various tasks with
SNNs that rely on temporal features. Our code is available at
https://github.com/synsense/sinabs-exodus.","['Felix Christian Bauer', 'Gregor Lenz', 'Saeid Haghighatshoar', 'Sadique Sheik']","['cs.NE', 'cs.LG', 'stat.ML']",2022-05-20 15:13:58+00:00
http://arxiv.org/abs/2205.10217v3,Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization,"The Neural Tangent Kernel (NTK) has emerged as a powerful tool to provide
memorization, optimization and generalization guarantees in deep neural
networks. A line of work has studied the NTK spectrum for two-layer and deep
networks with at least a layer with $\Omega(N)$ neurons, $N$ being the number
of training samples. Furthermore, there is increasing evidence suggesting that
deep networks with sub-linear layer widths are powerful memorizers and
optimizers, as long as the number of parameters exceeds the number of samples.
Thus, a natural open question is whether the NTK is well conditioned in such a
challenging sub-linear setup. In this paper, we answer this question in the
affirmative. Our key technical contribution is a lower bound on the smallest
NTK eigenvalue for deep networks with the minimum possible
over-parameterization: the number of parameters is roughly $\Omega(N)$ and,
hence, the number of neurons is as little as $\Omega(\sqrt{N})$. To showcase
the applicability of our NTK bounds, we provide two results concerning
memorization capacity and optimization guarantees for gradient descent
training.","['Simone Bombari', 'Mohammad Hossein Amani', 'Marco Mondelli']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2022-05-20 14:50:24+00:00
http://arxiv.org/abs/2205.10200v2,The Fairness of Credit Scoring Models,"In credit markets, screening algorithms aim to discriminate between good-type
and bad-type borrowers. However, when doing so, they can also discriminate
between individuals sharing a protected attribute (e.g. gender, age, racial
origin) and the rest of the population. This can be unintentional and originate
from the training dataset or from the model itself. We show how to formally
test the algorithmic fairness of scoring models and how to identify the
variables responsible for any lack of fairness. We then use these variables to
optimize the fairness-performance trade-off. Our framework provides guidance on
how algorithmic fairness can be monitored by lenders, controlled by their
regulators, improved for the benefit of protected groups, while still
maintaining a high level of forecasting accuracy.","['Christophe Hurlin', 'Christophe Pérignon', 'Sébastien Saurin']","['stat.ML', 'cs.LG', 'q-fin.RM']",2022-05-20 14:20:40+00:00
http://arxiv.org/abs/2205.10198v3,"A New Central Limit Theorem for the Augmented IPW Estimator: Variance Inflation, Cross-Fit Covariance and Beyond","Estimation of the average treatment effect (ATE) is a central problem in
causal inference. In recent times, inference for the ATE in the presence of
high-dimensional covariates has been extensively studied. Among the diverse
approaches that have been proposed, augmented inverse probability weighting
(AIPW) with cross-fitting has emerged a popular choice in practice. In this
work, we study this cross-fit AIPW estimator under well-specified outcome
regression and propensity score models in a high-dimensional regime where the
number of features and samples are both large and comparable. Under assumptions
on the covariate distribution, we establish a new central limit theorem for the
suitably scaled cross-fit AIPW that applies without any sparsity assumptions on
the underlying high-dimensional parameters. Our CLT uncovers two crucial
phenomena among others: (i) the AIPW exhibits a substantial variance inflation
that can be precisely quantified in terms of the signal-to-noise ratio and
other problem parameters, (ii) the asymptotic covariance between the
pre-cross-fit estimators is non-negligible even on the root-n scale. These
findings are strikingly different from their classical counterparts. On the
technical front, our work utilizes a novel interplay between three distinct
tools--approximate message passing theory, the theory of deterministic
equivalents, and the leave-one-out approach. We believe our proof techniques
should be useful for analyzing other two-stage estimators in this
high-dimensional regime. Finally, we complement our theoretical results with
simulations that demonstrate both the finite sample efficacy of our CLT and its
robustness to our assumptions.","['Kuanhao Jiang', 'Rajarshi Mukherjee', 'Subhabrata Sen', 'Pragya Sur']","['math.ST', 'econ.EM', 'stat.ME', 'stat.ML', 'stat.TH']",2022-05-20 14:17:53+00:00
http://arxiv.org/abs/2205.10088v2,Semi-self-supervised Automated ICD Coding,"Clinical Text Notes (CTNs) contain physicians' reasoning process, written in
an unstructured free text format, as they examine and interview patients. In
recent years, several studies have been published that provide evidence for the
utility of machine learning for predicting doctors' diagnoses from CTNs, a task
known as ICD coding. Data annotation is time consuming, particularly when a
degree of specialization is needed, as is the case for medical data. This paper
presents a method of augmenting a sparsely annotated dataset of Icelandic CTNs
with a machine-learned imputation in a semi-self-supervised manner. We train a
neural network on a small set of annotated CTNs and use it to extract clinical
features from a set of un-annotated CTNs. These clinical features consist of
answers to about a thousand potential questions that a physician might find the
answers to during a consultation of a patient. The features are then used to
train a classifier for the diagnosis of certain types of diseases. We report
the results of an evaluation of this data augmentation method over three tiers
of data availability to the physician. Our data augmentation method shows a
significant positive effect which is diminished when clinical features from the
examination of the patient and diagnostics are made available. We recommend our
method for augmenting scarce datasets for systems that take decisions based on
clinical features that do not include examinations or tests.","['Hlynur D. Hlynsson', 'Steindór Ellertsson', 'Jón F. Daðason', 'Emil L. Sigurdsson', 'Hrafn Loftsson']","['cs.CL', 'cs.LG', 'stat.ML']",2022-05-20 11:12:54+00:00
http://arxiv.org/abs/2205.10082v2,On the Calibration of Probabilistic Classifier Sets,"Multi-class classification methods that produce sets of probabilistic
classifiers, such as ensemble learning methods, are able to model aleatoric and
epistemic uncertainty. Aleatoric uncertainty is then typically quantified via
the Bayes error, and epistemic uncertainty via the size of the set. In this
paper, we extend the notion of calibration, which is commonly used to evaluate
the validity of the aleatoric uncertainty representation of a single
probabilistic classifier, to assess the validity of an epistemic uncertainty
representation obtained by sets of probabilistic classifiers. Broadly speaking,
we call a set of probabilistic classifiers calibrated if one can find a
calibrated convex combination of these classifiers. To evaluate this notion of
calibration, we propose a novel nonparametric calibration test that generalizes
an existing test for single probabilistic classifiers to the case of sets of
probabilistic classifiers. Making use of this test, we empirically show that
ensembles of deep neural networks are often not well calibrated.","['Thomas Mortier', 'Viktor Bengs', 'Eyke Hüllermeier', 'Stijn Luca', 'Willem Waegeman']","['stat.ML', 'cs.LG']",2022-05-20 10:57:46+00:00
http://arxiv.org/abs/2205.10060v3,The Unreasonable Effectiveness of Deep Evidential Regression,"There is a significant need for principled uncertainty reasoning in machine
learning systems as they are increasingly deployed in safety-critical domains.
A new approach with uncertainty-aware regression-based neural networks (NNs),
based on learning evidential distributions for aleatoric and epistemic
uncertainties, shows promise over traditional deterministic methods and typical
Bayesian NNs, notably with the capabilities to disentangle aleatoric and
epistemic uncertainties. Despite some empirical success of Deep Evidential
Regression (DER), there are important gaps in the mathematical foundation that
raise the question of why the proposed technique seemingly works. We detail the
theoretical shortcomings and analyze the performance on synthetic and
real-world data sets, showing that Deep Evidential Regression is a heuristic
rather than an exact uncertainty quantification. We go on to discuss
corrections and redefinitions of how aleatoric and epistemic uncertainties
should be extracted from NNs.","['Nis Meinert', 'Jakob Gawlikowski', 'Alexander Lavin']","['cs.LG', 'cs.NE', 'stat.ML']",2022-05-20 10:10:32+00:00
http://arxiv.org/abs/2205.10055v2,A Case of Exponential Convergence Rates for SVM,"Classification is often the first problem described in introductory machine
learning classes. Generalization guarantees of classification have historically
been offered by Vapnik-Chervonenkis theory. Yet those guarantees are based on
intractable algorithms, which has led to the theory of surrogate methods in
classification. Guarantees offered by surrogate methods are based on
calibration inequalities, which have been shown to be highly sub-optimal under
some margin conditions, failing short to capture exponential convergence
phenomena. Those ""super"" fast rates are becoming to be well understood for
smooth surrogates, but the picture remains blurry for non-smooth losses such as
the hinge loss, associated with the renowned support vector machines. In this
paper, we present a simple mechanism to obtain fast convergence rates and we
investigate its usage for SVM. In particular, we show that SVM can exhibit
exponential convergence rates even without assuming the hard Tsybakov margin
condition.","['Vivien Cabannes', 'Stefano Vigogna']","['stat.ML', 'cs.AI', 'cs.LG', '68T05', 'G.3']",2022-05-20 09:49:27+00:00
http://arxiv.org/abs/2205.10041v2,Posterior Refinement Improves Sample Efficiency in Bayesian Neural Networks,"Monte Carlo (MC) integration is the de facto method for approximating the
predictive distribution of Bayesian neural networks (BNNs). But, even with many
MC samples, Gaussian-based BNNs could still yield bad predictive performance
due to the posterior approximation's error. Meanwhile, alternatives to MC
integration tend to be more expensive or biased. In this work, we
experimentally show that the key to good MC-approximated predictive
distributions is the quality of the approximate posterior itself. However,
previous methods for obtaining accurate posterior approximations are expensive
and non-trivial to implement. We, therefore, propose to refine Gaussian
approximate posteriors with normalizing flows. When applied to last-layer BNNs,
it yields a simple \emph{post hoc} method for improving pre-existing parametric
approximations. We show that the resulting posterior approximation is
competitive with even the gold-standard full-batch Hamiltonian Monte Carlo.","['Agustinus Kristiadi', 'Runa Eschenhagen', 'Philipp Hennig']","['cs.LG', 'stat.ML']",2022-05-20 09:24:39+00:00
http://arxiv.org/abs/2205.10024v1,Trend analysis and forecasting air pollution in Rwanda,"Air pollution is a major public health problem worldwide although the lack of
data is a global issue for most low and middle income countries. Ambient air
pollution in the form of fine particulate matter (PM2.5) exceeds the World
Health Organization guidelines in Rwanda with a daily average of around 42.6
microgram per meter cube. Monitoring and mitigation strategies require an
expensive investment in equipment to collect pollution data. Low-cost sensor
technology and machine learning methods have appeared as an alternative
solution to get reliable information for decision making. This paper analyzes
the trend of air pollution in Rwanda and proposes forecasting models suitable
to data collected by a network of low-cost sensors deployed in Rwanda.","['Paterne Gahungu', 'Jean Remy Kubwimana']","['stat.ML', 'cs.LG']",2022-05-20 08:39:42+00:00
http://arxiv.org/abs/2205.09968v1,A General Framework for quantifying Aleatoric and Epistemic uncertainty in Graph Neural Networks,"Graph Neural Networks (GNN) provide a powerful framework that elegantly
integrates Graph theory with Machine learning for modeling and analysis of
networked data. We consider the problem of quantifying the uncertainty in
predictions of GNN stemming from modeling errors and measurement uncertainty.
We consider aleatoric uncertainty in the form of probabilistic links and noise
in feature vector of nodes, while epistemic uncertainty is incorporated via a
probability distribution over the model parameters. We propose a unified
approach to treat both sources of uncertainty in a Bayesian framework, where
Assumed Density Filtering is used to quantify aleatoric uncertainty and Monte
Carlo dropout captures uncertainty in model parameters. Finally, the two
sources of uncertainty are aggregated to estimate the total uncertainty in
predictions of a GNN. Results in the real-world datasets demonstrate that the
Bayesian model performs at par with a frequentist model and provides additional
information about predictions uncertainty that are sensitive to uncertainties
in the data and model.","['Sai Munikoti', 'Deepesh Agarwal', 'Laya Das', 'Balasubramaniam Natarajan']","['cs.LG', 'stat.ML']",2022-05-20 05:25:40+00:00
http://arxiv.org/abs/2205.09940v2,Conformal Prediction with Temporal Quantile Adjustments,"We develop Temporal Quantile Adjustment (TQA), a general method to construct
efficient and valid prediction intervals (PIs) for regression on
cross-sectional time series data. Such data is common in many domains,
including econometrics and healthcare. A canonical example in healthcare is
predicting patient outcomes using physiological time-series data, where a
population of patients composes a cross-section. Reliable PI estimators in this
setting must address two distinct notions of coverage: cross-sectional coverage
across a cross-sectional slice, and longitudinal coverage along the temporal
dimension for each time series. Recent works have explored adapting Conformal
Prediction (CP) to obtain PIs in the time series context. However, none handles
both notions of coverage simultaneously. CP methods typically query a
pre-specified quantile from the distribution of nonconformity scores on a
calibration set. TQA adjusts the quantile to query in CP at each time $t$,
accounting for both cross-sectional and longitudinal coverage in a
theoretically-grounded manner. The post-hoc nature of TQA facilitates its use
as a general wrapper around any time series regression model. We validate TQA's
performance through extensive experimentation: TQA generally obtains efficient
PIs and improves longitudinal coverage while preserving cross-sectional
coverage.","['Zhen Lin', 'Shubhendu Trivedi', 'Jimeng Sun']","['stat.ML', 'cs.LG', 'stat.ME']",2022-05-20 03:31:03+00:00
http://arxiv.org/abs/2205.09914v1,Robust Expected Information Gain for Optimal Bayesian Experimental Design Using Ambiguity Sets,"The ranking of experiments by expected information gain (EIG) in Bayesian
experimental design is sensitive to changes in the model's prior distribution,
and the approximation of EIG yielded by sampling will have errors similar to
the use of a perturbed prior. We define and analyze \emph{robust expected
information gain} (REIG), a modification of the objective in EIG maximization
by minimizing an affine relaxation of EIG over an ambiguity set of
distributions that are close to the original prior in KL-divergence. We show
that, when combined with a sampling-based approach to estimating EIG, REIG
corresponds to a `log-sum-exp' stabilization of the samples used to estimate
EIG, meaning that it can be efficiently implemented in practice. Numerical
tests combining REIG with variational nested Monte Carlo (VNMC), adaptive
contrastive estimation (ACE) and mutual information neural estimation (MINE)
suggest that in practice REIG also compensates for the variability of
under-sampled estimators.","['Jinwoo Go', 'Tobin Isaac']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.CO', 'stat.ME']",2022-05-20 01:07:41+00:00
http://arxiv.org/abs/2205.09909v2,Sparse Infinite Random Feature Latent Variable Modeling,"We propose a non-linear, Bayesian non-parametric latent variable model where
the latent space is assumed to be sparse and infinite dimensional a priori
using an Indian buffet process prior. A posteriori, the number of instantiated
dimensions in the latent space is guaranteed to be finite. The purpose of
placing the Indian buffet process on the latent variables is to: 1.)
Automatically and probabilistically select the number of latent dimensions. 2.)
Impose sparsity in the latent space, where the Indian buffet process will
select which elements are exactly zero. Our proposed model allows for sparse,
non-linear latent variable modeling where the number of latent dimensions is
selected automatically. Inference is made tractable using the random Fourier
approximation and we can easily implement posterior inference through Markov
chain Monte Carlo sampling. This approach is amenable to many observation
models beyond the Gaussian setting. We demonstrate the utility of our method on
a variety of synthetic, biological and text datasets and show that we can
obtain superior test set performance compared to previous latent variable
models.",['Michael Minyi Zhang'],"['stat.ML', 'cs.AI', 'cs.LG']",2022-05-20 00:29:28+00:00
http://arxiv.org/abs/2205.09906v1,Data Augmentation for Compositional Data: Advancing Predictive Models of the Microbiome,"Data augmentation plays a key role in modern machine learning pipelines.
While numerous augmentation strategies have been studied in the context of
computer vision and natural language processing, less is known for other data
modalities. Our work extends the success of data augmentation to compositional
data, i.e., simplex-valued data, which is of particular interest in the context
of the human microbiome. Drawing on key principles from compositional data
analysis, such as the Aitchison geometry of the simplex and subcompositions, we
define novel augmentation strategies for this data modality. Incorporating our
data augmentations into standard supervised learning pipelines results in
consistent performance gains across a wide range of standard benchmark
datasets. In particular, we set a new state-of-the-art for key disease
prediction tasks including colorectal cancer, type 2 diabetes, and Crohn's
disease. In addition, our data augmentations enable us to define a novel
contrastive learning model, which improves on previous representation learning
approaches for microbiome compositional data. Our code is available at
https://github.com/cunningham-lab/AugCoDa.","['Elliott Gordon-Rodriguez', 'Thomas P. Quinn', 'John P. Cunningham']","['stat.ML', 'cs.LG']",2022-05-20 00:24:00+00:00
http://arxiv.org/abs/2205.09899v1,Breaking the $\sqrt{T}$ Barrier: Instance-Independent Logarithmic Regret in Stochastic Contextual Linear Bandits,"We prove an instance independent (poly) logarithmic regret for stochastic
contextual bandits with linear payoff. Previously, in \cite{chu2011contextual},
a lower bound of $\mathcal{O}(\sqrt{T})$ is shown for the contextual linear
bandit problem with arbitrary (adversarily chosen) contexts. In this paper, we
show that stochastic contexts indeed help to reduce the regret from $\sqrt{T}$
to $\polylog(T)$. We propose Low Regret Stochastic Contextual Bandits
(\texttt{LR-SCB}), which takes advantage of the stochastic contexts and
performs parameter estimation (in $\ell_2$ norm) and regret minimization
simultaneously. \texttt{LR-SCB} works in epochs, where the parameter estimation
of the previous epoch is used to reduce the regret of the current epoch. The
(poly) logarithmic regret of \texttt{LR-SCB} stems from two crucial facts: (a)
the application of a norm adaptive algorithm to exploit the parameter
estimation and (b) an analysis of the shifted linear contextual bandit
algorithm, showing that shifting results in increasing regret. We have also
shown experimentally that stochastic contexts indeed incurs a regret that
scales with $\polylog(T)$.","['Avishek Ghosh', 'Abishek Sankararaman']","['stat.ML', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT']",2022-05-19 23:41:46+00:00
http://arxiv.org/abs/2205.09860v2,Mean-Field Analysis of Two-Layer Neural Networks: Global Optimality with Linear Convergence Rates,"We consider optimizing two-layer neural networks in the mean-field regime
where the learning dynamics of network weights can be approximated by the
evolution in the space of probability measures over the weight parameters
associated with the neurons. The mean-field regime is a theoretically
attractive alternative to the NTK (lazy training) regime which is only
restricted locally in the so-called neural tangent kernel space around
specialized initializations. Several prior works (\cite{chizat2018global,
mei2018mean}) establish the asymptotic global optimality of the mean-field
regime, but it is still challenging to obtain a quantitative convergence rate
due to the complicated unbounded nonlinearity of the training dynamics. This
work establishes the first linear convergence result for vanilla two-layer
neural networks trained by continuous-time noisy gradient descent in the
mean-field regime. Our result relies on a novel time-depdendent estimate of the
logarithmic Sobolev constants for a family of measures determined by the
evolving distribution of hidden neurons.","['Jingwei Zhang', 'Xunpeng Huang', 'Jincheng Yu']","['cs.LG', 'math.AP', 'stat.ML']",2022-05-19 21:05:40+00:00
http://arxiv.org/abs/2205.09838v1,Why GANs are overkill for NLP,"This work offers a novel theoretical perspective on why, despite numerous
attempts, adversarial approaches to generative modeling (e.g., GANs) have not
been as popular for certain generation tasks, particularly sequential tasks
such as Natural Language Generation, as they have in others, such as Computer
Vision. In particular, on sequential data such as text, maximum-likelihood
approaches are significantly more utilized than GANs. We show that, while it
may seem that maximizing likelihood is inherently different than minimizing
distinguishability, this distinction is largely artificial and only holds for
limited models. We argue that minimizing KL-divergence (i.e., maximizing
likelihood) is a more efficient approach to effectively minimizing the same
distinguishability criteria that adversarial models seek to optimize.
Reductions show that minimizing distinguishability can be seen as simply
boosting likelihood for certain families of models including n-gram models and
neural networks with a softmax output layer. To achieve a full polynomial-time
reduction, a novel next-token distinguishability model is considered.","['David Alvarez-Melis', 'Vikas Garg', 'Adam Tauman Kalai']","['cs.LG', 'stat.ML']",2022-05-19 20:26:51+00:00
http://arxiv.org/abs/2205.09825v2,Algorithms for Weak Optimal Transport with an Application to Economics,"The theory of weak optimal transport (WOT), introduced by [Gozlan et al.,
2017], generalizes the classic Monge-Kantorovich framework by allowing the
transport cost between one point and the points it is matched with to be
nonlinear. In the so-called barycentric version of WOT, the cost for
transporting a point $x$ only depends on $x$ and on the barycenter of the
points it is matched with. This aggregation property of WOT is appealing in
machine learning, economics and finance. Yet algorithms to compute WOT have
only been developed for the special case of quadratic barycentric WOT, or
depend on neural networks with no guarantee on the computed value and matching.
The main difficulty lies in the transportation constraints which are costly to
project onto. In this paper, we propose to use mirror descent algorithms to
solve the primal and dual versions of the WOT problem. We also apply our
algorithms to the variant of WOT introduced by [Chon\'e et al., 2022] where
mass is distributed from one space to another through unnormalized kernels
(WOTUK). We empirically compare the solutions of WOT and WOTUK with classical
OT. We illustrate our numerical methods to the economic framework of [Chon\'e
and Kramarz, 2021], namely the matching between workers and firms on labor
markets.","['François-Pierre Paty', 'Philippe Choné', 'Francis Kramarz']","['stat.ML', 'cs.LG', 'stat.AP']",2022-05-19 19:53:20+00:00
http://arxiv.org/abs/2205.09824v3,Deep Learning Methods for Proximal Inference via Maximum Moment Restriction,"The No Unmeasured Confounding Assumption is widely used to identify causal
effects in observational studies. Recent work on proximal inference has
provided alternative identification results that succeed even in the presence
of unobserved confounders, provided that one has measured a sufficiently rich
set of proxy variables, satisfying specific structural conditions. However,
proximal inference requires solving an ill-posed integral equation. Previous
approaches have used a variety of machine learning techniques to estimate a
solution to this integral equation, commonly referred to as the bridge
function. However, prior work has often been limited by relying on
pre-specified kernel functions, which are not data adaptive and struggle to
scale to large datasets. In this work, we introduce a flexible and scalable
method based on a deep neural network to estimate causal effects in the
presence of unmeasured confounding using proximal inference. Our method
achieves state of the art performance on two well-established proximal
inference benchmarks. Finally, we provide theoretical consistency guarantees
for our method.","['Benjamin Kompa', 'David R. Bellamy', 'Thomas Kolokotrones', 'James M. Robins', 'Andrew L. Beam']","['stat.ML', 'cs.LG']",2022-05-19 19:51:42+00:00
http://arxiv.org/abs/2205.09801v3,Representation Power of Graph Neural Networks: Improved Expressivity via Algebraic Analysis,"Despite the remarkable success of Graph Neural Networks (GNNs), the common
belief is that their representation power is limited and that they are at most
as expressive as the Weisfeiler-Lehman (WL) algorithm. In this paper, we argue
the opposite and show that standard GNNs, with anonymous inputs, produce more
discriminative representations than the WL algorithm. Our novel analysis
employs linear algebraic tools and characterizes the representation power of
GNNs with respect to the eigenvalue decomposition of the graph operators. We
prove that GNNs are able to generate distinctive outputs from white
uninformative inputs, for, at least, all graphs that have different
eigenvalues. We also show that simple convolutional architectures with white
inputs, produce equivariant features that count the closed paths in the graph
and are provably more expressive than the WL representations. Thorough
experimental analysis on graph isomorphism and graph classification datasets
corroborates our theoretical results and demonstrates the effectiveness of the
proposed approach.","['Charilaos I. Kanatsoulis', 'Alejandro Ribeiro']","['cs.LG', 'cs.AI', 'eess.SP', 'stat.ML']",2022-05-19 18:40:25+00:00
http://arxiv.org/abs/2205.09787v4,Causal Discovery and Knowledge Injection for Contestable Neural Networks (with Appendices),"Neural networks have proven to be effective at solving machine learning tasks
but it is unclear whether they learn any relevant causal relationships, while
their black-box nature makes it difficult for modellers to understand and debug
them. We propose a novel method overcoming these issues by allowing a two-way
interaction whereby neural-network-empowered machines can expose the
underpinning learnt causal graphs and humans can contest the machines by
modifying the causal graphs before re-injecting them into the machines. The
learnt models are guaranteed to conform to the graphs and adhere to expert
knowledge, some of which can also be given up-front. By building a window into
the model behaviour and enabling knowledge injection, our method allows
practitioners to debug networks based on the causal structure discovered from
the data and underpinning the predictions. Experiments with real and synthetic
tabular data show that our method improves predictive performance up to 2.4x
while producing parsimonious networks, up to 7x smaller in the input layer,
compared to SOTA regularised networks.","['Fabrizio Russo', 'Francesca Toni']","['cs.LG', 'cs.AI', 'cs.HC', 'stat.ML']",2022-05-19 18:21:12+00:00
http://arxiv.org/abs/2205.09735v2,Foundation Posteriors for Approximate Probabilistic Inference,"Probabilistic programs provide an expressive representation language for
generative models. Given a probabilistic program, we are interested in the task
of posterior inference: estimating a latent variable given a set of observed
variables. Existing techniques for inference in probabilistic programs often
require choosing many hyper-parameters, are computationally expensive, and/or
only work for restricted classes of programs. Here we formulate inference as
masked language modeling: given a program, we generate a supervised dataset of
variables and assignments, and randomly mask a subset of the assignments. We
then train a neural network to unmask the random values, defining an
approximate posterior distribution. By optimizing a single neural network
across a range of programs we amortize the cost of training, yielding a
""foundation"" posterior able to do zero-shot inference for new programs. The
foundation posterior can also be fine-tuned for a particular program and
dataset by optimizing a variational inference objective. We show the efficacy
of the approach, zero-shot and fine-tuned, on a benchmark of STAN programs.","['Mike Wu', 'Noah Goodman']","['cs.LG', 'stat.ML']",2022-05-19 17:42:37+00:00
http://arxiv.org/abs/2205.09727v2,The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics,"Many high-dimensional statistical inference problems are believed to possess
inherent computational hardness. Various frameworks have been proposed to give
rigorous evidence for such hardness, including lower bounds against restricted
models of computation (such as low-degree functions), as well as methods rooted
in statistical physics that are based on free energy landscapes. This paper
aims to make a rigorous connection between the seemingly different low-degree
and free-energy based approaches. We define a free-energy based criterion for
hardness and formally connect it to the well-established notion of low-degree
hardness for a broad class of statistical problems, namely all Gaussian
additive models and certain models with a sparse planted signal. By leveraging
these rigorous connections we are able to: establish that for Gaussian additive
models the ""algebraic"" notion of low-degree hardness implies failure of
""geometric"" local MCMC algorithms, and provide new low-degree lower bounds for
sparse linear regression which seem difficult to prove directly. These results
provide both conceptual insights into the connections between different notions
of hardness, as well as concrete technical tools such as new methods for
proving low-degree lower bounds.","['Afonso S. Bandeira', 'Ahmed El Alaoui', 'Samuel B. Hopkins', 'Tselil Schramm', 'Alexander S. Wein', 'Ilias Zadik']","['math.ST', 'cond-mat.stat-mech', 'cs.CC', 'cs.DS', 'stat.ML', 'stat.TH']",2022-05-19 17:39:29+00:00
http://arxiv.org/abs/2205.09717v1,Flexible Modeling and Multitask Learning using Differentiable Tree Ensembles,"Decision tree ensembles are widely used and competitive learning models.
Despite their success, popular toolkits for learning tree ensembles have
limited modeling capabilities. For instance, these toolkits support a limited
number of loss functions and are restricted to single task learning. We propose
a flexible framework for learning tree ensembles, which goes beyond existing
toolkits to support arbitrary loss functions, missing responses, and multi-task
learning. Our framework builds on differentiable (a.k.a. soft) tree ensembles,
which can be trained using first-order methods. However, unlike classical
trees, differentiable trees are difficult to scale. We therefore propose a
novel tensor-based formulation of differentiable trees that allows for
efficient vectorization on GPUs. We perform experiments on a collection of 28
real open-source and proprietary datasets, which demonstrate that our framework
can lead to 100x more compact and 23% more expressive tree ensembles than those
by popular toolkits.","['Shibal Ibrahim', 'Hussein Hazimeh', 'Rahul Mazumder']","['cs.LG', 'stat.ML']",2022-05-19 17:30:49+00:00
http://arxiv.org/abs/2205.09674v1,Disentangling Active and Passive Cosponsorship in the U.S. Congress,"In the U.S. Congress, legislators can use active and passive cosponsorship to
support bills. We show that these two types of cosponsorship are driven by two
different motivations: the backing of political colleagues and the backing of
the bill's content. To this end, we develop an Encoder+RGCN based model that
learns legislator representations from bill texts and speech transcripts. These
representations predict active and passive cosponsorship with an F1-score of
0.88. Applying our representations to predict voting decisions, we show that
they are interpretable and generalize to unseen tasks.","['Giuseppe Russo', 'Christoph Gote', 'Laurence Brandenberger', 'Sophia Schlosser', 'Frank Schweitzer']","['cs.LG', 'cs.CL', 'cs.CY', 'physics.data-an', 'stat.ML']",2022-05-19 16:33:46+00:00
