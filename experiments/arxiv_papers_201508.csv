id,title,abstract,authors,categories,date
http://arxiv.org/abs/1509.08535v3,Boolean Matrix Factorization and Noisy Completion via Message Passing,"Boolean matrix factorization and Boolean matrix completion from noisy
observations are desirable unsupervised data-analysis methods due to their
interpretability, but hard to perform due to their NP-hardness. We treat these
problems as maximum a posteriori inference problems in a graphical model and
present a message passing approach that scales linearly with the number of
observations and factors. Our empirical study demonstrates that message passing
is able to recover low-rank Boolean matrices, in the boundaries of
theoretically possible recovery and compares favorably with state-of-the-art in
real-world applications, such collaborative filtering with large-scale Boolean
data.","['Siamak Ravanbakhsh', 'Barnabas Poczos', 'Russell Greiner']","['math.ST', 'cs.AI', 'cs.DM', 'stat.ML', 'stat.TH']",2015-09-28 23:11:16+00:00
http://arxiv.org/abs/1509.08455v1,Efficient Empowerment,"Empowerment quantifies the influence an agent has on its environment. This is
formally achieved by the maximum of the expected KL-divergence between the
distribution of the successor state conditioned on a specific action and a
distribution where the actions are marginalised out. This is a natural
candidate for an intrinsic reward signal in the context of reinforcement
learning: the agent will place itself in a situation where its action have
maximum stability and maximum influence on the future. The limiting factor so
far has been the computational complexity of the method: the only way of
calculation has so far been a brute force algorithm, reducing the applicability
of the method to environments with a small set discrete states. In this work,
we propose to use an efficient approximation for marginalising out the actions
in the case of continuous environments. This allows fast evaluation of
empowerment, paving the way towards challenging environments such as real world
robotics. The method is presented on a pendulum swing up problem.","['Maximilian Karl', 'Justin Bayer', 'Patrick van der Smagt']","['stat.ML', 'cs.LG']",2015-09-28 19:58:31+00:00
http://arxiv.org/abs/1509.08387v2,Distance-Penalized Active Learning Using Quantile Search,"Adaptive sampling theory has shown that, with proper assumptions on the
signal class, algorithms exist to reconstruct a signal in $\mathbb{R}^{d}$ with
an optimal number of samples. We generalize this problem to the case of spatial
signals, where the sampling cost is a function of both the number of samples
taken and the distance traveled during estimation. This is motivated by our
work studying regions of low oxygen concentration in the Great Lakes. We show
that for one-dimensional threshold classifiers, a tradeoff between the number
of samples taken and distance traveled can be achieved using a generalization
of binary search, which we refer to as quantile search. We characterize both
the estimation error after a fixed number of samples and the distance traveled
in the noiseless case, as well as the estimation error in the case of noisy
measurements. We illustrate our results in both simulations and experiments and
show that our method outperforms existing algorithms in the majority of
practical scenarios.","['John Lipor', 'Brandon Wong', 'Donald Scavia', 'Branko Kerkez', 'Laura Balzano']","['stat.ML', 'cs.LG', '62L05', 'G.3; H.3.3']",2015-09-28 16:48:39+00:00
http://arxiv.org/abs/1509.08360v1,Compressive spectral embedding: sidestepping the SVD,"Spectral embedding based on the Singular Value Decomposition (SVD) is a
widely used ""preprocessing"" step in many learning tasks, typically leading to
dimensionality reduction by projecting onto a number of dominant singular
vectors and rescaling the coordinate axes (by a predefined function of the
singular value). However, the number of such vectors required to capture
problem structure grows with problem size, and even partial SVD computation
becomes a bottleneck. In this paper, we propose a low-complexity it compressive
spectral embedding algorithm, which employs random projections and finite order
polynomial expansions to compute approximations to SVD-based embedding. For an
m times n matrix with T non-zeros, its time complexity is O((T+m+n)log(m+n)),
and the embedding dimension is O(log(m+n)), both of which are independent of
the number of singular vectors whose effect we wish to capture. To the best of
our knowledge, this is the first work to circumvent this dependence on the
number of singular vectors for general SVD-based embeddings. The key to
sidestepping the SVD is the observation that, for downstream inference tasks
such as clustering and classification, we are only interested in using the
resulting embedding to evaluate pairwise similarity metrics derived from the
euclidean norm, rather than capturing the effect of the underlying matrix on
arbitrary vectors as a partial SVD tries to do. Our numerical results on
network datasets demonstrate the efficacy of the proposed method, and motivate
further exploration of its application to large-scale inference tasks.","['Dinesh Ramasamy', 'Upamanyu Madhow']","['stat.ML', 'cs.LG']",2015-09-28 15:32:20+00:00
http://arxiv.org/abs/1509.08333v3,High-dimensional Time Series Prediction with Missing Values,"High-dimensional time series prediction is needed in applications as diverse
as demand forecasting and climatology. Often, such applications require methods
that are both highly scalable, and deal with noisy data in terms of corruptions
or missing values. Classical time series methods usually fall short of handling
both these issues. In this paper, we propose to adapt matrix matrix completion
approaches that have previously been successfully applied to large scale noisy
data, but which fail to adequately model high-dimensional time series due to
temporal dependencies. We present a novel temporal regularized matrix
factorization (TRMF) framework which supports data-driven temporal dependency
learning and enables forecasting ability to our new matrix factorization
approach. TRMF is highly general, and subsumes many existing matrix
factorization approaches for time series data. We make interesting connections
to graph regularized matrix factorization methods in the context of learning
the dependencies. Experiments on both real and synthetic data show that TRMF
outperforms several existing approaches for common time series tasks.","['Hsiang-Fu Yu', 'Nikhil Rao', 'Inderjit S. Dhillon']","['cs.LG', 'stat.ML']",2015-09-28 14:37:14+00:00
http://arxiv.org/abs/1509.08329v1,Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA for the Design of Training Graphs,"Slow feature analysis (SFA) is an unsupervised learning algorithm that
extracts slowly varying features from a time series. Graph-based SFA (GSFA) is
a supervised extension that can solve regression problems if followed by a
post-processing regression algorithm. A training graph specifies arbitrary
connections between the training samples. The connections in current graphs,
however, only depend on the rank of the involved labels. Exploiting the exact
label values makes further improvements in estimation accuracy possible.
  In this article, we propose the exact label learning (ELL) method to create a
graph that codes the desired label explicitly, so that GSFA is able to extract
a normalized version of it directly. The ELL method is used for three tasks:
(1) We estimate gender from artificial images of human faces (regression) and
show the advantage of coding additional labels, particularly skin color. (2) We
analyze two existing graphs for regression. (3) We extract compact
discriminative features to classify traffic sign images. When the number of
output features is limited, a higher classification rate is obtained compared
to a graph equivalent to nonlinear Fisher discriminant analysis. The method is
versatile, directly supports multiple labels, and provides higher accuracy
compared to current graphs for the problems considered.","['Alberto N. Escalante-B.', 'Laurenz Wiskott']","['cs.AI', 'cs.CV', 'stat.ML']",2015-09-28 14:19:59+00:00
http://arxiv.org/abs/1509.08327v2,Unbiased Bayesian Inference for Population Markov Jump Processes via Random Truncations,"We consider continuous time Markovian processes where populations of
individual agents interact stochastically according to kinetic rules. Despite
the increasing prominence of such models in fields ranging from biology to
smart cities, Bayesian inference for such systems remains challenging, as these
are continuous time, discrete state systems with potentially infinite
state-space. Here we propose a novel efficient algorithm for joint state /
parameter posterior sampling in population Markov Jump processes. We introduce
a class of pseudo-marginal sampling algorithms based on a random truncation
method which enables a principled treatment of infinite state spaces. Extensive
evaluation on a number of benchmark models shows that this approach achieves
considerable savings compared to state of the art methods, retaining accuracy
and fast convergence. We also present results on a synthetic biology data set
showing the potential for practical usefulness of our work.","['Anastasis Georgoulas', 'Jane Hillston', 'Guido Sanguinetti']",['stat.ML'],2015-09-28 14:14:07+00:00
http://arxiv.org/abs/1509.08144v2,Optimal Copula Transport for Clustering Multivariate Time Series,"This paper presents a new methodology for clustering multivariate time series
leveraging optimal transport between copulas. Copulas are used to encode both
(i) intra-dependence of a multivariate time series, and (ii) inter-dependence
between two time series. Then, optimal copula transport allows us to define two
distances between multivariate time series: (i) one for measuring
intra-dependence dissimilarity, (ii) another one for measuring inter-dependence
dissimilarity based on a new multivariate dependence coefficient which is
robust to noise, deterministic, and which can target specified dependencies.","['Gautier Marti', 'Frank Nielsen', 'Philippe Donnat']","['cs.LG', 'stat.ML']",2015-09-27 21:17:09+00:00
http://arxiv.org/abs/1509.07982v2,Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes,"We consider the problem of jointly estimating multiple inverse covariance
matrices from high-dimensional data consisting of distinct classes. An
$\ell_2$-penalized maximum likelihood approach is employed. The suggested
approach is flexible and generic, incorporating several other
$\ell_2$-penalized estimators as special cases. In addition, the approach
allows specification of target matrices through which prior knowledge may be
incorporated and which can stabilize the estimation procedure in
high-dimensional settings. The result is a targeted fused ridge estimator that
is of use when the precision matrices of the constituent classes are believed
to chiefly share the same structure while potentially differing in a number of
locations of interest. It has many applications in (multi)factorial study
designs. We focus on the graphical interpretation of precision matrices with
the proposed estimator then serving as a basis for integrative or meta-analytic
Gaussian graphical modeling. Situations are considered in which the classes are
defined by data sets and subtypes of diseases. The performance of the proposed
estimator in the graphical modeling setting is assessed through extensive
simulation experiments. Its practical usability is illustrated by the
differential network modeling of 12 large-scale gene expression data sets of
diffuse large B-cell lymphoma subtypes. The estimator and its related
procedures are incorporated into the R-package rags2ridges.","['Anders Ellern Bilgrau', 'Carel F. W. Peeters', 'Poul Svante Eriksen', 'Martin Bøgsted', 'Wessel N. van Wieringen']","['stat.ME', 'q-bio.MN', 'stat.ML']",2015-09-26 14:08:14+00:00
http://arxiv.org/abs/1509.07892v2,Evasion and Hardening of Tree Ensemble Classifiers,"Classifier evasion consists in finding for a given instance $x$ the nearest
instance $x'$ such that the classifier predictions of $x$ and $x'$ are
different. We present two novel algorithms for systematically computing
evasions for tree ensembles such as boosted trees and random forests. Our first
algorithm uses a Mixed Integer Linear Program solver and finds the optimal
evading instance under an expressive set of constraints. Our second algorithm
trades off optimality for speed by using symbolic prediction, a novel algorithm
for fast finite differences on tree ensembles. On a digit recognition task, we
demonstrate that both gradient boosted trees and random forests are extremely
susceptible to evasions. Finally, we harden a boosted tree model without loss
of predictive accuracy by augmenting the training set of each boosting round
with evading instances, a technique we call adversarial boosting.","['Alex Kantchelian', 'J. D. Tygar', 'Anthony D. Joseph']","['cs.LG', 'cs.CR', 'stat.ML']",2015-09-25 20:57:35+00:00
http://arxiv.org/abs/1509.07859v2,Information Limits for Recovering a Hidden Community,"We study the problem of recovering a hidden community of cardinality $K$ from
an $n \times n$ symmetric data matrix $A$, where for distinct indices $i,j$,
$A_{ij} \sim P$ if $i, j$ both belong to the community and $A_{ij} \sim Q$
otherwise, for two known probability distributions $P$ and $Q$ depending on
$n$. If $P={\rm Bern}(p)$ and $Q={\rm Bern}(q)$ with $p>q$, it reduces to the
problem of finding a densely-connected $K$-subgraph planted in a large
Erd\""os-R\'enyi graph; if $P=\mathcal{N}(\mu,1)$ and $Q=\mathcal{N}(0,1)$ with
$\mu>0$, it corresponds to the problem of locating a $K \times K$ principal
submatrix of elevated means in a large Gaussian random matrix. We focus on two
types of asymptotic recovery guarantees as $n \to \infty$: (1) weak recovery:
expected number of classification errors is $o(K)$; (2) exact recovery:
probability of classifying all indices correctly converges to one. Under mild
assumptions on $P$ and $Q$, and allowing the community size to scale
sublinearly with $n$, we derive a set of sufficient conditions and a set of
necessary conditions for recovery, which are asymptotically tight with sharp
constants. The results hold in particular for the Gaussian case, and for the
case of bounded log likelihood ratio, including the Bernoulli case whenever
$\frac{p}{q}$ and $\frac{1-p}{1-q}$ are bounded away from zero and infinity. An
important algorithmic implication is that, whenever exact recovery is
information theoretically possible, any algorithm that provides weak recovery
when the community size is concentrated near $K$ can be upgraded to achieve
exact recovery in linear additional time by a simple voting procedure.","['Bruce Hajek', 'Yihong Wu', 'Jiaming Xu']","['stat.ML', 'cs.IT', 'math.IT']",2015-09-25 19:56:35+00:00
http://arxiv.org/abs/1509.07751v1,Efficient Computation of the Quasi Likelihood function for Discretely Observed Diffusion Processes,"We introduce a simple method for nearly simultaneous computation of all
moments needed for quasi maximum likelihood estimation of parameters in
discretely observed stochastic differential equations commonly seen in finance.
The method proposed in this papers is not restricted to any particular dynamics
of the differential equation and is virtually insensitive to the sampling
interval. The key contribution of the paper is that computational complexity is
sublinear in the number of observations as we compute all moments through a
single operation. Furthermore, that operation can be done offline. The
simulations show that the method is unbiased for all practical purposes for any
sampling design, including random sampling, and that the computational cost is
comparable (actually faster for moderate and large data sets) to the simple,
often severely biased, Euler-Maruyama approximation.","['Lars Josef Höök', 'Erik Lindström']","['stat.CO', 'q-fin.ST', 'stat.ML', '65C20, 65C30, 65C60, 68U20']",2015-09-25 15:17:37+00:00
http://arxiv.org/abs/1509.07636v2,Validity of time reversal for testing Granger causality,"Inferring causal interactions from observed data is a challenging problem,
especially in the presence of measurement noise. To alleviate the problem of
spurious causality, Haufe et al. (2013) proposed to contrast measures of
information flow obtained on the original data against the same measures
obtained on time-reversed data. They show that this procedure, time-reversed
Granger causality (TRGC), robustly rejects causal interpretations on mixtures
of independent signals. While promising results have been achieved in
simulations, it was so far unknown whether time reversal leads to valid
measures of information flow in the presence of true interaction. Here we prove
that, for linear finite-order autoregressive processes with unidirectional
information flow, the application of time reversal for testing Granger
causality indeed leads to correct estimates of information flow and its
directionality. Using simulations, we further show that TRGC is able to infer
correct directionality with similar statistical power as the net Granger
causality between two variables, while being much more robust to the presence
of measurement noise.","['Irene Winkler', 'Danny Panknin', 'Daniel Bartz', 'Klaus-Robert Müller', 'Stefan Haufe']","['math.ST', 'stat.ML', 'stat.TH']",2015-09-25 08:58:24+00:00
http://arxiv.org/abs/1509.07553v2,Linear-time Learning on Distributions with Approximate Kernel Embeddings,"Many interesting machine learning problems are best posed by considering
instances that are distributions, or sample sets drawn from distributions.
Previous work devoted to machine learning tasks with distributional inputs has
done so through pairwise kernel evaluations between pdfs (or sample sets).
While such an approach is fine for smaller datasets, the computation of an $N
\times N$ Gram matrix is prohibitive in large datasets. Recent scalable
estimators that work over pdfs have done so only with kernels that use
Euclidean metrics, like the $L_2$ distance. However, there are a myriad of
other useful metrics available, such as total variation, Hellinger distance,
and the Jensen-Shannon divergence. This work develops the first random features
for pdfs whose dot product approximates kernels using these non-Euclidean
metrics, allowing estimators using such kernels to scale to large datasets by
working in a primal space, without computing large Gram matrices. We provide an
analysis of the approximation error in using our proposed random features and
show empirically the quality of our approximation both in estimating a Gram
matrix and in solving learning tasks in real-world and synthetic data.","['Danica J. Sutherland', 'Junier B. Oliva', 'Barnabás Póczos', 'Jeff Schneider']","['stat.ML', 'cs.LG']",2015-09-24 22:26:02+00:00
http://arxiv.org/abs/1509.07497v2,High Dimensional Data Modeling Techniques for Detection of Chemical Plumes and Anomalies in Hyperspectral Images and Movies,"We briefly review recent progress in techniques for modeling and analyzing
hyperspectral images and movies, in particular for detecting plumes of both
known and unknown chemicals. For detecting chemicals of known spectrum, we
extend the technique of using a single subspace for modeling the background to
a ""mixture of subspaces"" model to tackle more complicated background.
Furthermore, we use partial least squares regression on a resampled training
set to boost performance. For the detection of unknown chemicals we view the
problem as an anomaly detection problem, and use novel estimators with
low-sampled complexity for intrinsically low-dimensional data in
high-dimensions that enable us to model the ""normal"" spectra and detect
anomalies. We apply these algorithms to benchmark data sets made available by
the Automated Target Detection program co-funded by NSF, DTRA and NGA, and
compare, when applicable, to current state-of-the-art algorithms, with
favorable results.","['Yi', 'Wang', 'Guangliang Chen', 'Mauro Maggioni']",['stat.ML'],2015-09-24 19:59:46+00:00
http://arxiv.org/abs/1509.07469v2,Channel Vector Subspace Estimation from Low-Dimensional Projections,"Massive MIMO is a variant of multiuser MIMO where the number of base-station
antennas $M$ is very large (typically 100), and generally much larger than the
number of spatially multiplexed data streams (typically 10). Unfortunately, the
front-end A/D conversion necessary to drive hundreds of antennas, with a signal
bandwidth of the order of 10 to 100 MHz, requires very large sampling bit-rate
and power consumption.
  In order to reduce such implementation requirements, Hybrid Digital-Analog
architectures have been proposed. In particular, our work in this paper is
motivated by one of such schemes named Joint Spatial Division and Multiplexing
(JSDM), where the downlink precoder (resp., uplink linear receiver) is split
into the product of a baseband linear projection (digital) and an RF
reconfigurable beamforming network (analog), such that only a reduced number $m
\ll M$ of A/D converters and RF modulation/demodulation chains is needed. In
JSDM, users are grouped according to the similarity of their channel dominant
subspaces, and these groups are separated by the analog beamforming stage,
where the multiplexing gain in each group is achieved using the digital
precoder. Therefore, it is apparent that extracting the channel subspace
information of the $M$-dim channel vectors from snapshots of $m$-dim
projections, with $m \ll M$, plays a fundamental role in JSDM implementation.
  In this paper, we develop novel efficient algorithms that require sampling
only $m = O(2\sqrt{M})$ specific array elements according to a coprime sampling
scheme, and for a given $p \ll M$, return a $p$-dim beamformer that has a
performance comparable with the best p-dim beamformer that can be designed from
the full knowledge of the exact channel covariance matrix. We assess the
performance of our proposed estimators both analytically and empirically via
numerical simulations.","['Saeid Haghighatshoar', 'Giuseppe Caire']","['cs.IT', 'math.IT', 'stat.ML']",2015-09-24 18:39:57+00:00
http://arxiv.org/abs/1509.07577v1,A Review of Feature Selection Methods Based on Mutual Information,"In this work we present a review of the state of the art of information
theoretic feature selection methods. The concepts of feature relevance,
redundance and complementarity (synergy) are clearly defined, as well as Markov
blanket. The problem of optimal feature selection is defined. A unifying
theoretical framework is described, which can retrofit successful heuristic
criteria, indicating the approximations made by each method. A number of open
problems in the field are presented.","['Jorge R. Vergara', 'Pablo A. Estévez']","['cs.LG', 'stat.ML']",2015-09-24 16:36:10+00:00
http://arxiv.org/abs/1509.07385v3,Provable approximation properties for deep neural networks,"We discuss approximation of functions using deep neural nets. Given a
function $f$ on a $d$-dimensional manifold $\Gamma \subset \mathbb{R}^m$, we
construct a sparsely-connected depth-4 neural network and bound its error in
approximating $f$. The size of the network depends on dimension and curvature
of the manifold $\Gamma$, the complexity of $f$, in terms of its wavelet
description, and only weakly on the ambient dimension $m$. Essentially, our
network computes wavelet functions, which are computed from Rectified Linear
Units (ReLU)","['Uri Shaham', 'Alexander Cloninger', 'Ronald R. Coifman']","['stat.ML', 'cs.LG', 'cs.NE']",2015-09-24 14:20:29+00:00
http://arxiv.org/abs/1509.07344v1,Opinion mining from twitter data using evolutionary multinomial mixture models,"Image of an entity can be defined as a structured and dynamic representation
which can be extracted from the opinions of a group of users or population.
Automatic extraction of such an image has certain importance in political
science and sociology related studies, e.g., when an extended inquiry from
large-scale data is required. We study the images of two politically
significant entities of France. These images are constructed by analyzing the
opinions collected from a well known social media called Twitter. Our goal is
to build a system which can be used to automatically extract the image of
entities over time.
  In this paper, we propose a novel evolutionary clustering method based on the
parametric link among Multinomial mixture models. First we propose the
formulation of a generalized model that establishes parametric links among the
Multinomial distributions. Afterward, we follow a model-based clustering
approach to explore different parametric sub-models and select the best model.
For the experiments, first we use synthetic temporal data. Next, we apply the
method to analyze the annotated social media data. Results show that the
proposed method is better than the state-of-the-art based on the common
evaluation metrics. Additionally, our method can provide interpretation about
the temporal evolution of the clusters.","['Md. Abul Hasnat', 'Julien Velcin', 'Stéphane Bonnevay', 'Julien Jacques']","['cs.IR', 'stat.ML']",2015-09-24 12:40:12+00:00
http://arxiv.org/abs/1509.07179v1,IllinoisSL: A JAVA Library for Structured Prediction,"IllinoisSL is a Java library for learning structured prediction models. It
supports structured Support Vector Machines and structured Perceptron. The
library consists of a core learning module and several applications, which can
be executed from command-lines. Documentation is provided to guide users. In
Comparison to other structured learning libraries, IllinoisSL is efficient,
general, and easy to use.","['Kai-Wei Chang', 'Shyam Upadhyay', 'Ming-Wei Chang', 'Vivek Srikumar', 'Dan Roth']","['cs.LG', 'cs.CL', 'stat.ML']",2015-09-23 23:22:38+00:00
http://arxiv.org/abs/1509.07093v1,A review of learning vector quantization classifiers,"In this work we present a review of the state of the art of Learning Vector
Quantization (LVQ) classifiers. A taxonomy is proposed which integrates the
most relevant LVQ approaches to date. The main concepts associated with modern
LVQ approaches are defined. A comparison is made among eleven LVQ classifiers
using one real-world and two artificial datasets.","['David Nova', 'Pablo A. Estevez']","['cs.LG', 'astro-ph.IM', 'cs.NE', 'stat.ML']",2015-09-23 18:46:31+00:00
http://arxiv.org/abs/1509.07087v1,Deep Temporal Sigmoid Belief Networks for Sequence Modeling,"Deep dynamic generative models are developed to learn sequential dependencies
in time-series data. The multi-layered model is designed by constructing a
hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential
stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden
state, inherited from the previous SBNs in the sequence, and is used to
regulate its hidden bias. Scalable learning and inference algorithms are
derived by introducing a recognition model that yields fast sampling from the
variational posterior. This recognition model is trained jointly with the
generative model, by maximizing its variational lower bound on the
log-likelihood. Experimental results on bouncing balls, polyphonic music,
motion capture, and text streams show that the proposed approach achieves
state-of-the-art predictive performance, and has the capacity to synthesize
various sequences.","['Zhe Gan', 'Chunyuan Li', 'Ricardo Henao', 'David Carlson', 'Lawrence Carin']","['stat.ML', 'cs.LG']",2015-09-23 18:36:42+00:00
http://arxiv.org/abs/1509.07078v2,Detecting phase transitions in collective behavior using manifold's curvature,"If a given behavior of a multi-agent system restricts the phase variable to a
invariant manifold, then we define a phase transition as change of physical
characteristics such as speed, coordination, and structure. We define such a
phase transition as splitting an underlying manifold into two sub-manifolds
with distinct dimensionalities around the singularity where the phase
transition physically exists. Here, we propose a method of detecting phase
transitions and splitting the manifold into phase transitions free
sub-manifolds. Therein, we utilize a relationship between curvature and
singular value ratio of points sampled in a curve, and then extend the
assertion into higher-dimensions using the shape operator. Then we attest that
the same phase transition can also be approximated by singular value ratios
computed locally over the data in a neighborhood on the manifold. We validate
the phase transitions detection method using one particle simulation and three
real world examples.","['Kelum Gajamannage', 'Erik M. Bollt']","['math.DS', 'cs.LG', 'cs.MA', 'math.GT', 'stat.ML', '57M60 (Primary), 37E99, 57N35 (Secondary)']",2015-09-23 18:04:56+00:00
http://arxiv.org/abs/1509.06957v2,Fast k-NN search,"Efficient index structures for fast approximate nearest neighbor queries are
required in many applications such as recommendation systems. In
high-dimensional spaces, many conventional methods suffer from excessive usage
of memory and slow response times. We propose a method where multiple random
projection trees are combined by a novel voting scheme. The key idea is to
exploit the redundancy in a large number of candidate sets obtained by
independently generated random projections in order to reduce the number of
expensive exact distance evaluations. The method is straightforward to
implement using sparse projections which leads to a reduced memory footprint
and fast index construction. Furthermore, it enables grouping of the required
computations into big matrix multiplications, which leads to additional savings
due to cache effects and low-level parallelization. We demonstrate by extensive
experiments on a wide variety of data sets that the method is faster than
existing partitioning tree or hashing based approaches, making it the fastest
available technique on high accuracy levels.","['Ville Hyvönen', 'Teemu Pitkänen', 'Sotiris Tasoulis', 'Elias Jääsaari', 'Risto Tuomainen', 'Liang Wang', 'Jukka Corander', 'Teemu Roos']","['stat.ML', 'cs.DS', 'cs.LG']",2015-09-23 13:10:36+00:00
http://arxiv.org/abs/1509.06920v1,Predicting Climate Variability over the Indian Region Using Data Mining Strategies,"In this paper an approach based on expectation maximization (EM) clustering
to find the climate regions and a support vector machine to build a predictive
model for each of these regions is proposed. To minimize the biases in the
estimations a ten cross fold validation is adopted both for obtaining clusters
and building the predictive models. The EM clustering could identify all the
zones as per the Koppen classification over Indian region. The proposed
strategy when employed for predicting temperature has resulted in an RMSE of
$1.19$ in the Montane climate region and $0.89$ in the Humid Sub Tropical
region as compared to $2.9$ and $0.95$ respectively predicted using k-means and
linear regression method.",['Naresh Kumar Mallenahalli'],"['stat.ML', 'physics.ao-ph']",2015-09-23 11:04:42+00:00
http://arxiv.org/abs/1509.06893v1,Efficient reconstruction of transmission probabilities in a spreading process from partial observations,"An important problem of reconstruction of diffusion network and transmission
probabilities from the data has attracted a considerable attention in the past
several years. A number of recent papers introduced efficient algorithms for
the estimation of spreading parameters, based on the maximization of the
likelihood of observed cascades, assuming that the full information for all the
nodes in the network is available. In this work, we focus on a more realistic
and restricted scenario, in which only a partial information on the cascades is
available: either the set of activation times for a limited number of nodes, or
the states of nodes for a subset of observation times. To tackle this problem,
we first introduce a framework based on the maximization of the likelihood of
the incomplete diffusion trace. However, we argue that the computation of this
incomplete likelihood is a computationally hard problem, and show that a fast
and robust reconstruction of transmission probabilities in sparse networks can
be achieved with a new algorithm based on recently introduced dynamic
message-passing equations for the spreading processes. The suggested approach
can be easily generalized to a large class of discrete and continuous dynamic
models, as well as to the cases of dynamically-changing networks and noisy
information.","['Andrey Y. Lokhov', 'Theodor Misiakiewicz']","['physics.soc-ph', 'cond-mat.stat-mech', 'cs.LG', 'cs.SI', 'stat.ML']",2015-09-23 09:14:34+00:00
http://arxiv.org/abs/1509.06849v1,Minimum Weight Perfect Matching via Blossom Belief Propagation,"Max-product Belief Propagation (BP) is a popular message-passing algorithm
for computing a Maximum-A-Posteriori (MAP) assignment over a distribution
represented by a Graphical Model (GM). It has been shown that BP can solve a
number of combinatorial optimization problems including minimum weight
matching, shortest path, network flow and vertex cover under the following
common assumption: the respective Linear Programming (LP) relaxation is tight,
i.e., no integrality gap is present. However, when LP shows an integrality gap,
no model has been known which can be solved systematically via sequential
applications of BP. In this paper, we develop the first such algorithm, coined
Blossom-BP, for solving the minimum weight matching problem over arbitrary
graphs. Each step of the sequential algorithm requires applying BP over a
modified graph constructed by contractions and expansions of blossoms, i.e.,
odd sets of vertices. Our scheme guarantees termination in O(n^2) of BP runs,
where n is the number of vertices in the original graph. In essence, the
Blossom-BP offers a distributed version of the celebrated Edmonds' Blossom
algorithm by jumping at once over many sub-steps with a single BP. Moreover,
our result provides an interpretation of the Edmonds' algorithm as a sequence
of LPs.","['Sungsoo Ahn', 'Sejun Park', 'Michael Chertkov', 'Jinwoo Shin']","['cs.DS', 'cs.AI', 'stat.ML']",2015-09-23 05:49:53+00:00
http://arxiv.org/abs/1509.06831v1,Density Estimation via Discrepancy,"Given i.i.d samples from some unknown continuous density on hyper-rectangle
$[0, 1]^d$, we attempt to learn a piecewise constant function that approximates
this underlying density non-parametrically. Our density estimate is defined on
a binary split of $[0, 1]^d$ and built up sequentially according to discrepancy
criteria; the key ingredient is to control the discrepancy adaptively in each
sub-rectangle to achieve overall bound. We prove that the estimate, even though
simple as it appears, preserves most of the estimation power. By exploiting its
structure, it can be directly applied to some important pattern recognition
tasks such as mode seeking and density landscape exploration. We demonstrate
its applicability through simulations and examples.","['Kun Yang', 'Hao Su', 'Wing Hung Wang']",['stat.ML'],2015-09-23 03:20:28+00:00
http://arxiv.org/abs/1509.06807v1,Bandit Label Inference for Weakly Supervised Learning,"The scarcity of data annotated at the desired level of granularity is a
recurring issue in many applications. Significant amounts of effort have been
devoted to developing weakly supervised methods tailored to each individual
setting, which are often carefully designed to take advantage of the particular
properties of weak supervision regimes, form of available data and prior
knowledge of the task at hand. Unfortunately, it is difficult to adapt these
methods to new tasks and/or forms of data, which often require different weak
supervision regimes or models. We present a general-purpose method that can
solve any weakly supervised learning problem irrespective of the weak
supervision regime or the model. The proposed method turns any off-the-shelf
strongly supervised classifier into a weakly supervised classifier and allows
the user to specify any arbitrary weakly supervision regime via a loss
function. We apply the method to several different weak supervision regimes and
demonstrate competitive results compared to methods specifically engineered for
those settings.","['Ke Li', 'Jitendra Malik']","['cs.LG', 'stat.ML']",2015-09-22 23:13:12+00:00
http://arxiv.org/abs/1509.06673v1,Classification error in multiclass discrimination from Markov data,"As a model for an on-line classification setting we consider a stochastic
process $(X_{-n},Y_{-n})_{n}$, the present time-point being denoted by 0, with
observables $ \ldots,X_{-n},X_{-n+1},\ldots, X_{-1}, X_0$ from which the
pattern $Y_0$ is to be inferred. So in this classification setting, in addition
to the present observation $X_0$ a number $l$ of preceding observations may be
used for classification, thus taking a possible dependence structure into
account as it occurs e.g. in an ongoing classification of handwritten
characters. We treat the question how the performance of classifiers is
improved by using such additional information. For our analysis, a hidden
Markov model is used. Letting $R_l$ denote the minimal risk of
misclassification using $l$ preceding observations we show that the difference
$\sup_k |R_l - R_{l+k}|$ decreases exponentially fast as $l$ increases. This
suggests that a small $l$ might already lead to a noticeable improvement. To
follow this point we look at the use of past observations for kernel
classification rules. Our practical findings in simulated hidden Markov models
and in the classification of handwritten characters indicate that using $l=1$,
i.e. just the last preceding observation in addition to $X_0$, can lead to a
substantial reduction of the risk of misclassification. So, in the presence of
stochastic dependencies, we advocate to use $ X_{-1},X_0$ for finding the
pattern $Y_0$ instead of only $X_0$ as one would in the independent situation.","['Sören Christensen', 'Albrecht Irle', 'Lars Willert']","['stat.ML', 'math.ST', 'stat.TH']",2015-09-22 16:32:30+00:00
http://arxiv.org/abs/1509.06492v1,Modifying iterated Laplace approximations,"In this paper, several modifications are introduced to the functional
approximation method iterLap to reduce the approximation error, including
stopping rule adjustment, proposal of new residual function, starting point
selection for numerical optimisation, scaling of Hessian matrix. Illustrative
examples are also provided to show the trade-off between running time and
accuracy of the original and modified methods.","['Tiep Mai', 'Simon Wilson']","['stat.ME', 'stat.ML']",2015-09-22 08:09:21+00:00
http://arxiv.org/abs/1509.06459v1,Stochastic gradient descent methods for estimation with large data sets,"We develop methods for parameter estimation in settings with large-scale data
sets, where traditional methods are no longer tenable. Our methods rely on
stochastic approximations, which are computationally efficient as they maintain
one iterate as a parameter estimate, and successively update that iterate based
on a single data point. When the update is based on a noisy gradient, the
stochastic approximation is known as standard stochastic gradient descent,
which has been fundamental in modern applications with large data sets.
Additionally, our methods are numerically stable because they employ implicit
updates of the iterates. Intuitively, an implicit update is a shrinked version
of a standard one, where the shrinkage factor depends on the observed Fisher
information at the corresponding data point. This shrinkage prevents numerical
divergence of the iterates, which can be caused either by excess noise or
outliers. Our sgd package in R offers the most extensive and robust
implementation of stochastic gradient descent methods. We demonstrate that sgd
dominates alternative software in runtime for several estimation problems with
massive data sets. Our applications include the wide class of generalized
linear models as well as M-estimation for robust regression.","['Dustin Tran', 'Panos Toulis', 'Edoardo M. Airoldi']","['stat.CO', 'stat.ME', 'stat.ML']",2015-09-22 04:25:54+00:00
http://arxiv.org/abs/1509.06457v2,Identifying collusion groups using spectral clustering,"In an illiquid stock, traders can collude and place orders on a predetermined
price and quantity at a fixed schedule. This is usually done to manipulate the
price of the stock or to create artificial liquidity in the stock, which may
mislead genuine investors. Here, the problem is to identify such group of
colluding traders. We modeled the problem instance as a graph, where each
trader corresponds to a vertex of the graph and trade corresponds to edges of
the graph. Further, we assign weights on edges depending on total volume, total
number of trades, maximum change in the price and commonality between two
vertices. Spectral clustering algorithms are used on the constructed graph to
identify colluding group(s). We have compared our results with simulated data
to show the effectiveness of spectral clustering to detecting colluding groups.
Moreover, we also have used parameters of real data to test the effectiveness
of our algorithm.","['Suneel Sarswat', 'Kandathil Mathew Abraham', 'Subir Kumar Ghosh']","['q-fin.TR', 'cs.CE', 'stat.ML']",2015-09-22 04:03:18+00:00
http://arxiv.org/abs/1509.06449v1,Efficient Neighborhood Selection for Gaussian Graphical Models,"This paper addresses the problem of neighborhood selection for Gaussian
graphical models. We present two heuristic algorithms: a forward-backward
greedy algorithm for general Gaussian graphical models based on mutual
information test, and a threshold-based algorithm for walk summable Gaussian
graphical models. Both algorithms are shown to be structurally consistent, and
efficient. Numerical results show that both algorithms work very well.","['Yingxiang Yang', 'Jalal Etesami', 'Negar Kiyavash']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2015-09-22 02:51:30+00:00
http://arxiv.org/abs/1509.06290v1,A Bayesian Compressed Sensing Kalman Filter for Direction of Arrival Estimation,"In this paper, we look to address the problem of estimating the dynamic
direction of arrival (DOA) of a narrowband signal impinging on a sensor array
from the far field. The initial estimate is made using a Bayesian compressive
sensing (BCS) framework and then tracked using a Bayesian compressed sensing
Kalman filter (BCSKF). The BCS framework splits the angular region into N
potential DOAs and enforces a belief that only a few of the DOAs will have a
non-zero valued signal present. A BCSKF can then be used to track the change in
the DOA using the same framework. There can be an issue when the DOA approaches
the endfire of the array. In this angular region current methods can struggle
to accurately estimate and track changes in the DOAs. To tackle this problem,
we propose changing the traditional sparse belief associated with BCS to a
belief that the estimated signals will match the predicted signals given a
known DOA change. This is done by modelling the difference between the expected
sparse received signals and the estimated sparse received signals as a Gaussian
distribution. Example test scenarios are provided and comparisons made with the
traditional BCS based estimation method. They show that an improvement in
estimation accuracy is possible without a significant increase in computational
complexity.","['Matthew Hawes', 'Lyudmila Mihaylova', 'Francois Septier', 'Simon Godsill']","['stat.ML', 'cs.IT', 'math.IT']",2015-09-21 16:29:16+00:00
http://arxiv.org/abs/1509.06088v1,"Significance Analysis of High-Dimensional, Low-Sample Size Partially Labeled Data","Classification and clustering are both important topics in statistical
learning. A natural question herein is whether predefined classes are really
different from one another, or whether clusters are really there. Specifically,
we may be interested in knowing whether the two classes defined by some class
labels (when they are provided), or the two clusters tagged by a clustering
algorithm (where class labels are not provided), are from the same underlying
distribution. Although both are challenging questions for the high-dimensional,
low-sample size data, there has been some recent development for both. However,
when it is costly to manually place labels on observations, it is often that
only a small portion of the class labels is available. In this article, we
propose a significance analysis approach for such type of data, namely
partially labeled data. Our method makes use of the whole data and tries to
test the class difference as if all the labels were observed. Compared to a
testing method that ignores the label information, our method provides a
greater power, meanwhile, maintaining the size, illustrated by a comprehensive
simulation study. Theoretical properties of the proposed method are studied
with emphasis on the high-dimensional, low-sample size setting. Our simulated
examples help to understand when and how the information extracted from the
labeled data can be effective. A real data example further illustrates the
usefulness of the proposed method.","['Qiyi Lu', 'Xingye Qiao']","['stat.ML', 'cs.LG', 'stat.ME']",2015-09-21 01:23:45+00:00
http://arxiv.org/abs/1509.06061v1,A Statistical Theory of Deep Learning via Proximal Splitting,"In this paper we develop a statistical theory and an implementation of deep
learning models. We show that an elegant variable splitting scheme for the
alternating direction method of multipliers optimises a deep learning
objective. We allow for non-smooth non-convex regularisation penalties to
induce sparsity in parameter weights. We provide a link between traditional
shallow layer statistical models such as principal component and sliced inverse
regression and deep layer models. We also define the degrees of freedom of a
deep learning predictor and a predictive MSE criteria to perform model
selection for comparing architecture designs. We focus on deep multiclass
logistic learning although our methods apply more generally. Our results
suggest an interesting and previously under-exploited relationship between deep
learning and proximal splitting techniques. To illustrate our methodology, we
provide a multi-class logit classification analysis of Fisher's Iris data where
we illustrate the convergence of our algorithm. Finally, we conclude with
directions for future research.","['Nicholas G. Polson', 'Brandon T. Willard', 'Massoud Heidari']",['stat.ML'],2015-09-20 21:39:47+00:00
http://arxiv.org/abs/1509.05962v2,Telugu OCR Framework using Deep Learning,"In this paper, we address the task of Optical Character Recognition(OCR) for
the Telugu script. We present an end-to-end framework that segments the text
image, classifies the characters and extracts lines using a language model. The
segmentation is based on mathematical morphology. The classification module,
which is the most challenging task of the three, is a deep convolutional neural
network. The language is modelled as a third degree markov chain at the glyph
level. Telugu script is a complex alphasyllabary and the language is
agglutinative, making the problem hard. In this paper we apply the latest
advances in neural networks to achieve state-of-the-art error rates. We also
review convolutional neural networks in great detail and expound the
statistical justification behind the many tricks needed to make Deep Learning
work.","['Rakesh Achanta', 'Trevor Hastie']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.NE']",2015-09-20 03:35:05+00:00
http://arxiv.org/abs/1509.05808v1,"Word, graph and manifold embedding from Markov processes","Continuous vector representations of words and objects appear to carry
surprisingly rich semantic content. In this paper, we advance both the
conceptual and theoretical understanding of word embeddings in three ways.
First, we ground embeddings in semantic spaces studied in
cognitive-psychometric literature and introduce new evaluation tasks. Second,
in contrast to prior work, we take metric recovery as the key object of study,
unify existing algorithms as consistent metric recovery methods based on
co-occurrence counts from simple Markov random walks, and propose a new
recovery algorithm. Third, we generalize metric recovery to graphs and
manifolds, relating co-occurence counts on random walks in graphs and random
processes on manifolds to the underlying metric to be recovered, thereby
reconciling manifold estimation and embedding algorithms. We compare embedding
algorithms across a range of tasks, from nonlinear dimensionality reduction to
three semantic language tasks, including analogies, sequence completion, and
classification.","['Tatsunori B. Hashimoto', 'David Alvarez-Melis', 'Tommi S. Jaakkola']","['cs.CL', 'cs.LG', 'stat.ML']",2015-09-18 21:50:38+00:00
http://arxiv.org/abs/1509.05789v3,BLC: Private Matrix Factorization Recommenders via Automatic Group Learning,"We propose a privacy-enhanced matrix factorization recommender that exploits
the fact that users can often be grouped together by interest. This allows a
form of ""hiding in the crowd"" privacy. We introduce a novel matrix
factorization approach suited to making recommendations in a shared group (or
nym) setting and the BLC algorithm for carrying out this matrix factorization
in a privacy-enhanced manner. We demonstrate that the increased privacy does
not come at the cost of reduced recommendation accuracy.","['Alessandro Checco', 'Giuseppe Bianchi', 'Doug Leith']","['cs.LG', 'stat.ML']",2015-09-18 20:21:43+00:00
http://arxiv.org/abs/1509.05760v3,Accelerating Optimization via Adaptive Prediction,"We present a powerful general framework for designing data-dependent
optimization algorithms, building upon and unifying recent techniques in
adaptive regularization, optimistic gradient predictions, and problem-dependent
randomization. We first present a series of new regret guarantees that hold at
any time and under very minimal assumptions, and then show how different
relaxations recover existing algorithms, both basic as well as more recent
sophisticated ones. Finally, we show how combining adaptivity, optimism, and
problem-dependent randomization can guide the design of algorithms that benefit
from more favorable guarantees than recent state-of-the-art methods.","['Mehryar Mohri', 'Scott Yang']","['stat.ML', 'cs.LG']",2015-09-18 19:49:54+00:00
http://arxiv.org/abs/1509.05753v1,Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses,"We show that discrete synaptic weights can be efficiently used for learning
in large scale neural systems, and lead to unanticipated computational
performance. We focus on the representative case of learning random patterns
with binary synapses in single layer networks. The standard statistical
analysis shows that this problem is exponentially dominated by isolated
solutions that are extremely hard to find algorithmically. Here, we introduce a
novel method that allows us to find analytical evidence for the existence of
subdominant and extremely dense regions of solutions. Numerical experiments
confirm these findings. We also show that the dense regions are surprisingly
accessible by simple learning protocols, and that these synaptic configurations
are robust to perturbations and generalize better than typical solutions. These
outcomes extend to synapses with multiple states and to deeper neural
architectures. The large deviation measure also suggests how to design novel
algorithmic schemes for optimization based on local entropy maximization.","['Carlo Baldassi', 'Alessandro Ingrosso', 'Carlo Lucibello', 'Luca Saglietti', 'Riccardo Zecchina']","['cond-mat.dis-nn', 'q-bio.NC', 'stat.ML']",2015-09-18 19:12:55+00:00
http://arxiv.org/abs/1509.05742v1,Evaluation of Protein-protein Interaction Predictors with Noisy Partially Labeled Data Sets,"Protein-protein interaction (PPI) prediction is an important problem in
machine learning and computational biology. However, there is no data set for
training or evaluation purposes, where all the instances are accurately
labeled. Instead, what is available are instances of positive class (with
possibly noisy labels) and no instances of negative class. The non-availability
of negative class data is typically handled with the observation that randomly
chosen protein-pairs have a nearly 100% chance of being negative class, as only
1 in 1,500 protein pairs expected is expected to be an interacting pair. In
this paper, we focused on the problem that non-availability of accurately
labeled testing data sets in the domain of protein-protein interaction (PPI)
prediction may lead to biased evaluation results. We first showed that not
acknowledging the inherent skew in the interactome (i.e. rare occurrence of
positive instances) leads to an over-estimated accuracy of the predictor. Then
we show that, with the belief that positive interactions are a rare category,
sampling random pairs of proteins excluding known interacting proteins set as
the negative testing data set could lead to an under-estimated evaluation
result. We formalized those two problems to validate the above claim, and based
on the formalization, we proposed a balancing method to cancel out the
over-estimation with under-estimation. Finally, our experiments validated the
theoretical aspects and showed that this balancing evaluation could evaluate
the exact performance without availability of golden standard data sets.","['Haohan Wang', 'Madhavi K. Ganapathiraju']","['cs.AI', 'stat.ML']",2015-09-18 18:45:49+00:00
http://arxiv.org/abs/1509.05722v1,Energy saving in smart homes based on consumer behaviour: A case study,"This paper presents a case study of a recommender system that can be used to
save energy in smart homes without lowering the comfort of the inhabitants. We
present an algorithm that uses consumer behavior data only and uses machine
learning to suggest actions for inhabitants to reduce the energy consumption of
their homes. The system mines for frequent and periodic patterns in the event
data provided by the Digitalstrom home automation system. These patterns are
converted into association rules, prioritized and compared with the current
behavior of the inhabitants. If the system detects an opportunities to save
energy without decreasing the comfort level it sends a recommendation to the
residents.","['Michael Zehnder', 'Holger Wache', 'Hans-Friedrich Witschel', 'Danilo Zanatta', 'Miguel Rodriguez']","['stat.ML', 'cs.AI', 'cs.MA', 'cs.SY']",2015-09-18 17:31:25+00:00
http://arxiv.org/abs/1509.05438v1,Sparse Fisher's Linear Discriminant Analysis for Partially Labeled Data,"Classification is an important tool with many useful applications. Among the
many classification methods, Fisher's Linear Discriminant Analysis (LDA) is a
traditional model-based approach which makes use of the covariance information.
However, in the high-dimensional, low-sample size setting, LDA cannot be
directly deployed because the sample covariance is not invertible. While there
are modern methods designed to deal with high-dimensional data, they may not
fully use the covariance information as LDA does. Hence in some situations, it
is still desirable to use a model-based method such as LDA for classification.
This article exploits the potential of LDA in more complicated data settings.
In many real applications, it is costly to manually place labels on
observations; hence it is often that only a small portion of labeled data is
available while a large number of observations are left without a label. It is
a great challenge to obtain good classification performance through the labeled
data alone, especially when the dimension is greater than the size of the
labeled data. In order to overcome this issue, we propose a semi-supervised
sparse LDA classifier to take advantage of the seemingly useless unlabeled
data. They provide additional information which helps to boost the
classification performance in some situations. A direct estimation method is
used to reconstruct LDA and achieve the sparsity; meanwhile we employ the
difference-convex algorithm to handle the non-convex loss function associated
with the unlabeled data. Theoretical properties of the proposed classifier are
studied. Our simulated examples help to understand when and how the information
extracted from the unlabeled data can be useful. A real data example further
illustrates the usefulness of the proposed method.","['Qiyi Lu', 'Xingye Qiao']","['stat.ML', 'stat.ME', 'Primary 62H30, Secondary 68T10']",2015-09-17 20:42:42+00:00
http://arxiv.org/abs/1509.05285v1,Decadal climate predictions using sequential learning algorithms,"Ensembles of climate models are commonly used to improve climate predictions
and assess the uncertainties associated with them. Weighting the models
according to their performances holds the promise of further improving their
predictions. Here, we use an ensemble of decadal climate predictions to
demonstrate the ability of sequential learning algorithms (SLAs) to reduce the
forecast errors and reduce the uncertainties. Three different SLAs are
considered, and their performances are compared with those of an equally
weighted ensemble, a linear regression and the climatology. Predictions of four
different variables--the surface temperature, the zonal and meridional wind,
and pressure--are considered. The spatial distributions of the performances are
presented, and the statistical significance of the improvements achieved by the
SLAs is tested. Based on the performances of the SLAs, we propose one to be
highly suitable for the improvement of decadal climate predictions.","['Ehud Strobach', 'Golan Bel']","['physics.ao-ph', 'physics.data-an', 'stat.ML', '62C99']",2015-09-17 15:16:05+00:00
http://arxiv.org/abs/1509.05257v1,(Blue) Taxi Destination and Trip Time Prediction from Partial Trajectories,"Real-time estimation of destination and travel time for taxis is of great
importance for existing electronic dispatch systems. We present an approach
based on trip matching and ensemble learning, in which we leverage the patterns
observed in a dataset of roughly 1.7 million taxi journeys to predict the
corresponding final destination and travel time for ongoing taxi trips, as a
solution for the ECML/PKDD Discovery Challenge 2015 competition. The results of
our empirical evaluation show that our approach is effective and very robust,
which led our team -- BlueTaxi -- to the 3rd and 7th position of the final
rankings for the trip time and destination prediction tasks, respectively.
Given the fact that the final rankings were computed using a very small test
set (with only 320 trips) we believe that our approach is one of the most
robust solutions for the challenge based on the consistency of our good results
across the test sets.","['Hoang Thanh Lam', 'Ernesto Diaz-Aviles', 'Alessandra Pascale', 'Yiannis Gkoufas', 'Bei Chen']","['stat.ML', 'cs.AI', 'cs.CY', 'cs.LG', 'I.2.6; I.5.2']",2015-09-17 13:51:55+00:00
http://arxiv.org/abs/1509.05172v2,Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis,"We consider the off-policy evaluation problem in Markov decision processes
with function approximation. We propose a generalization of the recently
introduced \emph{emphatic temporal differences} (ETD) algorithm
\citep{SuttonMW15}, which encompasses the original ETD($\lambda$), as well as
several other off-policy evaluation algorithms as special cases. We call this
framework \ETD, where our introduced parameter $\beta$ controls the decay rate
of an importance-sampling term. We study conditions under which the projected
fixed-point equation underlying \ETD\ involves a contraction operator, allowing
us to present the first asymptotic error bounds (bias) for \ETD. Our results
show that the original ETD algorithm always involves a contraction operator,
and its bias is bounded. Moreover, by controlling $\beta$, our proposed
generalization allows trading-off bias for variance reduction, thereby
achieving a lower total error.","['Assaf Hallak', 'Aviv Tamar', 'Remi Munos', 'Shie Mannor']","['stat.ML', 'cs.LG']",2015-09-17 09:03:35+00:00
http://arxiv.org/abs/1509.05142v6,Fast Gaussian Process Regression for Big Data,"Gaussian Processes are widely used for regression tasks. A known limitation
in the application of Gaussian Processes to regression tasks is that the
computation of the solution requires performing a matrix inversion. The
solution also requires the storage of a large matrix in memory. These factors
restrict the application of Gaussian Process regression to small and moderate
size data sets. We present an algorithm that combines estimates from models
developed using subsets of the data obtained in a manner similar to the
bootstrap. The sample size is a critical parameter for this algorithm.
Guidelines for reasonable choices of algorithm parameters, based on detailed
experimental study, are provided. Various techniques have been proposed to
scale Gaussian Processes to large scale regression tasks. The most appropriate
choice depends on the problem context. The proposed method is most appropriate
for problems where an additive model works well and the response depends on a
small number of features. The minimax rate of convergence for such problems is
attractive and we can build effective models with a small subset of the data.
The Stochastic Variational Gaussian Process and the Sparse Gaussian Process are
also appropriate choices for such problems. These methods pick a subset of data
based on theoretical considerations. The proposed algorithm uses bagging and
random sampling. Results from experiments conducted as part of this study
indicate that the algorithm presented in this work can be as effective as these
methods. Model stacking can be used to combine the model developed with the
proposed method with models from other methods for large scale regression such
as Gradient Boosted Trees. This can yield performance gains.","['Sourish Das', 'Sasanka Roy', 'Rajiv Sambasivan']","['cs.LG', 'stat.ML']",2015-09-17 06:18:08+00:00
http://arxiv.org/abs/1509.05113v2,Revealed Preference at Scale: Learning Personalized Preferences from Assortment Choices,"We consider the problem of learning the preferences of a heterogeneous
population by observing choices from an assortment of products, ads, or other
offerings. Our observation model takes a form common in assortment planning
applications: each arriving customer is offered an assortment consisting of a
subset of all possible offerings; we observe only the assortment and the
customer's single choice.
  In this paper we propose a mixture choice model with a natural underlying
low-dimensional structure, and show how to estimate its parameters. In our
model, the preferences of each customer or segment follow a separate parametric
choice model, but the underlying structure of these parameters over all the
models has low dimension. We show that a nuclear-norm regularized maximum
likelihood estimator can learn the preferences of all customers using a number
of observations much smaller than the number of item-customer combinations.
This result shows the potential for structural assumptions to speed up learning
and improve revenues in assortment planning and customization. We provide a
specialized factored gradient descent algorithm and study the success of the
approach empirically.","['Nathan Kallus', 'Madeleine Udell']","['stat.ML', 'cs.LG', 'math.OC']",2015-09-17 03:37:38+00:00
http://arxiv.org/abs/1509.05111v2,Optimal Subsampling Approaches for Large Sample Linear Regression,"A significant hurdle for analyzing large sample data is the lack of effective
statistical computing and inference methods. An emerging powerful approach for
analyzing large sample data is subsampling, by which one takes a random
subsample from the original full sample and uses it as a surrogate for
subsequent computation and estimation. In this paper, we study subsampling
methods under two scenarios: approximating the full sample ordinary
least-square (OLS) estimator and estimating the coefficients in linear
regression. We present two algorithms, weighted estimation algorithm and
unweighted estimation algorithm, and analyze asymptotic behaviors of their
resulting subsample estimators under general conditions. For the weighted
estimation algorithm, we propose a criterion for selecting the optimal sampling
probability by making use of the asymptotic results. On the basis of the
criterion, we provide two novel subsampling methods, the optimal subsampling
and the predictor- length subsampling methods. The predictor-length subsampling
method is based on the L2 norm of predictors rather than leverage scores. Its
computational cost is scalable. For unweighted estimation algorithm, we show
that its resulting subsample estimator is not consistent to the full sample OLS
estimator. However, it has better performance than the weighted estimation
algorithm for estimating the coefficients. Simulation studies and a real data
example are used to demonstrate the effectiveness of our proposed subsampling
methods.","['Rong Zhu', 'Ping Ma', 'Michael W. Mahoney', 'Bin Yu']","['stat.ME', 'stat.ML']",2015-09-17 03:25:21+00:00
http://arxiv.org/abs/1509.05009v3,On the Expressive Power of Deep Learning: A Tensor Analysis,"It has long been conjectured that hypotheses spaces suitable for data that is
compositional in nature, such as text or images, may be more efficiently
represented with deep hierarchical networks than with shallow ones. Despite the
vast empirical evidence supporting this belief, theoretical justifications to
date are limited. In particular, they do not account for the locality, sharing
and pooling constructs of convolutional networks, the most successful deep
learning architecture to date. In this work we derive a deep network
architecture based on arithmetic circuits that inherently employs locality,
sharing and pooling. An equivalence between the networks and hierarchical
tensor factorizations is established. We show that a shallow network
corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to
Hierarchical Tucker decomposition. Using tools from measure theory and matrix
algebra, we prove that besides a negligible set, all functions that can be
implemented by a deep network of polynomial size, require exponential size in
order to be realized (or even approximated) by a shallow network. Since
log-space computation transforms our networks into SimNets, the result applies
directly to a deep learning architecture demonstrating promising empirical
performance. The construction and theory developed in this paper shed new light
on various practices and ideas employed by the deep learning community.","['Nadav Cohen', 'Or Sharir', 'Amnon Shashua']","['cs.NE', 'cs.LG', 'cs.NA', 'stat.ML']",2015-09-16 19:32:54+00:00
http://arxiv.org/abs/1509.04783v1,Group Membership Prediction,"The group membership prediction (GMP) problem involves predicting whether or
not a collection of instances share a certain semantic property. For instance,
in kinship verification given a collection of images, the goal is to predict
whether or not they share a {\it familial} relationship. In this context we
propose a novel probability model and introduce latent {\em view-specific} and
{\em view-shared} random variables to jointly account for the view-specific
appearance and cross-view similarities among data instances. Our model posits
that data from each view is independent conditioned on the shared variables.
This postulate leads to a parametric probability model that decomposes group
membership likelihood into a tensor product of data-independent parameters and
data-dependent factors. We propose learning the data-independent parameters in
a discriminative way with bilinear classifiers, and test our prediction
algorithm on challenging visual recognition tasks such as multi-camera person
re-identification and kinship verification. On most benchmark datasets, our
method can significantly outperform the current state-of-the-art.","['Ziming Zhang', 'Yuting Chen', 'Venkatesh Saligrama']","['cs.CV', 'stat.ML']",2015-09-16 01:22:40+00:00
http://arxiv.org/abs/1509.04781v1,Dirichlet Fragmentation Processes,"Tree structures are ubiquitous in data across many domains, and many datasets
are naturally modelled by unobserved tree structures. In this paper, first we
review the theory of random fragmentation processes [Bertoin, 2006], and a
number of existing methods for modelling trees, including the popular nested
Chinese restaurant process (nCRP). Then we define a general class of
probability distributions over trees: the Dirichlet fragmentation process (DFP)
through a novel combination of the theory of Dirichlet processes and random
fragmentation processes. This DFP presents a stick-breaking construction, and
relates to the nCRP in the same way the Dirichlet process relates to the
Chinese restaurant process. Furthermore, we develop a novel hierarchical
mixture model with the DFP, and empirically compare the new model to similar
models in machine learning. Experiments show the DFP mixture model to be
convincingly better than existing state-of-the-art approaches for hierarchical
clustering and density modelling.","['Hong Ge', 'Yarin Gal', 'Zoubin Ghahramani']",['stat.ML'],2015-09-16 01:07:24+00:00
http://arxiv.org/abs/1509.04771v2,Mapping Heritability of Large-Scale Brain Networks with a Billion Connections {\em via} Persistent Homology,"In many human brain network studies, we do not have sufficient number (n) of
images relative to the number (p) of voxels due to the prohibitively expensive
cost of scanning enough subjects. Thus, brain network models usually suffer the
small-n large-p problem. Such a problem is often remedied by sparse network
models, which are usually solved numerically by optimizing L1-penalties.
Unfortunately, due to the computational bottleneck associated with optimizing
L1-penalties, it is not practical to apply such methods to construct
large-scale brain networks at the voxel-level. In this paper, we propose a new
scalable sparse network model using cross-correlations that bypass the
computational bottleneck. Our model can build sparse brain networks at the
voxel level with p > 25000. Instead of using a single sparse parameter that may
not be optimal in other studies and datasets, the computational speed gain
enables us to analyze the collection of networks at every possible sparse
parameter in a coherent mathematical framework via persistent homology. The
method is subsequently applied in determining the extent of heritability on a
functional brain network at the voxel-level for the first time using twin fMRI.","['Moo K. Chung', 'Victoria Vilalta-Gil', 'Paul J. Rathouz', 'Benjamin B. Lahey', 'David H. Zald']","['cs.AI', 'q-bio.NC', 'stat.ML']",2015-09-15 23:54:12+00:00
http://arxiv.org/abs/1509.04767v2,Zero-Shot Learning via Semantic Similarity Embedding,"In this paper we consider a version of the zero-shot learning problem where
seen class source and target domain data are provided. The goal during
test-time is to accurately predict the class label of an unseen target domain
instance based on revealed source domain side information (\eg attributes) for
unseen classes. Our method is based on viewing each source or target data as a
mixture of seen class proportions and we postulate that the mixture patterns
have to be similar if the two instances belong to the same unseen class. This
perspective leads us to learning source/target embedding functions that map an
arbitrary source/target domain data into a same semantic space where similarity
can be readily measured. We develop a max-margin framework to learn these
similarity functions and jointly optimize parameters by means of cross
validation. Our test results are compelling, leading to significant improvement
in terms of accuracy on most benchmark datasets for zero-shot recognition.","['Ziming Zhang', 'Venkatesh Saligrama']","['cs.CV', 'stat.ML']",2015-09-15 23:18:52+00:00
http://arxiv.org/abs/1509.04752v3,Bayesian inference for spatio-temporal spike-and-slab priors,"In this work, we address the problem of solving a series of underdetermined
linear inverse problems subject to a sparsity constraint. We generalize the
spike-and-slab prior distribution to encode a priori correlation of the support
of the solution in both space and time by imposing a transformed Gaussian
process on the spike-and-slab probabilities. An expectation propagation (EP)
algorithm for posterior inference under the proposed model is derived. For
large scale problems, the standard EP algorithm can be prohibitively slow. We
therefore introduce three different approximation schemes to reduce the
computational complexity. Finally, we demonstrate the proposed model using
numerical experiments based on both synthetic and real data sets.","['Michael Riis Andersen', 'Aki Vehtari', 'Ole Winther', 'Lars Kai Hansen']","['stat.ML', 'stat.CO', 'stat.ME']",2015-09-15 21:58:12+00:00
http://arxiv.org/abs/1509.04740v3,Modeling sequences and temporal networks with dynamic community structures,"In evolving complex systems such as air traffic and social organizations,
collective effects emerge from their many components' dynamic interactions.
While the dynamic interactions can be represented by temporal networks with
nodes and links that change over time, they remain highly complex. It is
therefore often necessary to use methods that extract the temporal networks'
large-scale dynamic community structure. However, such methods are subject to
overfitting or suffer from effects of arbitrary, a priori imposed timescales,
which should instead be extracted from data. Here we simultaneously address
both problems and develop a principled data-driven method that determines
relevant timescales and identifies patterns of dynamics that take place on
networks as well as shape the networks themselves. We base our method on an
arbitrary-order Markov chain model with community structure, and develop a
nonparametric Bayesian inference framework that identifies the simplest such
model that can explain temporal interaction data.","['Tiago P. Peixoto', 'Martin Rosvall']","['cs.SI', 'cond-mat.stat-mech', 'physics.soc-ph', 'stat.ML']",2015-09-15 21:08:52+00:00
http://arxiv.org/abs/1509.04681v2,Large-Scale Optimization Algorithms for Sparse Conditional Gaussian Graphical Models,"This paper addresses the problem of scalable optimization for L1-regularized
conditional Gaussian graphical models. Conditional Gaussian graphical models
generalize the well-known Gaussian graphical models to conditional
distributions to model the output network influenced by conditioning input
variables. While highly scalable optimization methods exist for sparse Gaussian
graphical model estimation, state-of-the-art methods for conditional Gaussian
graphical models are not efficient enough and more importantly, fail due to
memory constraints for very large problems. In this paper, we propose a new
optimization procedure based on a Newton method that efficiently iterates over
two sub-problems, leading to drastic improvement in computation time compared
to the previous methods. We then extend our method to scale to large problems
under memory constraints, using block coordinate descent to limit memory usage
while achieving fast convergence. Using synthetic and genomic data, we show
that our methods can solve one million dimensional problems to high accuracy in
a little over a day on a single machine.","['Calvin McCarter', 'Seyoung Kim']",['stat.ML'],2015-09-15 19:03:48+00:00
http://arxiv.org/abs/1509.04640v1,Dynamic Poisson Factorization,"Models for recommender systems use latent factors to explain the preferences
and behaviors of users with respect to a set of items (e.g., movies, books,
academic papers). Typically, the latent factors are assumed to be static and,
given these factors, the observed preferences and behaviors of users are
assumed to be generated without order. These assumptions limit the explorative
and predictive capabilities of such models, since users' interests and item
popularity may evolve over time. To address this, we propose dPF, a dynamic
matrix factorization model based on the recent Poisson factorization model for
recommendations. dPF models the time evolving latent factors with a Kalman
filter and the actions with Poisson distributions. We derive a scalable
variational inference algorithm to infer the latent factors. Finally, we
demonstrate dPF on 10 years of user click data from arXiv.org, one of the
largest repository of scientific papers and a formidable source of information
about the behavior of scientists. Empirically we show performance improvement
over both static and, more recently proposed, dynamic recommendation models. We
also provide a thorough exploration of the inferred posteriors over the latent
variables.","['Laurent Charlin', 'Rajesh Ranganath', 'James McInerney', 'David M. Blei']","['cs.LG', 'cs.IR', 'stat.ML']",2015-09-15 16:57:15+00:00
http://arxiv.org/abs/1509.04634v2,Modeling and interpolation of the ambient magnetic field by Gaussian processes,"Anomalies in the ambient magnetic field can be used as features in indoor
positioning and navigation. By using Maxwell's equations, we derive and present
a Bayesian non-parametric probabilistic modeling approach for interpolation and
extrapolation of the magnetic field. We model the magnetic field components
jointly by imposing a Gaussian process (GP) prior on the latent scalar
potential of the magnetic field. By rewriting the GP model in terms of a
Hilbert space representation, we circumvent the computational pitfalls
associated with GP modeling and provide a computationally efficient and
physically justified modeling tool for the ambient magnetic field. The model
allows for sequential updating of the estimate and time-dependent changes in
the magnetic field. The model is shown to work well in practice in different
applications: we demonstrate mapping of the magnetic field both with an
inexpensive Raspberry Pi powered robot and on foot using a standard smartphone.","['Arno Solin', 'Manon Kok', 'Niklas Wahlström', 'Thomas B. Schön', 'Simo Särkkä']","['cs.RO', 'stat.ML']",2015-09-15 16:42:08+00:00
http://arxiv.org/abs/1509.04632v2,The Shape of Data and Probability Measures,"We introduce the notion of multiscale covariance tensor fields (CTF)
associated with Euclidean random variables as a gateway to the shape of their
distributions. Multiscale CTFs quantify variation of the data about every point
in the data landscape at all spatial scales, unlike the usual covariance tensor
that only quantifies global variation about the mean. Empirical forms of
localized covariance previously have been used in data analysis and
visualization, but we develop a framework for the systematic treatment of
theoretical questions and computational models based on localized covariance.
We prove strong stability theorems with respect to the Wasserstein distance
between probability measures, obtain consistency results, as well as estimates
for the rate of convergence of empirical CTFs. These results ensure that CTFs
are robust to sampling, noise and outliers. We provide numerous illustrations
of how CTFs let us extract shape from data and also apply CTFs to manifold
clustering, the problem of categorizing data points according to their noisy
membership in a collection of possibly intersecting, smooth submanifolds of
Euclidean space. We prove that the proposed manifold clustering method is
stable and carry out several experiments to validate the method.","['Diego Hernán Díaz Martínez', 'Facundo Mémoli', 'Washington Mio']","['stat.ML', 'math.MG', 'math.ST', 'stat.TH']",2015-09-15 16:36:26+00:00
http://arxiv.org/abs/1509.04612v2,Adapting Resilient Propagation for Deep Learning,"The Resilient Propagation (Rprop) algorithm has been very popular for
backpropagation training of multilayer feed-forward neural networks in various
applications. The standard Rprop however encounters difficulties in the context
of deep neural networks as typically happens with gradient-based learning
algorithms. In this paper, we propose a modification of the Rprop that combines
standard Rprop steps with a special drop out technique. We apply the method for
training Deep Neural Networks as standalone components and in ensemble
formulations. Results on the MNIST dataset show that the proposed modification
alleviates standard Rprop's problems demonstrating improved learning speed and
accuracy.","['Alan Mosca', 'George D. Magoulas']","['cs.NE', 'cs.CV', 'cs.LG', 'stat.ML']",2015-09-15 15:55:29+00:00
http://arxiv.org/abs/1509.04610v2,Macau: Scalable Bayesian Multi-relational Factorization with Side Information using MCMC,"We propose Macau, a powerful and flexible Bayesian factorization method for
heterogeneous data. Our model can factorize any set of entities and relations
that can be represented by a relational model, including tensors and also
multiple relations for each entity. Macau can also incorporate side
information, specifically entity and relation features, which are crucial for
predicting sparsely observed relations. Macau scales to millions of entity
instances, hundred millions of observations, and sparse entity features with
millions of dimensions. To achieve the scale up, we specially designed sampling
procedure for entity and relation features that relies primarily on noise
injection in linear regressions. We show performance and advanced features of
Macau in a set of experiments, including challenging drug-protein activity
prediction task.","['Jaak Simm', 'Adam Arany', 'Pooya Zakeri', 'Tom Haber', 'Jörg K. Wegner', 'Vladimir Chupakhin', 'Hugo Ceulemans', 'Yves Moreau']",['stat.ML'],2015-09-15 15:52:22+00:00
http://arxiv.org/abs/1509.04580v1,Maximum Correntropy Kalman Filter,"Traditional Kalman filter (KF) is derived under the well-known minimum mean
square error (MMSE) criterion, which is optimal under Gaussian assumption.
However, when the signals are non-Gaussian, especially when the system is
disturbed by some heavy-tailed impulsive noises, the performance of KF will
deteriorate seriously. To improve the robustness of KF against impulsive
noises, we propose in this work a new Kalman filter, called the maximum
correntropy Kalman filter (MCKF), which adopts the robust maximum correntropy
criterion (MCC) as the optimality criterion, instead of using the MMSE. Similar
to the traditional KF, the state mean and covariance matrix propagation
equations are used to give prior estimations of the state and covariance matrix
in MCKF. A novel fixed-point algorithm is then used to update the posterior
estimations. A sufficient condition that guarantees the convergence of the
fixed-point algorithm is given. Illustration examples are presented to
demonstrate the effectiveness and robustness of the new algorithm.","['Badong Chen', 'Xi Liu', 'Haiquan Zhao', 'José C. Príncipe']","['stat.ML', 'cs.SY']",2015-09-15 14:34:20+00:00
http://arxiv.org/abs/1509.04541v1,When are Kalman-filter restless bandits indexable?,"We study the restless bandit associated with an extremely simple scalar
Kalman filter model in discrete time. Under certain assumptions, we prove that
the problem is indexable in the sense that the Whittle index is a
non-decreasing function of the relevant belief state. In spite of the long
history of this problem, this appears to be the first such proof. We use
results about Schur-convexity and mechanical words, which are particular binary
strings intimately related to palindromes.","['Christopher R. Dance', 'Tomi Silander']",['stat.ML'],2015-09-15 13:33:52+00:00
http://arxiv.org/abs/1509.04491v2,Sparse Multinomial Logistic Regression via Approximate Message Passing,"For the problem of multi-class linear classification and feature selection,
we propose approximate message passing approaches to sparse multinomial
logistic regression (MLR). First, we propose two algorithms based on the Hybrid
Generalized Approximate Message Passing (HyGAMP) framework: one finds the
maximum a posteriori (MAP) linear classifier and the other finds an
approximation of the test-error-rate minimizing linear classifier. Then we
design computationally simplified variants of these two algorithms. Next, we
detail methods to tune the hyperparameters of their assumed statistical models
using Stein's unbiased risk estimate (SURE) and expectation-maximization (EM),
respectively. Finally, using both synthetic and real-world datasets, we
demonstrate improved error-rate and runtime performance relative to existing
state-of-the-art approaches to sparse MLR.","['Evan Byrne', 'Philip Schniter']","['cs.IT', 'math.IT', 'stat.ML']",2015-09-15 11:08:33+00:00
http://arxiv.org/abs/1509.04397v1,Exponential Family Matrix Completion under Structural Constraints,"We consider the matrix completion problem of recovering a structured matrix
from noisy and partial measurements. Recent works have proposed tractable
estimators with strong statistical guarantees for the case where the underlying
matrix is low--rank, and the measurements consist of a subset, either of the
exact individual entries, or of the entries perturbed by additive Gaussian
noise, which is thus implicitly suited for thin--tailed continuous data.
Arguably, common applications of matrix completion require estimators for (a)
heterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)
for heterogeneous noise models (beyond Gaussian), which capture varied
uncertainty in the measurements, and (c) heterogeneous structural constraints
beyond low--rank, such as block--sparsity, or a superposition structure of
low--rank plus elementwise sparseness, among others. In this paper, we provide
a vastly unified framework for generalized matrix completion by considering a
matrix completion setting wherein the matrix entries are sampled from any
member of the rich family of exponential family distributions; and impose
general structural constraints on the underlying matrix, as captured by a
general regularizer $\mathcal{R}(.)$. We propose a simple convex regularized
$M$--estimator for the generalized framework, and provide a unified and novel
statistical analysis for this general class of estimators. We finally
corroborate our theoretical results on simulated datasets.","['Suriya Gunasekar', 'Pradeep Ravikumar', 'Joydeep Ghosh']","['stat.ML', 'cs.LG']",2015-09-15 04:49:57+00:00
http://arxiv.org/abs/1509.04376v1,Precise Phase Transition of Total Variation Minimization,"Characterizing the phase transitions of convex optimizations in recovering
structured signals or data is of central importance in compressed sensing,
machine learning and statistics. The phase transitions of many convex
optimization signal recovery methods such as $\ell_1$ minimization and nuclear
norm minimization are well understood through recent years' research. However,
rigorously characterizing the phase transition of total variation (TV)
minimization in recovering sparse-gradient signal is still open. In this paper,
we fully characterize the phase transition curve of the TV minimization. Our
proof builds on Donoho, Johnstone and Montanari's conjectured phase transition
curve for the TV approximate message passing algorithm (AMP), together with the
linkage between the minmax Mean Square Error of a denoising problem and the
high-dimensional convex geometry for TV minimization.","['Bingwen Zhang', 'Weiyu Xu', 'Jian-Feng Cai', 'Lifeng Lai']","['cs.IT', 'cs.LG', 'math.IT', 'math.OC', 'stat.ML']",2015-09-15 02:18:58+00:00
http://arxiv.org/abs/1509.04332v1,Learning without Recall by Random Walks on Directed Graphs,"We consider a network of agents that aim to learn some unknown state of the
world using private observations and exchange of beliefs. At each time, agents
observe private signals generated based on the true unknown state. Each agent
might not be able to distinguish the true state based only on her private
observations. This occurs when some other states are observationally equivalent
to the true state from the agent's perspective. To overcome this shortcoming,
agents must communicate with each other to benefit from local observations. We
propose a model where each agent selects one of her neighbors randomly at each
time. Then, she refines her opinion using her private signal and the prior of
that particular neighbor. The proposed rule can be thought of as a Bayesian
agent who cannot recall the priors based on which other agents make inferences.
This learning without recall approach preserves some aspects of the Bayesian
inference while being computationally tractable. By establishing a
correspondence with a random walk on the network graph, we prove that under the
described protocol, agents learn the truth exponentially fast in the almost
sure sense. The asymptotic rate is expressed as the sum of the relative
entropies between the signal structures of every agent weighted by the
stationary distribution of the random walk.","['Mohammad Amin Rahimian', 'Shahin Shahrampour', 'Ali Jadbabaie']","['cs.SY', 'math.OC', 'stat.ML']",2015-09-14 21:23:50+00:00
http://arxiv.org/abs/1509.04238v1,A Practioner's Guide to Evaluating Entity Resolution Results,"Entity resolution (ER) is the task of identifying records belonging to the
same entity (e.g. individual, group) across one or multiple databases.
Ironically, it has multiple names: deduplication and record linkage, among
others. In this paper we survey metrics used to evaluate ER results in order to
iteratively improve performance and guarantee sufficient quality prior to
deployment. Some of these metrics are borrowed from multi-class classification
and clustering domains, though some key differences exist differentiating
entity resolution from general clustering. Menestrina et al. empirically showed
rankings from these metrics often conflict with each other, thus our primary
motivation for studying them. This paper provides practitioners the basic
knowledge to begin evaluating their entity resolution results.",['Matt Barnes'],"['cs.DB', 'stat.ML']",2015-09-14 18:57:02+00:00
http://arxiv.org/abs/1509.04210v3,Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A Systematic Study,"This paper presents Rudra, a parameter server based distributed computing
framework tuned for training large-scale deep neural networks. Using variants
of the asynchronous stochastic gradient descent algorithm we study the impact
of synchronization protocol, stale gradient updates, minibatch size, learning
rates, and number of learners on runtime performance and model accuracy. We
introduce a new learning rate modulation strategy to counter the effect of
stale gradients and propose a new synchronization protocol that can effectively
bound the staleness in gradients, improve runtime performance and achieve good
model accuracy. Our empirical investigation reveals a principled approach for
distributed training of neural networks: the mini-batch size per learner should
be reduced as more learners are added to the system to preserve the model
accuracy. We validate this approach using commonly-used image classification
benchmarks: CIFAR10 and ImageNet.","['Suyog Gupta', 'Wei Zhang', 'Fei Wang']","['stat.ML', 'cs.DC', 'cs.LG', 'cs.NE']",2015-09-14 17:14:52+00:00
http://arxiv.org/abs/1509.04072v3,Robust Gaussian Filtering using a Pseudo Measurement,"Many sensors, such as range, sonar, radar, GPS and visual devices, produce
measurements which are contaminated by outliers. This problem can be addressed
by using fat-tailed sensor models, which account for the possibility of
outliers. Unfortunately, all estimation algorithms belonging to the family of
Gaussian filters (such as the widely-used extended Kalman filter and unscented
Kalman filter) are inherently incompatible with such fat-tailed sensor models.
The contribution of this paper is to show that any Gaussian filter can be made
compatible with fat-tailed sensor models by applying one simple change: Instead
of filtering with the physical measurement, we propose to filter with a pseudo
measurement obtained by applying a feature function to the physical
measurement. We derive such a feature function which is optimal under some
conditions. Simulation results show that the proposed method can effectively
handle measurement outliers and allows for robust filtering in both linear and
nonlinear systems.","['Manuel Wüthrich', 'Cristina Garcia Cifuentes', 'Sebastian Trimpe', 'Franziska Meier', 'Jeannette Bohg', 'Jan Issac', 'Stefan Schaal']","['stat.ML', 'cs.SY']",2015-09-14 13:04:42+00:00
http://arxiv.org/abs/1509.03977v1,Optimization of anemia treatment in hemodialysis patients via reinforcement learning,"Objective: Anemia is a frequent comorbidity in hemodialysis patients that can
be successfully treated by administering erythropoiesis-stimulating agents
(ESAs). ESAs dosing is currently based on clinical protocols that often do not
account for the high inter- and intra-individual variability in the patient's
response. As a result, the hemoglobin level of some patients oscillates around
the target range, which is associated with multiple risks and side-effects.
This work proposes a methodology based on reinforcement learning (RL) to
optimize ESA therapy.
  Methods: RL is a data-driven approach for solving sequential decision-making
problems that are formulated as Markov decision processes (MDPs). Computing
optimal drug administration strategies for chronic diseases is a sequential
decision-making problem in which the goal is to find the best sequence of drug
doses. MDPs are particularly suitable for modeling these problems due to their
ability to capture the uncertainty associated with the outcome of the treatment
and the stochastic nature of the underlying process. The RL algorithm employed
in the proposed methodology is fitted Q iteration, which stands out for its
ability to make an efficient use of data.
  Results: The experiments reported here are based on a computational model
that describes the effect of ESAs on the hemoglobin level. The performance of
the proposed method is evaluated and compared with the well-known Q-learning
algorithm and with a standard protocol. Simulation results show that the
performance of Q-learning is substantially lower than FQI and the protocol.
  Conclusion: Although prospective validation is required, promising results
demonstrate the potential of RL to become an alternative to current protocols.","['Pablo Escandell-Montero', 'Milena Chermisi', 'José M. Martínez-Martínez', 'Juan Gómez-Sanchis', 'Carlo Barbieri', 'Emilio Soria-Olivas', 'Flavio Mari', 'Joan Vila-Francés', 'Andrea Stopper', 'Emanuele Gatti', 'José D. Martín-Guerrero']","['stat.ML', 'cs.AI', 'cs.LG']",2015-09-14 07:52:00+00:00
http://arxiv.org/abs/1509.03935v1,Markov Boundary Discovery with Ridge Regularized Linear Models,"Ridge regularized linear models (RRLMs), such as ridge regression and the
SVM, are a popular group of methods that are used in conjunction with
coefficient hypothesis testing to discover explanatory variables with a
significant multivariate association to a response. However, many investigators
are reluctant to draw causal interpretations of the selected variables due to
the incomplete knowledge of the capabilities of RRLMs in causal inference.
Under reasonable assumptions, we show that a modified form of RRLMs can get
very close to identifying a subset of the Markov boundary by providing a
worst-case bound on the space of possible solutions. The results hold for any
convex loss, even when the underlying functional relationship is nonlinear, and
the solution is not unique. Our approach combines ideas in Markov boundary and
sufficient dimension reduction theory. Experimental results show that the
modified RRLMs are competitive against state-of-the-art algorithms in
discovering part of the Markov boundary from gene expression data.","['Eric V. Strobl', 'Shyam Visweswaran']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2015-09-14 02:58:58+00:00
http://arxiv.org/abs/1509.03917v3,Dropping Convexity for Faster Semi-definite Optimization,"We study the minimization of a convex function $f(X)$ over the set of
$n\times n$ positive semi-definite matrices, but when the problem is recast as
$\min_U g(U) := f(UU^\top)$, with $U \in \mathbb{R}^{n \times r}$ and $r \leq
n$. We study the performance of gradient descent on $g$---which we refer to as
Factored Gradient Descent (FGD)---under standard assumptions on the original
function $f$.
  We provide a rule for selecting the step size and, with this choice, show
that the local convergence rate of FGD mirrors that of standard gradient
descent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ for
smooth $f$, and exponentially small in $k$ when $f$ is (restricted) strongly
convex. In addition, we provide a procedure to initialize FGD for (restricted)
strongly convex objectives and when one only has access to $f$ via a
first-order oracle; for several problem instances, such proper initialization
leads to global convergence guarantees.
  FGD and similar procedures are widely used in practice for problems that can
be posed as matrix factorization. To the best of our knowledge, this is the
first paper to provide precise convergence rate guarantees for general convex
functions under standard convex assumptions.","['Srinadh Bhojanapalli', 'Anastasios Kyrillidis', 'Sujay Sanghavi']","['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'cs.NA', 'math.IT', 'math.OC']",2015-09-14 00:40:11+00:00
http://arxiv.org/abs/1509.03808v3,A Markov Jump Process for More Efficient Hamiltonian Monte Carlo,"In most sampling algorithms, including Hamiltonian Monte Carlo, transition
rates between states correspond to the probability of making a transition in a
single time step, and are constrained to be less than or equal to 1. We derive
a Hamiltonian Monte Carlo algorithm using a continuous time Markov jump
process, and are thus able to escape this constraint. Transition rates in a
Markov jump process need only be non-negative. We demonstrate that the new
algorithm leads to improved mixing for several example problems, both by
evaluating the spectral gap of the Markov operator, and by computing
autocorrelation as a function of compute time. We release the algorithm as an
open source Python package.","['Andrew B. Berger', 'Mayur Mudigonda', 'Michael R. DeWeese', 'Jascha Sohl-Dickstein']","['stat.ML', 'stat.CO']",2015-09-13 04:29:13+00:00
http://arxiv.org/abs/1509.03475v2,Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks,"Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable
performance in the area of speech and handwriting recognition. The performance
of an MDRNN is improved by further increasing its depth, and the difficulty of
learning the deeper network is overcome by using Hessian-free (HF)
optimization. Given that connectionist temporal classification (CTC) is
utilized as an objective of learning an MDRNN for sequence labeling, the
non-convexity of CTC poses a problem when applying HF to the network. As a
solution, a convex approximation of CTC is formulated and its relationship with
the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to
a depth of 15 layers is successfully trained using HF, resulting in an improved
performance for sequence labeling.","['Minhyung Cho', 'Chandra Shekhar Dhir', 'Jaehyung Lee']","['cs.LG', 'cs.NE', 'stat.ML']",2015-09-11 12:28:36+00:00
http://arxiv.org/abs/1509.03381v1,Learning the Number of Autoregressive Mixtures in Time Series Using the Gap Statistics,"Using a proper model to characterize a time series is crucial in making
accurate predictions. In this work we use time-varying autoregressive process
(TVAR) to describe non-stationary time series and model it as a mixture of
multiple stable autoregressive (AR) processes. We introduce a new model
selection technique based on Gap statistics to learn the appropriate number of
AR filters needed to model a time series. We define a new distance measure
between stable AR filters and draw a reference curve that is used to measure
how much adding a new AR filter improves the performance of the model, and then
choose the number of AR filters that has the maximum gap with the reference
curve. To that end, we propose a new method in order to generate uniform random
stable AR filters in root domain. Numerical results are provided demonstrating
the performance of the proposed approach.","['Jie Ding', 'Mohammad Noshad', 'Vahid Tarokh']",['stat.ML'],2015-09-11 03:16:52+00:00
http://arxiv.org/abs/1509.03302v1,Performance Bounds for Pairwise Entity Resolution,"One significant challenge to scaling entity resolution algorithms to massive
datasets is understanding how performance changes after moving beyond the realm
of small, manually labeled reference datasets. Unlike traditional machine
learning tasks, when an entity resolution algorithm performs well on small
hold-out datasets, there is no guarantee this performance holds on larger
hold-out datasets. We prove simple bounding properties between the performance
of a match function on a small validation set and the performance of a pairwise
entity resolution algorithm on arbitrarily sized datasets. Thus, our approach
enables optimization of pairwise entity resolution algorithms for large
datasets, using a small set of labeled data.","['Matt Barnes', 'Kyle Miller', 'Artur Dubrawski']","['stat.ML', 'cs.CY', 'cs.DB', 'cs.LG']",2015-09-10 19:58:44+00:00
http://arxiv.org/abs/1509.03281v2,Density Evolution in the Degree-correlated Stochastic Block Model,"There is a recent surge of interest in identifying the sharp recovery
thresholds for cluster recovery under the stochastic block model. In this
paper, we address the more refined question of how many vertices that will be
misclassified on average. We consider the binary form of the stochastic block
model, where $n$ vertices are partitioned into two clusters with edge
probability $a/n$ within the first cluster, $c/n$ within the second cluster,
and $b/n$ across clusters. Suppose that as $n \to \infty$, $a= b+ \mu \sqrt{ b}
$, $c=b+ \nu \sqrt{ b} $ for two fixed constants $\mu, \nu$, and $b \to \infty$
with $b=n^{o(1)}$. When the cluster sizes are balanced and $\mu \neq \nu$, we
show that the minimum fraction of misclassified vertices on average is given by
$Q(\sqrt{v^*})$, where $Q(x)$ is the Q-function for standard normal, $v^*$ is
the unique fixed point of $v= \frac{(\mu-\nu)^2}{16} + \frac{ (\mu+\nu)^2 }{16}
\mathbb{E}[ \tanh(v+ \sqrt{v} Z)],$ and $Z$ is standard normal. Moreover, the
minimum misclassified fraction on average is attained by a local algorithm,
namely belief propagation, in time linear in the number of edges. Our proof
techniques are based on connecting the cluster recovery problem to tree
reconstruction problems, and analyzing the density evolution of belief
propagation on trees with Gaussian approximations.","['Elchanan Mossel', 'Jiaming Xu']","['stat.ML', 'cs.IT', 'math.IT', 'math.PR']",2015-09-10 19:11:56+00:00
http://arxiv.org/abs/1509.03248v1,A deep matrix factorization method for learning attribute representations,"Semi-Non-negative Matrix Factorization is a technique that learns a
low-dimensional representation of a dataset that lends itself to a clustering
interpretation. It is possible that the mapping between this new representation
and our original data matrix contains rather complex hierarchical information
with implicit lower-level hidden attributes, that classical one level
clustering methodologies can not interpret. In this work we propose a novel
model, Deep Semi-NMF, that is able to learn such hidden representations that
allow themselves to an interpretation of clustering according to different,
unknown attributes of a given dataset. We also present a semi-supervised
version of the algorithm, named Deep WSF, that allows the use of (partial)
prior information for each of the known attributes of a dataset, that allows
the model to be used on datasets with mixed attribute knowledge. Finally, we
show that our models are able to learn low-dimensional representations that are
better suited for clustering, but also classification, outperforming
Semi-Non-negative Matrix Factorization, but also other state-of-the-art
methodologies variants.","['George Trigeorgis', 'Konstantinos Bousmalis', 'Stefanos Zafeiriou', 'Bjoern W. Schuller']","['cs.CV', 'cs.LG', 'stat.ML']",2015-09-10 17:57:03+00:00
http://arxiv.org/abs/1509.03025v1,Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees,"Optimization problems with rank constraints arise in many applications,
including matrix regression, structured PCA, matrix completion and matrix
decomposition problems. An attractive heuristic for solving such problems is to
factorize the low-rank matrix, and to run projected gradient descent on the
nonconvex factorized optimization problem. The goal of this problem is to
provide a general theoretical framework for understanding when such methods
work well, and to characterize the nature of the resulting fixed point. We
provide a simple set of conditions under which projected gradient descent, when
given a suitable initialization, converges geometrically to a statistically
useful solution. Our results are applicable even when the initial solution is
outside any region of local convexity, and even when the problem is globally
concave. Working in a non-asymptotic framework, we show that our conditions are
satisfied for a wide range of concrete models, including matrix regression,
structured PCA, matrix completion with real and quantized observations, matrix
decomposition, and graph clustering problems. Simulation results show excellent
agreement with the theoretical predictions.","['Yudong Chen', 'Martin J. Wainwright']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2015-09-10 06:07:52+00:00
http://arxiv.org/abs/1509.03005v1,Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies,"This paper proposes GProp, a deep reinforcement learning algorithm for
continuous policies with compatible function approximation. The algorithm is
based on two innovations. Firstly, we present a temporal-difference based
method for learning the gradient of the value-function. Secondly, we present
the deviator-actor-critic (DAC) model, which comprises three neural networks
that estimate the value function, its gradient, and determine the actor's
policy respectively. We evaluate GProp on two challenging tasks: a contextual
bandit problem constructed from nonparametric regression datasets that is
designed to probe the ability of reinforcement learning algorithms to
accurately estimate gradients; and the octopus arm, a challenging reinforcement
learning benchmark. GProp is competitive with fully supervised methods on the
bandit task and achieves the best performance to date on the octopus arm.","['David Balduzzi', 'Muhammad Ghifary']","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",2015-09-10 04:14:54+00:00
http://arxiv.org/abs/1509.02971v6,Continuous control with deep reinforcement learning,"We adapt the ideas underlying the success of Deep Q-Learning to the
continuous action domain. We present an actor-critic, model-free algorithm
based on the deterministic policy gradient that can operate over continuous
action spaces. Using the same learning algorithm, network architecture and
hyper-parameters, our algorithm robustly solves more than 20 simulated physics
tasks, including classic problems such as cartpole swing-up, dexterous
manipulation, legged locomotion and car driving. Our algorithm is able to find
policies whose performance is competitive with those found by a planning
algorithm with full access to the dynamics of the domain and its derivatives.
We further demonstrate that for many of the tasks the algorithm can learn
policies end-to-end: directly from raw pixel inputs.","['Timothy P. Lillicrap', 'Jonathan J. Hunt', 'Alexander Pritzel', 'Nicolas Heess', 'Tom Erez', 'Yuval Tassa', 'David Silver', 'Daan Wierstra']","['cs.LG', 'stat.ML']",2015-09-09 23:01:36+00:00
http://arxiv.org/abs/1509.02962v1,Coarse-to-Fine Sequential Monte Carlo for Probabilistic Programs,"Many practical techniques for probabilistic inference require a sequence of
distributions that interpolate between a tractable distribution and an
intractable distribution of interest. Usually, the sequences used are simple,
e.g., based on geometric averages between distributions. When models are
expressed as probabilistic programs, the models themselves are highly
structured objects that can be used to derive annealing sequences that are more
sensitive to domain structure. We propose an algorithm for transforming
probabilistic programs to coarse-to-fine programs which have the same marginal
distribution as the original programs, but generate the data at increasing
levels of detail, from coarse to fine. We apply this algorithm to an Ising
model, its depth-from-disparity variation, and a factorial hidden Markov model.
We show preliminary evidence that the use of coarse-to-fine models can make
existing generic inference algorithms more efficient.","['Andreas Stuhlmüller', 'Robert X. D. Hawkins', 'N. Siddharth', 'Noah D. Goodman']","['cs.AI', 'stat.ML']",2015-09-09 21:48:22+00:00
http://arxiv.org/abs/1509.02957v2,Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized Huber Loss Regression and Quantile Regression,"We propose an algorithm, semismooth Newton coordinate descent (SNCD), for the
elastic-net penalized Huber loss regression and quantile regression in high
dimensional settings. Unlike existing coordinate descent type algorithms, the
SNCD updates each regression coefficient and its corresponding subgradient
simultaneously in each iteration. It combines the strengths of the coordinate
descent and the semismooth Newton algorithm, and effectively solves the
computational challenges posed by dimensionality and nonsmoothness. We
establish the convergence properties of the algorithm. In addition, we present
an adaptive version of the ""strong rule"" for screening predictors to gain extra
efficiency. Through numerical experiments, we demonstrate that the proposed
algorithm is very efficient and scalable to ultra-high dimensions. We
illustrate the application via a real data example.","['Congrui Yi', 'Jian Huang']","['stat.CO', 'stat.ML']",2015-09-09 21:26:39+00:00
http://arxiv.org/abs/1509.02954v1,Sensor Selection by Linear Programming,"We learn sensor trees from training data to minimize sensor acquisition costs
during test time. Our system adaptively selects sensors at each stage if
necessary to make a confident classification. We pose the problem as empirical
risk minimization over the choice of trees and node decision rules. We
decompose the problem, which is known to be intractable, into combinatorial
(tree structures) and continuous parts (node decision rules) and propose to
solve them separately. Using training data we greedily solve for the
combinatorial tree structures and for the continuous part, which is a
non-convex multilinear objective function, we derive convex surrogate loss
functions that are piecewise linear. The resulting problem can be cast as a
linear program and has the advantage of guaranteed convergence, global
optimality, repeatability and computational efficiency. We show that our
proposed approach outperforms the state-of-art on a number of benchmark
datasets.","['Joseph Wang', 'Kirill Trapeznikov', 'Venkatesh Saligrama']","['stat.ML', 'cs.LG']",2015-09-09 21:15:32+00:00
http://arxiv.org/abs/1509.02900v2,"Statistical Inference, Learning and Models in Big Data","The need for new methods to deal with big data is a common theme in most
scientific fields, although its definition tends to vary with the context.
Statistical ideas are an essential part of this, and as a partial response, a
thematic program on statistical inference, learning, and models in big data was
held in 2015 in Canada, under the general direction of the Canadian Statistical
Sciences Institute, with major funding from, and most activities located at,
the Fields Institute for Research in Mathematical Sciences. This paper gives an
overview of the topics covered, describing challenges and strategies that seem
common to many different areas of application, and including some examples of
applications to make these challenges and strategies more concrete.","['Beate Franke', 'Jean-François Plante', 'Ribana Roscher', 'Annie Lee', 'Cathal Smyth', 'Armin Hatefi', 'Fuqi Chen', 'Einat Gil', 'Alexander Schwing', 'Alessandro Selvitella', 'Michael M. Hoffman', 'Roger Grosse', 'Dieter Hendricks', 'Nancy Reid']","['stat.ML', 'cs.LG', '62-07', 'I.2.6; I.2.3; I.5.1; G.3']",2015-09-09 19:33:31+00:00
http://arxiv.org/abs/1509.02873v1,Sélection de variables par le GLM-Lasso pour la prédiction du risque palustre,"In this study, we propose an automatic learning method for variables
selection based on Lasso in epidemiology context. One of the aim of this
approach is to overcome the pretreatment of experts in medicine and
epidemiology on collected data. These pretreatment consist in recoding some
variables and to choose some interactions based on expertise. The approach
proposed uses all available explanatory variables without treatment and
generate automatically all interactions between them. This lead to high
dimension. We use Lasso, one of the robust methods of variable selection in
high dimension. To avoid over fitting a two levels cross-validation is used.
Because the target variable is account variable and the lasso estimators are
biased, variables selected by lasso are debiased by a GLM and used to predict
the distribution of the main vector of malaria which is Anopheles. Results show
that only few climatic and environmental variables are the mains factors
associated to the malaria risk exposure.","['Bienvenue Kouwayè', 'Noël Fonton', 'Fabrice Rossi']",['stat.ML'],2015-09-09 17:59:23+00:00
http://arxiv.org/abs/1509.02866v2,Fast Second-Order Stochastic Backpropagation for Variational Inference,"We propose a second-order (Hessian or Hessian-free) based optimization method
for variational inference inspired by Gaussian backpropagation, and argue that
quasi-Newton optimization can be developed as well. This is accomplished by
generalizing the gradient computation in stochastic backpropagation via a
reparametrization trick with lower complexity. As an illustrative example, we
apply this approach to the problems of Bayesian logistic regression and
variational auto-encoder (VAE). Additionally, we compute bounds on the
estimator variance of intractable expectations for the family of Lipschitz
continuous function. Our method is practical, scalable and model free. We
demonstrate our method on several real-world datasets and provide comparisons
with other stochastic gradient methods to show substantial enhancement in
convergence rates.","['Kai Fan', 'Ziteng Wang', 'Jeff Beck', 'James Kwok', 'Katherine Heller']",['stat.ML'],2015-09-09 17:44:37+00:00
http://arxiv.org/abs/1509.02857v3,Asymptotically Optimal Multi-Armed Bandit Policies under a Cost Constraint,"We develop asymptotically optimal policies for the multi armed bandit (MAB),
problem, under a cost constraint. This model is applicable in situations where
each sample (or activation) from a population (bandit) incurs a known bandit
dependent cost. Successive samples from each population are iid random
variables with unknown distribution. The objective is to design a feasible
policy for deciding from which population to sample from, so as to maximize the
expected sum of outcomes of $n$ total samples or equivalently to minimize the
regret due to lack on information on sample distributions, For this problem we
consider the class of feasible uniformly fast (f-UF) convergent policies, that
satisfy the cost constraint sample-path wise. We first establish a necessary
asymptotic lower bound for the rate of increase of the regret function of f-UF
policies. Then we construct a class of f-UF policies and provide conditions
under which they are asymptotically optimal within the class of f-UF policies,
achieving this asymptotic lower bound. At the end we provide the explicit form
of such policies for the case in which the unknown distributions are Normal
with unknown means and known variances.","['Apostolos N. Burnetas', 'Odysseas Kanavetas', 'Michael N. Katehakis']","['stat.ML', 'math.OC']",2015-09-09 17:27:19+00:00
http://arxiv.org/abs/1509.02805v3,Clustering by Hierarchical Nearest Neighbor Descent (H-NND),"Previously in 2014, we proposed the Nearest Descent (ND) method, capable of
generating an efficient Graph, called the in-tree (IT). Due to some beautiful
and effective features, this IT structure proves well suited for data
clustering. Although there exist some redundant edges in IT, they usually have
salient features and thus it is not hard to remove them.
  Subsequently, in order to prevent the seemingly redundant edges from
occurring, we proposed the Nearest Neighbor Descent (NND) by adding the
""Neighborhood"" constraint on ND. Consequently, clusters automatically emerged,
without the additional requirement of removing the redundant edges. However,
NND proved still not perfect, since it brought in a new yet worse problem, the
""over-partitioning"" problem.
  Now, in this paper, we propose a method, called the Hierarchical Nearest
Neighbor Descent (H-NND), which overcomes the over-partitioning problem of NND
via using the hierarchical strategy. Specifically, H-NND uses ND to effectively
merge the over-segmented sub-graphs or clusters that NND produces. Like ND,
H-NND also generates the IT structure, in which the redundant edges once again
appear. This seemingly comes back to the situation that ND faces. However,
compared with ND, the redundant edges in the IT structure generated by H-NND
generally become more salient, thus being much easier and more reliable to be
identified even by the simplest edge-removing method which takes the edge
length as the only measure. In other words, the IT structure constructed by
H-NND becomes more fitted for data clustering. We prove this on several
clustering datasets of varying shapes, dimensions and attributes. Besides,
compared with ND, H-NND generally takes less computation time to construct the
IT data structure for the input data.","['Teng Qiu', 'Yongjie Li']","['stat.ML', 'cs.CV', 'cs.LG', 'stat.ME']",2015-09-09 15:15:44+00:00
http://arxiv.org/abs/1509.02438v1,A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression,"Online Passive-Aggressive (PA) learning is a class of online margin-based
algorithms suitable for a wide range of real-time prediction tasks, including
classification and regression. PA algorithms are formulated in terms of
deterministic point-estimation problems governed by a set of user-defined
hyperparameters: the approach fails to capture model/prediction uncertainty and
makes their performance highly sensitive to hyperparameter configurations. In
this paper, we introduce a novel PA learning framework for regression that
overcomes the above limitations. We contribute a Bayesian state-space
interpretation of PA regression, along with a novel online variational
inference scheme, that not only produces probabilistic predictions, but also
offers the benefit of automatic hyperparameter tuning. Experiments with various
real-world data sets show that our approach performs significantly better than
a more standard, linear Gaussian state-space model.","['Arnold Salas', 'Stephen J. Roberts', 'Michael A. Osborne']",['stat.ML'],2015-09-08 16:42:39+00:00
http://arxiv.org/abs/1509.02357v1,Empirical risk minimization is consistent with the mean absolute percentage error,"We study in this paper the consequences of using the Mean Absolute Percentage
Error (MAPE) as a measure of quality for regression models. We show that
finding the best model under the MAPE is equivalent to doing weighted Mean
Absolute Error (MAE) regression. We also show that, under some asumptions,
universal consistency of Empirical Risk Minimization remains possible using the
MAPE.","['Arnaud De Myttenaere', 'Bénédicte Le Grand', 'Fabrice Rossi']",['stat.ML'],2015-09-08 13:17:28+00:00
http://arxiv.org/abs/1509.02348v1,On the complexity of piecewise affine system identification,"The paper provides results regarding the computational complexity of hybrid
system identification. More precisely, we focus on the estimation of piecewise
affine (PWA) maps from input-output data and analyze the complexity of
computing a global minimizer of the error. Previous work showed that a global
solution could be obtained for continuous PWA maps with a worst-case complexity
exponential in the number of data. In this paper, we show how global optimality
can be reached for a slightly more general class of possibly discontinuous PWA
maps with a complexity only polynomial in the number of data, however with an
exponential complexity with respect to the data dimension. This result is
obtained via an analysis of the intrinsic classification subproblem of
associating the data points to the different modes. In addition, we prove that
the problem is NP-hard, and thus that the exponential complexity in the
dimension is a natural expectation for any exact algorithm.",['Fabien Lauer'],"['stat.ML', 'cs.CC']",2015-09-08 13:03:19+00:00
http://arxiv.org/abs/1509.02347v1,Modelling time evolving interactions in networks through a non stationary extension of stochastic block models,"In this paper, we focus on the stochastic block model (SBM),a probabilistic
tool describing interactions between nodes of a network using latent clusters.
The SBM assumes that the networkhas a stationary structure, in which
connections of time varying intensity are not taken into account. In other
words, interactions between two groups are forced to have the same features
during the whole observation time. To overcome this limitation,we propose a
partition of the whole time horizon, in which interactions are observed, and
develop a non stationary extension of the SBM,allowing to simultaneously
cluster the nodes in a network along with fixed time intervals in which the
interactions take place. The number of clusters (K for nodes, D for time
intervals) as well as the class memberships are finallyobtained through
maximizing the complete-data integrated likelihood by means of a greedy search
approach. After showing that the model works properly with simulated data, we
focus on a real data set. We thus consider the three days ACM Hypertext
conference held in Turin,June 29th - July 1st 2009. Proximity interactions
between attendees during the first day are modelled and an
interestingclustering of the daily hours is finally obtained, with times of
social gathering (e.g. coffee breaks) recovered by the approach. Applications
to large networks are limited due to the computational complexity of the greedy
search which is dominated bythe number $K\_{max}$ and $D\_{max}$ of clusters
used in the initialization. Therefore,advanced clustering tools are considered
to reduce the number of clusters expected in the data, making the greedy search
applicable to large networks.","['Marco Corneli', 'Pierre Latouche', 'Fabrice Rossi']",['stat.ML'],2015-09-08 12:59:19+00:00
http://arxiv.org/abs/1509.02314v2,A Scalable and Extensible Framework for Superposition-Structured Models,"In many learning tasks, structural models usually lead to better
interpretability and higher generalization performance. In recent years,
however, the simple structural models such as lasso are frequently proved to be
insufficient. Accordingly, there has been a lot of work on
""superposition-structured"" models where multiple structural constraints are
imposed. To efficiently solve these ""superposition-structured"" statistical
models, we develop a framework based on a proximal Newton-type method.
Employing the smoothed conic dual approach with the LBFGS updating formula, we
propose a scalable and extensible proximal quasi-Newton (SEP-QN) framework.
Empirical analysis on various datasets shows that our framework is potentially
powerful, and achieves super-linear convergence rate for optimizing some
popular ""superposition-structured"" statistical models such as the fused sparse
group lasso.","['Shenjian Zhao', 'Cong Xie', 'Zhihua Zhang']","['cs.NA', 'math.OC', 'stat.ML']",2015-09-08 10:33:27+00:00
http://arxiv.org/abs/1509.02237v2,On Wasserstein Two Sample Testing and Related Families of Nonparametric Tests,"Nonparametric two sample or homogeneity testing is a decision theoretic
problem that involves identifying differences between two random variables
without making parametric assumptions about their underlying distributions. The
literature is old and rich, with a wide variety of statistics having being
intelligently designed and analyzed, both for the unidimensional and the
multivariate setting. Our contribution is to tie together many of these tests,
drawing connections between seemingly very different statistics. In this work,
our central object is the Wasserstein distance, as we form a chain of
connections from univariate methods like the Kolmogorov-Smirnov test, PP/QQ
plots and ROC/ODC curves, to multivariate tests involving energy statistics and
kernel based maximum mean discrepancy. Some connections proceed through the
construction of a \textit{smoothed} Wasserstein distance, and others through
the pursuit of a ""distribution-free"" Wasserstein test. Some observations in
this chain are implicit in the literature, while others seem to have not been
noticed thus far. Given nonparametric two sample testing's classical and
continued importance, we aim to provide useful connections for theorists and
practitioners familiar with one subset of methods but not others.","['Aaditya Ramdas', 'Nicolas Garcia', 'Marco Cuturi']","['math.ST', 'stat.ML', 'stat.TH']",2015-09-08 01:08:04+00:00
http://arxiv.org/abs/1509.02216v1,Fuzzy Jets,"Collimated streams of particles produced in high energy physics experiments
are organized using clustering algorithms to form jets. To construct jets, the
experimental collaborations based at the Large Hadron Collider (LHC) primarily
use agglomerative hierarchical clustering schemes known as sequential
recombination. We propose a new class of algorithms for clustering jets that
use infrared and collinear safe mixture models. These new algorithms, known as
fuzzy jets, are clustered using maximum likelihood techniques and can
dynamically determine various properties of jets like their size. We show that
the fuzzy jet size adds additional information to conventional jet tagging
variables. Furthermore, we study the impact of pileup and show that with some
slight modifications to the algorithm, fuzzy jets can be stable up to high
pileup interaction multiplicities.","['Lester Mackey', 'Benjamin Nachman', 'Ariel Schwartzman', 'Conrad Stansbury']","['hep-ph', 'stat.ML']",2015-09-07 22:49:43+00:00
