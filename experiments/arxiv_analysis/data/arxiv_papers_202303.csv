id,title,abstract,authors,categories,date
http://arxiv.org/abs/2304.09853v3,Bridging RL Theory and Practice with the Effective Horizon,"Deep reinforcement learning (RL) works impressively in some environments and
fails catastrophically in others. Ideally, RL theory should be able to provide
an understanding of why this is, i.e. bounds predictive of practical
performance. Unfortunately, current theory does not quite have this ability. We
compare standard deep RL algorithms to prior sample complexity bounds by
introducing a new dataset, BRIDGE. It consists of 155 deterministic MDPs from
common deep RL benchmarks, along with their corresponding tabular
representations, which enables us to exactly compute instance-dependent bounds.
We choose to focus on deterministic environments because they share many
interesting properties of stochastic environments, but are easier to analyze.
Using BRIDGE, we find that prior bounds do not correlate well with when deep RL
succeeds vs. fails, but discover a surprising property that does. When actions
with the highest Q-values under the random policy also have the highest
Q-values under the optimal policy (i.e. when it is optimal to be greedy on the
random policy's Q function), deep RL tends to succeed; when they don't, deep RL
tends to fail. We generalize this property into a new complexity measure of an
MDP that we call the effective horizon, which roughly corresponds to how many
steps of lookahead search would be needed in that MDP in order to identify the
next optimal action, when leaf nodes are evaluated with random rollouts. Using
BRIDGE, we show that the effective horizon-based bounds are more closely
reflective of the empirical performance of PPO and DQN than prior sample
complexity bounds across four metrics. We also find that, unlike existing
bounds, the effective horizon can predict the effects of using reward shaping
or a pre-trained exploration policy. Our code and data are available at
https://github.com/cassidylaidlaw/effective-horizon","['Cassidy Laidlaw', 'Stuart Russell', 'Anca Dragan']","['cs.LG', 'stat.ML']",2023-04-19 17:59:01+00:00
http://arxiv.org/abs/2304.09836v2,Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts,"Multivariate probabilistic time series forecasts are commonly evaluated via
proper scoring rules, i.e., functions that are minimal in expectation for the
ground-truth distribution. However, this property is not sufficient to
guarantee good discrimination in the non-asymptotic regime. In this paper, we
provide the first systematic finite-sample study of proper scoring rules for
time-series forecasting evaluation. Through a power analysis, we identify the
""region of reliability"" of a scoring rule, i.e., the set of practical
conditions where it can be relied on to identify forecasting errors. We carry
out our analysis on a comprehensive synthetic benchmark, specifically designed
to test several key discrepancies between ground-truth and forecast
distributions, and we gauge the generalizability of our findings to real-world
tasks with an application to an electricity production problem. Our results
reveal critical shortcomings in the evaluation of multivariate probabilistic
forecasts as commonly performed in the literature.","['Étienne Marcotte', 'Valentina Zantedeschi', 'Alexandre Drouin', 'Nicolas Chapados']","['cs.LG', 'stat.ML']",2023-04-19 17:38:42+00:00
http://arxiv.org/abs/2304.09802v1,Generalization and Estimation Error Bounds for Model-based Neural Networks,"Model-based neural networks provide unparalleled performance for various
tasks, such as sparse coding and compressed sensing problems. Due to the strong
connection with the sensing model, these networks are interpretable and inherit
prior structure of the problem. In practice, model-based neural networks
exhibit higher generalization capability compared to ReLU neural networks.
However, this phenomenon was not addressed theoretically. Here, we leverage
complexity measures including the global and local Rademacher complexities, in
order to provide upper bounds on the generalization and estimation errors of
model-based networks. We show that the generalization abilities of model-based
networks for sparse recovery outperform those of regular ReLU networks, and
derive practical design rules that allow to construct model-based networks with
guaranteed high generalization. We demonstrate through a series of experiments
that our theoretical insights shed light on a few behaviours experienced in
practice, including the fact that ISTA and ADMM networks exhibit higher
generalization abilities (especially for small number of training samples),
compared to ReLU networks.","['Avner Shultzman', 'Eyar Azar', 'Miguel R. D. Rodrigues', 'Yonina C. Eldar']","['cs.LG', 'stat.ML']",2023-04-19 16:39:44+00:00
http://arxiv.org/abs/2304.09663v2,Generative modeling of time-dependent densities via optimal transport and projection pursuit,"Motivated by the computational difficulties incurred by popular deep learning
algorithms for the generative modeling of temporal densities, we propose a
cheap alternative which requires minimal hyperparameter tuning and scales
favorably to high dimensional problems. In particular, we use a
projection-based optimal transport solver [Meng et al., 2019] to join
successive samples and subsequently use transport splines [Chewi et al., 2020]
to interpolate the evolving density. When the sampling frequency is
sufficiently high, the optimal maps are close to the identity and are thus
computationally efficient to compute. Moreover, the training process is highly
parallelizable as all optimal maps are independent and can thus be learned
simultaneously. Finally, the approach is based solely on numerical linear
algebra rather than minimizing a nonconvex objective function, allowing us to
easily analyze and control the algorithm. We present several numerical
experiments on both synthetic and real-world datasets to demonstrate the
efficiency of our method. In particular, these experiments show that the
proposed approach is highly competitive compared with state-of-the-art
normalizing flows conditioned on time across a wide range of dimensionalities.","['Jonah Botvinick-Greenhouse', 'Yunan Yang', 'Romit Maulik']","['stat.ML', 'cs.LG']",2023-04-19 13:50:13+00:00
http://arxiv.org/abs/2304.09576v2,Leveraging the two timescale regime to demonstrate convergence of neural networks,"We study the training dynamics of shallow neural networks, in a two-timescale
regime in which the stepsizes for the inner layer are much smaller than those
for the outer layer. In this regime, we prove convergence of the gradient flow
to a global optimum of the non-convex optimization problem in a simple
univariate setting. The number of neurons need not be asymptotically large for
our result to hold, distinguishing our result from popular recent approaches
such as the neural tangent kernel or mean-field regimes. Experimental
illustration is provided, showing that the stochastic gradient descent behaves
according to our description of the gradient flow and thus converges to a
global optimum in the two-timescale regime, but can fail outside of this
regime.","['Pierre Marion', 'Raphaël Berthier']","['math.OC', 'cs.LG', 'stat.ML']",2023-04-19 11:27:09+00:00
http://arxiv.org/abs/2304.09552v1,Denoising Cosine Similarity: A Theory-Driven Approach for Efficient Representation Learning,"Representation learning has been increasing its impact on the research and
practice of machine learning, since it enables to learn representations that
can apply to various downstream tasks efficiently. However, recent works pay
little attention to the fact that real-world datasets used during the stage of
representation learning are commonly contaminated by noise, which can degrade
the quality of learned representations. This paper tackles the problem to learn
robust representations against noise in a raw dataset. To this end, inspired by
recent works on denoising and the success of the cosine-similarity-based
objective functions in representation learning, we propose the denoising
Cosine-Similarity (dCS) loss. The dCS loss is a modified cosine-similarity loss
and incorporates a denoising property, which is supported by both our
theoretical and empirical findings. To make the dCS loss implementable, we also
construct the estimators of the dCS loss with statistical guarantees. Finally,
we empirically show the efficiency of the dCS loss over the baseline objective
functions in vision and speech domains.","['Takumi Nakagawa', 'Yutaro Sanada', 'Hiroki Waida', 'Yuhui Zhang', 'Yuichiro Wada', 'Kōsaku Takanashi', 'Tomonori Yamada', 'Takafumi Kanamori']","['stat.ML', 'cs.LG']",2023-04-19 10:33:39+00:00
http://arxiv.org/abs/2304.09424v2,Loss Minimization Yields Multicalibration for Large Neural Networks,"Multicalibration is a notion of fairness for predictors that requires them to
provide calibrated predictions across a large set of protected groups.
Multicalibration is known to be a distinct goal than loss minimization, even
for simple predictors such as linear functions.
  In this work, we consider the setting where the protected groups can be
represented by neural networks of size $k$, and the predictors are neural
networks of size $n > k$. We show that minimizing the squared loss over all
neural nets of size $n$ implies multicalibration for all but a bounded number
of unlucky values of $n$. We also give evidence that our bound on the number of
unlucky values is tight, given our proof technique. Previously, results of the
flavor that loss minimization yields multicalibration were known only for
predictors that were near the ground truth, hence were rather limited in
applicability. Unlike these, our results rely on the expressivity of neural
nets and utilize the representation of the predictor.","['Jarosław Błasiok', 'Parikshit Gopalan', 'Lunjia Hu', 'Adam Tauman Kalai', 'Preetum Nakkiran']","['cs.LG', 'cs.AI', 'stat.ML']",2023-04-19 05:16:20+00:00
http://arxiv.org/abs/2304.09398v2,Minimax Signal Detection in Sparse Additive Models,"Sparse additive models are an attractive choice in circumstances calling for
modelling flexibility in the face of high dimensionality. We study the signal
detection problem and establish the minimax separation rate for the detection
of a sparse additive signal. Our result is nonasymptotic and applicable to the
general case where the univariate component functions belong to a generic
reproducing kernel Hilbert space. Unlike the estimation theory, the minimax
separation rate reveals a nontrivial interaction between sparsity and the
choice of function space. We also investigate adaptation to sparsity and
establish an adaptive testing rate for a generic function space; adaptation is
possible in some spaces while others impose an unavoidable cost. Finally,
adaptation to both sparsity and smoothness is studied in the setting of Sobolev
space, and we correct some existing claims in the literature.","['Subhodh Kotekal', 'Chao Gao']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2023-04-19 03:22:04+00:00
http://arxiv.org/abs/2304.09868v3,Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression,"This paper proposes a novel framework for accelerating support vector
clustering. The proposed method first computes much smaller compressed data
sets while preserving the key cluster properties of the original data sets
based on a novel spectral data compression approach. Then, the resultant
spectrally-compressed data sets are leveraged for the development of fast and
high quality algorithm for support vector clustering. We conducted extensive
experiments using real-world data sets and obtained very promising results. The
proposed method allows us to achieve 100X and 115X speedups over the state of
the art SVC method on the Pendigits and USPS data sets, respectively, while
achieving even better clustering quality. To the best of our knowledge, this
represents the first practical method for high-quality and fast SVC on
large-scale real-world data sets","['Yuxuan Song', 'Yongyu Wang']","['cs.LG', 'cs.AI', 'stat.ML']",2023-04-19 01:35:05+00:00
http://arxiv.org/abs/2304.09310v3,The Adaptive $τ$-Lasso: Robustness and Oracle Properties,"This paper introduces a new regularized version of the robust
$\tau$-regression estimator for analyzing high-dimensional datasets subject to
gross contamination in the response variables and covariates (explanatory
variables). The resulting estimator, termed adaptive $\tau$-Lasso, is robust to
outliers and high-leverage points. It also incorporates an adaptive
$\ell_1$-norm penalty term, which enables the selection of relevant variables
and reduces the bias associated with large true regression coefficients. More
specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each
regression coefficient. For a fixed number of predictors $p$, we show that the
adaptive $\tau$-Lasso has the oracle property, ensuring both variable-selection
consistency and asymptotic normality. Asymptotic normality applies only to the
entries of the regression vector corresponding to the true support, assuming
knowledge of the true regression vector support. We characterize its robustness
by establishing the finite-sample breakdown point and the influence function.
We carry out extensive simulations and observe that the class of $\tau$-Lasso
estimators exhibits robustness and reliable performance in both contaminated
and uncontaminated data settings. We also validate our theoretical findings on
robustness properties through simulations. In the face of outliers and
high-leverage points, the adaptive $\tau$-Lasso and $\tau$-Lasso estimators
achieve the best performance or close-to-best performance in terms of
prediction and variable selection accuracy compared to other competing
regularized estimators for all scenarios considered in this study. Therefore,
the adaptive $\tau$-Lasso and $\tau$-Lasso estimators provide attractive tools
for a variety of sparse linear regression problems, particularly in
high-dimensional settings and when the data is contaminated by outliers and
high-leverage points.","['Emadaldin Mozafari-Majd', 'Visa Koivunen']","['stat.ML', 'cs.LG', 'eess.SP']",2023-04-18 21:34:14+00:00
http://arxiv.org/abs/2304.09278v1,A Data Driven Sequential Learning Framework to Accelerate and Optimize Multi-Objective Manufacturing Decisions,"Manufacturing advanced materials and products with a specific property or
combination of properties is often warranted. To achieve that it is crucial to
find out the optimum recipe or processing conditions that can generate the
ideal combination of these properties. Most of the time, a sufficient number of
experiments are needed to generate a Pareto front. However, manufacturing
experiments are usually costly and even conducting a single experiment can be a
time-consuming process. So, it's critical to determine the optimal location for
data collection to gain the most comprehensive understanding of the process.
Sequential learning is a promising approach to actively learn from the ongoing
experiments, iteratively update the underlying optimization routine, and adapt
the data collection process on the go. This paper presents a novel data-driven
Bayesian optimization framework that utilizes sequential learning to
efficiently optimize complex systems with multiple conflicting objectives.
Additionally, this paper proposes a novel metric for evaluating multi-objective
data-driven optimization approaches. This metric considers both the quality of
the Pareto front and the amount of data used to generate it. The proposed
framework is particularly beneficial in practical applications where acquiring
data can be expensive and resource intensive. To demonstrate the effectiveness
of the proposed algorithm and metric, the algorithm is evaluated on a
manufacturing dataset. The results indicate that the proposed algorithm can
achieve the actual Pareto front while processing significantly less data. It
implies that the proposed data-driven framework can lead to similar
manufacturing decisions with reduced costs and time.","['Hamed Khosravi', 'Taofeeq Olajire', 'Ahmed Shoyeb Raihan', 'Imtiaz Ahmed']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2023-04-18 20:33:08+00:00
http://arxiv.org/abs/2304.09242v2,A Framework for Analyzing Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition,"Precise estimation of cross-correlation or similarity between two random
variables lies at the heart of signal detection, hyperdimensional computing,
associative memories, and neural networks. Although a vast literature exists on
different methods for estimating cross-correlations, the question what is the
best and simplest method to estimate cross-correlations using finite samples ?
is still unclear. In this paper, we first argue that the standard empirical
approach might not be the optimal method even though the estimator exhibits
uniform convergence to the true cross-correlation. Instead, we show that there
exists a large class of simple non-linear functions that can be used to
construct cross-correlators with a higher signal-to-noise ratio (SNR). To
demonstrate this, we first present a general mathematical framework using
Price's Theorem that allows us to analyze cross-correlators constructed using a
mixture of piece-wise linear functions. Using this framework and
high-dimensional embedding, we show that some of the most promising
cross-correlators are based on Huber's loss functions, margin-propagation (MP)
functions, and the log-sum-exp (LSE) functions.","['Zhili Xiao', 'Shantanu Chakrabartty']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2023-04-18 19:03:27+00:00
http://arxiv.org/abs/2304.09221v2,Convergence of stochastic gradient descent under a local Lojasiewicz condition for deep neural networks,"We study the convergence of stochastic gradient descent (SGD) for non-convex
objective functions. We establish the local convergence with positive
probability under the local \L{}ojasiewicz condition introduced by Chatterjee
in \cite{chatterjee2022convergence} and an additional local structural
assumption of the loss function landscape. A key component of our proof is to
ensure that the whole trajectories of SGD stay inside the local region with a
positive probability. We also provide examples of neural networks with finite
widths such that our assumptions hold.","['Jing An', 'Jianfeng Lu']","['cs.LG', 'math.OC', 'stat.ML']",2023-04-18 18:20:52+00:00
http://arxiv.org/abs/2304.09157v3,Neural networks for geospatial data,"Analysis of geospatial data has traditionally been model-based, with a mean
model, customarily specified as a linear regression on the covariates, and a
covariance model, encoding the spatial dependence. We relax the strong
assumption of linearity and propose embedding neural networks directly within
the traditional geostatistical models to accommodate non-linear mean functions
while retaining all other advantages including use of Gaussian Processes to
explicitly model the spatial covariance, enabling inference on the covariate
effect through the mean and on the spatial dependence through the covariance,
and offering predictions at new locations via kriging. We propose NN-GLS, a new
neural network estimation algorithm for the non-linear mean in GP models that
explicitly accounts for the spatial covariance through generalized least
squares (GLS), the same loss used in the linear case. We show that NN-GLS
admits a representation as a special type of graph neural network (GNN). This
connection facilitates use of standard neural network computational techniques
for irregular geospatial data, enabling novel and scalable mini-batching,
backpropagation, and kriging schemes. Theoretically, we show that NN-GLS will
be consistent for irregularly observed spatially correlated data processes. We
also provide a finite sample concentration rate, which quantifies the need to
accurately model the spatial covariance in neural networks for dependent data.
To our knowledge, these are the first large-sample results for any neural
network algorithm for irregular spatial data. We demonstrate the methodology
through simulated and real datasets.","['Wentao Zhan', 'Abhirup Datta']","['stat.ML', 'cs.LG', 'stat.ME']",2023-04-18 17:52:23+00:00
http://arxiv.org/abs/2304.09154v1,Sharp-SSL: Selective high-dimensional axis-aligned random projections for semi-supervised learning,"We propose a new method for high-dimensional semi-supervised learning
problems based on the careful aggregation of the results of a low-dimensional
procedure applied to many axis-aligned random projections of the data. Our
primary goal is to identify important variables for distinguishing between the
classes; existing low-dimensional methods can then be applied for final class
assignment. Motivated by a generalized Rayleigh quotient, we score projections
according to the traces of the estimated whitened between-class covariance
matrices on the projected data. This enables us to assign an importance weight
to each variable for a given projection, and to select our signal variables by
aggregating these weights over high-scoring projections. Our theory shows that
the resulting Sharp-SSL algorithm is able to recover the signal coordinates
with high probability when we aggregate over sufficiently many random
projections and when the base procedure estimates the whitened between-class
covariance matrix sufficiently well. The Gaussian EM algorithm is a natural
choice as a base procedure, and we provide a new analysis of its performance in
semi-supervised settings that controls the parameter estimation error in terms
of the proportion of labeled data in the sample. Numerical results on both
simulated data and a real colon tumor dataset support the excellent empirical
performance of the method.","['Tengyao Wang', 'Edgar Dobriban', 'Milana Gataric', 'Richard J. Samworth']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH', '62H30']",2023-04-18 17:49:02+00:00
http://arxiv.org/abs/2304.09123v2,Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics,"This paper provides a finite-sample analysis of a passive stochastic gradient
Langevin dynamics algorithm (PSGLD) designed to achieve adaptive inverse
reinforcement learning (IRL). By passive, we mean that the noisy gradients
available to the PSGLD algorithm (inverse learning process) are evaluated at
randomly chosen points by an external stochastic gradient algorithm (forward
learner) that aims to optimize a cost function. The PSGLD algorithm acts as a
randomized sampler to achieve adaptive IRL by reconstructing this cost function
nonparametrically from the stationary measure of a Langevin diffusion. Previous
work has analyzed the asymptotic performance of this passive algorithm using
weak convergence techniques. This paper analyzes the non-asymptotic
(finite-sample) performance using a logarithmic-Sobolev inequality and the
Otto-Villani Theorem. We obtain finite-sample bounds on the 2-Wasserstein
distance between the estimates generated by the PSGLD algorithm and the cost
function. Apart from achieving finite-sample guarantees for adaptive IRL, this
work extends a line of research in analysis of passive stochastic gradient
algorithms to the finite-sample regime for Langevin dynamics.","['Luke Snow', 'Vikram Krishnamurthy']","['cs.LG', 'stat.ML']",2023-04-18 16:39:51+00:00
http://arxiv.org/abs/2304.09053v1,Bayes Hilbert Spaces for Posterior Approximation,"Performing inference in Bayesian models requires sampling algorithms to draw
samples from the posterior. This becomes prohibitively expensive as the size of
data sets increase. Constructing approximations to the posterior which are
cheap to evaluate is a popular approach to circumvent this issue. This begs the
question of what is an appropriate space to perform approximation of Bayesian
posterior measures. This manuscript studies the application of Bayes Hilbert
spaces to the posterior approximation problem. Bayes Hilbert spaces are studied
in functional data analysis in the context where observed functions are
probability density functions and their application to computational Bayesian
problems is in its infancy. This manuscript shall outline Bayes Hilbert spaces
and their connection to Bayesian computation, in particular novel connections
between Bayes Hilbert spaces, Bayesian coreset algorithms and kernel-based
distances.",['George Wynne'],"['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2023-04-18 15:17:16+00:00
http://arxiv.org/abs/2304.09050v1,Decoding Neural Activity to Assess Individual Latent State in Ecologically Valid Contexts,"There exist very few ways to isolate cognitive processes, historically
defined via highly controlled laboratory studies, in more ecologically valid
contexts. Specifically, it remains unclear as to what extent patterns of neural
activity observed under such constraints actually manifest outside the
laboratory in a manner that can be used to make an accurate inference about the
latent state, associated cognitive process, or proximal behavior of the
individual. Improving our understanding of when and how specific patterns of
neural activity manifest in ecologically valid scenarios would provide
validation for laboratory-based approaches that study similar neural phenomena
in isolation and meaningful insight into the latent states that occur during
complex tasks. We argue that domain generalization methods from the
brain-computer interface community have the potential to address this
challenge. We previously used such an approach to decode phasic neural
responses associated with visual target discrimination. Here, we extend that
work to more tonic phenomena such as internal latent states. We use data from
two highly controlled laboratory paradigms to train two separate
domain-generalized models. We apply the trained models to an ecologically valid
paradigm in which participants performed multiple, concurrent driving-related
tasks. Using the pretrained models, we derive estimates of the underlying
latent state and associated patterns of neural activity. Importantly, as the
patterns of neural activity change along the axis defined by the original
training data, we find changes in behavior and task performance consistent with
the observations from the original, laboratory paradigms. We argue that these
results lend ecological validity to those experimental designs and provide a
methodology for understanding the relationship between observed neural activity
and behavior during complex tasks.","['Stephen M. Gordon', 'Jonathan R. McDaniel', 'Kevin W. King', 'Vernon J. Lawhern', 'Jonathan Touryan']","['q-bio.NC', 'cs.LG', 'stat.ML']",2023-04-18 15:15:00+00:00
http://arxiv.org/abs/2304.08740v1,"Estimating Joint Probability Distribution With Low-Rank Tensor Decomposition, Radon Transforms and Dictionaries","In this paper, we describe a method for estimating the joint probability
density from data samples by assuming that the underlying distribution can be
decomposed as a mixture of product densities with few mixture components. Prior
works have used such a decomposition to estimate the joint density from
lower-dimensional marginals, which can be estimated more reliably with the same
number of samples. We combine two key ideas: dictionaries to represent 1-D
densities, and random projections to estimate the joint distribution from 1-D
marginals, explored separately in prior work. Our algorithm benefits from
improved sample complexity over the previous dictionary-based approach by using
1-D marginals for reconstruction. We evaluate the performance of our method on
estimating synthetic probability densities and compare it with the previous
dictionary-based approach and Gaussian Mixture Models (GMMs). Our algorithm
outperforms these other approaches in all the experimental settings.","['Pranava Singhal', 'Waqar Mirza', 'Ajit Rajwade', 'Karthik S. Gurumoorthy']","['stat.ML', 'cs.LG', 'eess.SP', '62G07']",2023-04-18 05:37:15+00:00
http://arxiv.org/abs/2304.08712v1,Impossibility of Characterizing Distribution Learning -- a simple solution to a long-standing problem,"We consider the long-standing question of finding a parameter of a class of
probability distributions that characterizes its PAC learnability. We provide a
rather surprising answer - no such parameter exists. Our techniques allow us to
show similar results for several general notions of characterizing learnability
and for several learning tasks. We show that there is no notion of dimension
that characterizes the sample complexity of learning distribution classes. We
then consider the weaker requirement of only characterizing learnability
(rather than the quantitative sample complexity function). We propose some
natural requirements for such a characterization and go on to show that there
exists no characterization of learnability that satisfies these requirements
for classes of distributions. Furthermore, we show that our results hold for
various other learning problems. In particular, we show that there is no notion
of dimension characterizing (or characterization of learnability) for any of
the tasks: classification learning for distribution classes, learning of binary
classifications w.r.t. a restricted set of marginal distributions, and
learnability of classes of real-valued functions with continuous losses.","['Tosca Lechner', 'Shai Ben-David']","['cs.LG', 'stat.ML']",2023-04-18 03:23:39+00:00
http://arxiv.org/abs/2304.08673v1,Semi-supervised Learning of Pushforwards For Domain Translation & Adaptation,"Given two probability densities on related data spaces, we seek a map pushing
one density to the other while satisfying application-dependent constraints.
For maps to have utility in a broad application space (including domain
translation, domain adaptation, and generative modeling), the map must be
available to apply on out-of-sample data points and should correspond to a
probabilistic model over the two spaces. Unfortunately, existing approaches,
which are primarily based on optimal transport, do not address these needs. In
this paper, we introduce a novel pushforward map learning algorithm that
utilizes normalizing flows to parameterize the map. We first re-formulate the
classical optimal transport problem to be map-focused and propose a learning
algorithm to select from all possible maps under the constraint that the map
minimizes a probability distance and application-specific regularizers; thus,
our method can be seen as solving a modified optimal transport problem. Once
the map is learned, it can be used to map samples from a source domain to a
target domain. In addition, because the map is parameterized as a composition
of normalizing flows, it models the empirical distributions over the two data
spaces and allows both sampling and likelihood evaluation for both data sets.
We compare our method (parOT) to related optimal transport approaches in the
context of domain adaptation and domain translation on benchmark data sets.
Finally, to illustrate the impact of our work on applied problems, we apply
parOT to a real scientific application: spectral calibration for
high-dimensional measurements from two vastly different environments","['Nishant Panda', 'Natalie Klein', 'Dominic Yang', 'Patrick Gasda', 'Diane Oyen']","['cs.LG', 'stat.ML']",2023-04-18 00:35:32+00:00
http://arxiv.org/abs/2304.08589v1,Fast and Straggler-Tolerant Distributed SGD with Reduced Computation Load,"In distributed machine learning, a central node outsources computationally
expensive calculations to external worker nodes. The properties of optimization
procedures like stochastic gradient descent (SGD) can be leveraged to mitigate
the effect of unresponsive or slow workers called stragglers, that otherwise
degrade the benefit of outsourcing the computation. This can be done by only
waiting for a subset of the workers to finish their computation at each
iteration of the algorithm. Previous works proposed to adapt the number of
workers to wait for as the algorithm evolves to optimize the speed of
convergence. In contrast, we model the communication and computation times
using independent random variables. Considering this model, we construct a
novel scheme that adapts both the number of workers and the computation load
throughout the run-time of the algorithm. Consequently, we improve the
convergence speed of distributed SGD while significantly reducing the
computation load, at the expense of a slight increase in communication load.","['Maximilian Egger', 'Serge Kas Hanna', 'Rawad Bitar']","['cs.DC', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2023-04-17 20:12:18+00:00
http://arxiv.org/abs/2304.08424v5,Long-term Forecasting with TiDE: Time-series Dense Encoder,"Recent work has shown that simple linear models can outperform several
Transformer based approaches in long term time-series forecasting. Motivated by
this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model,
Time-series Dense Encoder (TiDE), for long-term time-series forecasting that
enjoys the simplicity and speed of linear models while also being able to
handle covariates and non-linear dependencies. Theoretically, we prove that the
simplest linear analogue of our model can achieve near optimal error rate for
linear dynamical systems (LDS) under some assumptions. Empirically, we show
that our method can match or outperform prior approaches on popular long-term
time-series forecasting benchmarks while being 5-10x faster than the best
Transformer based model.","['Abhimanyu Das', 'Weihao Kong', 'Andrew Leach', 'Shaan Mathur', 'Rajat Sen', 'Rose Yu']","['stat.ML', 'cs.LG']",2023-04-17 16:46:48+00:00
http://arxiv.org/abs/2304.08342v2,NF-ULA: Langevin Monte Carlo with Normalizing Flow Prior for Imaging Inverse Problems,"Bayesian methods for solving inverse problems are a powerful alternative to
classical methods since the Bayesian approach offers the ability to quantify
the uncertainty in the solution. In recent years, data-driven techniques for
solving inverse problems have also been remarkably successful, due to their
superior representation ability. In this work, we incorporate data-based models
into a class of Langevin-based sampling algorithms for Bayesian inference in
imaging inverse problems. In particular, we introduce NF-ULA (Normalizing
Flow-based Unadjusted Langevin algorithm), which involves learning a
normalizing flow (NF) as the image prior. We use NF to learn the prior because
a tractable closed-form expression for the log prior enables the
differentiation of it using autograd libraries. Our algorithm only requires a
normalizing flow-based generative network, which can be pre-trained
independently of the considered inverse problem and the forward operator. We
perform theoretical analysis by investigating the well-posedness and
non-asymptotic convergence of the resulting NF-ULA algorithm. The efficacy of
the proposed NF-ULA algorithm is demonstrated in various image restoration
problems such as image deblurring, image inpainting, and limited-angle X-ray
computed tomography (CT) reconstruction. NF-ULA is found to perform better than
competing methods for severely ill-posed inverse problems.","['Ziruo Cai', 'Junqi Tang', 'Subhadip Mukherjee', 'Jinglai Li', 'Carola Bibiane Schönlieb', 'Xiaoqun Zhang']","['math.NA', 'cs.CV', 'cs.NA', 'stat.ML']",2023-04-17 15:03:45+00:00
http://arxiv.org/abs/2304.08309v2,Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization,"The linearized-Laplace approximation (LLA) has been shown to be effective and
efficient in constructing Bayesian neural networks. It is theoretically
compelling since it can be seen as a Gaussian process posterior with the mean
function given by the neural network's maximum-a-posteriori predictive function
and the covariance function induced by the empirical neural tangent kernel.
However, while its efficacy has been studied in large-scale tasks like image
classification, it has not been studied in sequential decision-making problems
like Bayesian optimization where Gaussian processes -- with simple mean
functions and kernels such as the radial basis function -- are the de-facto
surrogate models. In this work, we study the usefulness of the LLA in Bayesian
optimization and highlight its strong performance and flexibility. However, we
also present some pitfalls that might arise and a potential problem with the
LLA when the search space is unbounded.","['Agustinus Kristiadi', 'Alexander Immer', 'Runa Eschenhagen', 'Vincent Fortuin']","['cs.LG', 'stat.ML']",2023-04-17 14:23:43+00:00
http://arxiv.org/abs/2304.08278v1,Compositional Probabilistic and Causal Inference using Tractable Circuit Models,"Probabilistic circuits (PCs) are a class of tractable probabilistic models,
which admit efficient inference routines depending on their structural
properties. In this paper, we introduce md-vtrees, a novel structural
formulation of (marginal) determinism in structured decomposable PCs, which
generalizes previously proposed classes such as probabilistic sentential
decision diagrams. Crucially, we show how mdvtrees can be used to derive
tractability conditions and efficient algorithms for advanced inference queries
expressed as arbitrary compositions of basic probabilistic operations, such as
marginalization, multiplication and reciprocals, in a sound and generalizable
manner. In particular, we derive the first polytime algorithms for causal
inference queries such as backdoor adjustment on PCs. As a practical
instantiation of the framework, we propose MDNets, a novel PC architecture
using md-vtrees, and empirically demonstrate their application to causal
inference.","['Benjie Wang', 'Marta Kwiatkowska']","['cs.AI', 'stat.ML']",2023-04-17 13:48:16+00:00
http://arxiv.org/abs/2304.08151v1,Prediction-Oriented Bayesian Active Learning,"Information-theoretic approaches to active learning have traditionally
focused on maximising the information gathered about the model parameters, most
commonly by optimising the BALD score. We highlight that this can be suboptimal
from the perspective of predictive performance. For example, BALD lacks a
notion of an input distribution and so is prone to prioritise data of limited
relevance. To address this we propose the expected predictive information gain
(EPIG), an acquisition function that measures information gain in the space of
predictions rather than parameters. We find that using EPIG leads to stronger
predictive performance compared with BALD across a range of datasets and
models, and thus provides an appealing drop-in replacement.","['Freddie Bickford Smith', 'Andreas Kirsch', 'Sebastian Farquhar', 'Yarin Gal', 'Adam Foster', 'Tom Rainforth']","['cs.LG', 'stat.ML']",2023-04-17 10:59:57+00:00
http://arxiv.org/abs/2304.08135v1,Detection of Dense Subhypergraphs by Low-Degree Polynomials,"Detection of a planted dense subgraph in a random graph is a fundamental
statistical and computational problem that has been extensively studied in
recent years. We study a hypergraph version of the problem. Let $G^r(n,p)$
denote the $r$-uniform Erd\H{o}s-R\'enyi hypergraph model with $n$ vertices and
edge density $p$. We consider detecting the presence of a planted
$G^r(n^\gamma, n^{-\alpha})$ subhypergraph in a $G^r(n, n^{-\beta})$
hypergraph, where $0< \alpha < \beta < r-1$ and $0 < \gamma < 1$. Focusing on
tests that are degree-$n^{o(1)}$ polynomials of the entries of the adjacency
tensor, we determine the threshold between the easy and hard regimes for the
detection problem. More precisely, for $0 < \gamma < 1/2$, the threshold is
given by $\alpha = \beta \gamma$, and for $1/2 \le \gamma < 1$, the threshold
is given by $\alpha = \beta/2 + r(\gamma - 1/2)$.
  Our results are already new in the graph case $r=2$, as we consider the
subtle log-density regime where hardness based on average-case reductions is
not known. Our proof of low-degree hardness is based on a conditional variant
of the standard low-degree likelihood calculation.","['Abhishek Dhawan', 'Cheng Mao', 'Alexander S. Wein']","['cs.DS', 'cs.CC', 'math.ST', 'stat.ML', 'stat.TH']",2023-04-17 10:38:08+00:00
http://arxiv.org/abs/2304.08113v1,Analysis of Interpolating Regression Models and the Double Descent Phenomenon,"A regression model with more parameters than data points in the training data
is overparametrized and has the capability to interpolate the training data.
Based on the classical bias-variance tradeoff expressions, it is commonly
assumed that models which interpolate noisy training data are poor to
generalize. In some cases, this is not true. The best models obtained are
overparametrized and the testing error exhibits the double descent behavior as
the model order increases. In this contribution, we provide some analysis to
explain the double descent phenomenon, first reported in the machine learning
literature. We focus on interpolating models derived from the minimum norm
solution to the classical least-squares problem and also briefly discuss model
fitting using ridge regression. We derive a result based on the behavior of the
smallest singular value of the regression matrix that explains the peak
location and the double descent shape of the testing error as a function of
model order.",['Tomas McKelvey'],"['cs.LG', 'eess.SP', 'stat.ML', '68T07', 'G.3']",2023-04-17 09:44:33+00:00
http://arxiv.org/abs/2304.08054v1,Fed-MIWAE: Federated Imputation of Incomplete Data via Deep Generative Models,"Federated learning allows for the training of machine learning models on
multiple decentralized local datasets without requiring explicit data exchange.
However, data pre-processing, including strategies for handling missing data,
remains a major bottleneck in real-world federated learning deployment, and is
typically performed locally. This approach may be biased, since the
subpopulations locally observed at each center may not be representative of the
overall one. To address this issue, this paper first proposes a more consistent
approach to data standardization through a federated model. Additionally, we
propose Fed-MIWAE, a federated version of the state-of-the-art imputation
method MIWAE, a deep latent variable model for missing data imputation based on
variational autoencoders. MIWAE has the great advantage of being easily
trainable with classical federated aggregators. Furthermore, it is able to deal
with MAR (Missing At Random) data, a more challenging missing-data mechanism
than MCAR (Missing Completely At Random), where the missingness of a variable
can depend on the observed ones. We evaluate our method on multi-modal medical
imaging data and clinical scores from a simulated federated scenario with the
ADNI dataset. We compare Fed-MIWAE with respect to classical imputation
methods, either performed locally or in a centralized fashion. Fed-MIWAE allows
to achieve imputation accuracy comparable with the best centralized method,
even when local data distributions are highly heterogeneous. In addition,
thanks to the variational nature of Fed-MIWAE, our method is designed to
perform multiple imputation, allowing for the quantification of the imputation
uncertainty in the federated scenario.","['Irene Balelli', 'Aude Sportisse', 'Francesco Cremonesi', 'Pierre-Alexandre Mattei', 'Marco Lorenzi']","['stat.ML', 'cs.LG']",2023-04-17 08:14:08+00:00
http://arxiv.org/abs/2304.07993v3,In-Context Operator Learning with Data Prompts for Differential Equation Problems,"This paper introduces a new neural-network-based approach, namely In-Context
Operator Networks (ICON), to simultaneously learn operators from the prompted
data and apply it to new questions during the inference stage, without any
weight update. Existing methods are limited to using a neural network to
approximate a specific equation solution or a specific operator, requiring
retraining when switching to a new problem with different equations. By
training a single neural network as an operator learner, we can not only get
rid of retraining (even fine-tuning) the neural network for new problems, but
also leverage the commonalities shared across operators so that only a few
demos in the prompt are needed when learning a new operator. Our numerical
results show the neural network's capability as a few-shot operator learner for
a diversified type of differential equation problems, including forward and
inverse problems of ordinary differential equations (ODEs), partial
differential equations (PDEs), and mean-field control (MFC) problems, and also
show that it can generalize its learning capability to operators beyond the
training distribution.","['Liu Yang', 'Siting Liu', 'Tingwei Meng', 'Stanley J. Osher']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2023-04-17 05:22:26+00:00
http://arxiv.org/abs/2304.07949v1,Metrics for Bayesian Optimal Experiment Design under Model Misspecification,"The conventional approach to Bayesian decision-theoretic experiment design
involves searching over possible experiments to select a design that maximizes
the expected value of a specified utility function. The expectation is over the
joint distribution of all unknown variables implied by the statistical model
that will be used to analyze the collected data. The utility function defines
the objective of the experiment where a common utility function is the
information gain. This article introduces an expanded framework for this
process, where we go beyond the traditional Expected Information Gain criteria
and introduce the Expected General Information Gain which measures robustness
to the model discrepancy and Expected Discriminatory Information as a criterion
to quantify how well an experiment can detect model discrepancy. The
functionality of the framework is showcased through its application to a
scenario involving a linearized spring mass damper system and an F-16 model
where the model discrepancy is taken into account while doing Bayesian optimal
experiment design.","['Tommie A. Catanach', 'Niladri Das']","['stat.ME', 'cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2023-04-17 02:13:20+00:00
http://arxiv.org/abs/2304.07918v1,Likelihood-Based Generative Radiance Field with Latent Space Energy-Based Model for 3D-Aware Disentangled Image Representation,"We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image
generative model that incorporates 3D representation via Neural Radiance Fields
(NeRF) and 2D imaging process via differentiable volume rendering. The model
represents an image as a rendering process from 3D object to 2D image and is
conditioned on some latent variables that account for object characteristics
and are assumed to follow informative trainable energy-based prior models. We
propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i)
maximum likelihood estimation with Markov chain Monte Carlo-based inference and
(ii) variational inference with the reparameterization trick. We study our
models in the scenarios with both known and unknown camera poses. Experiments
on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D
object structures from 2D images, generate 2D images with novel views and
objects, learn from incomplete 2D images, and learn from 2D images with known
or unknown camera poses.","['Yaxuan Zhu', 'Jianwen Xie', 'Ping Li']","['cs.CV', 'stat.ML']",2023-04-16 23:44:41+00:00
http://arxiv.org/abs/2304.07896v3,Out-of-Variable Generalization for Discriminative Models,"The ability of an agent to do well in new environments is a critical aspect
of intelligence. In machine learning, this ability is known as
$\textit{strong}$ or $\textit{out-of-distribution}$ generalization. However,
merely considering differences in data distributions is inadequate for fully
capturing differences between learning environments. In the present paper, we
investigate $\textit{out-of-variable}$ generalization, which pertains to an
agent's generalization capabilities concerning environments with variables that
were never jointly observed before. This skill closely reflects the process of
animate learning: we, too, explore Nature by probing, observing, and measuring
$\textit{subsets}$ of variables at any given time. Mathematically,
$\textit{out-of-variable}$ generalization requires the efficient re-use of past
marginal information, i.e., information over subsets of previously observed
variables. We study this problem, focusing on prediction tasks across
environments that contain overlapping, yet distinct, sets of causes. We show
that after fitting a classifier, the residual distribution in one environment
reveals the partial derivative of the true generating function with respect to
the unobserved causal parent in that environment. We leverage this information
and propose a method that exhibits non-trivial out-of-variable generalization
performance when facing an overlapping, yet distinct, set of causal predictors.","['Siyuan Guo', 'Jonas Wildberger', 'Bernhard Schölkopf']","['cs.LG', 'cs.AI', 'stat.ML']",2023-04-16 21:29:54+00:00
http://arxiv.org/abs/2304.07855v1,Penalized Likelihood Inference with Survey Data,"This paper extends three Lasso inferential methods, Debiased Lasso,
$C(\alpha)$ and Selective Inference to a survey environment. We establish the
asymptotic validity of the inference procedures in generalized linear models
with survey weights and/or heteroskedasticity. Moreover, we generalize the
methods to inference on nonlinear parameter functions e.g. the average marginal
effect in survey logit models. We illustrate the effectiveness of the approach
in simulated data and Canadian Internet Use Survey 2020 data.","['Joann Jasiak', 'Purevdorj Tuvaandorj']","['econ.EM', 'math.ST', 'stat.ML', 'stat.TH']",2023-04-16 18:38:14+00:00
http://arxiv.org/abs/2304.07718v3,Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value,"Data valuation is a powerful framework for providing statistical insights
into which data are beneficial or detrimental to model training. Many
Shapley-based data valuation methods have shown promising results in various
downstream tasks, however, they are well known to be computationally
challenging as it requires training a large number of models. As a result, it
has been recognized as infeasible to apply to large datasets. To address this
issue, we propose Data-OOB, a new data valuation method for a bagging model
that utilizes the out-of-bag estimate. The proposed method is computationally
efficient and can scale to millions of data by reusing trained weak learners.
Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor
when there are $10^6$ samples to evaluate and the input dimension is 100.
Furthermore, Data-OOB has solid theoretical interpretations in that it
identifies the same important data point as the infinitesimal jackknife
influence function when two different points are compared. We conduct
comprehensive experiments using 12 classification datasets, each with thousands
of sample sizes. We demonstrate that the proposed method significantly
outperforms existing state-of-the-art data valuation methods in identifying
mislabeled data and finding a set of helpful (or harmful) data points,
highlighting the potential for applying data values in real-world applications.","['Yongchan Kwon', 'James Zou']","['cs.LG', 'stat.ML']",2023-04-16 08:03:58+00:00
http://arxiv.org/abs/2304.07689v3,Learning Empirical Bregman Divergence for Uncertain Distance Representation,"Deep metric learning techniques have been used for visual representation in
various supervised and unsupervised learning tasks through learning embeddings
of samples with deep networks. However, classic approaches, which employ a
fixed distance metric as a similarity function between two embeddings, may lead
to suboptimal performance for capturing the complex data distribution. The
Bregman divergence generalizes measures of various distance metrics and arises
throughout many fields of deep metric learning. In this paper, we first show
how deep metric learning loss can arise from the Bregman divergence. We then
introduce a novel method for learning empirical Bregman divergence directly
from data based on parameterizing the convex function underlying the Bregman
divergence with a deep learning setting. We further experimentally show that
our approach performs effectively on five popular public datasets compared to
other SOTA deep metric learning methods, particularly for pattern recognition
problems.","['Zhiyuan Li', 'Ziru Liu', 'Anna Zou', 'Anca L. Ralescu']","['cs.CV', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2023-04-16 04:16:28+00:00
http://arxiv.org/abs/2304.07665v2,Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling,"Active learning provides a framework to adaptively query the most informative
experiments towards learning an unknown black-box function. Various approaches
of active learning have been proposed in the literature, however, they either
focus on exploration or exploitation in the design space. Methods that do
consider exploration-exploitation simultaneously employ fixed or ad-hoc
measures to control the trade-off that may not be optimal. In this paper, we
develop a Bayesian hierarchical approach, referred as BHEEM, to dynamically
balance the exploration-exploitation trade-off as more data points are queried.
To sample from the posterior distribution of the trade-off parameter, We
subsequently formulate an approximate Bayesian computation approach based on
the linear dependence of queried data in the feature space. Simulated and
real-world examples show the proposed approach achieves at least 21% and 11%
average improvement when compared to pure exploration and exploitation
strategies respectively. More importantly, we note that by optimally balancing
the trade-off between exploration and exploitation, BHEEM performs better or at
least as well as either pure exploration or pure exploitation.","['Upala Junaida Islam', 'Kamran Paynabar', 'George Runger', 'Ashif Sikandar Iquebal']","['cs.LG', 'stat.ML']",2023-04-16 01:40:48+00:00
http://arxiv.org/abs/2304.07658v2,Dimensionality Reduction as Probabilistic Inference,"Dimensionality reduction (DR) algorithms compress high-dimensional data into
a lower dimensional representation while preserving important features of the
data. DR is a critical step in many analysis pipelines as it enables
visualisation, noise reduction and efficient downstream processing of the data.
In this work, we introduce the ProbDR variational framework, which interprets a
wide range of classical DR algorithms as probabilistic inference algorithms in
this framework. ProbDR encompasses PCA, CMDS, LLE, LE, MVU, diffusion maps,
kPCA, Isomap, (t-)SNE, and UMAP. In our framework, a low-dimensional latent
variable is used to construct a covariance, precision, or a graph Laplacian
matrix, which can be used as part of a generative model for the data. Inference
is done by optimizing an evidence lower bound. We demonstrate the internal
consistency of our framework and show that it enables the use of probabilistic
programming languages (PPLs) for DR. Additionally, we illustrate that the
framework facilitates reasoning about unseen data and argue that our generative
models approximate Gaussian processes (GPs) on manifolds. By providing a
unified view of DR, our framework facilitates communication, reasoning about
uncertainties, model composition, and extensions, particularly when domain
knowledge is present.","['Aditya Ravuri', 'Francisco Vargas', 'Vidhi Lalchand', 'Neil D. Lawrence']","['stat.ML', 'cs.LG']",2023-04-15 23:48:59+00:00
http://arxiv.org/abs/2304.07504v2,Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis,"We study finite-sum distributed optimization problems involving a master node
and $n-1$ local nodes under the popular $\delta$-similarity and $\mu$-strong
convexity conditions. We propose two new algorithms, SVRS and AccSVRS,
motivated by previous works. The non-accelerated SVRS method combines the
techniques of gradient sliding and variance reduction and achieves a better
communication complexity of $\tilde{\mathcal{O}}(n {+} \sqrt{n}\delta/\mu)$
compared to existing non-accelerated algorithms. Applying the framework
proposed in Katyusha X, we also develop a directly accelerated version named
AccSVRS with the $\tilde{\mathcal{O}}(n {+} n^{3/4}\sqrt{\delta/\mu})$
communication complexity. In contrast to existing results, our complexity
bounds are entirely smoothness-free and exhibit superiority in ill-conditioned
cases. Furthermore, we establish a nearly matched lower bound to verify the
tightness of our AccSVRS method.","['Dachao Lin', 'Yuze Han', 'Haishan Ye', 'Zhihua Zhang']","['cs.LG', 'math.OC', 'stat.ML']",2023-04-15 08:18:47+00:00
http://arxiv.org/abs/2304.07472v3,Efficient Convex Algorithms for Universal Kernel Learning,"The accuracy and complexity of machine learning algorithms based on kernel
optimization are determined by the set of kernels over which they are able to
optimize. An ideal set of kernels should: admit a linear parameterization (for
tractability); be dense in the set of all kernels (for robustness); be
universal (for accuracy). Recently, a framework was proposed for using positive
matrices to parameterize a class of positive semi-separable kernels. Although
this class can be shown to meet all three criteria, previous algorithms for
optimization of such kernels were limited to classification and furthermore
relied on computationally complex Semidefinite Programming (SDP) algorithms. In
this paper, we pose the problem of learning semiseparable kernels as a minimax
optimization problem and propose a SVD-QCQP primal-dual algorithm which
dramatically reduces the computational complexity as compared with previous
SDP-based approaches. Furthermore, we provide an efficient implementation of
this algorithm for both classification and regression -- an implementation
which enables us to solve problems with 100 features and up to 30,000 datums.
Finally, when applied to benchmark data, the algorithm demonstrates the
potential for significant improvement in accuracy over typical (but non-convex)
approaches such as Neural Nets and Random Forest with similar or better
computation time.","['Aleksandr Talitckii', 'Brendon K. Colbert', 'Matthew M. Peet']","['stat.ML', 'cs.LG']",2023-04-15 04:57:37+00:00
http://arxiv.org/abs/2304.07451v1,Multivariate regression modeling in integrative analysis via sparse regularization,"The multivariate regression model basically offers the analysis of a single
dataset with multiple responses. However, such a single-dataset analysis often
leads to unsatisfactory results. Integrative analysis is an effective method to
pool useful information from multiple independent datasets and provides better
performance than single-dataset analysis. In this study, we propose a
multivariate regression modeling in integrative analysis. The integration is
achieved by sparse estimation that performs variable and group selection. Based
on the idea of alternating direction method of multipliers, we develop its
computational algorithm that enjoys the convergence property. The performance
of the proposed method is demonstrated through Monte Carlo simulation and
analyzing wastewater treatment data with microbe measurements.","['Shuichi Kawano', 'Toshikazu Fukushima', 'Junichi Nakagawa', 'Mamoru Oshiki']","['stat.ME', 'stat.ML']",2023-04-15 02:27:51+00:00
http://arxiv.org/abs/2304.07407v2,Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents,"Motivated by a number of real-world applications from domains like healthcare
and sustainable transportation, in this paper we study a scenario of repeated
principal-agent games within a multi-armed bandit (MAB) framework, where: the
principal gives a different incentive for each bandit arm, the agent picks a
bandit arm to maximize its own expected reward plus incentive, and the
principal observes which arm is chosen and receives a reward (different than
that of the agent) for the chosen arm. Designing policies for the principal is
challenging because the principal cannot directly observe the reward that the
agent receives for their chosen actions, and so the principal cannot directly
learn the expected reward using existing estimation techniques. As a result,
the problem of designing policies for this scenario, as well as similar ones,
remains mostly unexplored. In this paper, we construct a policy that achieves a
low regret (i.e., square-root regret up to a log factor) in this scenario for
the case where the agent has perfect-knowledge about its own expected rewards
for each bandit arm. We design our policy by first constructing an estimator
for the agent's expected reward for each bandit arm. Since our estimator uses
as data the sequence of incentives offered and subsequently chosen arms, the
principal's estimation can be regarded as an analogy of online inverse
optimization in MAB's. Next we construct a policy that we prove achieves a low
regret by deriving finite-sample concentration bounds for our estimator. We
conclude with numerical simulations demonstrating the applicability of our
policy to real-life setting from collaborative transportation planning.","['Ilgin Dogan', 'Zuo-Jun Max Shen', 'Anil Aswani']","['cs.LG', 'cs.AI', 'cs.GT', 'stat.ML']",2023-04-14 21:57:16+00:00
http://arxiv.org/abs/2304.07401v2,Bayesian Inference on Brain-Computer Interfaces via GLASS,"Brain-computer interfaces (BCIs), particularly the P300 BCI, facilitate
direct communication between the brain and computers. The fundamental
statistical problem in P300 BCIs lies in classifying target and non-target
stimuli based on electroencephalogram (EEG) signals. However, the low
signal-to-noise ratio (SNR) and complex spatial/temporal correlations of EEG
signals present challenges in modeling and computation, especially for
individuals with severe physical disabilities-BCI's primary users. To address
these challenges, we introduce a novel Gaussian Latent channel model with
Sparse time-varying effects (GLASS) under a fully Bayesian framework. GLASS is
built upon a constrained multinomial logistic regression particularly designed
for the imbalanced target and non-target stimuli. The novel latent channel
decomposition efficiently alleviates strong spatial correlations between EEG
channels, while the soft-thresholded Gaussian process (STGP) prior ensures
sparse and smooth time-varying effects. We demonstrate GLASS substantially
improves BCI's performance in participants with amyotrophic lateral sclerosis
(ALS) and identifies important EEG channels (PO8, Oz, PO7, and Pz) in parietal
and occipital regions that align with existing literature. For broader
accessibility, we develop an efficient gradient-based variational inference
(GBVI) algorithm for posterior computation and provide a user-friendly Python
module available at https://github.com/BangyaoZhao/GLASS.","['Bangyao Zhao', 'Jane E. Huggins', 'Jian Kang']","['stat.AP', 'stat.ML']",2023-04-14 21:29:00+00:00
http://arxiv.org/abs/2304.07358v1,Exact Subspace Diffusion for Decentralized Multitask Learning,"Classical paradigms for distributed learning, such as federated or
decentralized gradient descent, employ consensus mechanisms to enforce
homogeneity among agents. While these strategies have proven effective in
i.i.d. scenarios, they can result in significant performance degradation when
agents follow heterogeneous objectives or data. Distributed strategies for
multitask learning, on the other hand, induce relationships between agents in a
more nuanced manner, and encourage collaboration without enforcing consensus.
We develop a generalization of the exact diffusion algorithm for subspace
constrained multitask learning over networks, and derive an accurate expression
for its mean-squared deviation when utilizing noisy gradient approximations. We
verify numerically the accuracy of the predicted performance expressions, as
well as the improved performance of the proposed approach over alternatives
based on approximate projections.","['Shreya Wadehra', 'Roula Nassif', 'Stefan Vlaski']","['cs.LG', 'cs.DC', 'eess.SP', 'math.OC', 'stat.ML']",2023-04-14 19:42:19+00:00
http://arxiv.org/abs/2304.07347v2,Differential geometry with extreme eigenvalues in the positive semidefinite cone,"Differential geometric approaches to the analysis and processing of data in
the form of symmetric positive definite (SPD) matrices have had notable
successful applications to numerous fields including computer vision, medical
imaging, and machine learning. The dominant geometric paradigm for such
applications has consisted of a few Riemannian geometries associated with
spectral computations that are costly at high scale and in high dimensions. We
present a route to a scalable geometric framework for the analysis and
processing of SPD-valued data based on the efficient computation of extreme
generalized eigenvalues through the Hilbert and Thompson geometries of the
semidefinite cone. We explore a particular geodesic space structure based on
Thompson geometry in detail and establish several properties associated with
this structure. Furthermore, we define a novel iterative mean of SPD matrices
based on this geometry and prove its existence and uniqueness for a given
finite collection of points. Finally, we state and prove a number of desirable
properties that are satisfied by this mean.","['Cyrus Mostajeran', 'Nathaël Da Costa', 'Graham Van Goffrier', 'Rodolphe Sepulchre']","['math.DG', 'stat.CO', 'stat.ML']",2023-04-14 18:37:49+00:00
http://arxiv.org/abs/2304.07288v2,Cross-Entropy Loss Functions: Theoretical Analysis and Applications,"Cross-entropy is a widely used loss function in applications. It coincides
with the logistic loss applied to the outputs of a neural network, when the
softmax is used. But, what guarantees can we rely on when using cross-entropy
as a surrogate loss? We present a theoretical analysis of a broad family of
loss functions, comp-sum losses, that includes cross-entropy (or logistic
loss), generalized cross-entropy, the mean absolute error and other
cross-entropy-like loss functions. We give the first $H$-consistency bounds for
these loss functions. These are non-asymptotic guarantees that upper bound the
zero-one loss estimation error in terms of the estimation error of a surrogate
loss, for the specific hypothesis set $H$ used. We further show that our bounds
are tight. These bounds depend on quantities called minimizability gaps. To
make them more explicit, we give a specific analysis of these gaps for comp-sum
losses. We also introduce a new family of loss functions, smooth adversarial
comp-sum losses, that are derived from their comp-sum counterparts by adding in
a related smooth term. We show that these loss functions are beneficial in the
adversarial setting by proving that they admit $H$-consistency bounds. This
leads to new adversarial robustness algorithms that consist of minimizing a
regularized smooth adversarial comp-sum loss. While our main purpose is a
theoretical analysis, we also present an extensive empirical analysis comparing
comp-sum losses. We further report the results of a series of experiments
demonstrating that our adversarial robustness algorithms outperform the current
state-of-the-art, while also achieving a superior non-adversarial accuracy.","['Anqi Mao', 'Mehryar Mohri', 'Yutao Zhong']","['cs.LG', 'stat.ML']",2023-04-14 17:58:23+00:00
http://arxiv.org/abs/2304.07278v2,Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning,"This paper studies reward-agnostic exploration in reinforcement learning (RL)
-- a scenario where the learner is unware of the reward functions during the
exploration stage -- and designs an algorithm that improves over the state of
the art. More precisely, consider a finite-horizon inhomogeneous Markov
decision process with $S$ states, $A$ actions, and horizon length $H$, and
suppose that there are no more than a polynomial number of given reward
functions of interest. By collecting an order of \begin{align*}
  \frac{SAH^3}{\varepsilon^2} \text{ sample episodes (up to log factor)}
\end{align*} without guidance of the reward information, our algorithm is able
to find $\varepsilon$-optimal policies for all these reward functions, provided
that $\varepsilon$ is sufficiently small. This forms the first reward-agnostic
exploration scheme in this context that achieves provable minimax optimality.
Furthermore, once the sample size exceeds $\frac{S^2AH^3}{\varepsilon^2}$
episodes (up to log factor), our algorithm is able to yield $\varepsilon$
accuracy for arbitrarily many reward functions (even when they are
adversarially designed), a task commonly dubbed as ``reward-free exploration.''
The novelty of our algorithm design draws on insights from offline RL: the
exploration scheme attempts to maximize a critical reward-agnostic quantity
that dictates the performance of offline RL, while the policy learning paradigm
leverages ideas from sample-optimal offline RL paradigms.","['Gen Li', 'Yuling Yan', 'Yuxin Chen', 'Jianqing Fan']","['cs.LG', 'cs.IT', 'cs.SY', 'eess.SY', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2023-04-14 17:46:49+00:00
http://arxiv.org/abs/2304.07245v1,Machine Learning-Based Multi-Objective Design Exploration Of Flexible Disc Elements,"Design exploration is an important step in the engineering design process.
This involves the search for design/s that meet the specified design criteria
and accomplishes the predefined objective/s. In recent years, machine
learning-based approaches have been widely used in engineering design problems.
This paper showcases Artificial Neural Network (ANN) architecture applied to an
engineering design problem to explore and identify improved design solutions.
The case problem of this study is the design of flexible disc elements used in
disc couplings. We are required to improve the design of the disc elements by
lowering the mass and stress without lowering the torque transmission and
misalignment capability. To accomplish this objective, we employ ANN coupled
with genetic algorithm in the design exploration step to identify designs that
meet the specified criteria (torque and misalignment) while having minimum mass
and stress. The results are comparable to the optimized results obtained from
the traditional response surface method. This can have huge advantage when we
are evaluating conceptual designs against multiple conflicting requirements.","['Gehendra Sharma', 'Sungkwang Mun', 'Nayeon Lee', 'Luke Peterson', 'Daniela Tellkamp', 'Anand Balu Nellippallil']","['cs.NE', 'cs.LG', 'stat.ML']",2023-04-14 16:48:51+00:00
http://arxiv.org/abs/2304.07235v4,Mapping of attention mechanisms to a generalized Potts model,"Transformers are neural networks that revolutionized natural language
processing and machine learning. They process sequences of inputs, like words,
using a mechanism called self-attention, which is trained via masked language
modeling (MLM). In MLM, a word is randomly masked in an input sequence, and the
network is trained to predict the missing word. Despite the practical success
of transformers, it remains unclear what type of data distribution
self-attention can learn efficiently. Here, we show analytically that if one
decouples the treatment of word positions and embeddings, a single layer of
self-attention learns the conditionals of a generalized Potts model with
interactions between sites and Potts colors. Moreover, we show that training
this neural network is exactly equivalent to solving the inverse Potts problem
by the so-called pseudo-likelihood method, well known in statistical physics.
Using this mapping, we compute the generalization error of self-attention in a
model scenario analytically using the replica method.","['Riccardo Rende', 'Federica Gerace', 'Alessandro Laio', 'Sebastian Goldt']","['cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.CL', 'stat.ML']",2023-04-14 16:32:56+00:00
http://arxiv.org/abs/2304.07048v2,Wasserstein PAC-Bayes Learning: Exploiting Optimisation Guarantees to Explain Generalisation,"PAC-Bayes learning is an established framework to both assess the
generalisation ability of learning algorithms, and design new learning
algorithm by exploiting generalisation bounds as training objectives. Most of
the exisiting bounds involve a \emph{Kullback-Leibler} (KL) divergence, which
fails to capture the geometric properties of the loss function which are often
useful in optimisation. We address this by extending the emerging
\emph{Wasserstein PAC-Bayes} theory. We develop new PAC-Bayes bounds with
Wasserstein distances replacing the usual KL, and demonstrate that sound
optimisation guarantees translate to good generalisation abilities. In
particular we provide generalisation bounds for the \emph{Bures-Wasserstein
SGD} by exploiting its optimisation properties.","['Maxime Haddouche', 'Benjamin Guedj']","['stat.ML', 'cs.LG', 'math.OC']",2023-04-14 10:48:48+00:00
http://arxiv.org/abs/2304.07045v1,Ledoit-Wolf linear shrinkage with unknown mean,"This work addresses large dimensional covariance matrix estimation with
unknown mean. The empirical covariance estimator fails when dimension and
number of samples are proportional and tend to infinity, settings known as
Kolmogorov asymptotics. When the mean is known, Ledoit and Wolf (2004) proposed
a linear shrinkage estimator and proved its convergence under those
asymptotics. To the best of our knowledge, no formal proof has been proposed
when the mean is unknown. To address this issue, we propose a new estimator and
prove its quadratic convergence under the Ledoit and Wolf assumptions. Finally,
we show empirically that it outperforms other standard estimators.","['Benoit Oriol', 'Alexandre Miot']","['math.ST', 'q-fin.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2023-04-14 10:40:30+00:00
http://arxiv.org/abs/2304.07025v1,Continuous time recurrent neural networks: overview and application to forecasting blood glucose in the intensive care unit,"Irregularly measured time series are common in many of the applied settings
in which time series modelling is a key statistical tool, including medicine.
This provides challenges in model choice, often necessitating imputation or
similar strategies. Continuous time autoregressive recurrent neural networks
(CTRNNs) are a deep learning model that account for irregular observations
through incorporating continuous evolution of the hidden states between
observations. This is achieved using a neural ordinary differential equation
(ODE) or neural flow layer. In this manuscript, we give an overview of these
models, including the varying architectures that have been proposed to account
for issues such as ongoing medical interventions. Further, we demonstrate the
application of these models to probabilistic forecasting of blood glucose in a
critical care setting using electronic medical record and simulated data. The
experiments confirm that addition of a neural ODE or neural flow layer
generally improves the performance of autoregressive recurrent neural networks
in the irregular measurement setting. However, several CTRNN architecture are
outperformed by an autoregressive gradient boosted tree model (Catboost), with
only a long short-term memory (LSTM) and neural ODE based architecture
(ODE-LSTM) achieving comparable performance on probabilistic forecasting
metrics such as the continuous ranked probability score (ODE-LSTM:
0.118$\pm$0.001; Catboost: 0.118$\pm$0.001), ignorance score (0.152$\pm$0.008;
0.149$\pm$0.002) and interval score (175$\pm$1; 176$\pm$1).","['Oisin Fitzgerald', 'Oscar Perez-Concha', 'Blanca Gallego-Luxan', 'Alejandro Metke-Jimenez', 'Lachlan Rudd', 'Louisa Jorm']","['stat.ML', 'cs.LG', 'stat.AP']",2023-04-14 09:39:06+00:00
http://arxiv.org/abs/2304.07003v1,Detection and Estimation of Structural Breaks in High-Dimensional Functional Time Series,"In this paper, we consider detecting and estimating breaks in heterogeneous
mean functions of high-dimensional functional time series which are allowed to
be cross-sectionally correlated and temporally dependent. A new test statistic
combining the functional CUSUM statistic and power enhancement component is
proposed with asymptotic null distribution theory comparable to the
conventional CUSUM theory derived for a single functional time series. In
particular, the extra power enhancement component enlarges the region where the
proposed test has power, and results in stable power performance when breaks
are sparse in the alternative hypothesis. Furthermore, we impose a latent group
structure on the subjects with heterogeneous break points and introduce an
easy-to-implement clustering algorithm with an information criterion to
consistently estimate the unknown group number and membership. The estimated
group structure can subsequently improve the convergence property of the
post-clustering break point estimate. Monte-Carlo simulation studies and
empirical applications show that the proposed estimation and testing techniques
have satisfactory performance in finite samples.","['Degui Li', 'Runze Li', 'Han Lin Shang']","['stat.ME', 'econ.EM', 'math.ST', 'stat.ML', 'stat.TH']",2023-04-14 08:56:31+00:00
http://arxiv.org/abs/2304.06993v2,Dimension-free mixing times of Gibbs samplers for Bayesian hierarchical models,"Gibbs samplers are popular algorithms to approximate posterior distributions
arising from Bayesian hierarchical models. Despite their popularity and good
empirical performances, however, there are still relatively few quantitative
results on their convergence properties, e.g. much less than for gradient-based
sampling methods. In this work we analyse the behaviour of total variation
mixing times of Gibbs samplers targeting hierarchical models using tools from
Bayesian asymptotics. We obtain dimension-free convergence results under random
data-generating assumptions, for a broad class of two-level models with generic
likelihood function. Specific examples with Gaussian, binomial and categorical
likelihoods are discussed.","['Filippo Ascolani', 'Giacomo Zanella']","['stat.CO', 'math.ST', 'stat.ML', 'stat.TH']",2023-04-14 08:30:40+00:00
http://arxiv.org/abs/2304.06833v3,Estimate-Then-Optimize versus Integrated-Estimation-Optimization versus Sample Average Approximation: A Stochastic Dominance Perspective,"In data-driven stochastic optimization, model parameters of the underlying
distribution need to be estimated from data in addition to the optimization
task. Recent literature considers integrating the estimation and optimization
processes by selecting model parameters that lead to the best empirical
objective performance. This integrated approach, which we call
integrated-estimation-optimization (IEO), can be readily shown to outperform
simple estimate-then-optimize (ETO) when the model is misspecified. In this
paper, we show that a reverse behavior appears when the model class is
well-specified and there is sufficient data. Specifically, for a general class
of nonlinear stochastic optimization problems, we show that simple ETO
outperforms IEO asymptotically when the model class covers the ground truth, in
the strong sense of stochastic dominance of the regret. Namely, the entire
distribution of the regret, not only its mean or other moments, is always
better for ETO compared to IEO. Our results also apply to constrained,
contextual optimization problems where the decision depends on observed
features. Whenever applicable, we also demonstrate how standard sample average
approximation (SAA) performs the worst when the model class is well-specified
in terms of regret, and best when it is misspecified. Finally, we provide
experimental results to support our theoretical comparisons and illustrate when
our insights hold in finite-sample regimes and under various degrees of
misspecification.","['Adam N. Elmachtoub', 'Henry Lam', 'Haofeng Zhang', 'Yunfan Zhao']","['stat.ML', 'cs.LG', 'stat.ME']",2023-04-13 21:54:53+00:00
http://arxiv.org/abs/2304.06821v1,Ranking from Pairwise Comparisons in General Graphs and Graphs with Locality,"This technical report studies the problem of ranking from pairwise
comparisons in the classical Bradley-Terry-Luce (BTL) model, with a focus on
score estimation. For general graphs, we show that, with sufficiently many
samples, maximum likelihood estimation (MLE) achieves an entrywise estimation
error matching the Cram\'er-Rao lower bound, which can be stated in terms of
effective resistances; the key to our analysis is a connection between
statistical estimation and iterative optimization by preconditioned gradient
descent. We are also particularly interested in graphs with locality, where
only nearby items can be connected by edges; our analysis identifies conditions
under which locality does not hurt, i.e. comparing the scores between a pair of
items that are far apart in the graph is nearly as easy as comparing a pair of
nearby items. We further explore divide-and-conquer algorithms that can
provably achieve similar guarantees even in the regime with the sparsest
samples, while enjoying certain computational advantages. Numerical results
validate our theory and confirm the efficacy of the proposed algorithms.",['Yanxi Chen'],"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2023-04-13 21:14:30+00:00
http://arxiv.org/abs/2304.06808v2,Active Cost-aware Labeling of Streaming Data,"We study actively labeling streaming data, where an active learner is faced
with a stream of data points and must carefully choose which of these points to
label via an expensive experiment. Such problems frequently arise in
applications such as healthcare and astronomy. We first study a setting when
the data's inputs belong to one of $K$ discrete distributions and formalize
this problem via a loss that captures the labeling cost and the prediction
error. When the labeling cost is $B$, our algorithm, which chooses to label a
point if the uncertainty is larger than a time and cost dependent threshold,
achieves a worst-case upper bound of $\widetilde{O}(B^{\frac{1}{3}}
K^{\frac{1}{3}} T^{\frac{2}{3}})$ on the loss after $T$ rounds. We also provide
a more nuanced upper bound which demonstrates that the algorithm can adapt to
the arrival pattern, and achieves better performance when the arrival pattern
is more favorable. We complement both upper bounds with matching lower bounds.
We next study this problem when the inputs belong to a continuous domain and
the output of the experiment is a smooth function with bounded RKHS norm. After
$T$ rounds in $d$ dimensions, we show that the loss is bounded by
$\widetilde{O}(B^{\frac{1}{d+3}} T^{\frac{d+2}{d+3}})$ in an RKHS with a
squared exponential kernel and by $\widetilde{O}(B^{\frac{1}{2d+3}}
T^{\frac{2d+2}{2d+3}})$ in an RKHS with a Mat\'ern kernel. Our empirical
evaluation demonstrates that our method outperforms other baselines in several
synthetic experiments and two real experiments in medicine and astronomy.","['Ting Cai', 'Kirthevasan Kandasamy']","['cs.LG', 'stat.ML']",2023-04-13 20:23:27+00:00
http://arxiv.org/abs/2304.06803v2,Sample Average Approximation for Black-Box VI,"We present a novel approach for black-box VI that bypasses the difficulties
of stochastic gradient ascent, including the task of selecting step-sizes. Our
approach involves using a sequence of sample average approximation (SAA)
problems. SAA approximates the solution of stochastic optimization problems by
transforming them into deterministic ones. We use quasi-Newton methods and line
search to solve each deterministic optimization problem and present a heuristic
policy to automate hyperparameter selection. Our experiments show that our
method simplifies the VI problem and achieves faster performance than existing
methods.","['Javier Burroni', 'Justin Domke', 'Daniel Sheldon']","['cs.LG', 'math.OC', 'stat.ML']",2023-04-13 20:04:47+00:00
http://arxiv.org/abs/2304.06787v4,"A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions","We present the first $\varepsilon$-differentially private, computationally
efficient algorithm that estimates the means of product distributions over
$\{0,1\}^d$ accurately in total-variation distance, whilst attaining the
optimal sample complexity to within polylogarithmic factors. The prior work had
either solved this problem efficiently and optimally under weaker notions of
privacy, or had solved it optimally while having exponential running times.",['Vikrant Singhal'],"['cs.DS', 'cs.CR', 'cs.LG', 'stat.ML']",2023-04-13 19:13:19+00:00
http://arxiv.org/abs/2304.06767v4,RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment,"Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially serious
consequences. Consequently, aligning these models with human ethics and
preferences is an essential step toward ensuring their responsible and
effective deployment in real-world applications. Prior research has primarily
employed Reinforcement Learning from Human Feedback (RLHF) to address this
problem, where generative models are fine-tuned with RL algorithms guided by a
human-feedback-informed reward model. However, the inefficiencies and
instabilities associated with RL algorithms frequently present substantial
obstacles to the successful alignment, necessitating the development of a more
robust and streamlined approach. To this end, we introduce a new framework,
Reward rAnked FineTuning (RAFT), designed to align generative models
effectively. Utilizing a reward model and a sufficient number of samples, our
approach selects the high-quality samples, discarding those that exhibit
undesired behavior, and subsequently enhancing the model by fine-tuning on
these filtered samples. Our studies show that RAFT can effectively improve the
model performance in both reward learning and other automated metrics in both
large language models and diffusion models.","['Hanze Dong', 'Wei Xiong', 'Deepanshu Goyal', 'Yihan Zhang', 'Winnie Chow', 'Rui Pan', 'Shizhe Diao', 'Jipeng Zhang', 'Kashun Shum', 'Tong Zhang']","['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'stat.ML']",2023-04-13 18:22:40+00:00
http://arxiv.org/abs/2304.06704v1,How Will It Drape Like? Capturing Fabric Mechanics from Depth Images,"We propose a method to estimate the mechanical parameters of fabrics using a
casual capture setup with a depth camera. Our approach enables to create
mechanically-correct digital representations of real-world textile materials,
which is a fundamental step for many interactive design and engineering
applications. As opposed to existing capture methods, which typically require
expensive setups, video sequences, or manual intervention, our solution can
capture at scale, is agnostic to the optical appearance of the textile, and
facilitates fabric arrangement by non-expert operators. To this end, we propose
a sim-to-real strategy to train a learning-based framework that can take as
input one or multiple images and outputs a full set of mechanical parameters.
Thanks to carefully designed data augmentation and transfer learning protocols,
our solution generalizes to real images despite being trained only on synthetic
data, hence successfully closing the sim-to-real loop.Key in our work is to
demonstrate that evaluating the regression accuracy based on the similarity at
parameter space leads to an inaccurate distances that do not match the human
perception. To overcome this, we propose a novel metric for fabric drape
similarity that operates on the image domain instead on the parameter space,
allowing us to evaluate our estimation within the context of a similarity rank.
We show that out metric correlates with human judgments about the perception of
drape similarity, and that our model predictions produce perceptually accurate
results compared to the ground truth parameters.","['Carlos Rodriguez-Pardo', 'Melania Prieto-Martin', 'Dan Casas', 'Elena Garces']","['cs.CV', 'cs.AI', 'cs.CG', 'cs.GR', 'stat.ML', '68T45 (Primary) 68U05 (Secondary)', 'I.5.4; I.3.5; I.4.8; I.5.1']",2023-04-13 17:54:08+00:00
http://arxiv.org/abs/2304.06686v3,OKRidge: Scalable Optimal k-Sparse Ridge Regression,"We consider an important problem in scientific discovery, namely identifying
sparse governing equations for nonlinear dynamical systems. This involves
solving sparse ridge regression problems to provable optimality in order to
determine which terms drive the underlying dynamics. We propose a fast
algorithm, OKRidge, for sparse ridge regression, using a novel lower bound
calculation involving, first, a saddle point formulation, and from there,
either solving (i) a linear system or (ii) using an ADMM-based approach, where
the proximal operators can be efficiently evaluated by solving another linear
system and an isotonic regression problem. We also propose a method to
warm-start our solver, which leverages a beam search. Experimentally, our
methods attain provable optimality with run times that are orders of magnitude
faster than those of the existing MIP formulations solved by the commercial
solver Gurobi.","['Jiachang Liu', 'Sam Rosen', 'Chudi Zhong', 'Cynthia Rudin']","['cs.LG', 'stat.ML']",2023-04-13 17:34:44+00:00
http://arxiv.org/abs/2304.06670v1,Do deep neural networks have an inbuilt Occam's razor?,"The remarkable performance of overparameterized deep neural networks (DNNs)
must arise from an interplay between network architecture, training algorithms,
and structure in the data. To disentangle these three components, we apply a
Bayesian picture, based on the functions expressed by a DNN, to supervised
learning. The prior over functions is determined by the network, and is varied
by exploiting a transition between ordered and chaotic regimes. For Boolean
function classification, we approximate the likelihood using the error spectrum
of functions on data. When combined with the prior, this accurately predicts
the posterior, measured for DNNs trained with stochastic gradient descent. This
analysis reveals that structured data, combined with an intrinsic Occam's
razor-like inductive bias towards (Kolmogorov) simple functions that is strong
enough to counteract the exponential growth of the number of functions with
complexity, is a key to the success of DNNs.","['Chris Mingard', 'Henry Rees', 'Guillermo Valle-Pérez', 'Ard A. Louis']","['cs.LG', 'cs.AI', 'stat.ML']",2023-04-13 16:58:21+00:00
http://arxiv.org/abs/2304.06616v2,Convergence rate of Tsallis entropic regularized optimal transport,"In this paper, we study the Tsallis entropic regularized optimal transport in
the continuous setting and establish fundamental results such as the
$\Gamma$-convergence of the Tsallis regularized optimal transport to the
Monge--Kantorovich problem as the regularization parameter tends to zero. In
addition, using the quantization and shadow arguments developed by
Eckstein--Nutz, we derive the convergence rate of the Tsallis entropic
regularization and provide explicit constants. Furthermore, we compare these
results with the well-known case of the Kullback--Leibler (KL) divergence
regularization and show that the KL regularization achieves the fastest
convergence rate in the Tsallis framework.","['Takeshi Suguro', 'Toshiaki Yachimura']","['math.OC', 'math.FA', 'math.PR', 'stat.ML']",2023-04-13 15:37:14+00:00
http://arxiv.org/abs/2304.06592v1,Bayesian Inference for Jump-Diffusion Approximations of Biochemical Reaction Networks,"Biochemical reaction networks are an amalgamation of reactions where each
reaction represents the interaction of different species. Generally, these
networks exhibit a multi-scale behavior caused by the high variability in
reaction rates and abundances of species. The so-called jump-diffusion
approximation is a valuable tool in the modeling of such systems. The
approximation is constructed by partitioning the reaction network into a fast
and slow subgroup of fast and slow reactions, respectively. This enables the
modeling of the dynamics using a Langevin equation for the fast group, while a
Markov jump process model is kept for the dynamics of the slow group. Most
often biochemical processes are poorly characterized in terms of parameters and
population states. As a result of this, methods for estimating hidden
quantities are of significant interest. In this paper, we develop a tractable
Bayesian inference algorithm based on Markov chain Monte Carlo. The presented
blocked Gibbs particle smoothing algorithm utilizes a sequential Monte Carlo
method to estimate the latent states and performs distinct Gibbs steps for the
parameters of a biochemical reaction network, by exploiting a jump-diffusion
approximation model. The presented blocked Gibbs sampler is based on the two
distinct steps of state inference and parameter inference. We estimate states
via a continuous-time forward-filtering backward-smoothing procedure in the
state inference step. By utilizing bootstrap particle filtering within a
backward-smoothing procedure, we sample a smoothing trajectory. For estimating
the hidden parameters, we utilize a separate Markov chain Monte Carlo sampler
within the Gibbs sampler that uses the path-wise continuous-time representation
of the reaction counters. Finally, the algorithm is numerically evaluated for a
partially observed multi-scale birth-death process example.","['Derya Altıntan', 'Bastian Alt', 'Heinz Koeppl']","['q-bio.QM', 'q-bio.MN', 'stat.ME', 'stat.ML']",2023-04-13 14:57:22+00:00
http://arxiv.org/abs/2304.06574v1,Bayes classifier cannot be learned from noisy responses with unknown noise rates,"Training a classifier with noisy labels typically requires the learner to
specify the distribution of label noise, which is often unknown in practice.
Although there have been some recent attempts to relax that requirement, we
show that the Bayes decision rule is unidentified in most classification
problems with noisy labels. This suggests it is generally not possible to
bypass/relax the requirement. In the special cases in which the Bayes decision
rule is identified, we develop a simple algorithm to learn the Bayes decision
rule, that does not require knowledge of the noise distribution.","['Soham Bakshi', 'Subha Maity']","['stat.ML', 'cs.LG']",2023-04-13 14:35:57+00:00
http://arxiv.org/abs/2304.06569v2,counterfactuals: An R Package for Counterfactual Explanation Methods,"Counterfactual explanation methods provide information on how feature values
of individual observations must be changed to obtain a desired prediction.
Despite the increasing amount of proposed methods in research, only a few
implementations exist whose interfaces and requirements vary widely. In this
work, we introduce the counterfactuals R package, which provides a modular and
unified R6-based interface for counterfactual explanation methods. We
implemented three existing counterfactual explanation methods and propose some
optional methodological extensions to generalize these methods to different
scenarios and to make them more comparable. We explain the structure and
workflow of the package using real use cases and show how to integrate
additional counterfactual explanation methods into the package. In addition, we
compared the implemented methods for a variety of models and datasets with
regard to the quality of their counterfactual explanations and their runtime
behavior.","['Susanne Dandl', 'Andreas Hofheinz', 'Martin Binder', 'Bernd Bischl', 'Giuseppe Casalicchio']","['stat.ML', 'cs.LG', 'stat.CO']",2023-04-13 14:29:15+00:00
http://arxiv.org/abs/2304.06549v2,Non-asymptotic convergence bounds for Sinkhorn iterates and their gradients: a coupling approach,"Computational optimal transport (OT) has recently emerged as a powerful
framework with applications in various fields. In this paper we focus on a
relaxation of the original OT problem, the entropic OT problem, which allows to
implement efficient and practical algorithmic solutions, even in high
dimensional settings. This formulation, also known as the Schr\""odinger Bridge
problem, notably connects with Stochastic Optimal Control (SOC) and can be
solved with the popular Sinkhorn algorithm. In the case of discrete-state
spaces, this algorithm is known to have exponential convergence; however,
achieving a similar rate of convergence in a more general setting is still an
active area of research. In this work, we analyze the convergence of the
Sinkhorn algorithm for probability measures defined on the $d$-dimensional
torus $\mathbb{T}_L^d$, that admit densities with respect to the Haar measure
of $\mathbb{T}_L^d$. In particular, we prove pointwise exponential convergence
of Sinkhorn iterates and their gradient. Our proof relies on the connection
between these iterates and the evolution along the Hamilton-Jacobi-Bellman
equations of value functions obtained from SOC-problems. Our approach is novel
in that it is purely probabilistic and relies on coupling by reflection
techniques for controlled diffusions on the torus.","['Giacomo Greco', 'Maxence Noble', 'Giovanni Conforti', 'Alain Durmus']","['math.PR', 'math.OC', 'stat.ML', '49Q22, 93E20 (Primary) 49N05, 90C25, 47D07']",2023-04-13 13:58:25+00:00
http://arxiv.org/abs/2304.06522v1,Signal identification without signal formulation,"When there are signals and noises, physicists try to identify signals by
modeling them, whereas statisticians oppositely try to model noise to identify
signals. In this study, we applied the statisticians' concept of signal
detection of physics data with small-size samples and high dimensions without
modeling the signals. Most of the data in nature, whether noises or signals,
are assumed to be generated by dynamical systems; thus, there is essentially no
distinction between these generating processes. We propose that the correlation
length of a dynamical system and the number of samples are crucial for the
practical definition of noise variables among the signal variables generated by
such a system. Since variables with short-term correlations reach normal
distributions faster as the number of samples decreases, they are regarded to
be ``noise-like'' variables, whereas variables with opposite properties are
``signal-like'' variables. Normality tests are not effective for data of
small-size samples with high dimensions. Therefore, we modeled noises on the
basis of the property of a noise variable, that is, the uniformity of the
histogram of the probability that a variable is a noise. We devised a method of
detecting signal variables from the structural change of the histogram
according to the decrease in the number of samples. We applied our method to
the data generated by globally coupled map, which can produce time series data
with different correlation lengths, and also applied to gene expression data,
which are typical static data of small-size samples with high dimensions, and
we successfully detected signal variables from them. Moreover, we verified the
assumption that the gene expression data also potentially have a dynamical
system as their generation model, and found that the assumption is compatible
with the results of signal extraction.","['Yoh-ichi Mototake', 'Y-h. Taguchi']","['physics.data-an', 'stat.ML']",2023-04-13 13:18:36+00:00
http://arxiv.org/abs/2304.06412v1,Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective,"This paper introduces a comprehensive, multi-stage machine learning
methodology that effectively integrates information systems and artificial
intelligence to enhance decision-making processes within the domain of
operations research. The proposed framework adeptly addresses common
limitations of existing solutions, such as the neglect of data-driven
estimation for vital production parameters, exclusive generation of point
forecasts without considering model uncertainty, and lacking explanations
regarding the sources of such uncertainty. Our approach employs Quantile
Regression Forests for generating interval predictions, alongside both local
and global variants of SHapley Additive Explanations for the examined
predictive process monitoring problem. The practical applicability of the
proposed methodology is substantiated through a real-world production planning
case study, emphasizing the potential of prescriptive analytics in refining
decision-making procedures. This paper accentuates the imperative of addressing
these challenges to fully harness the extensive and rich data resources
accessible for well-informed decision-making.","['Nijat Mehdiyev', 'Maxim Majlatow', 'Peter Fettke']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2023-04-13 11:18:22+00:00
http://arxiv.org/abs/2304.06326v2,Understanding Overfitting in Adversarial Training via Kernel Regression,"Adversarial training and data augmentation with noise are widely adopted
techniques to enhance the performance of neural networks. This paper
investigates adversarial training and data augmentation with noise in the
context of regularized regression in a reproducing kernel Hilbert space (RKHS).
We establish the limiting formula for these techniques as the attack and noise
size, as well as the regularization parameter, tend to zero. Based on this
limiting formula, we analyze specific scenarios and demonstrate that, without
appropriate regularization, these two methods may have larger generalization
error and Lipschitz constant than standard kernel regression. However, by
selecting the appropriate regularization parameter, these two methods can
outperform standard kernel regression and achieve smaller generalization error
and Lipschitz constant. These findings support the empirical observations that
adversarial training can lead to overfitting, and appropriate regularization
methods, such as early stopping, can alleviate this issue.","['Teng Zhang', 'Kang Li']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2023-04-13 08:06:25+00:00
http://arxiv.org/abs/2304.06254v1,Fair Grading Algorithms for Randomized Exams,"This paper studies grading algorithms for randomized exams. In a randomized
exam, each student is asked a small number of random questions from a large
question bank. The predominant grading rule is simple averaging, i.e.,
calculating grades by averaging scores on the questions each student is asked,
which is fair ex-ante, over the randomized questions, but not fair ex-post, on
the realized questions. The fair grading problem is to estimate the average
grade of each student on the full question bank. The maximum-likelihood
estimator for the Bradley-Terry-Luce model on the bipartite student-question
graph is shown to be consistent with high probability when the number of
questions asked to each student is at least the cubed-logarithm of the number
of students. In an empirical study on exam data and in simulations, our
algorithm based on the maximum-likelihood estimator significantly outperforms
simple averaging in prediction accuracy and ex-post fairness even with a small
class and exam size.","['Jiale Chen', 'Jason Hartline', 'Onno Zoeter']","['stat.ML', 'cs.GT']",2023-04-13 04:21:37+00:00
http://arxiv.org/abs/2304.06251v2,Importance is Important: Generalized Markov Chain Importance Sampling Methods,"We show that for any multiple-try Metropolis algorithm, one can always accept
the proposal and evaluate the importance weight that is needed to correct for
the bias without extra computational cost. This results in a general,
convenient, and rejection-free Markov chain Monte Carlo (MCMC) sampling scheme.
By further leveraging the importance sampling perspective on
Metropolis--Hastings algorithms, we propose an alternative MCMC sampler on
discrete spaces that is also outside the Metropolis--Hastings framework, along
with a general theory on its complexity. Numerical examples suggest that the
proposed algorithms are consistently more efficient than the original
Metropolis--Hastings versions.","['Guanxun Li', 'Aaron Smith', 'Quan Zhou']","['stat.CO', 'stat.ME', 'stat.ML', '65C05, 60J10']",2023-04-13 04:04:09+00:00
http://arxiv.org/abs/2304.06158v3,Post-selection Inference for Conformal Prediction: Trading off Coverage for Precision,"Conformal inference has played a pivotal role in providing uncertainty
quantification for black-box ML prediction algorithms with finite sample
guarantees. Traditionally, conformal prediction inference requires a
data-independent specification of miscoverage level. In practical applications,
one might want to update the miscoverage level after computing the prediction
set. For example, in the context of binary classification, the analyst might
start with a 95$\%$ prediction sets and see that most prediction sets contain
all outcome classes. Prediction sets with both classes being undesirable, the
analyst might desire to consider, say 80$\%$ prediction set. Construction of
prediction sets that guarantee coverage with data-dependent miscoverage level
can be considered as a post-selection inference problem. In this work, we
develop simultaneous conformal inference to account for data-dependent
miscoverage levels. Under the assumption of independent and identically
distributed observations, our proposed methods have a finite sample
simultaneous guarantee over all miscoverage levels. This allows practitioners
to trade freely coverage probability for the quality of the prediction set by
any criterion of their choice (say size of prediction set) while maintaining
the finite sample guarantees similar to traditional conformal inference.","['Siddhaarth Sarkar', 'Arun Kumar Kuchibhotla']","['stat.ME', 'stat.ML']",2023-04-12 20:56:43+00:00
http://arxiv.org/abs/2304.06094v4,Energy-guided Entropic Neural Optimal Transport,"Energy-based models (EBMs) are known in the Machine Learning community for
decades. Since the seminal works devoted to EBMs dating back to the noughties,
there have been a lot of efficient methods which solve the generative modelling
problem by means of energy potentials (unnormalized likelihood functions). In
contrast, the realm of Optimal Transport (OT) and, in particular, neural OT
solvers is much less explored and limited by few recent works (excluding
WGAN-based approaches which utilize OT as a loss function and do not model OT
maps themselves). In our work, we bridge the gap between EBMs and
Entropy-regularized OT. We present a novel methodology which allows utilizing
the recent developments and technical improvements of the former in order to
enrich the latter. From the theoretical perspective, we prove generalization
bounds for our technique. In practice, we validate its applicability in toy 2D
and image domains. To showcase the scalability, we empower our method with a
pre-trained StyleGAN and apply it to high-res AFHQ $512\times 512$ unpaired I2I
translation. For simplicity, we choose simple short- and long-run EBMs as a
backbone of our Energy-guided Entropic OT approach, leaving the application of
more sophisticated EBMs for future research. Our code is available at:
https://github.com/PetrMokrov/Energy-guided-Entropic-OT","['Petr Mokrov', 'Alexander Korotin', 'Alexander Kolesov', 'Nikita Gushchin', 'Evgeny Burnaev']","['cs.LG', 'stat.ML']",2023-04-12 18:20:58+00:00
http://arxiv.org/abs/2304.06029v2,Fluctuation based interpretable analysis scheme for quantum many-body snapshots,"Microscopically understanding and classifying phases of matter is at the
heart of strongly-correlated quantum physics. With quantum simulations, genuine
projective measurements (snapshots) of the many-body state can be taken, which
include the full information of correlations in the system. The rise of deep
neural networks has made it possible to routinely solve abstract processing and
classification tasks of large datasets, which can act as a guiding hand for
quantum data analysis. However, though proven to be successful in
differentiating between different phases of matter, conventional neural
networks mostly lack interpretability on a physical footing. Here, we combine
confusion learning with correlation convolutional neural networks, which yields
fully interpretable phase detection in terms of correlation functions. In
particular, we study thermodynamic properties of the 2D Heisenberg model,
whereby the trained network is shown to pick up qualitative changes in the
snapshots above and below a characteristic temperature where magnetic
correlations become significantly long-range. We identify the full counting
statistics of nearest neighbor spin correlations as the most important quantity
for the decision process of the neural network, which go beyond averages of
local observables. With access to the fluctuations of second-order correlations
-- which indirectly include contributions from higher order, long-range
correlations -- the network is able to detect changes of the specific heat and
spin susceptibility, the latter being in analogy to magnetic properties of the
pseudogap phase in high-temperature superconductors. By combining the confusion
learning scheme with transformer neural networks, our work opens new directions
in interpretable quantum image processing being sensible to long-range order.","['Henning Schlömer', 'Annabelle Bohrdt']","['cond-mat.quant-gas', 'cond-mat.str-el', 'quant-ph', 'stat.ML']",2023-04-12 17:59:59+00:00
http://arxiv.org/abs/2304.05976v1,Bayesian Causal Inference in Doubly Gaussian DAG-probit Models,"We consider modeling a binary response variable together with a set of
covariates for two groups under observational data. The grouping variable can
be the confounding variable (the common cause of treatment and outcome),
gender, case/control, ethnicity, etc. Given the covariates and a binary latent
variable, the goal is to construct two directed acyclic graphs (DAGs), while
sharing some common parameters. The set of nodes, which represent the
variables, are the same for both groups but the directed edges between nodes,
which represent the causal relationships between the variables, can be
potentially different. For each group, we also estimate the effect size for
each node. We assume that each group follows a Gaussian distribution under its
DAG. Given the parent nodes, the joint distribution of DAG is conditionally
independent due to the Markov property of DAGs. We introduce the concept of
Gaussian DAG-probit model under two groups and hence doubly Gaussian DAG-probit
model. To estimate the skeleton of the DAGs and the model parameters, we took
samples from the posterior distribution of doubly Gaussian DAG-probit model via
MCMC method. We validated the proposed method using a comprehensive simulation
experiment and applied it on two real datasets. Furthermore, we validated the
results of the real data analysis using well-known experimental studies to show
the value of the proposed grouping variable in the causality domain.","['Rasool Tahmasbi', 'Keyvan Tahmasbi']","['stat.ME', 'cs.AI', 'stat.ML']",2023-04-12 16:57:47+00:00
http://arxiv.org/abs/2304.06052v2,Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling,"Deploying deep learning models in real-world certified systems requires the
ability to provide confidence estimates that accurately reflect their
uncertainty. In this paper, we demonstrate the use of the conformal prediction
framework to construct reliable and trustworthy predictors for detecting
railway signals. Our approach is based on a novel dataset that includes images
taken from the perspective of a train operator and state-of-the-art object
detectors. We test several conformal approaches and introduce a new method
based on conformal risk control. Our findings demonstrate the potential of the
conformal prediction framework to evaluate model performance and provide
practical guidance for achieving formally guaranteed uncertainty bounds.","['Léo Andéol', 'Thomas Fel', 'Florence De Grancey', 'Luca Mossina']","['cs.LG', 'stat.ML']",2023-04-12 08:10:13+00:00
http://arxiv.org/abs/2304.05527v4,"Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box","Automatic differentiation variational inference (ADVI) offers fast and
easy-to-use posterior approximation in multiple modern probabilistic
programming languages. However, its stochastic optimizer lacks clear
convergence criteria and requires tuning parameters. Moreover, ADVI inherits
the poor posterior uncertainty estimates of mean-field variational Bayes
(MFVB). We introduce ""deterministic ADVI"" (DADVI) to address these issues.
DADVI replaces the intractable MFVB objective with a fixed Monte Carlo
approximation, a technique known in the stochastic optimization literature as
the ""sample average approximation"" (SAA). By optimizing an approximate but
deterministic objective, DADVI can use off-the-shelf second-order optimization,
and, unlike standard mean-field ADVI, is amenable to more accurate posterior
covariances via linear response (LR). In contrast to existing worst-case
theory, we show that, on certain classes of common statistical problems, DADVI
and the SAA can perform well with relatively few samples even in very high
dimensions, though we also show that such favorable results cannot extend to
variational approximations that are too expressive relative to mean-field ADVI.
We show on a variety of real-world problems that DADVI reliably finds good
solutions with default settings (unlike ADVI) and, together with LR
covariances, is typically faster and more accurate than standard ADVI.","['Ryan Giordano', 'Martin Ingram', 'Tamara Broderick']","['cs.LG', 'stat.ME', 'stat.ML']",2023-04-11 22:45:18+00:00
http://arxiv.org/abs/2304.05376v5,ChemCrow: Augmenting large-language models with chemistry tools,"Over the last decades, excellent computational chemistry tools have been
developed. Integrating them into a single platform with enhanced accessibility
could help reaching their full potential by overcoming steep learning curves.
Recently, large-language models (LLMs) have shown strong performance in tasks
across domains, but struggle with chemistry-related problems. Moreover, these
models lack access to external knowledge sources, limiting their usefulness in
scientific applications. In this study, we introduce ChemCrow, an LLM chemistry
agent designed to accomplish tasks across organic synthesis, drug discovery,
and materials design. By integrating 18 expert-designed tools, ChemCrow
augments the LLM performance in chemistry, and new capabilities emerge. Our
agent autonomously planned and executed the syntheses of an insect repellent,
three organocatalysts, and guided the discovery of a novel chromophore. Our
evaluation, including both LLM and expert assessments, demonstrates ChemCrow's
effectiveness in automating a diverse set of chemical tasks. Surprisingly, we
find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4
completions and Chemcrow's performance. Our work not only aids expert chemists
and lowers barriers for non-experts, but also fosters scientific advancement by
bridging the gap between experimental and computational chemistry.","['Andres M Bran', 'Sam Cox', 'Oliver Schilter', 'Carlo Baldassari', 'Andrew D White', 'Philippe Schwaller']","['physics.chem-ph', 'stat.ML']",2023-04-11 17:41:13+00:00
http://arxiv.org/abs/2304.05366v3,"The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning","No free lunch theorems for supervised learning state that no learner can
solve all problems or that all learners achieve exactly the same accuracy on
average over a uniform distribution on learning problems. Accordingly, these
theorems are often referenced in support of the notion that individual problems
require specially tailored inductive biases. While virtually all uniformly
sampled datasets have high complexity, real-world problems disproportionately
generate low-complexity data, and we argue that neural network models share
this same preference, formalized using Kolmogorov complexity. Notably, we show
that architectures designed for a particular domain, such as computer vision,
can compress datasets on a variety of seemingly unrelated domains. Our
experiments show that pre-trained and even randomly initialized language models
prefer to generate low-complexity sequences. Whereas no free lunch theorems
seemingly indicate that individual problems require specialized learners, we
explain how tasks that often require human intervention such as picking an
appropriately sized model when labeled data is scarce or plentiful can be
automated into a single learning algorithm. These observations justify the
trend in deep learning of unifying seemingly disparate problems with an
increasingly small set of machine learning models.","['Micah Goldblum', 'Marc Finzi', 'Keefer Rowan', 'Andrew Gordon Wilson']","['cs.LG', 'stat.ML']",2023-04-11 17:22:22+00:00
http://arxiv.org/abs/2304.05365v6,Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling,"There is a growing interest in using reinforcement learning (RL) to
personalize sequences of treatments in digital health to support users in
adopting healthier behaviors. Such sequential decision-making problems involve
decisions about when to treat and how to treat based on the user's context
(e.g., prior activity level, location, etc.). Online RL is a promising
data-driven approach for this problem as it learns based on each user's
historical responses and uses that knowledge to personalize these decisions.
However, to decide whether the RL algorithm should be included in an
``optimized'' intervention for real-world deployment, we must assess the data
evidence indicating that the RL algorithm is actually personalizing the
treatments to its users. Due to the stochasticity in the RL algorithm, one may
get a false impression that it is learning in certain states and using this
learning to provide specific treatments. We use a working definition of
personalization and introduce a resampling-based methodology for investigating
whether the personalization exhibited by the RL algorithm is an artifact of the
RL algorithm stochasticity. We illustrate our methodology with a case study by
analyzing the data from a physical activity clinical trial called HeartSteps,
which included the use of an online RL algorithm. We demonstrate how our
approach enhances data-driven truth-in-advertising of algorithm personalization
both across all users as well as within specific users in the study.","['Susobhan Ghosh', 'Raphael Kim', 'Prasidh Chhabria', 'Raaz Dwivedi', 'Predrag Klasnja', 'Peng Liao', 'Kelly Zhang', 'Susan Murphy']","['cs.LG', 'stat.AP', 'stat.ME', 'stat.ML']",2023-04-11 17:20:37+00:00
http://arxiv.org/abs/2304.05364v2,Diffusion Models for Constrained Domains,"Denoising diffusion models are a novel class of generative algorithms that
achieve state-of-the-art performance across a range of domains, including image
generation and text-to-image tasks. Building on this success, diffusion models
have recently been extended to the Riemannian manifold setting, broadening
their applicability to a range of problems from the natural and engineering
sciences. However, these Riemannian diffusion models are built on the
assumption that their forward and backward processes are well-defined for all
times, preventing them from being applied to an important set of tasks that
consider manifolds defined via a set of inequality constraints. In this work,
we introduce a principled framework to bridge this gap. We present two distinct
noising processes based on (i) the logarithmic barrier metric and (ii) the
reflected Brownian motion induced by the constraints. As existing diffusion
model techniques cannot be applied in this setting, we derive new tools to
define such models in our framework. We then demonstrate the practical utility
of our methods on a number of synthetic and real-world tasks, including
applications from robotics and protein design.","['Nic Fishman', 'Leo Klarner', 'Valentin De Bortoli', 'Emile Mathieu', 'Michael Hutchinson']","['cs.LG', 'stat.ML']",2023-04-11 17:19:45+00:00
http://arxiv.org/abs/2304.05305v1,Generative Modeling via Hierarchical Tensor Sketching,"We propose a hierarchical tensor-network approach for approximating
high-dimensional probability density via empirical distribution. This leverages
randomized singular value decomposition (SVD) techniques and involves solving
linear equations for tensor cores in this tensor network. The complexity of the
resulting algorithm scales linearly in the dimension of the high-dimensional
density. An analysis of estimation error demonstrates the effectiveness of this
method through several numerical experiments.","['Yifan Peng', 'Yian Chen', 'E. Miles Stoudenmire', 'Yuehaw Khoo']","['math.NA', 'cs.LG', 'cs.NA', 'stat.ML', '15A69, 62Gxx']",2023-04-11 15:55:13+00:00
http://arxiv.org/abs/2304.05294v5,Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery,"Robust feature selection is vital for creating reliable and interpretable
Machine Learning (ML) models. When designing statistical prediction models in
cases where domain knowledge is limited and underlying interactions are
unknown, choosing the optimal set of features is often difficult. To mitigate
this issue, we introduce a Multidata (M) causal feature selection approach that
simultaneously processes an ensemble of time series datasets and produces a
single set of causal drivers. This approach uses the causal discovery
algorithms PC1 or PCMCI that are implemented in the Tigramite Python package.
These algorithms utilize conditional independence tests to infer parts of the
causal graph. Our causal feature selection approach filters out
causally-spurious links before passing the remaining causal features as inputs
to ML models (Multiple linear regression, Random Forest) that predict the
targets. We apply our framework to the statistical intensity prediction of
Western Pacific Tropical Cyclones (TC), for which it is often difficult to
accurately choose drivers and their dimensionality reduction (time lags,
vertical levels, and area-averaging). Using more stringent significance
thresholds in the conditional independence tests helps eliminate spurious
causal relationships, thus helping the ML model generalize better to unseen TC
cases. M-PC1 with a reduced number of features outperforms M-PCMCI, non-causal
ML, and other feature selection methods (lagged correlation, random), even
slightly outperforming feature selection based on eXplainable Artificial
Intelligence. The optimal causal drivers obtained from our causal feature
selection help improve our understanding of underlying relationships and
suggest new potential drivers of TC intensification.","['Saranya Ganesh S.', 'Tom Beucler', 'Frederick Iat-Hin Tam', 'Milton S. Gomez', 'Jakob Runge', 'Andreas Gerhardus']","['stat.ML', 'cs.LG', 'physics.ao-ph', 'physics.comp-ph']",2023-04-11 15:43:34+00:00
http://arxiv.org/abs/2304.09096v1,Privacy-Preserving Matrix Factorization for Recommendation Systems using Gaussian Mechanism,"Building a recommendation system involves analyzing user data, which can
potentially leak sensitive information about users. Anonymizing user data is
often not sufficient for preserving user privacy. Motivated by this, we propose
a privacy-preserving recommendation system based on the differential privacy
framework and matrix factorization, which is one of the most popular algorithms
for recommendation systems. As differential privacy is a powerful and robust
mathematical framework for designing privacy-preserving machine learning
algorithms, it is possible to prevent adversaries from extracting sensitive
user information even if the adversary possesses their publicly available
(auxiliary) information. We implement differential privacy via the Gaussian
mechanism in the form of output perturbation and release user profiles that
satisfy privacy definitions. We employ R\'enyi Differential Privacy for a tight
characterization of the overall privacy loss. We perform extensive experiments
on real data to demonstrate that our proposed algorithm can offer excellent
utility for some parameter choices, while guaranteeing strict privacy.","['Sohan Salahuddin Mugdho', 'Hafiz Imtiaz']","['cs.IR', 'cs.CR', 'cs.LG', 'stat.ML']",2023-04-11 13:50:39+00:00
http://arxiv.org/abs/2304.05223v3,"Inhomogeneous graph trend filtering via a l2,0 cardinality penalty","We study estimation of piecewise smooth signals over a graph. We propose a
$\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate
piecewise smooth graph signals that exhibit inhomogeneous levels of smoothness
across the nodes. We prove that the proposed GTF model is simultaneously a
k-means clustering on the signal over the nodes and a minimum graph cut on the
edges of the graph, where the clustering and the cut share the same assignment
matrix. We propose two methods to solve the proposed GTF model: a spectral
decomposition method and a method based on simulated annealing. In the
experiment on synthetic and real-world datasets, we show that the proposed GTF
model has a better performances compared with existing approaches on the tasks
of denoising, support recovery and semi-supervised classification. We also show
that the proposed GTF model can be solved more efficiently than existing models
for the dataset with a large edge set.","['Xiaoqing Huang', 'Andersen Ang', 'Kun Huang', 'Jie Zhang', 'Yijie Wang']","['cs.LG', 'cs.SI', 'stat.ML', '65F50, 68U01, 68R01', 'G.1.6; G.1.10']",2023-04-11 13:46:59+00:00
http://arxiv.org/abs/2304.05187v1,Automatic Gradient Descent: Deep Learning without Hyperparameters,"The architecture of a deep neural network is defined explicitly in terms of
the number of layers, the width of each layer and the general network topology.
Existing optimisation frameworks neglect this information in favour of implicit
architectural information (e.g. second-order methods) or architecture-agnostic
distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser
in practice, Adam, is based on heuristics. This paper builds a new framework
for deriving optimisation algorithms that explicitly leverage neural
architecture. The theory extends mirror descent to non-convex composite
objective functions: the idea is to transform a Bregman divergence to account
for the non-linear structure of neural architecture. Working through the
details for deep fully-connected networks yields automatic gradient descent: a
first-order optimiser without any hyperparameters. Automatic gradient descent
trains both fully-connected and convolutional networks out-of-the-box and at
ImageNet scale. A PyTorch implementation is available at
https://github.com/jxbz/agd and also in Appendix B. Overall, the paper supplies
a rigorous theoretical foundation for a next-generation of
architecture-dependent optimisers that work automatically and without
hyperparameters.","['Jeremy Bernstein', 'Chris Mingard', 'Kevin Huang', 'Navid Azizan', 'Yisong Yue']","['cs.LG', 'cs.AI', 'cs.NA', 'cs.NE', 'math.NA', 'stat.ML']",2023-04-11 12:45:52+00:00
http://arxiv.org/abs/2304.05093v1,Generative modeling for time series via Schr{ö}dinger bridge,"We propose a novel generative model for time series based on Schr{\""o}dinger
bridge (SB) approach. This consists in the entropic interpolation via optimal
transport between a reference probability measure on path space and a target
measure consistent with the joint data distribution of the time series. The
solution is characterized by a stochastic differential equation on finite
horizon with a path-dependent drift function, hence respecting the temporal
dynamics of the time series distribution. We can estimate the drift function
from data samples either by kernel regression methods or with LSTM neural
networks, and the simulation of the SB diffusion yields new synthetic data
samples of the time series. The performance of our generative model is
evaluated through a series of numerical experiments. First, we test with a toy
autoregressive model, a GARCH Model, and the example of fractional Brownian
motion, and measure the accuracy of our algorithm with marginal and temporal
dependencies metrics. Next, we use our SB generated synthetic samples for the
application to deep hedging on real-data sets. Finally, we illustrate the SB
approach for generating sequence of images.","['Mohamed Hamdouche', 'Pierre Henry-Labordere', 'Huyên Pham']","['math.OC', 'math.PR', 'q-fin.CP', 'stat.ML']",2023-04-11 09:45:06+00:00
http://arxiv.org/abs/2304.05091v1,Actually Sparse Variational Gaussian Processes,"Gaussian processes (GPs) are typically criticised for their unfavourable
scaling in both computational and memory requirements. For large datasets,
sparse GPs reduce these demands by conditioning on a small set of inducing
variables designed to summarise the data. In practice however, for large
datasets requiring many inducing variables, such as low-lengthscale spatial
data, even sparse GPs can become computationally expensive, limited by the
number of inducing variables one can use. In this work, we propose a new class
of inter-domain variational GP, constructed by projecting a GP onto a set of
compactly supported B-spline basis functions. The key benefit of our approach
is that the compact support of the B-spline basis functions admits the use of
sparse linear algebra to significantly speed up matrix operations and
drastically reduce the memory footprint. This allows us to very efficiently
model fast-varying spatial phenomena with tens of thousands of inducing
variables, where previous approaches failed.","['Harry Jake Cunningham', 'Daniel Augusto de Souza', 'So Takao', 'Mark van der Wilk', 'Marc Peter Deisenroth']","['stat.ML', 'cs.LG']",2023-04-11 09:38:58+00:00
http://arxiv.org/abs/2304.04916v3,A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models,"We study dynamic discrete choice models, where a commonly studied problem
involves estimating parameters of agent reward functions (also known as
""structural"" parameters), using agent behavioral data. Maximum likelihood
estimation for such models requires dynamic programming, which is limited by
the curse of dimensionality. In this work, we present a novel algorithm that
provides a data-driven method for selecting and aggregating states, which
lowers the computational and sample complexity of estimation. Our method works
in two stages. In the first stage, we use a flexible inverse reinforcement
learning approach to estimate agent Q-functions. We use these estimated
Q-functions, along with a clustering algorithm, to select a subset of states
that are the most pivotal for driving changes in Q-functions. In the second
stage, with these selected ""aggregated"" states, we conduct maximum likelihood
estimation using a commonly used nested fixed-point algorithm. The proposed
two-stage approach mitigates the curse of dimensionality by reducing the
problem dimension. Theoretically, we derive finite-sample bounds on the
associated estimation error, which also characterize the trade-off of
computational complexity, estimation error, and sample complexity. We
demonstrate the empirical performance of the algorithm in two classic dynamic
discrete choice estimation applications.","['Sinong Geng', 'Houssam Nassif', 'Carlos A. Manzanares']","['cs.LG', 'stat.ML']",2023-04-11 01:07:24+00:00
http://arxiv.org/abs/2304.04824v1,Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning,"Predictions made by deep learning models are prone to data perturbations,
adversarial attacks, and out-of-distribution inputs. To build a trusted AI
system, it is therefore critical to accurately quantify the prediction
uncertainties. While current efforts focus on improving uncertainty
quantification accuracy and efficiency, there is a need to identify uncertainty
sources and take actions to mitigate their effects on predictions. Therefore,
we propose to develop explainable and actionable Bayesian deep learning methods
to not only perform accurate uncertainty quantification but also explain the
uncertainties, identify their sources, and propose strategies to mitigate the
uncertainty impacts. Specifically, we introduce a gradient-based uncertainty
attribution method to identify the most problematic regions of the input that
contribute to the prediction uncertainty. Compared to existing methods, the
proposed UA-Backprop has competitive accuracy, relaxed assumptions, and high
efficiency. Moreover, we propose an uncertainty mitigation strategy that
leverages the attribution results as attention to further improve the model
performance. Both qualitative and quantitative evaluations are conducted to
demonstrate the effectiveness of our proposed methods.","['Hanjing Wang', 'Dhiraj Joshi', 'Shiqiang Wang', 'Qiang Ji']","['cs.LG', 'cs.CV', 'cs.IT', 'math.IT', 'stat.ML']",2023-04-10 19:14:15+00:00
http://arxiv.org/abs/2304.04784v1,Criticality versus uniformity in deep neural networks,"Deep feedforward networks initialized along the edge of chaos exhibit
exponentially superior training ability as quantified by maximum trainable
depth. In this work, we explore the effect of saturation of the tanh activation
function along the edge of chaos. In particular, we determine the line of
uniformity in phase space along which the post-activation distribution has
maximum entropy. This line intersects the edge of chaos, and indicates the
regime beyond which saturation of the activation function begins to impede
training efficiency. Our results suggest that initialization along the edge of
chaos is a necessary but not sufficient condition for optimal trainability.","['Aleksandar Bukva', 'Jurriaan de Gier', 'Kevin T. Grosvenor', 'Ro Jefferson', 'Koenraad Schalm', 'Eliot Schwander']","['cs.LG', 'stat.ML']",2023-04-10 18:00:00+00:00
http://arxiv.org/abs/2304.04782v1,Reinforcement Learning from Passive Data via Latent Intentions,"Passive observational data, such as human videos, is abundant and rich in
information, yet remains largely untapped by current RL methods. Perhaps
surprisingly, we show that passive data, despite not having reward or action
labels, can still be used to learn features that accelerate downstream RL. Our
approach learns from passive data by modeling intentions: measuring how the
likelihood of future outcomes change when the agent acts to achieve a
particular task. We propose a temporal difference learning objective to learn
about intentions, resulting in an algorithm similar to conventional RL, but
which learns entirely from passive data. When optimizing this objective, our
agent simultaneously learns representations of states, of policies, and of
possible outcomes in an environment, all from raw observational data. Both
theoretically and empirically, this scheme learns features amenable for value
prediction for downstream tasks, and our experiments demonstrate the ability to
learn from many forms of passive data, including cross-embodiment video data
and YouTube videos.","['Dibya Ghosh', 'Chethan Bhateja', 'Sergey Levine']","['cs.LG', 'cs.AI', 'stat.ML']",2023-04-10 17:59:05+00:00
http://arxiv.org/abs/2304.04740v3,Reflected Diffusion Models,"Score-based diffusion models learn to reverse a stochastic differential
equation that maps data to noise. However, for complex tasks, numerical error
can compound and result in highly unnatural samples. Previous work mitigates
this drift with thresholding, which projects to the natural data domain (such
as pixel space for images) after each diffusion step, but this leads to a
mismatch between the training and generative processes. To incorporate data
constraints in a principled manner, we present Reflected Diffusion Models,
which instead reverse a reflected stochastic differential equation evolving on
the support of the data. Our approach learns the perturbed score function
through a generalized score matching loss and extends key components of
standard diffusion models including diffusion guidance, likelihood-based
training, and ODE sampling. We also bridge the theoretical gap with
thresholding: such schemes are just discretizations of reflected SDEs. On
standard image benchmarks, our method is competitive with or surpasses the
state of the art without architectural modifications and, for classifier-free
guidance, our approach enables fast exact sampling with ODEs and produces more
faithful samples under high guidance weight.","['Aaron Lou', 'Stefano Ermon']","['stat.ML', 'cs.LG']",2023-04-10 17:54:38+00:00
http://arxiv.org/abs/2304.04724v2,When does Metropolized Hamiltonian Monte Carlo provably outperform Metropolis-adjusted Langevin algorithm?,"We analyze the mixing time of Metropolized Hamiltonian Monte Carlo (HMC) with
the leapfrog integrator to sample from a distribution on $\mathbb{R}^d$ whose
log-density is smooth, has Lipschitz Hessian in Frobenius norm and satisfies
isoperimetry. We bound the gradient complexity to reach $\epsilon$ error in
total variation distance from a warm start by $\tilde
O(d^{1/4}\text{polylog}(1/\epsilon))$ and demonstrate the benefit of choosing
the number of leapfrog steps to be larger than 1. To surpass previous analysis
on Metropolis-adjusted Langevin algorithm (MALA) that has
$\tilde{O}(d^{1/2}\text{polylog}(1/\epsilon))$ dimension dependency in Wu et
al. (2022), we reveal a key feature in our proof that the joint distribution of
the location and velocity variables of the discretization of the continuous HMC
dynamics stays approximately invariant. This key feature, when shown via
induction over the number of leapfrog steps, enables us to obtain estimates on
moments of various quantities that appear in the acceptance rate control of
Metropolized HMC. Moreover, to deal with another bottleneck on the HMC proposal
distribution overlap control in the literature, we provide a new approach to
upper bound the Kullback-Leibler divergence between push-forwards of the
Gaussian distribution through HMC dynamics initialized at two different points.
Notably, our analysis does not require log-concavity or independence of the
marginals, and only relies on an isoperimetric inequality. To illustrate the
applicability of our result, several examples of natural functions that fall
into our framework are discussed.","['Yuansi Chen', 'Khashayar Gatmiry']","['stat.CO', 'cs.CC', 'stat.ML']",2023-04-10 17:35:57+00:00
http://arxiv.org/abs/2304.04692v1,Scalable Randomized Kernel Methods for Multiview Data Integration and Prediction,"We develop scalable randomized kernel methods for jointly associating data
from multiple sources and simultaneously predicting an outcome or classifying a
unit into one of two or more classes. The proposed methods model nonlinear
relationships in multiview data together with predicting a clinical outcome and
are capable of identifying variables or groups of variables that best
contribute to the relationships among the views. We use the idea that random
Fourier bases can approximate shift-invariant kernel functions to construct
nonlinear mappings of each view and we use these mappings and the outcome
variable to learn view-independent low-dimensional representations. Through
simulation studies, we show that the proposed methods outperform several other
linear and nonlinear methods for multiview data integration. When the proposed
methods were applied to gene expression, metabolomics, proteomics, and
lipidomics data pertaining to COVID-19, we identified several molecular
signatures forCOVID-19 status and severity. Results from our real data
application and simulations with small sample sizes suggest that the proposed
methods may be useful for small sample size problems. Availability: Our
algorithms are implemented in Pytorch and interfaced in R and would be made
available at: https://github.com/lasandrall/RandMVLearn.","['Sandra E. Safo', 'Han Lu']","['stat.ME', 'stat.AP', 'stat.ML']",2023-04-10 16:14:42+00:00
http://arxiv.org/abs/2304.04657v4,On the strong stability of ergodic iterations,"We revisit processes generated by iterated random functions driven by a
stationary and ergodic sequence. Such a process is called strongly stable if a
random initialization exists, for which the process is stationary and ergodic,
and for any other initialization, the difference between the two processes
converges to zero almost surely. Under some mild conditions on the
corresponding recursive map, without any condition on the driving sequence, we
show the strong stability of iterations. Several applications are surveyed such
as generalized autoregression and queuing. Furthermore, new results are deduced
for Langevin-type iterations with dependent noise and for multitype branching
processes.","['László Györfi', 'Attila Lovas', 'Miklós Rásonyi']","['math.PR', 'stat.ML', '60G10, 37H12', 'G.3']",2023-04-10 15:33:56+00:00
http://arxiv.org/abs/2304.04615v1,A Survey on Recent Teacher-student Learning Studies,"Knowledge distillation is a method of transferring the knowledge from a
complex deep neural network (DNN) to a smaller and faster DNN, while preserving
its accuracy. Recent variants of knowledge distillation include teaching
assistant distillation, curriculum distillation, mask distillation, and
decoupling distillation, which aim to improve the performance of knowledge
distillation by introducing additional components or by changing the learning
process. Teaching assistant distillation involves an intermediate model called
the teaching assistant, while curriculum distillation follows a curriculum
similar to human education. Mask distillation focuses on transferring the
attention mechanism learned by the teacher, and decoupling distillation
decouples the distillation loss from the task loss. Overall, these variants of
knowledge distillation have shown promising results in improving the
performance of knowledge distillation.",['Minghong Gao'],"['cs.LG', 'stat.ML']",2023-04-10 14:30:28+00:00
