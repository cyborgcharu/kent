id,title,abstract,authors,categories,date
http://arxiv.org/abs/1603.08150v1,Data-Driven Dynamic Decision Models,"This article outlines a method for automatically generating models of dynamic
decision-making that both have strong predictive power and are interpretable in
human terms. This is useful for designing empirically grounded agent-based
simulations and for gaining direct insight into observed dynamic processes. We
use an efficient model representation and a genetic algorithm-based estimation
process to generate simple approximations that explain most of the structure of
complex stochastic processes. This method, implemented in C++ and R, scales
well to large data sets. We apply our methods to empirical data from human
subjects game experiments and international relations. We also demonstrate the
method's ability to recover known data-generating processes by simulating data
with agent-based models and correctly deriving the underlying decision models
for multiple agent models and degrees of stochasticity.","['John J. Nay', 'Jonathan M. Gilligan']","['stat.ML', 'cs.GT', 'cs.MA', 'cs.NE']",2016-03-26 22:45:13+00:00
http://arxiv.org/abs/1603.08113v3,Reconstructing undirected graphs from eigenspaces,"In this paper, we aim at recovering an undirected weighted graph of $N$
vertices from the knowledge of a perturbed version of the eigenspaces of its
adjacency matrix $W$. For instance, this situation arises for stationary
signals on graphs or for Markov chains observed at random times. Our approach
is based on minimizing a cost function given by the Frobenius norm of the
commutator $\mathsf{A} \mathsf{B}-\mathsf{B} \mathsf{A}$ between symmetric
matrices $\mathsf{A}$ and $\mathsf{B}$.
  In the Erd\H{o}s-R\'enyi model with no self-loops, we show that
identifiability (i.e., the ability to reconstruct $W$ from the knowledge of its
eigenspaces) follows a sharp phase transition on the expected number of edges
with threshold function $N\log N/2$.
  Given an estimation of the eigenspaces based on a $n$-sample, we provide
support selection procedures from theoretical and practical point of views. In
particular, when deleting an edge from the active support, our study unveils
that our test statistic is the order of $\mathcal O(1/n)$ when we overestimate
the true support and lower bounded by a positive constant when the estimated
support is smaller than the true support. This feature leads to a powerful
practical support estimation procedure. Simulated and real life numerical
experiments assert our new methodology.","['Yohann De Castro', 'Thibault Espinasse', 'Paul Rochet']","['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']",2016-03-26 14:56:35+00:00
http://arxiv.org/abs/1603.08048v1,"""Did I Say Something Wrong?"" A Word-Level Analysis of Wikipedia Articles for Deletion Discussions","This thesis focuses on gaining linguistic insights into textual discussions
on a word level. It was of special interest to distinguish messages that
constructively contribute to a discussion from those that are detrimental to
them. Thereby, we wanted to determine whether ""I""- and ""You""-messages are
indicators for either of the two discussion styles. These messages are nowadays
often used in guidelines for successful communication. Although their effects
have been successfully evaluated multiple times, a large-scale analysis has
never been conducted.
  Thus, we used Wikipedia Articles for Deletion (short: AfD) discussions
together with the records of blocked users and developed a fully automated
creation of an annotated data set. In this data set, messages were labelled
either constructive or disruptive. We applied binary classifiers to the data to
determine characteristic words for both discussion styles. Thereby, we also
investigated whether function words like pronouns and conjunctions play an
important role in distinguishing the two.
  We found that ""You""-messages were a strong indicator for disruptive messages
which matches their attributed effects on communication. However, we found
""I""-messages to be indicative for disruptive messages as well which is contrary
to their attributed effects. The importance of function words could neither be
confirmed nor refuted. Other characteristic words for either communication
style were not found. Yet, the results suggest that a different model might
represent disruptive and constructive messages in textual discussions better.",['Michael Ruster'],"['cs.CL', 'cs.SI', 'stat.ML']",2016-03-25 22:36:40+00:00
http://arxiv.org/abs/1603.08035v2,On kernel methods for covariates that are rankings,"Permutation-valued features arise in a variety of applications, either in a
direct way when preferences are elicited over a collection of items, or an
indirect way in which numerical ratings are converted to a ranking. To date,
there has been relatively limited study of regression, classification, and
testing problems based on permutation-valued features, as opposed to
permutation-valued responses. This paper studies the use of reproducing kernel
Hilbert space methods for learning from permutation-valued features. These
methods embed the rankings into an implicitly defined function space, and allow
for efficient estimation of regression and test functions in this richer space.
Our first contribution is to characterize both the feature spaces and spectral
properties associated with two kernels for rankings, the Kendall and Mallows
kernels. Using tools from representation theory, we explain the limited
expressive power of the Kendall kernel by characterizing its degenerate
spectrum, and in sharp contrast, we prove that Mallows' kernel is universal and
characteristic. We also introduce families of polynomial kernels that
interpolate between the Kendall (degree one) and Mallows' (infinite degree)
kernels. We show the practical effectiveness of our methods via applications to
Eurobarometer survey data as well as a Movielens ratings dataset.","['Horia Mania', 'Aaditya Ramdas', 'Martin J. Wainwright', 'Michael I. Jordan', 'Benjamin Recht']","['stat.ML', 'cs.DM', 'cs.LG']",2016-03-25 21:09:54+00:00
http://arxiv.org/abs/1603.08029v1,Resnet in Resnet: Generalizing Residual Architectures,"Residual networks (ResNets) have recently achieved state-of-the-art on
challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep
dual-stream architecture that generalizes ResNets and standard CNNs and is
easily implemented with no computational overhead. RiR consistently improves
performance over ResNets, outperforms architectures with similar amounts of
augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.","['Sasha Targ', 'Diogo Almeida', 'Kevin Lyman']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2016-03-25 20:55:40+00:00
http://arxiv.org/abs/1603.07879v1,Hybridization of Expectation-Maximization and K-Means Algorithms for Better Clustering Performance,"The present work proposes hybridization of Expectation-Maximization (EM) and
K-Means techniques as an attempt to speed-up the clustering process. Though
both K-Means and EM techniques look into different areas, K-means can be viewed
as an approximate way to obtain maximum likelihood estimates for the means.
Along with the proposed algorithm for hybridization, the present work also
experiments with the Standard EM algorithm. Six different datasets are used for
the experiments of which three are synthetic datasets. Clustering fitness and
Sum of Squared Errors (SSE) are computed for measuring the clustering
performance. In all the experiments it is observed that the proposed algorithm
for hybridization of EM and K-Means techniques is consistently taking less
execution time with acceptable Clustering Fitness value and less SSE than the
standard EM algorithm. It is also observed that the proposed algorithm is
producing better clustering results than the Cluster package of Purdue
University.","['D. Raja Kishor', 'N. B. Venkateswarlu']","['cs.LG', 'stat.ML']",2016-03-25 11:09:22+00:00
http://arxiv.org/abs/1603.07871v2,Exact Bayesian inference for off-line change-point detection in tree-structured graphical models,"We consider the problem of change-point detection in multivariate
time-series. The multivariate distribution of the observations is supposed to
follow a graphical model, whose graph and parameters are affected by abrupt
changes throughout time. We demonstrate that it is possible to perform exact
Bayesian inference whenever one considers a simple class of undirected graphs
called spanning trees as possible structures. We are then able to integrate on
the graph and segmentation spaces at the same time by combining classical
dynamic programming with algebraic results pertaining to spanning trees. In
particular, we show that quantities such as posterior distributions for
change-points or posterior edge probabilities over time can efficiently be
obtained. We illustrate our results on both synthetic and experimental data
arising from biology and neuroscience.","['Loïc Schwaller', 'Stéphane Robin']",['stat.ML'],2016-03-25 10:43:47+00:00
http://arxiv.org/abs/1603.07850v1,Markov substitute processes : a new model for linguistics and beyond,"We introduce Markov substitute processes, a new model at the crossroad of
statistics and formal grammars, and prove its main property : Markov substitute
processes with a given support form an exponential family.","['Olivier Catoni', 'Thomas Mainguy']","['stat.ML', 'math.ST', 'stat.TH', '62M09, 60J10, 91F20, 68T50']",2016-03-25 08:53:29+00:00
http://arxiv.org/abs/1603.07834v1,An end-to-end convolutional selective autoencoder approach to Soybean Cyst Nematode eggs detection,"This paper proposes a novel selective autoencoder approach within the
framework of deep convolutional networks. The crux of the idea is to train a
deep convolutional autoencoder to suppress undesired parts of an image frame
while allowing the desired parts resulting in efficient object detection. The
efficacy of the framework is demonstrated on a critical plant science problem.
In the United States, approximately $1 billion is lost per annum due to a
nematode infection on soybean plants. Currently, plant-pathologists rely on
labor-intensive and time-consuming identification of Soybean Cyst Nematode
(SCN) eggs in soil samples via manual microscopy. The proposed framework
attempts to significantly expedite the process by using a series of manually
labeled microscopic images for training followed by automated high-throughput
egg detection. The problem is particularly difficult due to the presence of a
large population of non-egg particles (disturbances) in the image frames that
are very similar to SCN eggs in shape, pose and illumination. Therefore, the
selective autoencoder is trained to learn unique features related to the
invariant shapes and sizes of the SCN eggs without handcrafting. After that, a
composite non-maximum suppression and differencing is applied at the
post-processing stage.","['Adedotun Akintayo', 'Nigel Lee', 'Vikas Chawla', 'Mark Mullaney', 'Christopher Marett', 'Asheesh Singh', 'Arti Singh', 'Greg Tylka', 'Baskar Ganapathysubramaniam', 'Soumik Sarkar']","['cs.CV', 'cs.LG', 'stat.ML']",2016-03-25 07:12:32+00:00
http://arxiv.org/abs/1603.07758v1,"A universal tradeoff between power, precision and speed in physical communication","Maximizing the speed and precision of communication while minimizing power
dissipation is a fundamental engineering design goal. Also, biological systems
achieve remarkable speed, precision and power efficiency using poorly
understood physical design principles. Powerful theories like information
theory and thermodynamics do not provide general limits on power, precision and
speed. Here we go beyond these classical theories to prove that the product of
precision and speed is universally bounded by power dissipation in any physical
communication channel whose dynamics is faster than that of the signal.
Moreover, our derivation involves a novel connection between friction and
information geometry. These results may yield insight into both the engineering
design of communication devices and the structure and function of biological
signaling systems.","['Subhaneil Lahiri', 'Jascha Sohl-Dickstein', 'Surya Ganguli']","['cond-mat.stat-mech', 'cs.IT', 'math.IT', 'physics.bio-ph', 'q-bio.NC', 'stat.ML']",2016-03-24 21:10:40+00:00
http://arxiv.org/abs/1603.07749v1,Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High Dimensional Mediators,"In many scientific studies, it becomes increasingly important to delineate
the causal pathways through a large number of mediators, such as genetic and
brain mediators. Structural equation modeling (SEM) is a popular technique to
estimate the pathway effects, commonly expressed as products of coefficients.
However, it becomes unstable to fit such models with high dimensional
mediators, especially for a general setting where all the mediators are
causally dependent but the exact causal relationships between them are unknown.
This paper proposes a sparse mediation model using a regularized SEM approach,
where sparsity here means that a small number of mediators have nonzero
mediation effects between a treatment and an outcome. To address the model
selection challenge, we innovate by introducing a new penalty called Pathway
Lasso. This penalty function is a convex relaxation of the non-convex product
function, and it enables a computationally tractable optimization criterion to
estimate and select many pathway effects simultaneously. We develop a fast
ADMM-type algorithm to compute the model parameters, and we show that the
iterative updates can be expressed in closed form. On both simulated data and a
real fMRI dataset, the proposed approach yields higher pathway selection
accuracy and lower estimation bias than other competing methods.","['Yi Zhao', 'Xi Luo']","['stat.ML', 'stat.AP', 'stat.ME', '62J07, 62P10, 90C25']",2016-03-24 20:46:24+00:00
http://arxiv.org/abs/1603.07738v1,Skill-Based Differences in Spatio-Temporal Team Behavior in Defence of The Ancients 2,"Multiplayer Online Battle Arena (MOBA) games are among the most played
digital games in the world. In these games, teams of players fight against each
other in arena environments, and the gameplay is focused on tactical combat.
Mastering MOBAs requires extensive practice, as is exemplified in the popular
MOBA Defence of the Ancients 2 (DotA 2). In this paper, we present three
data-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2)
Distribution of team members and: 3) Time series clustering via a fuzzy
approach. We present a method for obtaining accurate positional data from DotA
2. We investigate how behavior varies across these measures as a function of
the skill level of teams, using four tiers from novice to professional players.
Results indicate that spatio-temporal behavior of MOBA teams is related to team
skill, with professional teams having smaller within-team distances and
conducting more zone changes than amateur teams. The temporal distribution of
the within-team distances of professional and high-skilled teams also generally
follows patterns distinct from lower skill ranks.","['Anders Drachen', 'Matthew Yancey', 'John Maguire', 'Derrek Chu', 'Iris Yuhui Wang', 'Tobias Mahlmann', 'Matthias Schubert', 'Diego Klabjan']",['stat.ML'],2016-03-24 20:09:25+00:00
http://arxiv.org/abs/1603.07692v1,Predictive Analytics Using Smartphone Sensors for Depressive Episodes,"The behaviors of patients with depression are usually difficult to predict
because the patients demonstrate the symptoms of a depressive episode without a
warning at unexpected times. The goal of this research is to build algorithms
that detect signals of such unusual moments so that doctors can be proactive in
approaching already diagnosed patients before they fall in depression. Each
patient is equipped with a smartphone with the capability to track its sensors.
We first find the home location of a patient, which is then augmented with
other sensor data to identify sleep patterns and select communication patterns.
The algorithms require two to three weeks of training data to build standard
patterns, which are considered normal behaviors; and then, the methods identify
any anomalies in day-to-day data readings of sensors. Four smartphone sensors,
including the accelerometer, the gyroscope, the location probe and the
communication log probe are used for anomaly detection in sleeping and
communication patterns.","['Taeheon Jeong', 'Diego Klabjan', 'Justin Starren']","['cs.CY', 'cs.HC', 'stat.ML']",2016-03-24 18:14:43+00:00
http://arxiv.org/abs/1603.07624v1,Semantic Properties of Customer Sentiment in Tweets,"An increasing number of people are using online social networking services
(SNSs), and a significant amount of information related to experiences in
consumption is shared in this new media form. Text mining is an emerging
technique for mining useful information from the web. We aim at discovering in
particular tweets semantic patterns in consumers' discussions on social media.
Specifically, the purposes of this study are twofold: 1) finding similarity and
dissimilarity between two sets of textual documents that include consumers'
sentiment polarities, two forms of positive vs. negative opinions and 2)
driving actual content from the textual data that has a semantic trend. The
considered tweets include consumers opinions on US retail companies (e.g.,
Amazon, Walmart). Cosine similarity and K-means clustering methods are used to
achieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic
modeling algorithm, is used for the latter purpose. This is the first study
which discover semantic properties of textual data in consumption context
beyond sentiment analysis. In addition to major findings, we apply LDA (Latent
Dirichlet Allocations) to the same data and drew latent topics that represent
consumers' positive opinions and negative opinions on social media.","['Eun Hee Ko', 'Diego Klabjan']","['cs.CL', 'cs.IR', 'cs.SI', 'stat.ML']",2016-03-24 15:22:52+00:00
http://arxiv.org/abs/1603.07610v1,Going Out of Business: Auction House Behavior in the Massively Multi-Player Online Game,"The in-game economies of massively multi-player online games (MMOGs) are
complex systems that have to be carefully designed and managed. This paper
presents the results of an analysis of auction house data from the MMOG Glitch,
across a 14 month time period, the entire lifetime of the game. The data
comprise almost 3 million data points, over 20,000 unique players and more than
650 products. Furthermore, an interactive visualization, based on Sankey flow
diagrams, is presented which shows the proportion of the different clusters
across each time bin, as well as the flow of players between clusters. The
diagram allows evaluation of migration of players between clusters as a
function of time, as well as churn analysis. The presented work provides a
template analysis and visualization model for progression-based or
temporal-based analysis of player behavior broadly applicable to games.","['Anders Drachen', 'Joseph Riley', 'Shawna Baskin', 'Diego Klabjan']","['cs.CY', 'cs.HC', 'stat.ML']",2016-03-24 15:00:35+00:00
http://arxiv.org/abs/1603.07602v1,Clustering Time-Series Energy Data from Smart Meters,"Investigations have been performed into using clustering methods in data
mining time-series data from smart meters. The problem is to identify patterns
and trends in energy usage profiles of commercial and industrial customers over
24-hour periods, and group similar profiles. We tested our method on energy
usage data provided by several U.S. power utilities. The results show accurate
grouping of accounts similar in their energy usage patterns, and potential for
the method to be utilized in energy efficiency programs.","['Alexander Lavin', 'Diego Klabjan']",['stat.ML'],2016-03-24 14:44:52+00:00
http://arxiv.org/abs/1603.07593v2,Evaluating the Performance of Offensive Linemen in the NFL,"How does one objectively measure the performance of an individual offensive
lineman in the NFL? The existing literature proposes various measures that rely
on subjective assessments of game film, but has yet to develop an objective
methodology to evaluate performance. Using a variety of statistics related to
an offensive lineman's performance, we develop a framework to objectively
analyze the overall performance of an individual offensive lineman and
determine specific linemen who are overvalued or undervalued relative to their
salary. We identify eight players across the 2013-2014 and 2014-2015 NFL
seasons that are considered to be overvalued or undervalued and corroborate the
results with existing metrics that are based on subjective evaluation. To the
best of our knowledge, the techniques set forth in this work have not been
utilized in previous works to evaluate the performance of NFL players at any
position, including offensive linemen.","['Nikhil Byanna', 'Diego Klabjan']",['stat.ML'],2016-03-24 14:36:51+00:00
http://arxiv.org/abs/1603.07394v1,Predicting litigation likelihood and time to litigation for patents,"Patent lawsuits are costly and time-consuming. An ability to forecast a
patent litigation and time to litigation allows companies to better allocate
budget and time in managing their patent portfolios. We develop predictive
models for estimating the likelihood of litigation for patents and the expected
time to litigation based on both textual and non-textual features. Our work
focuses on improving the state-of-the-art by relying on a different set of
features and employing more sophisticated algorithms with more realistic data.
The rate of patent litigations is very low, which consequently makes the
problem difficult. The initial model for predicting the likelihood is further
modified to capture a time-to-litigation perspective.","['Papis Wongchaisuwat', 'Diego Klabjan', 'John O. McGinnis']",['stat.ML'],2016-03-23 23:42:02+00:00
http://arxiv.org/abs/1603.07341v1,Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices,"In recent years, deep neural networks (DNN) have demonstrated significant
business impact in large scale analysis and classification tasks such as speech
recognition, visual object detection, pattern extraction, etc. Training of
large DNNs, however, is universally considered as time consuming and
computationally intensive task that demands datacenter-scale computational
resources recruited for many days. Here we propose a concept of resistive
processing unit (RPU) devices that can potentially accelerate DNN training by
orders of magnitude while using much less power. The proposed RPU device can
store and update the weight values locally thus minimizing data movement during
training and allowing to fully exploit the locality and the parallelism of the
training algorithm. We identify the RPU device and system specifications for
implementation of an accelerator chip for DNN training in a realistic
CMOS-compatible technology. For large DNNs with about 1 billion weights this
massively parallel RPU architecture can achieve acceleration factors of 30,000X
compared to state-of-the-art microprocessors while providing power efficiency
of 84,000 GigaOps/s/W. Problems that currently require days of training on a
datacenter-size cluster with thousands of machines can be addressed within
hours on a single RPU accelerator. A system consisted of a cluster of RPU
accelerators will be able to tackle Big Data problems with trillions of
parameters that is impossible to address today like, for example, natural
speech recognition and translation between all world languages, real-time
analytics on large streams of business and scientific data, integration and
analysis of multimodal sensory data flows from massive number of IoT (Internet
of Things) sensors.","['Tayfun Gokmen', 'Yurii Vlasov']","['cs.LG', 'cs.NE', 'stat.ML']",2016-03-23 20:13:11+00:00
http://arxiv.org/abs/1603.07294v2,On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis,"Bayesian inference has great promise for the privacy-preserving analysis of
sensitive data, as posterior sampling automatically preserves differential
privacy, an algorithmic notion of data privacy, under certain conditions
(Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample
(OPS) approach elegantly provides privacy ""for free,"" it is data inefficient in
the sense of asymptotic relative efficiency (ARE). We show that a simple
alternative based on the Laplace mechanism, the workhorse of differential
privacy, is as asymptotically efficient as non-private posterior inference,
under general assumptions. This technique also has practical advantages
including efficient use of the privacy budget for MCMC. We demonstrate the
practicality of our approach on a time-series analysis of sensitive military
records from the Afghanistan and Iraq wars disclosed by the Wikileaks
organization.","['James Foulds', 'Joseph Geumlek', 'Max Welling', 'Kamalika Chaudhuri']","['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML']",2016-03-23 18:31:05+00:00
http://arxiv.org/abs/1603.07292v1,Debugging Machine Learning Tasks,"Unlike traditional programs (such as operating systems or word processors)
which have large amounts of code, machine learning tasks use programs with
relatively small amounts of code (written in machine learning libraries), but
voluminous amounts of data. Just like developers of traditional programs debug
errors in their code, developers of machine learning tasks debug and fix errors
in their data. However, algorithms and tools for debugging and fixing errors in
data are less common, when compared to their counterparts for detecting and
fixing errors in code. In this paper, we consider classification tasks where
errors in training data lead to misclassifications in test points, and propose
an automated method to find the root causes of such misclassifications. Our
root cause analysis is based on Pearl's theory of causation, and uses Pearl's
PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,
encodes the computation of PS as a probabilistic program, and uses recent work
on probabilistic programs and transformations on probabilistic programs (along
with gray-box models of machine learning algorithms) to efficiently compute PS.
Psi is able to identify root causes of data errors in interesting data sets.","['Aleksandar Chakarov', 'Aditya Nori', 'Sriram Rajamani', 'Shayak Sen', 'Deepak Vijaykeerthy']","['cs.LG', 'cs.AI', 'cs.PL', 'stat.ML', 'D.2.5; I.2.3']",2016-03-23 18:30:37+00:00
http://arxiv.org/abs/1603.07285v2,A guide to convolution arithmetic for deep learning,"We introduce a guide to help deep learning practitioners understand and
manipulate convolutional neural network architectures. The guide clarifies the
relationship between various properties (input shape, kernel shape, zero
padding, strides and output shape) of convolutional, pooling and transposed
convolutional layers, as well as the relationship between convolutional and
transposed convolutional layers. Relationships are derived for various cases,
and are illustrated in order to make them intuitive.","['Vincent Dumoulin', 'Francesco Visin']","['stat.ML', 'cs.LG', 'cs.NE']",2016-03-23 17:52:21+00:00
http://arxiv.org/abs/1603.07094v1,Predicting Glaucoma Visual Field Loss by Hierarchically Aggregating Clustering-based Predictors,"This study addresses the issue of predicting the glaucomatous visual field
loss from patient disease datasets. Our goal is to accurately predict the
progress of the disease in individual patients. As very few measurements are
available for each patient, it is difficult to produce good predictors for
individuals. A recently proposed clustering-based method enhances the power of
prediction using patient data with similar spatiotemporal patterns. Each
patient is categorized into a cluster of patients, and a predictive model is
constructed using all of the data in the class. Predictions are highly
dependent on the quality of clustering, but it is difficult to identify the
best clustering method. Thus, we propose a method for aggregating cluster-based
predictors to obtain better prediction accuracy than from a single
cluster-based prediction. Further, the method shows very high performances by
hierarchically aggregating experts generated from several cluster-based
methods. We use real datasets to demonstrate that our method performs
significantly better than conventional clustering-based and patient-wise
regression methods, because the hierarchical aggregating strategy has a
mechanism whereby good predictors in a small community can thrive.","['Motohide Higaki', 'Kai Morino', 'Hiroshi Murata', 'Ryo Asaoka', 'Kenji Yamanishi']","['stat.ML', 'cs.LG']",2016-03-23 09:06:19+00:00
http://arxiv.org/abs/1603.06923v1,Inference via Message Passing on Partially Labeled Stochastic Block Models,"We study the community detection and recovery problem in partially-labeled
stochastic block models (SBM). We develop a fast linearized message-passing
algorithm to reconstruct labels for SBM (with $n$ nodes, $k$ blocks, $p,q$
intra and inter block connectivity) when $\delta$ proportion of node labels are
revealed. The signal-to-noise ratio ${\sf SNR}(n,k,p,q,\delta)$ is shown to
characterize the fundamental limitations of inference via local algorithms. On
the one hand, when ${\sf SNR}>1$, the linearized message-passing algorithm
provides the statistical inference guarantee with mis-classification rate at
most $\exp(-({\sf SNR}-1)/2)$, thus interpolating smoothly between strong and
weak consistency. This exponential dependence improves upon the known error
rate $({\sf SNR}-1)^{-1}$ in the literature on weak recovery. On the other
hand, when ${\sf SNR}<1$ (for $k=2$) and ${\sf SNR}<1/4$ (for general growing
$k$), we prove that local algorithms suffer an error rate at least $\frac{1}{2}
- \sqrt{\delta \cdot {\sf SNR}}$, which is only slightly better than random
guess for small $\delta$.","['T. Tony Cai', 'Tengyuan Liang', 'Alexander Rakhlin']","['math.ST', 'stat.ML', 'stat.TH']",2016-03-22 19:30:14+00:00
http://arxiv.org/abs/1603.06915v1,Completely random measures for modeling power laws in sparse graphs,"Network data appear in a number of applications, such as online social
networks and biological networks, and there is growing interest in both
developing models for networks as well as studying the properties of such data.
Since individual network datasets continue to grow in size, it is necessary to
develop models that accurately represent the real-life scaling properties of
networks. One behavior of interest is having a power law in the degree
distribution. However, other types of power laws that have been observed
empirically and considered for applications such as clustering and feature
allocation models have not been studied as frequently in models for graph data.
In this paper, we enumerate desirable asymptotic behavior that may be of
interest for modeling graph data, including sparsity and several types of power
laws. We outline a general framework for graph generative models using
completely random measures; by contrast to the pioneering work of Caron and Fox
(2015), we consider instantiating more of the existing atoms of the random
measure as the dataset size increases rather than adding new atoms to the
measure. We see that these two models can be complementary; they respectively
yield interpretations as (1) time passing among existing members of a network
and (2) new individuals joining a network. We detail a particular instance of
this framework and show simulated results that suggest this model exhibits some
desirable asymptotic power-law behavior.","['Diana Cai', 'Tamara Broderick']","['stat.ML', 'math.ST', 'stat.ME', 'stat.TH']",2016-03-22 19:14:55+00:00
http://arxiv.org/abs/1607.01354v1,Learning Discriminative Features using Encoder-Decoder type Deep Neural Nets,"As machine learning is applied to an increasing variety of complex problems,
which are defined by high dimensional and complex data sets, the necessity for
task oriented feature learning grows in importance. With the advancement of
Deep Learning algorithms, various successful feature learning techniques have
evolved. In this paper, we present a novel way of learning discriminative
features by training Deep Neural Nets which have Encoder or Decoder type
architecture similar to an Autoencoder. We demonstrate that our approach can
learn discriminative features which can perform better at pattern
classification tasks when the number of training samples is relatively small in
size.","['Vishwajeet Singh', 'Killamsetti Ravi Kumar', 'K Eswaran']","['cs.LG', 'stat.ML', 'I.5; I.5.3']",2016-03-22 18:46:13+00:00
http://arxiv.org/abs/1603.06898v1,Edge-exchangeable graphs and sparsity,"A known failing of many popular random graph models is that the Aldous-Hoover
Theorem guarantees these graphs are dense with probability one; that is, the
number of edges grows quadratically with the number of nodes. This behavior is
considered unrealistic in observed graphs. We define a notion of edge
exchangeability for random graphs in contrast to the established notion of
infinite exchangeability for random graphs --- which has traditionally relied
on exchangeability of nodes (rather than edges) in a graph. We show that,
unlike node exchangeability, edge exchangeability encompasses models that are
known to provide a projective sequence of random graphs that circumvent the
Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the
number of edges with the number of nodes. We show how edge-exchangeability of
graphs relates naturally to existing notions of exchangeability from clustering
(a.k.a. partitions) and other familiar combinatorial structures.","['Tamara Broderick', 'Diana Cai']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2016-03-22 18:28:09+00:00
http://arxiv.org/abs/1603.06895v2,A Selection of Giant Radio Sources from NVSS,"Results of the application of pattern recognition techniques to the problem
of identifying Giant Radio Sources (GRS) from the data in the NVSS catalog are
presented and issues affecting the process are explored. Decision-tree pattern
recognition software was applied to training set source pairs developed from
known NVSS large angular size radio galaxies. The full training set consisted
of 51,195 source pairs, 48 of which were known GRS for which each lobe was
primarily represented by a single catalog component. The source pairs had a
maximum separation of 20 arc minutes and a minimum component area of 1.87
square arc minutes at the 1.4 mJy level. The importance of comparing resulting
probability distributions of the training and application sets for cases of
unknown class ratio is demonstrated. The probability of correctly ranking a
randomly selected (GRS, non-GRS) pair from the best of the tested classifiers
was determined to be 97.8 +/- 1.5%. The best classifiers were applied to the
over 870,000 candidate pairs from the entire catalog. Images of higher ranked
sources were visually screened and a table of over sixteen hundred candidates,
including morphological annotation, is presented. These systems include doubles
and triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shaped
systems, and core-jets and resolved cores. While some resolved lobe systems are
recovered with this technique, generally it is expected that such systems would
require a different approach.",['D. D. Proctor'],"['astro-ph.GA', 'cs.CV', 'stat.ML']",2016-03-22 18:18:16+00:00
http://arxiv.org/abs/1603.06881v1,Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of Pairwise Comparisons,"We study methods for aggregating pairwise comparison data in order to
estimate outcome probabilities for future comparisons among a collection of n
items. Working within a flexible framework that imposes only a form of strong
stochastic transitivity (SST), we introduce an adaptivity index defined by the
indifference sets of the pairwise comparison probabilities. In addition to
measuring the usual worst-case risk of an estimator, this adaptivity index also
captures the extent to which the estimator adapts to instance-specific
difficulty relative to an oracle estimator. We prove three main results that
involve this adaptivity index and different algorithms. First, we propose a
three-step estimator termed Count-Randomize-Least squares (CRL), and show that
it has adaptivity index upper bounded as $\sqrt{n}$ up to logarithmic factors.
We then show that that conditional on the hardness of planted clique, no
computationally efficient estimator can achieve an adaptivity index smaller
than $\sqrt{n}$. Second, we show that a regularized least squares estimator can
achieve a poly-logarithmic adaptivity index, thereby demonstrating a
$\sqrt{n}$-gap between optimal and computationally achievable adaptivity.
Finally, we prove that the standard least squares estimator, which is known to
be optimally adaptive in several closely related problems, fails to adapt in
the context of estimating pairwise probabilities.","['Nihar B. Shah', 'Sivaraman Balakrishnan', 'Martin J. Wainwright']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",2016-03-22 17:28:08+00:00
http://arxiv.org/abs/1603.06861v1,Trading-off variance and complexity in stochastic gradient descent,"Stochastic gradient descent is the method of choice for large-scale machine
learning problems, by virtue of its light complexity per iteration. However, it
lags behind its non-stochastic counterparts with respect to the convergence
rate, due to high variance introduced by the stochastic updates. The popular
Stochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming,
introducing a new update rule which requires infrequent passes over the entire
input dataset to compute the full-gradient.
  In this work, we propose CheapSVRG, a stochastic variance-reduction
optimization scheme. Our algorithm is similar to SVRG but instead of the full
gradient, it uses a surrogate which can be efficiently computed on a small
subset of the input data. It achieves a linear convergence rate ---up to some
error level, depending on the nature of the optimization problem---and features
a trade-off between the computational complexity and the convergence rate.
Empirical evaluation shows that CheapSVRG performs at least competitively
compared to the state of the art.","['Vatsal Shah', 'Megasthenis Asteris', 'Anastasios Kyrillidis', 'Sujay Sanghavi']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC']",2016-03-22 16:34:26+00:00
http://arxiv.org/abs/1603.06859v1,Enhanced perceptrons using contrastive biclusters,"Perceptrons are neuronal devices capable of fully discriminating linearly
separable classes. Although straightforward to implement and train, their
applicability is usually hindered by non-trivial requirements imposed by
real-world classification problems. Therefore, several approaches, such as
kernel perceptrons, have been conceived to counteract such difficulties. In
this paper, we investigate an enhanced perceptron model based on the notion of
contrastive biclusters. From this perspective, a good discriminative bicluster
comprises a subset of data instances belonging to one class that show high
coherence across a subset of features and high differentiation from nearest
instances of the other class under the same features (referred to as its
contrastive bicluster). Upon each local subspace associated with a pair of
contrastive biclusters a perceptron is trained and the model with highest area
under the receiver operating characteristic curve (AUC) value is selected as
the final classifier. Experiments conducted on a range of data sets, including
those related to a difficult biosignal classification problem, show that the
proposed variant can be indeed very useful, prevailing in most of the cases
upon standard and kernel perceptrons in terms of accuracy and AUC measures.","['André L. V. Coelho', 'Fabrício O. de França']","['cs.NE', 'cs.LG', 'stat.ML']",2016-03-22 16:32:26+00:00
http://arxiv.org/abs/1603.06846v3,A new class of metrics for learning on real-valued and structured data,"We propose a new class of metrics on sets, vectors, and functions that can be
used in various stages of data mining, including exploratory data analysis,
learning, and result interpretation. These new distance functions unify and
generalize some of the popular metrics, such as the Jaccard and bag distances
on sets, Manhattan distance on vector spaces, and Marczewski-Steinhaus distance
on integrable functions. We prove that the new metrics are complete and show
useful relationships with $f$-divergences for probability distributions. To
further extend our approach to structured objects such as concept hierarchies
and ontologies, we introduce information-theoretic metrics on directed acyclic
graphs drawn according to a fixed probability distribution. We conduct
empirical investigation to demonstrate intuitive interpretation of the new
metrics and their effectiveness on real-valued, high-dimensional, and
structured data. Extensive comparative evaluation demonstrates that the new
metrics outperformed multiple similarity and dissimilarity functions
traditionally used in data mining, including the Minkowski family, the
fractional $L^p$ family, two $f$-divergences, cosine distance, and two
correlation coefficients. Finally, we argue that the new class of metrics is
particularly appropriate for rapid processing of high-dimensional and
structured data in distance-based learning.","['Ruiyu Yang', 'Yuxiang Jiang', 'Scott Mathews', 'Elizabeth A. Housworth', 'Matthew W. Hahn', 'Predrag Radivojac']",['stat.ML'],2016-03-22 16:01:57+00:00
http://arxiv.org/abs/1603.06785v1,Multi-domain machine translation enhancements by parallel data extraction from comparable corpora,"Parallel texts are a relatively rare language resource, however, they
constitute a very useful research material with a wide range of applications.
This study presents and analyses new methodologies we developed for obtaining
such data from previously built comparable corpora. The methodologies are
automatic and unsupervised which makes them good for large scale research. The
task is highly practical as non-parallel multilingual data occur much more
frequently than parallel corpora and accessing them is easy, although parallel
sentences are a considerably more useful resource. In this study, we propose a
method of automatic web crawling in order to build topic-aligned comparable
corpora, e.g. based on the Wikipedia or Euronews.com. We also developed new
methods of obtaining parallel sentences from comparable data and proposed
methods of filtration of corpora capable of selecting inconsistent or only
partially equivalent translations. Our methods are easily scalable to other
languages. Evaluation of the quality of the created corpora was performed by
analysing the impact of their use on statistical machine translation systems.
Experiments were presented on the basis of the Polish-English language pair for
texts from different domains, i.e. lectures, phrasebooks, film dialogues,
European Parliament proceedings and texts contained medicines leaflets. We also
tested a second method of creating parallel corpora based on data from
comparable corpora which allows for automatically expanding the existing corpus
of sentences about a given domain on the basis of analogies found between them.
It does not require, therefore, having past parallel resources in order to
train a classifier.","['Krzysztof Wołk', 'Emilia Rejmund', 'Krzysztof Marasek']","['cs.CL', 'stat.ML']",2016-03-22 13:34:28+00:00
http://arxiv.org/abs/1603.06743v3,Localized Lasso for High-Dimensional Regression,"We introduce the localized Lasso, which is suited for learning models that
are both interpretable and have a high predictive power in problems with high
dimensionality $d$ and small sample size $n$. More specifically, we consider a
function defined by local sparse models, one at each data point. We introduce
sample-wise network regularization to borrow strength across the models, and
sample-wise exclusive group sparsity (a.k.a., $\ell_{1,2}$ norm) to introduce
diversity into the choice of feature sets in the local models. The local models
are interpretable in terms of similarity of their sparsity patterns. The cost
function is convex, and thus has a globally optimal solution. Moreover, we
propose a simple yet efficient iterative least-squares based optimization
procedure for the localized Lasso, which does not need a tuning parameter, and
is guaranteed to converge to a globally optimal solution. The solution is
empirically shown to outperform alternatives for both simulated and genomic
personalized medicine data.","['Makoto Yamada', 'Koh Takeuchi', 'Tomoharu Iwata', 'John Shawe-Taylor', 'Samuel Kaski']","['stat.ML', 'cs.LG', 'stat.ME']",2016-03-22 11:41:28+00:00
http://arxiv.org/abs/1603.06624v1,Variational Autoencoders for Feature Detection of Magnetic Resonance Imaging Data,"Independent component analysis (ICA), as an approach to the blind
source-separation (BSS) problem, has become the de-facto standard in many
medical imaging settings. Despite successes and a large ongoing research
effort, the limitation of ICA to square linear transformations have not been
overcome, so that general INFOMAX is still far from being realized. As an
alternative, we present feature analysis in medical imaging as a problem solved
by Helmholtz machines, which include dimensionality reduction and
reconstruction of the raw data under the same objective, and which recently
have overcome major difficulties in inference and learning with deep and
nonlinear configurations. We demonstrate one approach to training Helmholtz
machines, variational auto-encoders (VAE), as a viable approach toward feature
extraction with magnetic resonance imaging (MRI) data.","['R. Devon Hjelm', 'Sergey M. Plis', 'Vince C. Calhoun']","['cs.LG', 'cs.NE', 'stat.ML']",2016-03-21 21:31:36+00:00
http://arxiv.org/abs/1603.06560v4,Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,"Performance of machine learning algorithms depends critically on identifying
a good set of hyperparameters. While recent approaches use Bayesian
optimization to adaptively select configurations, we focus on speeding up
random search through adaptive resource allocation and early-stopping. We
formulate hyperparameter optimization as a pure-exploration non-stochastic
infinite-armed bandit problem where a predefined resource like iterations, data
samples, or features is allocated to randomly sampled configurations. We
introduce a novel algorithm, Hyperband, for this framework and analyze its
theoretical properties, providing several desirable guarantees. Furthermore, we
compare Hyperband with popular Bayesian optimization methods on a suite of
hyperparameter optimization problems. We observe that Hyperband can provide
over an order-of-magnitude speedup over our competitor set on a variety of
deep-learning and kernel-based learning problems.","['Lisha Li', 'Kevin Jamieson', 'Giulia DeSalvo', 'Afshin Rostamizadeh', 'Ameet Talwalkar']","['cs.LG', 'stat.ML']",2016-03-21 19:51:04+00:00
http://arxiv.org/abs/1603.06541v1,A Comparison Study of Nonlinear Kernels,"In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF
(folded RBF), acos, and acos-$\chi^2$, on a wide range of publicly available
datasets. The proposed fRBF kernel performs very similarly to the RBF kernel.
Both RBF and fRBF kernels require an important tuning parameter ($\gamma$).
Interestingly, for a significant portion of the datasets, the min-max kernel
outperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\chi^2$
kernel also perform well in general and in some datasets achieve the best
accuracies.
  One crucial issue with the use of nonlinear kernels is the excessive
computational and memory cost. These days, one increasingly popular strategy is
to linearize the kernels through various randomization algorithms. In our
study, the randomization method for the min-max kernel demonstrates excellent
performance compared to the randomization methods for other types of nonlinear
kernels, measured in terms of the number of nonzero terms in the transformed
dataset.
  Our study provides evidence for supporting the use of the min-max kernel and
the corresponding randomized linearization method (i.e., the so-called ""0-bit
CWS""). Furthermore, the results motivate at least two directions for future
research: (i) To develop new (and linearizable) nonlinear kernels for better
accuracies; and (ii) To develop better linearization algorithms for improving
the current linearization methods for the RBF kernel, the acos kernel, and the
acos-$\chi^2$ kernel. One attempt is to combine the min-max kernel with the
acos kernel or the acos-$\chi^2$ kernel. The advantages of these two new and
tuning-free nonlinear kernels are demonstrated vias our extensive experiments.",['Ping Li'],"['stat.ML', 'cs.LG']",2016-03-21 19:11:50+00:00
http://arxiv.org/abs/1603.06340v1,Data Augmentation via Levy Processes,"If a document is about travel, we may expect that short snippets of the
document should also be about travel. We introduce a general framework for
incorporating these types of invariances into a discriminative classifier. The
framework imagines data as being drawn from a slice of a Levy process. If we
slice the Levy process at an earlier point in time, we obtain additional
pseudo-examples, which can be used to train the classifier. We show that this
scheme has two desirable properties: it preserves the Bayes decision boundary,
and it is equivalent to fitting a generative model in the limit where we rewind
time back to 0. Our construction captures popular schemes such as Gaussian
feature noising and dropout training, as well as admitting new generalizations.","['Stefan Wager', 'William Fithian', 'Percy Liang']",['stat.ML'],2016-03-21 07:13:51+00:00
http://arxiv.org/abs/1603.06318v6,Harnessing Deep Neural Networks with Logic Rules,"Combining deep neural networks with structured logic rules is desirable to
harness flexibility and reduce uninterpretability of the neural models. We
propose a general framework capable of enhancing various types of neural
networks (e.g., CNNs and RNNs) with declarative first-order logic rules.
Specifically, we develop an iterative distillation method that transfers the
structured information of logic rules into the weights of neural networks. We
deploy the framework on a CNN for sentiment analysis, and an RNN for named
entity recognition. With a few highly intuitive rules, we obtain substantial
improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.","['Zhiting Hu', 'Xuezhe Ma', 'Zhengzhong Liu', 'Eduard Hovy', 'Eric Xing']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2016-03-21 03:33:20+00:00
http://arxiv.org/abs/1603.06313v2,Convex block-sparse linear regression with expanders -- provably,"Sparse matrices are favorable objects in machine learning and optimization.
When such matrices are used, in place of dense ones, the overall complexity
requirements in optimization can be significantly reduced in practice, both in
terms of space and run-time. Prompted by this observation, we study a convex
optimization scheme for block-sparse recovery from linear measurements. To
obtain linear sketches, we use expander matrices, i.e., sparse matrices
containing only few non-zeros per column. Hitherto, to the best of our
knowledge, such algorithmic solutions have been only studied from a non-convex
perspective. Our aim here is to theoretically characterize the performance of
convex approaches under such setting.
  Our key novelty is the expression of the recovery error in terms of the
model-based norm, while assuring that solution lives in the model. To achieve
this, we show that sparse model-based matrices satisfy a group version of the
null-space property. Our experimental findings on synthetic and real
applications support our claims for faster recovery in the convex setting -- as
opposed to using dense sensing matrices, while showing a competitive recovery
performance.","['Anastasios Kyrillidis', 'Bubacarr Bah', 'Rouzbeh Hasheminezhad', 'Quoc Tran-Dinh', 'Luca Baldassarre', 'Volkan Cevher']","['cs.IT', 'math.IT', 'math.OC', 'stat.ML']",2016-03-21 02:34:26+00:00
http://arxiv.org/abs/1603.06288v4,Multi-fidelity Gaussian Process Bandit Optimisation,"In many scientific and engineering applications, we are tasked with the
maximisation of an expensive to evaluate black box function $f$. Traditional
settings for this problem assume just the availability of this single function.
However, in many cases, cheap approximations to $f$ may be obtainable. For
example, the expensive real world behaviour of a robot can be approximated by a
cheap computer simulation. We can use these approximations to eliminate low
function value regions cheaply and use the expensive evaluations of $f$ in a
small but promising region and speedily identify the optimum. We formalise this
task as a \emph{multi-fidelity} bandit problem where the target function and
its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a
novel method based on upper confidence bound techniques. In our theoretical
analysis we demonstrate that it exhibits precisely the above behaviour, and
achieves better regret than strategies which ignore multi-fidelity information.
Empirically, MF-GP-UCB outperforms such naive strategies and other
multi-fidelity methods on several synthetic and real experiments.","['Kirthevasan Kandasamy', 'Gautam Dasarathy', 'Junier B. Oliva', 'Jeff Schneider', 'Barnabas Poczos']","['stat.ML', 'cs.AI', 'cs.LG']",2016-03-20 22:58:43+00:00
http://arxiv.org/abs/1603.06277v5,Composing graphical models with neural networks for structured representations and fast inference,"We propose a general modeling and inference framework that composes
probabilistic graphical models with deep learning methods and combines their
respective strengths. Our model family augments graphical structure in latent
variables with neural network observation models. For inference, we extend
variational autoencoders to use graphical model approximating distributions
with recognition networks that output conjugate potentials. All components of
these models are learned simultaneously with a single objective, giving a
scalable algorithm that leverages stochastic variational inference, natural
gradients, graphical model message passing, and the reparameterization trick.
We illustrate this framework with several example models and an application to
mouse behavioral phenotyping.","['Matthew J. Johnson', 'David Duvenaud', 'Alexander B. Wiltschko', 'Sandeep R. Datta', 'Ryan P. Adams']",['stat.ML'],2016-03-20 22:01:02+00:00
http://arxiv.org/abs/1603.06202v2,Extracting Predictive Information from Heterogeneous Data Streams using Gaussian Processes,"Financial markets are notoriously complex environments, presenting vast
amounts of noisy, yet potentially informative data. We consider the problem of
forecasting financial time series from a wide range of information sources
using online Gaussian Processes with Automatic Relevance Determination (ARD)
kernels. We measure the performance gain, quantified in terms of Normalised
Root Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson
correlation, from fusing each of four separate data domains: time series
technicals, sentiment analysis, options market data and broker recommendations.
We show evidence that ARD kernels produce meaningful feature rankings that help
retain salient inputs and reduce input dimensionality, providing a framework
for sifting through financial complexity. We measure the performance gain from
fusing each domain's heterogeneous data streams into a single probabilistic
model. In particular our findings highlight the critical value of options data
in mapping out the curvature of price space and inspire an intuitive, novel
direction for research in financial prediction.","['Sid Ghoshal', 'Stephen Roberts']","['q-fin.ST', 'stat.ML']",2016-03-20 11:11:54+00:00
http://arxiv.org/abs/1603.06186v2,The Multiscale Laplacian Graph Kernel,"Many real world graphs, such as the graphs of molecules, exhibit structure at
multiple different scales, but most existing kernels between graphs are either
purely local or purely global in character. In contrast, by building a
hierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLG
kernels) that we define in this paper can account for structure at a range of
different scales. At the heart of the MLG construction is another new graph
kernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which has
the property that it can lift a base kernel defined on the vertices of two
graphs to a kernel between the graphs. The MLG kernel applies such FLG kernels
to subgraphs recursively. To make the MLG kernel computationally feasible, we
also introduce a randomized projection procedure, similar to the Nystr\""om
method, but for RKHS operators.","['Risi Kondor', 'Horace Pan']",['stat.ML'],2016-03-20 06:33:43+00:00
http://arxiv.org/abs/1603.06170v2,Joint Stochastic Approximation learning of Helmholtz Machines,"Though with progress, model learning and performing posterior inference still
remains a common challenge for using deep generative models, especially for
handling discrete hidden variables. This paper is mainly concerned with
algorithms for learning Helmholz machines, which is characterized by pairing
the generative model with an auxiliary inference model. A common drawback of
previous learning algorithms is that they indirectly optimize some bounds of
the targeted marginal log-likelihood. In contrast, we successfully develop a
new class of algorithms, based on stochastic approximation (SA) theory of the
Robbins-Monro type, to directly optimize the marginal log-likelihood and
simultaneously minimize the inclusive KL-divergence. The resulting learning
algorithm is thus called joint SA (JSA). Moreover, we construct an effective
MCMC operator for JSA. Our results on the MNIST datasets demonstrate that the
JSA's performance is consistently superior to that of competing algorithms like
RWS, for learning a range of difficult models.","['Haotian Xu', 'Zhijian Ou']","['cs.LG', 'stat.ML']",2016-03-20 00:55:06+00:00
http://arxiv.org/abs/1603.06160v2,Stochastic Variance Reduction for Nonconvex Optimization,"We study nonconvex finite-sum problems and analyze stochastic variance
reduced gradient (SVRG) methods for them. SVRG and related methods have
recently surged into prominence for convex optimization given their edge over
stochastic gradient descent (SGD); but their theoretical analysis almost
exclusively assumes convexity. In contrast, we prove non-asymptotic rates of
convergence (to stationary points) of SVRG for nonconvex optimization, and show
that it is provably faster than SGD and gradient descent. We also analyze a
subclass of nonconvex problems on which SVRG attains linear convergence to the
global optimum. We extend our analysis to mini-batch variants of SVRG, showing
(theoretical) linear speedup due to mini-batching in parallel settings.","['Sashank J. Reddi', 'Ahmed Hefny', 'Suvrit Sra', 'Barnabas Poczos', 'Alex Smola']","['math.OC', 'cs.LG', 'cs.NE', 'stat.ML']",2016-03-19 23:37:38+00:00
http://arxiv.org/abs/1603.06159v1,Fast Incremental Method for Nonconvex Optimization,"We analyze a fast incremental aggregated gradient method for optimizing
nonconvex problems of the form $\min_x \sum_i f_i(x)$. Specifically, we analyze
the SAGA algorithm within an Incremental First-order Oracle framework, and show
that it converges to a stationary point provably faster than both gradient
descent and stochastic gradient descent. We also discuss a Polyak's special
class of nonconvex problems for which SAGA converges at a linear rate to the
global optimum. Finally, we analyze the practically valuable regularized and
minibatch variants of SAGA. To our knowledge, this paper presents the first
analysis of fast convergence for an incremental aggregated gradient method for
nonconvex problems.","['Sashank J. Reddi', 'Suvrit Sra', 'Barnabas Poczos', 'Alex Smola']","['math.OC', 'cs.LG', 'stat.ML']",2016-03-19 23:28:44+00:00
http://arxiv.org/abs/1603.06125v1,The Computational Power of Dynamic Bayesian Networks,"This paper considers the computational power of constant size, dynamic
Bayesian networks. Although discrete dynamic Bayesian networks are no more
powerful than hidden Markov models, dynamic Bayesian networks with continuous
random variables and discrete children of continuous parents are capable of
performing Turing-complete computation. With modified versions of existing
algorithms for belief propagation, such a simulation can be carried out in real
time. This result suggests that dynamic Bayesian networks may be more powerful
than previously considered. Relationships to causal models and recurrent neural
networks are also discussed.",['Joshua Brulé'],"['cs.AI', 'stat.ML']",2016-03-19 18:30:02+00:00
http://arxiv.org/abs/1603.06038v2,Tensor Methods and Recommender Systems,"A substantial progress in development of new and efficient tensor
factorization techniques has led to an extensive research of their
applicability in recommender systems field. Tensor-based recommender models
push the boundaries of traditional collaborative filtering techniques by taking
into account a multifaceted nature of real environments, which allows to
produce more accurate, situational (e.g. context-aware, criteria-driven)
recommendations. Despite the promising results, tensor-based methods are poorly
covered in existing recommender systems surveys. This survey aims to complement
previous works and provide a comprehensive overview on the subject. To the best
of our knowledge, this is the first attempt to consolidate studies from various
application domains in an easily readable, digestible format, which helps to
get a notion of the current state of the field. We also provide a high level
discussion of the future perspectives and directions for further improvement of
tensor-based recommendation systems.","['Evgeny Frolov', 'Ivan Oseledets']","['cs.LG', 'cs.IR', 'stat.ML']",2016-03-19 03:38:47+00:00
http://arxiv.org/abs/1603.06035v1,L0-norm Sparse Graph-regularized SVD for Biclustering,"Learning the ""blocking"" structure is a central challenge for high dimensional
data (e.g., gene expression data). Recently, a sparse singular value
decomposition (SVD) has been used as a biclustering tool to achieve this goal.
However, this model ignores the structural information between variables (e.g.,
gene interaction graph). Although typical graph-regularized norm can
incorporate such prior graph information to get accurate discovery and better
interpretability, it fails to consider the opposite effect of variables with
different signs. Motivated by the development of sparse coding and
graph-regularized norm, we propose a novel sparse graph-regularized SVD as a
powerful biclustering tool for analyzing high-dimensional data. The key of this
method is to impose two penalties including a novel graph-regularized norm
($|\pmb{u}|\pmb{L}|\pmb{u}|$) and $L_0$-norm ($\|\pmb{u}\|_0$) on singular
vectors to induce structural sparsity and enhance interpretability. We design
an efficient Alternating Iterative Sparse Projection (AISP) algorithm to solve
it. Finally, we apply our method and related ones to simulated and real data to
show its efficiency in capturing natural blocking structures.","['Wenwen Min', 'Juan Liu', 'Shihua Zhang']","['cs.LG', 'stat.ML', 'I.5.1, I.5.3, H.2.8']",2016-03-19 02:53:48+00:00
http://arxiv.org/abs/1603.06002v1,A Message Passing Algorithm for the Problem of Path Packing in Graphs,"We consider the problem of packing node-disjoint directed paths in a directed
graph. We consider a variant of this problem where each path starts within a
fixed subset of root nodes, subject to a given bound on the length of paths.
This problem is motivated by the so-called kidney exchange problem, but has
potential other applications and is interesting in its own right.
  We propose a new algorithm for this problem based on the message
passing/belief propagation technique. A priori this problem does not have an
associated graphical model, so in order to apply a belief propagation algorithm
we provide a novel representation of the problem as a graphical model. Standard
belief propagation on this model has poor scaling behavior, so we provide an
efficient implementation that significantly decreases the complexity. We
provide numerical results comparing the performance of our algorithm on both
artificially created graphs and real world networks to several alternative
algorithms, including algorithms based on integer programming (IP) techniques.
These comparisons show that our algorithm scales better to large instances than
IP-based algorithms and often finds better solutions than a simple algorithm
that greedily selects the longest path from each root node. In some cases it
also finds better solutions than the ones found by IP-based algorithms even
when the latter are allowed to run significantly longer than our algorithm.","['Patrick Eschenfeldt', 'David Gamarnik']","['cs.DS', 'stat.ML']",2016-03-18 21:26:59+00:00
http://arxiv.org/abs/1603.05953v6,Katyusha: The First Direct Acceleration of Stochastic Gradient Methods,"Nesterov's momentum trick is famously known for accelerating gradient
descent, and has been proven useful in building fast iterative algorithms.
However, in the stochastic setting, counterexamples exist and prevent
Nesterov's momentum from providing similar acceleration, even if the underlying
problem is convex and finite-sum.
  We introduce $\mathtt{Katyusha}$, a direct, primal-only stochastic gradient
method to fix this issue. In convex finite-sum stochastic optimization,
$\mathtt{Katyusha}$ has an optimal accelerated convergence rate, and enjoys an
optimal parallel linear speedup in the mini-batch setting.
  The main ingredient is $\textit{Katyusha momentum}$, a novel ""negative
momentum"" on top of Nesterov's momentum. It can be incorporated into a
variance-reduction based algorithm and speed it up, both in terms of
$\textit{sequential and parallel}$ performance. Since variance reduction has
been successfully applied to a growing list of practical problems, our paper
suggests that in each of such cases, one could potentially try to give Katyusha
a hug.",['Zeyuan Allen-Zhu'],"['math.OC', 'cs.DS', 'cs.LG', 'stat.ML']",2016-03-18 18:46:05+00:00
http://arxiv.org/abs/1603.05876v2,Generalized support vector regression: duality and tensor-kernel representation,"In this paper we study the variational problem associated to support vector
regression in Banach function spaces. Using the Fenchel-Rockafellar duality
theory, we give explicit formulation of the dual problem as well as of the
related optimality conditions. Moreover, we provide a new computational
framework for solving the problem which relies on a tensor-kernel
representation. This analysis overcomes the typical difficulties connected to
learning in Banach spaces. We finally present a large class of tensor-kernels
to which our theory fully applies: power series tensor kernels. This type of
kernels describe Banach spaces of analytic functions and include
generalizations of the exponential and polynomial kernels as well as, in the
complex case, generalizations of the Szeg\""o and Bergman kernels.","['Saverio Salzo', 'Johan A. K. Suykens']","['math.OC', 'math.FA', 'stat.ML', '46E22, 46E15, 62G08, 65K10']",2016-03-18 13:53:29+00:00
http://arxiv.org/abs/1603.05800v1,A Comparison between Deep Neural Nets and Kernel Acoustic Models for Speech Recognition,"We study large-scale kernel methods for acoustic modeling and compare to DNNs
on performance metrics related to both acoustic modeling and recognition.
Measuring perplexity and frame-level classification accuracy, kernel-based
acoustic models are as effective as their DNN counterparts. However, on
token-error-rates DNN models can be significantly better. We have discovered
that this might be attributed to DNN's unique strength in reducing both the
perplexity and the entropy of the predicted posterior probabilities. Motivated
by our findings, we propose a new technique, entropy regularized perplexity,
for model selection. This technique can noticeably improve the recognition
performance of both types of models, and reduces the gap between them. While
effective on Broadcast News, this technique could be also applicable to other
tasks.","['Zhiyun Lu', 'Dong Guo', 'Alireza Bagheri Garakani', 'Kuan Liu', 'Avner May', 'Aurelien Bellet', 'Linxi Fan', 'Michael Collins', 'Brian Kingsbury', 'Michael Picheny', 'Fei Sha']","['cs.LG', 'stat.ML']",2016-03-18 09:16:01+00:00
http://arxiv.org/abs/1603.05770v1,A Probabilistic Machine Learning Approach to Detect Industrial Plant Faults,"Fault detection in industrial plants is a hot research area as more and more
sensor data are being collected throughout the industrial process. Automatic
data-driven approaches are widely needed and seen as a promising area of
investment. This paper proposes an effective machine learning algorithm to
predict industrial plant faults based on classification methods such as
penalized logistic regression, random forest and gradient boosted tree. A
fault's start time and end time are predicted sequentially in two steps by
formulating the original prediction problems as classification problems. The
algorithms described in this paper won first place in the Prognostics and
Health Management Society 2015 Data Challenge.",['Wei Xiao'],"['stat.ML', 'stat.AP']",2016-03-18 05:31:12+00:00
http://arxiv.org/abs/1603.05729v3,Convergence of Contrastive Divergence Algorithm in Exponential Family,"The Contrastive Divergence (CD) algorithm has achieved notable success in
training energy-based models including Restricted Boltzmann Machines and played
a key role in the emergence of deep learning. The idea of this algorithm is to
approximate the intractable term in the exact gradient of the log-likelihood
function by using short Markov chain Monte Carlo (MCMC) runs. The approximate
gradient is computationally-cheap but biased. Whether and why the CD algorithm
provides an asymptotically consistent estimate are still open questions. This
paper studies the asymptotic properties of the CD algorithm in canonical
exponential families, which are special cases of the energy-based model.
Suppose the CD algorithm runs $m$ MCMC transition steps at each iteration $t$
and iteratively generates a sequence of parameter estimates $\{\theta_t\}_{t
\ge 0}$ given an i.i.d. data sample $\{X_i\}_{i=1}^n \sim p_{\theta_\star}$.
Under conditions which are commonly obeyed by the CD algorithm in practice, we
prove the existence of some bounded $m$ such that any limit point of the time
average $\left. \sum_{s=0}^{t-1} \theta_s \right/ t$ as $t \to \infty$ is a
consistent estimate for the true parameter $\theta_\star$. Our proof is based
on the fact that $\{\theta_t\}_{t \ge 0}$ is a homogenous Markov chain
conditional on the data sample $\{X_i\}_{i=1}^n$. This chain meets the
Foster-Lyapunov drift criterion and converges to a random walk around the
Maximum Likelihood Estimate. The range of the random walk shrinks to zero at
rate $\mathcal{O}(1/\sqrt[3]{n})$ as the sample size $n \to \infty$.","['Bai Jiang', 'Tung-Yu Wu', 'Yifan Jin', 'Wing H. Wong']","['stat.ML', '68W48, 60J20, 93E15']",2016-03-17 23:48:15+00:00
http://arxiv.org/abs/1603.05691v4,Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,"Yes, they do. This paper provides the first empirical demonstration that deep
convolutional models really need to be both deep and convolutional, even when
trained with methods such as distillation that allow small or shallow models of
high accuracy to be trained. Although previous research showed that shallow
feed-forward nets sometimes can learn the complex functions previously learned
by deep nets while using the same number of parameters as the deep models they
mimic, in this paper we demonstrate that the same methods cannot be used to
train accurate models on CIFAR-10 unless the student models contain multiple
layers of convolution. Although the student models do not have to be as deep as
the teacher model they mimic, the students need multiple convolutional layers
to learn functions of comparable accuracy as the deep convolutional teacher.","['Gregor Urban', 'Krzysztof J. Geras', 'Samira Ebrahimi Kahou', 'Ozlem Aslan', 'Shengjie Wang', 'Rich Caruana', 'Abdelrahman Mohamed', 'Matthai Philipose', 'Matt Richardson']","['stat.ML', 'cs.LG']",2016-03-17 21:10:38+00:00
http://arxiv.org/abs/1603.05643v2,Variance Reduction for Faster Non-Convex Optimization,"We consider the fundamental problem in non-convex optimization of efficiently
reaching a stationary point. In contrast to the convex case, in the long
history of this basic problem, the only known theoretical results on
first-order non-convex optimization remain to be full gradient descent that
converges in $O(1/\varepsilon)$ iterations for smooth objectives, and
stochastic gradient descent that converges in $O(1/\varepsilon^2)$ iterations
for objectives that are sum of smooth functions.
  We provide the first improvement in this line of research. Our result is
based on the variance reduction trick recently introduced to convex
optimization, as well as a brand new analysis of variance reduction that is
suitable for non-convex optimization. For objectives that are sum of smooth
functions, our first-order minibatch stochastic method converges with an
$O(1/\varepsilon)$ rate, and is faster than full gradient descent by
$\Omega(n^{1/3})$.
  We demonstrate the effectiveness of our methods on empirical risk
minimizations with non-convex loss functions and training neural nets.","['Zeyuan Allen-Zhu', 'Elad Hazan']","['math.OC', 'cs.DS', 'cs.LG', 'cs.NE', 'stat.ML']",2016-03-17 19:55:12+00:00
http://arxiv.org/abs/1603.05642v3,Optimal Black-Box Reductions Between Optimization Objectives,"The diverse world of machine learning applications has given rise to a
plethora of algorithms and optimization methods, finely tuned to the specific
regression or classification task at hand. We reduce the complexity of
algorithm design for machine learning by reductions: we develop reductions that
take a method developed for one setting and apply it to the entire spectrum of
smoothness and strong-convexity in applications.
  Furthermore, unlike existing results, our new reductions are OPTIMAL and more
PRACTICAL. We show how these new reductions give rise to new and faster running
times on training linear classifiers for various families of loss functions,
and conclude with experiments showing their successes also in practice.","['Zeyuan Allen-Zhu', 'Elad Hazan']","['math.OC', 'cs.DS', 'cs.LG', 'stat.ML']",2016-03-17 19:51:59+00:00
http://arxiv.org/abs/1603.05594v1,"Mapping Temporal Variables into the NeuCube for Improved Pattern Recognition, Predictive Modelling and Understanding of Stream Data","This paper proposes a new method for an optimized mapping of temporal
variables, describing a temporal stream data, into the recently proposed
NeuCube spiking neural network architecture. This optimized mapping extends the
use of the NeuCube, which was initially designed for spatiotemporal brain data,
to work on arbitrary stream data and to achieve a better accuracy of temporal
pattern recognition, a better and earlier event prediction and a better
understanding of complex temporal stream data through visualization of the
NeuCube connectivity. The effect of the new mapping is demonstrated on three
bench mark problems. The first one is early prediction of patient sleep stage
event from temporal physiological data. The second one is pattern recognition
of dynamic temporal patterns of traffic in the Bay Area of California and the
last one is the Challenge 2012 contest data set. In all cases the use of the
proposed mapping leads to an improved accuracy of pattern recognition and event
prediction and a better understanding of the data when compared to traditional
machine learning techniques or spiking neural network reservoirs with arbitrary
mapping of the variables.","['Enmei Tu', 'Nikola Kasabov', 'Jie Yang']","['cs.NE', 'cs.AI', 'stat.ML']",2016-03-17 17:58:48+00:00
http://arxiv.org/abs/1603.05486v2,A flexible state space model for learning nonlinear dynamical systems,"We consider a nonlinear state-space model with the state transition and
observation functions expressed as basis function expansions. The coefficients
in the basis function expansions are learned from data. Using a connection to
Gaussian processes we also develop priors on the coefficients, for tuning the
model flexibility and to prevent overfitting to data, akin to a Gaussian
process state-space model. The priors can alternatively be seen as a
regularization, and helps the model in generalizing the data without
sacrificing the richness offered by the basis function expansion. To learn the
coefficients and other unknown parameters efficiently, we tailor an algorithm
using state-of-the-art sequential Monte Carlo methods, which comes with
theoretical guarantees on the learning. Our approach indicates promising
results when evaluated on a classical benchmark as well as real data.","['Andreas Svensson', 'Thomas B. Schön']","['stat.CO', 'cs.SY', 'stat.ML']",2016-03-17 13:51:17+00:00
http://arxiv.org/abs/1603.05412v2,Online semi-parametric learning for inverse dynamics modeling,"This paper presents a semi-parametric algorithm for online learning of a
robot inverse dynamics model. It combines the strength of the parametric and
non-parametric modeling. The former exploits the rigid body dynamics equa-
tion, while the latter exploits a suitable kernel function. We provide an
extensive comparison with other methods from the literature using real data
from the iCub humanoid robot. In doing so we also compare two different
techniques, namely cross validation and marginal likelihood optimization, for
estimating the hyperparameters of the kernel function.","['Diego Romeres', 'Mattia Zorzi', 'Raffaello Camoriano', 'Alessandro Chiuso']","['math.OC', 'cs.LG', 'stat.ML']",2016-03-17 10:14:27+00:00
http://arxiv.org/abs/1603.05359v2,Cascading Bandits for Large-Scale Recommendation Problems,"Most recommender systems recommend a list of items. The user examines the
list, from the first item to the last, and often chooses the first attractive
item and does not examine the rest. This type of user behavior can be modeled
by the cascade model. In this work, we study cascading bandits, an online
learning variant of the cascade model where the goal is to recommend $K$ most
attractive items from a large set of $L$ candidate items. We propose two
algorithms for solving this problem, which are based on the idea of linear
generalization. The key idea in our solutions is that we learn a predictor of
the attraction probabilities of items from their features, as opposing to
learning the attraction probability of each item independently as in the
existing work. This results in practical learning algorithms whose regret does
not depend on the number of items $L$. We bound the regret of one algorithm and
comprehensively evaluate the other on a range of recommendation problems. The
algorithm performs well and outperforms all baselines.","['Shi Zong', 'Hao Ni', 'Kenny Sung', 'Nan Rosemary Ke', 'Zheng Wen', 'Branislav Kveton']","['cs.LG', 'stat.ML']",2016-03-17 05:37:12+00:00
http://arxiv.org/abs/1603.05305v4,Near-Optimal Stochastic Approximation for Online Principal Component Estimation,"Principal component analysis (PCA) has been a prominent tool for
high-dimensional data analysis. Online algorithms that estimate the principal
component by processing streaming data are of tremendous practical and
theoretical interests. Despite its rich applications, theoretical convergence
analysis remains largely open. In this paper, we cast online PCA into a
stochastic nonconvex optimization problem, and we analyze the online PCA
algorithm as a stochastic approximation iteration. The stochastic approximation
iteration processes data points incrementally and maintains a running estimate
of the principal component. We prove for the first time a nearly optimal
finite-sample error bound for the online PCA algorithm. Under the subgaussian
assumption, we show that the finite-sample error bound closely matches the
minimax information lower bound.","['Chris Junchi Li', 'Mengdi Wang', 'Han Liu', 'Tong Zhang']","['math.OC', 'stat.ML']",2016-03-16 22:35:58+00:00
http://arxiv.org/abs/1603.05296v5,Exact Clustering of Weighted Graphs via Semidefinite Programming,"As a model problem for clustering, we consider the densest k-disjoint-clique
problem of partitioning a weighted complete graph into k disjoint subgraphs
such that the sum of the densities of these subgraphs is maximized. We
establish that such subgraphs can be recovered from the solution of a
particular semidefinite relaxation with high probability if the input graph is
sampled from a distribution of clusterable graphs. Specifically, the
semidefinite relaxation is exact if the graph consists of k large disjoint
subgraphs, corresponding to clusters, with weight concentrated within these
subgraphs, plus a moderate number of outliers. Further, we establish that if
noise is weakly obscuring these clusters, i.e, the between-cluster edges are
assigned very small weights, then we can recover significantly smaller
clusters. For example, we show that in approximately sparse graphs, where the
between-cluster weights tend to zero as the size n of the graph tends to
infinity, we can recover clusters of size polylogarithmic in n. Empirical
evidence from numerical simulations is also provided to support these
theoretical phase transitions to perfect recovery of the cluster structure.","['Aleksis Pirinen', 'Brendan Ames']","['math.OC', 'stat.ML']",2016-03-16 22:04:28+00:00
http://arxiv.org/abs/1603.05152v1,Feature Selection as a Multiagent Coordination Problem,"Datasets with hundreds to tens of thousands features is the new norm. Feature
selection constitutes a central problem in machine learning, where the aim is
to derive a representative set of features from which to construct a
classification (or prediction) model for a specific task. Our experimental
study involves microarray gene expression datasets, these are high-dimensional
and noisy datasets that contain genetic data typically used for distinguishing
between benign or malicious tissues or classifying different types of cancer.
In this paper, we formulate feature selection as a multiagent coordination
problem and propose a novel feature selection method using multiagent
reinforcement learning. The central idea of the proposed approach is to
""assign"" a reinforcement learning agent to each feature where each agent learns
to control a single feature, we refer to this approach as MARL. Applying this
to microarray datasets creates an enormous multiagent coordination problem
between thousands of learning agents. To address the scalability challenge we
apply a form of reward shaping called CLEAN rewards. We compare in total nine
feature selection methods, including state-of-the-art methods, and show that
the proposed method using CLEAN rewards can significantly scale-up, thus
outperforming the rest of learning-based methods. We further show that a hybrid
variant of MARL achieves the best overall performance.","['Kleanthis Malialis', 'Jun Wang', 'Gary Brooks', 'George Frangou']","['cs.LG', 'stat.ML']",2016-03-16 15:49:37+00:00
http://arxiv.org/abs/1603.05106v2,One-Shot Generalization in Deep Generative Models,"Humans have an impressive ability to reason about new concepts and
experiences from just a single example. In particular, humans have an ability
for one-shot generalization: an ability to encounter a new concept, understand
its structure, and then be able to generate compelling alternative variations
of the concept. We develop machine learning systems with this important
capacity by developing new deep generative models, models that combine the
representational power of deep learning with the inferential power of Bayesian
reasoning. We develop a class of sequential generative models that are built on
the principles of feedback and attention. These two characteristics lead to
generative models that are among the state-of-the art in density estimation and
image generation. We demonstrate the one-shot generalization ability of our
models using three tasks: unconditional sampling, generating new exemplars of a
given concept, and generating new exemplars of a family of concepts. In all
cases our models are able to generate compelling and diverse samples---having
seen new examples just once---providing an important class of general-purpose
models for one-shot machine learning.","['Danilo Jimenez Rezende', 'Shakir Mohamed', 'Ivo Danihelka', 'Karol Gregor', 'Daan Wierstra']","['stat.ML', 'cs.AI', 'cs.LG']",2016-03-16 14:10:00+00:00
http://arxiv.org/abs/1603.05060v1,Short-term time series prediction using Hilbert space embeddings of autoregressive processes,"Linear autoregressive models serve as basic representations of discrete time
stochastic processes. Different attempts have been made to provide non-linear
versions of the basic autoregressive process, including different versions
based on kernel methods. Motivated by the powerful framework of Hilbert space
embeddings of distributions, in this paper we apply this methodology for the
kernel embedding of an autoregressive process of order $p$. By doing so, we
provide a non-linear version of an autoregressive process, that shows increased
performance over the linear model in highly complex time series. We use the
method proposed for one-step ahead forecasting of different time-series, and
compare its performance against other non-linear methods.","['Edgar A. Valencia', 'Mauricio A. Álvarez']",['stat.ML'],2016-03-16 12:24:24+00:00
http://arxiv.org/abs/1603.04981v6,An Approximate Dynamic Programming Approach to Adversarial Online Learning,"We describe an approximate dynamic programming (ADP) approach to compute
approximations of the optimal strategies and of the minimal losses that can be
guaranteed in discounted repeated games with vector-valued losses. Such games
prominently arise in the analysis of regret in repeated decision-making in
adversarial environments, also known as adversarial online learning. At the
core of our approach is a characterization of the lower Pareto frontier of the
set of expected losses that a player can guarantee in these games as the unique
fixed point of a set-valued dynamic programming operator. When applied to the
problem of regret minimization with discounted losses, our approach yields
algorithms that achieve markedly improved performance bounds compared to
off-the-shelf online learning algorithms like Hedge. These results thus suggest
the significant potential of ADP-based approaches in adversarial online
learning.","['Vijay Kamble', 'Patrick Loiseau', 'Jean Walrand']","['cs.GT', 'cs.DS', 'cs.LG', 'stat.ML']",2016-03-16 07:04:24+00:00
http://arxiv.org/abs/1603.04918v1,Data Clustering and Graph Partitioning via Simulated Mixing,"Spectral clustering approaches have led to well-accepted algorithms for
finding accurate clusters in a given dataset. However, their application to
large-scale datasets has been hindered by computational complexity of
eigenvalue decompositions. Several algorithms have been proposed in the recent
past to accelerate spectral clustering, however they compromise on the accuracy
of the spectral clustering to achieve faster speed. In this paper, we propose a
novel spectral clustering algorithm based on a mixing process on a graph.
Unlike the existing spectral clustering algorithms, our algorithm does not
require computing eigenvectors. Specifically, it finds the equivalent of a
linear combination of eigenvectors of the normalized similarity matrix weighted
with corresponding eigenvalues. This linear combination is then used to
partition the dataset into meaningful clusters. Simulations on real datasets
show that partitioning datasets based on such linear combinations of
eigenvectors achieves better accuracy than standard spectral clustering methods
as the number of clusters increase. Our algorithm can easily be implemented in
a distributed setting.","['Shahzad Bhatti', 'Carolyn Beck', 'Angelia Nedic']","['cs.LG', 'stat.ML']",2016-03-15 23:06:19+00:00
http://arxiv.org/abs/1603.04904v2,Turing learning: a metric-free approach to inferring behavior and its application to swarms,"We propose Turing Learning, a novel system identification method for
inferring the behavior of natural or artificial systems. Turing Learning
simultaneously optimizes two populations of computer programs, one representing
models of the behavior of the system under investigation, and the other
representing classifiers. By observing the behavior of the system as well as
the behaviors produced by the models, two sets of data samples are obtained.
The classifiers are rewarded for discriminating between these two sets, that
is, for correctly categorizing data samples as either genuine or counterfeit.
Conversely, the models are rewarded for 'tricking' the classifiers into
categorizing their data samples as genuine. Unlike other methods for system
identification, Turing Learning does not require predefined metrics to quantify
the difference between the system and its models. We present two case studies
with swarms of simulated robots and prove that the underlying behaviors cannot
be inferred by a metric-based system identification method. By contrast, Turing
Learning infers the behaviors with high accuracy. It also produces a useful
by-product - the classifiers - that can be used to detect abnormal behavior in
the swarm. Moreover, we show that Turing Learning also successfully infers the
behavior of physical robot swarms. The results show that collective behaviors
can be directly inferred from motion trajectories of individuals in the swarm,
which may have significant implications for the study of animal collectives.
Furthermore, Turing Learning could prove useful whenever a behavior is not
easily characterizable using metrics, making it suitable for a wide range of
applications.","['Wei Li', 'Melvin Gauci', 'Roderich Gross']","['stat.ML', 'cs.LG', 'cs.NE', 'I.2.4; I.2.6; I.2.9; I.2.11; I.5.1']",2016-03-15 22:20:52+00:00
http://arxiv.org/abs/1603.04882v1,Bias Correction for Regularized Regression and its Application in Learning with Streaming Data,"We propose an approach to reduce the bias of ridge regression and
regularization kernel network. When applied to a single data set the new
algorithms have comparable learning performance with the original ones. When
applied to incremental learning with block wise streaming data the new
algorithms are more efficient due to bias reduction. Both theoretical
characterizations and simulation studies are used to verify the effectiveness
of these new algorithms.",['Qiang Wu'],"['stat.ML', 'cs.LG']",2016-03-15 20:46:46+00:00
http://arxiv.org/abs/1603.04833v1,Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images,"Vision impairment due to pathological damage of the retina can largely be
prevented through periodic screening using fundus color imaging. However the
challenge with large scale screening is the inability to exhaustively detect
fine blood vessels crucial to disease diagnosis. In this work we present a
computational imaging framework using deep and ensemble learning for reliable
detection of blood vessels in fundus color images. An ensemble of deep
convolutional neural networks is trained to segment vessel and non-vessel areas
of a color fundus image. During inference, the responses of the individual
ConvNets of the ensemble are averaged to form the final segmentation. In
experimental evaluation with the DRIVE database, we achieve the objective of
vessel detection with maximum average accuracy of 94.7\% and area under ROC
curve of 0.9283.","['Debapriya Maji', 'Anirban Santara', 'Pabitra Mitra', 'Debdoot Sheet']","['cs.LG', 'cs.CV', 'stat.ML']",2016-03-15 19:40:34+00:00
http://arxiv.org/abs/1603.04733v5,Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors,"We introduce a variational Bayesian neural network where the parameters are
governed via a probability distribution on random matrices. Specifically, we
employ a matrix variate Gaussian \cite{gupta1999matrix} parameter posterior
distribution where we explicitly model the covariance among the input and
output dimensions of each layer. Furthermore, with approximate covariance
matrices we can achieve a more efficient way to represent those correlations
that is also cheaper than fully factorized parameter posteriors. We further
show that with the ""local reprarametrization trick""
\cite{kingma2015variational} on this posterior distribution we arrive at a
Gaussian Process \cite{rasmussen2006gaussian} interpretation of the hidden
units in each layer and we, similarly with \cite{gal2015dropout}, provide
connections with deep Gaussian processes. We continue in taking advantage of
this duality and incorporate ""pseudo-data"" \cite{snelson2005sparse} in our
model, which in turn allows for more efficient sampling while maintaining the
properties of the original model. The validity of the proposed approach is
verified through extensive experiments.","['Christos Louizos', 'Max Welling']","['stat.ML', 'cs.LG']",2016-03-15 16:01:14+00:00
http://arxiv.org/abs/1603.04628v1,Accelerating a hybrid continuum-atomistic fluidic model with on-the-fly machine learning,"We present a hybrid continuum-atomistic scheme which combines molecular
dynamics (MD) simulations with on-the-fly machine learning techniques for the
accurate and efficient prediction of multiscale fluidic systems. By using a
Gaussian process as a surrogate model for the computationally expensive MD
simulations, we use Bayesian inference to predict the system behaviour at the
atomistic scale, purely by consideration of the macroscopic inputs and outputs.
Whenever the uncertainty of this prediction is greater than a predetermined
acceptable threshold, a new MD simulation is performed to continually augment
the database, which is never required to be complete. This provides a
substantial enhancement to the current generation of hybrid methods, which
often require many similar atomistic simulations to be performed, discarding
information after it is used once.
  We apply our hybrid scheme to nano-confined unsteady flow through a
high-aspect-ratio converging-diverging channel, and make comparisons between
the new scheme and full MD simulations for a range of uncertainty thresholds
and initial databases. For low thresholds, our hybrid solution is highly
accurate\,---\,within the thermal noise of a full MD simulation. As the
uncertainty threshold is raised, the accuracy of our scheme decreases and the
computational speed-up increases (relative to a full MD simulation), enabling
the compromise between precision and efficiency to be tuned. The speed-up of
our hybrid solution ranges from an order of magnitude, with no initial
database, to cases where an extensive initial database ensures no new MD
simulations are required.","['David Stephenson', 'James R Kermode', 'Duncan A Lockerby']","['physics.flu-dyn', 'cond-mat.mes-hall', 'stat.ML']",2016-03-15 10:45:15+00:00
http://arxiv.org/abs/1603.04572v1,On the exact recovery of sparse signals via conic relaxations,"In this note we compare two recently proposed semidefinite relaxations for
the sparse linear regression problem by Pilanci, Wainwright and El Ghaoui
(Sparse learning via boolean relaxations, 2015) and Dong, Chen and Linderoth
(Relaxation vs. Regularization A conic optimization perspective of statistical
variable selection, 2015). We focus on the cardinality constrained formulation,
and prove that the relaxation proposed by Dong, etc. is theoretically no weaker
than the one proposed by Pilanci, etc. Therefore any sufficient condition of
exact recovery derived by Pilanci can be readily applied to the other
relaxation, including their results on high probability recovery for Gaussian
ensemble. Finally we provide empirical evidence that the relaxation by Dong,
etc. requires much fewer observations to guarantee the recovery of true
support.",['Hongbo Dong'],"['stat.ML', 'math.OC', '90C22, 90C25, 90C90']",2016-03-15 07:06:33+00:00
http://arxiv.org/abs/1603.04549v7,Matching while Learning,"We consider the problem faced by a service platform that needs to match
limited supply with demand but also to learn the attributes of new users in
order to match them better in the future. We introduce a benchmark model with
heterogeneous ""workers"" (demand) and a limited supply of ""jobs"" that arrive
over time. Job types are known to the platform, but worker types are unknown
and must be learned by observing match outcomes. Workers depart after
performing a certain number of jobs. The expected payoff from a match depends
on the pair of types and the goal is to maximize the steady-state rate of
accumulation of payoff. Though we use terminology inspired by labor markets,
our framework applies more broadly to platforms where a limited supply of
heterogeneous products is matched to users over time.
  Our main contribution is a complete characterization of the structure of the
optimal policy in the limit that each worker performs many jobs. The platform
faces a trade-off for each worker between myopically maximizing payoffs
(exploitation) and learning the type of the worker (exploration). This creates
a multitude of multi-armed bandit problems, one for each worker, coupled
together by the constraint on availability of jobs of different types (capacity
constraints). We find that the platform should estimate a shadow price for each
job type, and use the payoffs adjusted by these prices, first, to determine its
learning goals and then, for each worker, (i) to balance learning with payoffs
during the ""exploration phase,"" and (ii) to myopically match after it has
achieved its learning goals during the ""exploitation phase.""","['Ramesh Johari', 'Vijay Kamble', 'Yash Kanoria']","['cs.LG', 'cs.DS', 'stat.ME', 'stat.ML']",2016-03-15 04:29:31+00:00
http://arxiv.org/abs/1603.04419v3,Modeling and Estimation of Discrete-Time Reciprocal Processes via Probabilistic Graphical Models,"Reciprocal processes are acausal generalizations of Markov processes
introduced by Bernstein in 1932. In the literature, a significant amount of
attention has been focused on developing dynamical models for reciprocal
processes. In this paper, we provide a probabilistic graphical model for
reciprocal processes. This leads to a principled solution of the smoothing
problem via message passing algorithms. For the finite state space case,
convergence analysis is revisited via the Hilbert metric.",['Francesca Paola Carli'],"['stat.ML', 'math.OC']",2016-03-14 19:52:04+00:00
http://arxiv.org/abs/1603.04381v2,A ranking approach to global optimization,"We consider the problem of maximizing an unknown function over a compact and
convex set using as few observations as possible. We observe that the
optimization of the function essentially relies on learning the induced
bipartite ranking rule of f. Based on this idea, we relate global optimization
to bipartite ranking which allows to address problems with high dimensional
input space, as well as cases of functions with weak regularity properties. The
paper introduces novel meta-algorithms for global optimization which rely on
the choice of any bipartite ranking method. Theoretical properties are provided
as well as convergence guarantees and equivalences between various optimization
methods are obtained as a by-product. Eventually, numerical evidence is given
to show that the main algorithm of the paper which adapts empirically to the
underlying ranking structure essentially outperforms existing state-of-the-art
global optimization algorithms in typical benchmarks.","['Cédric Malherbe', 'Nicolas Vayatis']",['stat.ML'],2016-03-14 18:40:54+00:00
http://arxiv.org/abs/1603.04319v1,Learning Network of Multivariate Hawkes Processes: A Time Series Approach,"Learning the influence structure of multiple time series data is of great
interest to many disciplines. This paper studies the problem of recovering the
causal structure in network of multivariate linear Hawkes processes. In such
processes, the occurrence of an event in one process affects the probability of
occurrence of new events in some other processes. Thus, a natural notion of
causality exists between such processes captured by the support of the
excitation matrix. We show that the resulting causal influence network is
equivalent to the Directed Information graph (DIG) of the processes, which
encodes the causal factorization of the joint distribution of the processes.
Furthermore, we present an algorithm for learning the support of excitation
matrix (or equivalently the DIG). The performance of the algorithm is evaluated
on synthesized multivariate Hawkes networks as well as a stock market and
MemeTracker real-world dataset.","['Jalal Etesami', 'Negar Kiyavash', 'Kun Zhang', 'Kushagra Singhal']","['cs.LG', 'cs.AI', 'stat.ML']",2016-03-14 16:08:26+00:00
http://arxiv.org/abs/1603.04245v1,A Variational Perspective on Accelerated Methods in Optimization,"Accelerated gradient methods play a central role in optimization, achieving
optimal rates in many settings. While many generalizations and extensions of
Nesterov's original acceleration method have been proposed, it is not yet clear
what is the natural scope of the acceleration concept. In this paper, we study
accelerated methods from a continuous-time perspective. We show that there is a
Lagrangian functional that we call the \emph{Bregman Lagrangian} which
generates a large class of accelerated methods in continuous time, including
(but not limited to) accelerated gradient descent, its non-Euclidean extension,
and accelerated higher-order gradient methods. We show that the continuous-time
limit of all of these methods correspond to traveling the same curve in
spacetime at different speeds. From this perspective, Nesterov's technique and
many of its generalizations can be viewed as a systematic way to go from the
continuous-time curves generated by the Bregman Lagrangian to a family of
discrete-time accelerated algorithms.","['Andre Wibisono', 'Ashia C. Wilson', 'Michael I. Jordan']","['math.OC', 'cs.LG', 'stat.ML']",2016-03-14 13:00:18+00:00
http://arxiv.org/abs/1603.04190v2,Online Isotonic Regression,"We consider the online version of the isotonic regression problem. Given a
set of linearly ordered points (e.g., on the real line), the learner must
predict labels sequentially at adversarially chosen positions and is evaluated
by her total squared loss compared against the best isotonic (non-decreasing)
function in hindsight. We survey several standard online learning algorithms
and show that none of them achieve the optimal regret exponent; in fact, most
of them (including Online Gradient Descent, Follow the Leader and Exponential
Weights) incur linear regret. We then prove that the Exponential Weights
algorithm played over a covering net of isotonic functions has a regret bounded
by $O\big(T^{1/3} \log^{2/3}(T)\big)$ and present a matching $\Omega(T^{1/3})$
lower bound on regret. We provide a computationally efficient version of this
algorithm. We also analyze the noise-free case, in which the revealed labels
are isotonic, and show that the bound can be improved to $O(\log T)$ or even to
$O(1)$ (when the labels are revealed in isotonic order). Finally, we extend the
analysis beyond squared loss and give bounds for entropic loss and absolute
loss.","['Wojciech Kotłowski', 'Wouter M. Koolen', 'Alan Malek']","['cs.LG', 'stat.ML']",2016-03-14 10:26:23+00:00
http://arxiv.org/abs/1603.04153v1,Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal,"We explore the top-$K$ rank aggregation problem. Suppose a collection of
items is compared in pairs repeatedly, and we aim to recover a consistent
ordering that focuses on the top-$K$ ranked items based on partially revealed
preference information. We investigate the Bradley-Terry-Luce model in which
one ranks items according to their perceived utilities modeled as noisy
observations of their underlying true utilities. Our main contributions are
two-fold. First, in a general comparison model where item pairs to compare are
given a priori, we attain an upper and lower bound on the sample size for
reliable recovery of the top-$K$ ranked items. Second, more importantly,
extending the result to a random comparison model where item pairs to compare
are chosen independently with some probability, we show that in slightly
restricted regimes, the gap between the derived bounds reduces to a constant
factor, hence reveals that a spectral method can achieve the minimax optimality
on the (order-wise) sample size required for top-$K$ ranking. That is to say,
we demonstrate a spectral method alone to be sufficient to achieve the
optimality and advantageous in terms of computational complexity, as it does
not require an additional stage of maximum likelihood estimation that a
state-of-the-art scheme employs to achieve the optimality. We corroborate our
main results by numerical experiments.","['Minje Jang', 'Sunghyun Kim', 'Changho Suh', 'Sewoong Oh']","['cs.LG', 'cs.IT', 'cs.SI', 'math.IT', 'stat.ML']",2016-03-14 07:01:28+00:00
http://arxiv.org/abs/1603.04136v4,On the Influence of Momentum Acceleration on Online Learning,"The article examines in some detail the convergence rate and
mean-square-error performance of momentum stochastic gradient methods in the
constant step-size and slow adaptation regime. The results establish that
momentum methods are equivalent to the standard stochastic gradient method with
a re-scaled (larger) step-size value. The size of the re-scaling is determined
by the value of the momentum parameter. The equivalence result is established
for all time instants and not only in steady-state. The analysis is carried out
for general strongly convex and smooth risk functions, and is not limited to
quadratic risks. One notable conclusion is that the well-known bene ts of
momentum constructions for deterministic optimization problems do not
necessarily carry over to the adaptive online setting when small constant
step-sizes are used to enable continuous adaptation and learn- ing in the
presence of persistent gradient noise. From simulations, the equivalence
between momentum and standard stochastic gradient methods is also observed for
non-differentiable and non-convex problems.","['Kun Yuan', 'Bicheng Ying', 'Ali H. Sayed']","['math.OC', 'cs.LG', 'stat.ML']",2016-03-14 05:05:54+00:00
http://arxiv.org/abs/1603.04119v1,Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains,"High-dimensional observations and complex real-world dynamics present major
challenges in reinforcement learning for both function approximation and
exploration. We address both of these challenges with two complementary
techniques: First, we develop a gradient-boosting style, non-parametric
function approximator for learning on $Q$-function residuals. And second, we
propose an exploration strategy inspired by the principles of state abstraction
and information acquisition under uncertainty. We demonstrate the empirical
effectiveness of these techniques, first, as a preliminary check, on two
standard tasks (Blackjack and $n$-Chain), and then on two much larger and more
realistic tasks with high-dimensional observation spaces. Specifically, we
introduce two benchmarks built within the game Minecraft where the observations
are pixel arrays of the agent's visual field. A combination of our two
algorithmic techniques performs competitively on the standard
reinforcement-learning tasks while consistently and substantially outperforming
baselines on the two tasks with high-dimensional observation spaces. The new
function approximator, exploration strategy, and evaluation benchmarks are each
of independent interest in the pursuit of reinforcement-learning methods that
scale to real-world domains.","['David Abel', 'Alekh Agarwal', 'Fernando Diaz', 'Akshay Krishnamurthy', 'Robert E. Schapire']","['cs.AI', 'cs.LG', 'stat.ML']",2016-03-14 03:16:25+00:00
http://arxiv.org/abs/1603.04118v2,Active Algorithms For Preference Learning Problems with Multiple Populations,"In this paper we model the problem of learning preferences of a population as
an active learning problem. We propose an algorithm can adaptively choose pairs
of items to show to users coming from a heterogeneous population, and use the
obtained reward to decide which pair of items to show next. We provide
computationally efficient algorithms with provable sample complexity guarantees
for this problem in both the noiseless and noisy cases. In the process of
establishing sample complexity guarantees for our algorithms, we establish new
results using a Nystr{\""o}m-like method which can be of independent interest.
We supplement our theoretical results with experimental comparisons.","['Aniruddha Bhargava', 'Ravi Ganti', 'Robert Nowak']","['stat.ML', 'cs.AI', 'cs.LG']",2016-03-14 03:08:24+00:00
http://arxiv.org/abs/1603.04064v1,A Grothendieck-type inequality for local maxima,"A large number of problems in optimization, machine learning, signal
processing can be effectively addressed by suitable semidefinite programming
(SDP) relaxations. Unfortunately, generic SDP solvers hardly scale beyond
instances with a few hundreds variables (in the underlying combinatorial
problem). On the other hand, it has been observed empirically that an effective
strategy amounts to introducing a (non-convex) rank constraint, and solving the
resulting smooth optimization problem by ascent methods. This non-convex
problem has --generically-- a large number of local maxima, and the reason for
this success is therefore unclear.
  This paper provides rigorous support for this approach. For the problem of
maximizing a linear functional over the elliptope, we prove that all local
maxima are within a small gap from the SDP optimum. In several problems of
interest, arbitrarily small relative error can be achieved by taking the rank
constraint $k$ to be of order one, independently of the problem size.",['Andrea Montanari'],"['math.OC', 'stat.ML']",2016-03-13 18:44:24+00:00
http://arxiv.org/abs/1603.04017v2,Clustering Financial Time Series: How Long is Enough?,"Researchers have used from 30 days to several years of daily returns as
source data for clustering financial time series based on their correlations.
This paper sets up a statistical framework to study the validity of such
practices. We first show that clustering correlated random variables from their
observed values is statistically consistent. Then, we also give a first
empirical answer to the much debated question: How long should the time series
be? If too short, the clusters found can be spurious; if too long, dynamics can
be smoothed out.","['Gautier Marti', 'Sébastien Andler', 'Frank Nielsen', 'Philippe Donnat']","['stat.ML', 'q-fin.ST']",2016-03-13 10:47:00+00:00
http://arxiv.org/abs/1603.03980v2,On Learning High Dimensional Structured Single Index Models,"Single Index Models (SIMs) are simple yet flexible semi-parametric models for
machine learning, where the response variable is modeled as a monotonic
function of a linear combination of features. Estimation in this context
requires learning both the feature weights and the nonlinear function that
relates features to observations. While methods have been described to learn
SIMs in the low dimensional regime, a method that can efficiently learn SIMs in
high dimensions, and under general structural assumptions, has not been
forthcoming. In this paper, we propose computationally efficient algorithms for
SIM inference in high dimensions with structural constraints. Our general
approach specializes to sparsity, group sparsity, and low-rank assumptions
among others. Experiments show that the proposed method enjoys superior
predictive performance when compared to generalized linear models, and achieves
results comparable to or better than single layer feedforward neural networks
with significantly less computational cost.","['Nikhil Rao', 'Ravi Ganti', 'Laura Balzano', 'Rebecca Willett', 'Robert Nowak']","['stat.ML', 'cs.AI', 'cs.LG']",2016-03-13 01:53:40+00:00
http://arxiv.org/abs/1603.03977v3,Pufferfish Privacy Mechanisms for Correlated Data,"Many modern databases include personal and sensitive correlated data, such as
private information on users connected together in a social network, and
measurements of physical activity of single subjects across time. However,
differential privacy, the current gold standard in data privacy, does not
adequately address privacy issues in this kind of data.
  This work looks at a recent generalization of differential privacy, called
Pufferfish, that can be used to address privacy in correlated data. The main
challenge in applying Pufferfish is a lack of suitable mechanisms. We provide
the first mechanism -- the Wasserstein Mechanism -- which applies to any
general Pufferfish framework. Since this mechanism may be computationally
inefficient, we provide an additional mechanism that applies to some practical
cases such as physical activity measurements across time, and is
computationally efficient. Our experimental evaluations indicate that this
mechanism provides privacy and utility for synthetic as well as real data in
two separate domains.","['Shuang Song', 'Yizhen Wang', 'Kamalika Chaudhuri']","['cs.LG', 'cs.CR', 'stat.ML']",2016-03-13 00:47:15+00:00
http://arxiv.org/abs/1603.03972v2,"Laplacian Eigenmaps from Sparse, Noisy Similarity Measurements","Manifold learning and dimensionality reduction techniques are ubiquitous in
science and engineering, but can be computationally expensive procedures when
applied to large data sets or when similarities are expensive to compute. To
date, little work has been done to investigate the tradeoff between
computational resources and the quality of learned representations. We present
both theoretical and experimental explorations of this question. In particular,
we consider Laplacian eigenmaps embeddings based on a kernel matrix, and
explore how the embeddings behave when this kernel matrix is corrupted by
occlusion and noise. Our main theoretical result shows that under modest noise
and occlusion assumptions, we can (with high probability) recover a good
approximation to the Laplacian eigenmaps embedding based on the uncorrupted
kernel matrix. Our results also show how regularization can aid this
approximation. Experimentally, we explore the effects of noise and occlusion on
Laplacian eigenmaps embeddings of two real-world data sets, one from speech
processing and one from neuroscience, as well as a synthetic data set.","['Keith Levin', 'Vince Lyzinski']",['stat.ML'],2016-03-12 23:02:20+00:00
http://arxiv.org/abs/1603.03827v1,Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks,"Recent approaches based on artificial neural networks (ANNs) have shown
promising results for short-text classification. However, many short texts
occur in sequences (e.g., sentences in a document or utterances in a dialog),
and most existing ANN-based systems do not leverage the preceding short texts
when classifying a subsequent one. In this work, we present a model based on
recurrent neural networks and convolutional neural networks that incorporates
the preceding short texts. Our model achieves state-of-the-art results on three
different datasets for dialog act prediction.","['Ji Young Lee', 'Franck Dernoncourt']","['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']",2016-03-12 00:02:51+00:00
http://arxiv.org/abs/1603.03805v2,Median-Truncated Nonconvex Approach for Phase Retrieval with Outliers,"This paper investigates the phase retrieval problem, which aims to recover a
signal from the magnitudes of its linear measurements. We develop statistically
and computationally efficient algorithms for the situation when the
measurements are corrupted by sparse outliers that can take arbitrary values.
We propose a novel approach to robustify the gradient descent algorithm by
using the sample median as a guide for pruning spurious samples in
initialization and local search. Adopting the Poisson loss and the reshaped
quadratic loss respectively, we obtain two algorithms termed median-TWF and
median-RWF, both of which provably recover the signal from a near-optimal
number of measurements when the measurement vectors are composed of i.i.d.
Gaussian entries, up to a logarithmic factor, even when a constant fraction of
the measurements are adversarially corrupted. We further show that both
algorithms are stable in the presence of additional dense bounded noise. Our
analysis is accomplished by developing non-trivial concentration results of
median-related quantities, which may be of independent interest. We provide
numerical experiments to demonstrate the effectiveness of our approach.","['Huishuai Zhang', 'Yuejie Chi', 'Yingbin Liang']",['stat.ML'],2016-03-11 22:10:04+00:00
http://arxiv.org/abs/1603.03799v2,$\ell_1$ Adaptive Trend Filter via Fast Coordinate Descent,"Identifying the unknown underlying trend of a given noisy signal is extremely
useful for a wide range of applications. The number of potential trends might
be exponential, which can be computationally exhaustive even for short signals.
Another challenge, is the presence of abrupt changes and outliers at unknown
times which impart resourceful information regarding the signal's
characteristics. In this paper, we present the $\ell_1$ Adaptive Trend Filter,
which can consistently identify the components in the underlying trend and
multiple level-shifts, even in the presence of outliers. Additionally, an
enhanced coordinate descent algorithm which exploit the filter design is
presented. Some implementation details are discussed and a version in the Julia
language is presented along with two distinct applications to illustrate the
filter's potential.","['Mario Souto', 'Joaquim D. Garcia', 'Gustavo C. Amaral']","['stat.AP', 'math.OC', 'stat.ML']",2016-03-11 21:58:02+00:00
http://arxiv.org/abs/1603.03788v1,A Primer on the Signature Method in Machine Learning,"In these notes, we wish to provide an introduction to the signature method,
focusing on its basic theoretical properties and recent numerical applications.
  The notes are split into two parts. The first part focuses on the definition
and fundamental properties of the signature of a path, or the path signature.
We have aimed for a minimalistic approach, assuming only familiarity with
classical real analysis and integration theory, and supplementing theory with
straightforward examples. We have chosen to focus in detail on the principle
properties of the signature which we believe are fundamental to understanding
its role in applications. We also present an informal discussion on some of its
deeper properties and briefly mention the role of the signature in rough paths
theory, which we hope could serve as a light introduction to rough paths for
the interested reader.
  The second part of these notes discusses practical applications of the path
signature to the area of machine learning. The signature approach represents a
non-parametric way for extraction of characteristic features from data. The
data are converted into a multi-dimensional path by means of various embedding
algorithms and then processed for computation of individual terms of the
signature which summarise certain information contained in the data. The
signature thus transforms raw data into a set of features which are used in
machine learning tasks. We will review current progress in applications of
signatures to machine learning problems.","['Ilya Chevyrev', 'Andrey Kormilitzin']","['stat.ML', 'cs.LG', 'stat.ME']",2016-03-11 21:24:42+00:00
http://arxiv.org/abs/1603.03724v1,Efficient Clustering of Correlated Variables and Variable Selection in High-Dimensional Linear Models,"In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable
selection in high dimensional sparse regression models with strongly correlated
variables. To handle correlated variables, the concept of clustering or
grouping variables and then pursuing model fitting is widely accepted. When the
dimension is very high, finding an appropriate group structure is as difficult
as the original problem. The ACL is a three-stage procedure where, at the first
stage, we use the Lasso(or its adaptive or thresholded version) to do initial
selection, then we also include those variables which are not selected by the
Lasso but are strongly correlated with the variables selected by the Lasso. At
the second stage we cluster the variables based on the reduced set of
predictors and in the third stage we perform sparse estimation such as Lasso on
cluster representatives or the group Lasso based on the structures generated by
clustering procedure. We show that our procedure is consistent and efficient in
finding true underlying population group structure(under assumption of
irrepresentable and beta-min conditions). We also study the group selection
consistency of our method and we support the theory using simulated and
pseudo-real dataset examples.","['Niharika Gauraha', 'Swapan K. Parui']","['stat.ML', 'cs.LG']",2016-03-11 19:06:33+00:00
http://arxiv.org/abs/1603.03678v2,Nonstationary Distance Metric Learning,"Recent work in distance metric learning has focused on learning
transformations of data that best align with provided sets of pairwise
similarity and dissimilarity constraints. The learned transformations lead to
improved retrieval, classification, and clustering algorithms due to the better
adapted distance or similarity measures. Here, we introduce the problem of
learning these transformations when the underlying constraint generation
process is nonstationary. This nonstationarity can be due to changes in either
the ground-truth clustering used to generate constraints or changes to the
feature subspaces in which the class structure is apparent. We propose and
evaluate COMID-SADL, an adaptive, online approach for learning and tracking
optimal metrics as they change over time that is highly robust to a variety of
nonstationary behaviors in the changing metric. We demonstrate COMID-SADL on
both real and synthetic data sets and show significant performance improvements
relative to previously proposed batch and online distance metric learning
algorithms.","['Kristjan Greenewald', 'Stephen Kelley', 'Alfred Hero']","['stat.ML', 'cs.LG']",2016-03-11 16:16:45+00:00
http://arxiv.org/abs/1603.03629v2,Square Root Graphical Models: Multivariate Generalizations of Univariate Exponential Families that Permit Positive Dependencies,"We develop Square Root Graphical Models (SQR), a novel class of parametric
graphical models that provides multivariate generalizations of univariate
exponential family distributions. Previous multivariate graphical models [Yang
et al. 2015] did not allow positive dependencies for the exponential and
Poisson generalizations. However, in many real-world datasets, variables
clearly have positive dependencies. For example, the airport delay time in New
York---modeled as an exponential distribution---is positively related to the
delay time in Boston. With this motivation, we give an example of our model
class derived from the univariate exponential distribution that allows for
almost arbitrary positive and negative dependencies with only a mild condition
on the parameter matrix---a condition akin to the positive definiteness of the
Gaussian covariance matrix. Our Poisson generalization allows for both positive
and negative dependencies without any constraints on the parameter values. We
also develop parameter estimation methods using node-wise regressions with
$\ell_1$ regularization and likelihood approximation methods using sampling.
Finally, we demonstrate our exponential generalization on a synthetic dataset
and a real-world dataset of airport delay times.","['David I. Inouye', 'Pradeep Ravikumar', 'Inderjit S. Dhillon']",['stat.ML'],2016-03-11 14:02:20+00:00
http://arxiv.org/abs/1603.03236v4,Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation,"Optimization on manifolds is a class of methods for optimization of an
objective function, subject to constraints which are smooth, in the sense that
the set of points which satisfy the constraints admits the structure of a
differentiable manifold. While many optimization problems are of the described
form, technicalities of differential geometry and the laborious calculation of
derivatives pose a significant barrier for experimenting with these methods.
  We introduce Pymanopt (available at https://pymanopt.github.io), a toolbox
for optimization on manifolds, implemented in Python, that---similarly to the
Manopt Matlab toolbox---implements several manifold geometries and optimization
algorithms. Moreover, we lower the barriers to users further by using automated
differentiation for calculating derivative information, saving users time and
saving them from potential calculation and implementation errors.","['James Townsend', 'Niklas Koep', 'Sebastian Weichwald']","['cs.MS', 'cs.LG', 'math.OC', 'stat.ML']",2016-03-10 12:23:12+00:00
http://arxiv.org/abs/1603.03130v3,Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning,"In PU learning, a binary classifier is trained from positive (P) and
unlabeled (U) data without negative (N) data. Although N data is missing, it
sometimes outperforms PN learning (i.e., ordinary supervised learning).
Hitherto, neither theoretical nor experimental analysis has been given to
explain this phenomenon. In this paper, we theoretically compare PU (and NU)
learning against PN learning based on the upper bounds on estimation errors. We
find simple conditions when PU and NU learning are likely to outperform PN
learning, and we prove that, in terms of the upper bounds, either PU or NU
learning (depending on the class-prior probability and the sizes of P and N
data) given infinite U data will improve on PN learning. Our theoretical
findings well agree with the experimental results on artificial and benchmark
data even when the experimental setup does not match the theoretical
assumptions exactly.","['Gang Niu', 'Marthinus Christoffel du Plessis', 'Tomoya Sakai', 'Yao Ma', 'Masashi Sugiyama']","['cs.LG', 'stat.ML']",2016-03-10 02:53:52+00:00
