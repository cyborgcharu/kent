id,title,abstract,authors,categories,date
http://arxiv.org/abs/1812.04744v2,Generative Adversarial Networks for Recovering Missing Spectral Information,"Ultra-wideband (UWB) radar systems nowadays typical operate in the low
frequency spectrum to achieve penetration capability. However, this spectrum is
also shared by many others communication systems, which causes missing
information in the frequency bands. To recover this missing spectral
information, we propose a generative adversarial network, called SARGAN, that
learns the relationship between original and missing band signals by observing
these training pairs in a clever way. Initial results shows that this approach
is promising in tackling this challenging missing band problem.","['Dung N. Tran', 'Trac D. Tran', 'Lam Nguyen']","['cs.LG', 'stat.ML']",2018-12-11 23:42:28+00:00
http://arxiv.org/abs/1812.06591v1,SMART: An Open Source Data Labeling Platform for Supervised Learning,"SMART is an open source web application designed to help data scientists and
research teams efficiently build labeled training data sets for supervised
machine learning tasks. SMART provides users with an intuitive interface for
creating labeled data sets, supports active learning to help reduce the
required amount of labeled data, and incorporates inter-rater reliability
statistics to provide insight into label quality. SMART is designed to be
platform agnostic and easily deployable to meet the needs of as many different
research teams as possible. The project website contains links to the code
repository and extensive user documentation.","['Rob Chew', 'Michael Wenger', 'Caroline Kery', 'Jason Nance', 'Keith Richards', 'Emily Hadley', 'Peter Baumgartner']","['stat.ML', 'cs.LG']",2018-12-11 22:49:14+00:00
http://arxiv.org/abs/1812.04700v4,Predictive Learning on Hidden Tree-Structured Ising Models,"We provide high-probability sample complexity guarantees for exact structure
recovery and accurate predictive learning using noise-corrupted samples from an
acyclic (tree-shaped) graphical model. The hidden variables follow a
tree-structured Ising model distribution, whereas the observable variables are
generated by a binary symmetric channel taking the hidden variables as its
input (flipping each bit independently with some constant probability $q\in
[0,1/2)$). In the absence of noise, predictive learning on Ising models was
recently studied by Bresler and Karzand (2020); this paper quantifies how noise
in the hidden model impacts the tasks of structure recovery and marginal
distribution estimation by proving upper and lower bounds on the sample
complexity. Our results generalize state-of-the-art bounds reported in prior
work, and they exactly recover the noiseless case ($q=0$). In fact, for any
tree with $p$ vertices and probability of incorrect recovery $\delta>0$, the
sufficient number of samples remains logarithmic as in the noiseless case,
i.e., $\mathcal{O}(\log(p/\delta))$, while the dependence on $q$ is
$\mathcal{O}\big( 1/(1-2q)^{4} \big)$, for both aforementioned tasks. We also
present a new equivalent of Isserlis' Theorem for sign-valued tree-structured
distributions, yielding a new low-complexity algorithm for higher-order moment
estimation.","['Konstantinos E. Nikolakakis', 'Dionysios S. Kalogerias', 'Anand D. Sarwate']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",2018-12-11 21:32:16+00:00
http://arxiv.org/abs/1812.04697v1,Anomaly Generation using Generative Adversarial Networks in Host Based Intrusion Detection,"Generative adversarial networks have been able to generate striking results
in various domains. This generation capability can be general while the
networks gain deep understanding regarding the data distribution. In many
domains, this data distribution consists of anomalies and normal data, with the
anomalies commonly occurring relatively less, creating datasets that are
imbalanced. The capabilities that generative adversarial networks offer can be
leveraged to examine these anomalies and help alleviate the challenge that
imbalanced datasets propose via creating synthetic anomalies. This anomaly
generation can be specifically beneficial in domains that have costly data
creation processes as well as inherently imbalanced datasets. One of the
domains that fits this description is the host-based intrusion detection
domain. In this work, ADFA-LD dataset is chosen as the dataset of interest
containing system calls of small foot-print next generation attacks. The data
is first converted into images, and then a Cycle-GAN is used to create images
of anomalous data from images of normal data. The generated data is combined
with the original dataset and is used to train a model to detect anomalies. By
doing so, it is shown that the classification results are improved, with the
AUC rising from 0.55 to 0.71, and the anomaly detection rate rising from 17.07%
to 80.49%. The results are also compared to SMOTE, showing the potential
presented by generative adversarial networks in anomaly generation.","['Milad Salem', 'Shayan Taheri', 'Jiann Shiun Yuan']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2018-12-11 21:21:09+00:00
http://arxiv.org/abs/1812.04693v1,ECG Arrhythmia Classification Using Transfer Learning from 2-Dimensional Deep CNN Features,"Due to the recent advances in the area of deep learning, it has been
demonstrated that a deep neural network, trained on a huge amount of data, can
recognize cardiac arrhythmias better than cardiologists. Moreover,
traditionally feature extraction was considered an integral part of ECG pattern
recognition; however, recent findings have shown that deep neural networks can
carry out the task of feature extraction directly from the data itself. In
order to use deep neural networks for their accuracy and feature extraction,
high volume of training data is required, which in the case of independent
studies is not pragmatic. To arise to this challenge, in this work, the
identification and classification of four ECG patterns are studied from a
transfer learning perspective, transferring knowledge learned from the image
classification domain to the ECG signal classification domain. It is
demonstrated that feature maps learned in a deep neural network trained on
great amounts of generic input images can be used as general descriptors for
the ECG signal spectrograms and result in features that enable classification
of arrhythmias. Overall, an accuracy of 97.23 percent is achieved in
classifying near 7000 instances by ten-fold cross validation.","['Milad Salem', 'Shayan Taheri', 'Jiann Shiun-Yuan']","['cs.LG', 'cs.CV', 'stat.ML']",2018-12-11 21:11:30+00:00
http://arxiv.org/abs/1812.04690v1,Learning representations of molecules and materials with atomistic neural networks,"Deep Learning has been shown to learn efficient representations for
structured data such as image, text or audio. In this chapter, we present
neural network architectures that are able to learn efficient representations
of molecules and materials. In particular, the continuous-filter convolutional
network SchNet accurately predicts chemical properties across compositional and
configurational space on a variety of datasets. Beyond that, we analyze the
obtained representations to find evidence that their spatial and chemical
properties agree with chemical intuition.","['Kristof T. Schütt', 'Alexandre Tkatchenko', 'Klaus-Robert Müller']","['physics.comp-ph', 'cs.LG', 'stat.ML']",2018-12-11 21:02:28+00:00
http://arxiv.org/abs/1812.04677v1,Contrastive Training for Models of Information Cascades,"This paper proposes a model of information cascades as directed spanning
trees (DSTs) over observed documents. In addition, we propose a contrastive
training procedure that exploits partial temporal ordering of node infections
in lieu of labeled training links. This combination of model and unsupervised
training makes it possible to improve on models that use infection times alone
and to exploit arbitrary features of the nodes and of the text content of
messages in information cascades. With only basic node and time lag features
similar to previous models, the DST model achieves performance with
unsupervised training comparable to strong baselines on a blog network
inference task. Unsupervised training with additional content features achieves
significantly better results, reaching half the accuracy of a fully supervised
model.","['Shaobin Xu', 'David A. Smith']","['cs.SI', 'cs.LG', 'stat.ML']",2018-12-11 20:23:46+00:00
http://arxiv.org/abs/1812.04650v1,"Reproduction Report on ""Learn to Pay Attention""","We have successfully implemented the ""Learn to Pay Attention"" model of
attention mechanism in convolutional neural networks, and have replicated the
results of the original paper in the categories of image classification and
fine-grained recognition.","['Levan Shugliashvili', 'Davit Soselia', 'Shota Amashukeli', 'Irakli Koberidze']","['cs.LG', 'stat.ML']",2018-12-11 19:05:23+00:00
http://arxiv.org/abs/1812.04634v2,On the Curved Geometry of Accelerated Optimization,"In this work we propose a differential geometric motivation for Nesterov's
accelerated gradient method (AGM) for strongly-convex problems. By considering
the optimization procedure as occurring on a Riemannian manifold with a natural
structure, The AGM method can be seen as the proximal point method applied in
this curved space. This viewpoint can also be extended to the continuous time
case, where the accelerated gradient method arises from the natural
block-implicit Euler discretization of an ODE on the manifold. We provide an
analysis of the convergence rate of this ODE for quadratic objectives.",['Aaron Defazio'],"['cs.LG', 'math.OC', 'stat.ML']",2018-12-11 19:00:07+00:00
http://arxiv.org/abs/1812.04606v3,Deep Anomaly Detection with Outlier Exposure,"It is important to detect anomalous inputs when deploying machine learning
systems. The use of larger and more complex inputs in deep learning magnifies
the difficulty of distinguishing between anomalous and in-distribution
examples. At the same time, diverse image and text data are available in
enormous quantities. We propose leveraging these data to improve deep anomaly
detection by training anomaly detectors against an auxiliary dataset of
outliers, an approach we call Outlier Exposure (OE). This enables anomaly
detectors to generalize and detect unseen anomalies. In extensive experiments
on natural language processing and small- and large-scale vision tasks, we find
that Outlier Exposure significantly improves detection performance. We also
observe that cutting-edge generative models trained on CIFAR-10 may assign
higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to
mitigate this issue. We also analyze the flexibility and robustness of Outlier
Exposure, and identify characteristics of the auxiliary dataset that improve
performance.","['Dan Hendrycks', 'Mantas Mazeika', 'Thomas Dietterich']","['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']",2018-12-11 18:49:50+00:00
http://arxiv.org/abs/1812.04599v3,Adversarial Framing for Image and Video Classification,"Neural networks are prone to adversarial attacks. In general, such attacks
deteriorate the quality of the input by either slightly modifying most of its
pixels, or by occluding it with a patch. In this paper, we propose a method
that keeps the image unchanged and only adds an adversarial framing on the
border of the image. We show empirically that our method is able to
successfully attack state-of-the-art methods on both image and video
classification problems. Notably, the proposed method results in a universal
attack which is very fast at test time. Source code can be found at
https://github.com/zajaczajac/adv_framing .","['Konrad Zolna', 'Michal Zajac', 'Negar Rostamzadeh', 'Pedro O. Pinheiro']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2018-12-11 18:39:29+00:00
http://arxiv.org/abs/1812.04597v2,Preventing Failures Due to Dataset Shift: Learning Predictive Models That Transport,"Classical supervised learning produces unreliable models when training and
target distributions differ, with most existing solutions requiring samples
from the target domain. We propose a proactive approach which learns a
relationship in the training domain that will generalize to the target domain
by incorporating prior knowledge of aspects of the data generating process that
are expected to differ as expressed in a causal selection diagram.
Specifically, we remove variables generated by unstable mechanisms from the
joint factorization to yield the Surgery Estimator---an interventional
distribution that is invariant to the differences across environments. We prove
that the surgery estimator finds stable relationships in strictly more
scenarios than previous approaches which only consider conditional
relationships, and demonstrate this in simulated experiments. We also evaluate
on real world data for which the true causal diagram is unknown, performing
competitively against entirely data-driven approaches.","['Adarsh Subbaswamy', 'Peter Schulam', 'Suchi Saria']","['stat.ML', 'cs.AI', 'cs.LG']",2018-12-11 18:32:52+00:00
http://arxiv.org/abs/1812.04594v1,Bounding the Error From Reference Set Kernel Maximum Mean Discrepancy,"In this paper, we bound the error induced by using a weighted skeletonization
of two data sets for computing a two sample test with kernel maximum mean
discrepancy. The error is quantified in terms of the speed in which heat
diffuses from those points to the rest of the data, as well as how at the
weights on the reference points are, and gives a non-asymptotic,
non-probabilistic bound. The result ties into the problem of the eigenvector
triple product, which appears in a number of important problems. The error
bound also suggests an optimization scheme for choosing the best set of
reference points and weights. The method is tested on a several two sample test
examples.",['Alexander Cloninger'],"['stat.ML', 'cs.LG']",2018-12-11 18:28:17+00:00
http://arxiv.org/abs/1812.05556v2,Informing Artificial Intelligence Generative Techniques using Cognitive Theories of Human Creativity,"The common view that our creativity is what makes us uniquely human suggests
that incorporating research on human creativity into generative deep learning
techniques might be a fruitful avenue for making their outputs more compelling
and human-like. Using an original synthesis of Deep Dream-based convolutional
neural networks and cognitive based computational art rendering systems, we
show how honing theory, intrinsic motivation, and the notion of a 'seed
incident' can be implemented computationally, and demonstrate their impact on
the resulting generative art. Conversely, we discuss how explorations in deep
learn-ing convolutional neural net generative systems can inform our
understanding of human creativity. We conclude with ideas for further
cross-fertilization between AI based computational creativity and psychology of
creativity.","['Steve DiPaola', 'Liane Gabora', 'Graeme McCaig']","['cs.AI', 'cs.LG', 'q-bio.NC', 'stat.ML']",2018-12-11 18:12:44+00:00
http://arxiv.org/abs/1812.04549v2,Controlling Covariate Shift using Balanced Normalization of Weights,"We introduce a new normalization technique that exhibits the fast convergence
properties of batch normalization using a transformation of layer weights
instead of layer outputs. The proposed technique keeps the contribution of
positive and negative weights to the layer output balanced. We validate our
method on a set of standard benchmarks including CIFAR-10/100, SVHN and ILSVRC
2012 ImageNet.","['Aaron Defazio', 'Léon Bottou']","['cs.LG', 'stat.ML']",2018-12-11 17:12:50+00:00
http://arxiv.org/abs/1812.04529v2,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning,"The application of stochastic variance reduction to optimization has shown
remarkable recent theoretical and practical success. The applicability of these
techniques to the hard non-convex optimization problems encountered during
training of modern deep neural networks is an open problem. We show that naive
application of the SVRG technique and related approaches fail, and explore why.","['Aaron Defazio', 'Léon Bottou']","['cs.LG', 'stat.ML']",2018-12-11 16:40:59+00:00
http://arxiv.org/abs/1812.10384v2,Identification of Cancer -- Mesothelioma Disease Using Logistic Regression and Association Rule,"Malignant Pleural Mesothelioma (MPM) or malignant mesothelioma (MM) is an
atypical, aggressive tumor that matures into cancer in the pleura, a stratum of
tissue bordering the lungs. Diagnosis of MPM is difficult and it accounts for
about seventy-five percent of all mesothelioma diagnosed yearly in the United
States of America. Being a fatal disease, early identification of MPM is
crucial for patient survival. Our study implements logistic regression and
develops association rules to identify early stage symptoms of MM. We retrieved
medical reports generated by Dicle University and implemented logistic
regression to measure the model accuracy. We conducted (a) logistic
correlation, (b) Omnibus test and (c) Hosmer and Lemeshow test for model
evaluation. Moreover, we also developed association rules by confidence, rule
support, lift, condition support and deployability. Categorical logistic
regression increases the training accuracy from 72.30% to 81.40% with a testing
accuracy of 63.46%. The study also shows the top 5 symptoms that is mostly
likely indicates the presence in MM. This study concludes that using predictive
modeling can enhance primary presentation and diagnosis of MM.",['Avishek Choudhury'],"['cs.CY', 'cs.LG', 'q-bio.QM', 'stat.ML']",2018-12-11 16:20:31+00:00
http://arxiv.org/abs/1812.11028v2,Evaluating Patient Readmission Risk: A Predictive Analytics Approach,"With the emergence of the Hospital Readmission Reduction Program of the
Center for Medicare and Medicaid Services on October 1, 2012, forecasting
unplanned patient readmission risk became crucial to the healthcare domain.
There are tangible works in the literature emphasizing on developing
readmission risk prediction models; However, the models are not accurate enough
to be deployed in an actual clinical setting. Our study considers patient
readmission risk as the objective for optimization and develops a useful risk
prediction model to address unplanned readmissions. Furthermore, Genetic
Algorithm and Greedy Ensemble is used to optimize the developed model
constraints.","['Avishek Choudhury', 'Christopher M Greene']","['cs.CY', 'cs.LG', 'stat.ML']",2018-12-11 16:18:45+00:00
http://arxiv.org/abs/1812.04513v1,The Impact of Quantity of Training Data on Recognition of Eating Gestures,"This paper considers the problem of recognizing eating gestures by tracking
wrist motion. Eating gestures can have large variability in motion depending on
the subject, utensil, and type of food or beverage being consumed. Previous
works have shown viable proofs-of-concept of recognizing eating gestures in
laboratory settings with small numbers of subjects and food types, but it is
unclear how well these methods would work if tested on a larger population in
natural settings. As more subjects, locations and foods are tested, a larger
amount of motion variability could cause a decrease in recognition accuracy. To
explore this issue, this paper describes the collection and annotation of
51,614 eating gestures taken by 269 subjects eating a meal in a cafeteria.
Experiments are described that explore the complexity of hidden Markov models
(HMMs) and the amount of training data needed to adequately capture the motion
variability across this large data set. Results found that HMMs needed a
complexity of 13 states and 5 Gaussians to reach a plateau in accuracy,
signifying that a minimum of 65 samples per gesture type are needed. Results
also found that 500 training samples per gesture type were needed to identify
the point of diminishing returns in recognition accuracy. Overall, the findings
provide evidence that the size a data set typically used to demonstrate a
laboratory proofs-of-concept may not be sufficiently large enough to capture
all the motion variability that could be expected in transitioning to
deployment with a larger population. Our data set, which is 1-2 orders of
magnitude larger than all data sets tested in previous works, is being made
publicly available.","['Yiru Shen', 'Eric Muth', 'Adam Hoover']","['cs.LG', 'stat.ML']",2018-12-11 16:09:07+00:00
http://arxiv.org/abs/1812.04456v1,Semi-supervised dual graph regularized dictionary learning,"In this paper, we propose a semi-supervised dictionary learning method that
uses both the information in labelled and unlabelled data and jointly trains a
linear classifier embedded on the sparse codes. The manifold structure of the
data in the sparse code space is preserved using the same approach as the
Locally Linear Embedding method (LLE). This enables one to enforce the
predictive power of the unlabelled data sparse codes. We show that our approach
provides significant improvements over other methods. The results can be
further improved by training a simple nonlinear classifier as SVM on the sparse
codes.","['Khanh-Hung Tran', 'Fred-Maurice Ngole-Mboula', 'Jean-Luc Starck']","['cs.LG', 'stat.ML']",2018-12-11 15:16:33+00:00
http://arxiv.org/abs/1812.04446v1,Data Strategies for Fleetwide Predictive Maintenance,"For predictive maintenance, we examine one of the largest public datasets for
machine failures derived along with their corresponding precursors as error
rates, historical part replacements, and sensor inputs. To simplify the time
and accuracy comparison between 27 different algorithms, we treat the imbalance
between normal and failing states with nominal under-sampling. We identify 3
promising regression and discriminant algorithms with both higher accuracy
(96%) and twenty-fold faster execution times than previous work. Because
predictive maintenance success hinges on input features prior to prediction, we
provide a methodology to rank-order feature importance and show that for this
dataset, error counts prove more predictive than scheduled maintenance might
imply solely based on more traditional factors such as machine age or last
replacement times.",['David Noever'],"['cs.LG', 'stat.ML']",2018-12-11 14:57:57+00:00
http://arxiv.org/abs/1812.04439v1,Synergy Effect between Convolutional Neural Networks and the Multiplicity of SMILES for Improvement of Molecular Prediction,"In our study, we demonstrate the synergy effect between convolutional neural
networks and the multiplicity of SMILES. The model we propose, the so-called
Convolutional Neural Fingerprint (CNF) model, reaches the accuracy of
traditional descriptors such as Dragon (Mauri et al. [22]), RDKit (Landrum
[18]), CDK2 (Willighagen et al. [43]) and PyDescriptor (Masand and Rastija
[20]). Moreover the CNF model generally performs better than highly fine-tuned
traditional descriptors, especially on small data sets, which is of great
interest for the chemical field where data sets are generally small due to
experimental costs, the availability of molecules or accessibility to private
databases. We evaluate the CNF model along with SMILES augmentation during both
training and testing. To the best of our knowledge, this is the first time that
such a methodology is presented. We show that using the multiplicity of SMILES
during training acts as a regulariser and therefore avoids overfitting and can
be seen as ensemble learning when considered for testing.","['Talia B. Kimber', 'Sebastian Engelke', 'Igor V. Tetko', 'Eric Bruno', 'Guillaume Godin']","['cs.LG', 'stat.ML']",2018-12-11 14:46:58+00:00
http://arxiv.org/abs/1812.04428v3,Efficient learning of smooth probability functions from Bernoulli tests with guarantees,"We study the fundamental problem of learning an unknown, smooth probability
function via pointwise Bernoulli tests. We provide a scalable algorithm for
efficiently solving this problem with rigorous guarantees. In particular, we
prove the convergence rate of our posterior update rule to the true probability
function in L2-norm. Moreover, we allow the Bernoulli tests to depend on
contextual features and provide a modified inference engine with provable
guarantees for this novel setting. Numerical results show that the empirical
convergence rates match the theory, and illustrate the superiority of our
approach in handling contextual features over the state-of-the-art.","['Paul Rolland', 'Ali Kavis', 'Alex Immer', 'Adish Singla', 'Volkan Cevher']","['cs.LG', 'stat.ML']",2018-12-11 14:29:13+00:00
http://arxiv.org/abs/1812.04407v1,Learning Item-Interaction Embeddings for User Recommendations,"Industry-scale recommendation systems have become a cornerstone of the
e-commerce shopping experience. For Etsy, an online marketplace with over 50
million handmade and vintage items, users come to rely on personalized
recommendations to surface relevant items from its massive inventory. One
hallmark of Etsy's shopping experience is the multitude of ways in which a user
can interact with an item they are interested in: they can view it, favorite
it, add it to a collection, add it to cart, purchase it, etc. We hypothesize
that the different ways in which a user interacts with an item indicates
different kinds of intent. Consequently, a user's recommendations should be
based not only on the item from their past activity, but also the way in which
they interacted with that item. In this paper, we propose a novel method for
learning interaction-based item embeddings that encode the co-occurrence
patterns of not only the item itself, but also the interaction type. The
learned embeddings give us a convenient way of approximating the likelihood
that one item-interaction pair would co-occur with another by way of a simple
inner product. Because of its computational efficiency, our model lends itself
naturally as a candidate set selection method, and we evaluate it as such in an
industry-scale recommendation system that serves live traffic on Etsy.com. Our
experiments reveal that taking interaction type into account shows promising
results in improving the accuracy of modeling user shopping behavior.","['Xiaoting Zhao', 'Raphael Louca', 'Diane Hu', 'Liangjie Hong']","['cs.IR', 'cs.LG', 'stat.ML']",2018-12-11 14:06:13+00:00
http://arxiv.org/abs/1812.04403v1,Encoding prior knowledge in the structure of the likelihood,"The inference of deep hierarchical models is problematic due to strong
dependencies between the hierarchies. We investigate a specific transformation
of the model parameters based on the multivariate distributional transform.
This transformation is a special form of the reparametrization trick, flattens
the hierarchy and leads to a standard Gaussian prior on all resulting
parameters. The transformation also transfers all the prior information into
the structure of the likelihood, hereby decoupling the transformed parameters a
priori from each other. A variational Gaussian approximation in this
standardized space will be excellent in situations of relatively uninformative
data. Additionally, the curvature of the log-posterior is well-conditioned in
directions that are weakly constrained by the data, allowing for fast inference
in such a scenario. In an example we perform the transformation explicitly for
Gaussian process regression with a priori unknown correlation structure. Deep
models are inferred rapidly in highly and slowly in poorly informed situations.
The flat model show exactly the opposite performance pattern. A synthesis of
both, the deep and the flat perspective, provides their combined advantages and
overcomes the individual limitations, leading to a faster inference.","['Jakob Knollmüller', 'Torsten A. Enßlin']","['stat.ML', 'cs.LG', 'stat.CO']",2018-12-11 14:03:55+00:00
http://arxiv.org/abs/1812.04397v1,From Adaptive Kernel Density Estimation to Sparse Mixture Models,"We introduce a balloon estimator in a generalized expectation-maximization
method for estimating all parameters of a Gaussian mixture model given one data
sample per mixture component. Instead of limiting explicitly the model size,
this regularization strategy yields low-complexity sparse models where the
number of effective mixture components reduces with an increase of a smoothing
probability parameter $\mathbf{P>0}$. This semi-parametric method bridges from
non-parametric adaptive kernel density estimation (KDE) to parametric ordinary
least-squares when $\mathbf{P=1}$. Experiments show that simpler sparse mixture
models retain the level of details present in the adaptive KDE solution.","['Colas Schretter', 'Jianyong Sun', 'Peter Schelkens']","['stat.ML', 'cs.LG']",2018-12-11 13:54:37+00:00
http://arxiv.org/abs/1812.04370v1,Sparse component separation from Poisson measurements,"Blind source separation (BSS) aims at recovering signals from mixtures. This
problem has been extensively studied in cases where the mixtures are
contaminated with additive Gaussian noise. However, it is not well suited to
describe data that are corrupted with Poisson measurements such as in low
photon count optics or in high-energy astronomical imaging (e.g. observations
from the Chandra or Fermi telescopes). To that purpose, we propose a novel BSS
algorithm coined pGMCA that specifically tackles the blind separation of sparse
sources from Poisson measurements.","['I. El Hamzaoui', 'J. Bobin']","['stat.ML', 'cs.LG', 'physics.data-an']",2018-12-11 12:58:18+00:00
http://arxiv.org/abs/1812.04369v3,Variational Bayesian Weighted Complex Network Reconstruction,"Complex network reconstruction is a hot topic in many fields. Currently, the
most popular data-driven reconstruction framework is based on lasso. However,
it is found that, in the presence of noise, lasso loses efficiency for weighted
networks. This paper builds a new framework to cope with this problem. The key
idea is to employ a series of linear regression problems to model the
relationship between network nodes, and then to use an efficient variational
Bayesian algorithm to infer the unknown coefficients. The numerical experiments
conducted on both synthetic and real data demonstrate that the new method
outperforms lasso with regard to both reconstruction accuracy and running
speed.","['Shuang Xu', 'Chun-Xia Zhang', 'Pei Wang', 'Jiangshe Zhang']","['stat.ML', 'cs.LG', 'cs.SI', 'stat.AP']",2018-12-11 12:53:31+00:00
http://arxiv.org/abs/1812.04363v1,Exploration Bonus for Regret Minimization in Undiscounted Discrete and Continuous Markov Decision Processes,"We introduce and analyse two algorithms for exploration-exploitation in
discrete and continuous Markov Decision Processes (MDPs) based on exploration
bonuses. SCAL$^+$ is a variant of SCAL (Fruit et al., 2018) that performs
efficient exploration-exploitation in any unknown weakly-communicating MDP for
which an upper bound C on the span of the optimal bias function is known. For
an MDP with $S$ states, $A$ actions and $\Gamma \leq S$ possible next states,
we prove that SCAL$^+$ achieves the same theoretical guarantees as SCAL (i.e.,
a high probability regret bound of $\widetilde{O}(C\sqrt{\Gamma SAT})$), with a
much smaller computational complexity. Similarly, C-SCAL$^+$ exploits an
exploration bonus to achieve sublinear regret in any undiscounted MDP with
continuous state space. We show that C-SCAL$^+$ achieves the same regret bound
as UCCRL (Ortner and Ryabko, 2012) while being the first implementable
algorithm with regret guarantees in this setting. While optimistic algorithms
such as UCRL, SCAL or UCCRL maintain a high-confidence set of plausible MDPs
around the true unknown MDP, SCAL$^+$ and C-SCAL$^+$ leverage on an exploration
bonus to directly plan on the empirically estimated MDP, thus being more
computationally efficient.","['Jian Qian', 'Ronan Fruit', 'Matteo Pirotta', 'Alessandro Lazaric']","['cs.LG', 'stat.ML']",2018-12-11 12:44:52+00:00
http://arxiv.org/abs/1812.04359v1,Efficient Model-Free Reinforcement Learning Using Gaussian Process,"Efficient Reinforcement Learning usually takes advantage of demonstration or
good exploration strategy. By applying posterior sampling in model-free RL
under the hypothesis of GP, we propose Gaussian Process Posterior Sampling
Reinforcement Learning(GPPSTD) algorithm in continuous state space, giving
theoretical justifications and empirical results. We also provide theoretical
and empirical results that various demonstration could lower expected
uncertainty and benefit posterior sampling exploration. In this way, we
combined the demonstration and exploration process together to achieve a more
efficient reinforcement learning.","['Ying Fan', 'Letian Chen', 'Yizhou Wang']","['cs.LG', 'stat.ML']",2018-12-11 12:37:24+00:00
http://arxiv.org/abs/1812.04356v3,Robust Bregman Clustering,"Using a trimming approach, we investigate a k-means type method based on
Bregman divergences for clustering data possibly corrupted with clutter noise.
The main interest of Bregman divergences is that the standard Lloyd algorithm
adapts to these distortion measures, and they are well-suited for clustering
data sampled according to mixture models from exponential families. We prove
that there exists an optimal codebook, and that an empirically optimal codebook
converges a.s. to an optimal codebook in the distortion sense. Moreover, we
obtain the sub-Gaussian rate of convergence for k-means 1 $\sqrt$ n under mild
tail assumptions. Also, we derive a Lloyd-type algorithm with a trimming
parameter that can be selected from data according to some heuristic, and
present some experimental results.","['Aurélie Fischer', 'Clément Levrard', 'Claire Brécheteau']","['math.ST', 'stat.ML', 'stat.TH']",2018-12-11 12:35:36+00:00
http://arxiv.org/abs/1812.04346v1,Towards Automatic Personality Prediction Using Facebook Like Categories,"We demonstrate that effortlessly accessible digital records of behavior such
as Facebook Likes can be obtained and utilized to automatically distinguish a
wide range of highly delicate personal traits including: life satisfaction,
cultural ethnicity, political views, age, gender and personality traits. The
analysis presented based on a dataset of over 738,000 users who conferred their
Facebook Likes, social network activities, egocentric network, demographic
characteristics, and the results of various psychometric tests for our extended
personality analysis. The proposed model uses unique mapping technique between
each Facebook Like object to the corresponding Facebook page
category/sub-category object, which is then evaluated as features for a set of
machine learning algorithms to predict individual psycho-demographic profiles
from Likes. The model , distinguishes between a religious and non-religious
individual in 83% of circumstances, Asian and European in 87% of situations,
and between emotional stable and emotion unstable in 81% of situations. We
provide exemplars of correlations between attributes and Likes and present
suggestions for future directions.","['Raad Bin Tareaf', 'Philipp Berger', 'Patrick Hennig', 'Christoph Meinel']","['cs.SI', 'cs.LG', 'stat.ML']",2018-12-11 12:10:58+00:00
http://arxiv.org/abs/1812.04345v2,Closing the U.S. gender wage gap requires understanding its heterogeneity,"In 2016, the majority of full-time employed women in the U.S. earned
significantly less than comparable men. The extent to which women were affected
by gender inequality in earnings, however, depended greatly on socio-economic
characteristics, such as marital status or educational attainment. In this
paper, we analyzed data from the 2016 American Community Survey using a
high-dimensional wage regression and applying double lasso to quantify
heterogeneity in the gender wage gap. We found that the gap varied
substantially across women and was driven primarily by marital status, having
children at home, race, occupation, industry, and educational attainment. We
recommend that policy makers use these insights to design policies that will
reduce discrimination and unequal pay more effectively.","['Philipp Bach', 'Victor Chernozhukov', 'Martin Spindler']","['econ.EM', 'stat.AP', 'stat.ML']",2018-12-11 12:05:26+00:00
http://arxiv.org/abs/1901.03299v1,An Analysis of the Accuracy of the P300 BCI,"The P300 Brain-Computer Interface (BCI) is a well-established communication
channel for severely disabled people. The P300 event-related potential is
mostly characterized by its amplitude or its area, which correlate with the
spelling accuracy of the P300 speller. Here, we introduce a novel approach for
estimating the efficiency of this BCI by considering the P300 signal-to-noise
ratio (SNR), a parameter that estimates the spatial and temporal noise levels
and has a significantly stronger correlation with spelling accuracy.
Furthermore, we suggest a Gaussian noise model, which utilizes the P300
event-related potential SNR to predict spelling accuracy under various
conditions for LDA-based classification. We demonstrate the utility of this
analysis using real data and discuss its potential applications, such as
speeding up the process of electrode selection.","['Nitzan S. Artzi', 'Oren Shriki']","['eess.SP', 'cs.LG', 'q-bio.NC', 'stat.ML']",2018-12-11 11:34:11+00:00
http://arxiv.org/abs/1812.04314v2,Adversarial Autoencoders with Constant-Curvature Latent Manifolds,"Constant-curvature Riemannian manifolds (CCMs) have been shown to be ideal
embedding spaces in many application domains, as their non-Euclidean geometry
can naturally account for some relevant properties of data, like hierarchy and
circularity. In this work, we introduce the CCM adversarial autoencoder
(CCM-AAE), a probabilistic generative model trained to represent a data
distribution on a CCM. Our method works by matching the aggregated posterior of
the CCM-AAE with a probability distribution defined on a CCM, so that the
encoder implicitly learns to represent data on the CCM to fool the
discriminator network. The geometric constraint is also explicitly imposed by
jointly training the CCM-AAE to maximise the membership degree of the
embeddings to the CCM. While a few works in recent literature make use of
either hyperspherical or hyperbolic manifolds for different learning tasks,
ours is the first unified framework to seamlessly deal with CCMs of different
curvatures. We show the effectiveness of our model on three different datasets
characterised by non-trivial geometry: semi-supervised classification on MNIST,
link prediction on two popular citation datasets, and graph-based molecule
generation using the QM9 chemical database. Results show that our method
improves upon other autoencoders based on Euclidean and non-Euclidean
geometries on all tasks taken into account.","['Daniele Grattarola', 'Lorenzo Livi', 'Cesare Alippi']","['cs.LG', 'stat.ML']",2018-12-11 10:12:24+00:00
http://arxiv.org/abs/1812.04300v3,Deep neural networks algorithms for stochastic control problems on finite horizon: convergence analysis,"This paper develops algorithms for high-dimensional stochastic control
problems based on deep learning and dynamic programming. Unlike classical
approximate dynamic programming approaches, we first approximate the optimal
policy by means of neural networks in the spirit of deep reinforcement
learning, and then the value function by Monte Carlo regression. This is
achieved in the dynamic programming recursion by performance or hybrid
iteration, and regress now methods from numerical probabilities. We provide a
theoretical justification of these algorithms. Consistency and rate of
convergence for the control and value function estimates are analyzed and
expressed in terms of the universal approximation error of the neural networks,
and of the statistical error when estimating network function, leaving aside
the optimization error. Numerical results on various applications are presented
in a companion paper (arxiv.org/abs/1812.05916) and illustrate the performance
of the proposed algorithms.","['Côme Huré', 'Huyên Pham', 'Achref Bachouch', 'Nicolas Langrené']","['math.PR', 'math.OC', 'stat.ML', '65C05, 90C39, 93E35, 68T07']",2018-12-11 09:41:26+00:00
http://arxiv.org/abs/1812.04287v1,Deep Density-based Image Clustering,"Recently, deep clustering, which is able to perform feature learning that
favors clustering tasks via deep neural networks, has achieved remarkable
performance in image clustering applications. However, the existing deep
clustering algorithms generally need the number of clusters in advance, which
is usually unknown in real-world tasks. In addition, the initial cluster
centers in the learned feature space are generated by $k$-means. This only
works well on spherical clusters and probably leads to unstable clustering
results. In this paper, we propose a two-stage deep density-based image
clustering (DDC) framework to address these issues. The first stage is to train
a deep convolutional autoencoder (CAE) to extract low-dimensional feature
representations from high-dimensional image data, and then apply t-SNE to
further reduce the data to a 2-dimensional space favoring density-based
clustering algorithms. The second stage is to apply the developed density-based
clustering technique on the 2-dimensional embedded data to automatically
recognize an appropriate number of clusters with arbitrary shapes. Concretely,
a number of local clusters are generated to capture the local structures of
clusters, and then are merged via their density relationship to form the final
clustering result. Experiments demonstrate that the proposed DDC achieves
comparable or even better clustering performance than state-of-the-art deep
clustering methods, even though the number of clusters is not given.","['Yazhou Ren', 'Ni Wang', 'Mingxia Li', 'Zenglin Xu']","['cs.LG', 'cs.CV', 'stat.ML']",2018-12-11 09:27:20+00:00
http://arxiv.org/abs/1812.05501v1,Bayesian Spectral Deconvolution Based on Poisson Distribution: Bayesian Measurement and Virtual Measurement Analytics (VMA),"In this paper, we propose a new method of Bayesian measurement for spectral
deconvolution, which regresses spectral data into the sum of unimodal basis
function such as Gaussian or Lorentzian functions. Bayesian measurement is a
framework for considering not only the target physical model but also the
measurement model as a probabilistic model, and enables us to estimate the
parameter of a physical model with its confidence interval through a Bayesian
posterior distribution given a measurement data set. The measurement with
Poisson noise is one of the most effective system to apply our proposed method.
Since the measurement time is strongly related to the signal-to-noise ratio for
the Poisson noise model, Bayesian measurement with Poisson noise model enables
us to clarify the relationship between the measurement time and the limit of
estimation. In this study, we establish the probabilistic model with Poisson
noise for spectral deconvolution. Bayesian measurement enables us to perform
virtual and computer simulation for a certain measurement through the
established probabilistic model. This property is called ""Virtual Measurement
Analytics(VMA)"" in this paper. We also show that the relationship between the
measurement time and the limit of estimation can be extracted by using the
proposed method in a simulation of synthetic data and real data for XPS
measurement of MoS$_2$.","['Kenji Nagata', 'Yoh-ichi Mototake', 'Rei Muraoka', 'Takehiko Sasaki', 'Masato Okada']","['eess.SP', 'cs.LG', 'physics.data-an', 'stat.ML']",2018-12-11 08:50:50+00:00
http://arxiv.org/abs/1812.04951v1,The FLUXCOM ensemble of global land-atmosphere energy fluxes,"Although a key driver of Earth's climate system, global land-atmosphere
energy fluxes are poorly constrained. Here we use machine learning to merge
energy flux measurements from FLUXNET eddy covariance towers with remote
sensing and meteorological data to estimate net radiation, latent and sensible
heat and their uncertainties. The resulting FLUXCOM database comprises 147
global gridded products in two setups: (1) 0.0833${\deg}$ resolution using
MODIS remote sensing data (RS) and (2) 0.5${\deg}$ resolution using remote
sensing and meteorological data (RS+METEO). Within each setup we use a full
factorial design across machine learning methods, forcing datasets and energy
balance closure corrections. For RS and RS+METEO setups respectively, we
estimate 2001-2013 global (${\pm}$ 1 standard deviation) net radiation as
75.8${\pm}$1.4 ${W\ m^{-2}}$ and 77.6${\pm}$2 ${W\ m^{-2}}$, sensible heat as
33${\pm}$4 ${W\ m^{-2}}$ and 36${\pm}$5 ${W\ m^{-2}}$, and evapotranspiration
as 75.6${\pm}$10 ${\times}$ 10$^3$ ${km^3\ yr^{-1}}$ and 76${\pm}$6 ${\times}$
10$^3$ ${km^3\ yr^{-1}}$. FLUXCOM products are suitable to quantify global
land-atmosphere interactions and benchmark land surface model simulations.","['Martin Jung', 'Sujan Koirala', 'Ulrich Weber', 'Kazuhito Ichii', 'Fabian Gans', 'Gustau-Camps-Valls', 'Dario Papale', 'Christopher Schwalm', 'Gianluca Tramontana', 'Markus Reichstein']","['physics.ao-ph', 'cs.LG', 'stat.ML']",2018-12-11 08:42:02+00:00
http://arxiv.org/abs/1812.04618v1,DCASE 2018 Challenge: Solution for Task 5,"To address Task 5 in the Detection and Classification of Acoustic Scenes and
Events (DCASE) 2018 challenge, in this paper, we propose an ensemble learning
system. The proposed system consists of three different models, based on
convolutional neural network and long short memory recurrent neural network.
With extracted features such as spectrogram and mel-frequency cepstrum
coefficients from different channels, the proposed system can classify
different domestic activities effectively. Experimental results obtained from
the provided development dataset show that good performance with F1-score of
92.19% can be achieved. Compared with the baseline system, our proposed system
significantly improves the performance of F1-score by 7.69%.","['Jeremy Chew', 'Yingxiang Sun', 'Lahiru Jayasinghe', 'Chau Yuen']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2018-12-11 07:35:42+00:00
http://arxiv.org/abs/1812.10383v1,Classification of Cervical Cancer Dataset,"Cervical cancer is the leading gynecological malignancy worldwide. This paper
presents diverse classification techniques and shows the advantage of feature
selection approaches to the best predicting of cervical cancer disease. There
are thirty-two attributes with eight hundred and fifty-eight samples. Besides,
this data suffers from missing values and imbalance data. Therefore,
over-sampling, under-sampling and embedded over and under sampling have been
used. Furthermore, dimensionality reduction techniques are required for
improving the accuracy of the classifier. Therefore, feature selection methods
have been studied as they divided into two distinct categories, filters and
wrappers. The results show that age, first sexual intercourse, number of
pregnancies, smokes, hormonal contraceptives, and STDs: genital herpes are the
main predictive features with high accuracy with 97.5%. Decision Tree
classifier is shown to be advantageous in handling classification assignment
with excellent performance.","['Avishek Choudhury', 'Y. M. S Al Wesabi', 'Daehan Won']","['cs.CY', 'cs.LG', 'q-bio.QM', 'stat.ML']",2018-12-11 07:05:22+00:00
http://arxiv.org/abs/1812.10381v1,Decision Support System for Renal Transplantation,"The burgeoning need for kidney transplantation mandates immediate attention.
Mismatch of deceased donor-recipient kidney leads to post-transplant death. To
ensure ideal kidney donor-recipient match and minimize post-transplant deaths,
the paper develops a prediction model that identifies factors that determine
the probability of success of renal transplantation, that is, if the kidney
procured from the deceased donor can be transplanted or discarded. The paper
conducts a study enveloping data for 584 imported kidneys collected from 12
transplant centers associated with an organ procurement organization located in
New York City, NY. The predicting model yielding best performance measures can
be beneficial to the healthcare industry. Transplant centers and organ
procurement organizations can take advantage of the prediction model to
efficiently predict the outcome of kidney transplantation. Consequently, it
will reduce the mortality rate caused by mismatching of donor-recipient kidney
transplantation during the surgery. Keywords","['Ehsan Khan', 'Avishek Choudhury', 'Amy L Friedman', 'Daehan Won']","['cs.CY', 'cs.LG', 'q-bio.QM', 'stat.ML']",2018-12-11 07:00:37+00:00
http://arxiv.org/abs/1812.04227v1,Learning What to Remember: Long-term Episodic Memory Networks for Learning from Streaming Data,"Current generation of memory-augmented neural networks has limited
scalability as they cannot efficiently process data that are too large to fit
in the external memory storage. One example of this is lifelong learning
scenario where the model receives unlimited length of data stream as an input
which contains vast majority of uninformative entries. We tackle this problem
by proposing a memory network fit for long-term lifelong learning scenario,
which we refer to as Long-term Episodic Memory Networks (LEMN), that features a
RNN-based retention agent that learns to replace less important memory entries
based on the retention probability generated on each entry that is learned to
identify data instances of generic importance relative to other memory entries,
as well as its historical importance. Such learning of retention agent allows
our long-term episodic memory network to retain memory entries of generic
importance for a given task. We validate our model on a path-finding task as
well as synthetic and real question answering tasks, on which our model
achieves significant improvements over the memory augmented networks with
rule-based memory scheduling as well as an RL-based baseline that does not
consider relative or historical importance of the memory.","['Hyunwoo Jung', 'Moonsu Han', 'Minki Kang', 'Sungju Hwang']","['cs.LG', 'stat.ML']",2018-12-11 06:04:32+00:00
http://arxiv.org/abs/1812.04224v1,On the Dimensionality of Word Embedding,"In this paper, we provide a theoretical understanding of word embedding and
its dimensionality. Motivated by the unitary-invariance of word embedding, we
propose the Pairwise Inner Product (PIP) loss, a novel metric on the
dissimilarity between word embeddings. Using techniques from matrix
perturbation theory, we reveal a fundamental bias-variance trade-off in
dimensionality selection for word embeddings. This bias-variance trade-off
sheds light on many empirical observations which were previously unexplained,
for example the existence of an optimal dimensionality. Moreover, new insights
and discoveries, like when and how word embeddings are robust to over-fitting,
are revealed. By optimizing over the bias-variance trade-off of the PIP loss,
we can explicitly answer the open question of dimensionality selection for word
embedding.","['Zi Yin', 'Yuanyuan Shen']","['cs.LG', 'cs.CL', 'stat.ML']",2018-12-11 05:25:38+00:00
http://arxiv.org/abs/1812.04218v3,Learning Controllable Fair Representations,"Learning data representations that are transferable and are fair with respect
to certain protected attributes is crucial to reducing unfair decisions while
preserving the utility of the data. We propose an information-theoretically
motivated objective for learning maximally expressive representations subject
to fairness constraints. We demonstrate that a range of existing approaches
optimize approximations to the Lagrangian dual of our objective. In contrast to
these existing approaches, our objective allows the user to control the
fairness of the representations by specifying limits on unfairness. Exploiting
duality, we introduce a method that optimizes the model parameters as well as
the expressiveness-fairness trade-off. Empirical evidence suggests that our
proposed method can balance the trade-off between multiple notions of fairness
and achieves higher expressiveness at a lower computational cost.","['Jiaming Song', 'Pratyusha Kalluri', 'Aditya Grover', 'Shengjia Zhao', 'Stefano Ermon']","['cs.LG', 'cs.AI', 'stat.ML']",2018-12-11 04:44:48+00:00
http://arxiv.org/abs/1812.04845v1,A Tensor-based Structural Health Monitoring Approach for Aeroservoelastic Systems,"Structural health monitoring is a condition-based field of study utilised to
monitor infrastructure, via sensing systems. It is therefore used in the field
of aerospace engineering to assist in monitoring the health of aerospace
structures. A difficulty however is that in structural health monitoring the
data input is usually from sensor arrays, which results in data which are
highly redundant and correlated, an area in which traditional two-way matrix
approaches have had difficulty in deconstructing and interpreting. Newer
methods involving tensor analysis allow us to analyse this multi-way structural
data in a coherent manner. In our approach, we demonstrate the usefulness of
tensor-based learning coupled with for damage detection, on a novel $N$-DoF
Lagrangian aeroservoelastic model.","['Prasad Cheema', 'Nguyen Lu Dang Khoa', 'Moray Kidd', 'Gareth A. Vio']","['cs.LG', 'stat.ML']",2018-12-11 04:15:44+00:00
http://arxiv.org/abs/1812.04214v1,New Approaches to Inverse Structural Modification Theory using Random Projections,"In many contexts the modal properties of a structure change, either due to
the impact of a changing environment, fatigue, or due to the presence of
structural damage. For example during flight, an aircraft's modal properties
are known to change with both altitude and velocity. It is thus important to
quantify these changes given only a truncated set of modal data, which is
usually the case experimentally. This procedure is formally known as the
generalised inverse eigenvalue problem. In this paper we experimentally show
that first-order gradient-based methods that optimise objective functions
defined over a modal are prohibitive due to the required small step sizes. This
in turn leads to the justification of using a non-gradient, black box optimiser
in the form of particle swarm optimisation. We further show how it is possible
to solve such inverse eigenvalue problems in a lower dimensional space by the
use of random projections, which in many cases reduces the total dimensionality
of the optimisation problem by 80% to 99%. Two example problems are explored
involving a ten-dimensional mass-stiffness toy problem, and a one-dimensional
finite element mass-stiffness approximation for a Boeing 737-300 aircraft.","['Prasad Cheema', 'Mehrisadat M. Alamdari', 'Gareth A. Vio']","['cs.LG', 'stat.ML']",2018-12-11 04:12:36+00:00
http://arxiv.org/abs/1812.04202v3,Deep Learning on Graphs: A Survey,"Deep learning has been shown to be successful in a number of domains, ranging
from acoustics, images, to natural language processing. However, applying deep
learning to the ubiquitous graph data is non-trivial because of the unique
characteristics of graphs. Recently, substantial research efforts have been
devoted to applying deep learning methods to graphs, resulting in beneficial
advances in graph analysis techniques. In this survey, we comprehensively
review the different types of deep learning methods on graphs. We divide the
existing methods into five categories based on their model architectures and
training strategies: graph recurrent neural networks, graph convolutional
networks, graph autoencoders, graph reinforcement learning, and graph
adversarial methods. We then provide a comprehensive overview of these methods
in a systematic manner mainly by following their development history. We also
analyze the differences and compositions of different methods. Finally, we
briefly outline the applications in which they have been used and discuss
potential future research directions.","['Ziwei Zhang', 'Peng Cui', 'Wenwu Zhu']","['cs.LG', 'cs.SI', 'stat.ML']",2018-12-11 03:16:57+00:00
http://arxiv.org/abs/1812.04181v1,KF-LAX: Kronecker-factored curvature estimation for control variate optimization in reinforcement learning,"A key challenge for gradient based optimization methods in model-free
reinforcement learning is to develop an approach that is sample efficient and
has low variance. In this work, we apply Kronecker-factored curvature
estimation technique (KFAC) to a recently proposed gradient estimator for
control variate optimization, RELAX, to increase the sample efficiency of using
this gradient estimation method in reinforcement learning. The performance of
the proposed method is demonstrated on a synthetic problem and a set of three
discrete control task Atari games.",['Mohammad Firouzi'],"['cs.LG', 'stat.ML']",2018-12-11 01:56:39+00:00
http://arxiv.org/abs/1812.04155v4,Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention,"We present Vision-based Navigation with Language-based Assistance (VNLA), a
grounded vision-language task where an agent with visual perception is guided
via language to find objects in photorealistic indoor environments. The task
emulates a real-world scenario in that (a) the requester may not know how to
navigate to the target objects and thus makes requests by only specifying
high-level end-goals, and (b) the agent is capable of sensing when it is lost
and querying an advisor, who is more qualified at the task, to obtain language
subgoals to make progress. To model language-based assistance, we develop a
general framework termed Imitation Learning with Indirect Intervention (I3L),
and propose a solution that is effective on the VNLA task. Empirical results
show that this approach significantly improves the success rate of the learning
agent over other baselines in both seen and unseen environments. Our code and
data are publicly available at https://github.com/debadeepta/vnla .","['Khanh Nguyen', 'Debadeepta Dey', 'Chris Brockett', 'Bill Dolan']","['cs.LG', 'cs.CL', 'cs.CV', 'cs.RO', 'stat.ML']",2018-12-10 23:48:25+00:00
http://arxiv.org/abs/1812.04152v1,Duelling Bandits with Weak Regret in Adversarial Environments,"Research on the multi-armed bandit problem has studied the trade-off of
exploration and exploitation in depth. However, there are numerous applications
where the cardinal absolute-valued feedback model (e.g. ratings from one to
five) is not suitable. This has motivated the formulation of the duelling
bandits problem, where the learner picks a pair of actions and observes a noisy
binary feedback, indicating a relative preference between the two. There exist
a multitude of different settings and interpretations of the problem for two
reasons. First, due to the absence of a total order of actions, there is no
natural definition of the best action. Existing work either explicitly assumes
the existence of a linear order, or uses a custom definition for the winner.
Second, there are multiple reasonable notions of regret to measure the
learner's performance. Most prior work has been focussing on the
$\textit{strong regret}$, which averages the quality of the two actions picked.
This work focusses on the $\textit{weak regret}$, which is based on the quality
of the better of the two actions selected. Weak regret is the more appropriate
performance measure when the pair's inferior action has no significant
detrimental effect on the pair's quality.
  We study the duelling bandits problem in the adversarial setting. We provide
an algorithm which has theoretical guarantees in both the utility-based
setting, which implies a total order, and the unrestricted setting. For the
latter, we work with the $\textit{Borda winner}$, finding the action maximising
the probability of winning against an action sampled uniformly at random. The
thesis concludes with experimental results based on both real-world data and
synthetic data, showing the algorithm's performance and limitations.",['Lennard Hilgendorf'],"['cs.LG', 'stat.ML']",2018-12-10 23:43:36+00:00
http://arxiv.org/abs/1812.04145v1,Learning Sharing Behaviors with Arbitrary Numbers of Agents,"We propose a method for modeling and learning turn-taking behaviors for
accessing a shared resource. We model the individual behavior for each agent in
an interaction and then use a multi-agent fusion model to generate a summary
over the expected actions of the group to render the model independent of the
number of agents. The individual behavior models are weighted finite state
transducers (WFSTs) with weights dynamically updated during interactions, and
the multi-agent fusion model is a logistic regression classifier.
  We test our models in a multi-agent tower-building environment, where a
Q-learning agent learns to interact with rule-based agents. Our approach
accurately models the underlying behavior patterns of the rule-based agents
with accuracy ranging between 0.63 and 1.0 depending on the stochasticity of
the other agent behaviors. In addition we show using KL-divergence that the
model accurately captures the distribution of next actions when interacting
with both a single agent (KL-divergence < 0.1) and with multiple agents
(KL-divergence < 0.37). Finally, we demonstrate that our behavior model can be
used by a Q-learning agent to take turns in an interactive turn-taking
environment.","['Katherine Metcalf', 'Barry-John Theobald', 'Nicholas Apostoloff']","['cs.LG', 'cs.MA', 'stat.ML']",2018-12-10 23:20:45+00:00
http://arxiv.org/abs/1812.04118v1,Montage based 3D Medical Image Retrieval from Traumatic Brain Injury Cohort using Deep Convolutional Neural Network,"Brain imaging analysis on clinically acquired computed tomography (CT) is
essential for the diagnosis, risk prediction of progression, and treatment of
the structural phenotypes of traumatic brain injury (TBI). However, in real
clinical imaging scenarios, entire body CT images (e.g., neck, abdomen, chest,
pelvis) are typically captured along with whole brain CT scans. For instance,
in a typical sample of clinical TBI imaging cohort, only ~15% of CT scans
actually contain whole brain CT images suitable for volumetric brain analyses;
the remaining are partial brain or non-brain images. Therefore, a manual image
retrieval process is typically required to isolate the whole brain CT scans
from the entire cohort. However, the manual image retrieval is time and
resource consuming and even more difficult for the larger cohorts. To alleviate
the manual efforts, in this paper we propose an automated 3D medical image
retrieval pipeline, called deep montage-based image retrieval (dMIR), which
performs classification on 2D montage images via a deep convolutional neural
network. The novelty of the proposed method for image processing is to
characterize the medical image retrieval task based on the montage images. In a
cohort of 2000 clinically acquired TBI scans, 794 scans were used as training
data, 206 scans were used as validation data, and the remaining 1000 scans were
used as testing data. The proposed achieved accuracy=1.0, recall=1.0,
precision=1.0, f1=1.0 for validation data, while achieved accuracy=0.988,
recall=0.962, precision=0.962, f1=0.962 for testing data. Thus, the proposed
dMIR is able to perform accurate CT whole brain image retrieval from
large-scale clinical cohorts.","['Cailey I. Kerley', 'Yuankai Huo', 'Shikha Chaganti', 'Shunxing Bao', 'Mayur B. Patel', 'Bennett A. Landman']","['cs.IR', 'cs.LG', 'stat.ML']",2018-12-10 21:58:01+00:00
http://arxiv.org/abs/1812.04109v2,Top-N-Rank: A Scalable List-wise Ranking Method for Recommender Systems,"We propose Top-N-Rank, a novel family of list-wise Learning-to-Rank models
for reliably recommending the N top-ranked items. The proposed models optimize
a variant of the widely used discounted cumulative gain (DCG) objective
function which differs from DCG in two important aspects: (i) It limits the
evaluation of DCG only on the top N items in the ranked lists, thereby
eliminating the impact of low-ranked items on the learned ranking function; and
(ii) it incorporates weights that allow the model to leverage multiple types of
implicit feedback with differing levels of reliability or trustworthiness.
Because the resulting objective function is non-smooth and hence challenging to
optimize, we consider two smooth approximations of the objective function,
using the traditional sigmoid function and the rectified linear unit (ReLU). We
propose a family of learning-to-rank algorithms (Top-N-Rank) that work with any
smooth objective function. Then, a more efficient variant, Top-N-Rank.ReLU, is
introduced, which effectively exploits the properties of ReLU function to
reduce the computational complexity of Top-N-Rank from quadratic to linear in
the average number of items rated by users. The results of our experiments
using two widely used benchmarks, namely, the MovieLens data set and the Amazon
Video Games data set demonstrate that: (i) The `top-N truncation' of the
objective function substantially improves the ranking quality of the top N
recommendations; (ii) using the ReLU for smoothing the objective function
yields significant improvement in both ranking quality as well as runtime as
compared to using the sigmoid; and (iii) Top-N-Rank.ReLU substantially
outperforms the well-performing list-wise ranking methods in terms of ranking
quality.","['Junjie Liang', 'Jinlong Hu', 'Shoubin Dong', 'Vasant Honavar']","['cs.IR', 'cs.LG', 'stat.ML']",2018-12-10 21:39:16+00:00
http://arxiv.org/abs/1812.04103v2,Non-local U-Net for Biomedical Image Segmentation,"Deep learning has shown its great promise in various biomedical image
segmentation tasks. Existing models are typically based on U-Net and rely on an
encoder-decoder architecture with stacked local operators to aggregate
long-range information gradually. However, only using the local operators
limits the efficiency and effectiveness. In this work, we propose the non-local
U-Nets, which are equipped with flexible global aggregation blocks, for
biomedical image segmentation. These blocks can be inserted into U-Net as
size-preserving processes, as well as down-sampling and up-sampling layers. We
perform thorough experiments on the 3D multimodality isointense infant brain MR
image segmentation task to evaluate the non-local U-Nets. Results show that our
proposed models achieve top performances with fewer parameters and faster
computation.","['Zhengyang Wang', 'Na Zou', 'Dinggang Shen', 'Shuiwang Ji']","['cs.CV', 'cs.LG', 'stat.AP', 'stat.ML']",2018-12-10 21:28:55+00:00
http://arxiv.org/abs/1812.04069v3,Individual Fairness in Hindsight,"Since many critical decisions impacting human lives are increasingly being
made by algorithms, it is important to ensure that the treatment of individuals
under such algorithms is demonstrably fair under reasonable notions of
fairness. One compelling notion proposed in the literature is that of
individual fairness (IF), which advocates that similar individuals should be
treated similarly (Dwork et al. 2012). Originally proposed for offline
decisions, this notion does not, however, account for temporal considerations
relevant for online decision-making. In this paper, we extend the notion of IF
to account for the time at which a decision is made, in settings where there
exists a notion of conduciveness of decisions as perceived by the affected
individuals. We introduce two definitions: (i) fairness-across-time (FT) and
(ii) fairness-in-hindsight (FH). FT is the simplest temporal extension of IF
where treatment of individuals is required to be individually fair relative to
the past as well as future, while in FH, we require a one-sided notion of
individual fairness that is defined relative to only the past decisions. We
show that these two definitions can have drastically different implications in
the setting where the principal needs to learn the utility model. Linear regret
relative to optimal individually fair decisions is inevitable under FT for
non-trivial examples. On the other hand, we design a new algorithm: Cautious
Fair Exploration (CaFE), which satisfies FH and achieves sub-linear regret
guarantees for a broad range of settings. We characterize lower bounds showing
that these guarantees are order-optimal in the worst case. FH can thus be
embedded as a primary safeguard against unfair discrimination in algorithmic
deployments, without hindering the ability to take good decisions in the
long-run.","['Swati Gupta', 'Vijay Kamble']","['cs.LG', 'stat.ML']",2018-12-10 20:25:29+00:00
http://arxiv.org/abs/1812.04064v2,Attentional Heterogeneous Graph Neural Network: Application to Program Reidentification,"Program or process is an integral part of almost every IT/OT system. Can we
trust the identity/ID (e.g., executable name) of the program? To avoid
detection, malware may disguise itself using the ID of a legitimate program,
and a system tool (e.g., PowerShell) used by the attackers may have the fake ID
of another common software, which is less sensitive. However, existing
intrusion detection techniques often overlook this critical program
reidentification problem (i.e., checking the program's identity). In this
paper, we propose an attentional heterogeneous graph neural network model
(DeepHGNN) to verify the program's identity based on its system behaviors. The
key idea is to leverage the representation learning of the heterogeneous
program behavior graph to guide the reidentification process. We formulate the
program reidentification as a graph classification problem and develop an
effective attentional heterogeneous graph embedding algorithm to solve it.
Extensive experiments --- using real-world enterprise monitoring data and real
attacks --- demonstrate the effectiveness of DeepHGNN across multiple popular
metrics and the robustness to the normal dynamic changes like program version
upgrades.","['Shen Wang', 'Zhengzhang Chen', 'Ding Li', 'Lu-An Tang', 'Jingchao Ni', 'Zhichun Li', 'Junghwan Rhee', 'Haifeng Chen', 'Philip S. Yu']","['cs.CR', 'cs.LG', 'stat.ML']",2018-12-10 20:08:47+00:00
http://arxiv.org/abs/1812.04616v3,Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs,"The Softmax function is used in the final layer of nearly all existing
sequence-to-sequence models for language generation. However, it is usually the
slowest layer to compute which limits the vocabulary size to a subset of most
frequent types; and it has a large memory footprint. We propose a general
technique for replacing the softmax layer with a continuous embedding layer.
Our primary innovations are a novel probabilistic loss, and a training and
inference procedure in which we generate a probability distribution over
pre-trained word embeddings, instead of a multinomial distribution over the
vocabulary obtained via softmax. We evaluate this new class of
sequence-to-sequence models with continuous outputs on the task of neural
machine translation. We show that our models obtain upto 2.5x speed-up in
training time while performing on par with the state-of-the-art models in terms
of translation quality. These models are capable of handling very large
vocabularies without compromising on translation quality. They also produce
more meaningful errors than in the softmax-based models, as these errors
typically lie in a subspace of the vector space of the reference translations.","['Sachin Kumar', 'Yulia Tsvetkov']","['cs.CL', 'cs.LG', 'stat.ML']",2018-12-10 20:00:36+00:00
http://arxiv.org/abs/1812.03981v1,Theoretical Analysis of Auto Rate-Tuning by Batch Normalization,"Batch Normalization (BN) has become a cornerstone of deep learning across
diverse architectures, appearing to help optimization as well as
generalization. While the idea makes intuitive sense, theoretical analysis of
its effectiveness has been lacking. Here theoretical support is provided for
one of its conjectured properties, namely, the ability to allow gradient
descent to succeed with less tuning of learning rates. It is shown that even if
we fix the learning rate of scale-invariant parameters (e.g., weights of each
layer with BN) to a constant (say, $0.3$), gradient descent still approaches a
stationary point (i.e., a solution where gradient is zero) in the rate of
$T^{-1/2}$ in $T$ iterations, asymptotically matching the best bound for
gradient descent with well-tuned learning rates. A similar result with
convergence rate $T^{-1/4}$ is also shown for stochastic gradient descent.","['Sanjeev Arora', 'Zhiyuan Li', 'Kaifeng Lyu']","['cs.LG', 'stat.ML']",2018-12-10 18:58:12+00:00
http://arxiv.org/abs/1812.03973v3,Bayesian Layers: A Module for Neural Network Uncertainty,"We describe Bayesian Layers, a module designed for fast experimentation with
neural network uncertainty. It extends neural network libraries with drop-in
replacements for common layers. This enables composition via a unified
abstraction over deterministic and stochastic functions and allows for
scalability via the underlying system. These layers capture uncertainty over
weights (Bayesian neural nets), pre-activation units (dropout), activations
(""stochastic output layers""), or the function itself (Gaussian processes). They
can also be reversible to propagate uncertainty from input to output. We
include code examples for common architectures such as Bayesian LSTMs, deep
GPs, and flow-based models. As demonstration, we fit a 5-billion parameter
""Bayesian Transformer"" on 512 TPUv2 cores for uncertainty in machine
translation and a Bayesian dynamics model for model-based planning. Finally, we
show how Bayesian Layers can be used within the Edward2 probabilistic
programming language for probabilistic programs with stochastic processes.","['Dustin Tran', 'Michael W. Dusenberry', 'Mark van der Wilk', 'Danijar Hafner']","['cs.LG', 'cs.PL', 'stat.ML']",2018-12-10 18:46:21+00:00
http://arxiv.org/abs/1812.03965v1,Guided Dropout,"Dropout is often used in deep neural networks to prevent over-fitting.
Conventionally, dropout training invokes \textit{random drop} of nodes from the
hidden layers of a Neural Network. It is our hypothesis that a guided selection
of nodes for intelligent dropout can lead to better generalization as compared
to the traditional dropout. In this research, we propose ""guided dropout"" for
training deep neural network which drop nodes by measuring the strength of each
node. We also demonstrate that conventional dropout is a specific case of the
proposed guided dropout. Experimental evaluation on multiple datasets including
MNIST, CIFAR10, CIFAR100, SVHN, and Tiny ImageNet demonstrate the efficacy of
the proposed guided dropout.","['Rohit Keshari', 'Richa Singh', 'Mayank Vatsa']","['cs.LG', 'stat.ML']",2018-12-10 18:21:47+00:00
http://arxiv.org/abs/1812.03962v1,Disentangled Dynamic Representations from Unordered Data,"We present a deep generative model that learns disentangled static and
dynamic representations of data from unordered input. Our approach exploits
regularities in sequential data that exist regardless of the order in which the
data is viewed. The result of our factorized graphical model is a
well-organized and coherent latent space for data dynamics. We demonstrate our
method on several synthetic dynamic datasets and real video data featuring
various facial expressions and head poses.","['Leonhard Helminger', 'Abdelaziz Djelouah', 'Markus Gross', 'Romann M. Weber']","['stat.ML', 'cs.LG']",2018-12-10 18:19:04+00:00
http://arxiv.org/abs/1812.03955v1,Improving Model-Based Control and Active Exploration with Reconstruction Uncertainty Optimization,"Model based predictions of future trajectories of a dynamical system often
suffer from inaccuracies, forcing model based control algorithms to re-plan
often, thus being computationally expensive, suboptimal and not reliable. In
this work, we propose a model agnostic method for estimating the uncertainty of
a model?s predictions based on reconstruction error, using it in control and
exploration. As our experiments show, this uncertainty estimation can be used
to improve control performance on a wide variety of environments by choosing
predictions of which the model is confident. It can also be used for active
learning to explore more efficiently the environment by planning for
trajectories with high uncertainty, allowing faster model learning.","['Norman Di Palo', 'Harri Valpola']","['cs.LG', 'cs.AI', 'cs.NE', 'cs.RO', 'stat.ML']",2018-12-10 18:09:10+00:00
http://arxiv.org/abs/1812.03934v3,Stagewise Training Accelerates Convergence of Testing Error Over SGD,"Stagewise training strategy is widely used for learning neural networks,
which runs a stochastic algorithm (e.g., SGD) starting with a relatively large
step size (aka learning rate) and geometrically decreasing the step size after
a number of iterations. It has been observed that the stagewise SGD has much
faster convergence than the vanilla SGD with a polynomially decaying step size
in terms of both training error and testing error. {\it But how to explain this
phenomenon has been largely ignored by existing studies.} This paper provides
some theoretical evidence for explaining this faster convergence. In
particular, we consider a stagewise training strategy for minimizing empirical
risk that satisfies the Polyak-\L ojasiewicz (PL) condition, which has been
observed/proved for neural networks and also holds for a broad family of convex
functions. For convex loss functions and two classes of ""nice-behaviored""
non-convex objectives that are close to a convex function, we establish faster
convergence of stagewise training than the vanilla SGD under the PL condition
on both training error and testing error. Experiments on stagewise learning of
deep residual networks exhibits that it satisfies one type of non-convexity
assumption and therefore can be explained by our theory. Of independent
interest, the testing error bounds for the considered non-convex loss functions
are dimensionality and norm independent.","['Zhuoning Yuan', 'Yan Yan', 'Rong Jin', 'Tianbao Yang']","['stat.ML', 'cs.LG', 'math.OC']",2018-12-10 17:34:00+00:00
http://arxiv.org/abs/1812.03929v5,"An Introduction to Spiking Neural Networks: Probabilistic Models, Learning Rules, and Applications","Spiking Neural Networks (SNNs) are distributed trainable systems whose
computing elements, or neurons, are characterized by internal analog dynamics
and by digital and sparse synaptic communications. The sparsity of the synaptic
spiking inputs and the corresponding event-driven nature of neural processing
can be leveraged by hardware implementations that have demonstrated significant
energy reductions as compared to conventional Artificial Neural Networks
(ANNs). Most existing training algorithms for SNNs have been designed either
for biological plausibility or through conversion from pre-trained ANNs via
rate encoding. This paper aims at providing an introduction to SNNs by focusing
on a probabilistic signal processing methodology that enables the direct
derivation of learning rules leveraging the unique time encoding capabilities
of SNNs. To this end, the paper adopts discrete-time probabilistic models for
networked spiking neurons, and it derives supervised and unsupervised learning
rules from first principles by using variational inference. Examples and open
research problems are also provided.","['Hyeryung Jang', 'Osvaldo Simeone', 'Brian Gardner', 'André Grüning']","['eess.SP', 'cs.IT', 'cs.LG', 'cs.NE', 'math.IT', 'stat.ML']",2018-12-10 17:29:12+00:00
http://arxiv.org/abs/1812.03928v3,Learning Representations of Sets through Optimized Permutations,"Representations of sets are challenging to learn because operations on sets
should be permutation-invariant. To this end, we propose a
Permutation-Optimisation module that learns how to permute a set end-to-end.
The permuted set can be further processed to learn a permutation-invariant
representation of that set, avoiding a bottleneck in traditional set models. We
demonstrate our model's ability to learn permutations and set representations
with either explicit or implicit supervision on four datasets, on which we
achieve state-of-the-art results: number sorting, image mosaics, classification
from image mosaics, and visual question answering.","['Yan Zhang', 'Jonathon Hare', 'Adam Prügel-Bennett']","['cs.LG', 'cs.CV', 'stat.ML']",2018-12-10 17:26:25+00:00
http://arxiv.org/abs/1812.03915v1,Non-Intrusive Load Monitoring with Fully Convolutional Networks,"Non-intrusive load monitoring or energy disaggregation involves estimating
the power consumption of individual appliances from measurements of the total
power consumption of a home. Deep neural networks have been shown to be
effective for energy disaggregation. In this work, we present a deep neural
network architecture which achieves state of the art disaggregation performance
with substantially improved computational efficiency, reducing model training
time by a factor of 32 and prediction time by a factor of 43. This improvement
in efficiency could be especially useful for applications where disaggregation
must be performed in home on lower power devices, or for research experiments
which involve training a large number of models.","['Cillian Brewitt', 'Nigel Goddard']","['cs.LG', 'stat.ML']",2018-12-10 16:50:32+00:00
http://arxiv.org/abs/1812.04571v1,Deep Learning with Mixed Supervision for Brain Tumor Segmentation,"Most of the current state-of-the-art methods for tumor segmentation are based
on machine learning models trained on manually segmented images. This type of
training data is particularly costly, as manual delineation of tumors is not
only time-consuming but also requires medical expertise. On the other hand,
images with a provided global label (indicating presence or absence of a tumor)
are less informative but can be obtained at a substantially lower cost. In this
paper, we propose to use both types of training data (fully-annotated and
weakly-annotated) to train a deep learning model for segmentation. The idea of
our approach is to extend segmentation networks with an additional branch
performing image-level classification. The model is jointly trained for
segmentation and classification tasks in order to exploit information contained
in weakly-annotated images while preventing the network to learn features which
are irrelevant for the segmentation task. We evaluate our method on the
challenging task of brain tumor segmentation in Magnetic Resonance images from
BRATS 2018 challenge. We show that the proposed approach provides a significant
improvement of segmentation performance compared to the standard supervised
learning. The observed improvement is proportional to the ratio between
weakly-annotated and fully-annotated images available for training.","['Pawel Mlynarski', 'Hervé Delingette', 'Antonio Criminisi', 'Nicholas Ayache']","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",2018-12-10 16:03:27+00:00
http://arxiv.org/abs/1812.03894v4,Physics-Based Learning for Robotic Environmental Sensing,"We propose a physics-based method to learn environmental fields (EFs) using a
mobile robot. Common purely data-driven methods require prohibitively many
measurements to accurately learn such complex EFs. Alternatively, physics-based
models provide global knowledge of EFs but require experimental validation,
depend on uncertain parameters, and are intractable for mobile robots. To
address these challenges, we propose a Bayesian framework to select the most
likely physics-based models of EFs in real-time, from a pool of numerical
solutions generated offline as a function of the uncertain parameters.
Specifically, we focus on turbulent flow fields and utilize Gaussian processes
(GPs) to construct statistical models for them, using the pool of numerical
solutions to inform their prior mean. To incorporate flow measurements into
these GPs, we control a custom-built mobile robot through a sequence of
waypoints that maximize the information content of the measurements. We
experimentally demonstrate that our proposed framework constructs a posterior
distribution of the flow field that better approximates the real flow compared
to the prior numerical solutions and purely data-driven methods.","['Reza Khodayi-mehr', 'Michael M. Zavlanos']","['cs.RO', 'cs.LG', 'stat.ML']",2018-12-10 16:02:37+00:00
http://arxiv.org/abs/1812.03889v2,Regularization by architecture: A deep prior approach for inverse problems,"The present paper studies so-called deep image prior (DIP) techniques in the
context of ill-posed inverse problems. DIP networks have been recently
introduced for applications in image processing; also first experimental
results for applying DIP to inverse problems have been reported. This paper
aims at discussing different interpretations of DIP and to obtain analytic
results for specific network designs and linear operators. The main
contribution is to introduce the idea of viewing these approaches as the
optimization of Tikhonov functionals rather than optimizing networks. Besides
theoretical results, we present numerical verifications.","['Sören Dittmer', 'Tobias Kluth', 'Peter Maass', 'Daniel Otero Baguer']","['cs.LG', 'stat.ML']",2018-12-10 15:54:33+00:00
http://arxiv.org/abs/1812.07103v1,Style Transfer and Extraction for the Handwritten Letters Using Deep Learning,"How can we learn, transfer and extract handwriting styles using deep neural
networks? This paper explores these questions using a deep conditioned
autoencoder on the IRON-OFF handwriting data-set. We perform three experiments
that systematically explore the quality of our style extraction procedure.
First, We compare our model to handwriting benchmarks using multidimensional
performance metrics. Second, we explore the quality of style transfer, i.e. how
the model performs on new, unseen writers. In both experiments, we improve the
metrics of state of the art methods by a large margin. Lastly, we analyze the
latent space of our model, and we see that it separates consistently writing
styles.","['Omar Mohammed', 'Gerard Bailly', 'Damien Pellier']","['cs.CV', 'cs.LG', 'stat.ML']",2018-12-10 13:38:46+00:00
http://arxiv.org/abs/1812.03719v2,Can we learn where people go?,"In most agent-based simulators, pedestrians navigate from origins to
destinations. Consequently, destinations are essential input parameters to the
simulation. While many other relevant parameters as positions, speeds and
densities can be obtained from sensors, like cameras, destinations cannot be
observed directly. Our research question is: Can we obtain this information
from video data using machine learning methods? We use density heatmaps, which
indicate the pedestrian density within a given camera cutout, as input to
predict the destination distributions. For our proof of concept, we train a
Random Forest predictor on an exemplary data set generated with the Vadere
microscopic simulator. The scenario is a crossroad where pedestrians can head
left, straight or right. In addition, we gain first insights on suitable
placement of the camera. The results motivate an in-depth analysis of the
methodology.","['Marion Gödel', 'Gerta Köster', 'Daniel Lehmberg', 'Manfred Gruber', 'Angelika Kneidl', 'Florian Sesser']","['cs.CV', 'cs.LG', 'stat.ML']",2018-12-10 10:24:51+00:00
http://arxiv.org/abs/1812.03715v1,Modelling trait dependent speciation with Approximate Bayesian Computation,"Phylogeny is the field of modelling the temporal discrete dynamics of
speciation. Complex models can nowadays be studied using the Approximate
Bayesian Computation approach which avoids likelihood calculations. The field's
progression is hampered by the lack of robust software to estimate the numerous
parameters of the speciation process. In this work we present an R package,
pcmabc, based on Approximate Bayesian Computations, that implements three novel
phylogenetic algorithms for trait-dependent speciation modelling. Our
phylogenetic comparative methodology takes into account both the simulated
traits and phylogeny, attempting to estimate the parameters of the processes
generating the phenotype and the trait. The user is not restricted to a
predefined set of models and can specify a variety of evolutionary and
branching models. We illustrate the software with a simulation-reestimation
study focused around the branching Ornstein-Uhlenbeck process, where the
branching rate depends non-linearly on the value of the driving
Ornstein-Uhlenbeck process. Included in this work is a tutorial on how to use
the software.","['Krzysztof Bartoszek', 'Pietro Liò']","['q-bio.PE', 'cs.LG', 'stat.AP', 'stat.ML', '65C05, 62F15, 62P10, 92-08, 92B10']",2018-12-10 10:17:12+00:00
http://arxiv.org/abs/1812.03710v1,Ramp-based Twin Support Vector Clustering,"Traditional plane-based clustering methods measure the cost of within-cluster
and between-cluster by quadratic, linear or some other unbounded functions,
which may amplify the impact of cost. This letter introduces a ramp cost
function into the plane-based clustering to propose a new clustering method,
called ramp-based twin support vector clustering (RampTWSVC). RampTWSVC is more
robust because of its boundness, and thus it is more easier to find the
intrinsic clusters than other plane-based clustering methods. The non-convex
programming problem in RampTWSVC is solved efficiently through an alternating
iteration algorithm, and its local solution can be obtained in a finite number
of iterations theoretically. In addition, the nonlinear manifold-based
formation of RampTWSVC is also proposed by kernel trick. Experimental results
on several benchmark datasets show the better performance of our RampTWSVC
compared with other plane-based clustering methods.","['Zhen Wang', 'Xu Chen', 'Chun-Na Li', 'Yuan-Hai Shao']","['cs.LG', 'stat.ML']",2018-12-10 10:10:53+00:00
http://arxiv.org/abs/1812.03705v2,Defending Against Universal Perturbations With Shared Adversarial Training,"Classifiers such as deep neural networks have been shown to be vulnerable
against adversarial perturbations on problems with high-dimensional input
space. While adversarial training improves the robustness of image classifiers
against such adversarial perturbations, it leaves them sensitive to
perturbations on a non-negligible fraction of the inputs. In this work, we show
that adversarial training is more effective in preventing universal
perturbations, where the same perturbation needs to fool a classifier on many
inputs. Moreover, we investigate the trade-off between robustness against
universal perturbations and performance on unperturbed data and propose an
extension of adversarial training that handles this trade-off more gracefully.
We present results for image classification and semantic segmentation to
showcase that universal perturbations that fool a model hardened with
adversarial training become clearly perceptible and show patterns of the target
scene.","['Chaithanya Kumar Mummadi', 'Thomas Brox', 'Jan Hendrik Metzen']","['cs.CV', 'cs.CR', 'cs.LG', 'stat.ML']",2018-12-10 10:02:45+00:00
http://arxiv.org/abs/1812.03699v1,Taxi Demand-Supply Forecasting: Impact of Spatial Partitioning on the Performance of Neural Networks,"In this paper, we investigate the significance of choosing an appropriate
tessellation strategy for a spatio-temporal taxi demand-supply modeling
framework. Our study compares (i) the variable-sized polygon based Voronoi
tessellation, and (ii) the fixed-sized grid based Geohash tessellation, using
taxi demand-supply GPS data for the cities of Bengaluru, India and New York,
USA. Long Short-Term Memory (LSTM) networks are used for modeling and
incorporating information from spatial neighbors into the model. We find that
the LSTM model based on input features extracted from a variable-sized polygon
tessellation yields superior performance over the LSTM model based on
fixed-sized grid tessellation. Our study highlights the need to explore
multiple spatial partitioning techniques for improving the prediction
performance in neural network models.","['Neema Davis', 'Gaurav Raina', 'Krishna Jagannathan']","['cs.LG', 'stat.ML']",2018-12-10 09:53:04+00:00
http://arxiv.org/abs/1812.03684v3,Guided Graph Spectral Embedding: Application to the C. elegans Connectome,"Graph spectral analysis can yield meaningful embeddings of graphs by
providing insight into distributed features not directly accessible in nodal
domain. Recent efforts in graph signal processing have proposed new
decompositions-e.g., based on wavelets and Slepians-that can be applied to
filter signals defined on the graph. In this work, we take inspiration from
these constructions to define a new guided spectral embedding that combines
maximizing energy concentration with minimizing modified embedded distance for
a given importance weighting of the nodes. We show these optimization goals are
intrinsically opposite, leading to a well-defined and stable spectral
decomposition. The importance weighting allows to put the focus on particular
nodes and tune the trade-off between global and local effects. Following the
derivation of our new optimization criterion and its linear approximation, we
exemplify the methodology on the C. elegans structural connectome. The results
of our analyses confirm known observations on the nematode's neural network in
terms of functionality and importance of cells. Compared to Laplacian
embedding, the guided approach, focused on a certain class of cells (sensory,
inter- and motoneurons), provides more biological insights, such as the
distinction between somatic positions of cells, and their involvement in low or
high order processing functions.","['Miljan Petrović', 'Thomas A. W. Bolton', 'Maria Giulia Preti', 'Raphaël Liégeois', 'Dimitri Van De Ville']","['cs.LG', 'q-bio.NC', 'stat.ML']",2018-12-10 09:16:21+00:00
http://arxiv.org/abs/1812.03662v2,Capturing Between-Tasks Covariance and Similarities Using Multivariate Linear Mixed Models,"We consider the problem of predicting several response variables using the
same set of explanatory variables. This setting naturally induces a group
structure over the coefficient matrix, in which every explanatory variable
corresponds to a set of related coefficients. Most of the existing methods that
utilize this group formation assume that the similarities between related
coefficients arise solely through a joint sparsity structure. In this paper, we
propose a procedure for constructing an estimator of a multivariate regression
coefficient matrix that directly models and captures the within-group
similarities, by employing a multivariate linear mixed model formulation, with
a joint estimation of covariance matrices for coefficients and errors via
penalized likelihood. Our approach, which we term Multivariate random
Regression with Covariance Estimation (MrRCE) encourages structured similarity
in parameters, in which coefficients for the same variable in related tasks
sharing the same sign and similar magnitude. We illustrate the benefits of our
approach in synthetic and real examples, and show that the proposed method
outperforms natural competitors and alternative estimators under several model
settings.","['Aviv Navon', 'Saharon Rosset']","['stat.ME', 'stat.ML']",2018-12-10 07:52:15+00:00
http://arxiv.org/abs/1812.10413v1,Studying oppressive cityscapes of Bangladesh,"In a densely populated city like Dhaka (Bangladesh), a growing number of
high-rise buildings is an inevitable reality. However, they pose mental health
risks for citizens in terms of detachment from natural light, sky view,
greenery, and environmental landscapes. The housing economy and rent structure
in different areas may or may not take account of such environmental factors.
In this paper, we build a computer vision based pipeline to study factors like
sky visibility, greenery in the sidewalks, and dominant colors present in
streets from a pedestrian's perspective. We show that people in lower economy
classes may suffer from lower sky visibility, whereas people in higher economy
classes may suffer from lack of greenery in their environment, both of which
could be possibly addressed by implementing rent restructuring schemes.","['Halima Akhter', 'Nazmus Saquib', 'Deeni Fatiha']","['cs.CY', 'stat.ML']",2018-12-10 05:55:22+00:00
http://arxiv.org/abs/1812.03632v1,Statement networks: a power structure narrative as depicted by newspapers,"We report a data mining pipeline and subsequent analysis to understand the
core periphery power structure created in three national newspapers in
Bangladesh, as depicted by statements made by people appearing in news.
Statements made by one actor about another actor can be considered a form of
public conversation. Named entity recognition techniques can be used to create
a temporal actor network from such conversations, which shows some unique
structure, and reveals much room for improvement in news reporting and also the
top actors' conversation preferences. Our results indicate there is a presence
of cliquishness between powerful political leaders when it comes to their
appearance in news. We also show how these cohesive cores form through the news
articles, and how, over a decade, news cycles change the actors belonging in
these groups.","['Shoumik Sharar Chowdhury', 'Nazmus Saquib', 'Niamat Zawad', 'Manash Kumar Mandal', 'Syed Haque']","['cs.CY', 'stat.ML']",2018-12-10 05:40:54+00:00
http://arxiv.org/abs/1812.03599v2,Fast convergence rates of deep neural networks for classification,"We derive the fast convergence rates of a deep neural network (DNN)
classifier with the rectified linear unit (ReLU) activation function learned
using the hinge loss. We consider three cases for a true model: (1) a smooth
decision boundary, (2) smooth conditional class probability, and (3) the margin
condition (i.e., the probability of inputs near the decision boundary is
small). We show that the DNN classifier learned using the hinge loss achieves
fast rate convergences for all three cases provided that the architecture
(i.e., the number of layers, number of nodes and sparsity). is carefully
selected. An important implication is that DNN architectures are very flexible
for use in various cases without much modification. In addition, we consider a
DNN classifier learned by minimizing the cross-entropy, and show that the DNN
classifier achieves a fast convergence rate under the condition that the
conditional class probabilities of most data are sufficiently close to either 1
or zero. This assumption is not unusual for image recognition because human
beings are extremely good at recognizing most images. To confirm our
theoretical explanation, we present the results of a small numerical study
conducted to compare the hinge loss and cross-entropy.","['Yongdai Kim', 'Ilsang Ohn', 'Dongha Kim']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2018-12-10 02:41:06+00:00
http://arxiv.org/abs/1812.03596v3,Task-Free Continual Learning,"Methods proposed in the literature towards continual deep learning typically
operate in a task-based sequential learning setup. A sequence of tasks is
learned, one at a time, with all data of current task available but not of
previous or future tasks. Task boundaries and identities are known at all
times. This setup, however, is rarely encountered in practical applications.
Therefore we investigate how to transform continual learning to an online
setup. We develop a system that keeps on learning over time in a streaming
fashion, with data distributions gradually changing and without the notion of
separate tasks. To this end, we build on the work on Memory Aware Synapses, and
show how this method can be made online by providing a protocol to decide i)
when to update the importance weights, ii) which data to use to update them,
and iii) how to accumulate the importance weights at each update step.
Experimental results show the validity of the approach in the context of two
applications: (self-)supervised learning of a face recognition model by
watching soap series and learning a robot to avoid collisions.","['Rahaf Aljundi', 'Klaas Kelchtermans', 'Tinne Tuytelaars']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2018-12-10 02:07:57+00:00
http://arxiv.org/abs/1812.03580v1,Closed-form Inference and Prediction in Gaussian Process State-Space Models,"We examine an analytic variational inference scheme for the Gaussian Process
State Space Model (GPSSM) - a probabilistic model for system identification and
time-series modelling. Our approach performs variational inference over both
the system states and the transition function. We exploit Markov structure in
the true posterior, as well as an inducing point approximation to achieve
linear time complexity in the length of the time series. Contrary to previous
approaches, no Monte Carlo sampling is required: inference is cast as a
deterministic optimisation problem. In a number of experiments, we demonstrate
the ability to model non-linear dynamics in the presence of both process and
observation noise as well as to impute missing information (e.g. velocities
from raw positions through time), to de-noise, and to estimate the underlying
dimensionality of the system. Finally, we also introduce a closed-form method
for multi-step prediction, and a novel criterion for assessing the quality of
our approximate posterior.","['Alessandro Davide Ialongo', 'Mark van der Wilk', 'Carl Edward Rasmussen']","['stat.ML', 'cs.LG']",2018-12-10 00:00:33+00:00
http://arxiv.org/abs/1812.03565v2,The Gap Between Model-Based and Model-Free Methods on the Linear Quadratic Regulator: An Asymptotic Viewpoint,"The effectiveness of model-based versus model-free methods is a long-standing
question in reinforcement learning (RL). Motivated by recent empirical success
of RL on continuous control tasks, we study the sample complexity of popular
model-based and model-free algorithms on the Linear Quadratic Regulator (LQR).
We show that for policy evaluation, a simple model-based plugin method requires
asymptotically less samples than the classical least-squares temporal
difference (LSTD) estimator to reach the same quality of solution; the sample
complexity gap between the two methods can be at least a factor of state
dimension. For policy evaluation, we study a simple family of problem instances
and show that nominal (certainty equivalence principle) control also requires
several factors of state and input dimension fewer samples than the policy
gradient method to reach the same level of control performance on these
instances. Furthermore, the gap persists even when employing commonly used
baselines. To the best of our knowledge, this is the first theoretical result
which demonstrates a separation in the sample complexity between model-based
and model-free methods on a continuous control task.","['Stephen Tu', 'Benjamin Recht']","['cs.LG', 'math.OC', 'stat.ML']",2018-12-09 22:24:26+00:00
http://arxiv.org/abs/1812.04486v1,Trade Selection with Supervised Learning and OCA,"In recent years, state-of-the-art methods for supervised learning have
exploited increasingly gradient boosting techniques, with mainstream efficient
implementations such as xgboost or lightgbm. One of the key points in
generating proficient methods is Feature Selection (FS). It consists in
selecting the right valuable effective features. When facing hundreds of these
features, it becomes critical to select best features. While filter and
wrappers methods have come to some maturity, embedded methods are truly
necessary to find the best features set as they are hybrid methods combining
features filtering and wrapping. In this work, we tackle the problem of finding
through machine learning best a priori trades from an algorithmic strategy. We
derive this new method using coordinate ascent optimization and using block
variables. We compare our method to Recursive Feature Elimination (RFE) and
Binary Coordinate Ascent (BCA). We show on a real life example the capacity of
this method to select good trades a priori. Not only this method outperforms
the initial trading strategy as it avoids taking loosing trades, it also
surpasses other method, having the smallest feature set and the highest score
at the same time. The interest of this method goes beyond this simple trade
classification problem as it is a very general method to determine the optimal
feature set using some information about features relationship as well as using
coordinate ascent optimization.","['David Saltiel', 'Eric Benhamou']","['cs.LG', 'q-fin.CP', 'stat.ML', '68T01, 68T05']",2018-12-09 21:07:06+00:00
http://arxiv.org/abs/1812.07367v1,Deep Learning Approach in Automatic Iceberg - Ship Detection with SAR Remote Sensing Data,"Deep Learning is gaining traction with geophysics community to understand
subsurface structures, such as fault detection or salt body in seismic data.
This study describes using deep learning method for iceberg or ship recognition
with synthetic aperture radar (SAR) data. Drifting icebergs pose a potential
threat to activities offshore around the Arctic, including for both ship
navigation and oil rigs. Advancement of satellite imagery using
weather-independent cross-polarized radar has enabled us to monitor and
delineate icebergs and ships, however a human component is needed to classify
the images. Here we present Transfer Learning, a convolutional neural network
(CNN) designed to work with a limited training data and features, while
demonstrating its effectiveness in this problem. Key aspect of the approach is
data augmentation and stacking of multiple outputs, resulted in a significant
boost in accuracy (logarithmic score of 0.1463). This algorithm has been tested
through participation at the Statoil/C-Core Kaggle competition.","['Cheng Zhan', 'Licheng Zhang', 'Zhenzhen Zhong', 'Sher Didi-Ooi', 'Youzuo Lin', 'Yunxi Zhang', 'Shujiao Huang', 'Changchun Wang']","['cs.LG', 'stat.ML']",2018-12-09 16:32:40+00:00
http://arxiv.org/abs/1812.03511v1,Physics-informed deep generative models,"We consider the application of deep generative models in propagating
uncertainty through complex physical systems. Specifically, we put forth an
implicit variational inference formulation that constrains the generative model
output to satisfy given physical laws expressed by partial differential
equations. Such physics-informed constraints provide a regularization mechanism
for effectively training deep probabilistic models for modeling physical
systems in which the cost of data acquisition is high and training data-sets
are typically small. This provides a scalable framework for characterizing
uncertainty in the outputs of physical systems due to randomness in their
inputs or noise in their observations. We demonstrate the effectiveness of our
approach through a canonical example in transport dynamics.","['Yibo Yang', 'Paris Perdikaris']","['stat.ML', 'cs.LG']",2018-12-09 16:12:59+00:00
http://arxiv.org/abs/1812.03483v3,To Reverse the Gradient or Not: An Empirical Comparison of Adversarial and Multi-task Learning in Speech Recognition,"Transcribed datasets typically contain speaker identity for each instance in
the data. We investigate two ways to incorporate this information during
training: Multi-Task Learning and Adversarial Learning. In multi-task learning,
the goal is speaker prediction; we expect a performance improvement with this
joint training if the two tasks of speech recognition and speaker recognition
share a common set of underlying features. In contrast, adversarial learning is
a means to learn representations invariant to the speaker. We then expect
better performance if this learnt invariance helps generalizing to new
speakers. While the two approaches seem natural in the context of speech
recognition, they are incompatible because they correspond to opposite
gradients back-propagated to the model. In order to better understand the
effect of these approaches in terms of error rates, we compare both strategies
in controlled settings. Moreover, we explore the use of additional
untranscribed data in a semi-supervised, adversarial learning manner to improve
error rates. Our results show that deep models trained on big datasets already
develop invariant representations to speakers without any auxiliary loss. When
considering adversarial learning and multi-task learning, the impact on the
acoustic model seems minor. However, models trained in a semi-supervised manner
can improve error-rates.","['Yossi Adi', 'Neil Zeghidour', 'Ronan Collobert', 'Nicolas Usunier', 'Vitaliy Liptchinsky', 'Gabriel Synnaeve']","['cs.LG', 'cs.CL', 'cs.SD', 'eess.AS', 'stat.ML']",2018-12-09 13:18:02+00:00
http://arxiv.org/abs/1812.03472v1,"Theory of Curriculum Learning, with Convex Loss Functions","Curriculum Learning - the idea of teaching by gradually exposing the learner
to examples in a meaningful order, from easy to hard, has been investigated in
the context of machine learning long ago. Although methods based on this
concept have been empirically shown to improve performance of several learning
algorithms, no theoretical analysis has been provided even for simple cases. To
address this shortfall, we start by formulating an ideal definition of
difficulty score - the loss of the optimal hypothesis at a given datapoint. We
analyze the possible contribution of curriculum learning based on this score in
two convex problems - linear regression, and binary classification by hinge
loss minimization. We show that in both cases, the expected convergence rate
decreases monotonically with the ideal difficulty score, in accordance with
earlier empirical results. We also prove that when the ideal difficulty score
is fixed, the convergence rate is monotonically increasing with respect to the
loss of the current hypothesis at each point. We discuss how these results
bring to term two apparently contradicting heuristics: curriculum learning on
the one hand, and hard data mining on the other.","['Daphna Weinshall', 'Dan Amir']","['cs.LG', 'stat.ML']",2018-12-09 12:35:11+00:00
http://arxiv.org/abs/1812.03469v1,A matching based clustering algorithm for categorical data,"Cluster analysis is one of the essential tasks in data mining and knowledge
discovery. Each type of data poses unique challenges in achieving relatively
efficient partitioning of the data into homogeneous groups. While the
algorithms for numeric data are relatively well studied in the literature,
there are still challenges to address in case of categorical data. The main
issue is the unordered structure of categorical data, which makes the
implementation of the standard concepts of clustering algorithms difficult. For
instance, the assessment of distance between objects, the selection of
representatives for categorical data is not as straightforward as for
continuous data. Therefore, this paper presents a new framework for
partitioning categorical data, which does not use the distance measure as a key
concept. The Matching based clustering algorithm is designed based on the
similarity matrix and a framework for updating the latter using the feature
importance criteria. The experimental results show this algorithm can serve as
an alternative to existing ones and can be an efficient knowledge discovery
tool.","['Ruben A. Gevorgyan', 'Yenok B. Hakobyan']","['cs.LG', 'stat.ML', '62H30 (Primary) 62H17, 62H20 (Secondary)']",2018-12-09 12:20:43+00:00
http://arxiv.org/abs/1812.03468v2,Towards Neural Network Patching: Evaluating Engagement-Layers and Patch-Architectures,"In this report we investigate fundamental requirements for the application of
classifier patching on neural networks. Neural network patching is an approach
for adapting neural network models to handle concept drift in nonstationary
environments. Instead of creating or updating the existing network to
accommodate concept drift, neural network patching leverages the inner layers
of the network as well as its output to learn a patch that enhances the
classification and corrects errors caused by the drift. It learns (i) a
predictor that estimates whether the original network will misclassify an
instance, and (ii) a patching network that fixes the misclassification. Neural
network patching is based on the idea that the original network can still
classify a majority of instances well, and that the inner feature
representations encoded in the deep network aid the classifier to cope with
unseen or changed inputs. In order to apply this kind of patching, we evaluate
different engagement layers and patch architectures in this report, and find a
set of generally applicable heuristics, which aid in parametrizing the patching
procedure.","['Sebastian Kauschke', 'David Hermann Lehmann']","['cs.LG', 'stat.ML']",2018-12-09 12:17:24+00:00
http://arxiv.org/abs/1812.07102v1,Deep Learning with Attention to Predict Gestational Age of the Fetal Brain,"Fetal brain imaging is a cornerstone of prenatal screening and early
diagnosis of congenital anomalies. Knowledge of fetal gestational age is the
key to the accurate assessment of brain development. This study develops an
attention-based deep learning model to predict gestational age of the fetal
brain. The proposed model is an end-to-end framework that combines key insights
from multi-view MRI including axial, coronal, and sagittal views. The model
also uses age-activated weakly-supervised attention maps to enable
rotation-invariant localization of the fetal brain among background noise. We
evaluate our methods on the collected fetal brain MRI cohort with a large age
distribution from 125 to 273 days. Our extensive experiments show age
prediction performance with R2 = 0.94 using multi-view MRI and attention.","['Liyue Shen', 'Katie Shpanskaya', 'Edward Lee', 'Emily McKenna', 'Maryam Maleki', 'Quin Lu', 'Safwan Halabi', 'John Pauly', 'Kristen Yeom']","['cs.CV', 'cs.LG', 'stat.ML']",2018-12-09 09:30:11+00:00
http://arxiv.org/abs/1812.04480v3,A Hybrid Distribution Feeder Long-Term Load Forecasting Method Based on Sequence Prediction,"Distribution feeder long-term load forecast (LTLF) is a critical task many
electric utility companies perform on an annual basis. The goal of this task is
to forecast the annual load of distribution feeders. The previous top-down and
bottom-up LTLF methods are unable to incorporate different levels of
information. This paper proposes a hybrid modeling method using sequence
prediction for this classic and important task. The proposed method can
seamlessly integrate top-down, bottom-up and sequential information hidden in
multi-year data. Two advanced sequence prediction models Long Short-Term Memory
(LSTM) and Gated Recurrent Unit (GRU) networks are investigated in this paper.
They successfully solve the vanishing and exploding gradient problems a
standard recurrent neural network has. This paper firstly explains the theories
of LSTM and GRU networks and then discusses the steps of feature selection,
feature engineering and model implementation in detail. In the end, a
real-world application example for a large urban grid in West Canada is
provided. LSTM and GRU networks under different sequential configurations and
traditional models including bottom-up, ARIMA and feed-forward neural network
are all implemented and compared in detail. The proposed method demonstrates
superior performance and great practicality.","['Ming Dong', 'L. S. Grumbach']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2018-12-09 06:38:00+00:00
http://arxiv.org/abs/1812.07101v3,Application of Deep Learning in Fundus Image Processing for Ophthalmic Diagnosis -- A Review,"An overview of the applications of deep learning in ophthalmic diagnosis
using retinal fundus images is presented. We also review various retinal image
datasets that can be used for deep learning purposes. Applications of deep
learning for segmentation of optic disk, blood vessels and retinal layer as
well as detection of lesions are reviewed. Recent deep learning models for
classification of diseases such as age-related macular degeneration,
glaucoma,diabetic macular edema and diabetic retinopathy are also reported.","['Sourya Sengupta', 'Amitojdeep Singh', 'Henry A. Leopold', 'Tanmay Gulati', 'Vasudevan Lakshminarayanan']","['cs.CV', 'cs.LG', 'stat.ML']",2018-12-09 05:57:17+00:00
http://arxiv.org/abs/1812.03425v1,Zero Initialization of modified Gated Recurrent Encoder-Decoder Network for Short Term Load Forecasting,"Single layer Feedforward Neural Network(FNN) is used many a time as a last
layer in models such as seq2seq or could be a simple RNN network. The
importance of such layer is to transform the output to our required dimensions.
When it comes to weights and biases initialization, there is no such specific
technique that could speed up the learning process. We could depend on deep
network initialization techniques such as Xavier or He initialization. But such
initialization fails to show much improvement in learning speed or accuracy. In
this paper we propose Zero Initialization (ZI) for weights of a single layer
network. We first test this technique with on a simple RNN network and compare
the results against Xavier, He and Identity initialization. As a final test we
implement it on a seq2seq network. It was found that ZI considerably reduces
the number of epochs used and improve the accuracy. The developed model has
been applied for short-term load forecasting using the load data of Australian
Energy Market. The model is able to forecast the day ahead load accurately with
error of 0.94%.","['Vedanshu', 'M M Tripathi']","['cs.LG', 'stat.ML']",2018-12-09 04:29:41+00:00
http://arxiv.org/abs/1812.03412v2,Learning Multiplication-free Linear Transformations,"In this paper, we propose several dictionary learning algorithms for sparse
representations that also impose specific structures on the learned
dictionaries such that they are numerically efficient to use: reduced number of
addition/multiplications and even avoiding multiplications altogether. We base
our work on factorizations of the dictionary in highly structured basic
building blocks (binary orthonormal, scaling and shear transformations) for
which we can write closed-form solutions to the optimization problems that we
consider. We show the effectiveness of our methods on image data where we can
compare against well-known numerically efficient transforms such as the fast
Fourier and the fast discrete cosine transforms.",['Cristian Rusu'],"['cs.LG', 'cs.NA', 'stat.ML']",2018-12-09 02:03:53+00:00
http://arxiv.org/abs/1812.03410v1,Binary Input Layer: Training of CNN models with binary input data,"For the efficient execution of deep convolutional neural networks (CNN) on
edge devices, various approaches have been presented which reduce the bit width
of the network parameters down to 1 bit. Binarization of the first layer was
always excluded, as it leads to a significant error increase. Here, we present
the novel concept of binary input layer (BIL), which allows the usage of binary
input data by learning bit specific binary weights. The concept is evaluated on
three datasets (PAMAP2, SVHN, CIFAR-10). Our results show that this approach is
in particular beneficial for multimodal datasets (PAMAP2) where it outperforms
networks using full precision weights in the first layer by 1:92 percentage
points (pp) while consuming only 2 % of the chip area.","['Robert Dürichen', 'Thomas Rocznik', 'Oliver Renz', 'Christian Peters']","['cs.LG', 'cs.CC', 'stat.ML', 'I.2.6']",2018-12-09 00:17:32+00:00
http://arxiv.org/abs/1812.03405v1,AutoGAN: Robust Classifier Against Adversarial Attacks,"Classifiers fail to classify correctly input images that have been
purposefully and imperceptibly perturbed to cause misclassification. This
susceptability has been shown to be consistent across classifiers, regardless
of their type, architecture or parameters. Common defenses against adversarial
attacks modify the classifer boundary by training on additional adversarial
examples created in various ways. In this paper, we introduce AutoGAN, which
counters adversarial attacks by enhancing the lower-dimensional manifold
defined by the training data and by projecting perturbed data points onto it.
AutoGAN mitigates the need for knowing the attack type and magnitude as well as
the need for having adversarial samples of the attack. Our approach uses a
Generative Adversarial Network (GAN) with an autoencoder generator and a
discriminator that also serves as a classifier. We test AutoGAN against
adversarial samples generated with state-of-the-art Fast Gradient Sign Method
(FGSM) as well as samples generated with random Gaussian noise, both using the
MNIST dataset. For different magnitudes of perturbation in training and
testing, AutoGAN can surpass the accuracy of FGSM method by up to 25\% points
on samples perturbed using FGSM. Without an augmented training dataset, AutoGAN
achieves an accuracy of 89\% compared to 1\% achieved by FGSM method on FGSM
testing adversarial samples.","['Blerta Lindqvist', 'Shridatt Sugrim', 'Rauf Izmailov']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2018-12-08 23:50:11+00:00
http://arxiv.org/abs/1812.03399v1,Efficient transfer learning and online adaptation with latent variable models for continuous control,"Traditional model-based RL relies on hand-specified or learned models of
transition dynamics of the environment. These methods are sample efficient and
facilitate learning in the real world but fail to generalize to subtle
variations in the underlying dynamics, e.g., due to differences in mass,
friction, or actuators across robotic agents or across time. We propose using
variational inference to learn an explicit latent representation of unknown
environment properties that accelerates learning and facilitates generalization
on novel environments at test time. We use Online Bayesian Inference of these
learned latents to rapidly adapt online to changes in environments without
retaining large replay buffers of recent data. Combined with a neural network
ensemble that models dynamics and captures uncertainty over dynamics, our
approach demonstrates positive transfer during training and online adaptation
on the continuous control task HalfCheetah.","['Christian F. Perez', 'Felipe Petroski Such', 'Theofanis Karaletsos']","['cs.LG', 'stat.ML']",2018-12-08 22:46:37+00:00
http://arxiv.org/abs/1812.03395v1,Learning Graph Representation via Formal Concept Analysis,"We present a novel method that can learn a graph representation from
multivariate data. In our representation, each node represents a cluster of
data points and each edge represents the subset-superset relationship between
clusters, which can be mutually overlapped. The key to our method is to use
formal concept analysis (FCA), which can extract hierarchical relationships
between clusters based on the algebraic closedness property. We empirically
show that our method can effectively extract hierarchical structures of
clusters compared to the baseline method.","['Yuka Yoneda', 'Mahito Sugiyama', 'Takashi Washio']","['cs.LG', 'stat.ML']",2018-12-08 22:30:11+00:00
