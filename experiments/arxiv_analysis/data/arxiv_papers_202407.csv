id,title,abstract,authors,categories,date
http://arxiv.org/abs/2408.05854v2,On the Robustness of Kernel Goodness-of-Fit Tests,"Goodness-of-fit testing is often criticized for its lack of practical
relevance; since ``all models are wrong'', the null hypothesis that the data
conform to our model is ultimately always rejected when the sample size is
large enough. Despite this, probabilistic models are still used extensively,
raising the more pertinent question of whether the model is good enough for a
specific task. This question can be formalized as a robust goodness-of-fit
testing problem by asking whether the data were generated by a distribution
corresponding to our model up to some mild perturbation. In this paper, we show
that existing kernel goodness-of-fit tests are not robust according to common
notions of robustness including qualitative and quantitative robustness. We
also show that robust techniques based on tilted kernels from the parameter
estimation literature are not sufficient for ensuring both types of robustness
in the context of goodness-of-fit testing. We therefore propose the first
robust kernel goodness-of-fit test which resolves this open problem using
kernel Stein discrepancy balls, which encompass perturbation models such as
Huber contamination models and density uncertainty bands.","['Xing Liu', 'François-Xavier Briol']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2024-08-11 19:54:50+00:00
http://arxiv.org/abs/2408.05849v1,An End-to-End Model for Time Series Classification In the Presence of Missing Values,"Time series classification with missing data is a prevalent issue in time
series analysis, as temporal data often contain missing values in practical
applications. The traditional two-stage approach, which handles imputation and
classification separately, can result in sub-optimal performance as label
information is not utilized in the imputation process. On the other hand, a
one-stage approach can learn features under missing information, but feature
representation is limited as imputed errors are propagated in the
classification process. To overcome these challenges, this study proposes an
end-to-end neural network that unifies data imputation and representation
learning within a single framework, allowing the imputation process to take
advantage of label information. Differing from previous methods, our approach
places less emphasis on the accuracy of imputation data and instead prioritizes
classification performance. A specifically designed multi-scale feature
learning module is implemented to extract useful information from the
noise-imputation data. The proposed model is evaluated on 68 univariate time
series datasets from the UCR archive, as well as a multivariate time series
dataset with various missing data ratios and 4 real-world datasets with missing
information. The results indicate that the proposed model outperforms
state-of-the-art approaches for incomplete time series classification,
particularly in scenarios with high levels of missing data.","['Pengshuai Yao', 'Mengna Liu', 'Xu Cheng', 'Fan Shi', 'Huan Li', 'Xiufeng Liu', 'Shengyong Chen']","['cs.LG', 'stat.ML']",2024-08-11 19:39:12+00:00
http://arxiv.org/abs/2408.05843v1,Online Matrix Completion: A Collaborative Approach with Hott Items,"We investigate the low rank matrix completion problem in an online setting
with ${M}$ users, ${N}$ items, ${T}$ rounds, and an unknown rank-$r$ reward
matrix ${R}\in \mathbb{R}^{{M}\times {N}}$. This problem has been well-studied
in the literature and has several applications in practice. In each round, we
recommend ${S}$ carefully chosen distinct items to every user and observe noisy
rewards. In the regime where ${M},{N} >> {T}$, we propose two distinct
computationally efficient algorithms for recommending items to users and
analyze them under the benign \emph{hott items} assumption.1) First, for
${S}=1$, under additional incoherence/smoothness assumptions on ${R}$, we
propose the phased algorithm \textsc{PhasedClusterElim}. Our algorithm obtains
a near-optimal per-user regret of
$\tilde{O}({N}{M}^{-1}(\Delta^{-1}+\Delta_{{hott}}^{-2}))$ where
$\Delta_{{hott}},\Delta$ are problem-dependent gap parameters with
$\Delta_{{hott}} >> \Delta$ almost always. 2) Second, we consider a simplified
setting with ${S}=r$ where we make significantly milder assumptions on ${R}$.
Here, we introduce another phased algorithm, \textsc{DeterminantElim}, to
derive a regret guarantee of $\widetilde{O}({N}{M}^{-1/r}\Delta_{det}^{-1}))$
where $\Delta_{{det}}$ is another problem-dependent gap. Both algorithms
crucially use collaboration among users to jointly eliminate sub-optimal items
for groups of users successively in phases, but with distinctive and novel
approaches.","['Dheeraj Baby', 'Soumyabrata Pal']","['cs.LG', 'cs.IR', 'stat.ML']",2024-08-11 18:49:52+00:00
http://arxiv.org/abs/2408.05834v2,Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm,"Unexpected stimuli induce ""error"" or ""surprise"" signals in the brain. The
theory of predictive coding promises to explain these observations in terms of
Bayesian inference by suggesting that the cortex implements variational
inference in a probabilistic graphical model. However, when applied to machine
learning tasks, this family of algorithms has yet to perform on par with other
variational approaches in high-dimensional, structured inference problems. To
address this, we introduce a novel predictive coding algorithm for structured
generative models, that we call divide-and-conquer predictive coding (DCPC).
DCPC differs from other formulations of predictive coding, as it respects the
correlation structure of the generative model and provably performs
maximum-likelihood updates of model parameters, all without sacrificing
biological plausibility. Empirically, DCPC achieves better numerical
performance than competing algorithms and provides accurate inference in a
number of problems not previously addressed with predictive coding. We provide
an open implementation of DCPC in Pyro on Github.","['Eli Sennesh', 'Hao Wu', 'Tommaso Salvatori']","['stat.ML', 'cs.AI', 'cs.LG', 'q-bio.NC']",2024-08-11 17:29:03+00:00
http://arxiv.org/abs/2408.05819v1,On the Convergence of a Federated Expectation-Maximization Algorithm,"Data heterogeneity has been a long-standing bottleneck in studying the
convergence rates of Federated Learning algorithms. In order to better
understand the issue of data heterogeneity, we study the convergence rate of
the Expectation-Maximization (EM) algorithm for the Federated Mixture of $K$
Linear Regressions model. We fully characterize the convergence rate of the EM
algorithm under all regimes of $m/n$ where $m$ is the number of clients and $n$
is the number of data points per client. We show that with a
signal-to-noise-ratio (SNR) of order $\Omega(\sqrt{K})$, the well-initialized
EM algorithm converges within the minimax distance of the ground truth under
each of the regimes. Interestingly, we identify that when $m$ grows
exponentially in $n$, the EM algorithm only requires a constant number of
iterations to converge. We perform experiments on synthetic datasets to
illustrate our results. Surprisingly, the results show that rather than being a
bottleneck, data heterogeneity can accelerate the convergence of federated
learning algorithms.","['Zhixu Tao', 'Rajita Chandak', 'Sanjeev Kulkarni']","['stat.ML', 'cs.LG']",2024-08-11 16:46:42+00:00
http://arxiv.org/abs/2408.05807v3,Kernel Density Estimators in Large Dimensions,"This paper studies Kernel Density Estimation for a high-dimensional
distribution $\rho(x)$. Traditional approaches have focused on the limit of
large number of data points $n$ and fixed dimension $d$. We analyze instead the
regime where both the number $n$ of data points $y_i$ and their dimensionality
$d$ grow with a fixed ratio $\alpha=(\log n)/d$. Our study reveals three
distinct statistical regimes for the kernel-based estimate of the density $\hat
\rho_h^{\mathcal {D}}(x)=\frac{1}{n h^d}\sum_{i=1}^n
K\left(\frac{x-y_i}{h}\right)$, depending on the bandwidth $h$: a classical
regime for large bandwidth where the Central Limit Theorem (CLT) holds, which
is akin to the one found in traditional approaches. Below a certain value of
the bandwidth, $h_{CLT}(\alpha)$, we find that the CLT breaks down. The
statistics of $\hat\rho_h^{\mathcal {D}}(x)$ for a fixed $x$ drawn from
$\rho(x)$ is given by a heavy-tailed distribution (an alpha-stable
distribution). In particular below a value $h_G(\alpha)$, we find that
$\hat\rho_h^{\mathcal {D}}(x)$ is governed by extreme value statistics: only a
few points in the database matter and give the dominant contribution to the
density estimator. We provide a detailed analysis for high-dimensional
multivariate Gaussian data. We show that the optimal bandwidth threshold based
on Kullback-Leibler divergence lies in the new statistical regime identified in
this paper. As known by practitioners, when decreasing the bandwidth a
Kernel-estimated estimated changes from a smooth curve to a collections of
peaks centred on the data points. Our findings reveal that this general
phenomenon is related to sharp transitions between phases characterized by
different statistical properties, and offer new insights for Kernel density
estimation in high-dimensional settings.","['Giulio Biroli', 'Marc Mézard']","['cs.LG', 'cond-mat.dis-nn', 'math.ST', 'stat.ML', 'stat.TH']",2024-08-11 15:56:44+00:00
http://arxiv.org/abs/2408.05788v1,Continual Learning of Nonlinear Independent Representations,"Identifying the causal relations between interested variables plays a pivotal
role in representation learning as it provides deep insights into the dataset.
Identifiability, as the central theme of this approach, normally hinges on
leveraging data from multiple distributions (intervention, distribution shift,
time series, etc.). Despite the exciting development in this field, a practical
but often overlooked problem is: what if those distribution shifts happen
sequentially? In contrast, any intelligence possesses the capacity to abstract
and refine learned knowledge sequentially -- lifelong learning. In this paper,
with a particular focus on the nonlinear independent component analysis (ICA)
framework, we move one step forward toward the question of enabling models to
learn meaningful (identifiable) representations in a sequential manner, termed
continual causal representation learning. We theoretically demonstrate that
model identifiability progresses from a subspace level to a component-wise
level as the number of distributions increases. Empirically, we show that our
method achieves performance comparable to nonlinear ICA methods trained jointly
on multiple offline distributions and, surprisingly, the incoming new
distribution does not necessarily benefit the identification of all latent
variables.","['Boyang Sun', 'Ignavier Ng', 'Guangyi Chen', 'Yifan Shen', 'Qirong Ho', 'Kun Zhang']","['cs.LG', 'cs.AI', 'stat.ML']",2024-08-11 14:33:37+00:00
http://arxiv.org/abs/2408.05765v1,Scalable and Adaptive Spectral Embedding for Attributed Graph Clustering,"Attributed graph clustering, which aims to group the nodes of an attributed
graph into disjoint clusters, has made promising advancements in recent years.
However, most existing methods face challenges when applied to large graphs due
to the expensive computational cost and high memory usage. In this paper, we
introduce Scalable and Adaptive Spectral Embedding (SASE), a simple attributed
graph clustering method devoid of parameter learning. SASE comprises three main
components: node features smoothing via $k$-order simple graph convolution,
scalable spectral clustering using random Fourier features, and adaptive order
selection. With these designs, SASE not only effectively captures global
cluster structures but also exhibits linear time and space complexity relative
to the graph size. Empirical results demonstrate the superiority of SASE. For
example, on the ArXiv dataset with 169K nodes and 1.17M edges, SASE achieves a
6.9\% improvement in ACC and a $5.87\times$ speedup compared to the runner-up,
S3GC.","['Yunhui Liu', 'Tieke He', 'Qing Wu', 'Tao Zheng', 'Jianhua Zhao']","['cs.LG', 'stat.ML']",2024-08-11 12:57:50+00:00
http://arxiv.org/abs/2408.05740v1,MTSCI: A Conditional Diffusion Model for Multivariate Time Series Consistent Imputation,"Missing values are prevalent in multivariate time series, compromising the
integrity of analyses and degrading the performance of downstream tasks.
Consequently, research has focused on multivariate time series imputation,
aiming to accurately impute the missing values based on available observations.
A key research question is how to ensure imputation consistency, i.e.,
intra-consistency between observed and imputed values, and inter-consistency
between adjacent windows after imputation. However, previous methods rely
solely on the inductive bias of the imputation targets to guide the learning
process, ignoring imputation consistency and ultimately resulting in poor
performance. Diffusion models, known for their powerful generative abilities,
prefer to generate consistent results based on available observations.
Therefore, we propose a conditional diffusion model for Multivariate Time
Series Consistent Imputation (MTSCI). Specifically, MTSCI employs a contrastive
complementary mask to generate dual views during the forward noising process.
Then, the intra contrastive loss is calculated to ensure intra-consistency
between the imputed and observed values. Meanwhile, MTSCI utilizes a mixup
mechanism to incorporate conditional information from adjacent windows during
the denoising process, facilitating the inter-consistency between imputed
samples. Extensive experiments on multiple real-world datasets demonstrate that
our method achieves the state-of-the-art performance on multivariate time
series imputation task under different missing scenarios. Code is available at
https://github.com/JeremyChou28/MTSCI.","['Jianping Zhou', 'Junhao Li', 'Guanjie Zheng', 'Xinbing Wang', 'Chenghu Zhou']","['cs.LG', 'cs.AI', 'stat.ML']",2024-08-11 10:24:53+00:00
http://arxiv.org/abs/2408.05690v1,Strong denoising of financial time-series,"In this paper we introduce a method for significantly improving the signal to
noise ratio in financial data. The approach relies on combining a target
variable with different context variables and use auto-encoders (AEs) to learn
reconstructions of the combined inputs. The objective is to obtain agreement
among pairs of AEs which are trained on related but different inputs and for
which they are forced to find common ground. The training process is set up as
a ""conversation"" where the models take turns at producing a prediction
(speaking) and reconciling own predictions with the output of the other AE
(listening), until an agreement is reached. This leads to a new way of
constraining the complexity of the data representation generated by the AE.
Unlike standard regularization whose strength needs to be decided by the
designer, the proposed mutual regularization uses the partner network to detect
and amend the lack of generality of the learned representation of the data. The
integration of alternative perspectives enhances the de-noising capacity of a
single AE and allows us to discover new regularities in financial time-series
which can be converted into profitable trading strategies.",['Matthias J. Feiler'],"['q-fin.ST', 'stat.ML']",2024-08-11 03:58:40+00:00
http://arxiv.org/abs/2408.05620v1,A forward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations,"In this work, we present a novel forward differential deep learning-based
algorithm for solving high-dimensional nonlinear backward stochastic
differential equations (BSDEs). Motivated by the fact that differential deep
learning can efficiently approximate the labels and their derivatives with
respect to inputs, we transform the BSDE problem into a differential deep
learning problem. This is done by leveraging Malliavin calculus, resulting in a
system of BSDEs. The unknown solution of the BSDE system is a triple of
processes $(Y, Z, \Gamma)$, representing the solution, its gradient, and the
Hessian matrix. The main idea of our algorithm is to discretize the integrals
using the Euler-Maruyama method and approximate the unknown discrete solution
triple using three deep neural networks. The parameters of these networks are
then optimized by globally minimizing a differential learning loss function,
which is novelty defined as a weighted sum of the dynamics of the discretized
system of BSDEs. Through various high-dimensional examples, we demonstrate that
our proposed scheme is more efficient in terms of accuracy and computation time
compared to other contemporary forward deep learning-based methodologies.","['Lorenc Kapllani', 'Long Teng']","['math.NA', 'cs.NA', 'q-fin.CP', 'stat.ML', '65C30, 68T07, 60H07, 91G20']",2024-08-10 19:34:03+00:00
http://arxiv.org/abs/2408.05560v1,Incremental Gauss-Newton Descent for Machine Learning,"Stochastic Gradient Descent (SGD) is a popular technique used to solve
problems arising in machine learning. While very effective, SGD also has some
weaknesses and various modifications of the basic algorithm have been proposed
in order to at least partially tackle them, mostly yielding accelerated
versions of SGD. Filling a gap in the literature, we present a modification of
the SGD algorithm exploiting approximate second-order information based on the
Gauss-Newton approach. The new method, which we call Incremental Gauss-Newton
Descent (IGND), has essentially the same computational burden as standard SGD,
appears to converge faster on certain classes of problems, and can also be
accelerated. The key intuition making it possible to implement IGND efficiently
is that, in the incremental case, approximate second-order information can be
condensed into a scalar value that acts as a scaling constant of the update. We
derive IGND starting from the theory supporting Gauss-Newton methods in a
general setting and then explain how IGND can also be interpreted as a
well-scaled version of SGD, which makes tuning the algorithm simpler, and
provides increased robustness. Finally, we show how IGND can be used in
practice by solving supervised learning tasks as well as reinforcement learning
problems. The simulations show that IGND can significantly outperform SGD while
performing at least as well as SGD in the worst case.","['Mikalai Korbit', 'Mario Zanon']","['cs.LG', 'math.OC', 'stat.ML']",2024-08-10 13:52:40+00:00
http://arxiv.org/abs/2408.05537v1,S-SIRUS: an explainability algorithm for spatial regression Random Forest,"Random Forest (RF) is a widely used machine learning algorithm known for its
flexibility, user-friendliness, and high predictive performance across various
domains. However, it is non-interpretable. This can limit its usefulness in
applied sciences, where understanding the relationships between predictors and
response variable is crucial from a decision-making perspective. In the
literature, several methods have been proposed to explain RF, but none of them
addresses the challenge of explaining RF in the context of spatially dependent
data. Therefore, this work aims to explain regression RF in the case of
spatially dependent data by extracting a compact and simple list of rules. In
this respect, we propose S-SIRUS, a spatial extension of SIRUS, the latter
being a well-established regression rule algorithm able to extract a stable and
short list of rules from the classical regression RF algorithm. A simulation
study was conducted to evaluate the explainability capability of the proposed
S-SIRUS, in comparison to SIRUS, by considering different levels of spatial
dependence among the data. The results suggest that S-SIRUS exhibits a higher
test predictive accuracy than SIRUS when spatial correlation is present.
Moreover, for higher levels of spatial correlation, S-SIRUS produces a shorter
list of rules, easing the explanation of the mechanism behind the predictions.","['Luca Patelli', 'Natalia Golini', 'Rosaria Ignaccolo', 'Michela Cameletti']","['stat.ML', 'cs.LG']",2024-08-10 12:41:43+00:00
http://arxiv.org/abs/2408.05535v1,Latent class analysis for multi-layer categorical data,"Traditional categorical data, often collected in psychological tests and
educational assessments, are typically single-layer and gathered only once.This
paper considers a more general case, multi-layer categorical data with
polytomous responses. To model such data, we present a novel statistical model,
the multi-layer latent class model (multi-layer LCM). This model assumes that
all layers share common subjects and items. To discover subjects' latent
classes and other model parameters under this model, we develop three efficient
spectral methods based on the sum of response matrices, the sum of Gram
matrices, and the debiased sum of Gram matrices, respectively. Within the
framework of multi-layer LCM, we demonstrate the estimation consistency of
these methods under mild conditions regarding data sparsity. Our theoretical
findings reveal two key insights: (1) increasing the number of layers can
enhance the performance of the proposed methods, highlighting the advantages of
considering multiple layers in latent class analysis; (2) we theoretically show
that the algorithm based on the debiased sum of Gram matrices usually performs
best. Additionally, we propose an approach that combines the averaged
modularity metric with our methods to determine the number of latent classes.
Extensive experiments are conducted to support our theoretical results and show
the powerfulness of our methods in the task of learning latent classes and
estimating the number of latent classes in multi-layer categorical data with
polytomous responses.",['Huan Qing'],"['stat.ML', 'cs.LG']",2024-08-10 12:31:31+00:00
http://arxiv.org/abs/2408.05496v1,Variational Inference Failures Under Model Symmetries: Permutation Invariant Posteriors for Bayesian Neural Networks,"Weight space symmetries in neural network architectures, such as permutation
symmetries in MLPs, give rise to Bayesian neural network (BNN) posteriors with
many equivalent modes. This multimodality poses a challenge for variational
inference (VI) techniques, which typically rely on approximating the posterior
with a unimodal distribution. In this work, we investigate the impact of weight
space permutation symmetries on VI. We demonstrate, both theoretically and
empirically, that these symmetries lead to biases in the approximate posterior,
which degrade predictive performance and posterior fit if not explicitly
accounted for. To mitigate this behavior, we leverage the symmetric structure
of the posterior and devise a symmetrization mechanism for constructing
permutation invariant variational posteriors. We show that the symmetrized
distribution has a strictly better fit to the true posterior, and that it can
be trained using the original ELBO objective with a modified KL regularization
term. We demonstrate experimentally that our approach mitigates the
aforementioned biases and results in improved predictions and a higher ELBO.","['Yoav Gelberg', 'Tycho F. A. van der Ouderaa', 'Mark van der Wilk', 'Yarin Gal']","['cs.LG', 'stat.ML']",2024-08-10 09:06:34+00:00
http://arxiv.org/abs/2408.05486v1,Topological Blind Spots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity,"Topological deep learning (TDL) facilitates learning from data represented by
topological structures. The primary model utilized in this setting is
higher-order message-passing (HOMP), which extends traditional graph
message-passing neural networks (MPNN) to diverse topological domains. Given
the significant expressivity limitations of MPNNs, our paper aims to explore
both the strengths and weaknesses of HOMP's expressive power and subsequently
design novel architectures to address these limitations. We approach this from
several perspectives: First, we demonstrate HOMP's inability to distinguish
between topological objects based on fundamental topological and metric
properties such as diameter, orientability, planarity, and homology. Second, we
show HOMP's limitations in fully leveraging the topological structure of
objects constructed using common lifting and pooling operators on graphs.
Finally, we compare HOMP's expressive power to hypergraph networks, which are
the most extensively studied TDL methods. We then develop two new classes of
TDL models: multi-cellular networks (MCN) and scalable multi-cellular networks
(SMCN). These models draw inspiration from expressive graph architectures.
While MCN can reach full expressivity but is highly unscalable, SMCN offers a
more scalable alternative that still mitigates many of HOMP's expressivity
limitations. Finally, we construct a synthetic dataset, where TDL models are
tasked with separating pairs of topological objects based on basic topological
properties. We demonstrate that while HOMP is unable to distinguish between any
of the pairs in the dataset, SMCN successfully distinguishes all pairs,
empirically validating our theoretical findings. Our work opens a new design
space and new opportunities for TDL, paving the way for more expressive and
versatile models.","['Yam Eitan', 'Yoav Gelberg', 'Guy Bar-Shalom', 'Fabrizio Frasca', 'Michael Bronstein', 'Haggai Maron']","['cs.LG', 'math.AT', 'stat.ML']",2024-08-10 08:27:58+00:00
http://arxiv.org/abs/2408.05431v2,Simple and Nearly-Optimal Sampling for Rank-1 Tensor Completion via Gauss-Jordan,"We revisit the sample and computational complexity of completing a rank-1
tensor in $\otimes_{i=1}^{N} \mathbb{R}^{d}$, given a uniformly sampled subset
of its entries. We present a characterization of the problem (i.e. nonzero
entries) which admits an algorithm amounting to Gauss-Jordan on a pair of
random linear systems. For example, when $N = \Theta(1)$, we prove it uses no
more than $m = O(d^2 \log d)$ samples and runs in $O(md^2)$ time. Moreover, we
show any algorithm requires $\Omega(d\log d)$ samples.
  By contrast, existing upper bounds on the sample complexity are at least as
large as $d^{1.5} \mu^{\Omega(1)} \log^{\Omega(1)} d$, where $\mu$ can be
$\Theta(d)$ in the worst case. Prior work obtained these looser guarantees in
higher rank versions of our problem, and tend to involve more complicated
algorithms.","['Alejandro Gomez-Leos', 'Oscar López']","['cs.DS', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2024-08-10 04:26:19+00:00
http://arxiv.org/abs/2408.05428v1,Generalized Encouragement-Based Instrumental Variables for Counterfactual Regression,"In causal inference, encouragement designs (EDs) are widely used to analyze
causal effects, when randomized controlled trials (RCTs) are impractical or
compliance to treatment cannot be perfectly enforced. Unlike RCTs, which
directly allocate treatments, EDs randomly assign encouragement policies that
positively motivate individuals to engage in a specific treatment. These random
encouragements act as instrumental variables (IVs), facilitating the
identification of causal effects through leveraging exogenous perturbations in
discrete treatment scenarios. However, real-world applications of encouragement
designs often face challenges such as incomplete randomization, limited
experimental data, and significantly fewer encouragements compared to
treatments, hindering precise causal effect estimation. To address this, this
paper introduces novel theories and algorithms for identifying the Conditional
Average Treatment Effect (CATE) using variations in encouragement. Further, by
leveraging both observational and encouragement data, we propose a generalized
IV estimator, named Encouragement-based Counterfactual Regression (EnCounteR),
to effectively estimate the causal effects. Extensive experiments on both
synthetic and real-world datasets demonstrate the superiority of EnCounteR over
existing methods.","['Anpeng Wu', 'Kun Kuang', 'Ruoxuan Xiong', 'Xiangwei Chen', 'Zexu Sun', 'Fei Wu', 'Kun Zhang']","['cs.LG', 'stat.ME', 'stat.ML']",2024-08-10 04:21:04+00:00
http://arxiv.org/abs/2408.05393v1,fastkqr: A Fast Algorithm for Kernel Quantile Regression,"Quantile regression is a powerful tool for robust and heterogeneous learning
that has seen applications in a diverse range of applied areas. However, its
broader application is often hindered by the substantial computational demands
arising from the non-smooth quantile loss function. In this paper, we introduce
a novel algorithm named fastkqr, which significantly advances the computation
of quantile regression in reproducing kernel Hilbert spaces. The core of
fastkqr is a finite smoothing algorithm that magically produces exact
regression quantiles, rather than approximations. To further accelerate the
algorithm, we equip fastkqr with a novel spectral technique that carefully
reutilizes matrix computations. In addition, we extend fastkqr to accommodate a
flexible kernel quantile regression with a data-driven crossing penalty,
addressing the interpretability challenges of crossing quantile curves at
multiple levels. We have implemented fastkqr in a publicly available R package.
Extensive simulations and real applications show that fastkqr matches the
accuracy of state-of-the-art algorithms but can operate up to an order of
magnitude faster.","['Qian Tang', 'Yuwen Gu', 'Boxiang Wang']","['stat.ML', 'cs.LG']",2024-08-10 00:18:56+00:00
http://arxiv.org/abs/2408.05116v1,Concept learning of parameterized quantum models from limited measurements,"Classical learning of the expectation values of observables for quantum
states is a natural variant of learning quantum states or channels. While
learning-theoretic frameworks establish the sample complexity and the number of
measurement shots per sample required for learning such statistical quantities,
the interplay between these two variables has not been adequately quantified
before. In this work, we take the probabilistic nature of quantum measurements
into account in classical modelling and discuss these quantities under a single
unified learning framework. We provide provable guarantees for learning
parameterized quantum models that also quantify the asymmetrical effects and
interplay of the two variables on the performance of learning algorithms. These
results show that while increasing the sample size enhances the learning
performance of classical machines, even with single-shot estimates, the
improvements from increasing measurements become asymptotically trivial beyond
a constant factor. We further apply our framework and theoretical guarantees to
study the impact of measurement noise on the classical surrogation of
parameterized quantum circuit models. Our work provides new tools to analyse
the operational influence of finite measurement noise in the classical learning
of quantum systems.","['Beng Yee Gan', 'Po-Wei Huang', 'Elies Gil-Fuster', 'Patrick Rebentrost']","['quant-ph', 'cs.LG', 'stat.ML']",2024-08-09 15:07:42+00:00
http://arxiv.org/abs/2408.05085v1,On expected signatures and signature cumulants in semimartingale models,"The concept of signatures and expected signatures is vital in data science,
especially for sequential data analysis. The signature transform, a Cartan type
development, translates paths into high-dimensional feature vectors, capturing
their intrinsic characteristics. Under natural conditions, the expectation of
the signature determines the law of the signature, providing a statistical
summary of the data distribution. This property facilitates robust modeling and
inference in machine learning and stochastic processes. Building on previous
work by the present authors [Unified signature cumulants and generalized Magnus
expansions, FoM Sigma '22] we here revisit the actual computation of expected
signatures, in a general semimartingale setting. Several new formulae are
given. A log-transform of (expected) signatures leads to log-signatures
(signature cumulants), offering a significant reduction in complexity.","['Peter K. Friz', 'Paul P. Hager', 'Nikolas Tapia']","['stat.ML', 'math.PR', '60L10, 60L90, 60E10, 60G44, 60G48, 60G51, 60J76']",2024-08-09 14:16:21+00:00
http://arxiv.org/abs/2408.05058v1,Variational Bayesian Phylogenetic Inference with Semi-implicit Branch Length Distributions,"Reconstructing the evolutionary history relating a collection of molecular
sequences is the main subject of modern Bayesian phylogenetic inference.
However, the commonly used Markov chain Monte Carlo methods can be inefficient
due to the complicated space of phylogenetic trees, especially when the number
of sequences is large. An alternative approach is variational Bayesian
phylogenetic inference (VBPI) which transforms the inference problem into an
optimization problem. While effective, the default diagonal lognormal
approximation for the branch lengths of the tree used in VBPI is often
insufficient to capture the complexity of the exact posterior. In this work, we
propose a more flexible family of branch length variational posteriors based on
semi-implicit hierarchical distributions using graph neural networks. We show
that this semi-implicit construction emits straightforward permutation
equivariant distributions, and therefore can handle the non-Euclidean branch
length space across different tree topologies with ease. To deal with the
intractable marginal probability of semi-implicit variational distributions, we
develop several alternative lower bounds for stochastic optimization. We
demonstrate the effectiveness of our proposed method over baseline methods on
benchmark data examples, in terms of both marginal likelihood estimation and
branch length posterior approximation.","['Tianyu Xie', 'Frederick A. Matsen IV', 'Marc A. Suchard', 'Cheng Zhang']","['stat.ML', 'cs.LG']",2024-08-09 13:29:08+00:00
http://arxiv.org/abs/2408.05040v1,BoFire: Bayesian Optimization Framework Intended for Real Experiments,"Our open-source Python package BoFire combines Bayesian Optimization (BO)
with other design of experiments (DoE) strategies focusing on developing and
optimizing new chemistry. Previous BO implementations, for example as they
exist in the literature or software, require substantial adaptation for
effective real-world deployment in chemical industry. BoFire provides a rich
feature-set with extensive configurability and realizes our vision of
fast-tracking research contributions into industrial use via maintainable
open-source software. Owing to quality-of-life features like
JSON-serializability of problem formulations, BoFire enables seamless
integration of BO into RESTful APIs, a common architecture component for both
self-driving laboratories and human-in-the-loop setups. This paper discusses
the differences between BoFire and other BO implementations and outlines ways
that BO research needs to be adapted for real-world use in a chemistry setting.","['Johannes P. Dürholt', 'Thomas S. Asche', 'Johanna Kleinekorte', 'Gabriel Mancino-Ball', 'Benjamin Schiller', 'Simon Sung', 'Julian Keupp', 'Aaron Osburg', 'Toby Boyne', 'Ruth Misener', 'Rosona Eldred', 'Wagner Steuer Costa', 'Chrysoula Kappatou', 'Robert M. Lee', 'Dominik Linzner', 'David Walz', 'Niklas Wulkow', 'Behrang Shafei']","['cs.LG', 'math.OC', 'stat.ML']",2024-08-09 12:50:48+00:00
http://arxiv.org/abs/2408.04948v1,HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction,"Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain","['Bhaskarjit Sarmah', 'Benika Hall', 'Rohan Rao', 'Sunil Patel', 'Stefano Pasquali', 'Dhagash Mehta']","['cs.CL', 'cs.LG', 'q-fin.ST', 'stat.AP', 'stat.ML']",2024-08-09 09:07:48+00:00
http://arxiv.org/abs/2408.04933v1,Variance-based sensitivity analysis in the presence of correlated input variables,"In this paper we propose an extension of the classical Sobol' estimator for
the estimation of variance based sensitivity indices. The approach assumes a
linear correlation model between the input variables which is used to decompose
the contribution of an input variable into a correlated and an uncorrelated
part. This method provides sampling matrices following the original joint
probability distribution which are used directly to compute the model output
without any assumptions or approximations of the model response function.",['Thomas Most'],"['stat.ME', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2024-08-09 08:32:58+00:00
http://arxiv.org/abs/2408.04907v1,Causal Discovery of Linear Non-Gaussian Causal Models with Unobserved Confounding,"We consider linear non-Gaussian structural equation models that involve
latent confounding. In this setting, the causal structure is identifiable, but,
in general, it is not possible to identify the specific causal effects.
Instead, a finite number of different causal effects result in the same
observational distribution. Most existing algorithms for identifying these
causal effects use overcomplete independent component analysis (ICA), which
often suffers from convergence to local optima. Furthermore, the number of
latent variables must be known a priori. To address these issues, we propose an
algorithm that operates recursively rather than using overcomplete ICA. The
algorithm first infers a source, estimates the effect of the source and its
latent parents on their descendants, and then eliminates their influence from
the data. For both source identification and effect size estimation, we use
rank conditions on matrices formed from higher-order cumulants. We prove
asymptotic correctness under the mild assumption that locally, the number of
latent variables never exceeds the number of observed variables. Simulation
studies demonstrate that our method achieves comparable performance to
overcomplete ICA even though it does not know the number of latents in advance.","['Daniela Schkoda', 'Elina Robeva', 'Mathias Drton']","['stat.ML', 'cs.LG', 'stat.ME']",2024-08-09 07:24:12+00:00
http://arxiv.org/abs/2408.04869v3,UCB Exploration for Fixed-Budget Bayesian Best Arm Identification,"We study best-arm identification (BAI) in the fixed-budget setting. Adaptive
allocations based on upper confidence bounds (UCBs), such as UCBE, are known to
work well in BAI. However, it is well-known that its optimal regret is
theoretically dependent on instances, which we show to be an artifact in many
fixed-budget BAI problems. In this paper we propose an UCB exploration
algorithm that is both theoretically and empirically efficient for the fixed
budget BAI problem under a Bayesian setting. The key idea is to learn prior
information, which can enhance the performance of UCB-based BAI algorithm as it
has done in the cumulative regret minimization problem. We establish bounds on
the failure probability and the simple regret for the Bayesian BAI problem,
providing upper bounds of order $\tilde{O}(\sqrt{K/n})$, up to logarithmic
factors, where $n$ represents the budget and $K$ denotes the number of arms.
Furthermore, we demonstrate through empirical results that our approach
consistently outperforms state-of-the-art baselines.","['Rong J. B. Zhu', 'Yanqi Qiu']","['cs.LG', 'stat.ML']",2024-08-09 05:15:36+00:00
http://arxiv.org/abs/2408.04851v1,Your Classifier Can Be Secretly a Likelihood-Based OOD Detector,"The ability to detect out-of-distribution (OOD) inputs is critical to
guarantee the reliability of classification models deployed in an open
environment. A fundamental challenge in OOD detection is that a discriminative
classifier is typically trained to estimate the posterior probability p(y|z)
for class y given an input z, but lacks the explicit likelihood estimation of
p(z) ideally needed for OOD detection. While numerous OOD scoring functions
have been proposed for classification models, these estimate scores are often
heuristic-driven and cannot be rigorously interpreted as likelihood. To bridge
the gap, we propose Intrinsic Likelihood (INK), which offers rigorous
likelihood interpretation to modern discriminative-based classifiers.
Specifically, our proposed INK score operates on the constrained latent
embeddings of a discriminative classifier, which are modeled as a mixture of
hyperspherical embeddings with constant norm. We draw a novel connection
between the hyperspherical distribution and the intrinsic likelihood, which can
be effectively optimized in modern neural networks. Extensive experiments on
the OpenOOD benchmark empirically demonstrate that INK establishes a new
state-of-the-art in a variety of OOD detection setups, including both far-OOD
and near-OOD. Code is available at https://github.com/deeplearning-wisc/ink.","['Jirayu Burapacheep', 'Yixuan Li']","['cs.LG', 'stat.ML']",2024-08-09 04:00:53+00:00
http://arxiv.org/abs/2408.04847v1,A Pipeline for Data-Driven Learning of Topological Features with Applications to Protein Stability Prediction,"In this paper, we propose a data-driven method to learn interpretable
topological features of biomolecular data and demonstrate the efficacy of
parsimonious models trained on topological features in predicting the stability
of synthetic mini proteins. We compare models that leverage
automatically-learned structural features against models trained on a large set
of biophysical features determined by subject-matter experts (SME). Our models,
based only on topological features of the protein structures, achieved 92%-99%
of the performance of SME-based models in terms of the average precision score.
By interrogating model performance and feature importance metrics, we extract
numerous insights that uncover high correlations between topological features
and SME features. We further showcase how combining topological features and
SME features can lead to improved model performance over either feature set
used in isolation, suggesting that, in some settings, topological features may
provide new discriminating information not captured in existing SME features
that are useful for protein stability prediction.","['Amish Mishra', 'Francis Motta']","['stat.ML', 'cs.LG', 'physics.data-an']",2024-08-09 03:52:27+00:00
http://arxiv.org/abs/2408.04819v1,Interventional Causal Structure Discovery over Graphical Models with Convergence and Optimality Guarantees,"Learning causal structure from sampled data is a fundamental problem with
applications in various fields, including healthcare, machine learning and
artificial intelligence. Traditional methods predominantly rely on
observational data, but there exist limits regarding the identifiability of
causal structures with only observational data. Interventional data, on the
other hand, helps establish a cause-and-effect relationship by breaking the
influence of confounding variables. It remains to date under-explored to
develop a mathematical framework that seamlessly integrates both observational
and interventional data in causal structure learning. Furthermore, existing
studies often focus on centralized approaches, necessitating the transfer of
entire datasets to a single server, which lead to considerable communication
overhead and heightened risks to privacy. To tackle these challenges, we
develop a bilevel polynomial optimization (Bloom) framework. Bloom not only
provides a powerful mathematical modeling framework, underpinned by theoretical
support, for causal structure discovery from both interventional and
observational data, but also aspires to an efficient causal discovery algorithm
with convergence and optimality guarantees. We further extend Bloom to a
distributed setting to reduce the communication overhead and mitigate data
privacy risks. It is seen through experiments on both synthetic and real-world
datasets that Bloom markedly surpasses other leading learning algorithms.","['Qiu Chengbo', 'Yang Kai']","['cs.LG', 'stat.ML']",2024-08-09 02:22:50+00:00
http://arxiv.org/abs/2408.04796v1,A Density Ratio Super Learner,"The estimation of the ratio of two density probability functions is of great
interest in many statistics fields, including causal inference. In this study,
we develop an ensemble estimator of density ratios with a novel loss function
based on super learning. We show that this novel loss function is qualified for
building super learners. Two simulations corresponding to mediation analysis
and longitudinal modified treatment policy in causal inference, where density
ratios are nuisance parameters, are conducted to show our density ratio super
learner's performance empirically.","['Wencheng Wu', 'David Benkeser']","['stat.ML', 'cs.LG']",2024-08-09 00:28:22+00:00
http://arxiv.org/abs/2408.04765v1,Scalable learning of potentials to predict time-dependent Hartree-Fock dynamics,"We propose a framework to learn the time-dependent Hartree-Fock (TDHF)
inter-electronic potential of a molecule from its electron density dynamics.
Though the entire TDHF Hamiltonian, including the inter-electronic potential,
can be computed from first principles, we use this problem as a testbed to
develop strategies that can be applied to learn \emph{a priori} unknown terms
that arise in other methods/approaches to quantum dynamics, e.g., emerging
problems such as learning exchange-correlation potentials for time-dependent
density functional theory. We develop, train, and test three models of the TDHF
inter-electronic potential, each parameterized by a four-index tensor of size
up to $60 \times 60 \times 60 \times 60$. Two of the models preserve Hermitian
symmetry, while one model preserves an eight-fold permutation symmetry that
implies Hermitian symmetry. Across seven different molecular systems, we find
that accounting for the deeper eight-fold symmetry leads to the best-performing
model across three metrics: training efficiency, test set predictive power, and
direct comparison of true and learned inter-electronic potentials. All three
models, when trained on ensembles of field-free trajectories, generate accurate
electron dynamics predictions even in a field-on regime that lies outside the
training set. To enable our models to scale to large molecular systems, we
derive expressions for Jacobian-vector products that enable iterative,
matrix-free training.","['Harish S. Bhat', 'Prachi Gupta', 'Christine M. Isborn']","['physics.chem-ph', 'physics.comp-ph', 'physics.data-an', 'stat.ML']",2024-08-08 21:41:51+00:00
http://arxiv.org/abs/2408.04739v2,Accurate deep learning-based filtering for chaotic dynamics by identifying instabilities without an ensemble,"We investigate the ability to discover data assimilation (DA) schemes meant
for chaotic dynamics with deep learning. The focus is on learning the analysis
step of sequential DA, from state trajectories and their observations, using a
simple residual convolutional neural network, while assuming the dynamics to be
known. Experiments are performed with the Lorenz 96 dynamics, which display
spatiotemporal chaos and for which solid benchmarks for DA performance exist.
The accuracy of the states obtained from the learned analysis approaches that
of the best possibly tuned ensemble Kalman filter, and is far better than that
of variational DA alternatives. Critically, this can be achieved while
propagating even just a single state in the forecast step. We investigate the
reason for achieving ensemble filtering accuracy without an ensemble. We
diagnose that the analysis scheme actually identifies key dynamical
perturbations, mildly aligned with the unstable subspace, from the forecast
state alone, without any ensemble-based covariances representation. This
reveals that the analysis scheme has learned some multiplicative ergodic
theorem associated to the DA process seen as a non-autonomous random dynamical
system.","['Marc Bocquet', 'Alban Farchi', 'Tobias S. Finn', 'Charlotte Durand', 'Sibo Cheng', 'Yumeng Chen', 'Ivo Pasmans', 'Alberto Carrassi']","['nlin.CD', 'physics.ao-ph', 'stat.ML']",2024-08-08 19:44:57+00:00
http://arxiv.org/abs/2408.04718v1,Zero-Shot Uncertainty Quantification using Diffusion Probabilistic Models,"The success of diffusion probabilistic models in generative tasks, such as
text-to-image generation, has motivated the exploration of their application to
regression problems commonly encountered in scientific computing and various
other domains. In this context, the use of diffusion regression models for
ensemble prediction is becoming a practice with increasing popularity. Under
such background, we conducted a study to quantitatively evaluate the
effectiveness of ensemble methods on solving different regression problems
using diffusion models. We consider the ensemble prediction of a diffusion
model as a means for zero-shot uncertainty quantification, since the diffusion
models in our study are not trained with a loss function containing any
uncertainty estimation. Through extensive experiments on 1D and 2D data, we
demonstrate that ensemble methods consistently improve model prediction
accuracy across various regression tasks. Notably, we observed a larger
accuracy gain in auto-regressive prediction compared with point-wise
prediction, and that enhancements take place in both the mean-square error and
the physics-informed loss. Additionally, we reveal a statistical correlation
between ensemble prediction error and ensemble variance, offering insights into
balancing computational complexity with prediction accuracy and monitoring
prediction confidence in practical applications where the ground truth is
unknown. Our study provides a comprehensive view of the utility of diffusion
ensembles, serving as a useful reference for practitioners employing diffusion
models in regression problem-solving.","['Dule Shu', 'Amir Barati Farimani']","['cs.LG', 'stat.ML']",2024-08-08 18:34:52+00:00
http://arxiv.org/abs/2408.04607v2,Risk and cross validation in ridge regression with correlated samples,"Recent years have seen substantial advances in our understanding of
high-dimensional ridge regression, but existing theories assume that training
examples are independent. By leveraging recent techniques from random matrix
theory and free probability, we provide sharp asymptotics for the in- and
out-of-sample risks of ridge regression when the data points have arbitrary
correlations. We demonstrate that in this setting, the generalized cross
validation estimator (GCV) fails to correctly predict the out-of-sample risk.
However, in the case where the noise residuals have the same correlations as
the data points, one can modify the GCV to yield an efficiently-computable
unbiased estimator that concentrates in the high-dimensional limit, which we
dub CorrGCV. We further extend our asymptotic analysis to the case where the
test point has nontrivial correlations with the training set, a setting often
encountered in time series forecasting. Assuming knowledge of the correlation
structure of the time series, this again yields an extension of the GCV
estimator, and sharply characterizes the degree to which such test points yield
an overly optimistic prediction of long-time risk. We validate the predictions
of our theory across a variety of high dimensional data.","['Alexander Atanasov', 'Jacob A. Zavatone-Veth', 'Cengiz Pehlevan']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG']",2024-08-08 17:27:29+00:00
http://arxiv.org/abs/2408.04595v1,Inference with the Upper Confidence Bound Algorithm,"In this paper, we discuss the asymptotic behavior of the Upper Confidence
Bound (UCB) algorithm in the context of multiarmed bandit problems and discuss
its implication in downstream inferential tasks. While inferential tasks become
challenging when data is collected in a sequential manner, we argue that this
problem can be alleviated when the sequential algorithm at hand satisfies
certain stability property. This notion of stability is motivated from the
seminal work of Lai and Wei (1982). Our first main result shows that such a
stability property is always satisfied for the UCB algorithm, and as a result
the sample means for each arm are asymptotically normal. Next, we examine the
stability properties of the UCB algorithm when the number of arms $K$ is
allowed to grow with the number of arm pulls $T$. We show that in such a case
the arms are stable when $\frac{\log K}{\log T} \rightarrow 0$, and the number
of near-optimal arms are large.","['Koulik Khamaru', 'Cun-Hui Zhang']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.SY', 'eess.SY', 'math.ST', 'stat.TH']",2024-08-08 17:11:36+00:00
http://arxiv.org/abs/2408.04569v1,Activation thresholds and expressiveness of polynomial neural networks,"Polynomial neural networks have been implemented in a range of applications
and present an advantageous framework for theoretical machine learning. A
polynomial neural network of fixed architecture and activation degree gives an
algebraic map from the network's weights to a set of polynomials. The image of
this map is the space of functions representable by the network. Its Zariski
closure is an affine variety known as a neurovariety. The dimension of a
polynomial neural network's neurovariety provides a measure of its
expressivity. In this work, we introduce the notion of the activation threshold
of a network architecture which expresses when the dimension of a neurovariety
achieves its theoretical maximum. In addition, we prove expressiveness results
for polynomial neural networks with equi-width~architectures.","['Bella Finkel', 'Jose Israel Rodriguez', 'Chenxi Wu', 'Thomas Yahl']","['cs.LG', 'cs.NE', 'math.AG', 'stat.ML']",2024-08-08 16:28:56+00:00
http://arxiv.org/abs/2408.04526v1,Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs,"Hybrid Reinforcement Learning (RL), where an agent learns from both an
offline dataset and online explorations in an unknown environment, has garnered
significant recent interest. A crucial question posed by Xie et al. (2022) is
whether hybrid RL can improve upon the existing lower bounds established in
purely offline and purely online RL without relying on the single-policy
concentrability assumption. While Li et al. (2023) provided an affirmative
answer to this question in the tabular PAC RL case, the question remains
unsettled for both the regret-minimizing RL case and the non-tabular case.
  In this work, building upon recent advancements in offline RL and
reward-agnostic exploration, we develop computationally efficient algorithms
for both PAC and regret-minimizing RL with linear function approximation,
without single-policy concentrability. We demonstrate that these algorithms
achieve sharper error or regret bounds that are no worse than, and can improve
on, the optimal sample complexity in offline RL (the first algorithm, for PAC
RL) and online RL (the second algorithm, for regret-minimizing RL) in linear
Markov decision processes (MDPs), regardless of the quality of the behavior
policy. To our knowledge, this work establishes the tightest theoretical
guarantees currently available for hybrid RL in linear MDPs.","['Kevin Tan', 'Wei Fan', 'Yuting Wei']","['stat.ML', 'cs.LG']",2024-08-08 15:26:18+00:00
http://arxiv.org/abs/2408.04391v2,Robustness investigation of cross-validation based quality measures for model assessment,"In this paper the accuracy and robustness of quality measures for the
assessment of machine learning models are investigated. The prediction quality
of a machine learning model is evaluated model-independent based on a
cross-validation approach, where the approximation error is estimated for
unknown data. The presented measures quantify the amount of explained variation
in the model prediction. The reliability of these measures is assessed by means
of several numerical examples, where an additional data set for the
verification of the estimated prediction error is available. Furthermore, the
confidence bounds of the presented quality measures are estimated and local
quality measures are derived from the prediction residuals obtained by the
cross-validation approach.","['Thomas Most', 'Lars Gräning', 'Sebastian Wolff']","['stat.ML', 'cs.LG']",2024-08-08 11:51:34+00:00
http://arxiv.org/abs/2408.04313v1,Better Locally Private Sparse Estimation Given Multiple Samples Per User,"Previous studies yielded discouraging results for item-level locally
differentially private linear regression with $s^*$-sparsity assumption, where
the minimax rate for $nm$ samples is $\mathcal{O}(s^{*}d / nm\varepsilon^2)$.
This can be challenging for high-dimensional data, where the dimension $d$ is
extremely large. In this work, we investigate user-level locally differentially
private sparse linear regression. We show that with $n$ users each contributing
$m$ samples, the linear dependency of dimension $d$ can be eliminated, yielding
an error upper bound of $\mathcal{O}(s^{*2} / nm\varepsilon^2)$. We propose a
framework that first selects candidate variables and then conducts estimation
in the narrowed low-dimensional space, which is extendable to general sparse
estimation problems with tight error bounds. Experiments on both synthetic and
real datasets demonstrate the superiority of the proposed methods. Both the
theoretical and empirical results suggest that, with the same number of
samples, locally private sparse estimation is better conducted when multiple
samples per user are available.","['Yuheng Ma', 'Ke Jia', 'Hanfang Yang']","['stat.ML', 'cs.LG', 'stat.ME']",2024-08-08 08:47:20+00:00
http://arxiv.org/abs/2408.04179v1,An Upper Confidence Bound Approach to Estimating the Maximum Mean,"Estimating the maximum mean finds a variety of applications in practice. In
this paper, we study estimation of the maximum mean using an upper confidence
bound (UCB) approach where the sampling budget is adaptively allocated to one
of the systems. We study in depth the existing grand average (GA) estimator,
and propose a new largest-size average (LSA) estimator. Specifically, we
establish statistical guarantees, including strong consistency, asymptotic mean
squared errors, and central limit theorems (CLTs) for both estimators, which
are new to the literature. We show that LSA is preferable over GA, as the bias
of the former decays at a rate much faster than that of the latter when sample
size increases. By using the CLTs, we further construct asymptotically valid
confidence intervals for the maximum mean, and propose a single hypothesis test
for a multiple comparison problem with application to clinical trials.
Statistical efficiency of the resulting point and interval estimates and the
proposed single hypothesis test is demonstrated via numerical examples.","['Zhang Kun', 'Liu Guangwu', 'Shi Wen']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2024-08-08 02:53:09+00:00
http://arxiv.org/abs/2408.04154v1,The Data Addition Dilemma,"In many machine learning for healthcare tasks, standard datasets are
constructed by amassing data across many, often fundamentally dissimilar,
sources. But when does adding more data help, and when does it hinder progress
on desired model outcomes in real-world settings? We identify this situation as
the \textit{Data Addition Dilemma}, demonstrating that adding training data in
this multi-source scaling context can at times result in reduced overall
accuracy, uncertain fairness outcomes, and reduced worst-subgroup performance.
We find that this possibly arises from an empirically observed trade-off
between model performance improvements due to data scaling and model
deterioration from distribution shift. We thus establish baseline strategies
for navigating this dilemma, introducing distribution shift heuristics to guide
decision-making on which data sources to add in data scaling, in order to yield
the expected model performance improvements. We conclude with a discussion of
the required considerations for data collection and suggestions for studying
data composition and scale in the age of increasingly larger models.","['Judy Hanwen Shen', 'Inioluwa Deborah Raji', 'Irene Y. Chen']","['cs.LG', 'cs.AI', 'stat.ML']",2024-08-08 01:42:31+00:00
http://arxiv.org/abs/2408.04149v1,A tutorial on the dynamic Laplacian,"Spectral techniques are popular and robust approaches to data analysis. A
prominent example is the use of eigenvectors of a Laplacian, constructed from
data affinities, to identify natural data groupings or clusters, or to produce
a simplified representation of data lying on a manifold. This tutorial concerns
the dynamic Laplacian, which is a natural generalisation of the Laplacian to
handle data that has a time component and lies on a time-evolving manifold. In
this dynamic setting, clusters correspond to long-lived ``coherent''
collections. We begin with a gentle recap of spectral geometry before
describing the dynamic generalisations. We also discuss computational methods
and the automatic separation of many distinct features through the SEBA
algorithm. The purpose of this tutorial is to bring together many results from
the dynamic Laplacian literature into a single short document, written in an
accessible style.",['Gary Froyland'],"['math.DS', 'math.SP', 'stat.ML', '37C05, 37N10, 58J50']",2024-08-08 01:23:00+00:00
http://arxiv.org/abs/2408.03769v1,Nadaraya-Watson kernel smoothing as a random energy model,"We investigate the behavior of the Nadaraya-Watson kernel smoothing estimator
in high dimensions using its relationship to the random energy model and to
dense associative memories.","['Jacob A. Zavatone-Veth', 'Cengiz Pehlevan']","['cond-mat.dis-nn', 'stat.ML']",2024-08-07 13:43:21+00:00
http://arxiv.org/abs/2408.03746v1,Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion Posterior Sampling,"Bayesian Last Layer (BLL) models focus solely on uncertainty in the output
layer of neural networks, demonstrating comparable performance to more complex
Bayesian models. However, the use of Gaussian priors for last layer weights in
Bayesian Last Layer (BLL) models limits their expressive capacity when faced
with non-Gaussian, outlier-rich, or high-dimensional datasets. To address this
shortfall, we introduce a novel approach that combines diffusion techniques and
implicit priors for variational learning of Bayesian last layer weights. This
method leverages implicit distributions for modeling weight priors in BLL,
coupled with diffusion samplers for approximating true posterior predictions,
thereby establishing a comprehensive Bayesian prior and posterior estimation
strategy. By delivering an explicit and computationally efficient variational
lower bound, our method aims to augment the expressive abilities of BLL models,
enhancing model accuracy, calibration, and out-of-distribution detection
proficiency. Through detailed exploration and experimental validation, We
showcase the method's potential for improving predictive accuracy and
uncertainty quantification while ensuring computational efficiency.","['Jian Xu', 'Zhiqi Lin', 'Shigui Li', 'Min Chen', 'Junmei Yang', 'Delu Zeng', 'John Paisley']","['cs.LG', 'cs.AI', 'stat.ML']",2024-08-07 12:59:58+00:00
http://arxiv.org/abs/2408.03733v1,Bayes-optimal learning of an extensive-width neural network from quadratically many samples,"We consider the problem of learning a target function corresponding to a
single hidden layer neural network, with a quadratic activation function after
the first layer, and random weights. We consider the asymptotic limit where the
input dimension and the network width are proportionally large. Recent work
[Cui & al '23] established that linear regression provides Bayes-optimal test
error to learn such a function when the number of available samples is only
linear in the dimension. That work stressed the open challenge of theoretically
analyzing the optimal test error in the more interesting regime where the
number of samples is quadratic in the dimension. In this paper, we solve this
challenge for quadratic activations and derive a closed-form expression for the
Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,
which combines approximate message passing with rotationally invariant matrix
denoising, and that asymptotically achieves the optimal performance.
Technically, our result is enabled by establishing a link with recent works on
optimal denoising of extensive-rank matrices and on the ellipsoid fitting
problem. We further show empirically that, in the absence of noise,
randomly-initialized gradient descent seems to sample the space of weights,
leading to zero training loss, and averaging over initialization leads to a
test error equal to the Bayes-optimal one.","['Antoine Maillard', 'Emanuele Troiani', 'Simon Martin', 'Florent Krzakala', 'Lenka Zdeborová']","['stat.ML', 'cond-mat.dis-nn', 'cs.IT', 'cs.LG', 'math.IT', 'math.PR']",2024-08-07 12:41:56+00:00
http://arxiv.org/abs/2408.03626v1,On the choice of the non-trainable internal weights in random feature maps,"The computationally cheap machine learning architecture of random feature
maps can be viewed as a single-layer feedforward network in which the weights
of the hidden layer are random but fixed and only the outer weights are learned
via linear regression. The internal weights are typically chosen from a
prescribed distribution. The choice of the internal weights significantly
impacts the accuracy of random feature maps. We address here the task of how to
best select the internal weights. In particular, we consider the forecasting
problem whereby random feature maps are used to learn a one-step propagator map
for a dynamical system. We provide a computationally cheap hit-and-run
algorithm to select good internal weights which lead to good forecasting skill.
We show that the number of good features is the main factor controlling the
forecasting skill of random feature maps and acts as an effective feature
dimension. Lastly, we compare random feature maps with single-layer feedforward
neural networks in which the internal weights are now learned using gradient
descent. We find that random feature maps have superior forecasting
capabilities whilst having several orders of magnitude lower computational
cost.","['Pinak Mandal', 'Georg A. Gottwald']","['cs.LG', 'physics.data-an', 'stat.ME', 'stat.ML']",2024-08-07 08:37:23+00:00
http://arxiv.org/abs/2408.03590v1,Sensitivity analysis using the Metamodel of Optimal Prognosis,"In real case applications within the virtual prototyping process, it is not
always possible to reduce the complexity of the physical models and to obtain
numerical models which can be solved quickly. Usually, every single numerical
simulation takes hours or even days. Although the progresses in numerical
methods and high performance computing, in such cases, it is not possible to
explore various model configurations, hence efficient surrogate models are
required. Generally the available meta-model techniques show several advantages
and disadvantages depending on the investigated problem. In this paper we
present an automatic approach for the selection of the optimal suitable
meta-model for the actual problem. Together with an automatic reduction of the
variable space using advanced filter techniques an efficient approximation is
enabled also for high dimensional problems. This filter techniques enable a
reduction of the high dimensional variable space to a much smaller subspace
where meta-model-based sensitivity analyses are carried out to assess the
influence of important variables and to identify the optimal subspace with
corresponding surrogate model which enables the most accurate probabilistic
analysis. For this purpose we investigate variance-based and moment-free
sensitivity measures in combination with advanced meta-models as moving least
squares and kriging.","['Thomas Most', 'Johannes Will']","['stat.ME', 'cs.LG', 'stat.ML']",2024-08-07 07:09:06+00:00
http://arxiv.org/abs/2408.03569v1,Maximum a Posteriori Estimation for Linear Structural Dynamics Models Using Bayesian Optimization with Rational Polynomial Chaos Expansions,"Bayesian analysis enables combining prior knowledge with measurement data to
learn model parameters. Commonly, one resorts to computing the maximum a
posteriori (MAP) estimate, when only a point estimate of the parameters is of
interest. We apply MAP estimation in the context of structural dynamic models,
where the system response can be described by the frequency response function.
To alleviate high computational demands from repeated expensive model calls, we
utilize a rational polynomial chaos expansion (RPCE) surrogate model that
expresses the system frequency response as a rational of two polynomials with
complex coefficients. We propose an extension to an existing sparse Bayesian
learning approach for RPCE based on Laplace's approximation for the posterior
distribution of the denominator coefficients. Furthermore, we introduce a
Bayesian optimization approach, which allows to adaptively enrich the
experimental design throughout the optimization process of MAP estimation.
Thereby, we utilize the expected improvement acquisition function as a means to
identify sample points in the input space that are possibly associated with
large objective function values. The acquisition function is estimated through
Monte Carlo sampling based on the posterior distribution of the expansion
coefficients identified in the sparse Bayesian learning process. By combining
the sparsity-inducing learning procedure with the sequential experimental
design, we effectively reduce the number of model evaluations in the MAP
estimation problem. We demonstrate the applicability of the presented methods
on the parameter updating problem of an algebraic two-degree-of-freedom system
and the finite element model of a cross-laminated timber plate.","['Felix Schneider', 'Iason Papaioannou', 'Bruno Sudret', 'Gerhard Müller']","['stat.ML', 'cs.LG']",2024-08-07 06:11:37+00:00
http://arxiv.org/abs/2408.03560v2,In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models,"Despite advancements, fine-tuning Large Language Models (LLMs) remains costly
due to the extensive parameter count and substantial data requirements for
model generalization. Accessibility to computing resources remains a barrier
for the open-source community. To address this challenge, we propose the
In2Core algorithm, which selects a coreset by analyzing the correlation between
training and evaluation samples with a trained model. Notably, we assess the
model's internal gradients to estimate this relationship, aiming to rank the
contribution of each training point. To enhance efficiency, we propose an
optimization to compute influence functions with a reduced number of layers
while achieving similar accuracy. By applying our algorithm to instruction
fine-tuning data of LLMs, we can achieve similar performance with just 50% of
the training data. Meantime, using influence functions to analyze model
coverage to certain testing samples could provide a reliable and interpretable
signal on the training set's coverage of those test points.","['Ayrton San Joaquin', 'Bin Wang', 'Zhengyuan Liu', 'Nicholas Asher', 'Brian Lim', 'Philippe Muller', 'Nancy F. Chen']","['cs.LG', 'stat.ML']",2024-08-07 05:48:05+00:00
http://arxiv.org/abs/2408.03534v1,NeurAM: nonlinear dimensionality reduction for uncertainty quantification through neural active manifolds,"We present a new approach for nonlinear dimensionality reduction,
specifically designed for computationally expensive mathematical models. We
leverage autoencoders to discover a one-dimensional neural active manifold
(NeurAM) capturing the model output variability, plus a simultaneously learnt
surrogate model with inputs on this manifold. The proposed dimensionality
reduction framework can then be applied to perform outer loop many-query tasks,
like sensitivity analysis and uncertainty propagation. In particular, we prove,
both theoretically under idealized conditions, and numerically in challenging
test cases, how NeurAM can be used to obtain multifidelity sampling estimators
with reduced variance by sampling the models on the discovered low-dimensional
and shared manifold among models. Several numerical examples illustrate the
main features of the proposed dimensionality reduction strategy, and highlight
its advantages with respect to existing approaches in the literature.","['Andrea Zanoni', 'Gianluca Geraci', 'Matteo Salvador', 'Alison L. Marsden', 'Daniele E. Schiavazzi']","['math.NA', 'cs.NA', 'stat.ML']",2024-08-07 04:27:58+00:00
http://arxiv.org/abs/2408.03461v1,When does the mean network capture the topology of a sample of networks?,"The notion of Fr\'echet mean (also known as ""barycenter"") network is the
workhorse of most machine learning algorithms that require the estimation of a
""location"" parameter to analyse network-valued data. In this context, it is
critical that the network barycenter inherits the topological structure of the
networks in the training dataset. The metric - which measures the proximity
between networks - controls the structural properties of the barycenter. This
work is significant because it provides for the first time analytical estimates
of the sample Fr\'echet mean for the stochastic blockmodel, which is at the
cutting edge of rigorous probabilistic analysis of random networks. We show
that the mean network computed with the Hamming distance is unable to capture
the topology of the networks in the training sample, whereas the mean network
computed using the effective resistance distance recovers the correct
partitions and associated edge density. From a practical standpoint, our work
informs the choice of metrics in the context where the sample Fr\'echet mean
network is used to characterise the topology of networks for network-valued
machine learning",['François G Meyer'],"['stat.ML', 'cs.LG', 'cs.SI', 'physics.data-an']",2024-08-06 22:14:54+00:00
http://arxiv.org/abs/2408.03421v1,"Probabilistic Scores of Classifiers, Calibration is not Enough","In binary classification tasks, accurate representation of probabilistic
predictions is essential for various real-world applications such as predicting
payment defaults or assessing medical risks. The model must then be
well-calibrated to ensure alignment between predicted probabilities and actual
outcomes. However, when score heterogeneity deviates from the underlying data
probability distribution, traditional calibration metrics lose reliability,
failing to align score distribution with actual probabilities. In this study,
we highlight approaches that prioritize optimizing the alignment between
predicted scores and true probability distributions over minimizing traditional
performance or calibration metrics. When employing tree-based models such as
Random Forest and XGBoost, our analysis emphasizes the flexibility these models
offer in tuning hyperparameters to minimize the Kullback-Leibler (KL)
divergence between predicted and true distributions. Through extensive
empirical analysis across 10 UCI datasets and simulations, we demonstrate that
optimizing tree-based models based on KL divergence yields superior alignment
between predicted scores and actual probabilities without significant
performance loss. In real-world scenarios, the reference probability is
determined a priori as a Beta distribution estimated through maximum
likelihood. Conversely, minimizing traditional calibration metrics may lead to
suboptimal results, characterized by notable performance declines and inferior
KL values. Our findings reveal limitations in traditional calibration metrics,
which could undermine the reliability of predictive models for critical
decision-making.","['Agathe Fernandes Machado', 'Arthur Charpentier', 'Emmanuel Flachaire', 'Ewen Gallic', 'François Hu']","['cs.LG', 'stat.ML']",2024-08-06 19:53:00+00:00
http://arxiv.org/abs/2408.03414v2,"Logistic Regression makes small LLMs strong and explainable ""tens-of-shot"" classifiers","For simple classification tasks, we show that users can benefit from the
advantages of using small, local, generative language models instead of large
commercial models without a trade-off in performance or introducing extra
labelling costs. These advantages, including those around privacy,
availability, cost, and explainability, are important both in commercial
applications and in the broader democratisation of AI. Through experiments on
17 sentence classification tasks (2-4 classes), we show that penalised logistic
regression on the embeddings from a small LLM equals (and usually betters) the
performance of a large LLM in the ""tens-of-shot"" regime. This requires no more
labelled instances than are needed to validate the performance of the large
LLM. Finally, we extract stable and sensible explanations for classification
decisions.","['Marcus Buckmann', 'Edward Hill']","['cs.CL', 'cs.LG', 'stat.ML', '68T50 (Primary), 62J07 (Secondary)', 'I.2.7']",2024-08-06 19:23:42+00:00
http://arxiv.org/abs/2408.03307v2,Exchangeable Sequence Models Can Naturally Quantify Uncertainty Over Latent Concepts,"Intelligent agents must be able to articulate its own uncertainty. In this
work, we show that pre-trained sequence models are naturally capable of
probabilistic reasoning over exchangeable data points -- forming informed
beliefs and sharpening them as it gathers more information. A sequence model
learns the relationship between observations, which differs from typical
Bayesian models that quantify uncertainty over latent parameters through priors
and likelihoods (e.g., topic models). Despite the apparent difference, we
illustrate how exchangeable sequence modeling provides a valid Bayesian model
by going back to De Finetti's classical predictive view of probabilistic
reasoning: uncertainty comes from data that has not been observed yet, rather
than latent parameters. From this perspective, pre-training autoregressive
models is equivalent to formulating informed beliefs based on prior
observations (""empirical Bayes""), and forward generation is equivalent to
simulating instantiations of an environment (""posterior inference""). In
particular, exchangeable sequence models can explicitly perform statistical
inference; epistemic uncertainty over latent environments is captured by
variation in predicted future observations. Formally, we show the sequence
prediction loss controls the quality of uncertainty quantification, and propose
several approaches for encoding exchangeability in sequence model
architectures: data augmentation, regularization, and causal masking.","['Naimeng Ye', 'Hanming Yang', 'Andrew Siah', 'Hongseok Namkoong']","['stat.ML', 'cs.LG']",2024-08-06 17:16:10+00:00
http://arxiv.org/abs/2408.03144v1,Active Learning for Level Set Estimation Using Randomized Straddle Algorithms,"Level set estimation (LSE), the problem of identifying the set of input
points where a function takes value above (or below) a given threshold, is
important in practical applications. When the function is expensive-to-evaluate
and black-box, the \textit{straddle} algorithm, which is a representative
heuristic for LSE based on Gaussian process models, and its extensions having
theoretical guarantees have been developed. However, many of existing methods
include a confidence parameter $\beta^{1/2}_t$ that must be specified by the
user, and methods that choose $\beta^{1/2}_t$ heuristically do not provide
theoretical guarantees. In contrast, theoretically guaranteed values of
$\beta^{1/2}_t$ need to be increased depending on the number of iterations and
candidate points, and are conservative and not good for practical performance.
In this study, we propose a novel method, the \textit{randomized straddle}
algorithm, in which $\beta_t$ in the straddle algorithm is replaced by a random
sample from the chi-squared distribution with two degrees of freedom. The
confidence parameter in the proposed method has the advantages of not needing
adjustment, not depending on the number of iterations and candidate points, and
not being conservative. Furthermore, we show that the proposed method has
theoretical guarantees that depend on the sample complexity and the number of
iterations. Finally, we confirm the usefulness of the proposed method through
numerical experiments using synthetic and real data.","['Yu Inatsu', 'Shion Takeno', 'Kentaro Kutsukake', 'Ichiro Takeuchi']","['stat.ML', 'cs.LG']",2024-08-06 12:39:12+00:00
http://arxiv.org/abs/2408.03138v1,Predictive Performance Test based on the Exhaustive Nested Cross-Validation for High-dimensional data,"It is crucial to assess the predictive performance of a model in order to
establish its practicality and relevance in real-world scenarios, particularly
for high-dimensional data analysis. Among data splitting or resampling methods,
cross-validation (CV) is extensively used for several tasks such as estimating
the prediction error, tuning the regularization parameter, and selecting the
most suitable predictive model among competing alternatives. The K-fold
cross-validation is a popular CV method but its limitation is that the risk
estimates are highly dependent on the partitioning of the data (for training
and testing). Here, the issues regarding the reproducibility of the K-fold CV
estimator is demonstrated in hypothesis testing wherein different partitions
lead to notably disparate conclusions. This study presents an alternative novel
predictive performance test and valid confidence intervals based on exhaustive
nested cross-validation for determining the difference in prediction error
between two model-fitting algorithms. A naive implementation of the exhaustive
nested cross-validation is computationally costly. Here, we address concerns
regarding computational complexity by devising a computationally tractable
closed-form expression for the proposed cross-validation estimator using ridge
regularization. Our study also investigates strategies aimed at enhancing
statistical power within high-dimensional scenarios while controlling the Type
I error rate. To illustrate the practical utility of our method, we apply it to
an RNA sequencing study and demonstrate its effectiveness in the context of
biological data analysis.","['Iris Ivy Gauran', 'Hernando Ombao', 'Zhaoxia Yu']","['stat.ME', 'stat.ML']",2024-08-06 12:28:16+00:00
http://arxiv.org/abs/2408.02841v1,"Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration","Most machine learning classifiers are designed to output posterior
probabilities for the classes given the input sample. These probabilities may
be used to make the categorical decision on the class of the sample; provided
as input to a downstream system; or provided to a human for interpretation.
Evaluating the quality of the posteriors generated by these system is an
essential problem which was addressed decades ago with the invention of proper
scoring rules (PSRs). Unfortunately, much of the recent machine learning
literature uses calibration metrics -- most commonly, the expected calibration
error (ECE) -- as a proxy to assess posterior performance. The problem with
this approach is that calibration metrics reflect only one aspect of the
quality of the posteriors, ignoring the discrimination performance. For this
reason, we argue that calibration metrics should play no role in the assessment
of posterior quality. Expected PSRs should instead be used for this job,
preferably normalized for ease of interpretation. In this work, we first give a
brief review of PSRs from a practical perspective, motivating their definition
using Bayes decision theory. We discuss why expected PSRs provide a principled
measure of the quality of a system's posteriors and why calibration metrics are
not the right tool for this job. We argue that calibration metrics, while not
useful for performance assessment, may be used as diagnostic tools during
system development. With this purpose in mind, we discuss a simple and
practical calibration metric, called calibration loss, derived from a
decomposition of expected PSRs. We compare this metric with the ECE and with
the expected score divergence calibration metric from the PSR literature and
argue, using theoretical and empirical evidence, that calibration loss is
superior to these two metrics.","['Luciana Ferrer', 'Daniel Ramos']","['stat.ML', 'cs.LG']",2024-08-05 21:35:51+00:00
http://arxiv.org/abs/2408.02839v1,Optimizing Cox Models with Stochastic Gradient Descent: Theoretical Foundations and Practical Guidances,"Optimizing Cox regression and its neural network variants poses substantial
computational challenges in large-scale studies. Stochastic gradient descent
(SGD), known for its scalability in model optimization, has recently been
adapted to optimize Cox models. Unlike its conventional application, which
typically targets a sum of independent individual loss, SGD for Cox models
updates parameters based on the partial likelihood of a subset of data. Despite
its empirical success, the theoretical foundation for optimizing Cox partial
likelihood with SGD is largely underexplored. In this work, we demonstrate that
the SGD estimator targets an objective function that is batch-size-dependent.
We establish that the SGD estimator for the Cox neural network (Cox-NN) is
consistent and achieves the optimal minimax convergence rate up to a
polylogarithmic factor. For Cox regression, we further prove the
$\sqrt{n}$-consistency and asymptotic normality of the SGD estimator, with
variance depending on the batch size. Furthermore, we quantify the impact of
batch size on Cox-NN training and its effect on the SGD estimator's asymptotic
efficiency in Cox regression. These findings are validated by extensive
numerical experiments and provide guidance for selecting batch sizes in SGD
applications. Finally, we demonstrate the effectiveness of SGD in a real-world
application where GD is unfeasible due to the large scale of data.","['Lang Zeng', 'Weijing Tang', 'Zhao Ren', 'Ying Ding']","['stat.ML', 'cs.LG']",2024-08-05 21:25:10+00:00
http://arxiv.org/abs/2408.02838v1,Interpretation of the Intent Detection Problem as Dynamics in a Low-dimensional Space,"Intent detection is a text classification task whose aim is to recognize and
label the semantics behind a users query. It plays a critical role in various
business applications. The output of the intent detection module strongly
conditions the behavior of the whole system. This sequence analysis task is
mainly tackled using deep learning techniques. Despite the widespread use of
these techniques, the internal mechanisms used by networks to solve the problem
are poorly understood. Recent lines of work have analyzed the computational
mechanisms learned by RNNs from a dynamical systems perspective. In this work,
we investigate how different RNN architectures solve the SNIPS intent detection
problem. Sentences injected into trained networks can be interpreted as
trajectories traversing a hidden state space. This space is constrained to a
low-dimensional manifold whose dimensionality is related to the embedding and
hidden layer sizes. To generate predictions, RNN steers the trajectories
towards concrete regions, spatially aligned with the output layer matrix rows
directions. Underlying the system dynamics, an unexpected fixed point topology
has been identified with a limited number of attractors. Our results provide
new insights into the inner workings of networks that solve the intent
detection task.","['Eduardo Sanchez-Karhunen', 'Jose F. Quesada-Moreno', 'Miguel A. Gutiérrez-Naranjo']","['cs.LG', 'cs.CL', 'stat.ML']",2024-08-05 21:22:36+00:00
http://arxiv.org/abs/2408.02801v1,Sparse Deep Learning Models with the $\ell_1$ Regularization,"Sparse neural networks are highly desirable in deep learning in reducing its
complexity. The goal of this paper is to study how choices of regularization
parameters influence the sparsity level of learned neural networks. We first
derive the $\ell_1$-norm sparsity-promoting deep learning models including
single and multiple regularization parameters models, from a statistical
viewpoint. We then characterize the sparsity level of a regularized neural
network in terms of the choice of the regularization parameters. Based on the
characterizations, we develop iterative algorithms for selecting regularization
parameters so that the weight parameters of the resulting deep neural network
enjoy prescribed sparsity levels. Numerical experiments are presented to
demonstrate the effectiveness of the proposed algorithms in choosing desirable
regularization parameters and obtaining corresponding neural networks having
both of predetermined sparsity levels and satisfactory approximation accuracy.","['Lixin Shen', 'Rui Wang', 'Yuesheng Xu', 'Mingsong Yan']","['cs.LG', 'math.OC', 'stat.ML']",2024-08-05 19:38:45+00:00
http://arxiv.org/abs/2408.02760v1,Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An Improved ROCKET Algorithm for Multivariate Time Series Analysis,"Multivariate Time Series Classification (MTSC) is a ubiquitous problem in
science and engineering, particularly in neuroscience, where most data
acquisition modalities involve the simultaneous time-dependent recording of
brain activity in multiple brain regions. In recent years, Random Convolutional
Kernel models such as ROCKET and MiniRocket have emerged as highly effective
time series classification algorithms, capable of achieving state-of-the-art
accuracy results with low computational load. Despite their success, these
types of models face two major challenges when employed in neuroscience: 1)
they struggle to deal with high-dimensional data such as EEG and MEG, and 2)
they are difficult to interpret. In this work, we present a novel ROCKET-based
algorithm, named Detach-Rocket Ensemble, that is specifically designed to
address these two problems in MTSC. Our algorithm leverages pruning to provide
an integrated estimation of channel importance, and ensembles to achieve better
accuracy and provide a label probability. Using a synthetic multivariate time
series classification dataset in which we control the amount of information
carried by each of the channels, we first show that our algorithm is able to
correctly recover the channel importance for classification. Then, using two
real-world datasets, a MEG dataset and an EEG dataset, we show that
Detach-Rocket Ensemble is able to provide both interpretable channel relevance
and competitive classification accuracy, even when applied directly to the raw
brain data, without the need for feature engineering.","['Adrià Solana', 'Erik Fransén', 'Gonzalo Uribarri']","['cs.LG', 'cs.CE', 'q-bio.NC', 'stat.ML']",2024-08-05 18:24:09+00:00
http://arxiv.org/abs/2408.02558v4,Peer-induced Fairness: A Causal Approach for Algorithmic Fairness Auditing,"With the European Union's Artificial Intelligence Act taking effect on 1
August 2024, high-risk AI applications must adhere to stringent transparency
and fairness standards. This paper addresses a crucial question: how can we
scientifically audit algorithmic fairness? Current methods typically remain at
the basic detection stage of auditing, without accounting for more complex
scenarios. We propose a novel framework, ``peer-induced fairness'', which
combines the strengths of counterfactual fairness and peer comparison strategy,
creating a reliable and robust tool for auditing algorithmic fairness. Our
framework is universal, adaptable to various domains, and capable of handling
different levels of data quality, including skewed distributions. Moreover, it
can distinguish whether adverse decisions result from algorithmic
discrimination or inherent limitations of the subjects, thereby enhancing
transparency. This framework can serve as both a self-assessment tool for AI
developers and an external assessment tool for auditors to ensure compliance
with the EU AI Act. We demonstrate its utility in small and medium-sized
enterprises access to finance, uncovering significant unfairness-41.51% of
micro-firms face discrimination compared to non-micro firms. These findings
highlight the framework's potential for broader applications in ensuring
equitable AI-driven decision-making.","['Shiqi Fang', 'Zexun Chen', 'Jake Ansell']","['stat.AP', 'cs.CY', 'q-fin.CP', 'q-fin.ST', 'stat.ML']",2024-08-05 15:35:34+00:00
http://arxiv.org/abs/2408.02489v1,Full error analysis of policy gradient learning algorithms for exploratory linear quadratic mean-field control problem in continuous time with common noise,"We consider reinforcement learning (RL) methods for finding optimal policies
in linear quadratic (LQ) mean field control (MFC) problems over an infinite
horizon in continuous time, with common noise and entropy regularization. We
study policy gradient (PG) learning and first demonstrate convergence in a
model-based setting by establishing a suitable gradient domination
condition.Next, our main contribution is a comprehensive error analysis, where
we prove the global linear convergence and sample complexity of the PG
algorithm with two-point gradient estimates in a model-free setting with
unknown parameters. In this setting, the parameterized optimal policies are
learned from samples of the states and population distribution.Finally, we
provide numerical evidence supporting the convergence of our implemented
algorithms.","['Noufel Frikha', 'Huyên Pham', 'Xuanye Song']","['math.OC', 'stat.ML']",2024-08-05 14:11:51+00:00
http://arxiv.org/abs/2408.02481v1,On the influence of dependent features in classification problems: a game-theoretic perspective,"This paper deals with a new measure of the influence of each feature on the
response variable in classification problems, accounting for potential
dependencies among certain feature subsets. Within this framework, we consider
a sample of individuals characterized by specific features, each feature
encompassing a finite range of values, and classified based on a binary
response variable. This measure turns out to be an influence measure explored
in existing literature and related to cooperative game theory. We provide an
axiomatic characterization of our proposed influence measure by tailoring
properties from the cooperative game theory to our specific context.
Furthermore, we demonstrate that our influence measure becomes a general
characterization of the well-known Banzhaf-Owen value for games with a priori
unions, from the perspective of classification problems. The definitions and
results presented herein are illustrated through numerical examples and various
applications, offering practical insights into our methodologies.","['Laura Davila-Pena', 'Alejandro Saavedra-Nieves', 'Balbina Casas-Méndez']","['math.OC', 'stat.ML']",2024-08-05 14:02:26+00:00
http://arxiv.org/abs/2408.02433v1,On Probabilistic Embeddings in Optimal Dimension Reduction,"Dimension reduction algorithms are a crucial part of many data science
pipelines, including data exploration, feature creation and selection, and
denoising. Despite their wide utilization, many non-linear dimension reduction
algorithms are poorly understood from a theoretical perspective. In this work
we consider a generalized version of multidimensional scaling, which is posed
as an optimization problem in which a mapping from a high-dimensional feature
space to a lower-dimensional embedding space seeks to preserve either inner
products or norms of the distribution in feature space, and which encompasses
many commonly used dimension reduction algorithms. We analytically investigate
the variational properties of this problem, leading to the following insights:
1) Solutions found using standard particle descent methods may lead to
non-deterministic embeddings, 2) A relaxed or probabilistic formulation of the
problem admits solutions with easily interpretable necessary conditions, 3) The
globally optimal solutions to the relaxed problem actually must give a
deterministic embedding. This progression of results mirrors the classical
development of optimal transportation, and in a case relating to the
Gromov-Wasserstein distance actually gives explicit insight into the structure
of the optimal embeddings, which are parametrically determined and
discontinuous. Finally, we illustrate that a standard computational
implementation of this task does not learn deterministic embeddings, which
means that it learns sub-optimal mappings, and that the embeddings learned in
that context have highly misleading clustering structure, underscoring the
delicate nature of solving this problem computationally.","['Ryan Murray', 'Adam Pickarski']","['stat.ML', 'cs.LG', 'math.AP']",2024-08-05 12:46:21+00:00
http://arxiv.org/abs/2408.02393v1,Graphical Modelling without Independence Assumptions for Uncentered Data,"The independence assumption is a useful tool to increase the tractability of
one's modelling framework. However, this assumption does not match reality;
failing to take dependencies into account can cause models to fail
dramatically. The field of multi-axis graphical modelling (also called
multi-way modelling, Kronecker-separable modelling) has seen growth over the
past decade, but these models require that the data have zero mean. In the
multi-axis case, inference is typically done in the single sample scenario,
making mean inference impossible.
  In this paper, we demonstrate how the zero-mean assumption can cause
egregious modelling errors, as well as propose a relaxation to the zero-mean
assumption that allows the avoidance of such errors. Specifically, we propose
the ""Kronecker-sum-structured mean"" assumption, which leads to models with
nonconvex-but-unimodal log-likelihoods that can be solved efficiently with
coordinate descent.","['Bailey Andrew', 'David R. Westhead', 'Luisa Cutillo']","['stat.ME', 'stat.ML']",2024-08-05 11:40:23+00:00
http://arxiv.org/abs/2408.02355v1,Quantile Regression using Random Forest Proximities,"Due to the dynamic nature of financial markets, maintaining models that
produce precise predictions over time is difficult. Often the goal isn't just
point prediction but determining uncertainty. Quantifying uncertainty,
especially the aleatoric uncertainty due to the unpredictable nature of market
drivers, helps investors understand varying risk levels. Recently, quantile
regression forests (QRF) have emerged as a promising solution: Unlike most
basic quantile regression methods that need separate models for each quantile,
quantile regression forests estimate the entire conditional distribution of the
target variable with a single model, while retaining all the salient features
of a typical random forest. We introduce a novel approach to compute quantile
regressions from random forests that leverages the proximity (i.e., distance
metric) learned by the model and infers the conditional distribution of the
target variable. We evaluate the proposed methodology using publicly available
datasets and then apply it towards the problem of forecasting the average daily
volume of corporate bonds. We show that using quantile regression using Random
Forest proximities demonstrates superior performance in approximating
conditional target distributions and prediction intervals to the original
version of QRF. We also demonstrate that the proposed framework is
significantly more computationally efficient than traditional approaches to
quantile regressions.","['Mingshu Li', 'Bhaskarjit Sarmah', 'Dhruv Desai', 'Joshua Rosaler', 'Snigdha Bhagat', 'Philip Sommer', 'Dhagash Mehta']","['stat.ML', 'cs.LG', 'q-fin.ST', 'q-fin.TR']",2024-08-05 10:02:33+00:00
http://arxiv.org/abs/2408.02346v1,Exploiting Hankel-Toeplitz Structures for Fast Computation of Kernel Precision Matrices,"The Hilbert-space Gaussian Process (HGP) approach offers a
hyperparameter-independent basis function approximation for speeding up
Gaussian Process (GP) inference by projecting the GP onto M basis functions.
These properties result in a favorable data-independent $\mathcal{O}(M^3)$
computational complexity during hyperparameter optimization but require a
dominating one-time precomputation of the precision matrix costing
$\mathcal{O}(NM^2)$ operations. In this paper, we lower this dominating
computational complexity to $\mathcal{O}(NM)$ with no additional
approximations. We can do this because we realize that the precision matrix can
be split into a sum of Hankel-Toeplitz matrices, each having $\mathcal{O}(M)$
unique entries. Based on this realization we propose computing only these
unique entries at $\mathcal{O}(NM)$ costs. Further, we develop two theorems
that prescribe sufficient conditions for the complexity reduction to hold
generally for a wide range of other approximate GP models, such as the
Variational Fourier Feature (VFF) approach. The two theorems do this with no
assumptions on the data and no additional approximations of the GP models
themselves. Thus, our contribution provides a pure speed-up of several
existing, widely used, GP approximations, without further approximations.","['Frida Viset', 'Anton Kullberg', 'Frederiek Wesel', 'Arno Solin']","['cs.LG', 'stat.ML']",2024-08-05 09:45:31+00:00
http://arxiv.org/abs/2408.02326v1,Explosive neural networks via higher-order interactions in curved statistical manifolds,"Higher-order interactions underlie complex phenomena in systems such as
biological and artificial neural networks, but their study is challenging due
to the lack of tractable standard models. By leveraging the maximum entropy
principle in curved statistical manifolds, here we introduce curved neural
networks as a class of prototypical models for studying higher-order phenomena.
Through exact mean-field descriptions, we show that these curved neural
networks implement a self-regulating annealing process that can accelerate
memory retrieval, leading to explosive order-disorder phase transitions with
multi-stability and hysteresis effects. Moreover, by analytically exploring
their memory capacity using the replica trick near ferromagnetic and spin-glass
phase boundaries, we demonstrate that these networks enhance memory capacity
over the classical associative-memory networks. Overall, the proposed framework
provides parsimonious models amenable to analytical study, revealing novel
higher-order phenomena in complex network systems.","['Miguel Aguilera', 'Pablo A. Morales', 'Fernando E. Rosas', 'Hideaki Shimazaki']","['cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.IT', 'math.IT', 'nlin.AO', 'stat.ML']",2024-08-05 09:10:29+00:00
http://arxiv.org/abs/2408.02320v1,A Sharp Convergence Theory for The Probability Flow ODEs of Diffusion Models,"Diffusion models, which convert noise into new data instances by learning to
reverse a diffusion process, have become a cornerstone in contemporary
generative modeling. In this work, we develop non-asymptotic convergence theory
for a popular diffusion-based sampler (i.e., the probability flow ODE sampler)
in discrete time, assuming access to $\ell_2$-accurate estimates of the (Stein)
score functions. For distributions in $\mathbb{R}^d$, we prove that
$d/\varepsilon$ iterations -- modulo some logarithmic and lower-order terms --
are sufficient to approximate the target distribution to within $\varepsilon$
total-variation distance. This is the first result establishing nearly linear
dimension-dependency (in $d$) for the probability flow ODE sampler. Imposing
only minimal assumptions on the target data distribution (e.g., no smoothness
assumption is imposed), our results also characterize how $\ell_2$ score
estimation errors affect the quality of the data generation processes. In
contrast to prior works, our theory is developed based on an elementary yet
versatile non-asymptotic approach without the need of resorting to SDE and ODE
toolboxes.","['Gen Li', 'Yuting Wei', 'Yuejie Chi', 'Yuxin Chen']","['cs.LG', 'cs.NA', 'eess.SP', 'math.NA', 'math.ST', 'stat.ML', 'stat.TH']",2024-08-05 09:02:24+00:00
http://arxiv.org/abs/2408.02295v2,Generalized Gaussian Temporal Difference Error for Uncertainty-aware Reinforcement Learning,"Conventional uncertainty-aware temporal difference (TD) learning methods
often rely on simplistic assumptions, typically including a zero-mean Gaussian
distribution for TD errors. Such oversimplification can lead to inaccurate
error representations and compromised uncertainty estimation. In this paper, we
introduce a novel framework for generalized Gaussian error modeling in deep
reinforcement learning, applicable to both discrete and continuous control
settings. Our framework enhances the flexibility of error distribution modeling
by incorporating additional higher-order moment, particularly kurtosis, thereby
improving the estimation and mitigation of data-dependent noise, i.e.,
aleatoric uncertainty. We examine the influence of the shape parameter of the
generalized Gaussian distribution (GGD) on aleatoric uncertainty and provide a
closed-form expression that demonstrates an inverse relationship between
uncertainty and the shape parameter. Additionally, we propose a theoretically
grounded weighting scheme to fully leverage the GGD. To address epistemic
uncertainty, we enhance the batch inverse variance weighting by incorporating
bias reduction and kurtosis considerations, resulting in improved robustness.
Extensive experimental evaluations using policy gradient algorithms demonstrate
the consistent efficacy of our method, showcasing significant performance
improvements.","['Seyeon Kim', 'Joonhun Lee', 'Namhoon Cho', 'Sungjun Han', 'Wooseop Hwang']","['cs.LG', 'cs.AI', 'math.PR', 'stat.ML']",2024-08-05 08:12:25+00:00
http://arxiv.org/abs/2408.02279v1,DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting,"Long-term time series forecasting (LTSF) has been widely applied in finance,
traffic prediction, and other domains. Recently, patch-based transformers have
emerged as a promising approach, segmenting data into sub-level patches that
serve as input tokens. However, existing methods mostly rely on predetermined
patch lengths, necessitating expert knowledge and posing challenges in
capturing diverse characteristics across various scales. Moreover, time series
data exhibit diverse variations and fluctuations across different temporal
scales, which traditional approaches struggle to model effectively. In this
paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm
to capture diverse receptive fields and sparse patterns of time series data. In
order to build hierarchical receptive fields, we develop a multi-scale
Transformer model, coupled with multi-scale sequence extraction, capable of
capturing multi-resolution features. Additionally, we introduce a group-aware
rotary position encoding technique to enhance intra- and inter-group position
awareness among representations across different temporal scales. Our proposed
model, named DRFormer, is evaluated on various real-world datasets, and
experimental results demonstrate its superiority compared to existing methods.
Our code is available at: https://github.com/ruixindingECNU/DRFormer.","['Ruixin Ding', 'Yuqi Chen', 'Yu-Ting Lan', 'Wei Zhang']","['cs.LG', 'cs.AI', 'stat.ML', 'I.2.6']",2024-08-05 07:26:47+00:00
http://arxiv.org/abs/2408.02167v1,Embedding generalization within the learning dynamics: An approach based-on sample path large deviation theory,"We consider a typical learning problem of point estimations for modeling of
nonlinear functions or dynamical systems in which generalization, i.e.,
verifying a given learned model, can be embedded as an integral part of the
learning process or dynamics. In particular, we consider an empirical risk
minimization based learning problem that exploits gradient methods from
continuous-time perspective with small random perturbations, which is guided by
the training dataset loss. Here, we provide an asymptotic probability estimate
in the small noise limit based-on the Freidlin-Wentzell theory of large
deviations, when the sample path of the random process corresponding to the
randomly perturbed gradient dynamical system hits a certain target set, i.e., a
rare event, when the latter is specified by the testing dataset loss landscape.
Interestingly, the proposed framework can be viewed as one way of improving
generalization and robustness in learning problems that provides new insights
leading to optimal point estimates which is guided by training data loss,
while, at the same time, the learning dynamics has an access to the testing
dataset loss landscape in some form of future achievable or anticipated target
goal. Moreover, as a by-product, we establish a connection with optimal control
problem, where the target set, i.e., the rare event, is considered as the
desired outcome or achievable target goal for a certain optimal control
problem, for which we also provide a verification result reinforcing the
rationale behind the proposed framework. Finally, we present a computational
algorithm that solves the corresponding variational problem leading to an
optimal point estimates and, as part of this work, we also present some
numerical results for a typical case of nonlinear regression problem.",['Getachew K. Befekadu'],"['math.OC', 'stat.ML']",2024-08-04 23:31:35+00:00
http://arxiv.org/abs/2408.02159v1,SPINEX-TimeSeries: Similarity-based Predictions with Explainable Neighbors Exploration for Time Series and Forecasting Problems,"This paper introduces a new addition to the SPINEX (Similarity-based
Predictions with Explainable Neighbors Exploration) family, tailored
specifically for time series and forecasting analysis. This new algorithm
leverages the concept of similarity and higher-order temporal interactions
across multiple time scales to enhance predictive accuracy and interpretability
in forecasting. To evaluate the effectiveness of SPINEX, we present
comprehensive benchmarking experiments comparing it against 18 algorithms and
across 49 synthetic and real datasets characterized by varying trends,
seasonality, and noise levels. Our performance assessment focused on
forecasting accuracy and computational efficiency. Our findings reveal that
SPINEX consistently ranks among the top 5 performers in forecasting precision
and has a superior ability to handle complex temporal dynamics compared to
commonly adopted algorithms. Moreover, the algorithm's explainability features,
Pareto efficiency, and medium complexity (on the order of O(log n)) are
demonstrated through detailed visualizations to enhance the prediction and
decision-making process. We note that integrating similarity-based concepts
opens new avenues for research in predictive analytics, promising more accurate
and transparent decision making.","['Ahmed Z Naser', 'MZ Naser']","['stat.ME', 'cs.LG', 'stat.CO', 'stat.ML']",2024-08-04 22:26:34+00:00
http://arxiv.org/abs/2408.02111v2,Understanding Deep Learning via Notions of Rank,"Despite the extreme popularity of deep learning in science and industry, its
formal understanding is limited. This thesis puts forth notions of rank as key
for developing a theory of deep learning, focusing on the fundamental aspects
of generalization and expressiveness. In particular, we establish that
gradient-based training can induce an implicit regularization towards low rank
for several neural network architectures, and demonstrate empirically that this
phenomenon may facilitate an explanation of generalization over natural data
(e.g., audio, images, and text). Then, we characterize the ability of graph
neural networks to model interactions via a notion of rank, which is commonly
used for quantifying entanglement in quantum physics. A central tool underlying
these results is a connection between neural networks and tensor
factorizations. Practical implications of our theory for designing explicit
regularization schemes and data preprocessing algorithms are presented.",['Noam Razin'],"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",2024-08-04 18:47:55+00:00
http://arxiv.org/abs/2408.02701v1,Randomized Transport Plans via Hierarchical Fully Probabilistic Design,"An optimal randomized strategy for design of balanced, normalized mass
transport plans is developed. It replaces -- but specializes to -- the
deterministic, regularized optimal transport (OT) strategy, which yields only a
certainty-equivalent plan. The incompletely specified -- and therefore
uncertain -- transport plan is acknowledged to be a random process. Therefore,
hierarchical fully probabilistic design (HFPD) is adopted, yielding an optimal
hyperprior supported on the set of possible transport plans, and consistent
with prior mean constraints on the marginals of the uncertain plan. This
Bayesian resetting of the design problem for transport plans -- which we call
HFPD-OT -- confers new opportunities. These include (i) a strategy for the
generation of a random sample of joint transport plans; (ii) randomized
marginal contracts for individual source-target pairs; and (iii) consistent
measures of uncertainty in the plan and its contracts. An application in
algorithmic fairness is outlined, where HFPD-OT enables the recruitment of a
more diverse subset of contracts -- than is possible in classical OT -- into
the delivery of an expected plan. Also, it permits fairness proxies to be
endowed with uncertainty quantifiers.","['Sarah Boufelja Y.', 'Anthony Quinn', 'Robert Shorten']","['eess.SY', 'cs.SY', 'math.OC', 'stat.ML']",2024-08-04 18:24:02+00:00
http://arxiv.org/abs/2408.02065v1,A Multi-class Ride-hailing Service Subsidy System Utilizing Deep Causal Networks,"In the ride-hailing industry, subsidies are predominantly employed to
incentivize consumers to place more orders, thereby fostering market growth.
Causal inference techniques are employed to estimate the consumer elasticity
with different subsidy levels. However, the presence of confounding effects
poses challenges in achieving an unbiased estimate of the uplift effect. We
introduce a consumer subsidizing system to capture relationships between
subsidy propensity and the treatment effect, which proves effective while
maintaining a lightweight online environment.","['Zhe Yu', 'Chi Xia', 'Shaosheng Cao', 'Lin Zhou']","['cs.LG', 'stat.ML']",2024-08-04 15:30:15+00:00
http://arxiv.org/abs/2408.02060v1,Winners with Confidence: Discrete Argmin Inference with an Application to Model Selection,"We study the problem of finding the index of the minimum value of a vector
from noisy observations. This problem is relevant in population/policy
comparison, discrete maximum likelihood, and model selection. We develop a test
statistic that is asymptotically normal, even in high-dimensional settings and
with potentially many ties in the population mean vector, by integrating
concepts and tools from cross-validation and differential privacy. The key
technical ingredient is a central limit theorem for globally dependent data. We
also propose practical ways to select the tuning parameter that adapts to the
signal landscape.","['Tianyu Zhang', 'Hao Lee', 'Jing Lei']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2024-08-04 15:20:23+00:00
http://arxiv.org/abs/2408.02045v1,DNA-SE: Towards Deep Neural-Nets Assisted Semiparametric Estimation,"Semiparametric statistics play a pivotal role in a wide range of domains,
including but not limited to missing data, causal inference, and transfer
learning, to name a few. In many settings, semiparametric theory leads to
(nearly) statistically optimal procedures that yet involve numerically solving
Fredholm integral equations of the second kind. Traditional numerical methods,
such as polynomial or spline approximations, are difficult to scale to
multi-dimensional problems. Alternatively, statisticians may choose to
approximate the original integral equations by ones with closed-form solutions,
resulting in computationally more efficient, but statistically suboptimal or
even incorrect procedures. To bridge this gap, we propose a novel framework by
formulating the semiparametric estimation problem as a bi-level optimization
problem; and then we develop a scalable algorithm called Deep Neural-Nets
Assisted Semiparametric Estimation (DNA-SE) by leveraging the universal
approximation property of Deep Neural-Nets (DNN) to streamline semiparametric
procedures. Through extensive numerical experiments and a real data analysis,
we demonstrate the numerical and statistical advantages of $\dnase$ over
traditional methods. To the best of our knowledge, we are the first to bring
DNN into semiparametric statistics as a numerical solver of integral equations
in our proposed general framework.","['Qinshuo Liu', 'Zixin Wang', 'Xi-An Li', 'Xinyao Ji', 'Lei Zhang', 'Lin Liu', 'Zhonghua Liu']","['stat.ML', 'cs.LG']",2024-08-04 14:45:26+00:00
http://arxiv.org/abs/2408.01981v2,Multiview learning with twin parametric margin SVM,"Multiview learning (MVL) seeks to leverage the benefits of diverse
perspectives to complement each other, effectively extracting and utilizing the
latent information within the dataset. Several twin support vector
machine-based MVL (MvTSVM) models have been introduced and demonstrated
outstanding performance in various learning tasks. However, MvTSVM-based models
face significant challenges in the form of computational complexity due to four
matrix inversions, the need to reformulate optimization problems in order to
employ kernel-generated surfaces for handling non-linear cases, and the
constraint of uniform noise assumption in the training data. Particularly in
cases where the data possesses a heteroscedastic error structure, these
challenges become even more pronounced. In view of the aforementioned
challenges, we propose multiview twin parametric margin support vector machine
(MvTPMSVM). MvTPMSVM constructs parametric margin hyperplanes corresponding to
both classes, aiming to regulate and manage the impact of the heteroscedastic
noise structure existing within the data. The proposed MvTPMSVM model avoids
the explicit computation of matrix inversions in the dual formulation, leading
to enhanced computational efficiency. We perform an extensive assessment of the
MvTPMSVM model using benchmark datasets such as UCI, KEEL, synthetic, and
Animals with Attributes (AwA). Our experimental results, coupled with rigorous
statistical analyses, confirm the superior generalization capabilities of the
proposed MvTPMSVM model compared to the baseline models. The source code of the
proposed MvTPMSVM model is available at
\url{https://github.com/mtanveer1/MvTPMSVM}.","['A. Quadir', 'M. Tanveer']","['cs.LG', 'stat.ML']",2024-08-04 10:16:11+00:00
http://arxiv.org/abs/2408.01926v1,Efficient Decision Trees for Tensor Regressions,"We proposed the tensor-input tree (TT) method for scalar-on-tensor and
tensor-on-tensor regression problems. We first address scalar-on-tensor problem
by proposing scalar-output regression tree models whose input variable are
tensors (i.e., multi-way arrays). We devised and implemented fast randomized
and deterministic algorithms for efficient fitting of scalar-on-tensor trees,
making TT competitive against tensor-input GP models. Based on scalar-on-tensor
tree models, we extend our method to tensor-on-tensor problems using additive
tree ensemble approaches. Theoretical justification and extensive experiments
on real and synthetic datasets are provided to illustrate the performance of
TT.","['Hengrui Luo', 'Akira Horiguchi', 'Li Ma']","['cs.LG', 'stat.ME', 'stat.ML', '62G08, 15A69', 'G.3']",2024-08-04 04:42:02+00:00
http://arxiv.org/abs/2408.01868v1,Meta-Posterior Consistency for the Bayesian Inference of Metastable System,"The vast majority of the literature on learning dynamical systems or
stochastic processes from time series has focused on stable or ergodic systems,
for both Bayesian and frequentist inference procedures. However, most
real-world systems are only metastable, that is, the dynamics appear to be
stable on some time scale, but are in fact unstable over longer time scales.
Consistency of inference for metastable systems may not be possible, but one
can ask about metaconsistency: Do inference procedures converge when
observations are taken over a large but finite time interval, but diverge on
longer time scales? In this paper we introduce, discuss, and quantify
metaconsistency in a Bayesian framework. We discuss how metaconsistency can be
exploited to efficiently infer a model for a sub-system of a larger system,
where inference on the global behavior may require much more data. We also
discuss the relation between meta-consistency and the spectral properties of
the model dynamical system in the case of uniformly ergodic diffusions.","['Zachary P Adams', 'Sayan Mukherjee']","['stat.ML', 'cs.LG', 'math.PR', 'math.ST', 'stat.TH', '62F15, 60J70']",2024-08-03 21:39:43+00:00
http://arxiv.org/abs/2408.01861v1,Batch Active Learning in Gaussian Process Regression using Derivatives,"We investigate the use of derivative information for Batch Active Learning in
Gaussian Process regression models. The proposed approach employs the
predictive covariance matrix for selection of data batches to exploit full
correlation of samples. We theoretically analyse our proposed algorithm taking
different optimality criteria into consideration and provide empirical
comparisons highlighting the advantage of incorporating derivatives
information. Our results show the effectiveness of our approach across diverse
applications.","['Hon Sum Alec Yu', 'Christoph Zimmer', 'Duy Nguyen-Tuong']","['cs.LG', 'stat.ML']",2024-08-03 20:13:47+00:00
http://arxiv.org/abs/2408.01851v1,Cost-constrained multi-label group feature selection using shadow features,"We consider the problem of feature selection in multi-label classification,
considering the costs assigned to groups of features. In this task, the goal is
to select a subset of features that will be useful for predicting the label
vector, but at the same time, the cost associated with the selected features
will not exceed the assumed budget. Solving the problem is of great importance
in medicine, where we may be interested in predicting various diseases based on
groups of features. The groups may be associated with parameters obtained from
a certain diagnostic test, such as a blood test. Because diagnostic test costs
can be very high, considering cost information when selecting relevant features
becomes crucial to reducing the cost of making predictions. We focus on the
feature selection method based on information theory. The proposed method
consists of two steps. First, we select features sequentially while maximizing
conditional mutual information until the budget is exhausted. In the second
step, we select additional cost-free features, i.e., those coming from groups
that have already been used in previous steps. Limiting the number of added
features is possible using the stop rule based on the concept of so-called
shadow features, which are randomized counterparts of the original ones. In
contrast to existing approaches based on penalized criteria, in our method, we
avoid the need for computationally demanding optimization of the penalty
parameter. Experiments conducted on the MIMIC medical database show the
effectiveness of the method, especially when the assumed budget is limited.","['Tomasz Klonecki', 'Paweł Teisseyre', 'Jaesung Lee']","['stat.ML', 'cs.LG']",2024-08-03 19:31:59+00:00
http://arxiv.org/abs/2408.01736v1,Can LLMs predict the convergence of Stochastic Gradient Descent?,"Large-language models are notoriously famous for their impressive performance
across a wide range of tasks. One surprising example of such impressive
performance is a recently identified capacity of LLMs to understand the
governing principles of dynamical systems satisfying the Markovian property. In
this paper, we seek to explore this direction further by studying the dynamics
of stochastic gradient descent in convex and non-convex optimization. By
leveraging the theoretical link between the SGD and Markov chains, we show a
remarkable zero-shot performance of LLMs in predicting the local minima to
which SGD converges for previously unseen starting points. On a more general
level, we inquire about the possibility of using LLMs to perform zero-shot
randomized trials for larger deep learning models used in practice.","['Oussama Zekri', 'Abdelhakim Benechehab', 'Ievgen Redko']","['cs.LG', 'cs.AI', 'stat.ML']",2024-08-03 10:35:59+00:00
http://arxiv.org/abs/2408.01697v1,Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization,"Graph out-of-distribution (OOD) generalization remains a major challenge in
graph learning since graph neural networks (GNNs) often suffer from severe
performance degradation under distribution shifts. Invariant learning, aiming
to extract invariant features across varied distributions, has recently emerged
as a promising approach for OOD generation. Despite the great success of
invariant learning in OOD problems for Euclidean data (i.e., images), the
exploration within graph data remains constrained by the complex nature of
graphs. Existing studies, such as data augmentation or causal intervention,
either suffer from disruptions to invariance during the graph manipulation
process or face reliability issues due to a lack of supervised signals for
causal parts. In this work, we propose a novel framework, called Invariant
Graph Learning based on Information bottleneck theory (InfoIGL), to extract the
invariant features of graphs and enhance models' generalization ability to
unseen distributions. Specifically, InfoIGL introduces a redundancy filter to
compress task-irrelevant information related to environmental factors.
Cooperating with our designed multi-level contrastive learning, we maximize the
mutual information among graphs of the same class in the downstream
classification tasks, preserving invariant features for prediction to a great
extent. An appealing feature of InfoIGL is its strong generalization ability
without depending on supervised signal of invariance. Experiments on both
synthetic and real-world datasets demonstrate that our method achieves
state-of-the-art performance under OOD generalization for graph classification
tasks. The source code is available at https://github.com/maowenyu-11/InfoIGL.","['Wenyu Mao', 'Jiancan Wu', 'Haoyang Liu', 'Yongduo Sui', 'Xiang Wang']","['cs.LG', 'cs.AI', 'stat.ML']",2024-08-03 07:38:04+00:00
http://arxiv.org/abs/2408.01662v2,Principal component analysis balancing prediction and approximation accuracy for spatial data,"Dimension reduction is often the first step in statistical modeling or
prediction of multivariate spatial data. However, most existing dimension
reduction techniques do not account for the spatial correlation between
observations and do not take the downstream modeling task into consideration
when finding the lower-dimensional representation. We formalize the closeness
of approximation to the original data and the utility of lower-dimensional
scores for downstream modeling as two complementary, sometimes conflicting,
metrics for dimension reduction. We illustrate how existing methodologies fall
into this framework and propose a flexible dimension reduction algorithm that
achieves the optimal trade-off. We derive a computationally simple form for our
algorithm and illustrate its performance through simulation studies, as well as
two applications in air pollution modeling and spatial transcriptomics.","['Si Cheng', 'Magali N. Blanco', 'Timothy V. Larson', 'Lianne Sheppard', 'Adam Szpiro', 'Ali Shojaie']","['stat.ME', 'stat.CO', 'stat.ML']",2024-08-03 04:24:48+00:00
http://arxiv.org/abs/2408.01642v2,Neural Term Structure of Additive Process for Option Pricing,"The additive process generalizes the L\'evy process by relaxing its
assumption of time-homogeneous increments and hence covers a larger family of
stochastic processes. Recent research in option pricing shows that modeling the
underlying log price with an additive process has advantages in easier
construction of the risk-neural measure, an explicit option pricing formula and
characteristic function, and more flexibility to fit the implied volatility
surface. Still, the challenge of calibrating an additive model arises from its
time-dependent parameterization, for which one has to prescribe parametric
functions for the term structure. For this, we propose the neural term
structure model to utilize feedforward neural networks to represent the term
structure, which alleviates the difficulty of designing parametric functions
and thus attenuates the misspecification risk. Numerical studies with S\&P 500
option data are conducted to evaluate the performance of the neural term
structure.","['Jimin Lin', 'Guixin Liu']","['q-fin.CP', 'q-fin.MF', 'q-fin.PR', 'stat.ML']",2024-08-03 03:00:50+00:00
http://arxiv.org/abs/2408.01630v1,Fair Risk Minimization under Causal Path-Specific Effect Constraints,"This paper introduces a framework for estimating fair optimal predictions
using machine learning where the notion of fairness can be quantified using
path-specific causal effects. We use a recently developed approach based on
Lagrange multipliers for infinite-dimensional functional estimation to derive
closed-form solutions for constrained optimization based on mean squared error
and cross-entropy risk criteria. The theoretical forms of the solutions are
analyzed in detail and described as nuanced adjustments to the unconstrained
minimizer. This analysis highlights important trade-offs between risk
minimization and achieving fairnes. The theoretical solutions are also used as
the basis for construction of flexible semiparametric estimation strategies for
these nuisance components. We describe the robustness properties of our
estimators in terms of achieving the optimal constrained risk, as well as in
terms of controlling the value of the constraint. We study via simulation the
impact of using robust estimators of pathway-specific effects to validate our
theory. This work advances the discourse on algorithmic fairness by integrating
complex causal considerations into model training, thus providing strategies
for implementing fair models in real-world applications.","['Razieh Nabi', 'David Benkeser']","['cs.LG', 'stat.ML']",2024-08-03 02:05:43+00:00
http://arxiv.org/abs/2408.01582v1,Conformal Diffusion Models for Individual Treatment Effect Estimation and Inference,"Estimating treatment effects from observational data is of central interest
across numerous application domains. Individual treatment effect offers the
most granular measure of treatment effect on an individual level, and is the
most useful to facilitate personalized care. However, its estimation and
inference remain underdeveloped due to several challenges. In this article, we
propose a novel conformal diffusion model-based approach that addresses those
intricate challenges. We integrate the highly flexible diffusion modeling, the
model-free statistical inference paradigm of conformal inference, along with
propensity score and covariate local approximation that tackle distributional
shifts. We unbiasedly estimate the distributions of potential outcomes for
individual treatment effect, construct an informative confidence interval, and
establish rigorous theoretical guarantees. We demonstrate the competitive
performance of the proposed method over existing solutions through extensive
numerical studies.","['Hengrui Cai', 'Huaqing Jin', 'Lexin Li']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.ME']",2024-08-02 21:35:08+00:00
http://arxiv.org/abs/2408.01575v1,Deep Learning Framework for History Matching CO2 Storage with 4D Seismic and Monitoring Well Data,"Geological carbon storage entails the injection of megatonnes of
supercritical CO2 into subsurface formations. The properties of these
formations are usually highly uncertain, which makes design and optimization of
large-scale storage operations challenging. In this paper we introduce a
history matching strategy that enables the calibration of formation properties
based on early-time observations. Early-time assessments are essential to
assure the operation is performing as planned. Our framework involves two
fit-for-purpose deep learning surrogate models that provide predictions for
in-situ monitoring well data and interpreted time-lapse (4D) seismic saturation
data. These two types of data are at very different scales of resolution, so it
is appropriate to construct separate, specialized deep learning networks for
their prediction. This approach results in a workflow that is more
straightforward to design and more efficient to train than a single surrogate
that provides global high-fidelity predictions. The deep learning models are
integrated into a hierarchical Markov chain Monte Carlo (MCMC) history matching
procedure. History matching is performed on a synthetic case with and without
4D seismic data, which allows us to quantify the impact of 4D seismic on
uncertainty reduction. The use of both data types is shown to provide
substantial uncertainty reduction in key geomodel parameters and to enable
accurate predictions of CO2 plume dynamics. The overall history matching
framework developed in this study represents an efficient way to integrate
multiple data types and to assess the impact of each on uncertainty reduction
and performance predictions.","['Nanzhe Wang', 'Louis J. Durlofsky']","['cs.LG', 'physics.geo-ph', 'stat.ML']",2024-08-02 21:14:13+00:00
http://arxiv.org/abs/2408.01517v1,Gradient flow in parameter space is equivalent to linear interpolation in output space,"We prove that the usual gradient flow in parameter space that underlies many
training algorithms for neural networks in deep learning can be continuously
deformed into an adapted gradient flow which yields (constrained) Euclidean
gradient flow in output space. Moreover, if the Jacobian of the outputs with
respect to the parameters is full rank (for fixed training data), then the time
variable can be reparametrized so that the resulting flow is simply linear
interpolation, and a global minimum can be achieved.","['Thomas Chen', 'Patrícia Muñoz Ewald']","['cs.LG', 'cs.AI', 'math-ph', 'math.MP', 'math.OC', 'stat.ML', '62M45, 37C10']",2024-08-02 18:23:17+00:00
http://arxiv.org/abs/2408.01379v1,Resampling and averaging coordinates on data,"We introduce algorithms for robustly computing intrinsic coordinates on point
clouds. Our approach relies on generating many candidate coordinates by
subsampling the data and varying hyperparameters of the embedding algorithm
(e.g., manifold learning). We then identify a subset of representative
embeddings by clustering the collection of candidate coordinates and using
shape descriptors from topological data analysis. The final output is the
embedding obtained as an average of the representative embeddings using
generalized Procrustes analysis. We validate our algorithm on both synthetic
data and experimental measurements from genomics, demonstrating robustness to
noise and outliers.","['Andrew J. Blumberg', 'Mathieu Carriere', 'Jun Hou Fung', 'Michael A. Mandell']","['stat.ML', 'cs.CG', 'cs.LG']",2024-08-02 16:37:33+00:00
http://arxiv.org/abs/2408.01367v2,Transformers are Universal In-context Learners,"Transformers are deep architectures that define ""in-context mappings"" which
enable predicting new tokens based on a given set of tokens (such as a prompt
in NLP applications or a set of patches for a vision transformer). In this
work, we study in particular the ability of these architectures to handle an
arbitrarily large number of context tokens. To mathematically, uniformly
address their expressivity, we consider the case that the mappings are
conditioned on a context represented by a probability distribution of tokens
which becomes discrete for a finite number of these. The relevant notion of
smoothness then corresponds to continuity in terms of the Wasserstein distance
between these contexts. We demonstrate that deep transformers are universal and
can approximate continuous in-context mappings to arbitrary precision,
uniformly over compact token domains. A key aspect of our results, compared to
existing findings, is that for a fixed precision, a single transformer can
operate on an arbitrary (even infinite) number of tokens. Additionally, it
operates with a fixed embedding dimension of tokens (this dimension does not
increase with precision) and a fixed number of heads (proportional to the
dimension). The use of MLPs between multi-head attention layers is also
explicitly controlled. We consider both unmasked attentions (as used for the
vision transformer) and masked causal attentions (as used for NLP and time
series applications). We tackle the causal setting leveraging a space-time
lifting to analyze causal attention as a mapping over probability distributions
of tokens.","['Takashi Furuya', 'Maarten V. de Hoop', 'Gabriel Peyré']","['cs.CL', 'stat.ML']",2024-08-02 16:21:48+00:00
http://arxiv.org/abs/2408.01362v1,Autoencoders in Function Space,"Autoencoders have found widespread application, in both their original
deterministic form and in their variational formulation (VAEs). In scientific
applications it is often of interest to consider data that are comprised of
functions; the same perspective is useful in image processing. In practice,
discretisation (of differential equations arising in the sciences) or
pixellation (of images) renders problems finite dimensional, but conceiving
first of algorithms that operate on functions, and only then discretising or
pixellating, leads to better algorithms that smoothly operate between different
levels of discretisation or pixellation. In this paper function-space versions
of the autoencoder (FAE) and variational autoencoder (FVAE) are introduced,
analysed, and deployed. Well-definedness of the objective function governing
VAEs is a subtle issue, even in finite dimension, and more so on function
space. The FVAE objective is well defined whenever the data distribution is
compatible with the chosen generative model; this happens, for example, when
the data arise from a stochastic differential equation. The FAE objective is
valid much more broadly, and can be straightforwardly applied to data governed
by differential equations. Pairing these objectives with neural operator
architectures, which can thus be evaluated on any mesh, enables new
applications of autoencoders to inpainting, superresolution, and generative
modelling of scientific data.","['Justin Bunker', 'Mark Girolami', 'Hefin Lambley', 'Andrew M. Stuart', 'T. J. Sullivan']","['stat.ML', 'cs.LG', '62G07 (Primary) 65M99, 68T07 (Secondary)', 'I.2.6']",2024-08-02 16:13:51+00:00
http://arxiv.org/abs/2408.01336v1,Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and Contaminated by Outliers,"We investigate a problem estimating coefficients of linear regression under
sparsity assumption when covariates and noises are sampled from heavy tailed
distributions. Additionally, we consider the situation where not only
covariates and noises are sampled from heavy tailed distributions but also
contaminated by outliers. Our estimators can be computed efficiently, and
exhibit sharp error bounds.","['Takeyuki Sasai', 'Hironori Fujisawa']","['stat.ML', 'cs.LG', '62J07']",2024-08-02 15:33:04+00:00
http://arxiv.org/abs/2408.01318v1,Point Prediction for Streaming Data,"We present two new approaches for point prediction with streaming data. One
is based on the Count-Min sketch (CMS) and the other is based on Gaussian
process priors with a random bias. These methods are intended for the most
general predictive problems where no true model can be usefully formulated for
the data stream. In statistical contexts, this is often called the
$\mathcal{M}$-open problem class. Under the assumption that the data consists
of i.i.d samples from a fixed distribution function $F$, we show that the
CMS-based estimates of the distribution function are consistent.
  We compare our new methods with two established predictors in terms of
cumulative $L^1$ error. One is based on the Shtarkov solution (often called the
normalized maximum likelihood) in the normal experts setting and the other is
based on Dirichlet process priors. These comparisons are for two cases. The
first is one-pass meaning that the updating of the predictors is done using the
fact that the CMS is a sketch. For predictors that are not one-pass, we use
streaming $K$-means to give a representative subset of fixed size that can be
updated as data accumulate.
  Preliminary computational work suggests that the one-pass median version of
the CMS method is rarely outperformed by the other methods for sufficiently
complex data. We also find that predictors based on Gaussian process priors
with random biases perform well. The Shtarkov predictors we use here did not
perform as well probably because we were only using the simplest example. The
other predictors seemed to perform well mainly when the data did not look like
they came from an M-open data generator.","['Aleena Chanda', 'N. V. Vinodchandran', 'Bertrand Clarke']","['stat.ML', 'cs.LG', '35A01, 65L10, 65L12, 65L20, 65L70']",2024-08-02 15:12:52+00:00
http://arxiv.org/abs/2408.01301v1,A Decision-driven Methodology for Designing Uncertainty-aware AI Self-Assessment,"Artificial intelligence (AI) has revolutionized decision-making processes and
systems throughout society and, in particular, has emerged as a significant
technology in high-impact scenarios of national interest. Yet, despite AI's
impressive predictive capabilities in controlled settings, it still suffers
from a range of practical setbacks preventing its widespread use in various
critical scenarios. In particular, it is generally unclear if a given AI
system's predictions can be trusted by decision-makers in downstream
applications. To address the need for more transparent, robust, and trustworthy
AI systems, a suite of tools has been developed to quantify the uncertainty of
AI predictions and, more generally, enable AI to ""self-assess"" the reliability
of its predictions. In this manuscript, we categorize methods for AI
self-assessment along several key dimensions and provide guidelines for
selecting and designing the appropriate method for a practitioner's needs. In
particular, we focus on uncertainty estimation techniques that consider the
impact of self-assessment on the choices made by downstream decision-makers and
on the resulting costs and benefits of decision outcomes. To demonstrate the
utility of our methodology for self-assessment design, we illustrate its use
for two realistic national-interest scenarios. This manuscript is a practical
guide for machine learning engineers and AI system users to select the ideal
self-assessment techniques for each problem.","['Gregory Canal', 'Vladimir Leung', 'Philip Sage', 'Eric Heim', 'I-Jeng Wang']","['stat.ML', 'cs.AI', 'cs.LG']",2024-08-02 14:43:45+00:00
http://arxiv.org/abs/2408.01300v1,Assessing Robustness of Machine Learning Models using Covariate Perturbations,"As machine learning models become increasingly prevalent in critical
decision-making models and systems in fields like finance, healthcare, etc.,
ensuring their robustness against adversarial attacks and changes in the input
data is paramount, especially in cases where models potentially overfit. This
paper proposes a comprehensive framework for assessing the robustness of
machine learning models through covariate perturbation techniques. We explore
various perturbation strategies to assess robustness and examine their impact
on model predictions, including separate strategies for numeric and non-numeric
variables, summaries of perturbations to assess and compare model robustness
across different scenarios, and local robustness diagnosis to identify any
regions in the data where a model is particularly unstable. Through empirical
studies on real world dataset, we demonstrate the effectiveness of our approach
in comparing robustness across models, identifying the instabilities in the
model, and enhancing model robustness.","['Arun Prakash R', 'Anwesha Bhattacharyya', 'Joel Vaughan', 'Vijayan N. Nair']","['stat.ML', 'cs.LG']",2024-08-02 14:41:36+00:00
