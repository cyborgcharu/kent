id,title,abstract,authors,categories,date
http://arxiv.org/abs/2005.01886v1,A learning problem whose consistency is equivalent to the non-existence of real-valued measurable cardinals,"We show that the $k$-nearest neighbour learning rule is universally
consistent in a metric space $X$ if and only if it is universally consistent in
every separable subspace of $X$ and the density of $X$ is less than every
real-measurable cardinal. In particular, the $k$-NN classifier is universally
consistent in every metric space whose separable subspaces are sigma-finite
dimensional in the sense of Nagata and Preiss if and only if there are no
real-valued measurable cardinals. The latter assumption is relatively
consistent with ZFC, however the consistency of the existence of such cardinals
cannot be proved within ZFC. Our results were inspired by an example sketched
by C\'erou and Guyader in 2006 at an intuitive level of rigour.",['Vladimir G. Pestov'],"['cs.LG', 'math.LO', 'stat.ML', '62H30, 54F45, 03E55', 'I.2.6']",2020-05-04 23:40:28+00:00
http://arxiv.org/abs/2005.01862v1,Complex Amplitude-Phase Boltzmann Machines,"We extend the framework of Boltzmann machines to a network of complex-valued
neurons with variable amplitudes, referred to as Complex Amplitude-Phase
Boltzmann machine (CAP-BM). The model is capable of performing unsupervised
learning on the amplitude and relative phase distribution in complex data. The
sampling rule of the Gibbs distribution and the learning rules of the model are
presented. Learning in a Complex Amplitude-Phase restricted Boltzmann machine
(CAP-RBM) is demonstrated on synthetic complex-valued images, and handwritten
MNIST digits transformed by a complex wavelet transform. Specifically, we show
the necessity of a new amplitude-amplitude coupling term in our model. The
proposed model is potentially valuable for machine learning tasks involving
complex-valued data with amplitude variation, and for developing algorithms for
novel computation hardware, such as coupled oscillators and neuromorphic
hardware, on which Boltzmann sampling can be executed in the complex domain.","['Zengyi Li', 'Friedrich T. Sommer']","['stat.ML', 'cs.LG', 'cs.NE']",2020-05-04 21:44:59+00:00
http://arxiv.org/abs/2005.01856v4,Selecting Data Augmentation for Simulating Interventions,"Machine learning models trained with purely observational data and the
principle of empirical risk minimization \citep{vapnik_principles_1992} can
fail to generalize to unseen domains. In this paper, we focus on the case where
the problem arises through spurious correlation between the observed domains
and the actual task labels. We find that many domain generalization methods do
not explicitly take this spurious correlation into account. Instead, especially
in more application-oriented research areas like medical imaging or robotics,
data augmentation techniques that are based on heuristics are used to learn
domain invariant features. To bridge the gap between theory and practice, we
develop a causal perspective on the problem of domain generalization. We argue
that causal concepts can be used to explain the success of data augmentation by
describing how they can weaken the spurious correlation between the observed
domains and the task labels. We demonstrate that data augmentation can serve as
a tool for simulating interventional data. We use these theoretical insights to
derive a simple algorithm that is able to select data augmentation techniques
that will lead to better domain generalization.","['Maximilian Ilse', 'Jakub M. Tomczak', 'Patrick Forré']","['stat.ML', 'cs.CV', 'cs.LG']",2020-05-04 21:33:29+00:00
http://arxiv.org/abs/2005.01851v1,Ensemble Learning of Coarse-Grained Molecular Dynamics Force Fields with a Kernel Approach,"Gradient-domain machine learning (GDML) is an accurate and efficient approach
to learn a molecular potential and associated force field based on the kernel
ridge regression algorithm. Here, we demonstrate its application to learn an
effective coarse-grained (CG) model from all-atom simulation data in a sample
efficient manner. The coarse-grained force field is learned by following the
thermodynamic consistency principle, here by minimizing the error between the
predicted coarse-grained force and the all-atom mean force in the
coarse-grained coordinates. Solving this problem by GDML directly is impossible
because coarse-graining requires averaging over many training data points,
resulting in impractical memory requirements for storing the kernel matrices.
In this work, we propose a data-efficient and memory-saving alternative. Using
ensemble learning and stratified sampling, we propose a 2-layer training scheme
that enables GDML to learn an effective coarse-grained model. We illustrate our
method on a simple biomolecular system, alanine dipeptide, by reconstructing
the free energy landscape of a coarse-grained variant of this molecule. Our
novel GDML training scheme yields a smaller free energy error than neural
networks when the training set is small, and a comparably high accuracy when
the training set is sufficiently large.","['Jiang Wang', 'Stefan Chmiela', 'Klaus-Robert Müller', 'Frank Noè', 'Cecilia Clementi']","['physics.comp-ph', 'physics.chem-ph', 'stat.ML']",2020-05-04 21:20:01+00:00
http://arxiv.org/abs/2005.01807v1,Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation,"Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or
spikes) which can potentially lead to higher energy-efficiency in neuromorphic
hardware implementations. Many works have shown that an SNN for inference can
be formed by copying the weights from a trained Artificial Neural Network (ANN)
and setting the firing threshold for each layer as the maximum input received
in that layer. These type of converted SNNs require a large number of time
steps to achieve competitive accuracy which diminishes the energy savings. The
number of time steps can be reduced by training SNNs with spike-based
backpropagation from scratch, but that is computationally expensive and slow.
To address these challenges, we present a computationally-efficient training
technique for deep SNNs. We propose a hybrid training methodology: 1) take a
converted SNN and use its weights and thresholds as an initialization step for
spike-based backpropagation, and 2) perform incremental spike-timing dependent
backpropagation (STDB) on this carefully initialized network to obtain an SNN
that converges within few epochs and requires fewer time steps for input
processing. STDB is performed with a novel surrogate gradient function defined
using neuron's spike time. The proposed training methodology converges in less
than 20 epochs of spike-based backpropagation for most standard image
classification datasets, thereby greatly reducing the training complexity
compared to training SNNs from scratch. We perform experiments on CIFAR-10,
CIFAR-100, and ImageNet datasets for both VGG and ResNet architectures. We
achieve top-1 accuracy of 65.19% for ImageNet dataset on SNN with 250 time
steps, which is 10X faster compared to converted SNNs with similar accuracy.","['Nitin Rathi', 'Gopalakrishnan Srinivasan', 'Priyadarshini Panda', 'Kaushik Roy']","['cs.LG', 'cs.CV', 'stat.ML']",2020-05-04 19:30:43+00:00
http://arxiv.org/abs/2005.01800v1,Mind the Gap: On Bridging the Semantic Gap between Machine Learning and Information Security,"Despite the potential of Machine learning (ML) to learn the behavior of
malware, detect novel malware samples, and significantly improve information
security (InfoSec) we see few, if any, high-impact ML techniques in deployed
systems, notwithstanding multiple reported successes in open literature. We
hypothesize that the failure of ML in making high-impacts in InfoSec are rooted
in a disconnect between the two communities as evidenced by a semantic gap---a
difference in how executables are described (e.g. the data and features
extracted from the data). Specifically, current datasets and representations
used by ML are not suitable for learning the behaviors of an executable and
differ significantly from those used by the InfoSec community. In this paper,
we survey existing datasets used for classifying malware by ML algorithms and
the features that are extracted from the data. We observe that: 1) the current
set of extracted features are primarily syntactic, not behavioral, 2) datasets
generally contain extreme exemplars producing a dataset in which it is easy to
discriminate classes, and 3) the datasets provide significantly different
representations of the data encountered in real-world systems. For ML to make
more of an impact in the InfoSec community requires a change in the data
(including the features and labels) that is used to bridge the current semantic
gap. As a first step in enabling more behavioral analyses, we label existing
malware datasets with behavioral features using open-source threat reports
associated with malware families. This behavioral labeling alters the analysis
from identifying intent (e.g. good vs bad) or malware family membership to an
analysis of which behaviors are exhibited by an executable. We offer the
annotations with the hope of inspiring future improvements in the data that
will further bridge the semantic gap between the ML and InfoSec communities.","['Michael R. Smith', 'Nicholas T. Johnson', 'Joe B. Ingram', 'Armida J. Carbajal', 'Ramyaa Ramyaa', 'Evelyn Domschot', 'Christopher C. Lamb', 'Stephen J. Verzi', 'W. Philip Kegelmeyer']","['cs.CR', 'cs.LG', 'stat.ML']",2020-05-04 19:19:32+00:00
http://arxiv.org/abs/2005.01795v3,Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques,"Following each patient visit, physicians draft long semi-structured clinical
summaries called SOAP notes. While invaluable to clinicians and researchers,
creating digital SOAP notes is burdensome, contributing to physician burnout.
In this paper, we introduce the first complete pipelines to leverage deep
summarization models to generate these notes based on transcripts of
conversations between physicians and patients. After exploring a spectrum of
methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an
algorithm that (i) extracts important utterances relevant to each summary
section; (ii) clusters together related utterances; and then (iii) generates
one summary sentence per cluster. Cluster2Sent outperforms its purely
abstractive counterpart by 8 ROUGE-1 points, and produces significantly more
factual and coherent sentences as assessed by expert human evaluators. For
reproducibility, we demonstrate similar benefits on the publicly available AMI
dataset. Our results speak to the benefits of structuring summaries into
sections and annotating supporting evidence when constructing summarization
corpora.","['Kundan Krishna', 'Sopan Khosla', 'Jeffrey P. Bigham', 'Zachary C. Lipton']","['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']",2020-05-04 19:10:26+00:00
http://arxiv.org/abs/2005.01757v2,Sample Complexity of Uniform Convergence for Multicalibration,"There is a growing interest in societal concerns in machine learning systems,
especially in fairness. Multicalibration gives a comprehensive methodology to
address group fairness. In this work, we address the multicalibration error and
decouple it from the prediction error. The importance of decoupling the
fairness metric (multicalibration) and the accuracy (prediction error) is due
to the inherent trade-off between the two, and the societal decision regarding
the ""right tradeoff"" (as imposed many times by regulators). Our work gives
sample complexity bounds for uniform convergence guarantees of multicalibration
error, which implies that regardless of the accuracy, we can guarantee that the
empirical and (true) multicalibration errors are close. We emphasize that our
results: (1) are more general than previous bounds, as they apply to both
agnostic and realizable settings, and do not rely on a specific type of
algorithm (such as deferentially private), (2) improve over previous
multicalibration sample complexity bounds and (3) implies uniform convergence
guarantees for the classical calibration error.","['Eliran Shabat', 'Lee Cohen', 'Yishay Mansour']","['cs.LG', 'cs.DS', 'stat.ML', '68Q32', 'I.2.6']",2020-05-04 18:01:38+00:00
http://arxiv.org/abs/2005.01752v2,Fitting Laplacian Regularized Stratified Gaussian Models,"We consider the problem of jointly estimating multiple related zero-mean
Gaussian distributions from data. We propose to jointly estimate these
covariance matrices using Laplacian regularized stratified model fitting, which
includes loss and regularization terms for each covariance matrix, and also a
term that encourages the different covariances matrices to be close. This
method `borrows strength' from the neighboring covariances, to improve its
estimate. With well chosen hyper-parameters, such models can perform very well,
especially in the low data regime. We propose a distributed method that scales
to large problems, and illustrate the efficacy of the method with examples in
finance, radar signal processing, and weather forecasting.","['Jonathan Tuck', 'Stephen Boyd']","['stat.ML', 'cs.LG', 'eess.SP', 'math.OC']",2020-05-04 18:00:59+00:00
http://arxiv.org/abs/2005.01699v3,Depth-2 Neural Networks Under a Data-Poisoning Attack,"In this work, we study the possibility of defending against data-poisoning
attacks while training a shallow neural network in a regression setup. We focus
on doing supervised learning for a class of depth-2 finite-width neural
networks, which includes single-filter convolutional networks. In this class of
networks, we attempt to learn the network weights in the presence of a
malicious oracle doing stochastic, bounded and additive adversarial distortions
on the true output during training. For the non-gradient stochastic algorithm
that we construct, we prove worst-case near-optimal trade-offs among the
magnitude of the adversarial attack, the weight approximation accuracy, and the
confidence achieved by the proposed algorithm. As our algorithm uses
mini-batching, we analyze how the mini-batch size affects convergence. We also
show how to utilize the scaling of the outer layer weights to counter
output-poisoning attacks depending on the probability of attack. Lastly, we
give experimental evidence demonstrating how our algorithm outperforms
stochastic gradient descent under different input data distributions, including
instances of heavy-tailed distributions.","['Sayar Karmakar', 'Anirbit Mukherjee', 'Theodore Papamarkou']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML', '90C15 68W40 68T05']",2020-05-04 17:56:15+00:00
http://arxiv.org/abs/2005.01698v2,How to Train Your Energy-Based Model for Regression,"Energy-based models (EBMs) have become increasingly popular within computer
vision in recent years. While they are commonly employed for generative image
modeling, recent work has applied EBMs also for regression tasks, achieving
state-of-the-art performance on object detection and visual tracking. Training
EBMs is however known to be challenging. While a variety of different
techniques have been explored for generative modeling, the application of EBMs
to regression is not a well-studied problem. How EBMs should be trained for
best possible regression performance is thus currently unclear. We therefore
accept the task of providing the first detailed study of this problem. To that
end, we propose a simple yet highly effective extension of noise contrastive
estimation, and carefully compare its performance to six popular methods from
literature on the tasks of 1D regression and object detection. The results of
this comparison suggest that our training method should be considered the go-to
approach. We also apply our method to the visual tracking task, achieving
state-of-the-art performance on five datasets. Notably, our tracker achieves
63.7% AUC on LaSOT and 78.7% Success on TrackingNet. Code is available at
https://github.com/fregu856/ebms_regression.","['Fredrik K. Gustafsson', 'Martin Danelljan', 'Radu Timofte', 'Thomas B. Schön']","['cs.CV', 'cs.LG', 'cs.RO', 'stat.ML']",2020-05-04 17:55:01+00:00
http://arxiv.org/abs/2005.02209v1,Hyper-parameter Tuning for the Contextual Bandit,"We study here the problem of learning the exploration exploitation trade-off
in the contextual bandit problem with linear reward function setting. In the
traditional algorithms that solve the contextual bandit problem, the
exploration is a parameter that is tuned by the user. However, our proposed
algorithm learn to choose the right exploration parameters in an online manner
based on the observed context, and the immediate reward received for the chosen
action. We have presented here two algorithms that uses a bandit to find the
optimal exploration of the contextual bandit algorithm, which we hope is the
first step toward the automation of the multi-armed bandit algorithm.","['Djallel Bouneffouf', 'Emmanuelle Claeys']","['cs.LG', 'stat.ML']",2020-05-04 17:20:19+00:00
http://arxiv.org/abs/2005.01656v1,Categorized Bandits,"We introduce a new stochastic multi-armed bandit setting where arms are
grouped inside ``ordered'' categories. The motivating example comes from
e-commerce, where a customer typically has a greater appetence for items of a
specific well-identified but unknown category than any other one. We introduce
three concepts of ordering between categories, inspired by stochastic dominance
between random variables, which are gradually weaker so that more and more
bandit scenarios satisfy at least one of them. We first prove
instance-dependent lower bounds on the cumulative regret for each of these
models, indicating how the complexity of the bandit problems increases with the
generality of the ordering concept considered. We also provide algorithms that
fully leverage the structure of the model with their associated theoretical
guarantees. Finally, we have conducted an analysis on real data to highlight
that those ordered categories actually exist in practice.","['Matthieu Jedor', 'Jonathan Louedec', 'Vianney Perchet']","['cs.LG', 'stat.ML']",2020-05-04 17:09:22+00:00
http://arxiv.org/abs/2005.01643v3,"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems","In this tutorial article, we aim to provide the reader with the conceptual
tools needed to get started on research on offline reinforcement learning
algorithms: reinforcement learning algorithms that utilize previously collected
data, without additional online data collection. Offline reinforcement learning
algorithms hold tremendous promise for making it possible to turn large
datasets into powerful decision making engines. Effective offline reinforcement
learning methods would be able to extract policies with the maximum possible
utility out of the available data, thereby allowing automation of a wide range
of decision-making domains, from healthcare and education to robotics. However,
the limitations of current algorithms make this difficult. We will aim to
provide the reader with an understanding of these challenges, particularly in
the context of modern deep reinforcement learning methods, and describe some
potential solutions that have been explored in recent work to mitigate these
challenges, along with recent applications, and a discussion of perspectives on
open problems in the field.","['Sergey Levine', 'Aviral Kumar', 'George Tucker', 'Justin Fu']","['cs.LG', 'cs.AI', 'stat.ML']",2020-05-04 17:00:15+00:00
http://arxiv.org/abs/2005.01575v9,"StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics","In machine learning (ML), ensemble methods such as bagging, boosting, and
stacking are widely-established approaches that regularly achieve top-notch
predictive performance. Stacking (also called ""stacked generalization"") is an
ensemble method that combines heterogeneous base models, arranged in at least
one layer, and then employs another metamodel to summarize the predictions of
those models. Although it may be a highly-effective approach for increasing the
predictive performance of ML, generating a stack of models from scratch can be
a cumbersome trial-and-error process. This challenge stems from the enormous
space of available solutions, with different sets of data instances and
features that could be used for training, several algorithms to choose from,
and instantiations of these algorithms using diverse parameters (i.e., models)
that perform differently according to various metrics. In this work, we present
a knowledge generation model, which supports ensemble learning with the use of
visualization, and a visual analytics system for stacked generalization. Our
system, StackGenVis, assists users in dynamically adapting performance metrics,
managing data instances, selecting the most important features for a given data
set, choosing a set of top-performant and diverse algorithms, and measuring the
predictive performance. In consequence, our proposed tool helps users to decide
between distinct models and to reduce the complexity of the resulting stack by
removing overpromising and underperforming models. The applicability and
effectiveness of StackGenVis are demonstrated with two use cases: a real-world
healthcare data set and a collection of data related to sentiment/stance
detection in texts. Finally, the tool has been evaluated through interviews
with three ML experts.","['Angelos Chatzimparmpas', 'Rafael M. Martins', 'Kostiantyn Kucher', 'Andreas Kerren']","['cs.LG', 'cs.HC', 'stat.ML']",2020-05-04 15:43:55+00:00
http://arxiv.org/abs/2005.01571v3,Frugal Optimization for Cost-related Hyperparameters,"The increasing demand for democratizing machine learning algorithms calls for
hyperparameter optimization (HPO) solutions at low cost. Many machine learning
algorithms have hyperparameters which can cause a large variation in the
training cost. But this effect is largely ignored in existing HPO methods,
which are incapable to properly control cost during the optimization process.
To address this problem, we develop a new cost-frugal HPO solution. The core of
our solution is a simple but new randomized direct-search method, for which we
prove a convergence rate of $O(\frac{\sqrt{d}}{\sqrt{K}})$ and an
$O(d\epsilon^{-2})$-approximation guarantee on the total cost. We provide
strong empirical results in comparison with state-of-the-art HPO methods on
large AutoML benchmarks.","['Qingyun Wu', 'Chi Wang', 'Silu Huang']","['cs.LG', 'stat.ML']",2020-05-04 15:40:44+00:00
http://arxiv.org/abs/2005.01566v1,Lecture notes: Efficient approximation of kernel functions,"These lecture notes endeavour to collect in one place the mathematical
background required to understand the properties of kernels in general and the
Random Fourier Features approximation of Rahimi and Recht (NIPS 2007) in
particular. We briefly motivate the use of kernels in Machine Learning with the
example of the support vector machine. We discuss positive definite and
conditionally negative definite kernels in some detail. After a brief
discussion of Hilbert spaces, including the Reproducing Kernel Hilbert Space
construction, we present Mercer's theorem. We discuss the Random Fourier
Features technique and then present, with proofs, scalar and matrix
concentration results that help us estimate the error incurred by the
technique. These notes are the transcription of 10 lectures given at IIT Delhi
between January and April 2020.",['Amitabha Bagchi'],"['cs.LG', 'math.PR', 'stat.ML']",2020-05-04 15:30:06+00:00
http://arxiv.org/abs/2005.01560v1,A Dynamical Mean-Field Theory for Learning in Restricted Boltzmann Machines,"We define a message-passing algorithm for computing magnetizations in
Restricted Boltzmann machines, which are Ising models on bipartite graphs
introduced as neural network models for probability distributions over spin
configurations. To model nontrivial statistical dependencies between the spins'
couplings, we assume that the rectangular coupling matrix is drawn from an
arbitrary bi-rotation invariant random matrix ensemble. Using the dynamical
functional method of statistical mechanics we exactly analyze the dynamics of
the algorithm in the large system limit. We prove the global convergence of the
algorithm under a stability criterion and compute asymptotic convergence rates
showing excellent agreement with numerical simulations.","['Burak Çakmak', 'Manfred Opper']","['cs.LG', 'cond-mat.dis-nn', 'stat.ML']",2020-05-04 15:19:31+00:00
http://arxiv.org/abs/2005.01557v1,"Off-the-shelf deep learning is not enough: parsimony, Bayes and causality","Deep neural networks (""deep learning"") have emerged as a technology of choice
to tackle problems in natural language processing, computer vision, speech
recognition and gameplay, and in just a few years has led to superhuman level
performance and ushered in a new wave of ""AI."" Buoyed by these successes,
researchers in the physical sciences have made steady progress in incorporating
deep learning into their respective domains. However, such adoption brings
substantial challenges that need to be recognized and confronted. Here, we
discuss both opportunities and roadblocks to implementation of deep learning
within materials science, focusing on the relationship between correlative
nature of machine learning and causal hypothesis driven nature of physical
sciences. We argue that deep learning and AI are now well positioned to
revolutionize fields where causal links are known, as is the case for
applications in theory. When confounding factors are frozen or change only
weakly, this leaves open the pathway for effective deep learning solutions in
experimental domains. Similarly, these methods offer a pathway towards
understanding the physics of real-world systems, either via deriving reduced
representations, deducing algorithmic complexity, or recovering generative
physical models. However, extending deep learning and ""AI"" for models with
unclear causal relationship can produce misleading and potentially incorrect
results. Here, we argue the broad adoption of Bayesian methods incorporating
prior knowledge, development of DL solutions with incorporated physical
constraints, and ultimately adoption of causal models, offers a path forward
for fundamental and applied research. Most notably, while these advances can
change the way science is carried out in ways we cannot imagine, machine
learning is not going to substitute science any time soon.","['Rama K. Vasudevan', 'Maxim Ziatdinov', 'Lukas Vlcek', 'Sergei V. Kalinin']","['physics.comp-ph', 'cond-mat.dis-nn', 'cs.LG', 'stat.ML']",2020-05-04 15:16:30+00:00
http://arxiv.org/abs/2005.01538v1,A Solution for Large Scale Nonlinear Regression with High Rank and Degree at Constant Memory Complexity via Latent Tensor Reconstruction,"This paper proposes a novel method for learning highly nonlinear,
multivariate functions from examples. Our method takes advantage of the
property that continuous functions can be approximated by polynomials, which in
turn are representable by tensors. Hence the function learning problem is
transformed into a tensor reconstruction problem, an inverse problem of the
tensor decomposition. Our method incrementally builds up the unknown tensor
from rank-one terms, which lets us control the complexity of the learned model
and reduce the chance of overfitting. For learning the models, we present an
efficient gradient-based algorithm that can be implemented in linear time in
the sample size, order, rank of the tensor and the dimension of the input. In
addition to regression, we present extensions to classification, multi-view
learning and vector-valued output as well as a multi-layered formulation. The
method can work in an online fashion via processing mini-batches of the data
with constant memory complexity. Consequently, it can fit into systems equipped
only with limited resources such as embedded systems or mobile phones. Our
experiments demonstrate a favorable accuracy and running time compared to
competing methods.","['Sandor Szedmak', 'Anna Cichonska', 'Heli Julkunen', 'Tapio Pahikkala', 'Juho Rousu']","['cs.LG', 'stat.ML']",2020-05-04 14:49:14+00:00
http://arxiv.org/abs/2005.01452v2,Do Gradient-based Explanations Tell Anything About Adversarial Robustness to Android Malware?,"While machine-learning algorithms have demonstrated a strong ability in
detecting Android malware, they can be evaded by sparse evasion attacks crafted
by injecting a small set of fake components, e.g., permissions and system
calls, without compromising intrusive functionality. Previous work has shown
that, to improve robustness against such attacks, learning algorithms should
avoid overemphasizing few discriminant features, providing instead decisions
that rely upon a large subset of components. In this work, we investigate
whether gradient-based attribution methods, used to explain classifiers'
decisions by identifying the most relevant features, can be used to help
identify and select more robust algorithms. To this end, we propose to exploit
two different metrics that represent the evenness of explanations, and a new
compact security measure called Adversarial Robustness Metric. Our experiments
conducted on two different datasets and five classification algorithms for
Android malware detection show that a strong connection exists between the
uniformity of explanations and adversarial robustness. In particular, we found
that popular techniques like Gradient*Input and Integrated Gradients are
strongly correlated to security when applied to both linear and nonlinear
detectors, while more elementary explanation techniques like the simple
Gradient do not provide reliable information about the robustness of such
classifiers.","['Marco Melis', 'Michele Scalas', 'Ambra Demontis', 'Davide Maiorca', 'Battista Biggio', 'Giorgio Giacinto', 'Fabio Roli']","['cs.LG', 'cs.CR', 'stat.ML']",2020-05-04 13:12:31+00:00
http://arxiv.org/abs/2005.01449v1,Stochastic Sparse Subspace Clustering,"State-of-the-art subspace clustering methods are based on self-expressive
model, which represents each data point as a linear combination of other data
points. By enforcing such representation to be sparse, sparse subspace
clustering is guaranteed to produce a subspace-preserving data affinity where
two points are connected only if they are from the same subspace. On the other
hand, however, data points from the same subspace may not be well-connected,
leading to the issue of over-segmentation. We introduce dropout to address the
issue of over-segmentation, which is based on randomly dropping out data points
in self-expressive model. In particular, we show that dropout is equivalent to
adding a squared $\ell_2$ norm regularization on the representation
coefficients, therefore induces denser solutions. Then, we reformulate the
optimization problem as a consensus problem over a set of small-scale
subproblems. This leads to a scalable and flexible sparse subspace clustering
approach, termed Stochastic Sparse Subspace Clustering, which can effectively
handle large scale datasets. Extensive experiments on synthetic data and real
world datasets validate the efficiency and effectiveness of our proposal.","['Ying Chen', 'Chun-Guang Li', 'Chong You']","['cs.LG', 'cs.CV', 'stat.ML']",2020-05-04 13:09:17+00:00
http://arxiv.org/abs/2005.01432v2,Hierarchical Decomposition of Nonlinear Dynamics and Control for System Identification and Policy Distillation,"The control of nonlinear dynamical systems remains a major challenge for
autonomous agents. Current trends in reinforcement learning (RL) focus on
complex representations of dynamics and policies, which have yielded impressive
results in solving a variety of hard control tasks. However, this new
sophistication and extremely over-parameterized models have come with the cost
of an overall reduction in our ability to interpret the resulting policies. In
this paper, we take inspiration from the control community and apply the
principles of hybrid switching systems in order to break down complex dynamics
into simpler components. We exploit the rich representational power of
probabilistic graphical models and derive an expectation-maximization (EM)
algorithm for learning a sequence model to capture the temporal structure of
the data and automatically decompose nonlinear dynamics into stochastic
switching linear dynamical systems. Moreover, we show how this framework of
switching models enables extracting hierarchies of Markovian and
auto-regressive locally linear controllers from nonlinear experts in an
imitation learning scenario.","['Hany Abdulsamad', 'Jan Peters']","['cs.LG', 'stat.ML']",2020-05-04 12:40:59+00:00
http://arxiv.org/abs/2005.01427v3,LIMEtree: Consistent and Faithful Multi-class Explanations,"Explainable artificial intelligence provides tools to better understand
predictive models and their decisions, but many such methods are limited to
producing insights with respect to a single class. When generating explanations
for several classes, reasoning over them to obtain a complete view may be
difficult since they can present competing or contradictory evidence. To
address this challenge we introduce the novel paradigm of multi-class
explanations. We outline the theory behind such techniques and propose a local
surrogate model based on multi-output regression trees -- called LIMEtree --
that offers faithful and consistent explanations of multiple classes for
individual predictions while being post-hoc, model-agnostic and data-universal.
On top of strong fidelity guarantees, our implementation delivers a range of
diverse explanation types, including counterfactual statements favoured in the
literature. We evaluate our algorithm with respect to explainability
desiderata, through quantitative experiments and via a pilot user study, on
image and tabular data classification tasks, comparing it to LIME, which is a
state-of-the-art surrogate explainer. Our contributions demonstrate the
benefits of multi-class explanations and wide-ranging advantages of our method
across a diverse set of scenarios.","['Kacper Sokol', 'Peter Flach']","['cs.LG', 'cs.AI', 'stat.ML']",2020-05-04 12:31:29+00:00
http://arxiv.org/abs/2005.01404v3,Robust M-Estimation Based Bayesian Cluster Enumeration for Real Elliptically Symmetric Distributions,"Robustly determining the optimal number of clusters in a data set is an
essential factor in a wide range of applications. Cluster enumeration becomes
challenging when the true underlying structure in the observed data is
corrupted by heavy-tailed noise and outliers. Recently, Bayesian cluster
enumeration criteria have been derived by formulating cluster enumeration as
maximization of the posterior probability of candidate models. This article
generalizes robust Bayesian cluster enumeration so that it can be used with any
arbitrary Real Elliptically Symmetric (RES) distributed mixture model. Our
framework also covers the case of M-estimators that allow for mixture models,
which are decoupled from a specific probability distribution. Examples of
Huber's and Tukey's M-estimators are discussed. We derive a robust criterion
for data sets with finite sample size, and also provide an asymptotic
approximation to reduce the computational cost at large sample sizes. The
algorithms are applied to simulated and real-world data sets, including
radar-based person identification, and show a significant robustness
improvement in comparison to existing methods.","['Christian A. Schroth', 'Michael Muma']","['eess.SP', 'stat.ML']",2020-05-04 11:44:49+00:00
http://arxiv.org/abs/2005.01378v1,High-Dimensional Robust Mean Estimation via Gradient Descent,"We study the problem of high-dimensional robust mean estimation in the
presence of a constant fraction of adversarial outliers. A recent line of work
has provided sophisticated polynomial-time algorithms for this problem with
dimension-independent error guarantees for a range of natural distribution
families.
  In this work, we show that a natural non-convex formulation of the problem
can be solved directly by gradient descent. Our approach leverages a novel
structural lemma, roughly showing that any approximate stationary point of our
non-convex objective gives a near-optimal solution to the underlying robust
estimation task. Our work establishes an intriguing connection between
algorithmic high-dimensional robust statistics and non-convex optimization,
which may have broader applications to other robust estimation tasks.","['Yu Cheng', 'Ilias Diakonikolas', 'Rong Ge', 'Mahdi Soltanolkotabi']","['cs.LG', 'cs.DS', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",2020-05-04 10:48:04+00:00
http://arxiv.org/abs/2005.01350v3,A Finite Time Analysis of Two Time-Scale Actor Critic Methods,"Actor-critic (AC) methods have exhibited great empirical success compared
with other reinforcement learning algorithms, where the actor uses the policy
gradient to improve the learning policy and the critic uses temporal difference
learning to estimate the policy gradient. Under the two time-scale learning
rate schedule, the asymptotic convergence of AC has been well studied in the
literature. However, the non-asymptotic convergence and finite sample
complexity of actor-critic methods are largely open. In this work, we provide a
non-asymptotic analysis for two time-scale actor-critic methods under
non-i.i.d. setting. We prove that the actor-critic method is guaranteed to find
a first-order stationary point (i.e., $\|\nabla J(\boldsymbol{\theta})\|_2^2
\le \epsilon$) of the non-concave performance function
$J(\boldsymbol{\theta})$, with $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ sample
complexity. To the best of our knowledge, this is the first work providing
finite-time analysis and sample complexity bound for two time-scale
actor-critic methods.","['Yue Wu', 'Weitong Zhang', 'Pan Xu', 'Quanquan Gu']","['cs.LG', 'math.OC', 'stat.ML']",2020-05-04 09:45:18+00:00
http://arxiv.org/abs/2005.01317v2,"Robust Non-Linear Matrix Factorization for Dictionary Learning, Denoising, and Clustering","Low dimensional nonlinear structure abounds in datasets across computer
vision and machine learning. Kernelized matrix factorization techniques have
recently been proposed to learn these nonlinear structures for denoising,
classification, dictionary learning, and missing data imputation, by observing
that the image of the matrix in a sufficiently large feature space is low-rank.
However, these nonlinear methods fail in the presence of sparse noise or
outliers. In this work, we propose a new robust nonlinear factorization method
called Robust Non-Linear Matrix Factorization (RNLMF). RNLMF constructs a
dictionary for the data space by factoring a kernelized feature space; a noisy
matrix can then be decomposed as the sum of a sparse noise matrix and a clean
data matrix that lies in a low dimensional nonlinear manifold. RNLMF is robust
to sparse noise and outliers and scales to matrices with thousands of rows and
columns. Empirically, RNLMF achieves noticeable improvements over baseline
methods in denoising and clustering.","['Jicong Fan', 'Chengrun Yang', 'Madeleine Udell']","['cs.LG', 'stat.ML']",2020-05-04 08:32:21+00:00
http://arxiv.org/abs/2005.01309v4,Global sensitivity analysis for stochastic simulators based on generalized lambda surrogate models,"Global sensitivity analysis aims at quantifying the impact of input
variability onto the variation of the response of a computational model. It has
been widely applied to deterministic simulators, for which a set of input
parameters has a unique corresponding output value. Stochastic simulators,
however, have intrinsic randomness due to their use of (pseudo)random numbers,
so they give different results when run twice with the same input parameters
but non-common random numbers. Due to this random nature, conventional Sobol'
indices, used in global sensitivity analysis, can be extended to stochastic
simulators in different ways. In this paper, we discuss three possible
extensions and focus on those that depend only on the statistical dependence
between input and output. This choice ignores the detailed data generating
process involving the internal randomness, and can thus be applied to a wider
class of problems. We propose to use the generalized lambda model to emulate
the response distribution of stochastic simulators. Such a surrogate can be
constructed without the need for replications. The proposed method is applied
to three examples including two case studies in finance and epidemiology. The
results confirm the convergence of the approach for estimating the sensitivity
indices even with the presence of strong heteroskedasticity and small
signal-to-noise ratio.","['X. Zhu', 'B. Sudret']","['stat.CO', 'stat.ME', 'stat.ML']",2020-05-04 08:03:31+00:00
http://arxiv.org/abs/2005.01302v3,Simulation free reliability analysis: A physics-informed deep learning based approach,"This paper presents a simulation free framework for solving reliability
analysis problems. The method proposed is rooted in a recently developed deep
learning approach, referred to as the physics-informed neural network. The
primary idea is to learn the neural network parameters directly from the
physics of the problem. With this, the need for running simulation and
generating data is completely eliminated. Additionally, the proposed approach
also satisfies physical laws such as invariance properties and conservation
laws associated with the problem. The proposed approach is used for solving
three benchmark reliability analysis problems. Results obtained illustrates
that the proposed approach is highly accurate. Moreover, the primary bottleneck
of solving reliability analysis problems, i.e., running expensive simulations
to generate data, is eliminated with this method.",['Souvik Chakraborty'],['stat.ML'],2020-05-04 07:19:50+00:00
http://arxiv.org/abs/2005.01297v1,Sum-Product-Transform Networks: Exploiting Symmetries using Invertible Transformations,"In this work, we propose Sum-Product-Transform Networks (SPTN), an extension
of sum-product networks that uses invertible transformations as additional
internal nodes. The type and placement of transformations determine properties
of the resulting SPTN with many interesting special cases. Importantly, SPTN
with Gaussian leaves and affine transformations pose the same inference task
tractable that can be computed efficiently in SPNs. We propose to store affine
transformations in their SVD decompositions using an efficient parametrization
of unitary matrices by a set of Givens rotations. Last but not least, we
demonstrate that G-SPTNs achieve state-of-the-art results on the density
estimation task and are competitive with state-of-the-art methods for anomaly
detection.","['Tomas Pevny', 'Vasek Smidl', 'Martin Trapp', 'Ondrej Polacek', 'Tomas Oberhuber']","['stat.ML', 'cs.LG']",2020-05-04 07:05:51+00:00
http://arxiv.org/abs/2005.01285v3,Connecting the Dots: Numerical Randomized Hamiltonian Monte Carlo with State-Dependent Event Rates,"Numerical Generalized Randomized Hamiltonian Monte Carlo is introduced, as a
robust, easy to use and computationally fast alternative to conventional Markov
chain Monte Carlo methods for continuous target distributions. A wide class of
piecewise deterministic Markov processes generalizing Randomized HMC (Bou-Rabee
and Sanz-Serna, 2017) by allowing for state-dependent event rates is defined.
Under very mild restrictions, such processes will have the desired target
distribution as an invariant distribution. Secondly, the numerical
implementation of such processes, based on adaptive numerical integration of
second order ordinary differential equations (ODEs) is considered. The
numerical implementation yields an approximate, yet highly robust algorithm
that, unlike conventional Hamiltonian Monte Carlo, enables the exploitation of
the complete Hamiltonian trajectories (hence the title). The proposed algorithm
may yield large speedups and improvements in stability relative to relevant
benchmarks, while incurring numerical biases that are negligible relative to
the overall Monte Carlo errors. Granted access to a high-quality ODE code, the
proposed methodology is both easy to implement and use, even for highly
challenging and high-dimensional target distributions.",['Tore Selland Kleppe'],"['stat.CO', 'stat.ME', 'stat.ML']",2020-05-04 06:23:13+00:00
http://arxiv.org/abs/2005.02191v3,Localized active learning of Gaussian process state space models,"The performance of learning-based control techniques crucially depends on how
effectively the system is explored. While most exploration techniques aim to
achieve a globally accurate model, such approaches are generally unsuited for
systems with unbounded state spaces. Furthermore, a globally accurate model is
not required to achieve good performance in many common control applications,
e.g., local stabilization tasks. In this paper, we propose an active learning
strategy for Gaussian process state space models that aims to obtain an
accurate model on a bounded subset of the state-action space. Our approach aims
to maximize the mutual information of the exploration trajectories with respect
to a discretization of the region of interest. By employing model predictive
control, the proposed technique integrates information collected during
exploration and adaptively improves its exploration strategy. To enable
computational tractability, we decouple the choice of most informative data
points from the model predictive control optimization step. This yields two
optimization problems that can be solved in parallel. We apply the proposed
method to explore the state space of various dynamical systems and compare our
approach to a commonly used entropy-based exploration strategy. In all
experiments, our method yields a better model within the region of interest
than the entropy-based method.","['Alexandre Capone', 'Jonas Umlauft', 'Thomas Beckers', 'Armin Lederer', 'Sandra Hirche']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2020-05-04 05:35:02+00:00
http://arxiv.org/abs/2005.01246v1,Generalized Reinforcement Meta Learning for Few-Shot Optimization,"We present a generic and flexible Reinforcement Learning (RL) based
meta-learning framework for the problem of few-shot learning. During training,
it learns the best optimization algorithm to produce a learner
(ranker/classifier, etc) by exploiting stable patterns in loss surfaces. Our
method implicitly estimates the gradients of a scaled loss function while
retaining the general properties intact for parameter updates. Besides
providing improved performance on few-shot tasks, our framework could be easily
extended to do network architecture search. We further propose a novel dual
encoder, affinity-score based decoder topology that achieves additional
improvements to performance. Experiments on an internal dataset, MQ2007, and
AwA2 show our approach outperforms existing alternative approaches by 21%, 8%,
and 4% respectively on accuracy and NDCG metrics. On Mini-ImageNet dataset our
approach achieves comparable results with Prototypical Networks. Empirical
evaluations demonstrate that our approach provides a unified and effective
framework.","['Raviteja Anantha', 'Stephen Pulman', 'Srinivas Chappidi']","['cs.LG', 'cs.AI', 'stat.ML']",2020-05-04 03:21:05+00:00
http://arxiv.org/abs/2005.01214v2,Graph Homomorphism Convolution,"In this paper, we study the graph classification problem from the graph
homomorphism perspective. We consider the homomorphisms from $F$ to $G$, where
$G$ is a graph of interest (e.g. molecules or social networks) and $F$ belongs
to some family of graphs (e.g. paths or non-isomorphic trees). We show that
graph homomorphism numbers provide a natural invariant (isomorphism invariant
and $\mathcal{F}$-invariant) embedding maps which can be used for graph
classification. Viewing the expressive power of a graph classifier by the
$\mathcal{F}$-indistinguishable concept, we prove the universality property of
graph homomorphism vectors in approximating $\mathcal{F}$-invariant functions.
In practice, by choosing $\mathcal{F}$ whose elements have bounded tree-width,
we show that the homomorphism method is efficient compared with other methods.","['Hoang NT', 'Takanori Maehara']","['cs.LG', 'cs.DM', 'math.CO', 'stat.ML']",2020-05-03 23:56:20+00:00
http://arxiv.org/abs/2005.01209v2,Riemannian Stochastic Proximal Gradient Methods for Nonsmooth Optimization over the Stiefel Manifold,"Riemannian optimization has drawn a lot of attention due to its wide
applications in practice. Riemannian stochastic first-order algorithms have
been studied in the literature to solve large-scale machine learning problems
over Riemannian manifolds. However, most of the existing Riemannian stochastic
algorithms require the objective function to be differentiable, and they do not
apply to the case where the objective function is nonsmooth. In this paper, we
present two Riemannian stochastic proximal gradient methods for minimizing
nonsmooth function over the Stiefel manifold. The two methods, named R-ProxSGD
and R-ProxSPB, are generalizations of proximal SGD and proximal SpiderBoost in
Euclidean setting to the Riemannian setting. Analysis on the incremental
first-order oracle (IFO) complexity of the proposed algorithms is provided.
Specifically, the R-ProxSPB algorithm finds an $\epsilon$-stationary point with
$\O(\epsilon^{-3})$ IFOs in the online case, and $\O(n+\sqrt{n}\epsilon^{-2})$
IFOs in the finite-sum case with $n$ being the number of summands in the
objective. Experimental results on online sparse PCA and robust low-rank matrix
completion show that our proposed methods significantly outperform the existing
methods that use Riemannian subgradient information.","['Bokun Wang', 'Shiqian Ma', 'Lingzhou Xue']","['math.OC', 'cs.LG', 'stat.ML']",2020-05-03 23:41:35+00:00
http://arxiv.org/abs/2005.01138v1,Off-Policy Adversarial Inverse Reinforcement Learning,"Adversarial Imitation Learning (AIL) is a class of algorithms in
Reinforcement learning (RL), which tries to imitate an expert without taking
any reward from the environment and does not provide expert behavior directly
to the policy training. Rather, an agent learns a policy distribution that
minimizes the difference from expert behavior in an adversarial setting.
Adversarial Inverse Reinforcement Learning (AIRL) leverages the idea of AIL,
integrates a reward function approximation along with learning the policy, and
shows the utility of IRL in the transfer learning setting. But the reward
function approximator that enables transfer learning does not perform well in
imitation tasks. We propose an Off-Policy Adversarial Inverse Reinforcement
Learning (Off-policy-AIRL) algorithm which is sample efficient as well as gives
good imitation performance compared to the state-of-the-art AIL algorithm in
the continuous control tasks. For the same reward function approximator, we
show the utility of learning our algorithm over AIL by using the learned reward
function to retrain the policy over a task under significant variation where
expert demonstrations are absent.",['Samin Yeasar Arnob'],"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2020-05-03 16:51:40+00:00
http://arxiv.org/abs/2005.01123v1,Mutual Information Gradient Estimation for Representation Learning,"Mutual Information (MI) plays an important role in representation learning.
However, MI is unfortunately intractable in continuous and high-dimensional
settings. Recent advances establish tractable and scalable MI estimators to
discover useful representation. However, most of the existing methods are not
capable of providing an accurate estimation of MI with low-variance when the MI
is large. We argue that directly estimating the gradients of MI is more
appealing for representation learning than estimating MI in itself. To this
end, we propose the Mutual Information Gradient Estimator (MIGE) for
representation learning based on the score estimation of implicit
distributions. MIGE exhibits a tight and smooth gradient estimation of MI in
the high-dimensional and large-MI settings. We expand the applications of MIGE
in both unsupervised learning of deep representations based on InfoMax and the
Information Bottleneck method. Experimental results have indicated significant
performance improvement in learning useful representation.","['Liangjian Wen', 'Yiji Zhou', 'Lirong He', 'Mingyuan Zhou', 'Zenglin Xu']","['stat.ML', 'cs.CV', 'cs.LG']",2020-05-03 16:05:58+00:00
http://arxiv.org/abs/2005.01097v2,Adaptive Learning of the Optimal Batch Size of SGD,"Recent advances in the theoretical understanding of SGD led to a formula for
the optimal batch size minimizing the number of effective data passes, i.e.,
the number of iterations times the batch size. However, this formula is of no
practical value as it depends on the knowledge of the variance of the
stochastic gradients evaluated at the optimum. In this paper we design a
practical SGD method capable of learning the optimal batch size adaptively
throughout its iterations for strongly convex and smooth functions. Our method
does this provably, and in our experiments with synthetic and real data
robustly exhibits nearly optimal behaviour; that is, it works as if the optimal
batch size was known a-priori. Further, we generalize our method to several new
batch strategies not considered in the literature before, including a sampling
suitable for distributed implementations.","['Motasem Alfarra', 'Slavomir Hanzely', 'Alyazeed Albasyoni', 'Bernard Ghanem', 'Peter Richtarik']","['cs.LG', 'math.OC', 'stat.ML']",2020-05-03 14:28:32+00:00
http://arxiv.org/abs/2005.01095v3,A Causal View on Robustness of Neural Networks,"We present a causal view on the robustness of neural networks against input
manipulations, which applies not only to traditional classification tasks but
also to general measurement data. Based on this view, we design a deep causal
manipulation augmented model (deep CAMA) which explicitly models possible
manipulations on certain causes leading to changes in the observed effect. We
further develop data augmentation and test-time fine-tuning methods to improve
deep CAMA's robustness. When compared with discriminative deep neural networks,
our proposed model shows superior robustness against unseen manipulations. As a
by-product, our model achieves disentangled representation which separates the
representation of manipulations from those of other latent causes.","['Cheng Zhang', 'Kun Zhang', 'Yingzhen Li']","['cs.LG', 'stat.ML']",2020-05-03 14:20:05+00:00
http://arxiv.org/abs/2005.01026v3,Multi-Center Federated Learning: Clients Clustering for Better Personalization,"Federated learning has received great attention for its capability to train a
large-scale model in a decentralized manner without needing to access user data
directly. It helps protect the users' private data from centralized collecting.
Unlike distributed machine learning, federated learning aims to tackle non-IID
data from heterogeneous sources in various real-world applications, such as
those on smartphones. Existing federated learning approaches usually adopt a
single global model to capture the shared knowledge of all users by aggregating
their gradients, regardless of the discrepancy between their data
distributions. However, due to the diverse nature of user behaviors, assigning
users' gradients to different global models (i.e., centers) can better capture
the heterogeneity of data distributions across users. Our paper proposes a
novel multi-center aggregation mechanism for federated learning, which learns
multiple global models from the non-IID user data and simultaneously derives
the optimal matching between users and centers. We formulate the problem as a
joint optimization that can be efficiently solved by a stochastic expectation
maximization (EM) algorithm. Our experimental results on benchmark datasets
show that our method outperforms several popular federated learning methods.","['Guodong Long', 'Ming Xie', 'Tao Shen', 'Tianyi Zhou', 'Xianzhi Wang', 'Jing Jiang', 'Chengqi Zhang']","['cs.LG', 'cs.DC', 'stat.ML']",2020-05-03 09:14:31+00:00
http://arxiv.org/abs/2005.00959v3,On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective,"Ill-posed linear inverse problems appear in many scientific setups, and are
typically addressed by solving optimization problems, which are composed of
data fidelity and prior terms. Recently, several works have considered a
back-projection (BP) based fidelity term as an alternative to the common least
squares (LS), and demonstrated excellent results for popular inverse problems.
These works have also empirically shown that using the BP term, rather than the
LS term, requires fewer iterations of optimization algorithms. In this paper,
we examine the convergence rate of the projected gradient descent (PGD)
algorithm for the BP objective. Our analysis allows to identify an inherent
source for its faster convergence compared to using the LS objective, while
making only mild assumptions. We also analyze the more general proximal
gradient method under a relaxed contraction condition on the proximal mapping
of the prior. This analysis further highlights the advantage of BP when the
linear measurement operator is badly conditioned. Numerical experiments with
both $\ell_1$-norm and GAN-based priors corroborate our theoretical results.","['Tom Tirer', 'Raja Giryes']","['math.OC', 'cs.CV', 'cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2020-05-03 00:58:23+00:00
http://arxiv.org/abs/2005.00935v1,Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey,"Latest technological improvements increased the quality of transportation.
New data-driven approaches bring out a new research direction for all
control-based systems, e.g., in transportation, robotics, IoT and power
systems. Combining data-driven applications with transportation systems plays a
key role in recent transportation applications. In this paper, the latest deep
reinforcement learning (RL) based traffic control applications are surveyed.
Specifically, traffic signal control (TSC) applications based on (deep) RL,
which have been studied extensively in the literature, are discussed in detail.
Different problem formulations, RL parameters, and simulation environments for
TSC are discussed comprehensively. In the literature, there are also several
autonomous driving applications studied with deep RL models. Our survey
extensively summarizes existing works in this field by categorizing them with
respect to application types, control models and studied algorithms. In the
end, we discuss the challenges and open questions regarding deep RL-based
transportation applications.","['Ammar Haydari', 'Yasin Yilmaz']","['cs.LG', 'cs.MA', 'cs.SY', 'eess.SP', 'eess.SY', 'stat.ML']",2020-05-02 22:44:50+00:00
http://arxiv.org/abs/2005.05092v2,Active Training of Physics-Informed Neural Networks to Aggregate and Interpolate Parametric Solutions to the Navier-Stokes Equations,"The goal of this work is to train a neural network which approximates
solutions to the Navier-Stokes equations across a region of parameter space, in
which the parameters define physical properties such as domain shape and
boundary conditions. The contributions of this work are threefold:
  1) To demonstrate that neural networks can be efficient aggregators of whole
families of parameteric solutions to physical problems, trained using data
created with traditional, trusted numerical methods such as finite elements.
Advantages include extremely fast evaluation of pressure and velocity at any
point in physical and parameter space (asymptotically, ~3 $\mu s$ / query), and
data compression (the network requires 99\% less storage space compared to its
own training data).
  2) To demonstrate that the neural networks can accurately interpolate between
finite element solutions in parameter space, allowing them to be instantly
queried for pressure and velocity field solutions to problems for which
traditional simulations have never been performed.
  3) To introduce an active learning algorithm, so that during training, a
finite element solver can automatically be queried to obtain additional
training data in locations where the neural network's predictions are in most
need of improvement, thus autonomously acquiring and efficiently distributing
training data throughout parameter space.
  In addition to the obvious utility of Item 2, above, we demonstrate an
application of the network in rapid parameter sweeping, very precisely
predicting the degree of narrowing in a tube which would result in a 50\%
increase in end-to-end pressure difference at a given flow rate. This
capability could have applications in both medical diagnosis of arterial
disease, and in computer-aided design.","['Christopher J Arthurs', 'Andrew P King']","['physics.comp-ph', 'cs.CE', 'cs.LG', 'stat.ML']",2020-05-02 21:53:39+00:00
http://arxiv.org/abs/2005.00865v1,Neural Differential Equations for Single Image Super-resolution,"Although Neural Differential Equations have shown promise on toy problems
such as MNIST, they have yet to be successfully applied to more challenging
tasks. Inspired by variational methods for image restoration relying on partial
differential equations, we choose to benchmark several forms of Neural DEs and
backpropagation methods on single image super-resolution. The adjoint method
previously proposed for gradient estimation has no theoretical stability
guarantees; we find a practical case where this makes it unusable, and show
that discrete sensitivity analysis has better stability. In our experiments,
differential models match the performance of a state-of-the art
super-resolution model.",['Teven Le Scao'],"['eess.IV', 'cs.LG', 'stat.ML', 'G.1.8; I.4.4']",2020-05-02 15:46:45+00:00
http://arxiv.org/abs/2005.00845v1,Deep Convolutional Neural Networks to Diagnose COVID-19 and other Pneumonia Diseases from Posteroanterior Chest X-Rays,"The article explores different deep convolutional neural network
architectures trained and tested on posteroanterior chest X-rays of 327
patients who are healthy (152 patients), diagnosed with COVID-19 (125), and
other types of pneumonia (48). In particular, this paper looks at the deep
convolutional neural networks VGG16 and VGG19, InceptionResNetV2 and
InceptionV3, as well as Xception, all followed by a flat multi-layer perceptron
and a final 30% drop-out. The paper has found that the best performing network
is VGG16 with a final $30$% drop-out trained over 3 classes (COVID-19, No
Finding, Other Pneumonia). It has an internal cross-validated accuracy of
$93.9(\pm3.4)$%, a COVID-19 sensitivity of $87.7(-1.9,+2)$%, and a No Finding
sensitivity of $96.8(\pm0.8)$%. The respective external cross-validated values
are $84.1(\pm13.5)$%, $87.7(-1.9,2)$%, and $96.8(\pm0.8)$%. The model optimizer
was Adam with a 1e-4 learning rate, and categorical cross-entropy loss. It is
hoped that, once this research will be put to practice in hospitals, healthcare
professionals will be able in the medium to long-term to diagnosing through
machine learning tools possible pneumonia, and if detected, whether it is
linked to a COVID-19 infection, allowing the detection of new possible COVID-19
foyers after the end of possible ""stop-and-go"" lockdowns as expected by until a
vaccine is found and widespread. Furthermore, in the short-term, it is hoped
practitioners can compare the diagnosis from the deep convolutional neural
networks with possible RT-PCR testing results, and if clashing, a Computed
Tomography could be performed as they are more accurate in showing COVID-19
pneumonia.",['Pierre G. B. Moutounet-Cartan'],"['eess.IV', 'cs.LG', 'stat.ML']",2020-05-02 14:42:50+00:00
http://arxiv.org/abs/2005.00826v1,Learning Model Predictive Control for Competitive Autonomous Racing,"The goal of this thesis is to design a learning model predictive controller
(LMPC) that allows multiple agents to race competitively on a predefined race
track in real-time. This thesis addresses two major shortcomings in the already
existing single-agent formulation. Previously, the agent determines a locally
optimal trajectory but does not explore the state space, which may be necessary
for overtaking maneuvers. Additionally, obstacle avoidance for LMPC has been
achieved in the past by using a non-convex terminal set, which increases the
complexity for determining a solution to the optimization problem. The proposed
algorithm for multi-agent racing explores the state space by executing the LMPC
for multiple different initializations, which yields a richer terminal safe
set. Furthermore, a new method for selecting states in the terminal set is
developed, which keeps the convexity for the terminal safe set and allows for
taking suboptimal states.",['Lukas Brunke'],"['cs.LG', 'cs.RO', 'math.OC', 'stat.ML']",2020-05-02 13:05:31+00:00
http://arxiv.org/abs/2005.00817v4,A survey on modern trainable activation functions,"In neural networks literature, there is a strong interest in identifying and
defining activation functions which can improve neural network performance. In
recent years there has been a renovated interest of the scientific community in
investigating activation functions which can be trained during the learning
process, usually referred to as ""trainable"", ""learnable"" or ""adaptable""
activation functions. They appear to lead to better network performance.
Diverse and heterogeneous models of trainable activation function have been
proposed in the literature. In this paper, we present a survey of these models.
Starting from a discussion on the use of the term ""activation function"" in
literature, we propose a taxonomy of trainable activation functions, highlight
common and distinctive proprieties of recent and past models, and discuss main
advantages and limitations of this type of approach. We show that many of the
proposed approaches are equivalent to adding neuron layers which use fixed
(non-trainable) activation functions and some simple local rule that
constraints the corresponding weight layers.","['Andrea Apicella', 'Francesco Donnarumma', 'Francesco Isgrò', 'Roberto Prevete']","['cs.LG', 'cs.NE', 'stat.ML']",2020-05-02 12:38:43+00:00
http://arxiv.org/abs/2005.00797v2,Multi-consensus Decentralized Accelerated Gradient Descent,"This paper considers the decentralized convex optimization problem, which has
a wide range of applications in large-scale machine learning, sensor networks,
and control theory. We propose novel algorithms that achieve optimal
computation complexity and near optimal communication complexity. Our
theoretical results give affirmative answers to the open problem on whether
there exists an algorithm that can achieve a communication complexity (nearly)
matching the lower bound depending on the global condition number instead of
the local one. Furthermore, the linear convergence of our algorithms only
depends on the strong convexity of global objective and it does \emph{not}
require the local functions to be convex. The design of our methods relies on a
novel integration of well-known techniques including Nesterov's acceleration,
multi-consensus and gradient-tracking. Empirical studies show the
outperformance of our methods for machine learning applications.","['Haishan Ye', 'Luo Luo', 'Ziang Zhou', 'Tong Zhang']","['cs.LG', 'math.OC', 'stat.ML']",2020-05-02 11:10:32+00:00
http://arxiv.org/abs/2005.00792v4,ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,"Event forecasting is a challenging, yet important task, as humans seek to
constantly plan for the future. Existing automated forecasting studies rely
mostly on structured data, such as time-series or event-based knowledge graphs,
to help predict future events. In this work, we aim to formulate a task,
construct a dataset, and provide benchmarks for developing methods for event
forecasting with large volumes of unstructured text data. To simulate the
forecasting scenario on temporal news documents, we formulate the problem as a
restricted-domain, multiple-choice, question-answering (QA) task. Unlike
existing QA tasks, our task limits accessible information, and thus a model has
to make a forecasting judgement. To showcase the usefulness of this task
formulation, we introduce ForecastQA, a question-answering dataset consisting
of 10,392 event forecasting questions, which have been collected and verified
via crowdsourcing efforts. We present our experiments on ForecastQA using
BERT-based models and find that our best model achieves 60.1% accuracy on the
dataset, which still lags behind human performance by about 19%. We hope
ForecastQA will support future research efforts in bridging this gap.","['Woojeong Jin', 'Rahul Khanna', 'Suji Kim', 'Dong-Ho Lee', 'Fred Morstatter', 'Aram Galstyan', 'Xiang Ren']","['cs.LG', 'stat.ML']",2020-05-02 11:03:40+00:00
http://arxiv.org/abs/2005.00784v1,Ball k-means,"This paper presents a novel accelerated exact k-means algorithm called the
Ball k-means algorithm, which uses a ball to describe a cluster, focusing on
reducing the point-centroid distance computation. The Ball k-means can
accurately find the neighbor clusters for each cluster resulting distance
computations only between a point and its neighbor clusters' centroids instead
of all centroids. Moreover, each cluster can be divided into a stable area and
an active area, and the later one can be further divided into annulus areas.
The assigned cluster of the points in the stable area is not changed in the
current iteration while the points in the annulus area will be adjusted within
a few neighbor clusters in the current iteration. Also, there are no upper or
lower bounds in the proposed Ball k-means. Furthermore, reducing
centroid-centroid distance computation between iterations makes it efficient
for large k clustering. The fast speed, no extra parameters and simple design
of the Ball k-means make it an all-around replacement of the naive k-means
algorithm.","['Shuyin Xia', 'Daowan Peng', 'Deyu Meng', 'Changqing Zhang', 'Guoyin Wang', 'Zizhong Chen', 'Wei Wei']","['cs.LG', 'stat.ML']",2020-05-02 10:39:26+00:00
http://arxiv.org/abs/2005.00783v2,Differentially Private Generation of Small Images,"We explore the training of generative adversarial networks with differential
privacy to anonymize image data sets. On MNIST, we numerically measure the
privacy-utility trade-off using parameters from $\epsilon$-$\delta$
differential privacy and the inception score. Our experiments uncover a
saturated training regime where an increasing privacy budget adds little to the
quality of generated images. We also explain analytically why differentially
private Adam optimization is independent of the gradient clipping parameter.
Furthermore, we highlight common errors in previous works on differentially
private deep learning, which we uncovered in recent literature. Throughout the
treatment of the subject, we hope to prevent erroneous estimates of anonymity
in the future.","['Justus T. C. Schwabedal', 'Pascal Michel', 'Mario S. Riontino']","['cs.LG', 'cs.CR', 'stat.ML']",2020-05-02 10:37:46+00:00
http://arxiv.org/abs/2005.00718v1,Large-scale Uncertainty Estimation and Its Application in Revenue Forecast of SMEs,"The economic and banking importance of the small and medium enterprise (SME)
sector is well recognized in contemporary society. Business credit loans are
very important for the operation of SMEs, and the revenue is a key indicator of
credit limit management. Therefore, it is very beneficial to construct a
reliable revenue forecasting model. If the uncertainty of an enterprise's
revenue forecasting can be estimated, a more proper credit limit can be
granted. Natural gradient boosting approach, which estimates the uncertainty of
prediction by a multi-parameter boosting algorithm based on the natural
gradient. However, its original implementation is not easy to scale into big
data scenarios, and computationally expensive compared to state-of-the-art
tree-based models (such as XGBoost). In this paper, we propose a Scalable
Natural Gradient Boosting Machines that is simple to implement, readily
parallelizable, interpretable and yields high-quality predictive uncertainty
estimates. According to the characteristics of revenue distribution, we derive
an uncertainty quantification function. We demonstrate that our method can
distinguish between samples that are accurate and inaccurate on revenue
forecasting of SMEs. What's more, interpretability can be naturally obtained
from the model, satisfying the financial needs.","['Zebang Zhang', 'Kui Zhao', 'Kai Huang', 'Quanhui Jia', 'Yanming Fang', 'Quan Yu']","['cs.LG', 'stat.ML']",2020-05-02 06:17:44+00:00
http://arxiv.org/abs/2005.00695v3,On the Generalization Effects of Linear Transformations in Data Augmentation,"Data augmentation is a powerful technique to improve performance in
applications such as image and text classification tasks. Yet, there is little
rigorous understanding of why and how various augmentations work. In this work,
we consider a family of linear transformations and study their effects on the
ridge estimator in an over-parametrized linear regression setting. First, we
show that transformations that preserve the labels of the data can improve
estimation by enlarging the span of the training data. Second, we show that
transformations that mix data can improve estimation by playing a
regularization effect. Finally, we validate our theoretical insights on MNIST.
Based on the insights, we propose an augmentation scheme that searches over the
space of transformations by how uncertain the model is about the transformed
data. We validate our proposed scheme on image and text datasets. For example,
our method outperforms random sampling methods by 1.24% on CIFAR-100 using
Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA
Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.","['Sen Wu', 'Hongyang R. Zhang', 'Gregory Valiant', 'Christopher Ré']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2020-05-02 04:10:21+00:00
http://arxiv.org/abs/2005.00687v7,Open Graph Benchmark: Datasets for Machine Learning on Graphs,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and
realistic benchmark datasets to facilitate scalable, robust, and reproducible
graph machine learning (ML) research. OGB datasets are large-scale, encompass
multiple important graph ML tasks, and cover a diverse range of domains,
ranging from social and information networks to biological networks, molecular
graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a
unified evaluation protocol using meaningful application-specific data splits
and evaluation metrics. In addition to building the datasets, we also perform
extensive benchmark experiments for each dataset. Our experiments suggest that
OGB datasets present significant challenges of scalability to large-scale
graphs and out-of-distribution generalization under realistic data splits,
indicating fruitful opportunities for future research. Finally, OGB provides an
automated end-to-end graph ML pipeline that simplifies and standardizes the
process of graph data loading, experimental setup, and model evaluation. OGB
will be regularly updated and welcomes inputs from the community. OGB datasets
as well as data loaders, evaluation scripts, baseline code, and leaderboards
are publicly available at https://ogb.stanford.edu .","['Weihua Hu', 'Matthias Fey', 'Marinka Zitnik', 'Yuxiao Dong', 'Hongyu Ren', 'Bowen Liu', 'Michele Catasta', 'Jure Leskovec']","['cs.LG', 'cs.SI', 'stat.ML']",2020-05-02 03:09:50+00:00
http://arxiv.org/abs/2005.00670v1,Stochastic Neighbor Embedding of Multimodal Relational Data for Image-Text Simultaneous Visualization,"Multimodal relational data analysis has become of increasing importance in
recent years, for exploring across different domains of data, such as images
and their text tags obtained from social networking services (e.g., Flickr). A
variety of data analysis methods have been developed for visualization; to give
an example, t-Stochastic Neighbor Embedding (t-SNE) computes low-dimensional
feature vectors so that their similarities keep those of the observed data
vectors. However, t-SNE is designed only for a single domain of data but not
for multimodal data; this paper aims at visualizing multimodal relational data
consisting of data vectors in multiple domains with relations across these
vectors. By extending t-SNE, we herein propose Multimodal Relational Stochastic
Neighbor Embedding (MR-SNE), that (1) first computes augmented relations, where
we observe the relations across domains and compute those within each of
domains via the observed data vectors, and (2) jointly embeds the augmented
relations to a low-dimensional space. Through visualization of Flickr and
Animal with Attributes 2 datasets, proposed MR-SNE is compared with other graph
embedding-based approaches; MR-SNE demonstrates the promising performance.","['Morihiro Mizutani', 'Akifumi Okuno', 'Geewook Kim', 'Hidetoshi Shimodaira']","['cs.LG', 'cs.CL', 'cs.CV', 'cs.HC', 'stat.ML']",2020-05-02 00:39:29+00:00
http://arxiv.org/abs/2005.00653v1,A Transformer-based Approach for Source Code Summarization,"Generating a readable summary that describes the functionality of a program
is known as source code summarization. In this task, learning code
representation by modeling the pairwise relationship between code tokens to
capture their long-range dependencies is crucial. To learn code representation
for summarization, we explore the Transformer model that uses a self-attention
mechanism and has shown to be effective in capturing long-range dependencies.
In this work, we show that despite the approach is simple, it outperforms the
state-of-the-art techniques by a significant margin. We perform extensive
analysis and ablation studies that reveal several important findings, e.g., the
absolute encoding of source code tokens' position hinders, while relative
encoding significantly improves the summarization performance. We have made our
code publicly available to facilitate future research.","['Wasi Uddin Ahmad', 'Saikat Chakraborty', 'Baishakhi Ray', 'Kai-Wei Chang']","['cs.SE', 'cs.AI', 'cs.LG', 'stat.ML']",2020-05-01 23:29:36+00:00
http://arxiv.org/abs/2005.00631v1,Evaluating and Aggregating Feature-based Model Explanations,"A feature-based model explanation denotes how much each input feature
contributes to a model's output for a given data point. As the number of
proposed explanation functions grows, we lack quantitative evaluation criteria
to help practitioners know when to use which explanation function. This paper
proposes quantitative evaluation criteria for feature-based explanations: low
sensitivity, high faithfulness, and low complexity. We devise a framework for
aggregating explanation functions. We develop a procedure for learning an
aggregate explanation function with lower complexity and then derive a new
aggregate Shapley value explanation function that minimizes sensitivity.","['Umang Bhatt', 'Adrian Weller', 'José M. F. Moura']","['cs.LG', 'cs.AI', 'cs.CY', 'stat.ML']",2020-05-01 21:56:36+00:00
http://arxiv.org/abs/2005.00616v1,Robust Deep Learning as Optimal Control: Insights and Convergence Guarantees,"The fragility of deep neural networks to adversarially-chosen inputs has
motivated the need to revisit deep learning algorithms. Including adversarial
examples during training is a popular defense mechanism against adversarial
attacks. This mechanism can be formulated as a min-max optimization problem,
where the adversary seeks to maximize the loss function using an iterative
first-order algorithm while the learner attempts to minimize it. However,
finding adversarial examples in this way causes excessive computational
overhead during training. By interpreting the min-max problem as an optimal
control problem, it has recently been shown that one can exploit the
compositional structure of neural networks in the optimization problem to
improve the training time significantly. In this paper, we provide the first
convergence analysis of this adversarial training algorithm by combining
techniques from robust optimal control and inexact oracle methods in
optimization. Our analysis sheds light on how the hyperparameters of the
algorithm affect the its stability and convergence. We support our insights
with experiments on a robust classification problem.","['Jacob H. Seidman', 'Mahyar Fazlyab', 'Victor M. Preciado', 'George J. Pappas']","['math.OC', 'cs.LG', 'stat.ML']",2020-05-01 21:26:38+00:00
http://arxiv.org/abs/2005.00615v2,A Dual-Dimer Method for Training Physics-Constrained Neural Networks with Minimax Architecture,"Data sparsity is a common issue to train machine learning tools such as
neural networks for engineering and scientific applications, where experiments
and simulations are expensive. Recently physics-constrained neural networks
(PCNNs) were developed to reduce the required amount of training data. However,
the weights of different losses from data and physical constraints are adjusted
empirically in PCNNs. In this paper, a new physics-constrained neural network
with the minimax architecture (PCNN-MM) is proposed so that the weights of
different losses can be adjusted systematically. The training of the PCNN-MM is
searching the high-order saddle points of the objective function. A novel
saddle point search algorithm called Dual-Dimer method is developed. It is
demonstrated that the Dual-Dimer method is computationally more efficient than
the gradient descent ascent method for nonconvex-nonconcave functions and
provides additional eigenvalue information to verify search results. A heat
transfer example also shows that the convergence of PCNN-MMs is faster than
that of traditional PCNNs.","['Dehao Liu', 'Yan Wang']","['cs.LG', 'stat.ML']",2020-05-01 21:26:04+00:00
http://arxiv.org/abs/2005.00611v4,Neural Lyapunov Control,"We propose new methods for learning control policies and neural network
Lyapunov functions for nonlinear control problems, with provable guarantee of
stability. The framework consists of a learner that attempts to find the
control and Lyapunov functions, and a falsifier that finds counterexamples to
quickly guide the learner towards solutions. The procedure terminates when no
counterexample is found by the falsifier, in which case the controlled
nonlinear system is provably stable. The approach significantly simplifies the
process of Lyapunov control design, provides end-to-end correctness guarantee,
and can obtain much larger regions of attraction than existing methods such as
LQR and SOS/SDP. We show experiments on how the new methods obtain high-quality
solutions for challenging control problems.","['Ya-Chien Chang', 'Nima Roohi', 'Sicun Gao']","['cs.LG', 'cs.NE', 'cs.RO', 'cs.SY', 'eess.SY', 'stat.ML']",2020-05-01 21:18:39+00:00
http://arxiv.org/abs/2005.00605v1,"Rejoinder for the discussion of the paper ""A novel algorithmic approach to Bayesian Logic Regression""","In this rejoinder we summarize the comments, questions and remarks on the
paper ""A novel algorithmic approach to Bayesian Logic Regression"" from the
discussants. We then respond to those comments, questions and remarks, provide
several extensions of the original model and give a tutorial on our R-package
EMJMCMC (http://aliaksah.github.io/EMJMCMC2016/)","['Aliaksandr Hubin', 'Geir Storvik', 'Florian Frommlet']","['stat.ME', 'math.LO', 'stat.CO', 'stat.ML']",2020-05-01 20:59:56+00:00
http://arxiv.org/abs/2005.00596v1,Learning from Noisy Labels with Noise Modeling Network,"Multi-label image classification has generated significant interest in recent
years and the performance of such systems often suffers from the not so
infrequent occurrence of incorrect or missing labels in the training data. In
this paper, we extend the state-of the-art of training classifiers to jointly
deal with both forms of errorful data. We accomplish this by modeling noisy and
missing labels in multi-label images with a new Noise Modeling Network (NMN)
that follows our convolutional neural network (CNN), integrates with it,
forming an end-to-end deep learning system, which can jointly learn the noise
distribution and CNN parameters. The NMN learns the distribution of noise
patterns directly from the noisy data without the need for any clean training
data. The NMN can model label noise that depends only on the true label or is
also dependent on the image features. We show that the integrated NMN/CNN
learning system consistently improves the classification performance, for
different levels of label noise, on the MSR-COCO dataset and MSR-VTT dataset.
We also show that noise performance improvements are obtained when multiple
instance learning methods are used.","['Zhuolin Jiang', 'Jan Silovsky', 'Man-Hung Siu', 'William Hartmann', 'Herbert Gish', 'Sancar Adali']","['cs.CV', 'cs.LG', 'stat.ML']",2020-05-01 20:32:22+00:00
http://arxiv.org/abs/2005.00592v1,Integrated Time Series Summarization and Prediction Algorithm and its Application to COVID-19 Data Mining,"This paper proposes a simple method to extract from a set of multiple related
time series a compressed representation for each time series based on
statistics for the entire set of all time series. This is achieved by a
hierarchical algorithm that first generates an alphabet of shapelets based on
the segmentation of centroids for clustered data, before labels of these
shapelets are assigned to the segmentation of each single time series via
nearest neighbor search using unconstrained dynamic time warping as distance
measure to deal with non-uniform time series lenghts. Thereby, a sequence of
labels is assigned for each time series. Completion of the last label sequence
permits prediction of individual time series. Proposed method is evaluated on
two global COVID-19 datasets, first, for the number of daily net cases (daily
new infections minus daily recoveries), and, second, for the number of daily
deaths attributed to COVID-19 as of April 27, 2020. The first dataset involves
249 time series for different countries, each of length 96. The second dataset
involves 264 time series, each of length 96. Based on detected anomalies in
available data a decentralized exit strategy from lockdowns is advocated.",['Mogens Graf Plessen'],"['stat.AP', 'cs.LG', 'stat.ML']",2020-05-01 20:16:39+00:00
http://arxiv.org/abs/2005.00585v1,Improving Robustness via Risk Averse Distributional Reinforcement Learning,"One major obstacle that precludes the success of reinforcement learning in
real-world applications is the lack of robustness, either to model
uncertainties or external disturbances, of the trained policies. Robustness is
critical when the policies are trained in simulations instead of real world
environment. In this work, we propose a risk-aware algorithm to learn robust
policies in order to bridge the gap between simulation training and real-world
implementation. Our algorithm is based on recently discovered distributional RL
framework. We incorporate CVaR risk measure in sample based distributional
policy gradients (SDPG) for learning risk-averse policies to achieve robustness
against a range of system disturbances. We validate the robustness of
risk-aware SDPG on multiple environments.","['Rahul Singh', 'Qinsheng Zhang', 'Yongxin Chen']","['cs.LG', 'stat.ML']",2020-05-01 20:03:10+00:00
http://arxiv.org/abs/2005.00570v1,When Ensembling Smaller Models is More Efficient than Single Large Models,"Ensembling is a simple and popular technique for boosting evaluation
performance by training multiple models (e.g., with different initializations)
and aggregating their predictions. This approach is commonly reserved for the
largest models, as it is commonly held that increasing the model size provides
a more substantial reduction in error than ensembling smaller models. However,
we show results from experiments on CIFAR-10 and ImageNet that ensembles can
outperform single models with both higher accuracy and requiring fewer total
FLOPs to compute, even when those individual models' weights and
hyperparameters are highly optimized. Furthermore, this gap in improvement
widens as models become large. This presents an interesting observation that
output diversity in ensembling can often be more efficient than training larger
models, especially when the models approach the size of what their dataset can
foster. Instead of using the common practice of tuning a single large model,
one can use ensembles as a more flexible trade-off between a model's inference
speed and accuracy. This also potentially eases hardware design, e.g., an
easier way to parallelize the model across multiple workers for real-time or
distributed inference.","['Dan Kondratyuk', 'Mingxing Tan', 'Matthew Brown', 'Boqing Gong']","['cs.LG', 'cs.CV', 'stat.ML']",2020-05-01 18:56:18+00:00
http://arxiv.org/abs/2005.00568v2,Adversarial domain adaptation to reduce sample bias of a high energy physics classifier,"We apply adversarial domain adaptation in unsupervised setting to reduce
sample bias in a supervised high energy physics events classifier training. We
make use of a neural network containing event and domain classifier with a
gradient reversal layer to simultaneously enable signal versus background
events classification on the one hand, while on the other hand minimising the
difference in response of the network to background samples originating from
different MC models via adversarial domain classification loss. We show the
successful bias removal on the example of simulated events at the LHC with
$t\bar{t}H$ signal versus $t\bar{t}b\bar{b}$ background classification and
discuss implications and limitations of the method","['Jose M. Clavijo', 'Paul Glaysher', 'Judith M. Katzy', 'Jenia Jitsev']","['stat.ML', 'cs.LG', 'hep-ex', 'hep-ph']",2020-05-01 18:46:12+00:00
http://arxiv.org/abs/2005.00545v1,Low-Dimensional Hyperbolic Knowledge Graph Embeddings,"Knowledge graph (KG) embeddings learn low-dimensional representations of
entities and relations to predict missing facts. KGs often exhibit hierarchical
and logical patterns which must be preserved in the embedding space. For
hierarchical data, hyperbolic embedding methods have shown promise for
high-fidelity and parsimonious representations. However, existing hyperbolic
embedding methods do not account for the rich logical patterns in KGs. In this
work, we introduce a class of hyperbolic KG embedding models that
simultaneously capture hierarchical and logical patterns. Our approach combines
hyperbolic reflections and rotations with attention to model complex relational
patterns. Experimental results on standard KG benchmarks show that our method
improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in
mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that
different geometric transformations capture different types of relations while
attention-based transformations generalize to multiple relations. In high
dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR
and 57.7% on YAGO3-10.","['Ines Chami', 'Adva Wolf', 'Da-Cheng Juan', 'Frederic Sala', 'Sujith Ravi', 'Christopher Ré']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2020-05-01 18:00:02+00:00
http://arxiv.org/abs/2005.00527v2,Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?,"Learning to plan for long horizons is a central challenge in episodic
reinforcement learning problems. A fundamental question is to understand how
the difficulty of the problem scales as the horizon increases. Here the natural
measure of sample complexity is a normalized one: we are interested in the
number of episodes it takes to provably discover a policy whose value is
$\varepsilon$ near to that of the optimal value, where the value is measured by
the normalized cumulative reward in each episode. In a COLT 2018 open problem,
Jiang and Agarwal conjectured that, for tabular, episodic reinforcement
learning problems, there exists a sample complexity lower bound which exhibits
a polynomial dependence on the horizon -- a conjecture which is consistent with
all known sample complexity upper bounds. This work refutes this conjecture,
proving that tabular, episodic reinforcement learning is possible with a sample
complexity that scales only logarithmically with the planning horizon. In other
words, when the values are appropriately normalized (to lie in the unit
interval), this results shows that long horizon RL is no more difficult than
short horizon RL, at least in a minimax sense. Our analysis introduces two
ideas: (i) the construction of an $\varepsilon$-net for optimal policies whose
log-covering number scales only logarithmically with the planning horizon, and
(ii) the Online Trajectory Synthesis algorithm, which adaptively evaluates all
policies in a given policy class using sample complexity that scales with the
log-covering number of the given policy class. Both may be of independent
interest.","['Ruosong Wang', 'Simon S. Du', 'Lin F. Yang', 'Sham M. Kakade']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2020-05-01 17:56:38+00:00
http://arxiv.org/abs/2005.00502v1,Partially-Typed NER Datasets Integration: Connecting Practice to Theory,"While typical named entity recognition (NER) models require the training set
to be annotated with all target types, each available datasets may only cover a
part of them. Instead of relying on fully-typed NER datasets, many efforts have
been made to leverage multiple partially-typed ones for training and allow the
resulting model to cover a full type set. However, there is neither guarantee
on the quality of integrated datasets, nor guidance on the design of training
algorithms. Here, we conduct a systematic analysis and comparison between
partially-typed NER datasets and fully-typed ones, in both theoretical and
empirical manner. Firstly, we derive a bound to establish that models trained
with partially-typed annotations can reach a similar performance with the ones
trained with fully-typed annotations, which also provides guidance on the
algorithm design. Moreover, we conduct controlled experiments, which shows
partially-typed datasets leads to similar performance with the model trained
with the same amount of fully-typed annotations","['Shi Zhi', 'Liyuan Liu', 'Yu Zhang', 'Shiyin Wang', 'Qi Li', 'Chao Zhang', 'Jiawei Han']","['cs.LG', 'cs.CL', 'stat.ML']",2020-05-01 17:16:18+00:00
http://arxiv.org/abs/2005.00497v4,The Grammar of Interactive Explanatory Model Analysis,"The growing need for in-depth analysis of predictive models leads to a series
of new methods for explaining their local and global properties. Which of these
methods is the best? It turns out that this is an ill-posed question. One
cannot sufficiently explain a black-box machine learning model using a single
method that gives only one perspective. Isolated explanations are prone to
misunderstanding, leading to wrong or simplistic reasoning. This problem is
known as the Rashomon effect and refers to diverse, even contradictory,
interpretations of the same phenomenon. Surprisingly, most methods developed
for explainable and responsible machine learning focus on a single-aspect of
the model behavior. In contrast, we showcase the problem of explainability as
an interactive and sequential analysis of a model. This paper proposes how
different Explanatory Model Analysis (EMA) methods complement each other and
discusses why it is essential to juxtapose them. The introduced process of
Interactive EMA (IEMA) derives from the algorithmic side of explainable machine
learning and aims to embrace ideas developed in cognitive sciences. We
formalize the grammar of IEMA to describe potential human-model dialogues. It
is implemented in a widely used human-centered open-source software framework
that adopts interactivity, customizability and automation as its main traits.
We conduct a user study to evaluate the usefulness of IEMA, which indicates
that an interactive sequential analysis of a model increases the performance
and confidence of human decision making.","['Hubert Baniecki', 'Dariusz Parzych', 'Przemyslaw Biecek']","['cs.LG', 'cs.HC', 'stat.ML']",2020-05-01 17:12:22+00:00
http://arxiv.org/abs/2005.00478v3,DriveML: An R Package for Driverless Machine Learning,"In recent years, the concept of automated machine learning has become very
popular. Automated Machine Learning (AutoML) mainly refers to the automated
methods for model selection and hyper-parameter optimization of various
algorithms such as random forests, gradient boosting, neural networks, etc. In
this paper, we introduce a new package i.e. DriveML for automated machine
learning. DriveML helps in implementing some of the pillars of an automated
machine learning pipeline such as automated data preparation, feature
engineering, model building and model explanation by running the function
instead of writing lengthy R codes. The DriveML package is available in CRAN.
We compare the DriveML package with other relevant packages in CRAN/Github and
find that DriveML performs the best across different parameters. We also
provide an illustration by applying the DriveML package with default
configuration on a real world dataset. Overall, the main benefits of DriveML
are in development time savings, reduce developer's errors, optimal tuning of
machine learning models and reproducibility.","['Sayan Putatunda', 'Dayananda Ubrangala', 'Kiran Rama', 'Ravi Kondapalli']","['cs.LG', 'stat.ML']",2020-05-01 16:40:25+00:00
http://arxiv.org/abs/2005.00466v2,Thresholded Adaptive Validation: Tuning the Graphical Lasso for Graph Recovery,"Many Machine Learning algorithms are formulated as regularized optimization
problems, but their performance hinges on a regularization parameter that needs
to be calibrated to each application at hand. In this paper, we propose a
general calibration scheme for regularized optimization problems and apply it
to the graphical lasso, which is a method for Gaussian graphical modeling. The
scheme is equipped with theoretical guarantees and motivates a thresholding
pipeline that can improve graph recovery. Moreover, requiring at most one line
search over the regularization path, the calibration scheme is computationally
more efficient than competing schemes that are based on resampling. Finally, we
show in simulations that our approach can improve on the graph recovery of
other approaches considerably.","['Mike Laszkiewicz', 'Asja Fischer', 'Johannes Lederer']","['stat.ML', 'cs.LG', 'stat.ME']",2020-05-01 15:59:47+00:00
http://arxiv.org/abs/2005.00447v2,Image fusion using symmetric skip autoencodervia an Adversarial Regulariser,"It is a challenging task to extract the best of both worlds by combining the
spatial characteristics of a visible image and the spectral content of an
infrared image. In this work, we propose a spatially constrained adversarial
autoencoder that extracts deep features from the infrared and visible images to
obtain a more exhaustive and global representation. In this paper, we propose a
residual autoencoder architecture, regularised by a residual adversarial
network, to generate a more realistic fused image. The residual module serves
as primary building for the encoder, decoder and adversarial network, as an add
on the symmetric skip connections perform the functionality of embedding the
spatial characteristics directly from the initial layers of encoder structure
to the decoder part of the network. The spectral information in the infrared
image is incorporated by adding the feature maps over several layers in the
encoder part of the fusion structure, which makes inference on both the visual
and infrared images separately. In order to efficiently optimize the parameters
of the network, we propose an adversarial regulariser network which would
perform supervised learning on the fused image and the original visual image.","['Snigdha Bhagat', 'S. D. Joshi', 'Brejesh Lall']","['stat.ML', 'cs.LG', 'eess.IV']",2020-05-01 15:31:45+00:00
http://arxiv.org/abs/2005.00397v2,Multi-View Self-Attention for Interpretable Drug-Target Interaction Prediction,"The drug discovery stage is a vital aspect of the drug development process
and forms part of the initial stages of the development pipeline. In recent
times, machine learning-based methods are actively being used to model
drug-target interactions for rational drug discovery due to the successful
application of these methods in other domains. In machine learning approaches,
the numerical representation of molecules is critical to the performance of the
model. While significant progress has been made in molecular representation
engineering, this has resulted in several descriptors for both targets and
compounds. Also, the interpretability of model predictions is a vital feature
that could have several pharmacological applications. In this study, we propose
a self-attention-based multi-view representation learning approach for modeling
drug-target interactions. We evaluated our approach using three benchmark
kinase datasets and compared the proposed method to some baseline models. Our
experimental results demonstrate the ability of our method to achieve
competitive prediction performance and offer biologically plausible drug-target
interaction interpretations.","['Brighter Agyemang', 'Wei-Ping Wu', 'Michael Yelpengne Kpiebaareh', 'Zhihua Lei', 'Ebenezer Nanor', 'Lei Chen']","['cs.LG', 'stat.ML']",2020-05-01 14:28:17+00:00
http://arxiv.org/abs/2005.00393v2,Can a powerful neural network be a teacher for a weaker neural network?,"The transfer learning technique is widely used to learning in one context and
applying it to another, i.e. the capacity to apply acquired knowledge and
skills to new situations. But is it possible to transfer the learning from a
deep neural network to a weaker neural network? Is it possible to improve the
performance of a weak neural network using the knowledge acquired by a more
powerful neural network? In this work, during the training process of a weak
network, we add a loss function that minimizes the distance between the
features previously learned from a strong neural network with the features that
the weak network must try to learn. To demonstrate the effectiveness and
robustness of our approach, we conducted a large number of experiments using
three known datasets and demonstrated that a weak neural network can increase
its performance if its learning process is driven by a more powerful neural
network.","['Nicola Landro', 'Ignazio Gallo', 'Riccardo La Grassa']","['cs.LG', 'stat.ML']",2020-05-01 14:19:40+00:00
http://arxiv.org/abs/2005.00386v4,Scaled Vecchia approximation for fast computer-model emulation,"Many scientific phenomena are studied using computer experiments consisting
of multiple runs of a computer model while varying the input settings. Gaussian
processes (GPs) are a popular tool for the analysis of computer experiments,
enabling interpolation between input settings, but direct GP inference is
computationally infeasible for large datasets. We adapt and extend a powerful
class of GP methods from spatial statistics to enable the scalable analysis and
emulation of large computer experiments. Specifically, we apply Vecchia's
ordered conditional approximation in a transformed input space, with each input
scaled according to how strongly it relates to the computer-model response. The
scaling is learned from the data, by estimating parameters in the GP covariance
function using Fisher scoring. Our methods are highly scalable, enabling
estimation, joint prediction and simulation in near-linear time in the number
of model runs. In several numerical examples, our approach substantially
outperformed existing methods.","['Matthias Katzfuss', 'Joseph Guinness', 'Earl Lawrence']","['stat.ME', 'stat.CO', 'stat.ML']",2020-05-01 14:08:31+00:00
http://arxiv.org/abs/2005.01492v1,TRIPDECODER: Study Travel Time Attributes and Route Preferences of Metro Systems from Smart Card Data,"In this paper, we target at recovering the exact routes taken by commuters
inside a metro system that arenot captured by an Automated Fare Collection
(AFC) system and hence remain unknown. We strategicallypropose two inference
tasks to handle the recovering, one to infer the travel time of each travel
link thatcontributes to the total duration of any trip inside a metro network
and the other to infer the route preferencesbased on historical trip records
and the travel time of each travel link inferred in the previous inferencetask.
As these two inference tasks have interrelationship, most of existing works
perform these two taskssimultaneously. However, our solutionTripDecoderadopts a
totally different approach. To the best of ourknowledge,TripDecoderis the first
model that points out and fully utilizes the fact that there are some
tripsinside a metro system with only one practical route available. It
strategically decouples these two inferencetasks by only taking those trip
records with only one practical route as the input for the first inference
taskof travel time and feeding the inferred travel time to the second inference
task as an additional input whichnot only improves the accuracy but also
effectively reduces the complexity of both inference tasks. Twocase studies
have been performed based on the city-scale real trip records captured by the
AFC systems inSingapore and Taipei to compare the accuracy and efficiency
ofTripDecoderand its competitors. As expected,TripDecoderhas achieved the best
accuracy in both datasets, and it also demonstrates its superior efficiencyand
scalability.","['Xiancai Tian', 'Baihua Zheng', 'Yazhe Wang', 'Hsiao-Ting Huang', 'Chih-Chieh Hung']","['physics.soc-ph', 'cs.LG', 'stat.ML', 'I.5.1']",2020-05-01 08:39:48+00:00
http://arxiv.org/abs/2005.00259v1,Supervised Feature Subset Selection and Feature Ranking for Multivariate Time Series without Feature Extraction,"We introduce supervised feature ranking and feature subset selection
algorithms for multivariate time series (MTS) classification. Unlike most
existing supervised/unsupervised feature selection algorithms for MTS our
techniques do not require a feature extraction step to generate a
one-dimensional feature vector from the time series. Instead it is based on
directly computing similarity between individual time series and assessing how
well the resulting cluster structure matches the labels. The techniques are
amenable to heterogeneous MTS data, where the time series measurements may have
different sampling resolutions, and to multi-modal data.","['Shuchu Han', 'Alexandru Niculescu-Mizil']","['cs.LG', 'stat.ML']",2020-05-01 07:46:29+00:00
http://arxiv.org/abs/2005.01463v2,MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework,"We propose MeshfreeFlowNet, a novel deep learning-based super-resolution
framework to generate continuous (grid-free) spatio-temporal solutions from the
low-resolution inputs. While being computationally efficient, MeshfreeFlowNet
accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet
allows for: (i) the output to be sampled at all spatio-temporal resolutions,
(ii) a set of Partial Differential Equation (PDE) constraints to be imposed,
and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal
domains owing to its fully convolutional encoder. We empirically study the
performance of MeshfreeFlowNet on the task of super-resolution of turbulent
flows in the Rayleigh-Benard convection problem. Across a diverse set of
evaluation metrics, we show that MeshfreeFlowNet significantly outperforms
existing baselines. Furthermore, we provide a large scale implementation of
MeshfreeFlowNet and show that it efficiently scales across large clusters,
achieving 96.80% scaling efficiency on up to 128 GPUs and a training time of
less than 4 minutes.","['Chiyu Max Jiang', 'Soheil Esmaeilzadeh', 'Kamyar Azizzadenesheli', 'Karthik Kashinath', 'Mustafa Mustafa', 'Hamdi A. Tchelepi', 'Philip Marcus', 'Prabhat', 'Anima Anandkumar']","['cs.LG', 'eess.IV', 'physics.flu-dyn', 'stat.ML']",2020-05-01 05:29:25+00:00
http://arxiv.org/abs/2005.00220v2,Automatic Catalog of RRLyrae from $\sim$ 14 million VVV Light Curves: How far can we go with traditional machine-learning?,"The creation of a 3D map of the bulge using RRLyrae (RRL) is one of the main
goals of the VVV(X) surveys. The overwhelming number of sources under analysis
request the use of automatic procedures. In this context, previous works
introduced the use of Machine Learning (ML) methods for the variable star
classification. Our goal is the development and analysis of an automatic
procedure, based on ML, for the identification of RRLs in the VVV Survey. This
procedure will be use to generate reliable catalogs integrated over several
tiles in the survey. After the reconstruction of light-curves, we extract a set
of period and intensity-based features. We use for the first time a new subset
of pseudo color features. We discuss all the appropriate steps needed to define
our automatic pipeline: selection of quality measures; sampling procedures;
classifier setup and model selection. As final result, we construct an ensemble
classifier with an average Recall of 0.48 and average Precision of 0.86 over 15
tiles. We also make available our processed datasets and a catalog of candidate
RRLs. Perhaps most interestingly, from a classification perspective based on
photometric broad-band data, is that our results indicate that Color is an
informative feature type of the RRL that should be considered for automatic
classification methods via ML. We also argue that Recall and Precision in both
tables and curves are high quality metrics for this highly imbalanced problem.
Furthermore, we show for our VVV data-set that to have good estimates it is
important to use the original distribution more than reduced samples with an
artificial balance. Finally, we show that the use of ensemble classifiers helps
resolve the crucial model selection step, and that most errors in the
identification of RRLs are related to low quality observations of some sources
or to the difficulty to resolve the RRL-C type given the date.","['Juan B. Cabral', 'Felipe Ramos', 'Sebastián Gurovich', 'Pablo Granitto']","['astro-ph.IM', 'astro-ph.SR', 'cs.LG', 'stat.ML']",2020-05-01 04:35:57+00:00
http://arxiv.org/abs/2005.00218v2,Differentially Private Federated Learning with Laplacian Smoothing,"Federated learning aims to protect data privacy by collaboratively learning a
model without sharing private data among users. However, an adversary may still
be able to infer the private training data by attacking the released model.
Differential privacy provides a statistical protection against such attacks at
the price of significantly degrading the accuracy or utility of the trained
models. In this paper, we investigate a utility enhancement scheme based on
Laplacian smoothing for differentially private federated learning (DP-Fed-LS),
where the parameter aggregation with injected Gaussian noise is improved in
statistical precision without losing privacy budget. Our key observation is
that the aggregated gradients in federated learning often enjoy a type of
smoothness, i.e. sparsity in the graph Fourier basis with polynomial decays of
Fourier coefficients as frequency grows, which can be exploited by the
Laplacian smoothing efficiently. Under a prescribed differential privacy
budget, convergence error bounds with tight rates are provided for DP-Fed-LS
with uniform subsampling of heterogeneous Non-IID data, revealing possible
utility improvement of Laplacian smoothing in effective dimensionality and
variance reduction, among others. Experiments over MNIST, SVHN, and Shakespeare
datasets show that the proposed method can improve model accuracy with
DP-guarantee and membership privacy under both uniform and Poisson subsampling
mechanisms.","['Zhicong Liang', 'Bao Wang', 'Quanquan Gu', 'Stanley Osher', 'Yuan Yao']","['cs.LG', 'stat.ML']",2020-05-01 04:28:38+00:00
http://arxiv.org/abs/2005.00191v3,Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability,"A recent source of concern for the security of neural networks is the
emergence of clean-label dataset poisoning attacks, wherein correctly labeled
poison samples are injected into the training dataset. While these poison
samples look legitimate to the human observer, they contain malicious
characteristics that trigger a targeted misclassification during inference. We
propose a scalable and transferable clean-label poisoning attack against
transfer learning, which creates poison images with their center close to the
target image in the feature space. Our attack, Bullseye Polytope, improves the
attack success rate of the current state-of-the-art by 26.75% in end-to-end
transfer learning, while increasing attack speed by a factor of 12. We further
extend Bullseye Polytope to a more practical attack model by including multiple
images of the same object (e.g., from different angles) when crafting the
poison samples. We demonstrate that this extension improves attack
transferability by over 16% to unseen images (of the same object) without using
extra poison samples.","['Hojjat Aghakhani', 'Dongyu Meng', 'Yu-Xiang Wang', 'Christopher Kruegel', 'Giovanni Vigna']","['cs.LG', 'cs.CR', 'stat.ML']",2020-05-01 03:22:36+00:00
http://arxiv.org/abs/2005.00180v1,Generalization Error of Generalized Linear Models in High Dimensions,"At the heart of machine learning lies the question of generalizability of
learned rules over previously unseen data. While over-parameterized models
based on neural networks are now ubiquitous in machine learning applications,
our understanding of their generalization capabilities is incomplete. This task
is made harder by the non-convexity of the underlying learning problems. We
provide a general framework to characterize the asymptotic generalization error
for single-layer neural networks (i.e., generalized linear models) with
arbitrary non-linearities, making it applicable to regression as well as
classification problems. This framework enables analyzing the effect of (i)
over-parameterization and non-linearity during modeling; and (ii) choices of
loss function, initialization, and regularizer during learning. Our model also
captures mismatch between training and test distributions. As examples, we
analyze a few special cases, namely linear regression and logistic regression.
We are also able to rigorously and analytically explain the \emph{double
descent} phenomenon in generalized linear models.","['Melikasadat Emami', 'Mojtaba Sahraee-Ardakan', 'Parthe Pandit', 'Sundeep Rangan', 'Alyson K. Fletcher']","['cs.LG', 'stat.ML']",2020-05-01 02:17:47+00:00
http://arxiv.org/abs/2005.00178v1,On the Benefits of Invariance in Neural Networks,"Many real world data analysis problems exhibit invariant structure, and
models that take advantage of this structure have shown impressive empirical
performance, particularly in deep learning. While the literature contains a
variety of methods to incorporate invariance into models, theoretical
understanding is poor and there is no way to assess when one method should be
preferred over another. In this work, we analyze the benefits and limitations
of two widely used approaches in deep learning in the presence of invariance:
data augmentation and feature averaging. We prove that training with data
augmentation leads to better estimates of risk and gradients thereof, and we
provide a PAC-Bayes generalization bound for models trained with data
augmentation. We also show that compared to data augmentation, feature
averaging reduces generalization error when used with convex losses, and
tightens PAC-Bayes bounds. We provide empirical support of these theoretical
results, including a demonstration of why generalization may not improve by
training with data augmentation: the `learned invariance' fails outside of the
training distribution.","['Clare Lyle', 'Mark van der Wilk', 'Marta Kwiatkowska', 'Yarin Gal', 'Benjamin Bloem-Reddy']","['cs.LG', 'stat.ML']",2020-05-01 02:08:58+00:00
http://arxiv.org/abs/2005.00146v3,Addressing Catastrophic Forgetting in Few-Shot Problems,"Neural networks are known to suffer from catastrophic forgetting when trained
on sequential datasets. While there have been numerous attempts to solve this
problem in large-scale supervised classification, little has been done to
overcome catastrophic forgetting in few-shot classification problems. We
demonstrate that the popular gradient-based model-agnostic meta-learning
algorithm (MAML) indeed suffers from catastrophic forgetting and introduce a
Bayesian online meta-learning framework that tackles this problem. Our
framework utilises Bayesian online learning and meta-learning along with
Laplace approximation and variational inference to overcome catastrophic
forgetting in few-shot classification problems. The experimental evaluations
demonstrate that our framework can effectively achieve this goal in comparison
with various baselines. As an additional utility, we also demonstrate
empirically that our framework is capable of meta-learning on sequentially
arriving few-shot tasks from a stationary task distribution.","['Pauching Yap', 'Hippolyt Ritter', 'David Barber']","['cs.LG', 'stat.ML']",2020-04-30 23:56:18+00:00
http://arxiv.org/abs/2005.00130v1,Hide-and-Seek: A Template for Explainable AI,"Lack of transparency has been the Achilles heal of Neural Networks and their
wider adoption in industry. Despite significant interest this shortcoming has
not been adequately addressed. This study proposes a novel framework called
Hide-and-Seek (HnS) for training Interpretable Neural Networks and establishes
a theoretical foundation for exploring and comparing similar ideas. Extensive
experimentation indicates that a high degree of interpretability can be imputed
into Neural Networks, without sacrificing their predictive power.","['Thanos Tagaris', 'Andreas Stafylopatis']","['cs.LG', 'cs.AI', 'stat.ML', '62M45', 'I.2.6']",2020-04-30 22:34:37+00:00
http://arxiv.org/abs/2005.00123v2,Unsupervised Learning of KB Queries in Task-Oriented Dialogs,"Task-oriented dialog (TOD) systems often need to formulate knowledge base
(KB) queries corresponding to the user intent and use the query results to
generate system responses. Existing approaches require dialog datasets to
explicitly annotate these KB queries -- these annotations can be time
consuming, and expensive. In response, we define the novel problems of
predicting the KB query and training the dialog agent, without explicit KB
query annotation. For query prediction, we propose a reinforcement learning
(RL) baseline, which rewards the generation of those queries whose KB results
cover the entities mentioned in subsequent dialog. Further analysis reveals
that correlation among query attributes in KB can significantly confuse memory
augmented policy optimization (MAPO), an existing state of the art RL agent. To
address this, we improve the MAPO baseline with simple but important
modifications suited to our task. To train the full TOD system for our setting,
we propose a pipelined approach: it independently predicts when to make a KB
query (query position predictor), then predicts a KB query at the predicted
position (query predictor), and uses the results of predicted query in
subsequent dialog (next response predictor). Overall, our work proposes first
solutions to our novel problem, and our analysis highlights the research
challenges in training TOD systems without query annotation.","['Dinesh Raghu', 'Nikhil Gupta', 'Mausam']","['cs.LG', 'cs.CL', 'stat.ML']",2020-04-30 22:10:00+00:00
http://arxiv.org/abs/2005.02140v3,BlackBox: Generalizable Reconstruction of Extremal Values from Incomplete Spatio-Temporal Data,"We describe our submission to the Extreme Value Analysis 2019 Data Challenge
in which teams were asked to predict extremes of sea surface temperature
anomaly within spatio-temporal regions of missing data. We present a
computational framework which reconstructs missing data using convolutional
deep neural networks. Conditioned on incomplete data, we employ
autoencoder-like models as multivariate conditional distributions from which
possible reconstructions of the complete dataset are sampled using imputed
noise. In order to mitigate bias introduced by any one particular model, a
prediction ensemble is constructed to create the final distribution of extremal
values. Our method does not rely on expert knowledge in order to accurately
reproduce dynamic features of a complex oceanographic system with minimal
assumptions. The obtained results promise reusability and generalization to
other domains.","['Tomislav Ivek', 'Domagoj Vlah']","['cs.LG', 'stat.ML']",2020-04-30 21:33:46+00:00
http://arxiv.org/abs/2005.00113v2,Challenges in Benchmarking Stream Learning Algorithms with Real-world Data,"Streaming data are increasingly present in real-world applications such as
sensor measurements, satellite data feed, stock market, and financial data. The
main characteristics of these applications are the online arrival of data
observations at high speed and the susceptibility to changes in the data
distributions due to the dynamic nature of real environments. The data stream
mining community still faces some primary challenges and difficulties related
to the comparison and evaluation of new proposals, mainly due to the lack of
publicly available non-stationary real-world datasets. The comparison of stream
algorithms proposed in the literature is not an easy task, as authors do not
always follow the same recommendations, experimental evaluation procedures,
datasets, and assumptions. In this paper, we mitigate problems related to the
choice of datasets in the experimental evaluation of stream classifiers and
drift detectors. To that end, we propose a new public data repository for
benchmarking stream algorithms with real-world data. This repository contains
the most popular datasets from literature and new datasets related to a highly
relevant public health problem that involves the recognition of disease vector
insects using optical sensors. The main advantage of these new datasets is the
prior knowledge of their characteristics and patterns of changes to evaluate
new adaptive algorithm proposals adequately. We also present an in-depth
discussion about the characteristics, reasons, and issues that lead to
different types of changes in data distribution, as well as a critical review
of common problems concerning the current benchmark datasets available in the
literature.","['Vinicius M. A. Souza', 'Denis M. dos Reis', 'Andre G. Maletzke', 'Gustavo E. A. P. A. Batista']","['cs.LG', 'stat.ML', '68T05', 'I.2.6']",2020-04-30 21:31:34+00:00
http://arxiv.org/abs/2005.00065v4,"Generative Adversarial Networks (GANs Survey): Challenges, Solutions, and Future Directions","Generative Adversarial Networks (GANs) is a novel class of deep generative
models which has recently gained significant attention. GANs learns complex and
high-dimensional distributions implicitly over images, audio, and data.
However, there exists major challenges in training of GANs, i.e., mode
collapse, non-convergence and instability, due to inappropriate design of
network architecture, use of objective function and selection of optimization
algorithm. Recently, to address these challenges, several solutions for better
design and optimization of GANs have been investigated based on techniques of
re-engineered network architectures, new objective functions and alternative
optimization algorithms. To the best of our knowledge, there is no existing
survey that has particularly focused on broad and systematic developments of
these solutions. In this study, we perform a comprehensive survey of the
advancements in GANs design and optimization solutions proposed to handle GANs
challenges. We first identify key research issues within each design and
optimization technique and then propose a new taxonomy to structure solutions
by key research issues. In accordance with the taxonomy, we provide a detailed
discussion on different GANs variants proposed within each solution and their
relationships. Finally, based on the insights gained, we present the promising
research directions in this rapidly growing field.","['Divya Saxena', 'Jiannong Cao']","['cs.LG', 'eess.IV', 'stat.ML']",2020-04-30 19:26:46+00:00
http://arxiv.org/abs/2005.00061v2,Data-Space Inversion Using a Recurrent Autoencoder for Time-Series Parameterization,"Data-space inversion (DSI) and related procedures represent a family of
methods applicable for data assimilation in subsurface flow settings. These
methods differ from model-based techniques in that they provide only posterior
predictions for quantities (time series) of interest, not posterior models with
calibrated parameters. DSI methods require a large number of flow simulations
to first be performed on prior geological realizations. Given observed data,
posterior predictions can then be generated directly. DSI operates in a
Bayesian setting and provides posterior samples of the data vector. In this
work we develop and evaluate a new approach for data parameterization in DSI.
Parameterization reduces the number of variables to determine in the inversion,
and it maintains the physical character of the data variables. The new
parameterization uses a recurrent autoencoder (RAE) for dimension reduction,
and a long-short-term memory (LSTM) network to represent flow-rate time series.
The RAE-based parameterization is combined with an ensemble smoother with
multiple data assimilation (ESMDA) for posterior generation. Results are
presented for two- and three-phase flow in a 2D channelized system and a 3D
multi-Gaussian model. The RAE procedure, along with existing DSI treatments,
are assessed through comparison to reference rejection sampling (RS) results.
The new DSI methodology is shown to consistently outperform existing
approaches, in terms of statistical agreement with RS results. The method is
also shown to accurately capture derived quantities, which are computed from
variables considered directly in DSI. This requires correlation and covariance
between variables to be properly captured, and accuracy in these relationships
is demonstrated. The RAE-based parameterization developed here is clearly
useful in DSI, and it may also find application in other subsurface flow
problems.","['Su Jiang', 'Louis J. Durlofsky']","['stat.ML', 'cs.LG']",2020-04-30 19:17:58+00:00
http://arxiv.org/abs/2005.00060v2,Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness,"Mode connectivity provides novel geometric insights on analyzing loss
landscapes and enables building high-accuracy pathways between well-trained
neural networks. In this work, we propose to employ mode connectivity in loss
landscapes to study the adversarial robustness of deep neural networks, and
provide novel methods for improving this robustness. Our experiments cover
various types of adversarial attacks applied to different network architectures
and datasets. When network models are tampered with backdoor or error-injection
attacks, our results demonstrate that the path connection learned using limited
amount of bonafide data can effectively mitigate adversarial effects while
maintaining the original accuracy on clean data. Therefore, mode connectivity
provides users with the power to repair backdoored or error-injected models. We
also use mode connectivity to investigate the loss landscapes of regular and
robust models against evasion attacks. Experiments show that there exists a
barrier in adversarial robustness loss on the path connecting regular and
adversarially-trained models. A high correlation is observed between the
adversarial robustness loss and the largest eigenvalue of the input Hessian
matrix, for which theoretical justifications are provided. Our results suggest
that mode connectivity offers a holistic tool and practical means for
evaluating and improving adversarial robustness.","['Pu Zhao', 'Pin-Yu Chen', 'Payel Das', 'Karthikeyan Natesan Ramamurthy', 'Xue Lin']","['cs.LG', 'cs.CV', 'stat.ML']",2020-04-30 19:12:50+00:00
http://arxiv.org/abs/2005.00054v3,APo-VAE: Text Generation in Hyperbolic Space,"Natural language often exhibits inherent hierarchical structure ingrained
with complex syntax and semantics. However, most state-of-the-art deep
generative models learn embeddings only in Euclidean vector space, without
accounting for this structural property of language. In this paper, we
investigate text generation in a hyperbolic latent space to learn continuous
hierarchical representations. An Adversarial Poincare Variational Autoencoder
(APo-VAE) is presented, where both the prior and variational posterior of
latent variables are defined over a Poincare ball via wrapped normal
distributions. By adopting the primal-dual formulation of KL divergence, an
adversarial learning procedure is introduced to empower robust model training.
Extensive experiments in language modeling and dialog-response generation tasks
demonstrate the winning effectiveness of the proposed APo-VAE model over VAEs
in Euclidean latent space, thanks to its superb capabilities in capturing
latent language hierarchies in hyperbolic space.","['Shuyang Dai', 'Zhe Gan', 'Yu Cheng', 'Chenyang Tao', 'Lawrence Carin', 'Jingjing Liu']","['cs.LG', 'stat.ML']",2020-04-30 19:05:41+00:00
http://arxiv.org/abs/2005.00010v1,A Primer on Private Statistics,"Differentially private statistical estimation has seen a flurry of
developments over the last several years. Study has been divided into two
schools of thought, focusing on empirical statistics versus population
statistics. We suggest that these two lines of work are more similar than
different by giving examples of methods that were initially framed for
empirical statistics, but can be applied just as well to population statistics.
We also provide a thorough coverage of recent work in this area.","['Gautam Kamath', 'Jonathan Ullman']","['stat.ML', 'cs.CR', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']",2020-04-30 18:00:00+00:00
http://arxiv.org/abs/2004.14992v3,How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking,"Attribution methods assess the contribution of inputs to the model
prediction. One way to do so is erasure: a subset of inputs is considered
irrelevant if it can be removed without affecting the prediction. Though
conceptually simple, erasure's objective is intractable and approximate search
remains expensive with modern deep NLP models. Erasure is also susceptible to
the hindsight bias: the fact that an input can be dropped does not mean that
the model `knows' it can be dropped. The resulting pruning is over-aggressive
and does not reflect how the model arrives at the prediction. To deal with
these challenges, we introduce Differentiable Masking. DiffMask learns to
mask-out subsets of the input while maintaining differentiability. The decision
to include or disregard an input token is made with a simple model based on
intermediate hidden layers of the analyzed model. First, this makes the
approach efficient because we predict rather than search. Second, as with
probing classifiers, this reveals what the network `knows' at the corresponding
layers. This lets us not only plot attribution heatmaps but also analyze how
decisions are formed across network layers. We use DiffMask to study BERT
models on sentiment classification and question answering.","['Nicola De Cao', 'Michael Schlichtkrull', 'Wilker Aziz', 'Ivan Titov']","['cs.CL', 'stat.ML']",2020-04-30 17:36:14+00:00
http://arxiv.org/abs/2004.14990v5,Reinforcement Learning with Augmented Data,"Learning from visual observations is a fundamental yet challenging problem in
Reinforcement Learning (RL). Although algorithmic advances combined with
convolutional neural networks have proved to be a recipe for success, current
methods are still lacking on two fronts: (a) data-efficiency of learning and
(b) generalization to new environments. To this end, we present Reinforcement
Learning with Augmented Data (RAD), a simple plug-and-play module that can
enhance most RL algorithms. We perform the first extensive study of general
data augmentations for RL on both pixel-based and state-based inputs, and
introduce two new data augmentations - random translate and random amplitude
scale. We show that augmentations such as random translate, crop, color jitter,
patch cutout, random convolutions, and amplitude scale can enable simple RL
algorithms to outperform complex state-of-the-art methods across common
benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and
final performance on the DeepMind Control Suite benchmark for pixel-based
control as well as OpenAI Gym benchmark for state-based control. We further
demonstrate that RAD significantly improves test-time generalization over
existing methods on several OpenAI ProcGen benchmarks. Our RAD module and
training code are available at https://www.github.com/MishaLaskin/rad.","['Michael Laskin', 'Kimin Lee', 'Adam Stooke', 'Lerrel Pinto', 'Pieter Abbeel', 'Aravind Srinivas']","['cs.LG', 'stat.ML']",2020-04-30 17:35:32+00:00
http://arxiv.org/abs/2004.14958v1,A Call for More Rigor in Unsupervised Cross-lingual Learning,"We review motivations, definition, approaches, and methodology for
unsupervised cross-lingual learning and call for a more rigorous position in
each of them. An existing rationale for such research is based on the lack of
parallel data for many of the world's languages. However, we argue that a
scenario without any parallel data and abundant monolingual data is unrealistic
in practice. We also discuss different training signals that have been used in
previous work, which depart from the pure unsupervised setting. We then
describe common methodological issues in tuning and evaluation of unsupervised
cross-lingual models and present best practices. Finally, we provide a unified
outlook for different types of research in this area (i.e., cross-lingual word
embeddings, deep multilingual pretraining, and unsupervised machine
translation) and argue for comparable evaluation of these models.","['Mikel Artetxe', 'Sebastian Ruder', 'Dani Yogatama', 'Gorka Labaka', 'Eneko Agirre']","['cs.CL', 'cs.LG', 'stat.ML']",2020-04-30 17:06:23+00:00
http://arxiv.org/abs/2004.14954v1,On Deep Instrumental Variables Estimate,"The endogeneity issue is fundamentally important as many empirical
applications may suffer from the omission of explanatory variables, measurement
error, or simultaneous causality. Recently, \cite{hllt17} propose a ""Deep
Instrumental Variable (IV)"" framework based on deep neural networks to address
endogeneity, demonstrating superior performances than existing approaches. The
aim of this paper is to theoretically understand the empirical success of the
Deep IV. Specifically, we consider a two-stage estimator using deep neural
networks in the linear instrumental variables model. By imposing a latent
structural assumption on the reduced form equation between endogenous variables
and instrumental variables, the first-stage estimator can automatically capture
this latent structure and converge to the optimal instruments at the minimax
optimal rate, which is free of the dimension of instrumental variables and thus
mitigates the curse of dimensionality. Additionally, in comparison with
classical methods, due to the faster convergence rate of the first-stage
estimator, the second-stage estimator has {a smaller (second order) estimation
error} and requires a weaker condition on the smoothness of the optimal
instruments. Given that the depth and width of the employed deep neural network
are well chosen, we further show that the second-stage estimator achieves the
semiparametric efficiency bound. Simulation studies on synthetic data and
application to automobile market data confirm our theory.","['Ruiqi Liu', 'Zuofeng Shang', 'Guang Cheng']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2020-04-30 17:03:00+00:00
http://arxiv.org/abs/2004.14941v2,The Information Bottleneck Problem and Its Applications in Machine Learning,"Inference capabilities of machine learning (ML) systems skyrocketed in recent
years, now playing a pivotal role in various aspect of society. The goal in
statistical learning is to use data to obtain simple algorithms for predicting
a random variable $Y$ from a correlated observation $X$. Since the dimension of
$X$ is typically huge, computationally feasible solutions should summarize it
into a lower-dimensional feature vector $T$, from which $Y$ is predicted. The
algorithm will successfully make the prediction if $T$ is a good proxy of $Y$,
despite the said dimensionality-reduction. A myriad of ML algorithms (mostly
employing deep learning (DL)) for finding such representations $T$ based on
real-world data are now available. While these methods are often effective in
practice, their success is hindered by the lack of a comprehensive theory to
explain it. The information bottleneck (IB) theory recently emerged as a bold
information-theoretic paradigm for analyzing DL systems. Adopting mutual
information as the figure of merit, it suggests that the best representation
$T$ should be maximally informative about $Y$ while minimizing the mutual
information with $X$. In this tutorial we survey the information-theoretic
origins of this abstract principle, and its recent impact on DL. For the
latter, we cover implications of the IB problem on DL theory, as well as
practical algorithms inspired by it. Our goal is to provide a unified and
cohesive description. A clear view of current knowledge is particularly
important for further leveraging IB and other information-theoretic ideas to
study DL models.","['Ziv Goldfeld', 'Yury Polyanskiy']","['cs.LG', 'stat.ML']",2020-04-30 16:48:51+00:00
