id,title,abstract,authors,categories,date
http://arxiv.org/abs/2405.08235v1,Additive-Effect Assisted Learning,"It is quite popular nowadays for researchers and data analysts holding
different datasets to seek assistance from each other to enhance their modeling
performance. We consider a scenario where different learners hold datasets with
potentially distinct variables, and their observations can be aligned by a
nonprivate identifier. Their collaboration faces the following difficulties:
First, learners may need to keep data values or even variable names undisclosed
due to, e.g., commercial interest or privacy regulations; second, there are
restrictions on the number of transmission rounds between them due to e.g.,
communication costs. To address these challenges, we develop a two-stage
assisted learning architecture for an agent, Alice, to seek assistance from
another agent, Bob. In the first stage, we propose a privacy-aware hypothesis
testing-based screening method for Alice to decide on the usefulness of the
data from Bob, in a way that only requires Bob to transmit sketchy data. Once
Alice recognizes Bob's usefulness, Alice and Bob move to the second stage,
where they jointly apply a synergistic iterative model training procedure. With
limited transmissions of summary statistics, we show that Alice can achieve the
oracle performance as if the training were from centralized data, both
theoretically and numerically.","['Jiawei Zhang', 'Yuhong Yang', 'Jie Ding']","['stat.ML', 'cs.LG']",2024-05-13 23:24:25+00:00
http://arxiv.org/abs/2405.08217v1,Data Valuation with Gradient Similarity,"High-quality data is crucial for accurate machine learning and actionable
analytics, however, mislabeled or noisy data is a common problem in many
domains. Distinguishing low- from high-quality data can be challenging, often
requiring expert knowledge and considerable manual intervention. Data Valuation
algorithms are a class of methods that seek to quantify the value of each
sample in a dataset based on its contribution or importance to a given
predictive task. These data values have shown an impressive ability to identify
mislabeled observations, and filtering low-value data can boost machine
learning performance. In this work, we present a simple alternative to existing
methods, termed Data Valuation with Gradient Similarity (DVGS). This approach
can be easily applied to any gradient descent learning algorithm, scales well
to large datasets, and performs comparably or better than baseline valuation
methods for tasks such as corrupted label discovery and noise quantification.
We evaluate the DVGS method on tabular, image and RNA expression datasets to
show the effectiveness of the method across domains. Our approach has the
ability to rapidly and accurately identify low-quality data, which can reduce
the need for expert knowledge and manual intervention in data cleaning tasks.","['Nathaniel J. Evans', 'Gordon B. Mills', 'Guanming Wu', 'Xubo Song', 'Shannon McWeeney']","['cs.LG', 'q-bio.GN', 'q-bio.QM', 'stat.ML']",2024-05-13 22:10:00+00:00
http://arxiv.org/abs/2405.08179v1,Do Bayesian imaging methods report trustworthy probabilities?,"Bayesian statistics is a cornerstone of imaging sciences, underpinning many
and varied approaches from Markov random fields to score-based denoising
diffusion models. In addition to powerful image estimation methods, the
Bayesian paradigm also provides a framework for uncertainty quantification and
for using image data as quantitative evidence. These probabilistic capabilities
are important for the rigorous interpretation of experimental results and for
robust interfacing of quantitative imaging pipelines with scientific and
decision-making processes. However, are the probabilities delivered by existing
Bayesian imaging methods meaningful under replication of an experiment, or are
they only meaningful as subjective measures of belief? This paper presents a
Monte Carlo method to explore this question. We then leverage the proposed
Monte Carlo method and run a large experiment requiring 1,000 GPU-hours to
probe the accuracy of five canonical Bayesian imaging methods that are
representative of some of the main Bayesian imaging strategies from the past
decades (a score-based denoising diffusion technique, a plug-and-play Langevin
algorithm utilising a Lipschitz-regularised DnCNN denoiser, a Bayesian method
with a dictionary-based prior trained subject to a log-concavity constraint, an
empirical Bayesian method with a total-variation prior, and a hierarchical
Bayesian Gibbs sampler based on a Gaussian Markov random field model). We find
that, a few cases, the probabilities reported by modern Bayesian imaging
techniques are in broad agreement with long-term averages as observed over a
large number of replication of an experiment, but existing Bayesian imaging
methods are generally not able to deliver reliable uncertainty quantification
results.","['David Y. W. Thong', 'Charlesquin Kemajou Mbakam', 'Marcelo Pereyra']","['eess.IV', 'cs.LG', 'eess.SP', 'stat.AP', 'stat.ML', '65J22 (Primary), 62F15 (Secondary), 68U10']",2024-05-13 20:57:01+00:00
http://arxiv.org/abs/2405.07976v3,Localized Adaptive Risk Control,"Adaptive Risk Control (ARC) is an online calibration strategy based on set
prediction that offers worst-case deterministic long-term risk control, as well
as statistical marginal coverage guarantees. ARC adjusts the size of the
prediction set by varying a single scalar threshold based on feedback from past
decisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC),
an online calibration scheme that targets statistical localized risk guarantees
ranging from conditional risk to marginal risk, while preserving the worst-case
performance of ARC. L-ARC updates a threshold function within a reproducing
kernel Hilbert space (RKHS), with the kernel determining the level of
localization of the statistical risk guarantee. The theoretical results
highlight a trade-off between localization of the statistical risk and
convergence speed to the long-term risk target. Thanks to localization, L-ARC
is demonstrated via experiments to produce prediction sets with risk guarantees
across different data subpopulations, significantly improving the fairness of
the calibrated model for tasks such as image segmentation and beam selection in
wireless networks.","['Matteo Zecchin', 'Osvaldo Simeone']","['stat.ML', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT']",2024-05-13 17:48:45+00:00
http://arxiv.org/abs/2405.07971v1,"Sensitivity Analysis for Active Sampling, with Applications to the Simulation of Analog Circuits","We propose an active sampling flow, with the use-case of simulating the
impact of combined variations on analog circuits. In such a context, given the
large number of parameters, it is difficult to fit a surrogate model and to
efficiently explore the space of design features.
  By combining a drastic dimension reduction using sensitivity analysis and
Bayesian surrogate modeling, we obtain a flexible active sampling flow. On
synthetic and real datasets, this flow outperforms the usual Monte-Carlo
sampling which often forms the foundation of design space exploration.","['Reda Chhaibi', 'Fabrice Gamboa', 'Christophe Oger', 'Vinicius Oliveira', 'Clément Pellegrini', 'Damien Remot']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.ME']",2024-05-13 17:47:40+00:00
http://arxiv.org/abs/2405.07914v1,Distribution Learning Meets Graph Structure Sampling,"This work establishes a novel link between the problem of PAC-learning
high-dimensional graphical models and the task of (efficient) counting and
sampling of graph structures, using an online learning framework.
  We observe that if we apply the exponentially weighted average (EWA) or
randomized weighted majority (RWM) forecasters on a sequence of samples from a
distribution P using the log loss function, the average regret incurred by the
forecaster's predictions can be used to bound the expected KL divergence
between P and the predictions. Known regret bounds for EWA and RWM then yield
new sample complexity bounds for learning Bayes nets. Moreover, these
algorithms can be made computationally efficient for several interesting
classes of Bayes nets. Specifically, we give a new sample-optimal and
polynomial time learning algorithm with respect to trees of unknown structure
and the first polynomial sample and time algorithm for learning with respect to
Bayes nets over a given chordal skeleton.","['Arnab Bhattacharyya', 'Sutanu Gayen', 'Philips George John', 'Sayantan Sen', 'N. V. Vinodchandran']","['cs.LG', 'cs.DS', 'stat.ML']",2024-05-13 16:47:05+00:00
http://arxiv.org/abs/2405.07863v3,RLHF Workflow: From Reward Modeling to Online RLHF,"We present the workflow of Online Iterative Reinforcement Learning from Human
Feedback (RLHF) in this technical report, which is widely reported to
outperform its offline counterpart by a large margin in the recent large
language model (LLM) literature. However, existing open-source RLHF projects
are still largely confined to the offline learning setting. In this technical
report, we aim to fill in this gap and provide a detailed recipe that is easy
to reproduce for online iterative RLHF. In particular, since online human
feedback is usually infeasible for open-source communities with limited
resources, we start by constructing preference models using a diverse set of
open-source datasets and use the constructed proxy preference model to
approximate human feedback. Then, we discuss the theoretical insights and
algorithmic principles behind online iterative RLHF, followed by a detailed
practical implementation. Our trained LLM achieves impressive performance on
LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as
well as other academic benchmarks such as HumanEval and TruthfulQA. We have
shown that supervised fine-tuning (SFT) and iterative RLHF can obtain
state-of-the-art performance with fully open-source datasets. Further, we have
made our models, curated datasets, and comprehensive step-by-step code
guidebooks publicly available. Please refer to
https://github.com/RLHFlow/RLHF-Reward-Modeling and
https://github.com/RLHFlow/Online-RLHF for more detailed information.","['Hanze Dong', 'Wei Xiong', 'Bo Pang', 'Haoxiang Wang', 'Han Zhao', 'Yingbo Zhou', 'Nan Jiang', 'Doyen Sahoo', 'Caiming Xiong', 'Tong Zhang']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2024-05-13 15:50:39+00:00
http://arxiv.org/abs/2405.07860v3,Simultaneous Inference for Local Structural Parameters with Random Forests,"We construct simultaneous confidence intervals for solutions to conditional
moment equations. The intervals are built around a class of nonparametric
regression algorithms based on subsampled kernels. This class encompasses
various forms of subsampled random forest regression, including Generalized
Random Forests (Athey et al., 2019). Although simultaneous validity is often
desirable in practice -- for example, for fine-grained characterization of
treatment effect heterogeneity -- only confidence intervals that confer
pointwise guarantees were previously available. Our work closes this gap. As a
by-product, we obtain several new order-explicit results on the concentration
and normal approximation of high-dimensional U-statistics.","['David M. Ritzwoller', 'Vasilis Syrgkanis']","['econ.EM', 'math.ST', 'stat.ML', 'stat.TH']",2024-05-13 15:46:11+00:00
http://arxiv.org/abs/2405.07839v2,Constrained Exploration via Reflected Replica Exchange Stochastic Gradient Langevin Dynamics,"Replica exchange stochastic gradient Langevin dynamics (reSGLD) is an
effective sampler for non-convex learning in large-scale datasets. However, the
simulation may encounter stagnation issues when the high-temperature chain
delves too deeply into the distribution tails. To tackle this issue, we propose
reflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convex
exploration by utilizing reflection steps within a bounded domain.
Theoretically, we observe that reducing the diameter of the domain enhances
mixing rates, exhibiting a $\textit{quadratic}$ behavior. Empirically, we test
its performance through extensive experiments, including identifying dynamical
systems with physical constraints, simulations of constrained multi-modal
distributions, and image classification tasks. The theoretical and empirical
findings highlight the crucial role of constrained exploration in improving the
simulation efficiency.","['Haoyang Zheng', 'Hengrong Du', 'Qi Feng', 'Wei Deng', 'Guang Lin']","['cs.LG', 'cs.AI', 'stat.ML']",2024-05-13 15:25:03+00:00
http://arxiv.org/abs/2405.07795v1,Improved Bound for Robust Causal Bandits with Linear Models,"This paper investigates the robustness of causal bandits (CBs) in the face of
temporal model fluctuations. This setting deviates from the existing
literature's widely-adopted assumption of constant causal models. The focus is
on causal systems with linear structural equation models (SEMs). The SEMs and
the time-varying pre- and post-interventional statistical models are all
unknown and subject to variations over time. The goal is to design a sequence
of interventions that incur the smallest cumulative regret compared to an
oracle aware of the entire causal model and its fluctuations. A robust CB
algorithm is proposed, and its cumulative regret is analyzed by establishing
both upper and lower bounds on the regret. It is shown that in a graph with
maximum in-degree $d$, length of the largest causal path $L$, and an aggregate
model deviation $C$, the regret is upper bounded by
$\tilde{\mathcal{O}}(d^{L-\frac{1}{2}}(\sqrt{T} + C))$ and lower bounded by
$\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T}\; ,\; d^2C\})$. The proposed algorithm
achieves nearly optimal $\tilde{\mathcal{O}}(\sqrt{T})$ regret when $C$ is
$o(\sqrt{T})$, maintaining sub-linear regret for a broad range of $C$.","['Zirui Yan', 'Arpan Mukherjee', 'Burak Varıcı', 'Ali Tajer']","['stat.ML', 'cs.LG']",2024-05-13 14:41:28+00:00
http://arxiv.org/abs/2405.07791v3,Decentralized Kernel Ridge Regression Based on Data-Dependent Random Feature,"Random feature (RF) has been widely used for node consistency in
decentralized kernel ridge regression (KRR). Currently, the consistency is
guaranteed by imposing constraints on coefficients of features, necessitating
that the random features on different nodes are identical. However, in many
applications, data on different nodes varies significantly on the number or
distribution, which calls for adaptive and data-dependent methods that generate
different RFs. To tackle the essential difficulty, we propose a new
decentralized KRR algorithm that pursues consensus on decision functions, which
allows great flexibility and well adapts data on nodes. The convergence is
rigorously given and the effectiveness is numerically verified: by capturing
the characteristics of the data on each node, while maintaining the same
communication costs as other methods, we achieved an average regression
accuracy improvement of 25.5\% across six real-world data sets.","['Ruikai Yang', 'Fan He', 'Mingzhen He', 'Jie Yang', 'Xiaolin Huang']","['cs.LG', 'cs.DC', 'stat.ML']",2024-05-13 14:37:03+00:00
http://arxiv.org/abs/2405.07760v1,CAGES: Cost-Aware Gradient Entropy Search for Efficient Local Multi-Fidelity Bayesian Optimization,"Bayesian optimization (BO) is a popular approach for optimizing
expensive-to-evaluate black-box objective functions. An important challenge in
BO is its application to high-dimensional search spaces due in large part to
the curse of dimensionality. One way to overcome this challenge is to focus on
local BO methods that aim to efficiently learn gradients, which have shown
strong empirical performance on a variety of high-dimensional problems
including policy search in reinforcement learning (RL). However, current local
BO methods assume access to only a single high-fidelity information source
whereas, in many engineering and control problems, one has access to multiple
cheaper approximations of the objective. We propose a novel algorithm,
Cost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelity
black-box functions. CAGES makes no assumption about the relationship between
different information sources, making it more flexible than other
multi-fidelity methods. It also employs a new type of information-theoretic
acquisition function, which enables systematic identification of samples that
maximize the information gain about the unknown gradient per cost of the
evaluation. We demonstrate CAGES can achieve significant performance
improvements compared to other state-of-the-art methods on a variety of
synthetic and benchmark RL problems.","['Wei-Ting Tang', 'Joel A. Paulson']","['cs.LG', 'stat.ML']",2024-05-13 14:00:02+00:00
http://arxiv.org/abs/2405.07665v2,Partial information decomposition: redundancy as information bottleneck,"The partial information decomposition (PID) aims to quantify the amount of
redundant information that a set of sources provides about a target. Here, we
show that this goal can be formulated as a type of information bottleneck (IB)
problem, termed the ""redundancy bottleneck"" (RB). The RB formalizes a tradeoff
between prediction and compression: it extracts information from the sources
that best predict the target, without revealing which source provided the
information. It can be understood as a generalization of ""Blackwell
redundancy"", which we previously proposed as a principled measure of PID
redundancy. The ""RB curve"" quantifies the prediction--compression tradeoff at
multiple scales. This curve can also be quantified for individual sources,
allowing subsets of redundant sources to be identified without combinatorial
optimization. We provide an efficient iterative algorithm for computing the RB
curve.",['Artemy Kolchinsky'],"['cs.IT', 'math.IT', 'stat.ML']",2024-05-13 11:45:58+00:00
http://arxiv.org/abs/2405.07619v1,Analysis of the rate of convergence of an over-parametrized convolutional neural network image classifier learned by gradient descent,"Image classification based on over-parametrized convolutional neural networks
with a global average-pooling layer is considered. The weights of the network
are learned by gradient descent. A bound on the rate of convergence of the
difference between the misclassification risk of the newly introduced
convolutional neural network estimate and the minimal possible value is
derived.","['Michael Kohler', 'Adam Krzyzak', 'Benjamin Walter']","['stat.ML', 'cs.LG']",2024-05-13 10:26:28+00:00
http://arxiv.org/abs/2405.07552v3,Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery,"In this paper, we focus on distributed estimation and support recovery for
high-dimensional linear quantile regression. Quantile regression is a popular
alternative tool to the least squares regression for robustness against
outliers and data heterogeneity. However, the non-smoothness of the check loss
function poses big challenges to both computation and theory in the distributed
setting. To tackle these problems, we transform the original quantile
regression into the least-squares optimization. By applying a double-smoothing
approach, we extend a previous Newton-type distributed approach without the
restrictive independent assumption between the error term and covariates. An
efficient algorithm is developed, which enjoys high computation and
communication efficiency. Theoretically, the proposed distributed estimator
achieves a near-oracle convergence rate and high support recovery accuracy
after a constant number of iterations. Extensive experiments on synthetic
examples and a real data application further demonstrate the effectiveness of
the proposed method.","['Caixing Wang', 'Ziliang Shen']","['stat.ML', 'cs.LG', 'stat.ME']",2024-05-13 08:32:22+00:00
http://arxiv.org/abs/2405.07482v1,Marginal Fairness Sliced Wasserstein Barycenter,"The sliced Wasserstein barycenter (SWB) is a widely acknowledged method for
efficiently generalizing the averaging operation within probability measure
spaces. However, achieving marginal fairness SWB, ensuring approximately equal
distances from the barycenter to marginals, remains unexplored. The uniform
weighted SWB is not necessarily the optimal choice to obtain the desired
marginal fairness barycenter due to the heterogeneous structure of marginals
and the non-optimality of the optimization. As the first attempt to tackle the
problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB)
as a constrained SWB problem. Due to the computational disadvantages of the
formal definition, we propose two hyperparameter-free and computationally
tractable surrogate MFSWB problems that implicitly minimize the distances to
marginals and encourage marginal fairness at the same time. To further improve
the efficiency, we perform slicing distribution selection and obtain the third
surrogate definition by introducing a new slicing distribution that focuses
more on marginally unfair projecting directions. We discuss the relationship of
the three proposed problems and their relationship to sliced multi-marginal
Wasserstein distance. Finally, we conduct experiments on finding 3D
point-clouds averaging, color harmonization, and training of sliced Wasserstein
autoencoder with class-fairness representation to show the favorable
performance of the proposed surrogate MFSWB problems.","['Khai Nguyen', 'Hai Nguyen', 'Nhat Ho']","['stat.ML', 'cs.GR', 'cs.LG']",2024-05-13 05:48:37+00:00
http://arxiv.org/abs/2405.07473v1,Intrinsic Rewards for Exploration without Harm from Observational Noise: A Simulation Study Based on the Free Energy Principle,"In Reinforcement Learning (RL), artificial agents are trained to maximize
numerical rewards by performing tasks. Exploration is essential in RL because
agents must discover information before exploiting it. Two rewards encouraging
efficient exploration are the entropy of action policy and curiosity for
information gain. Entropy is well-established in literature, promoting
randomized action selection. Curiosity is defined in a broad variety of ways in
literature, promoting discovery of novel experiences. One example, prediction
error curiosity, rewards agents for discovering observations they cannot
accurately predict. However, such agents may be distracted by unpredictable
observational noises known as curiosity traps. Based on the Free Energy
Principle (FEP), this paper proposes hidden state curiosity, which rewards
agents by the KL divergence between the predictive prior and posterior
probabilities of latent variables. We trained six types of agents to navigate
mazes: baseline agents without rewards for entropy or curiosity, and agents
rewarded for entropy and/or either prediction error curiosity or hidden state
curiosity. We find entropy and curiosity result in efficient exploration,
especially both employed together. Notably, agents with hidden state curiosity
demonstrate resilience against curiosity traps, which hinder agents with
prediction error curiosity. This suggests implementing the FEP may enhance the
robustness and generalization of RL models, potentially aligning the learning
processes of artificial and biological agents.","['Theodore Jerome Tinker', 'Kenji Doya', 'Jun Tani']","['cs.LG', 'stat.ML', '68T01']",2024-05-13 05:18:23+00:00
http://arxiv.org/abs/2405.07432v1,Compressed Online Learning of Conditional Mean Embedding,"The conditional mean embedding (CME) encodes Markovian stochastic kernels
through their actions on probability distributions embedded within the
reproducing kernel Hilbert spaces (RKHS). The CME plays a key role in several
well-known machine learning tasks such as reinforcement learning, analysis of
dynamical systems, etc. We present an algorithm to learn the CME incrementally
from data via an operator-valued stochastic gradient descent. As is well-known,
function learning in RKHS suffers from scalability challenges from large data.
We utilize a compression mechanism to counter the scalability challenge. The
core contribution of this paper is a finite-sample performance guarantee on the
last iterate of the online compressed operator learning algorithm with
fast-mixing Markovian samples, when the target CME may not be contained in the
hypothesis space. We illustrate the efficacy of our algorithm by applying it to
the analysis of an example dynamical system.","['Boya Hou', 'Sina Sanjari', 'Alec Koppel', 'Subhonmesh Bose']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY']",2024-05-13 02:18:49+00:00
http://arxiv.org/abs/2405.08033v1,Predicting Ship Responses in Different Seaways using a Generalizable Force Correcting Machine Learning Method,"A machine learning (ML) method is generalizable if it can make predictions on
inputs which differ from the training dataset. For predictions of wave-induced
ship responses, generalizability is an important consideration if ML methods
are to be useful in design evaluations. Furthermore, the size of the training
dataset has a significant impact on the practicality of a method, especially
when training data is generated using high-fidelity numerical tools which are
expensive. This paper considers a hybrid machine learning method which corrects
the force in a low-fidelity equation of motion. The method is applied to two
different case studies: the nonlinear responses of a Duffing equation subject
to irregular excitation, and high-fidelity heave and pitch response data of a
Fast Displacement Ship (FDS) in head seas. The generalizability of the method
is determined in both cases by making predictions of the response in irregular
wave conditions that differ from those in the training dataset. The influence
that low-fidelity physics-based terms in the hybrid model have on
generalizability is also investigated. The predictions are compared to two
benchmarks: a linear physics-based model and a data-driven LSTM model. It is
found that the hybrid method offers an improvement in prediction accuracy and
generalizability when trained on a small dataset.","['Kyle E. Marlantes', 'Piotr J. Bandyk', 'Kevin J. Maki']","['cs.LG', 'physics.flu-dyn', 'stat.ML']",2024-05-13 01:04:27+00:00
http://arxiv.org/abs/2405.07374v2,Conformalized Survival Distributions: A Generic Post-Process to Increase Calibration,"Discrimination and calibration represent two important properties of survival
analysis, with the former assessing the model's ability to accurately rank
subjects and the latter evaluating the alignment of predicted outcomes with
actual events. With their distinct nature, it is hard for survival models to
simultaneously optimize both of them especially as many previous results found
improving calibration tends to diminish discrimination performance. This paper
introduces a novel approach utilizing conformal regression that can improve a
model's calibration without degrading discrimination. We provide theoretical
guarantees for the above claim, and rigorously validate the efficiency of our
approach across 11 real-world datasets, showcasing its practical applicability
and robustness in diverse scenarios.","['Shi-ang Qi', 'Yakun Yu', 'Russell Greiner']","['cs.LG', 'cs.AI', 'stat.ML']",2024-05-12 20:27:34+00:00
http://arxiv.org/abs/2405.08825v2,Thermodynamic limit in learning period three,"A continuous one-dimensional map with period three includes all periods. This
raises the following question: Can we obtain any types of periodic orbits
solely by learning three data points? In this letter, we report the answer to
be yes. Considering a random neural network in its thermodynamic limit, we show
that under certain conditions, learning period three can embed attractors with
all periods into the network as a bifurcation after learning. The associated
universality is explained by a topological conjugacy between the trained
network and the classical logistic map.","['Yuichiro Terasaki', 'Kohei Nakajima']","['stat.ML', 'cs.LG', 'nlin.AO', 'nlin.CD']",2024-05-12 17:57:25+00:00
http://arxiv.org/abs/2405.07331v1,Stochastic Bandits with ReLU Neural Networks,"We study the stochastic bandit problem with ReLU neural network structure. We
show that a $\tilde{O}(\sqrt{T})$ regret guarantee is achievable by considering
bandits with one-layer ReLU neural networks; to the best of our knowledge, our
work is the first to achieve such a guarantee. In this specific setting, we
propose an OFU-ReLU algorithm that can achieve this upper bound. The algorithm
first explores randomly until it reaches a linear regime, and then implements a
UCB-type linear bandit algorithm to balance exploration and exploitation. Our
key insight is that we can exploit the piecewise linear structure of ReLU
activations and convert the problem into a linear bandit in a transformed
feature space, once we learn the parameters of ReLU relatively accurately
during the exploration stage. To remove dependence on model parameters, we
design an OFU-ReLU+ algorithm based on a batching strategy, which can provide
the same theoretical guarantee.","['Kan Xu', 'Hamsa Bastani', 'Surbhi Goel', 'Osbert Bastani']","['cs.LG', 'cs.DS', 'stat.ML']",2024-05-12 16:54:57+00:00
http://arxiv.org/abs/2405.07220v1,On Discovery of Local Independence over Continuous Variables via Neural Contextual Decomposition,"Conditional independence provides a way to understand causal relationships
among the variables of interest. An underlying system may exhibit more
fine-grained causal relationships especially between a variable and its
parents, which will be called the local independence relationships. One of the
most widely studied local relationships is Context-Specific Independence (CSI),
which holds in a specific assignment of conditioned variables. However, its
applicability is often limited since it does not allow continuous variables:
data conditioned to the specific value of a continuous variable contains few
instances, if not none, making it infeasible to test independence. In this
work, we define and characterize the local independence relationship that holds
in a specific set of joint assignments of parental variables, which we call
context-set specific independence (CSSI). We then provide a canonical
representation of CSSI and prove its fundamental properties. Based on our
theoretical findings, we cast the problem of discovering multiple CSSI
relationships in a system as finding a partition of the joint outcome space.
Finally, we propose a novel method, coined neural contextual decomposition
(NCD), which learns such partition by imposing each set to induce CSSI via
modeling a conditional distribution. We empirically demonstrate that the
proposed method successfully discovers the ground truth local independence
relationships in both synthetic dataset and complex system reflecting the
real-world physical dynamics.","['Inwoo Hwang', 'Yunhyeok Kwak', 'Yeon-Ji Song', 'Byoung-Tak Zhang', 'Sanghack Lee']","['cs.LG', 'cs.AI', 'stat.ML']",2024-05-12 08:48:37+00:00
http://arxiv.org/abs/2405.07186v1,Adaptive-TMLE for the Average Treatment Effect based on Randomized Controlled Trial Augmented with Real-World Data,"We consider the problem of estimating the average treatment effect (ATE) when
both randomized control trial (RCT) data and real-world data (RWD) are
available. We decompose the ATE estimand as the difference between a pooled-ATE
estimand that integrates RCT and RWD and a bias estimand that captures the
conditional effect of RCT enrollment on the outcome. We introduce an adaptive
targeted minimum loss-based estimation (A-TMLE) framework to estimate them. We
prove that the A-TMLE estimator is root-n-consistent and asymptotically normal.
Moreover, in finite sample, it achieves the super-efficiency one would obtain
had one known the oracle model for the conditional effect of the RCT enrollment
on the outcome. Consequently, the smaller the working model of the bias induced
by the RWD is, the greater our estimator's efficiency, while our estimator will
always be at least as efficient as an efficient estimator that uses the RCT
data only. A-TMLE outperforms existing methods in simulations by having smaller
mean-squared-error and 95% confidence intervals. A-TMLE could help utilize RWD
to improve the efficiency of randomized trial results without biasing the
estimates of intervention effects. This approach could allow for smaller,
faster trials, decreasing the time until patients can receive effective
treatments.","['Mark van der Laan', 'Sky Qiu', 'Lars van der Laan']","['stat.ME', 'stat.ML']",2024-05-12 07:10:26+00:00
http://arxiv.org/abs/2405.07098v2,Interpretable global minima of deep ReLU neural networks on sequentially separable data,"We explicitly construct zero loss neural network classifiers. We write the
weight matrices and bias vectors in terms of cumulative parameters, which
determine truncation maps acting recursively on input space. The configurations
for the training data considered are (i) sufficiently small, well separated
clusters corresponding to each class, and (ii) equivalence classes which are
sequentially linearly separable. In the best case, for $Q$ classes of data in
$\mathbb{R}^M$, global minimizers can be described with $Q(M+2)$ parameters.","['Thomas Chen', 'Patricia Muñoz Ewald']","['cs.LG', 'cs.AI', 'math-ph', 'math.MP', 'math.OC', 'stat.ML', '57R70, 62M45']",2024-05-11 21:29:40+00:00
http://arxiv.org/abs/2405.07038v1,Conformal Online Auction Design,"This paper proposes the conformal online auction design (COAD), a novel
mechanism for maximizing revenue in online auctions by quantifying the
uncertainty in bidders' values without relying on assumptions about value
distributions. COAD incorporates both the bidder and item features and
leverages historical data to provide an incentive-compatible mechanism for
online auctions. Unlike traditional methods for online auctions, COAD employs a
distribution-free, prediction interval-based approach using conformal
prediction techniques. This novel approach ensures that the expected revenue
from our mechanism can achieve at least a constant fraction of the revenue
generated by the optimal mechanism. Additionally, COAD admits the use of a
broad array of modern machine-learning methods, including random forests,
kernel methods, and deep neural nets, for predicting bidders' values. It
ensures revenue performance under any finite sample of historical data.
Moreover, COAD introduces bidder-specific reserve prices based on the lower
confidence bounds of bidders' valuations, which is different from the uniform
reserve prices commonly used in the literature. We validate our theoretical
predictions through extensive simulations and a real-data application. All code
for using COAD and reproducing results is made available on GitHub.","['Jiale Han', 'Xiaowu Dai']","['cs.GT', 'cs.LG', 'stat.ML']",2024-05-11 15:28:25+00:00
http://arxiv.org/abs/2405.07020v1,Adaptive Online Bayesian Estimation of Frequency Distributions with Local Differential Privacy,"We propose a novel Bayesian approach for the adaptive and online estimation
of the frequency distribution of a finite number of categories under the local
differential privacy (LDP) framework. The proposed algorithm performs Bayesian
parameter estimation via posterior sampling and adapts the randomization
mechanism for LDP based on the obtained posterior samples. We propose a
randomized mechanism for LDP which uses a subset of categories as an input and
whose performance depends on the selected subset and the true frequency
distribution. By using the posterior sample as an estimate of the frequency
distribution, the algorithm performs a computationally tractable subset
selection step to maximize the utility of the privatized response of the next
user. We propose several utility functions related to well-known information
metrics, such as (but not limited to) Fisher information matrix, total
variation distance, and information entropy. We compare each of these utility
metrics in terms of their computational complexity. We employ stochastic
gradient Langevin dynamics for posterior sampling, a computationally efficient
approximate Markov chain Monte Carlo method. We provide a theoretical analysis
showing that (i) the posterior distribution targeted by the algorithm converges
to the true parameter even for approximate posterior sampling, and (ii) the
algorithm selects the optimal subset with high probability if posterior
sampling is performed exactly. We also provide numerical results that
empirically demonstrate the estimation accuracy of our algorithm where we
compare it with nonadaptive and semi-adaptive approaches under experimental
settings with various combinations of privacy parameters and population
distribution parameters.","['Soner Aydin', 'Sinan Yildirim']","['cs.LG', 'cs.CR', 'stat.ML']",2024-05-11 13:59:52+00:00
http://arxiv.org/abs/2405.06902v2,Causal Inference from Slowly Varying Nonstationary Processes,"Causal inference from observational data following the restricted structural
causal models (SCM) framework hinges largely on the asymmetry between cause and
effect from the data generating mechanisms, such as non-Gaussianity or
non-linearity. This methodology can be adapted to stationary time series, yet
inferring causal relationships from nonstationary time series remains a
challenging task. In this work, we propose a new class of restricted SCM, via a
time-varying filter and stationary noise, and exploit the asymmetry from
nonstationarity for causal identification in both bivariate and network
settings. We propose efficient procedures by leveraging powerful estimates of
the bivariate evolutionary spectra for slowly varying processes. Various
synthetic and real datasets that involve high-order and non-smooth filters are
evaluated to demonstrate the effectiveness of our proposed methodology.","['Kang Du', 'Yu Xiang']","['cs.LG', 'stat.ML']",2024-05-11 04:15:47+00:00
http://arxiv.org/abs/2405.06851v1,Nonlinear classification of neural manifolds with contextual information,"Understanding how neural systems efficiently process information through
distributed representations is a fundamental challenge at the interface of
neuroscience and machine learning. Recent approaches analyze the statistical
and geometrical attributes of neural representations as population-level
mechanistic descriptors of task implementation. In particular, manifold
capacity has emerged as a promising framework linking population geometry to
the separability of neural manifolds. However, this metric has been limited to
linear readouts. Here, we propose a theoretical framework that overcomes this
limitation by leveraging contextual input information. We derive an exact
formula for the context-dependent capacity that depends on manifold geometry
and context correlations, and validate it on synthetic and real data. Our
framework's increased expressivity captures representation untanglement in deep
networks at early stages of the layer hierarchy, previously inaccessible to
analysis. As context-dependent nonlinearity is ubiquitous in neural systems,
our data-driven and theoretically grounded approach promises to elucidate
context-dependent computation across scales, datasets, and models.","['Francesca Mignacco', 'Chi-Ning Chou', 'SueYeon Chung']","['q-bio.NC', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.NE', 'stat.ML']",2024-05-10 23:37:31+00:00
http://arxiv.org/abs/2405.06848v1,ISR: Invertible Symbolic Regression,"We introduce an Invertible Symbolic Regression (ISR) method. It is a machine
learning technique that generates analytical relationships between inputs and
outputs of a given dataset via invertible maps (or architectures). The proposed
ISR method naturally combines the principles of Invertible Neural Networks
(INNs) and Equation Learner (EQL), a neural network-based symbolic architecture
for function learning. In particular, we transform the affine coupling blocks
of INNs into a symbolic framework, resulting in an end-to-end differentiable
symbolic invertible architecture that allows for efficient gradient-based
learning. The proposed ISR framework also relies on sparsity promoting
regularization, allowing the discovery of concise and interpretable invertible
expressions. We show that ISR can serve as a (symbolic) normalizing flow for
density estimation tasks. Furthermore, we highlight its practical applicability
in solving inverse problems, including a benchmark inverse kinematics problem,
and notably, a geoacoustic inversion problem in oceanography aimed at inferring
posterior distributions of underlying seabed parameters from acoustic signals.","['Tony Tohme', 'Mohammad Javad Khojasteh', 'Mohsen Sadr', 'Florian Meyer', 'Kamal Youcef-Toumi']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",2024-05-10 23:20:46+00:00
http://arxiv.org/abs/2405.06774v1,Hedging American Put Options with Deep Reinforcement Learning,"This article leverages deep reinforcement learning (DRL) to hedge American
put options, utilizing the deep deterministic policy gradient (DDPG) method.
The agents are first trained and tested with Geometric Brownian Motion (GBM)
asset paths and demonstrate superior performance over traditional strategies
like the Black-Scholes (BS) Delta, particularly in the presence of transaction
costs. To assess the real-world applicability of DRL hedging, a second round of
experiments uses a market calibrated stochastic volatility model to train DRL
agents. Specifically, 80 put options across 8 symbols are collected, stochastic
volatility model coefficients are calibrated for each symbol, and a DRL agent
is trained for each of the 80 options by simulating paths of the respective
calibrated model. Not only do DRL agents outperform the BS Delta method when
testing is conducted using the same calibrated stochastic volatility model data
from training, but DRL agents achieves better results when hedging the true
asset path that occurred between the option sale date and the maturity. As
such, not only does this study present the first DRL agents tailored for
American put option hedging, but results on both simulated and empirical market
testing data also suggest the optimality of DRL agents over the BS Delta method
in real-world scenarios. Finally, note that this study employs a model-agnostic
Chebyshev interpolation method to provide DRL agents with option prices at each
time step when a stochastic volatility model is used, thereby providing a
general framework for an easy extension to more complex underlying asset
processes.","['Reilly Pickard', 'Finn Wredenhagen', 'Julio DeJesus', 'Mario Schlener', 'Yuri Lawryshyn']","['q-fin.RM', 'cs.LG', 'stat.ML', '91G20']",2024-05-10 18:59:12+00:00
http://arxiv.org/abs/2405.06627v3,Conformal Validity Guarantees Exist for Any Data Distribution (and How to Find Them),"As artificial intelligence (AI) / machine learning (ML) gain widespread
adoption, practitioners are increasingly seeking means to quantify and control
the risk these systems incur. This challenge is especially salient when such
systems have autonomy to collect their own data, such as in black-box
optimization and active learning, where their actions induce sequential
feedback-loop shifts in the data distribution. Conformal prediction is a
promising approach to uncertainty and risk quantification, but prior variants'
validity guarantees have assumed some form of ``quasi-exchangeability'' on the
data distribution, thereby excluding many types of sequential shifts. In this
paper we prove that conformal prediction can theoretically be extended to
\textit{any} joint data distribution, not just exchangeable or
quasi-exchangeable ones. Although the most general case is exceedingly
impractical to compute, for concrete practical applications we outline a
procedure for deriving specific conformal algorithms for any data distribution,
and we use this procedure to derive tractable algorithms for a series of
AI/ML-agent-induced covariate shifts. We evaluate the proposed algorithms
empirically on synthetic black-box optimization and active learning tasks.","['Drew Prinster', 'Samuel Stanton', 'Anqi Liu', 'Suchi Saria']","['cs.LG', 'cs.AI', 'stat.ML']",2024-05-10 17:40:24+00:00
http://arxiv.org/abs/2405.06582v3,The Role of Learning Algorithms in Collective Action,"Collective action in machine learning is the study of the control that a
coordinated group can have over machine learning algorithms. While previous
research has concentrated on assessing the impact of collectives against Bayes
(sub-)optimal classifiers, this perspective is limited in that it does not
account for the choice of learning algorithm. Since classifiers seldom behave
like Bayes classifiers and are influenced by the choice of learning algorithms
along with their inherent biases, in this work we initiate the study of how the
choice of the learning algorithm plays a role in the success of a collective in
practical settings. Specifically, we focus on distributionally robust
optimization (DRO), popular for improving a worst group error, and on the
ubiquitous stochastic gradient descent (SGD), due to its inductive bias for
""simpler"" functions. Our empirical results, supported by a theoretical
foundation, show that the effective size and success of the collective are
highly dependent on properties of the learning algorithm. This highlights the
necessity of taking the learning algorithm into account when studying the
impact of collective action in machine learning.","['Omri Ben-Dov', 'Jake Fawkes', 'Samira Samadi', 'Amartya Sanyal']","['cs.LG', 'cs.CY', 'stat.ML']",2024-05-10 16:36:59+00:00
http://arxiv.org/abs/2405.06575v1,No-Regret is not enough! Bandits with General Constraints through Adaptive Regret Minimization,"In the bandits with knapsacks framework (BwK) the learner has $m$
resource-consumption (packing) constraints. We focus on the generalization of
BwK in which the learner has a set of general long-term constraints. The goal
of the learner is to maximize their cumulative reward, while at the same time
achieving small cumulative constraints violations. In this scenario, there
exist simple instances where conventional methods for BwK fail to yield
sublinear violations of constraints. We show that it is possible to circumvent
this issue by requiring the primal and dual algorithm to be weakly adaptive.
Indeed, even in absence on any information on the Slater's parameter $\rho$
characterizing the problem, the interplay between weakly adaptive primal and
dual regret minimizers yields a ""self-bounding"" property of dual variables. In
particular, their norm remains suitably upper bounded across the entire time
horizon even without explicit projection steps. By exploiting this property, we
provide best-of-both-worlds guarantees for stochastic and adversarial inputs.
In the first case, we show that the algorithm guarantees sublinear regret. In
the latter case, we establish a tight competitive ratio of $\rho/(1+\rho)$. In
both settings, constraints violations are guaranteed to be sublinear in time.
Finally, this results allow us to obtain new result for the problem of
contextual bandits with linear constraints, providing the first
no-$\alpha$-regret guarantees for adversarial contexts.","['Martino Bernasconi', 'Matteo Castiglioni', 'Andrea Celli']","['cs.LG', 'stat.ML']",2024-05-10 16:22:33+00:00
http://arxiv.org/abs/2405.06558v2,Random matrix theory improved Fréchet mean of symmetric positive definite matrices,"In this study, we consider the realm of covariance matrices in machine
learning, particularly focusing on computing Fr\'echet means on the manifold of
symmetric positive definite matrices, commonly referred to as Karcher or
geometric means. Such means are leveraged in numerous machine-learning tasks.
Relying on advanced statistical tools, we introduce a random matrix
theory-based method that estimates Fr\'echet means, which is particularly
beneficial when dealing with low sample support and a high number of matrices
to average. Our experimental evaluation, involving both synthetic and
real-world EEG and hyperspectral datasets, shows that we largely outperform
state-of-the-art methods.","['Florent Bouchard', 'Ammar Mian', 'Malik Tiomoko', 'Guillaume Ginolhac', 'Frédéric Pascal']","['stat.ML', 'cs.LG', 'eess.SP', 'stat.ME']",2024-05-10 16:00:29+00:00
http://arxiv.org/abs/2405.06546v1,"Sharp analysis of out-of-distribution error for ""importance-weighted"" estimators in the overparameterized regime","Overparameterized models that achieve zero training error are observed to
generalize well on average, but degrade in performance when faced with data
that is under-represented in the training sample. In this work, we study an
overparameterized Gaussian mixture model imbued with a spurious feature, and
sharply analyze the in-distribution and out-of-distribution test error of a
cost-sensitive interpolating solution that incorporates ""importance weights"".
Compared to recent work Wang et al. (2021), Behnia et al. (2022), our analysis
is sharp with matching upper and lower bounds, and significantly weakens
required assumptions on data dimensionality. Our error characterizations also
apply to any choice of importance weights and unveil a novel tradeoff between
worst-case robustness to distribution shift and average accuracy as a function
of the importance weight magnitude.","['Kuo-Wei Lai', 'Vidya Muthukumar']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2024-05-10 15:43:17+00:00
http://arxiv.org/abs/2405.06727v1,Approximation Error and Complexity Bounds for ReLU Networks on Low-Regular Function Spaces,"In this work, we consider the approximation of a large class of bounded
functions, with minimal regularity assumptions, by ReLU neural networks. We
show that the approximation error can be bounded from above by a quantity
proportional to the uniform norm of the target function and inversely
proportional to the product of network width and depth. We inherit this
approximation error bound from Fourier features residual networks, a type of
neural network that uses complex exponential activation functions. Our proof is
constructive and proceeds by conducting a careful complexity analysis
associated with the approximation of a Fourier features residual network by a
ReLU network.","['Owen Davis', 'Gianluca Geraci', 'Mohammad Motamed']","['stat.ML', 'cs.LG', '41A25, 41A30, 41A46, 68T07']",2024-05-10 14:31:58+00:00
http://arxiv.org/abs/2405.06479v3,Informativeness of Weighted Conformal Prediction,"Weighted conformal prediction (WCP), a recently proposed framework, provides
uncertainty quantification with the flexibility to accommodate different
covariate distributions between training and test data. However, it is pointed
out in this paper that the effectiveness of WCP heavily relies on the overlap
between covariate distributions; insufficient overlap can lead to uninformative
prediction intervals. To enhance the informativeness of WCP, we propose two
methods for scenarios involving multiple sources with varied covariate
distributions. We establish theoretical guarantees for our proposed methods and
demonstrate their efficacy through simulations.","['Mufang Ying', 'Wenge Guo', 'Koulik Khamaru', 'Ying Hung']","['stat.ME', 'stat.ML']",2024-05-10 13:55:08+00:00
http://arxiv.org/abs/2405.06418v2,PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning,"While a number of knowledge graph representation learning (KGRL) methods have
been proposed over the past decade, very few theoretical analyses have been
conducted on them. In this paper, we present the first PAC-Bayesian
generalization bounds for KGRL methods. To analyze a broad class of KGRL
models, we propose a generic framework named ReED (Relation-aware
Encoder-Decoder), which consists of a relation-aware message passing encoder
and a triplet classification decoder. Our ReED framework can express at least
15 different existing KGRL models, including not only graph neural
network-based models such as R-GCN and CompGCN but also shallow-architecture
models such as RotatE and ANALOGY. Our generalization bounds for the ReED
framework provide theoretical grounds for the commonly used tricks in KGRL,
e.g., parameter-sharing and weight normalization schemes, and guide desirable
design choices for practical KGRL methods. We empirically show that the
critical factors in our generalization bounds can explain actual generalization
errors on three real-world knowledge graphs.","['Jaejun Lee', 'Minsung Hwang', 'Joyce Jiyoung Whang']","['cs.LG', 'cs.AI', 'stat.ML']",2024-05-10 12:03:53+00:00
http://arxiv.org/abs/2405.06415v1,Generalization analysis with deep ReLU networks for metric and similarity learning,"While considerable theoretical progress has been devoted to the study of
metric and similarity learning, the generalization mystery is still missing. In
this paper, we study the generalization performance of metric and similarity
learning by leveraging the specific structure of the true metric (the target
function). Specifically, by deriving the explicit form of the true metric for
metric and similarity learning with the hinge loss, we construct a structured
deep ReLU neural network as an approximation of the true metric, whose
approximation ability relies on the network complexity. Here, the network
complexity corresponds to the depth, the number of nonzero weights and the
computation units of the network. Consider the hypothesis space which consists
of the structured deep ReLU networks, we develop the excess generalization
error bounds for a metric and similarity learning problem by estimating the
approximation error and the estimation error carefully. An optimal excess risk
rate is derived by choosing the proper capacity of the constructed hypothesis
space. To the best of our knowledge, this is the first-ever-known
generalization analysis providing the excess generalization error for metric
and similarity learning. In addition, we investigate the properties of the true
metric of metric and similarity learning with general losses.","['Junyu Zhou', 'Puyu Wang', 'Ding-Xuan Zhou']","['stat.ML', 'cs.LG']",2024-05-10 11:55:27+00:00
http://arxiv.org/abs/2405.06107v2,Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar N = 4 Super Yang-Mills Theory,"We pursue the use of deep learning methods to improve state-of-the-art
computations in theoretical high-energy physics. Planar N = 4 Super Yang-Mills
theory is a close cousin to the theory that describes Higgs boson production at
the Large Hadron Collider; its scattering amplitudes are large mathematical
expressions containing integer coefficients. In this paper, we apply
Transformers to predict these coefficients. The problem can be formulated in a
language-like representation amenable to standard cross-entropy training
objectives. We design two related experiments and show that the model achieves
high accuracy (> 98%) on both tasks. Our work shows that Transformers can be
applied successfully to problems in theoretical physics that require exact
solutions.","['Tianji Cai', 'Garrett W. Merz', 'François Charton', 'Niklas Nolte', 'Matthias Wilhelm', 'Kyle Cranmer', 'Lance J. Dixon']","['cs.LG', 'cs.SC', 'hep-ph', 'hep-th', 'stat.ML']",2024-05-09 21:28:52+00:00
http://arxiv.org/abs/2405.06068v1,Deep Learning-Based Residual Useful Lifetime Prediction for Assets with Uncertain Failure Modes,"Industrial prognostics focuses on utilizing degradation signals to forecast
and continually update the residual useful life of complex engineering systems.
However, existing prognostic models for systems with multiple failure modes
face several challenges in real-world applications, including overlapping
degradation signals from multiple components, the presence of unlabeled
historical data, and the similarity of signals across different failure modes.
To tackle these issues, this research introduces two prognostic models that
integrate the mixture (log)-location-scale distribution with deep learning.
This integration facilitates the modeling of overlapping degradation signals,
eliminates the need for explicit failure mode identification, and utilizes deep
learning to capture complex nonlinear relationships between degradation signals
and residual useful lifetimes. Numerical studies validate the superior
performance of these proposed models compared to existing methods.","['Yuqi Su', 'Xiaolei Fang']","['cs.LG', 'eess.SP', 'stat.AP', 'stat.ML']",2024-05-09 19:37:57+00:00
http://arxiv.org/abs/2405.06008v2,Wilsonian Renormalization of Neural Network Gaussian Processes,"Separating relevant and irrelevant information is key to any modeling process
or scientific inquiry. Theoretical physics offers a powerful tool for achieving
this in the form of the renormalization group (RG). Here we demonstrate a
practical approach to performing Wilsonian RG in the context of Gaussian
Process (GP) Regression. We systematically integrate out the unlearnable modes
of the GP kernel, thereby obtaining an RG flow of the GP in which the data sets
the IR scale. In simple cases, this results in a universal flow of the ridge
parameter, which becomes input-dependent in the richer scenario in which
non-Gaussianities are included. In addition to being analytically tractable,
this approach goes beyond structural analogies between RG and neural networks
by providing a natural connection between RG flow and learnable vs. unlearnable
modes. Studying such flows may improve our understanding of feature learning in
deep neural networks, and enable us to identify potential universality classes
in these models.","['Jessica N. Howard', 'Ro Jefferson', 'Anindita Maiti', 'Zohar Ringel']","['cs.LG', 'cond-mat.dis-nn', 'hep-th', 'stat.ML']",2024-05-09 18:00:00+00:00
http://arxiv.org/abs/2405.05968v2,A Universal Growth Rate for Learning with Smooth Surrogate Losses,"This paper presents a comprehensive analysis of the growth rate of
$H$-consistency bounds (and excess error bounds) for various surrogate losses
used in classification. We prove a square-root growth rate near zero for smooth
margin-based surrogate losses in binary classification, providing both upper
and lower bounds under mild assumptions. This result also translates to excess
error bounds. Our lower bound requires weaker conditions than those in previous
work for excess error bounds, and our upper bound is entirely novel. Moreover,
we extend this analysis to multi-class classification with a series of novel
results, demonstrating a universal square-root growth rate for smooth comp-sum
and constrained losses, covering common choices for training neural networks in
multi-class classification. Given this universal rate, we turn to the question
of choosing among different surrogate losses. We first examine how
$H$-consistency bounds vary across surrogates based on the number of classes.
Next, ignoring constants and focusing on behavior near zero, we identify
minimizability gaps as the key differentiating factor in these bounds. Thus, we
thoroughly analyze these gaps, to guide surrogate loss selection, covering:
comparisons across different comp-sum losses, conditions where gaps become
zero, and general conditions leading to small gaps. Additionally, we
demonstrate the key role of minimizability gaps in comparing excess error
bounds and $H$-consistency bounds.","['Anqi Mao', 'Mehryar Mohri', 'Yutao Zhong']","['cs.LG', 'stat.ML']",2024-05-09 17:59:55+00:00
http://arxiv.org/abs/2405.05950v1,Federated Combinatorial Multi-Agent Multi-Armed Bandits,"This paper introduces a federated learning framework tailored for online
combinatorial optimization with bandit feedback. In this setting, agents select
subsets of arms, observe noisy rewards for these subsets without accessing
individual arm information, and can cooperate and share information at specific
intervals. Our framework transforms any offline resilient single-agent
$(\alpha-\epsilon)$-approximation algorithm, having a complexity of
$\tilde{\mathcal{O}}(\frac{\psi}{\epsilon^\beta})$, where the logarithm is
omitted, for some function $\psi$ and constant $\beta$, into an online
multi-agent algorithm with $m$ communicating agents and an $\alpha$-regret of
no more than $\tilde{\mathcal{O}}(m^{-\frac{1}{3+\beta}} \psi^\frac{1}{3+\beta}
T^\frac{2+\beta}{3+\beta})$. This approach not only eliminates the $\epsilon$
approximation error but also ensures sublinear growth with respect to the time
horizon $T$ and demonstrates a linear speedup with an increasing number of
communicating agents. Additionally, the algorithm is notably
communication-efficient, requiring only a sublinear number of communication
rounds, quantified as $\tilde{\mathcal{O}}\left(\psi
T^\frac{\beta}{\beta+1}\right)$. Furthermore, the framework has been
successfully applied to online stochastic submodular maximization using various
offline algorithms, yielding the first results for both single-agent and
multi-agent settings and recovering specialized single-agent theoretical
guarantees. We empirically validate our approach to a stochastic data
summarization problem, illustrating the effectiveness of the proposed
framework, even in single-agent scenarios.","['Fares Fourati', 'Mohamed-Slim Alouini', 'Vaneet Aggarwal']","['cs.LG', 'cs.AI', 'cs.DM', 'cs.MA', 'stat.ML']",2024-05-09 17:40:09+00:00
http://arxiv.org/abs/2405.05934v1,Theoretical Guarantees of Data Augmented Last Layer Retraining Methods,"Ensuring fair predictions across many distinct subpopulations in the training
data can be prohibitive for large models. Recently, simple linear last layer
retraining strategies, in combination with data augmentation methods such as
upweighting, downsampling and mixup, have been shown to achieve
state-of-the-art performance for worst-group accuracy, which quantifies
accuracy for the least prevalent subpopulation. For linear last layer
retraining and the abovementioned augmentations, we present the optimal
worst-group accuracy when modeling the distribution of the latent
representations (input to the last layer) as Gaussian for each subpopulation.
We evaluate and verify our results for both synthetic and large publicly
available datasets.","['Monica Welfert', 'Nathan Stromberg', 'Lalitha Sankar']","['cs.LG', 'cs.CV', 'cs.IT', 'math.IT', 'stat.ML']",2024-05-09 17:16:54+00:00
http://arxiv.org/abs/2405.06003v1,Binary Hypothesis Testing for Softmax Models and Leverage Score Models,"Softmax distributions are widely used in machine learning, including Large
Language Models (LLMs) where the attention unit uses softmax distributions. We
abstract the attention unit as the softmax model, where given a vector input,
the model produces an output drawn from the softmax distribution (which depends
on the vector input). We consider the fundamental problem of binary hypothesis
testing in the setting of softmax models. That is, given an unknown softmax
model, which is known to be one of the two given softmax models, how many
queries are needed to determine which one is the truth? We show that the sample
complexity is asymptotically $O(\epsilon^{-2})$ where $\epsilon$ is a certain
distance between the parameters of the models.
  Furthermore, we draw analogy between the softmax model and the leverage score
model, an important tool for algorithm design in linear algebra and graph
theory. The leverage score model, on a high level, is a model which, given
vector input, produces an output drawn from a distribution dependent on the
input. We obtain similar results for the binary hypothesis testing problem for
leverage score models.","['Yeqi Gao', 'Yuzhou Gu', 'Zhao Song']","['stat.ML', 'cs.LG']",2024-05-09 15:56:29+00:00
http://arxiv.org/abs/2405.05852v1,Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control,"Embodied AI agents require a fine-grained understanding of the physical world
mediated through visual and language inputs. Such capabilities are difficult to
learn solely from task-specific data. This has led to the emergence of
pre-trained vision-language models as a tool for transferring representations
learned from internet-scale data to downstream tasks and new domains. However,
commonly used contrastively trained representations such as in CLIP have been
shown to fail at enabling embodied agents to gain a sufficiently fine-grained
scene understanding -- a capability vital for control. To address this
shortcoming, we consider representations from pre-trained text-to-image
diffusion models, which are explicitly optimized to generate images from text
prompts and as such, contain text-conditioned representations that reflect
highly fine-grained visuo-spatial information. Using pre-trained text-to-image
diffusion models, we construct Stable Control Representations which allow
learning downstream control policies that generalize to complex, open-ended
environments. We show that policies learned using Stable Control
Representations are competitive with state-of-the-art representation learning
approaches across a broad range of simulated control settings, encompassing
challenging manipulation and navigation tasks. Most notably, we show that
Stable Control Representations enable learning policies that exhibit
state-of-the-art performance on OVMM, a difficult open-vocabulary navigation
benchmark.","['Gunshi Gupta', 'Karmesh Yadav', 'Yarin Gal', 'Dhruv Batra', 'Zsolt Kira', 'Cong Lu', 'Tim G. J. Rudner']","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.RO', 'stat.ML']",2024-05-09 15:39:54+00:00
http://arxiv.org/abs/2405.05733v2,Batched Stochastic Bandit for Nondegenerate Functions,"This paper studies batched bandit learning problems for nondegenerate
functions. We introduce an algorithm that solves the batched bandit problem for
nondegenerate functions near-optimally. More specifically, we introduce an
algorithm, called Geometric Narrowing (GN), whose regret bound is of order
$\widetilde{{\mathcal{O}}} ( A_{+}^d \sqrt{T} )$. In addition, GN only needs
$\mathcal{O} (\log \log T)$ batches to achieve this regret. We also provide
lower bound analysis for this problem. More specifically, we prove that over
some (compact) doubling metric space of doubling dimension $d$: 1. For any
policy $\pi$, there exists a problem instance on which $\pi$ admits a regret of
order ${\Omega} ( A_-^d \sqrt{T})$; 2. No policy can achieve a regret of order
$ A_-^d \sqrt{T} $ over all problem instances, using less than $ \Omega ( \log
\log T ) $ rounds of communications. Our lower bound analysis shows that the GN
algorithm achieves near optimal regret with minimal number of batches.","['Yu Liu', 'Yunlu Shu', 'Tianyu Wang']","['stat.ML', 'cs.LG']",2024-05-09 12:50:16+00:00
http://arxiv.org/abs/2405.05695v1,Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost,"We aim at exploiting additional auxiliary labels from an independent
(auxiliary) task to boost the primary task performance which we focus on, while
preserving a single task inference cost of the primary task. While most
existing auxiliary learning methods are optimization-based relying on loss
weights/gradients manipulation, our method is architecture-based with a
flexible asymmetric structure for the primary and auxiliary tasks, which
produces different networks for training and inference. Specifically, starting
from two single task networks/branches (each representing a task), we propose a
novel method with evolving networks where only primary-to-auxiliary links exist
as the cross-task connections after convergence. These connections can be
removed during the primary task inference, resulting in a single-task inference
cost. We achieve this by formulating a Neural Architecture Search (NAS)
problem, where we initialize bi-directional connections in the search space and
guide the NAS optimization converging to an architecture with only the
single-side primary-to-auxiliary connections. Moreover, our method can be
incorporated with optimization-based auxiliary learning approaches. Extensive
experiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using
VGG, ResNet, and ViT backbones validate the promising performance. The codes
are available at https://github.com/ethanygao/Aux-NAS.","['Yuan Gao', 'Weizhong Zhang', 'Wenhan Luo', 'Lin Ma', 'Jin-Gang Yu', 'Gui-Song Xia', 'Jiayi Ma']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2024-05-09 11:50:19+00:00
http://arxiv.org/abs/2405.05679v1,Non-asymptotic estimates for accelerated high order Langevin Monte Carlo algorithms,"In this paper, we propose two new algorithms, namely aHOLA and aHOLLA, to
sample from high-dimensional target distributions with possibly super-linearly
growing potentials. We establish non-asymptotic convergence bounds for aHOLA in
Wasserstein-1 and Wasserstein-2 distances with rates of convergence equal to
$1+q/2$ and $1/2+q/4$, respectively, under a local H\""{o}lder condition with
exponent $q\in(0,1]$ and a convexity at infinity condition on the potential of
the target distribution. Similar results are obtained for aHOLLA under certain
global continuity conditions and a dissipativity condition. Crucially, we
achieve state-of-the-art rates of convergence of the proposed algorithms in the
non-convex setting which are higher than those of the existing algorithms.
Numerical experiments are conducted to sample from several distributions and
the results support our main findings.","['Ariel Neufeld', 'Ying Zhang']","['math.ST', 'math.PR', 'stat.CO', 'stat.ML', 'stat.TH']",2024-05-09 11:12:03+00:00
http://arxiv.org/abs/2405.05673v1,Imprecise Multi-Armed Bandits,"We introduce a novel multi-armed bandit framework, where each arm is
associated with a fixed unknown credal set over the space of outcomes (which
can be richer than just the reward). The arm-to-credal-set correspondence comes
from a known class of hypotheses. We then define a notion of regret
corresponding to the lower prevision defined by these credal sets.
Equivalently, the setting can be regarded as a two-player zero-sum game, where,
on each round, the agent chooses an arm and the adversary chooses the
distribution over outcomes from a set of options associated with this arm. The
regret is defined with respect to the value of game. For certain natural
hypothesis classes, loosely analgous to stochastic linear bandits (which are a
special case of the resulting setting), we propose an algorithm and prove a
corresponding upper bound on regret. We also prove lower bounds on regret for
particular special cases.",['Vanessa Kosoy'],"['cs.LG', 'stat.ML']",2024-05-09 10:58:40+00:00
http://arxiv.org/abs/2405.05646v2,Outlier-robust Kalman Filtering through Generalised Bayes,"We derive a novel, provably robust, and closed-form Bayesian update rule for
online filtering in state-space models in the presence of outliers and
misspecified measurement models. Our method combines generalised Bayesian
inference with filtering methods such as the extended and ensemble Kalman
filter. We use the former to show robustness and the latter to ensure
computational efficiency in the case of nonlinear models. Our method matches or
outperforms other robust filtering methods (such as those based on variational
Bayes) at a much lower computational cost. We show this empirically on a range
of filtering problems with outlier measurements, such as object tracking, state
estimation in high-dimensional chaotic systems, and online learning of neural
networks.","['Gerardo Duran-Martin', 'Matias Altamirano', 'Alexander Y. Shestopaloff', 'Leandro Sánchez-Betancourt', 'Jeremias Knoblauch', 'Matt Jones', 'François-Xavier Briol', 'Kevin Murphy']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY']",2024-05-09 09:40:56+00:00
http://arxiv.org/abs/2405.05545v1,Deep Hierarchical Graph Alignment Kernels,"Typical R-convolution graph kernels invoke the kernel functions that
decompose graphs into non-isomorphic substructures and compare them. However,
overlooking implicit similarities and topological position information between
those substructures limits their performances. In this paper, we introduce Deep
Hierarchical Graph Alignment Kernels (DHGAK) to resolve this problem.
Specifically, the relational substructures are hierarchically aligned to
cluster distributions in their deep embedding space. The substructures
belonging to the same cluster are assigned the same feature map in the
Reproducing Kernel Hilbert Space (RKHS), where graph feature maps are derived
by kernel mean embedding. Theoretical analysis guarantees that DHGAK is
positive semi-definite and has linear separability in the RKHS. Comparison with
state-of-the-art graph kernels on various benchmark datasets demonstrates the
effectiveness and efficiency of DHGAK. The code is available at Github
(https://github.com/EWesternRa/DHGAK).","['Shuhao Tang', 'Hao Tian', 'Xiaofeng Cao', 'Wei Ye']","['cs.LG', 'stat.ML']",2024-05-09 05:08:30+00:00
http://arxiv.org/abs/2405.05485v1,Variance Control for Black Box Variational Inference Using The James-Stein Estimator,"Black Box Variational Inference is a promising framework in a succession of
recent efforts to make Variational Inference more ``black box"". However, in
basic version it either fails to converge due to instability or requires some
fine-tuning of the update steps prior to execution that hinder it from being
completely general purpose. We propose a method for regulating its parameter
updates by reframing stochastic gradient ascent as a multivariate estimation
problem. We examine the properties of the James-Stein estimator as a
replacement for the arithmetic mean of Monte Carlo estimates of the gradient of
the evidence lower bound. The proposed method provides relatively weaker
variance reduction than Rao-Blackwellization, but offers a tradeoff of being
simpler and requiring no fine tuning on the part of the analyst. Performance on
benchmark datasets also demonstrate a consistent performance at par or better
than the Rao-Blackwellized approach in terms of model fit and time to
convergence.",['Dominic B. Dayta'],"['cs.LG', 'stat.ML']",2024-05-09 01:04:34+00:00
http://arxiv.org/abs/2405.05468v1,Model-Free Robust $φ$-Divergence Reinforcement Learning Using Both Offline and Online Data,"The robust $\phi$-regularized Markov Decision Process (RRMDP) framework
focuses on designing control policies that are robust against parameter
uncertainties due to mismatches between the simulator (nominal) model and
real-world settings. This work makes two important contributions. First, we
propose a model-free algorithm called Robust $\phi$-regularized fitted
Q-iteration (RPQ) for learning an $\epsilon$-optimal robust policy that uses
only the historical data collected by rolling out a behavior policy (with
robust exploratory requirement) on the nominal model. To the best of our
knowledge, we provide the first unified analysis for a class of
$\phi$-divergences achieving robust optimal policies in high-dimensional
systems with general function approximation. Second, we introduce the hybrid
robust $\phi$-regularized reinforcement learning framework to learn an optimal
robust policy using both historical data and online sampling. Towards this
framework, we propose a model-free algorithm called Hybrid robust
Total-variation-regularized Q-iteration (HyTQ: pronounced height-Q). To the
best of our knowledge, we provide the first improved out-of-data-distribution
assumption in large-scale problems with general function approximation under
the hybrid robust $\phi$-regularized reinforcement learning framework. Finally,
we provide theoretical guarantees on the performance of the learned policies of
our algorithms on systems with arbitrary large state space.","['Kishan Panaganti', 'Adam Wierman', 'Eric Mazumdar']","['cs.LG', 'stat.ML']",2024-05-08 23:52:37+00:00
http://arxiv.org/abs/2405.05429v3,How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression,"Neural network representations of simple models, such as linear regression,
are being studied increasingly to better understand the underlying principles
of deep learning algorithms. However, neural representations of distributional
regression models, such as the Cox model, have received little attention so
far. We close this gap by proposing a framework for distributional regression
using inverse flow transformations (DRIFT), which includes neural
representations of the aforementioned models. We empirically demonstrate that
the neural representations of models in DRIFT can serve as a substitute for
their classical statistical counterparts in several applications involving
continuous, ordered, time-series, and survival outcomes. We confirm that models
in DRIFT empirically match the performance of several statistical methods in
terms of estimation of partial effects, prediction, and aleatoric uncertainty
quantification. DRIFT covers both interpretable statistical models and flexible
neural networks opening up new avenues in both statistical modeling and deep
learning.","['Lucas Kook', 'Chris Kolb', 'Philipp Schiele', 'Daniel Dold', 'Marcel Arpogaus', 'Cornelius Fritz', 'Philipp F. Baumann', 'Philipp Kopper', 'Tobias Pielok', 'Emilio Dorigatti', 'David Rügamer']","['cs.LG', 'cs.AI', 'stat.CO', 'stat.ML']",2024-05-08 21:19:18+00:00
http://arxiv.org/abs/2405.05398v1,ASPIRE: Iterative Amortized Posterior Inference for Bayesian Inverse Problems,"Due to their uncertainty quantification, Bayesian solutions to inverse
problems are the framework of choice in applications that are risk averse.
These benefits come at the cost of computations that are in general,
intractable. New advances in machine learning and variational inference (VI)
have lowered the computational barrier by learning from examples. Two VI
paradigms have emerged that represent different tradeoffs: amortized and
non-amortized. Amortized VI can produce fast results but due to generalizing to
many observed datasets it produces suboptimal inference results. Non-amortized
VI is slower at inference but finds better posterior approximations since it is
specialized towards a single observed dataset. Current amortized VI techniques
run into a sub-optimality wall that can not be improved without more expressive
neural networks or extra training data. We present a solution that enables
iterative improvement of amortized posteriors that uses the same networks
architectures and training data. The benefits of our method requires extra
computations but these remain frugal since they are based on physics-hybrid
methods and summary statistics. Importantly, these computations remain mostly
offline thus our method maintains cheap and reusable online evaluation while
bridging the approximation gap these two paradigms. We denote our proposed
method ASPIRE - Amortized posteriors with Summaries that are Physics-based and
Iteratively REfined. We first validate our method on a stylized problem with a
known posterior then demonstrate its practical use on a high-dimensional and
nonlinear transcranial medical imaging problem with ultrasound. Compared with
the baseline and previous methods from the literature our method stands out as
an computationally efficient and high-fidelity method for posterior inference.","['Rafael Orozco', 'Ali Siahkoohi', 'Mathias Louboutin', 'Felix J. Herrmann']","['cs.LG', 'stat.ML']",2024-05-08 20:03:12+00:00
http://arxiv.org/abs/2405.05393v1,Mutual information and the encoding of contingency tables,"Mutual information is commonly used as a measure of similarity between
competing labelings of a given set of objects, for example to quantify
performance in classification and community detection tasks. As argued
recently, however, the mutual information as conventionally defined can return
biased results because it neglects the information cost of the so-called
contingency table, a crucial component of the similarity calculation. In
principle the bias can be rectified by subtracting the appropriate information
cost, leading to the modified measure known as the reduced mutual information,
but in practice one can only ever compute an upper bound on this information
cost, and the value of the reduced mutual information depends crucially on how
good a bound is established. In this paper we describe an improved method for
encoding contingency tables that gives a substantially better bound in typical
use cases, and approaches the ideal value in the common case where the
labelings are closely similar, as we demonstrate with extensive numerical
results.","['Maximilian Jerdee', 'Alec Kirkley', 'M. E. J. Newman']","['cs.SI', 'cond-mat.stat-mech', 'stat.ML']",2024-05-08 19:49:39+00:00
http://arxiv.org/abs/2405.05386v1,Interpretability Needs a New Paradigm,"Interpretability is the study of explaining models in understandable terms to
humans. At present, interpretability is divided into two paradigms: the
intrinsic paradigm, which believes that only models designed to be explained
can be explained, and the post-hoc paradigm, which believes that black-box
models can be explained. At the core of this debate is how each paradigm
ensures its explanations are faithful, i.e., true to the model's behavior. This
is important, as false but convincing explanations lead to unsupported
confidence in artificial intelligence (AI), which can be dangerous. This
paper's position is that we should think about new paradigms while staying
vigilant regarding faithfulness. First, by examining the history of paradigms
in science, we see that paradigms are constantly evolving. Then, by examining
the current paradigms, we can understand their underlying beliefs, the value
they bring, and their limitations. Finally, this paper presents 3 emerging
paradigms for interpretability. The first paradigm designs models such that
faithfulness can be easily measured. Another optimizes models such that
explanations become faithful. The last paradigm proposes to develop models that
produce both a prediction and an explanation.","['Andreas Madsen', 'Himabindu Lakkaraju', 'Siva Reddy', 'Sarath Chandar']","['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']",2024-05-08 19:31:06+00:00
http://arxiv.org/abs/2405.05369v2,Model Reconstruction Using Counterfactual Explanations: A Perspective From Polytope Theory,"Counterfactual explanations provide ways of achieving a favorable model
outcome with minimum input perturbation. However, counterfactual explanations
can also be leveraged to reconstruct the model by strategically training a
surrogate model to give similar predictions as the original (target) model. In
this work, we analyze how model reconstruction using counterfactuals can be
improved by further leveraging the fact that the counterfactuals also lie quite
close to the decision boundary. Our main contribution is to derive novel
theoretical relationships between the error in model reconstruction and the
number of counterfactual queries required using polytope theory. Our
theoretical analysis leads us to propose a strategy for model reconstruction
that we call Counterfactual Clamping Attack (CCA) which trains a surrogate
model using a unique loss function that treats counterfactuals differently than
ordinary instances. Our approach also alleviates the related problem of
decision boundary shift that arises in existing model reconstruction approaches
when counterfactuals are treated as ordinary instances. Experimental results
demonstrate that our strategy improves fidelity between the target and
surrogate model predictions on several datasets.","['Pasan Dissanayake', 'Sanghamitra Dutta']","['cs.LG', 'cs.CR', 'cs.CY', 'cs.IT', 'math.IT', 'stat.ML']",2024-05-08 18:52:47+00:00
http://arxiv.org/abs/2405.05190v2,Is Transductive Learning Equivalent to PAC Learning?,"Much of learning theory is concerned with the design and analysis of probably
approximately correct (PAC) learners. The closely related transductive model of
learning has recently seen more scrutiny, with its learners often used as
precursors to PAC learners. Our goal in this work is to understand and quantify
the exact relationship between these two models. First, we observe that modest
extensions of existing results show the models to be essentially equivalent for
realizable learning for most natural loss functions, up to low order terms in
the error and sample complexity. The situation for agnostic learning appears
less straightforward, with sample complexities potentially separated by a
$\frac{1}{\epsilon}$ factor. This is therefore where our main contributions
lie. Our results are two-fold:
  1. For agnostic learning with bounded losses (including, for example,
multiclass classification), we show that PAC learning reduces to transductive
learning at the cost of low-order terms in the error and sample complexity via
an adaptation of the reduction of arXiv:2304.09167 to the agnostic setting.
  2. For agnostic binary classification, we show the converse: transductive
learning is essentially no more difficult than PAC learning. Together with our
first result this implies that the PAC and transductive models are essentially
equivalent for agnostic binary classification. This is our most technical
result, and involves two steps: A symmetrization argument on the agnostic
one-inclusion graph (OIG) of arXiv:2309.13692 to derive the worst-case agnostic
transductive instance, and expressing the error of the agnostic OIG algorithm
for this instance in terms of the empirical Rademacher complexity of the class.
  We leave as an intriguing open question whether our second result can be
extended beyond binary classification to show the transductive and PAC models
equivalent more broadly.","['Shaddin Dughmi', 'Yusuf Kalayci', 'Grayson York']","['stat.ML', 'cs.DS', 'cs.LG', 'math.ST', 'stat.TH']",2024-05-08 16:26:49+00:00
http://arxiv.org/abs/2405.05110v1,Uncertainty quantification in metric spaces,"This paper introduces a novel uncertainty quantification framework for
regression models where the response takes values in a separable metric space,
and the predictors are in a Euclidean space. The proposed algorithms can
efficiently handle large datasets and are agnostic to the predictive base model
used. Furthermore, the algorithms possess asymptotic consistency guarantees
and, in some special homoscedastic cases, we provide non-asymptotic guarantees.
To illustrate the effectiveness of the proposed uncertainty quantification
framework, we use a linear regression model for metric responses (known as the
global Fr\'echet model) in various clinical applications related to precision
and digital medicine. The different clinical outcomes analyzed are represented
as complex statistical objects, including multivariate Euclidean data,
Laplacian graphs, and probability distributions.","['Gábor Lugosi', 'Marcos Matabuena']","['math.ST', 'stat.ML', 'stat.TH']",2024-05-08 15:06:02+00:00
http://arxiv.org/abs/2405.05097v4,Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional neural networks,"Biological neural networks seem qualitatively superior (e.g. in learning,
flexibility, robustness) to current artificial like Multi-Layer Perceptron
(MLP) or Kolmogorov-Arnold Network (KAN). Simultaneously, in contrast to them:
biological have fundamentally multidirectional signal propagation~\cite{axon},
also of probability distributions e.g. for uncertainty estimation, and are
believed not being able to use standard backpropagation
training~\cite{backprop}. There are proposed novel artificial neurons based on
HCR (Hierarchical Correlation Reconstruction) allowing to remove the above low
level differences: with neurons containing local joint distribution model (of
its connections), representing joint density on normalized variables as just
linear combination of $(f_\mathbf{j})$ orthonormal polynomials:
$\rho(\mathbf{x})=\sum_{\mathbf{j}\in B} a_\mathbf{j} f_\mathbf{j}(\mathbf{x})$
for $\mathbf{x} \in [0,1]^d$ and $B$ some chosen basis, approaching complete
description of joint distribution with basis growth. By various index
summations of such $(a_\mathbf{j})$ tensor as neuron parameters, we get simple
formulas for e.g. conditional expected values for propagation in any direction,
like $E[x|y,z]$, $E[y|x]$, which degenerate to KAN-like parametrization if
restricting to pairwise dependencies. Such HCR network can also propagate
probability distributions (also joint) like $\rho(y,z|x)$. It also allows for
additional training approaches, like direct $(a_\mathbf{j})$ estimation,
through tensor decomposition, or more biologically plausible information
bottleneck training: layers directly influencing only neighbors, optimizing
content to maximize information about the next layer, and minimizing about the
previous to remove noise, extract crucial information.",['Jarek Duda'],"['cs.LG', 'stat.ML']",2024-05-08 14:49:27+00:00
http://arxiv.org/abs/2405.05081v1,Robust deep learning from weakly dependent data,"Recent developments on deep learning established some theoretical properties
of deep neural networks estimators. However, most of the existing works on this
topic are restricted to bounded loss functions or (sub)-Gaussian or bounded
input. This paper considers robust deep learning from weakly dependent
observations, with unbounded loss function and unbounded input/output. It is
only assumed that the output variable has a finite $r$ order moment, with $r
>1$. Non asymptotic bounds for the expected excess risk of the deep neural
network estimator are established under strong mixing, and $\psi$-weak
dependence assumptions on the observations. We derive a relationship between
these bounds and $r$, and when the data have moments of any order (that is
$r=\infty$), the convergence rate is close to some well-known results. When the
target predictor belongs to the class of H\""older smooth functions with
sufficiently large smoothness index, the rate of the expected excess risk for
exponentially strongly mixing data is close to or as same as those for obtained
with i.i.d. samples. Application to robust nonparametric regression and robust
nonparametric autoregression are considered. The simulation study for models
with heavy-tailed errors shows that, robust estimators with absolute loss and
Huber loss function outperform the least squares method.","['William Kengne', 'Modou Wade']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2024-05-08 14:25:40+00:00
http://arxiv.org/abs/2405.05033v1,Multi-fidelity Hamiltonian Monte Carlo,"Numerous applications in biology, statistics, science, and engineering
require generating samples from high-dimensional probability distributions. In
recent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a
state-of-the-art Markov chain Monte Carlo technique, exploiting the shape of
such high-dimensional target distributions to efficiently generate samples.
Despite its impressive empirical success and increasing popularity, its
wide-scale adoption remains limited due to the high computational cost of
gradient calculation. Moreover, applying this method is impossible when the
gradient of the posterior cannot be computed (for example, with black-box
simulators). To overcome these challenges, we propose a novel two-stage
Hamiltonian Monte Carlo algorithm with a surrogate model. In this
multi-fidelity algorithm, the acceptance probability is computed in the first
stage via a standard HMC proposal using an inexpensive differentiable surrogate
model, and if the proposal is accepted, the posterior is evaluated in the
second stage using the high-fidelity (HF) numerical solver. Splitting the
standard HMC algorithm into these two stages allows for approximating the
gradient of the posterior efficiently, while producing accurate posterior
samples by using HF numerical solvers in the second stage. We demonstrate the
effectiveness of this algorithm for a range of problems, including linear and
nonlinear Bayesian inverse problems with in-silico data and experimental data.
The proposed algorithm is shown to seamlessly integrate with various
low-fidelity and HF models, priors, and datasets. Remarkably, our proposed
method outperforms the traditional HMC algorithm in both computational and
statistical efficiency by several orders of magnitude, all while retaining or
improving the accuracy in computed posterior statistics.","['Dhruv V. Patel', 'Jonghyun Lee', 'Matthew W. Farthing', 'Peter K. Kitanidis', 'Eric F. Darve']","['cs.CE', 'cs.LG', 'stat.ML']",2024-05-08 13:03:55+00:00
http://arxiv.org/abs/2405.05025v1,"Learning Structural Causal Models through Deep Generative Models: Methods, Guarantees, and Challenges","This paper provides a comprehensive review of deep structural causal models
(DSCMs), particularly focusing on their ability to answer counterfactual
queries using observational data within known causal structures. It delves into
the characteristics of DSCMs by analyzing the hypotheses, guarantees, and
applications inherent to the underlying deep learning components and structural
causal models, fostering a finer understanding of their capabilities and
limitations in addressing different counterfactual queries. Furthermore, it
highlights the challenges and open questions in the field of deep structural
causal modeling. It sets the stages for researchers to identify future work
directions and for practitioners to get an overview in order to find out the
most appropriate methods for their needs.","['Audrey Poinsot', 'Alessandro Leite', 'Nicolas Chesneau', 'Michèle Sébag', 'Marc Schoenauer']","['stat.ML', 'cs.LG']",2024-05-08 12:56:33+00:00
http://arxiv.org/abs/2405.05294v1,Harmonizing Program Induction with Rate-Distortion Theory,"Many aspects of human learning have been proposed as a process of
constructing mental programs: from acquiring symbolic number representations to
intuitive theories about the world. In parallel, there is a long-tradition of
using information processing to model human cognition through Rate Distortion
Theory (RDT). Yet, it is still poorly understood how to apply RDT when mental
representations take the form of programs. In this work, we adapt RDT by
proposing a three way trade-off among rate (description length), distortion
(error), and computational costs (search budget). We use simulations on a
melody task to study the implications of this trade-off, and show that
constructing a shared program library across tasks provides global benefits.
However, this comes at the cost of sensitivity to curricula, which is also
characteristic of human learners. Finally, we use methods from partial
information decomposition to generate training curricula that induce more
effective libraries and better generalization.","['Hanqi Zhou', 'David G. Nagy', 'Charley M. Wu']","['cs.HC', 'cs.CL', 'cs.IT', 'cs.LG', 'cs.SC', 'math.IT', 'stat.ML']",2024-05-08 10:03:50+00:00
http://arxiv.org/abs/2405.04919v1,Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression,"We describe a fast computation method for leave-one-out cross-validation
(LOOCV) for $k$-nearest neighbours ($k$-NN) regression. We show that, under a
tie-breaking condition for nearest neighbours, the LOOCV estimate of the mean
square error for $k$-NN regression is identical to the mean square error of
$(k+1)$-NN regression evaluated on the training data, multiplied by the scaling
factor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs to
fit $(k+1)$-NN regression only once, and does not need to repeat
training-validation of $k$-NN regression for the number of training data.
Numerical experiments confirm the validity of the fast computation method.",['Motonobu Kanagawa'],"['stat.ML', 'cs.DS', 'cs.LG', 'stat.CO', 'stat.ME']",2024-05-08 09:41:25+00:00
http://arxiv.org/abs/2405.04917v1,Guiding adaptive shrinkage by co-data to improve regression-based prediction and feature selection,"The high dimensional nature of genomics data complicates feature selection,
in particular in low sample size studies - not uncommon in clinical prediction
settings. It is widely recognized that complementary data on the features,
`co-data', may improve results. Examples are prior feature groups or p-values
from a related study. Such co-data are ubiquitous in genomics settings due to
the availability of public repositories. Yet, the uptake of learning methods
that structurally use such co-data is limited. We review guided adaptive
shrinkage methods: a class of regression-based learners that use co-data to
adapt the shrinkage parameters, crucial for the performance of those learners.
We discuss technical aspects, but also the applicability in terms of types of
co-data that can be handled. This class of methods is contrasted with several
others. In particular, group-adaptive shrinkage is compared with the
better-known sparse group-lasso by evaluating feature selection. Finally, we
demonstrate the versatility of the guided shrinkage methodology by showing how
to `do-it-yourself': we integrate implementations of a co-data learner and the
spike-and-slab prior for the purpose of improving feature selection in genetics
studies.","['Mark A. van de Wiel', 'Wessel N. van Wieringen']","['stat.ME', 'stat.ML']",2024-05-08 09:38:11+00:00
http://arxiv.org/abs/2405.04910v1,Learning with Posterior Sampling for Revenue Management under Time-varying Demand,"This paper discusses the revenue management (RM) problem to maximize revenue
by pricing items or services. One challenge in this problem is that the demand
distribution is unknown and varies over time in real applications such as
airline and retail industries. In particular, the time-varying demand has not
been well studied under scenarios of unknown demand due to the difficulty of
jointly managing the remaining inventory and estimating the demand. To tackle
this challenge, we first introduce an episodic generalization of the RM problem
motivated by typical application scenarios. We then propose a computationally
efficient algorithm based on posterior sampling, which effectively optimizes
prices by solving linear programming. We derive a Bayesian regret upper bound
of this algorithm for general models where demand parameters can be correlated
between time periods, while also deriving a regret lower bound for generic
algorithms. Our empirical study shows that the proposed algorithm performs
better than other benchmark algorithms and comparably to the optimal policy in
hindsight. We also propose a heuristic modification of the proposed algorithm,
which further efficiently learns the pricing policy in the experiments.","['Kazuma Shimizu', 'Junya Honda', 'Shinji Ito', 'Shinji Nakadai']","['cs.LG', 'stat.ML']",2024-05-08 09:28:26+00:00
http://arxiv.org/abs/2405.04811v1,A general error analysis for randomized low-rank approximation with application to data assimilation,"Randomized algorithms have proven to perform well on a large class of
numerical linear algebra problems. Their theoretical analysis is critical to
provide guarantees on their behaviour, and in this sense, the stochastic
analysis of the randomized low-rank approximation error plays a central role.
Indeed, several randomized methods for the approximation of dominant eigen- or
singular modes can be rewritten as low-rank approximation methods. However,
despite the large variety of algorithms, the existing theoretical frameworks
for their analysis rely on a specific structure for the covariance matrix that
is not adapted to all the algorithms. We propose a general framework for the
stochastic analysis of the low-rank approximation error in Frobenius norm for
centered and non-standard Gaussian matrices. Under minimal assumptions on the
covariance matrix, we derive accurate bounds both in expectation and
probability. Our bounds have clear interpretations that enable us to derive
properties and motivate practical choices for the covariance matrix resulting
in efficient low-rank approximation algorithms. The most commonly used bounds
in the literature have been demonstrated as a specific instance of the bounds
proposed here, with the additional contribution of being tighter. Numerical
experiments related to data assimilation further illustrate that exploiting the
problem structure to select the covariance matrix improves the performance as
suggested by our bounds.","['Alexandre Scotto Di Perrotolo', 'Youssef Diouane', 'Selime Gürol', 'Xavier Vasseur']","['math.NA', 'cs.NA', 'stat.ML']",2024-05-08 04:51:56+00:00
http://arxiv.org/abs/2405.08005v2,Graphon Mean Field Games with a Representative Player: Analysis and Learning Algorithm,"We propose a discrete time graphon game formulation on continuous state and
action spaces using a representative player to study stochastic games with
heterogeneous interaction among agents. This formulation admits both
philosophical and mathematical advantages, compared to a widely adopted
formulation using a continuum of players. We prove the existence and uniqueness
of the graphon equilibrium with mild assumptions, and show that this
equilibrium can be used to construct an approximate solution for finite player
game on networks, which is challenging to analyze and solve due to curse of
dimensionality. An online oracle-free learning algorithm is developed to solve
the equilibrium numerically, and sample complexity analysis is provided for its
convergence.","['Fuzhong Zhou', 'Chenyu Zhang', 'Xu Chen', 'Xuan Di']","['math.OC', 'cs.AI', 'cs.GT', 'cs.LG', 'stat.ML']",2024-05-08 04:44:16+00:00
http://arxiv.org/abs/2405.04715v2,Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning,"Pursuing causality from data is a fundamental problem in scientific
discovery, treatment intervention, and transfer learning. This paper introduces
a novel algorithmic method for addressing nonparametric invariance and
causality learning in regression models across multiple environments, where the
joint distribution of response variables and covariates varies, but the
conditional expectations of outcome given an unknown set of quasi-causal
variables are invariant. The challenge of finding such an unknown set of
quasi-causal or invariant variables is compounded by the presence of endogenous
variables that have heterogeneous effects across different environments,
including even one of them in the regression would make the estimation
inconsistent. The proposed Focused Adversial Invariant Regularization (FAIR)
framework utilizes an innovative minimax optimization approach that breaks down
the barriers, driving regression models toward prediction-invariant solutions
through adversarial testing. Leveraging the representation power of neural
networks, FAIR neural networks (FAIR-NN) are introduced for causality pursuit.
It is shown that FAIR-NN can find the invariant variables and quasi-causal
variables under a minimal identification condition and that the resulting
procedure is adaptive to low-dimensional composition structures in a
non-asymptotic analysis. Under a structural causal model, variables identified
by FAIR-NN represent pragmatic causality and provably align with exact causal
mechanisms under conditions of sufficient heterogeneity. Computationally,
FAIR-NN employs a novel Gumbel approximation with decreased temperature and
stochastic gradient descent ascent algorithm. The procedures are convincingly
demonstrated using simulated and real-data examples.","['Yihong Gu', 'Cong Fang', 'Peter Bühlmann', 'Jianqing Fan']","['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH', '62G08']",2024-05-07 23:37:40+00:00
http://arxiv.org/abs/2405.04636v1,Data-driven Error Estimation: Upper Bounding Multiple Errors with No Technical Debt,"We formulate the problem of constructing multiple simultaneously valid
confidence intervals (CIs) as estimating a high probability upper bound on the
maximum error for a class/set of estimate-estimand-error tuples, and refer to
this as the error estimation problem. For a single such tuple, data-driven
confidence intervals can often be used to bound the error in our estimate.
However, for a class of estimate-estimand-error tuples, nontrivial high
probability upper bounds on the maximum error often require class complexity as
input -- limiting the practicality of such methods and often resulting in loose
bounds. Rather than deriving theoretical class complexity-based bounds, we
propose a completely data-driven approach to estimate an upper bound on the
maximum error. The simple and general nature of our solution to this
fundamental challenge lends itself to several applications including: multiple
CI construction, multiple hypothesis testing, estimating excess risk bounds (a
fundamental measure of uncertainty in machine learning) for any
training/fine-tuning algorithm, and enabling the development of a contextual
bandit pipeline that can leverage any reward model estimation procedure as
input (without additional mathematical analysis).","['Sanath Kumar Krishnamurthy', 'Susan Athey', 'Emma Brunskill']","['cs.LG', 'stat.ML']",2024-05-07 19:38:26+00:00
http://arxiv.org/abs/2405.04517v1,xLSTM: Extended Long Short-Term Memory,"In the 1990s, the constant error carousel and gating were introduced as the
central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have
stood the test of time and contributed to numerous deep learning success
stories, in particular they constituted the first Large Language Models (LLMs).
However, the advent of the Transformer technology with parallelizable
self-attention at its core marked the dawn of a new era, outpacing LSTMs at
scale. We now raise a simple question: How far do we get in language modeling
when scaling LSTMs to billions of parameters, leveraging the latest techniques
from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we
introduce exponential gating with appropriate normalization and stabilization
techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM
with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that
is fully parallelizable with a matrix memory and a covariance update rule.
Integrating these LSTM extensions into residual block backbones yields xLSTM
blocks that are then residually stacked into xLSTM architectures. Exponential
gating and modified memory structures boost xLSTM capabilities to perform
favorably when compared to state-of-the-art Transformers and State Space
Models, both in performance and scaling.","['Maximilian Beck', 'Korbinian Pöppel', 'Markus Spanring', 'Andreas Auer', 'Oleksandra Prudnikova', 'Michael Kopp', 'Günter Klambauer', 'Johannes Brandstetter', 'Sepp Hochreiter']","['cs.LG', 'cs.AI', 'stat.ML']",2024-05-07 17:50:21+00:00
http://arxiv.org/abs/2405.04566v1,Fast Decentralized Gradient Tracking for Federated Minimax Optimization with Local Updates,"Federated learning (FL) for minimax optimization has emerged as a powerful
paradigm for training models across distributed nodes/clients while preserving
data privacy and model robustness on data heterogeneity. In this work, we delve
into the decentralized implementation of federated minimax optimization by
proposing \texttt{K-GT-Minimax}, a novel decentralized minimax optimization
algorithm that combines local updates and gradient tracking techniques. Our
analysis showcases the algorithm's communication efficiency and convergence
rate for nonconvex-strongly-concave (NC-SC) minimax optimization, demonstrating
a superior convergence rate compared to existing methods.
\texttt{K-GT-Minimax}'s ability to handle data heterogeneity and ensure
robustness underscores its significance in advancing federated learning
research and applications.",['Chris Junchi Li'],"['cs.LG', 'cs.DC', 'stat.ML']",2024-05-07 17:25:56+00:00
http://arxiv.org/abs/2405.04393v1,Efficient Online Set-valued Classification with Bandit Feedback,"Conformal prediction is a distribution-free method that wraps a given machine
learning model and returns a set of plausible labels that contain the true
label with a prescribed coverage rate. In practice, the empirical coverage
achieved highly relies on fully observed label information from data both in
the training phase for model fitting and the calibration phase for quantile
estimation. This dependency poses a challenge in the context of online learning
with bandit feedback, where a learner only has access to the correctness of
actions (i.e., pulled an arm) but not the full information of the true label.
In particular, when the pulled arm is incorrect, the learner only knows that
the pulled one is not the true class label, but does not know which label is
true. Additionally, bandit feedback further results in a smaller labeled
dataset for calibration, limited to instances with correct actions, thereby
affecting the accuracy of quantile estimation. To address these limitations, we
propose Bandit Class-specific Conformal Prediction (BCCP), offering coverage
guarantees on a class-specific granularity. Using an unbiased estimation of an
estimand involving the true label, BCCP trains the model and makes set-valued
inferences through stochastic gradient descent. Our approach overcomes the
challenges of sparsely labeled data in each iteration and generalizes the
reliability and applicability of conformal prediction to online decision-making
environments.","['Zhou Wang', 'Xingye Qiao']","['stat.ML', 'cs.LG']",2024-05-07 15:14:51+00:00
http://arxiv.org/abs/2405.04346v2,Revisiting Character-level Adversarial Attacks for Language Models,"Adversarial attacks in Natural Language Processing apply perturbations in the
character or token levels. Token-level attacks, gaining prominence for their
use of gradient-based methods, are susceptible to altering sentence semantics,
leading to invalid adversarial examples. While character-level attacks easily
maintain semantics, they have received less attention as they cannot easily
adopt popular gradient-based methods, and are thought to be easy to defend.
Challenging these beliefs, we introduce Charmer, an efficient query-based
adversarial attack capable of achieving high attack success rate (ASR) while
generating highly similar adversarial examples. Our method successfully targets
both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2,
Charmer improves the ASR in 4.84% points and the USE similarity in 8% points
with respect to the previous art. Our implementation is available in
https://github.com/LIONS-EPFL/Charmer.","['Elias Abad Rocamora', 'Yongtao Wu', 'Fanghui Liu', 'Grigorios G. Chrysos', 'Volkan Cevher']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2024-05-07 14:23:22+00:00
http://arxiv.org/abs/2405.04147v1,Multiparameter regularization and aggregation in the context of polynomial functional regression,"Most of the recent results in polynomial functional regression have been
focused on an in-depth exploration of single-parameter regularization schemes.
In contrast, in this study we go beyond that framework by introducing an
algorithm for multiple parameter regularization and presenting a theoretically
grounded method for dealing with the associated parameters. This method
facilitates the aggregation of models with varying regularization parameters.
The efficacy of the proposed approach is assessed through evaluations on both
synthetic and some real-world medical data, revealing promising results.","['Elke R. Gizewski', 'Markus Holzleitner', 'Lukas Mayer-Suess', 'Sergiy Pereverzyev Jr.', 'Sergei V. Pereverzyev']","['stat.ML', 'cs.LG', 'cs.NA', 'math.NA', 'math.ST', 'stat.TH', '65K10, 62G20']",2024-05-07 09:26:20+00:00
http://arxiv.org/abs/2405.04043v1,Scalable Vertical Federated Learning via Data Augmentation and Amortized Inference,"Vertical federated learning (VFL) has emerged as a paradigm for collaborative
model estimation across multiple clients, each holding a distinct set of
covariates. This paper introduces the first comprehensive framework for fitting
Bayesian models in the VFL setting. We propose a novel approach that leverages
data augmentation techniques to transform VFL problems into a form compatible
with existing Bayesian federated learning algorithms. We present an innovative
model formulation for specific VFL scenarios where the joint likelihood
factorizes into a product of client-specific likelihoods. To mitigate the
dimensionality challenge posed by data augmentation, which scales with the
number of observations and clients, we develop a factorized amortized
variational approximation that achieves scalability independent of the number
of observations. We showcase the efficacy of our framework through extensive
numerical experiments on logistic regression, multilevel regression, and a
novel hierarchical Bayesian split neural net model. Our work paves the way for
privacy-preserving, decentralized Bayesian inference in vertically partitioned
data scenarios, opening up new avenues for research and applications in various
domains.","['Conor Hassan', 'Matthew Sutton', 'Antonietta Mira', 'Kerrie Mengersen']","['stat.CO', 'cs.LG', 'stat.ME', 'stat.ML']",2024-05-07 06:29:06+00:00
http://arxiv.org/abs/2405.04026v1,Federated Control in Markov Decision Processes,"We study problems of federated control in Markov Decision Processes. To solve
an MDP with large state space, multiple learning agents are introduced to
collaboratively learn its optimal policy without communication of locally
collected experience. In our settings, these agents have limited capabilities,
which means they are restricted within different regions of the overall state
space during the training process. In face of the difference among restricted
regions, we firstly introduce concepts of leakage probabilities to understand
how such heterogeneity affects the learning process, and then propose a novel
communication protocol that we call Federated-Q protocol (FedQ), which
periodically aggregates agents' knowledge of their restricted regions and
accordingly modifies their learning problems for further training. In terms of
theoretical analysis, we justify the correctness of FedQ as a communication
protocol, then give a general result on sample complexity of derived algorithms
FedQ-X with the RL oracle , and finally conduct a thorough study on the sample
complexity of FedQ-SynQ. Specifically, FedQ-X has been shown to enjoy linear
speedup in terms of sample complexity when workload is uniformly distributed
among agents. Moreover, we carry out experiments in various environments to
justify the efficiency of our methods.","['Hao Jin', 'Yang Peng', 'Liangyu Zhang', 'Zhihua Zhang']","['stat.ML', 'cs.LG']",2024-05-07 05:59:10+00:00
http://arxiv.org/abs/2405.04011v2,Adjoint Sensitivity Analysis on Multi-Scale Bioprocess Stochastic Reaction Network,"Motivated by the pressing challenges in the digital twin development for
biomanufacturing systems, we introduce an adjoint sensitivity analysis (SA)
approach to expedite the learning of mechanistic model parameters. In this
paper, we consider enzymatic stochastic reaction networks representing a
multi-scale bioprocess mechanistic model that allows us to integrate disparate
data from diverse production processes and leverage the information from
existing macro-kinetic and genome-scale models. To support forward prediction
and backward reasoning, we develop a convergent adjoint SA algorithm studying
how the perturbations of model parameters and inputs (e.g., initial state)
propagate through enzymatic reaction networks and impact on output trajectory
predictions. This SA can provide a sample efficient and interpretable way to
assess the sensitivities between inputs and outputs accounting for their causal
dependencies. Our empirical study underscores the resilience of these
sensitivities and illuminates a deeper comprehension of the regulatory
mechanisms behind bioprocess through sensitivities.","['Keilung Choy', 'Wei Xie']","['q-bio.MN', 'stat.ML']",2024-05-07 05:06:45+00:00
http://arxiv.org/abs/2405.03913v2,Digital Twin Calibration for Biological System-of-Systems: Cell Culture Manufacturing Process,"Biomanufacturing innovation relies on an efficient Design of Experiments
(DoEs) to optimize processes and product quality. Traditional DoE methods,
ignoring the underlying bioprocessing mechanisms, often suffer from a lack of
interpretability and sample efficiency. This limitation motivates us to create
a new optimal learning approach for digital twin model calibration. In this
study, we consider the cell culture process multi-scale mechanistic model, also
known as Biological System-of-Systems (Bio-SoS). This model with a modular
design, composed of sub-models, allows us to integrate data across various
production processes. To calibrate the Bio-SoS digital twin, we evaluate the
mean squared error of model prediction and develop a computational approach to
quantify the impact of parameter estimation error of individual sub-models on
the prediction accuracy of digital twin, which can guide sample-efficient and
interpretable DoEs.","['Fuqiang Cheng', 'Wei Xie', 'Hua Zheng']","['q-bio.QM', 'cs.LG', 'stat.ML']",2024-05-07 00:22:13+00:00
http://arxiv.org/abs/2405.03879v1,Scalable Amortized GPLVMs for Single Cell Transcriptomics Data,"Dimensionality reduction is crucial for analyzing large-scale single-cell
RNA-seq data. Gaussian Process Latent Variable Models (GPLVMs) offer an
interpretable dimensionality reduction method, but current scalable models lack
effectiveness in clustering cell types. We introduce an improved model, the
amortized stochastic variational Bayesian GPLVM (BGPLVM), tailored for
single-cell RNA-seq with specialized encoder, kernel, and likelihood designs.
This model matches the performance of the leading single-cell variational
inference (scVI) approach on synthetic and real-world COVID datasets and
effectively incorporates cell-cycle and batch information to reveal more
interpretable latent structures as we demonstrate on an innate immunity
dataset.","['Sarah Zhao', 'Aditya Ravuri', 'Vidhi Lalchand', 'Neil D. Lawrence']","['stat.ML', 'cs.LG', 'q-bio.GN', 'stat.AP']",2024-05-06 21:54:38+00:00
http://arxiv.org/abs/2405.03875v1,Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits,"Data Shapley provides a principled approach to data valuation and plays a
crucial role in data-centric machine learning (ML) research. Data selection is
considered a standard application of Data Shapley. However, its data selection
performance has shown to be inconsistent across settings in the literature.
This study aims to deepen our understanding of this phenomenon. We introduce a
hypothesis testing framework and show that Data Shapley's performance can be no
better than random selection without specific constraints on utility functions.
We identify a class of utility functions, monotonically transformed modular
functions, within which Data Shapley optimally selects data. Based on this
insight, we propose a heuristic for predicting Data Shapley's effectiveness in
data selection tasks. Our experiments corroborate these findings, adding new
insights into when Data Shapley may or may not succeed.","['Jiachen T. Wang', 'Tianji Yang', 'James Zou', 'Yongchan Kwon', 'Ruoxi Jia']","['cs.LG', 'stat.ML']",2024-05-06 21:46:10+00:00
http://arxiv.org/abs/2405.03664v2,A New Robust Partial $p$-Wasserstein-Based Metric for Comparing Distributions,"The $2$-Wasserstein distance is sensitive to minor geometric differences
between distributions, making it a very powerful dissimilarity metric. However,
due to this sensitivity, a small outlier mass can also cause a significant
increase in the $2$-Wasserstein distance between two similar distributions.
Similarly, sampling discrepancy can cause the empirical $2$-Wasserstein
distance on $n$ samples in $\mathbb{R}^2$ to converge to the true distance at a
rate of $n^{-1/4}$, which is significantly slower than the rate of $n^{-1/2}$
for $1$-Wasserstein distance. We introduce a new family of distances
parameterized by $k \ge 0$, called $k$-RPW that is based on computing the
partial $2$-Wasserstein distance. We show that (1) $k$-RPW satisfies the metric
properties, (2) $k$-RPW is robust to small outlier mass while retaining the
sensitivity of $2$-Wasserstein distance to minor geometric differences, and (3)
when $k$ is a constant, $k$-RPW distance between empirical distributions on $n$
samples in $\mathbb{R}^2$ converges to the true distance at a rate of
$n^{-1/3}$, which is faster than the convergence rate of $n^{-1/4}$ for the
$2$-Wasserstein distance. Using the partial $p$-Wasserstein distance, we extend
our distance to any $p \in [1,\infty]$. By setting parameters $k$ or $p$
appropriately, we can reduce our distance to the total variation,
$p$-Wasserstein, and the L\'evy-Prokhorov distances. Experiments show that our
distance function achieves higher accuracy in comparison to the
$1$-Wasserstein, $2$-Wasserstein, and TV distances for image retrieval tasks on
noisy real-world data sets.","['Sharath Raghvendra', 'Pouyan Shirzadian', 'Kaiyi Zhang']","['cs.LG', 'stat.ML']",2024-05-06 17:41:13+00:00
http://arxiv.org/abs/2405.03624v1,$ε$-Policy Gradient for Online Pricing,"Combining model-based and model-free reinforcement learning approaches, this
paper proposes and analyzes an $\epsilon$-policy gradient algorithm for the
online pricing learning task. The algorithm extends $\epsilon$-greedy algorithm
by replacing greedy exploitation with gradient descent step and facilitates
learning via model inference. We optimize the regret of the proposed algorithm
by quantifying the exploration cost in terms of the exploration probability
$\epsilon$ and the exploitation cost in terms of the gradient descent
optimization and gradient estimation errors. The algorithm achieves an expected
regret of order $\mathcal{O}(\sqrt{T})$ (up to a logarithmic factor) over $T$
trials.","['Lukasz Szpruch', 'Tanut Treetanthiploet', 'Yufei Zhang']","['cs.LG', 'math.OC', 'q-fin.ST', 'stat.ML', '62J12, 68Q32, 65Y20']",2024-05-06 16:41:52+00:00
http://arxiv.org/abs/2405.03549v1,Bridging discrete and continuous state spaces: Exploring the Ehrenfest process in time-continuous diffusion models,"Generative modeling via stochastic processes has led to remarkable empirical
results as well as to recent advances in their theoretical understanding. In
principle, both space and time of the processes can be discrete or continuous.
In this work, we study time-continuous Markov jump processes on discrete state
spaces and investigate their correspondence to state-continuous diffusion
processes given by SDEs. In particular, we revisit the $\textit{Ehrenfest
process}$, which converges to an Ornstein-Uhlenbeck process in the infinite
state space limit. Likewise, we can show that the time-reversal of the
Ehrenfest process converges to the time-reversed Ornstein-Uhlenbeck process.
This observation bridges discrete and continuous state spaces and allows to
carry over methods from one to the respective other setting. Additionally, we
suggest an algorithm for training the time-reversal of Markov jump processes
which relies on conditional expectations and can thus be directly related to
denoising score matching. We demonstrate our methods in multiple convincing
numerical experiments.","['Ludwig Winkler', 'Lorenz Richter', 'Manfred Opper']","['stat.ML', 'cs.LG', 'math.DS', 'math.PR']",2024-05-06 15:12:51+00:00
http://arxiv.org/abs/2405.04554v1,Differentially Private Synthetic Data with Private Density Estimation,"The need to analyze sensitive data, such as medical records or financial
data, has created a critical research challenge in recent years. In this paper,
we adopt the framework of differential privacy, and explore mechanisms for
generating an entire dataset which accurately captures characteristics of the
original data. We build upon the work of Boedihardjo et al, which laid the
foundations for a new optimization-based algorithm for generating private
synthetic data. Importantly, we adapt their algorithm by replacing a uniform
sampling step with a private distribution estimator; this allows us to obtain
better computational guarantees for discrete distributions, and develop a novel
algorithm suitable for continuous distributions. We also explore applications
of our work to several statistical tasks.","['Nikolija Bojkovic', 'Po-Ling Loh']","['cs.CR', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH', '62G07']",2024-05-06 14:06:12+00:00
http://arxiv.org/abs/2405.03468v1,Hierarchic Flows to Estimate and Sample High-dimensional Probabilities,"Finding low-dimensional interpretable models of complex physical fields such
as turbulence remains an open question, 80 years after the pioneer work of
Kolmogorov. Estimating high-dimensional probability distributions from data
samples suffers from an optimization and an approximation curse of
dimensionality. It may be avoided by following a hierarchic probability flow
from coarse to fine scales. This inverse renormalization group is defined by
conditional probabilities across scales, renormalized in a wavelet basis. For a
$\varphi^4$ scalar potential, sampling these hierarchic models avoids the
critical slowing down at the phase transition. An outstanding issue is to also
approximate non-Gaussian fields having long-range interactions in space and
across scales. We introduce low-dimensional models with robust multiscale
approximations of high order polynomial energies. They are calculated with a
second wavelet transform, which defines interactions over two hierarchies of
scales. We estimate and sample these wavelet scattering models to generate 2D
vorticity fields of turbulence, and images of dark matter densities.","['Etienne Lempereur', 'Stéphane Mallat']","['stat.ML', 'cs.LG', 'physics.flu-dyn']",2024-05-06 13:44:51+00:00
http://arxiv.org/abs/2405.03449v1,Byzantine-Robust Gossip: Insights from a Dual Approach,"Distributed approaches have many computational benefits, but they are
vulnerable to attacks from a subset of devices transmitting incorrect
information. This paper investigates Byzantine-resilient algorithms in a
decentralized setting, where devices communicate directly with one another. We
leverage the so-called dual approach to design a general robust decentralized
optimization method. We provide both global and local clipping rules in the
special case of average consensus, with tight convergence guarantees. These
clipping rules are practical, and yield results that finely characterize the
impact of Byzantine nodes, highlighting for instance a qualitative difference
in convergence between global and local clipping thresholds. Lastly, we
demonstrate that they can serve as a basis for designing efficient attacks.","['Renaud Gaucher', 'Hadrien Hendrikx', 'Aymeric Dieuleveut']","['cs.LG', 'stat.ML']",2024-05-06 13:22:54+00:00
http://arxiv.org/abs/2405.03329v2,Policy Learning for Balancing Short-Term and Long-Term Rewards,"Empirical researchers and decision-makers spanning various domains frequently
seek profound insights into the long-term impacts of interventions. While the
significance of long-term outcomes is undeniable, an overemphasis on them may
inadvertently overshadow short-term gains. Motivated by this, this paper
formalizes a new framework for learning the optimal policy that effectively
balances both long-term and short-term rewards, where some long-term outcomes
are allowed to be missing. In particular, we first present the identifiability
of both rewards under mild assumptions. Next, we deduce the semiparametric
efficiency bounds, along with the consistency and asymptotic normality of their
estimators. We also reveal that short-term outcomes, if associated, contribute
to improving the estimator of the long-term reward. Based on the proposed
estimators, we develop a principled policy learning approach and further derive
the convergence rates of regret and estimation errors associated with the
learned policy. Extensive experiments are conducted to validate the
effectiveness of the proposed method, demonstrating its practical
applicability.","['Peng Wu', 'Ziyu Shen', 'Feng Xie', 'Zhongyao Wang', 'Chunchen Liu', 'Yan Zeng']","['cs.LG', 'stat.ML']",2024-05-06 10:09:35+00:00
http://arxiv.org/abs/2405.03293v2,Deep Learning and genetic algorithms for cosmological Bayesian inference speed-up,"In this paper, we present a novel approach to accelerate the Bayesian
inference process, focusing specifically on the nested sampling algorithms.
Bayesian inference plays a crucial role in cosmological parameter estimation,
providing a robust framework for extracting theoretical insights from
observational data. However, its computational demands can be substantial,
primarily due to the need for numerous likelihood function evaluations. Our
proposed method utilizes the power of deep learning, employing feedforward
neural networks to approximate the likelihood function dynamically during the
Bayesian inference process. Unlike traditional approaches, our method trains
neural networks on-the-fly using the current set of live points as training
data, without the need for pre-training. This flexibility enables adaptation to
various theoretical models and datasets. We perform simple hyperparameter
optimization using genetic algorithms to suggest initial neural network
architectures for learning each likelihood function. Once sufficient accuracy
is achieved, the neural network replaces the original likelihood function. The
implementation integrates with nested sampling algorithms and has been
thoroughly evaluated using both simple cosmological dark energy models and
diverse observational datasets. Additionally, we explore the potential of
genetic algorithms for generating initial live points within nested sampling
inference, opening up new avenues for enhancing the efficiency and
effectiveness of Bayesian inference methods.","['Isidro Gómez-Vargas', 'J. Alberto Vázquez']","['astro-ph.IM', 'astro-ph.CO', 'cs.LG', 'cs.NE', 'stat.ML']",2024-05-06 09:14:58+00:00
http://arxiv.org/abs/2405.03236v1,Federated Reinforcement Learning with Constraint Heterogeneity,"We study a Federated Reinforcement Learning (FedRL) problem with constraint
heterogeneity. In our setting, we aim to solve a reinforcement learning problem
with multiple constraints while $N$ training agents are located in $N$
different environments with limited access to the constraint signals and they
are expected to collaboratively learn a policy satisfying all constraint
signals. Such learning problems are prevalent in scenarios of Large Language
Model (LLM) fine-tuning and healthcare applications. To solve the problem, we
propose federated primal-dual policy optimization methods based on traditional
policy gradient methods. Specifically, we introduce $N$ local Lagrange
functions for agents to perform local policy updates, and these agents are then
scheduled to periodically communicate on their local policies. Taking natural
policy gradient (NPG) and proximal policy optimization (PPO) as policy
optimization methods, we mainly focus on two instances of our algorithms, ie,
{FedNPG} and {FedPPO}. We show that FedNPG achieves global convergence with an
$\tilde{O}(1/\sqrt{T})$ rate, and FedPPO efficiently solves complicated
learning tasks with the use of deep neural networks.","['Hao Jin', 'Liangyu Zhang', 'Zhihua Zhang']","['cs.LG', 'stat.ML']",2024-05-06 07:44:50+00:00
http://arxiv.org/abs/2405.03198v1,Stability Evaluation via Distributional Perturbation Analysis,"The performance of learning models often deteriorates when deployed in
out-of-sample environments. To ensure reliable deployment, we propose a
stability evaluation criterion based on distributional perturbations.
Conceptually, our stability evaluation criterion is defined as the minimal
perturbation required on our observed dataset to induce a prescribed
deterioration in risk evaluation. In this paper, we utilize the optimal
transport (OT) discrepancy with moment constraints on the \textit{(sample,
density)} space to quantify this perturbation. Therefore, our stability
evaluation criterion can address both \emph{data corruptions} and
\emph{sub-population shifts} -- the two most common types of distribution
shifts in real-world scenarios. To further realize practical benefits, we
present a series of tractable convex formulations and computational methods
tailored to different classes of loss functions. The key technical tool to
achieve this is the strong duality theorem provided in this paper. Empirically,
we validate the practical utility of our stability evaluation criterion across
a host of real-world applications. These empirical studies showcase the
criterion's ability not only to compare the stability of different learning
models and features but also to provide valuable guidelines and strategies to
further improve models.","['Jose Blanchet', 'Peng Cui', 'Jiajin Li', 'Jiashuo Liu']","['stat.ML', 'cs.LG', 'math.OC']",2024-05-06 06:47:14+00:00
http://arxiv.org/abs/2405.03180v2,Braced Fourier Continuation and Regression for Anomaly Detection,"In this work, the concept of Braced Fourier Continuation and Regression
(BFCR) is introduced. BFCR is a novel and computationally efficient means of
finding nonlinear regressions or trend lines in arbitrary one-dimensional data
sets. The Braced Fourier Continuation (BFC) and BFCR algorithms are first
outlined, followed by a discussion of the properties of BFCR as well as
demonstrations of how BFCR trend lines may be used effectively for anomaly
detection both within and at the edges of arbitrary one-dimensional data sets.
Finally, potential issues which may arise while using BFCR for anomaly
detection as well as possible mitigation techniques are outlined and discussed.
All source code and example data sets are either referenced or available via
GitHub, and all associated code is written entirely in Python.",['Josef Sabuda'],"['stat.ML', 'cs.LG', 'cs.NA', 'math.NA', '65D10 (Primary)', 'G.1.2']",2024-05-06 06:05:41+00:00
http://arxiv.org/abs/2405.03723v1,Generative adversarial learning with optimal input dimension and its adaptive generator architecture,"We investigate the impact of the input dimension on the generalization error
in generative adversarial networks (GANs). In particular, we first provide both
theoretical and practical evidence to validate the existence of an optimal
input dimension (OID) that minimizes the generalization error. Then, to
identify the OID, we introduce a novel framework called generalized GANs
(G-GANs), which includes existing GANs as a special case. By incorporating the
group penalty and the architecture penalty developed in the paper, G-GANs have
several intriguing features. First, our framework offers adaptive
dimensionality reduction from the initial dimension to a dimension necessary
for generating the target distribution. Second, this reduction in
dimensionality also shrinks the required size of the generator network
architecture, which is automatically identified by the proposed architecture
penalty. Both reductions in dimensionality and the generator network
significantly improve the stability and the accuracy of the estimation and
prediction. Theoretical support for the consistent selection of the input
dimension and the generator network is provided. Third, the proposed algorithm
involves an end-to-end training process, and the algorithm allows for dynamic
adjustments between the input dimension and the generator network during
training, further enhancing the overall performance of G-GANs. Extensive
experiments conducted with simulated and benchmark data demonstrate the
superior performance of G-GANs. In particular, compared to that of
off-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the
CT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST
dataset in terms of the maximum mean discrepancy or Frechet inception distance.
Moreover, the features generated based on the input dimensions identified by
G-GANs align with visually significant features.","['Zhiyao Tan', 'Ling Zhou', 'Huazhen Lin']","['cs.LG', 'stat.ME', 'stat.ML']",2024-05-06 03:30:02+00:00
http://arxiv.org/abs/2405.03130v1,Deep Learning for Causal Inference: A Comparison of Architectures for Heterogeneous Treatment Effect Estimation,"Causal inference has gained much popularity in recent years, with interests
ranging from academic, to industrial, to educational, and all in between.
Concurrently, the study and usage of neural networks has also grown profoundly
(albeit at a far faster rate). What we aim to do in this blog write-up is
demonstrate a Neural Network causal inference architecture. We develop a fully
connected neural network implementation of the popular Bayesian Causal Forest
algorithm, a state of the art tree based method for estimating heterogeneous
treatment effects. We compare our implementation to existing neural network
causal inference methodologies, showing improvements in performance in
simulation settings. We apply our method to a dataset examining the effect of
stress on sleep.","['Demetrios Papakostas', 'Andrew Herren', 'P. Richard Hahn', 'Francisco Castillo']","['stat.ML', 'cs.LG']",2024-05-06 02:54:53+00:00
http://arxiv.org/abs/2405.03083v2,Causal K-Means Clustering,"Causal effects are often characterized with population summaries. These might
provide an incomplete picture when there are heterogeneous treatment effects
across subgroups. Since the subgroup structure is typically unknown, it is more
challenging to identify and evaluate subgroup effects than population effects.
We propose a new solution to this problem: Causal k-Means Clustering, which
harnesses the widely-used k-means clustering algorithm to uncover the unknown
subgroup structure. Our problem differs significantly from the conventional
clustering setup since the variables to be clustered are unknown counterfactual
functions. We present a plug-in estimator which is simple and readily
implementable using off-the-shelf algorithms, and study its rate of
convergence. We also develop a new bias-corrected estimator based on
nonparametric efficiency theory and double machine learning, and show that this
estimator achieves fast root-n rates and asymptotic normality in large
nonparametric models. Our proposed methods are especially useful for modern
outcome-wide studies with multiple treatment levels. Further, our framework is
extensible to clustering with generic pseudo-outcomes, such as partially
observed outcomes or otherwise unknown functions. Finally, we explore finite
sample properties via simulation, and illustrate the proposed methods in a
study of treatment programs for adolescent substance abuse.","['Kwangho Kim', 'Jisu Kim', 'Edward H. Kennedy']","['stat.ME', 'cs.LG', 'stat.ML']",2024-05-05 23:59:51+00:00
