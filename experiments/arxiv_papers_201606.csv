id,title,abstract,authors,categories,date
http://arxiv.org/abs/1607.08188v1,Online Trajectory Segmentation and Summary With Applications to Visualization and Retrieval,"Trajectory segmentation is the process of subdividing a trajectory into parts
either by grouping points similar with respect to some measure of interest, or
by minimizing a global objective function. Here we present a novel online
algorithm for segmentation and summary, based on point density along the
trajectory, and based on the nature of the naturally occurring structure of
intermittent bouts of locomotive and local activity. We show an application to
visualization of trajectory datasets, and discuss the use of the summary as an
index allowing efficient queries which are otherwise impossible or
computationally expensive, over very large datasets.",['Yehezkel S. Resheff'],"['cs.CV', 'cs.DB', 'stat.ML']",2016-07-24 14:50:45+00:00
http://arxiv.org/abs/1607.06996v6,Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction,"Sparse support vector machine (SVM) is a popular classification technique
that can simultaneously learn a small set of the most interpretable features
and identify the support vectors. It has achieved great successes in many
real-world applications. However, for large-scale problems involving a huge
number of samples and ultra-high dimensional features, solving sparse SVMs
remains challenging. By noting that sparse SVMs induce sparsities in both
feature and sample spaces, we propose a novel approach, which is based on
accurate estimations of the primal and dual optima of sparse SVMs, to
simultaneously identify the inactive features and samples that are guaranteed
to be irrelevant to the outputs. Thus, we can remove the identified inactive
samples and features from the training phase, leading to substantial savings in
the computational cost without sacrificing the accuracy. Moreover, we show that
our method can be extended to multi-class sparse support vector machines. To
the best of our knowledge, the proposed method is the \emph{first}
\emph{static} feature and sample reduction method for sparse SVMs and
multi-class sparse SVMs. Experiments on both synthetic and real data sets
demonstrate that our approach significantly outperforms state-of-the-art
methods and the speedup gained by our approach can be orders of magnitude.","['Weizhong Zhang', 'Bin Hong', 'Wei Liu', 'Jieping Ye', 'Deng Cai', 'Xiaofei He', 'Jie Wang']","['stat.ML', 'cs.LG']",2016-07-24 04:00:30+00:00
http://arxiv.org/abs/1607.06993v1,Community Detection in Degree-Corrected Block Models,"Community detection is a central problem of network data analysis. Given a
network, the goal of community detection is to partition the network nodes into
a small number of clusters, which could often help reveal interesting
structures. The present paper studies community detection in Degree-Corrected
Block Models (DCBMs). We first derive asymptotic minimax risks of the problem
for a misclassification proportion loss under appropriate conditions. The
minimax risks are shown to depend on degree-correction parameters, community
sizes, and average within and between community connectivities in an intuitive
and interpretable way. In addition, we propose a polynomial time algorithm to
adaptively perform consistent and even asymptotically optimal community
detection in DCBMs.","['Chao Gao', 'Zongming Ma', 'Anderson Y. Zhang', 'Harrison H. Zhou']","['math.ST', 'cs.SI', 'stat.ML', 'stat.TH']",2016-07-24 02:53:38+00:00
http://arxiv.org/abs/1607.06988v1,Interactive Learning from Multiple Noisy Labels,"Interactive learning is a process in which a machine learning algorithm is
provided with meaningful, well-chosen examples as opposed to randomly chosen
examples typical in standard supervised learning. In this paper, we propose a
new method for interactive learning from multiple noisy labels where we exploit
the disagreement among annotators to quantify the easiness (or meaningfulness)
of an example. We demonstrate the usefulness of this method in estimating the
parameters of a latent variable classification model, and conduct experimental
analyses on a range of synthetic and benchmark datasets. Furthermore, we
theoretically analyze the performance of perceptron in this interactive
learning framework.","['Shankar Vembu', 'Sandra Zilles']","['cs.LG', 'stat.ML']",2016-07-24 01:14:19+00:00
http://arxiv.org/abs/1607.06801v3,High-dimensional regression adjustments in randomized experiments,"We study the problem of treatment effect estimation in randomized experiments
with high-dimensional covariate information, and show that essentially any
risk-consistent regression adjustment can be used to obtain efficient estimates
of the average treatment effect. Our results considerably extend the range of
settings where high-dimensional regression adjustments are guaranteed to
provide valid inference about the population average treatment effect. We then
propose cross-estimation, a simple method for obtaining finite-sample-unbiased
treatment effect estimates that leverages high-dimensional regression
adjustments. Our method can be used when the regression model is estimated
using the lasso, the elastic net, subset selection, etc. Finally, we extend our
analysis to allow for adaptive specification search via cross-validation, and
flexible non-parametric regression adjustments with machine learning methods
such as random forests or neural networks.","['Stefan Wager', 'Wenfei Du', 'Jonathan Taylor', 'Robert Tibshirani']","['stat.ME', 'stat.ML']",2016-07-22 19:38:46+00:00
http://arxiv.org/abs/1607.06781v2,On the Use of Sparse Filtering for Covariate Shift Adaptation,"In this paper we formally analyse the use of sparse filtering algorithms to
perform covariate shift adaptation. We provide a theoretical analysis of sparse
filtering by evaluating the conditions required to perform covariate shift
adaptation. We prove that sparse filtering can perform adaptation only if the
conditional distribution of the labels has a structure explained by a cosine
metric. To overcome this limitation, we propose a new algorithm, named periodic
sparse filtering, and carry out the same theoretical analysis regarding
covariate shift adaptation. We show that periodic sparse filtering can perform
adaptation under the looser and more realistic requirement that the conditional
distribution of the labels has a periodic structure, which may be satisfied,
for instance, by user-dependent data sets. We experimentally validate our
theoretical results on synthetic data. Moreover, we apply periodic sparse
filtering to real-world data sets to demonstrate that this simple and
computationally efficient algorithm is able to achieve competitive
performances.","['Fabio Massimo Zennaro', 'Ke Chen']","['cs.LG', 'stat.ML']",2016-07-22 18:17:10+00:00
http://arxiv.org/abs/1607.06617v1,Latent Variable Discovery Using Dependency Patterns,"The causal discovery of Bayesian networks is an active and important research
area, and it is based upon searching the space of causal models for those which
can best explain a pattern of probabilistic dependencies shown in the data.
However, some of those dependencies are generated by causal structures
involving variables which have not been measured, i.e., latent variables. Some
such patterns of dependency ""reveal"" themselves, in that no model based solely
upon the observed variables can explain them as well as a model using a latent
variable. That is what latent variable discovery is based upon. Here we did a
search for finding them systematically, so that they may be applied in latent
variable discovery in a more rigorous fashion.","['Xuhui Zhang', 'Kevin B. Korb', 'Ann E. Nicholson', 'Steven Mascaro']","['cs.AI', 'stat.ML']",2016-07-22 09:48:25+00:00
http://arxiv.org/abs/1607.06534v3,The Landscape of Empirical Risk for Non-convex Losses,"Most high-dimensional estimation and prediction methods propose to minimize a
cost function (empirical risk) that is written as a sum of losses associated to
each data point. In this paper we focus on the case of non-convex losses, which
is practically important but still poorly understood. Classical empirical
process theory implies uniform convergence of the empirical risk to the
population risk. While uniform convergence implies consistency of the resulting
M-estimator, it does not ensure that the latter can be computed efficiently.
  In order to capture the complexity of computing M-estimators, we propose to
study the landscape of the empirical risk, namely its stationary points and
their properties. We establish uniform convergence of the gradient and Hessian
of the empirical risk to their population counterparts, as soon as the number
of samples becomes larger than the number of unknown parameters (modulo
logarithmic factors). Consequently, good properties of the population risk can
be carried to the empirical risk, and we can establish one-to-one
correspondence of their stationary points. We demonstrate that in several
problems such as non-convex binary classification, robust regression, and
Gaussian mixture model, this result implies a complete characterization of the
landscape of the empirical risk, and of the convergence properties of descent
algorithms.
  We extend our analysis to the very high-dimensional setting in which the
number of parameters exceeds the number of samples, and provide a
characterization of the empirical risk landscape under a nearly
information-theoretically minimal condition. Namely, if the number of samples
exceeds the sparsity of the unknown parameters vector (modulo logarithmic
factors), then a suitable uniform convergence result takes place. We apply this
result to non-convex binary classification and robust regression in very
high-dimension.","['Song Mei', 'Yu Bai', 'Andrea Montanari']",['stat.ML'],2016-07-22 01:37:30+00:00
http://arxiv.org/abs/1607.06520v1,Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings,"The blind application of machine learning runs the risk of amplifying biases
present in data. Such a danger is facing us with word embedding, a popular
framework to represent text data as vectors which has been used in many machine
learning and natural language processing tasks. We show that even word
embeddings trained on Google News articles exhibit female/male gender
stereotypes to a disturbing extent. This raises concerns because their
widespread use, as we describe, often tends to amplify these biases.
Geometrically, gender bias is first shown to be captured by a direction in the
word embedding. Second, gender neutral words are shown to be linearly separable
from gender definition words in the word embedding. Using these properties, we
provide a methodology for modifying an embedding to remove gender stereotypes,
such as the association between between the words receptionist and female,
while maintaining desired associations such as between the words queen and
female. We define metrics to quantify both direct and indirect gender biases in
embeddings, and develop algorithms to ""debias"" the embedding. Using
crowd-worker evaluation as well as standard benchmarks, we empirically
demonstrate that our algorithms significantly reduce gender bias in embeddings
while preserving the its useful properties such as the ability to cluster
related concepts and to solve analogy tasks. The resulting embeddings can be
used in applications without amplifying gender bias.","['Tolga Bolukbasi', 'Kai-Wei Chang', 'James Zou', 'Venkatesh Saligrama', 'Adam Kalai']","['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']",2016-07-21 22:26:20+00:00
http://arxiv.org/abs/1607.06450v1,Layer Normalization,"Training state-of-the-art, deep neural networks is computationally expensive.
One way to reduce the training time is to normalize the activities of the
neurons. A recently introduced technique called batch normalization uses the
distribution of the summed input to a neuron over a mini-batch of training
cases to compute a mean and variance which are then used to normalize the
summed input to that neuron on each training case. This significantly reduces
the training time in feed-forward neural networks. However, the effect of batch
normalization is dependent on the mini-batch size and it is not obvious how to
apply it to recurrent neural networks. In this paper, we transpose batch
normalization into layer normalization by computing the mean and variance used
for normalization from all of the summed inputs to the neurons in a layer on a
single training case. Like batch normalization, we also give each neuron its
own adaptive bias and gain which are applied after the normalization but before
the non-linearity. Unlike batch normalization, layer normalization performs
exactly the same computation at training and test times. It is also
straightforward to apply to recurrent neural networks by computing the
normalization statistics separately at each time step. Layer normalization is
very effective at stabilizing the hidden state dynamics in recurrent networks.
Empirically, we show that layer normalization can substantially reduce the
training time compared with previously published techniques.","['Jimmy Lei Ba', 'Jamie Ryan Kiros', 'Geoffrey E. Hinton']","['stat.ML', 'cs.LG']",2016-07-21 19:57:52+00:00
http://arxiv.org/abs/1607.06364v1,Distributed Supervised Learning using Neural Networks,"Distributed learning is the problem of inferring a function in the case where
training data is distributed among multiple geographically separated sources.
Particularly, the focus is on designing learning strategies with low
computational requirements, in which communication is restricted only to
neighboring agents, with no reliance on a centralized authority. In this
thesis, we analyze multiple distributed protocols for a large number of neural
network architectures. The first part of the thesis is devoted to a definition
of the problem, followed by an extensive overview of the state-of-the-art.
Next, we introduce different strategies for a relatively simple class of single
layer neural networks, where a linear output layer is preceded by a nonlinear
layer, whose weights are stochastically assigned in the beginning of the
learning process. We consider both batch and sequential learning, with
horizontally and vertically partitioned data. In the third part, we consider
instead the more complex problem of semi-supervised distributed learning, where
each agent is provided with an additional set of unlabeled training samples. We
propose two different algorithms based on diffusion processes for linear
support vector machines and kernel ridge regression. Subsequently, the fourth
part extends the discussion to learning with time-varying data (e.g.
time-series) using recurrent neural networks. We consider two different
families of networks, namely echo state networks (extending the algorithms
introduced in the second part), and spline adaptive filters. Overall, the
algorithms presented throughout the thesis cover a wide range of possible
practical applications, and lead the way to numerous future extensions, which
are briefly summarized in the conclusive chapter.",['Simone Scardapane'],"['stat.ML', 'cs.LG']",2016-07-21 15:32:47+00:00
http://arxiv.org/abs/1607.06335v1,Admissible Hierarchical Clustering Methods and Algorithms for Asymmetric Networks,"This paper characterizes hierarchical clustering methods that abide by two
previously introduced axioms -- thus, denominated admissible methods -- and
proposes tractable algorithms for their implementation. We leverage the fact
that, for asymmetric networks, every admissible method must be contained
between reciprocal and nonreciprocal clustering, and describe three families of
intermediate methods. Grafting methods exchange branches between dendrograms
generated by different admissible methods. The convex combination family
combines admissible methods through a convex operation in the space of
dendrograms, and thirdly, the semi-reciprocal family clusters nodes that are
related by strong cyclic influences in the network. Algorithms for the
computation of hierarchical clusters generated by reciprocal and nonreciprocal
clustering as well as the grafting, convex combination, and semi-reciprocal
families are derived using matrix operations in a dioid algebra. Finally, the
introduced clustering methods and algorithms are exemplified through their
application to a network describing the interrelation between sectors of the
United States (U.S.) economy.","['Gunnar Carlsson', 'Facundo Mémoli', 'Alejandro Ribeiro', 'Santiago Segarra']","['cs.LG', 'stat.ML']",2016-07-21 14:22:12+00:00
http://arxiv.org/abs/1607.06333v3,Uncovering Causality from Multivariate Hawkes Integrated Cumulants,"We design a new nonparametric method that allows one to estimate the matrix
of integrated kernels of a multivariate Hawkes process. This matrix not only
encodes the mutual influences of each nodes of the process, but also
disentangles the causality relationships between them. Our approach is the
first that leads to an estimation of this matrix without any parametric
modeling and estimation of the kernels themselves. A consequence is that it can
give an estimation of causality relationships between nodes (or users), based
on their activity timestamps (on a social network for instance), without
knowing or estimating the shape of the activities lifetime. For that purpose,
we introduce a moment matching method that fits the third-order integrated
cumulants of the process. We show on numerical experiments that our approach is
indeed very robust to the shape of the kernels, and gives appealing results on
the MemeTracker database.","['Massil Achab', 'Emmanuel Bacry', 'Stéphane Gaïffas', 'Iacopo Mastromatteo', 'Jean-Francois Muzy']","['stat.ML', 'cs.LG']",2016-07-21 14:19:23+00:00
http://arxiv.org/abs/1607.06294v1,Hierarchical Clustering of Asymmetric Networks,"This paper considers networks where relationships between nodes are
represented by directed dissimilarities. The goal is to study methods that,
based on the dissimilarity structure, output hierarchical clusters, i.e., a
family of nested partitions indexed by a connectivity parameter. Our
construction of hierarchical clustering methods is built around the concept of
admissible methods, which are those that abide by the axioms of value - nodes
in a network with two nodes are clustered together at the maximum of the two
dissimilarities between them - and transformation - when dissimilarities are
reduced, the network may become more clustered but not less. Two particular
methods, termed reciprocal and nonreciprocal clustering, are shown to provide
upper and lower bounds in the space of admissible methods. Furthermore,
alternative clustering methodologies and axioms are considered. In particular,
modifying the axiom of value such that clustering in two-node networks occurs
at the minimum of the two dissimilarities entails the existence of a unique
admissible clustering method.","['Gunnar Carlsson', 'Facundo Mémoli', 'Alejandro Ribeiro', 'Santiago Segarra']","['cs.LG', 'stat.ML']",2016-07-21 12:32:47+00:00
http://arxiv.org/abs/1607.06280v2,Explaining Classification Models Built on High-Dimensional Sparse Data,"Predictive modeling applications increasingly use data representing people's
behavior, opinions, and interactions. Fine-grained behavior data often has
different structure from traditional data, being very high-dimensional and
sparse. Models built from these data are quite difficult to interpret, since
they contain many thousands or even many millions of features. Listing features
with large model coefficients is not sufficient, because the model coefficients
do not incorporate information on feature presence, which is key when analysing
sparse data. In this paper we introduce two alternatives for explaining
predictive models by listing important features. We evaluate these alternatives
in terms of explanation ""bang for the buck,"", i.e., how many examples'
inferences are explained for a given number of features listed. The bottom
line: (i) The proposed alternatives have double the bang-for-the-buck as
compared to just listing the high-coefficient features, and (ii) interestingly,
although they come from different sources and motivations, the two new
alternatives provide strikingly similar rankings of important features.","['Julie Moeyersoms', ""Brian d'Alessandro"", 'Foster Provost', 'David Martens']","['stat.ML', 'cs.LG']",2016-07-21 11:50:41+00:00
http://arxiv.org/abs/1607.06146v1,"Supervised quantum gate ""teaching"" for quantum hardware design","We show how to train a quantum network of pairwise interacting qubits such
that its evolution implements a target quantum algorithm into a given network
subset. Our strategy is inspired by supervised learning and is designed to help
the physical construction of a quantum computer which operates with minimal
external classical control.","['Leonardo Banchi', 'Nicola Pancotti', 'Sougato Bose']","['cs.LG', 'quant-ph', 'stat.ML']",2016-07-20 22:46:32+00:00
http://arxiv.org/abs/1607.06017v2,Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition,"We study $k$-GenEV, the problem of finding the top $k$ generalized
eigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in
canonical-correlation analysis. We propose algorithms $\mathtt{LazyEV}$ and
$\mathtt{LazyCCA}$ to solve the two problems with running times linearly
dependent on the input size and on $k$.
  Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend
only on the square root of the matrix condition number, and on the square root
of the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA.
We also provide the first gap-free results, which provide running times that
depend on $1/\sqrt{\varepsilon}$ rather than the eigengap.","['Zeyuan Allen-Zhu', 'Yuanzhi Li']","['math.OC', 'cs.DS', 'cs.LG', 'stat.ML']",2016-07-20 16:43:18+00:00
http://arxiv.org/abs/1607.06011v1,On the Modeling of Error Functions as High Dimensional Landscapes for Weight Initialization in Learning Networks,"Next generation deep neural networks for classification hosted on embedded
platforms will rely on fast, efficient, and accurate learning algorithms.
Initialization of weights in learning networks has a great impact on the
classification accuracy. In this paper we focus on deriving good initial
weights by modeling the error function of a deep neural network as a
high-dimensional landscape. We observe that due to the inherent complexity in
its algebraic structure, such an error function may conform to general results
of the statistics of large systems. To this end we apply some results from
Random Matrix Theory to analyse these functions. We model the error function in
terms of a Hamiltonian in N-dimensions and derive some theoretical results
about its general behavior. These results are further used to make better
initial guesses of weights for the learning algorithm.","['Julius', 'Gopinath Mahale', 'Sumana T.', 'C. S. Adityakrishna']","['cs.LG', 'cs.CV', 'physics.data-an', 'stat.ML']",2016-07-20 16:25:27+00:00
http://arxiv.org/abs/1607.05974v1,Anomaly Detection and Localisation using Mixed Graphical Models,"We propose a method that performs anomaly detection and localisation within
heterogeneous data using a pairwise undirected mixed graphical model. The data
are a mixture of categorical and quantitative variables, and the model is
learned over a dataset that is supposed not to contain any anomaly. We then use
the model over temporal data, potentially a data stream, using a version of the
two-sided CUSUM algorithm. The proposed decision statistic is based on a
conditional likelihood ratio computed for each variable given the others. Our
results show that this function allows to detect anomalies variable by
variable, and thus to localise the variables involved in the anomalies more
precisely than univariate methods based on simple marginals.","['Romain Laby', 'François Roueff', 'Alexandre Gramfort']",['stat.ML'],2016-07-20 14:31:30+00:00
http://arxiv.org/abs/1607.05970v2,On the Identification and Mitigation of Weaknesses in the Knowledge Gradient Policy for Multi-Armed Bandits,"The Knowledge Gradient (KG) policy was originally proposed for online ranking
and selection problems but has recently been adapted for use in online decision
making in general and multi-armed bandit problems (MABs) in particular. We
study its use in a class of exponential family MABs and identify weaknesses,
including a propensity to take actions which are dominated with respect to both
exploitation and exploration. We propose variants of KG which avoid such
errors. These new policies include an index heuristic which deploys a KG
approach to develop an approximation to the Gittins index. A numerical study
shows this policy to perform well over a range of MABs including those for
which index policies are not optimal. While KG does not make dominated actions
when bandits are Gaussian, it fails to be index consistent and appears not to
enjoy a performance advantage over competitor policies when arms are correlated
to compensate for its greater computational demands.","['James Edwards', 'Paul Fearnhead', 'Kevin Glazebrook']","['stat.ML', 'cs.LG']",2016-07-20 14:21:42+00:00
http://arxiv.org/abs/1607.05966v1,Onsager-corrected deep learning for sparse linear inverse problems,"Deep learning has gained great popularity due to its widespread success on
many inference problems. We consider the application of deep learning to the
sparse linear inverse problem encountered in compressive sensing, where one
seeks to recover a sparse signal from a small number of noisy linear
measurements. In this paper, we propose a novel neural-network architecture
that decouples prediction errors across layers in the same way that the
approximate message passing (AMP) algorithm decouples them across iterations:
through Onsager correction. Numerical experiments suggest that our ""learned
AMP"" network significantly improves upon Gregor and LeCun's ""learned ISTA""
network in both accuracy and complexity.","['Mark Borgerding', 'Philip Schniter']","['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2016-07-20 14:14:49+00:00
http://arxiv.org/abs/1607.05832v1,Personalization Effect on Emotion Recognition from Physiological Data: An Investigation of Performance on Different Setups and Classifiers,"This paper addresses the problem of emotion recognition from physiological
signals. Features are extracted and ranked based on their effect on
classification accuracy. Different classifiers are compared. The inter-subject
variability and the personalization effect are thoroughly investigated, through
trial-based and subject-based cross-validation. Finally, a personalized model
is introduced, that would allow for enhanced emotional state prediction, based
on the physiological data of subjects that exhibit a certain degree of
similarity, without the requirement of further feedback.",['Varvara Kollia'],"['stat.ML', 'cs.HC']",2016-07-20 06:32:16+00:00
http://arxiv.org/abs/1607.05709v1,Multi-category Angle-based Classifier Refit,"Classification is an important statistical learning tool. In real
application, besides high prediction accuracy, it is often desirable to
estimate class conditional probabilities for new observations. For traditional
problems where the number of observations is large, there exist many well
developed approaches. Recently, high dimensional low sample size problems are
becoming increasingly popular. Margin-based classifiers, such as logistic
regression, are well established methods in the literature. On the other hand,
in terms of probability estimation, it is known that for binary classifiers,
the commonly used methods tend to under-estimate the norm of the classification
function. This can lead to biased probability estimation. Remedy approaches
have been proposed in the literature. However, for the simultaneous
multicategory classification framework, much less work has been done. We fill
the gap in this paper. In particular, we give theoretical insights on why heavy
regularization terms are often needed in high dimensional applications, and how
this can lead to bias in probability estimation. To overcome this difficulty,
we propose a new refit strategy for multicategory angle-based classifiers. Our
new method only adds a small computation cost to the problem, and is able to
attain prediction accuracy that is as good as the regular margin-based
classifiers. On the other hand, the improvement of probability estimation can
be very significant. Numerical results suggest that the new refit approach is
highly competitive.","['Guo Xian Yau', 'Chong Zhang']","['math.ST', 'stat.ML', 'stat.TH']",2016-07-19 19:49:55+00:00
http://arxiv.org/abs/1607.05691v1,Information-theoretical label embeddings for large-scale image classification,"We present a method for training multi-label, massively multi-class image
classification models, that is faster and more accurate than supervision via a
sigmoid cross-entropy loss (logistic regression). Our method consists in
embedding high-dimensional sparse labels onto a lower-dimensional dense sphere
of unit-normed vectors, and treating the classification problem as a cosine
proximity regression problem on this sphere. We test our method on a dataset of
300 million high-resolution images with 17,000 labels, where it yields
considerably faster convergence, as well as a 7% higher mean average precision
compared to logistic regression.",['François Chollet'],"['cs.CV', 'cs.LG', 'stat.ML']",2016-07-19 18:40:01+00:00
http://arxiv.org/abs/1607.05573v2,Combining Random Walks and Nonparametric Bayesian Topic Model for Community Detection,"Community detection has been an active research area for decades. Among all
probabilistic models, Stochastic Block Model has been the most popular one.
This paper introduces a novel probabilistic model: RW-HDP, based on random
walks and Hierarchical Dirichlet Process, for community extraction. In RW-HDP,
random walks conducted in a social network are treated as documents; nodes are
treated as words. By using Hierarchical Dirichlet Process, a nonparametric
Bayesian model, we are not only able to cluster nodes into different
communities, but also determine the number of communities automatically. We use
Stochastic Variational Inference for our model inference, which makes our
method time efficient and can be easily extended to an online learning
algorithm.","['Ruimin Zhu', 'Wenxin Jiang']","['stat.AP', 'stat.ML']",2016-07-19 13:46:15+00:00
http://arxiv.org/abs/1607.05506v2,Distribution-dependent concentration inequalities for tighter generalization bounds,"Concentration inequalities are indispensable tools for studying the
generalization capacity of learning models. Hoeffding's and McDiarmid's
inequalities are commonly used, giving bounds independent of the data
distribution. Although this makes them widely applicable, a drawback is that
the bounds can be too loose in some specific cases. Although efforts have been
devoted to improving the bounds, we find that the bounds can be further
tightened in some distribution-dependent scenarios and conditions for the
inequalities can be relaxed. In particular, we propose four types of conditions
for probabilistic boundedness and bounded differences, and derive several
distribution-dependent extensions of Hoeffding's and McDiarmid's inequalities.
These extensions provide bounds for functions not satisfying the conditions of
the existing inequalities, and in some special cases, tighter bounds.
Furthermore, we obtain generalization bounds for unbounded and
hierarchy-bounded loss functions. Finally we discuss the potential applications
of our extensions to learning theory.","['Xinxing Wu', 'Junping Zhang']",['stat.ML'],2016-07-19 10:23:57+00:00
http://arxiv.org/abs/1607.05432v3,Nested Kriging predictions for datasets with large number of observations,"This work falls within the context of predicting the value of a real function
at some input locations given a limited number of observations of this
function. The Kriging interpolation technique (or Gaussian process regression)
is often considered to tackle such a problem but the method suffers from its
computational burden when the number of observation points is large. We
introduce in this article nested Kriging predictors which are constructed by
aggregating sub-models based on subsets of observation points. This approach is
proven to have better theoretical properties than other aggregation methods
that can be found in the literature. Contrarily to some other methods it can be
shown that the proposed aggregation method is consistent. Finally, the
practical interest of the proposed method is illustrated on simulated datasets
and on an industrial test case with $10^4$ observations in a 6-dimensional
space.","['Didier Rullière', 'Nicolas Durrande', 'François Bachoc', 'Clément Chevalier']",['stat.ML'],2016-07-19 07:27:02+00:00
http://arxiv.org/abs/1607.05241v1,Imitation Learning with Recurrent Neural Networks,"We present a novel view that unifies two frameworks that aim to solve
sequential prediction problems: learning to search (L2S) and recurrent neural
networks (RNN). We point out equivalences between elements of the two
frameworks. By complementing what is missing from one framework comparing to
the other, we introduce a more advanced imitation learning framework that, on
one hand, augments L2S s notion of search space and, on the other hand,
enhances RNNs training procedure to be more robust to compounding errors
arising from training on highly correlated examples.",['Khanh Nguyen'],"['cs.CL', 'cs.LG', 'stat.ML']",2016-07-18 19:01:00+00:00
http://arxiv.org/abs/1607.05154v1,On the Application of Support Vector Machines to the Prediction of Propagation Losses at 169 MHz for Smart Metering Applications,"Recently, the need of deploying new wireless networks for smart gas metering
has raised the problem of radio planning in the169 MHz band. Unluckily,
software tools commonly adopted for radio planning in cellular communication
systems cannot be employed to solve this problem because of the substantially
lower transmission frequencies characterizing this application. In this
manuscript a novel data-centric solution, based on the use of support vector
machine techniques for classification and regression, is proposed. Our method
requires the availability of a limited set of received signal strength
measurements and the knowledge of a three-dimensional map of the propagation
environment of interest, and generates both an estimate of the coverage area
and a prediction of the field strength within it. Numerical results referring
to different Italian villages and cities evidence that our method is able to
achieve good accuracy at the price of an acceptable computational cost and of a
limited effort for the acquisition of measurements in the considered
environments.","['Martino Uccellari', 'Francesca Facchini', 'Matteo Sola', 'Emilio Sirignano', 'Giorgio M. Vitetta', 'Andrea Barbieri', 'Stefano Tondelli']","['stat.ML', 'stat.AP']",2016-07-18 16:08:36+00:00
http://arxiv.org/abs/1607.05047v1,"A Batch, Off-Policy, Actor-Critic Algorithm for Optimizing the Average Reward","We develop an off-policy actor-critic algorithm for learning an optimal
policy from a training set composed of data from multiple individuals. This
algorithm is developed with a view towards its use in mobile health.","['S. A. Murphy', 'Y. Deng', 'E. B. Laber', 'H. R. Maei', 'R. S. Sutton', 'K. Witkiewitz']","['stat.ML', 'cs.LG']",2016-07-18 12:43:40+00:00
http://arxiv.org/abs/1607.05002v1,Geometric Mean Metric Learning,"We revisit the task of learning a Euclidean metric from data. We approach
this problem from first principles and formulate it as a surprisingly simple
optimization problem. Indeed, our formulation even admits a closed form
solution. This solution possesses several very attractive properties: (i) an
innate geometric appeal through the Riemannian geometry of positive definite
matrices; (ii) ease of interpretability; and (iii) computational speed several
orders of magnitude faster than the widely used LMNN and ITML methods.
Furthermore, on standard benchmark datasets, our closed-form solution
consistently attains higher classification accuracy.","['Pourya Habib Zadeh', 'Reshad Hosseini', 'Suvrit Sra']","['stat.ML', 'cs.LG']",2016-07-18 10:14:46+00:00
http://arxiv.org/abs/1607.04903v3,Learning Unitary Operators with Help From u(n),"A major challenge in the training of recurrent neural networks is the
so-called vanishing or exploding gradient problem. The use of a norm-preserving
transition operator can address this issue, but parametrization is challenging.
In this work we focus on unitary operators and describe a parametrization using
the Lie algebra $\mathfrak{u}(n)$ associated with the Lie group $U(n)$ of $n
\times n$ unitary matrices. The exponential map provides a correspondence
between these spaces, and allows us to define a unitary matrix using $n^2$ real
coefficients relative to a basis of the Lie algebra. The parametrization is
closed under additive updates of these coefficients, and thus provides a simple
space in which to do gradient descent. We demonstrate the effectiveness of this
parametrization on the problem of learning arbitrary unitary operators,
comparing to several baselines and outperforming a recently-proposed
lower-dimensional parametrization. We additionally use our parametrization to
generalize a recently-proposed unitary recurrent neural network to arbitrary
unitary matrices, using it to solve standard long-memory tasks.","['Stephanie L. Hyland', 'Gunnar Rätsch']","['stat.ML', 'cs.LG']",2016-07-17 18:58:12+00:00
http://arxiv.org/abs/1607.04579v2,Learning from Conditional Distributions via Dual Embeddings,"Many machine learning tasks, such as learning with invariance and policy
evaluation in reinforcement learning, can be characterized as problems of
learning from conditional distributions. In such problems, each sample $x$
itself is associated with a conditional distribution $p(z|x)$ represented by
samples $\{z_i\}_{i=1}^M$, and the goal is to learn a function $f$ that links
these conditional distributions to target values $y$. These learning problems
become very challenging when we only have limited samples or in the extreme
case only one sample from each conditional distribution. Commonly used
approaches either assume that $z$ is independent of $x$, or require an
overwhelmingly large samples from each conditional distribution.
  To address these challenges, we propose a novel approach which employs a new
min-max reformulation of the learning from conditional distribution problem.
With such new reformulation, we only need to deal with the joint distribution
$p(z,x)$. We also design an efficient learning algorithm, Embedding-SGD, and
establish theoretical sample complexity for such problems. Finally, our
numerical experiments on both synthetic and real-world datasets show that the
proposed approach can significantly improve over the existing algorithms.","['Bo Dai', 'Niao He', 'Yunpeng Pan', 'Byron Boots', 'Le Song']","['cs.LG', 'math.OC', 'stat.ML']",2016-07-15 16:56:22+00:00
http://arxiv.org/abs/1607.04573v2,Analyzing features learned for Offline Signature Verification using Deep CNNs,"Research on Offline Handwritten Signature Verification explored a large
variety of handcrafted feature extractors, ranging from graphology, texture
descriptors to interest points. In spite of advancements in the last decades,
performance of such systems is still far from optimal when we test the systems
against skilled forgeries - signature forgeries that target a particular
individual. In previous research, we proposed a formulation of the problem to
learn features from data (signature images) in a Writer-Independent format,
using Deep Convolutional Neural Networks (CNNs), seeking to improve performance
on the task. In this research, we push further the performance of such method,
exploring a range of architectures, and obtaining a large improvement in
state-of-the-art performance on the GPDS dataset, the largest publicly
available dataset on the task. In the GPDS-160 dataset, we obtained an Equal
Error Rate of 2.74%, compared to 6.97% in the best result published in
literature (that used a combination of multiple classifiers). We also present a
visual analysis of the feature space learned by the model, and an analysis of
the errors made by the classifier. Our analysis shows that the model is very
effective in separating signatures that have a different global appearance,
while being particularly vulnerable to forgeries that very closely resemble
genuine signatures, even if their line quality is bad, which is the case of
slowly-traced forgeries.","['Luiz G. Hafemann', 'Robert Sabourin', 'Luiz S. Oliveira']","['cs.CV', 'stat.ML']",2016-07-15 16:35:20+00:00
http://arxiv.org/abs/1607.04566v1,Spectral Echolocation via the Wave Embedding,"Spectral embedding uses eigenfunctions of the discrete Laplacian on a
weighted graph to obtain coordinates for an embedding of an abstract data set
into Euclidean space. We propose a new pre-processing step of first using the
eigenfunctions to simulate a low-frequency wave moving over the data and using
both position as well as change in time of the wave to obtain a refined metric
to which classical methods of dimensionality reduction can then applied. This
is motivated by the behavior of waves, symmetries of the wave equation and the
hunting technique of bats. It is shown to be effective in practice and also
works for other partial differential equations -- the method yields improved
results even for the classical heat equation.","['Alexander Cloninger', 'Stefan Steinerberger']",['stat.ML'],2016-07-15 16:08:57+00:00
http://arxiv.org/abs/1607.04492v2,Neural Tree Indexers for Text Understanding,"Recurrent neural networks (RNNs) process input text sequentially and model
the conditional transition between word tokens. In contrast, the advantages of
recursive networks include that they explicitly model the compositionality and
the recursive structure of natural language. However, the current recursive
architecture is limited by its dependence on syntactic tree. In this paper, we
introduce a robust syntactic parsing-independent tree structured model, Neural
Tree Indexers (NTI) that provides a middle ground between the sequential RNNs
and the syntactic treebased recursive models. NTI constructs a full n-ary tree
by processing the input text with its node function in a bottom-up fashion.
Attention mechanism can then be applied to both structure and node function. We
implemented and evaluated a binarytree model of NTI, showing the model achieved
the state-of-the-art performance on three different NLP tasks: natural language
inference, answer sentence selection, and sentence classification,
outperforming state-of-the-art recurrent and recursive neural networks.","['Tsendsuren Munkhdalai', 'Hong Yu']","['cs.CL', 'cs.LG', 'stat.ML']",2016-07-15 12:59:01+00:00
http://arxiv.org/abs/1607.05073v1,Higher-Order Block Term Decomposition for Spatially Folded fMRI Data,"The growing use of neuroimaging technologies generates a massive amount of
biomedical data that exhibit high dimensionality. Tensor-based analysis of
brain imaging data has been proved quite effective in exploiting their multiway
nature. The advantages of tensorial methods over matrix-based approaches have
also been demonstrated in the characterization of functional magnetic resonance
imaging (fMRI) data, where the spatial (voxel) dimensions are commonly grouped
(unfolded) as a single way/mode of the 3-rd order array, the other two ways
corresponding to time and subjects. However, such methods are known to be
ineffective in more demanding scenarios, such as the ones with strong noise
and/or significant overlapping of activated regions. This paper aims at
investigating the possible gains from a better exploitation of the spatial
dimension, through a higher- (4 or 5) order tensor modeling of the fMRI signal.
In this context, and in order to increase the degrees of freedom of the
modeling process, a higher-order Block Term Decomposition (BTD) is applied, for
the first time in fMRI analysis. Its effectiveness is demonstrated via
extensive simulation results.","['Christos Chatzichristos', 'Eleftherios Kofidis', 'Giannis Kopsinis', 'Sergios Theodoridis']","['cs.NA', 'stat.ML']",2016-07-15 09:28:48+00:00
http://arxiv.org/abs/1607.04331v2,Random projections of random manifolds,"Interesting data often concentrate on low dimensional smooth manifolds inside
a high dimensional ambient space. Random projections are a simple, powerful
tool for dimensionality reduction of such data. Previous works have studied
bounds on how many projections are needed to accurately preserve the geometry
of these manifolds, given their intrinsic dimensionality, volume and curvature.
However, such works employ definitions of volume and curvature that are
inherently difficult to compute. Therefore such theory cannot be easily tested
against numerical simulations to understand the tightness of the proven bounds.
We instead study typical distortions arising in random projections of an
ensemble of smooth Gaussian random manifolds. We find explicitly computable,
approximate theoretical bounds on the number of projections required to
accurately preserve the geometry of these manifolds. Our bounds, while
approximate, can only be violated with a probability that is exponentially
small in the ambient dimension, and therefore they hold with high probability
in cases of practical interest. Moreover, unlike previous work, we test our
theoretical bounds against numerical experiments on the actual geometric
distortions that typically occur for random projections of random smooth
manifolds. We find our bounds are tighter than previous results by several
orders of magnitude.","['Subhaneil Lahiri', 'Peiran Gao', 'Surya Ganguli']","['stat.ML', 'cs.LG', 'q-bio.NC']",2016-07-14 21:43:39+00:00
http://arxiv.org/abs/1607.04315v3,Neural Semantic Encoders,"We present a memory augmented neural network for natural language
understanding: Neural Semantic Encoders. NSE is equipped with a novel memory
update rule and has a variable sized encoding memory that evolves over time and
maintains the understanding of input sequences through read}, compose and write
operations. NSE can also access multiple and shared memories. In this paper, we
demonstrated the effectiveness and the flexibility of NSE on five different
natural language tasks: natural language inference, question answering,
sentence classification, document sentiment analysis and machine translation
where NSE achieved state-of-the-art performance when evaluated on publically
available benchmarks. For example, our shared-memory model showed an
encouraging result on neural machine translation, improving an attention-based
baseline by approximately 1.0 BLEU.","['Tsendsuren Munkhdalai', 'Hong Yu']","['cs.LG', 'cs.CL', 'stat.ML']",2016-07-14 20:58:26+00:00
http://arxiv.org/abs/1607.04228v1,Fifty Shades of Ratings: How to Benefit from a Negative Feedback in Top-N Recommendations Tasks,"Conventional collaborative filtering techniques treat a top-n recommendations
problem as a task of generating a list of the most relevant items. This
formulation, however, disregards an opposite - avoiding recommendations with
completely irrelevant items. Due to that bias, standard algorithms, as well as
commonly used evaluation metrics, become insensitive to negative feedback. In
order to resolve this problem we propose to treat user feedback as a
categorical variable and model it with users and items in a ternary way. We
employ a third-order tensor factorization technique and implement a higher
order folding-in method to support online recommendations. The method is
equally sensitive to entire spectrum of user ratings and is able to accurately
predict relevant items even from a negative only feedback. Our method may
partially eliminate the need for complicated rating elicitation process as it
provides means for personalized recommendations from the very beginning of an
interaction with a recommender system. We also propose a modification of
standard metrics which helps to reveal unwanted biases and account for
sensitivity to a negative feedback. Our model achieves state-of-the-art quality
in standard recommendation tasks while significantly outperforming other
methods in the cold-start ""no-positive-feedback"" scenarios.","['Evgeny Frolov', 'Ivan Oseledets']","['cs.LG', 'cs.IR', 'stat.ML', 'H.3.3']",2016-07-14 17:55:33+00:00
http://arxiv.org/abs/1607.04209v1,Dynamic Question Ordering in Online Surveys,"Online surveys have the potential to support adaptive questions, where later
questions depend on earlier responses. Past work has taken a rule-based
approach, uniformly across all respondents. We envision a richer interpretation
of adaptive questions, which we call dynamic question ordering (DQO), where
question order is personalized. Such an approach could increase engagement, and
therefore response rate, as well as imputation quality. We present a DQO
framework to improve survey completion and imputation. In the general
survey-taking setting, we want to maximize survey completion, and so we focus
on ordering questions to engage the respondent and collect hopefully all
information, or at least the information that most characterizes the
respondent, for accurate imputations. In another scenario, our goal is to
provide a personalized prediction. Since it is possible to give reasonable
predictions with only a subset of questions, we are not concerned with
motivating users to answer all questions. Instead, we want to order questions
to get information that reduces prediction uncertainty, while not being too
burdensome. We illustrate this framework with an example of providing energy
estimates to prospective tenants. We also discuss DQO for national surveys and
consider connections between our statistics-based question-ordering approach
and cognitive survey methodology.","['Kirstin Early', 'Jennifer Mankoff', 'Stephen E. Fienberg']","['stat.OT', 'stat.ME', 'stat.ML']",2016-07-14 17:03:24+00:00
http://arxiv.org/abs/1607.03975v2,Estimating and Controlling the False Discovery Rate for the PC Algorithm Using Edge-Specific P-Values,"The PC algorithm allows investigators to estimate a complete partially
directed acyclic graph (CPDAG) from a finite dataset, but few groups have
investigated strategies for estimating and controlling the false discovery rate
(FDR) of the edges in the CPDAG. In this paper, we introduce PC with p-values
(PC-p), a fast algorithm which robustly computes edge-specific p-values and
then estimates and controls the FDR across the edges. PC-p specifically uses
the p-values returned by many conditional independence tests to upper bound the
p-values of more complex edge-specific hypothesis tests. The algorithm then
estimates and controls the FDR using the bounded p-values and the
Benjamini-Yekutieli FDR procedure. Modifications to the original PC algorithm
also help PC-p accurately compute the upper bounds despite non-zero Type II
error rates. Experiments show that PC-p yields more accurate FDR estimation and
control across the edges in a variety of CPDAGs compared to alternative
methods.","['Eric V. Strobl', 'Peter L. Spirtes', 'Shyam Visweswaran']","['stat.ML', 'stat.ME']",2016-07-14 01:34:57+00:00
http://arxiv.org/abs/1607.03954v1,Ensemble preconditioning for Markov chain Monte Carlo simulation,"We describe parallel Markov chain Monte Carlo methods that propagate a
collective ensemble of paths, with local covariance information calculated from
neighboring replicas. The use of collective dynamics eliminates multiplicative
noise and stabilizes the dynamics thus providing a practical approach to
difficult anisotropic sampling problems in high dimensions. Numerical
experiments with model problems demonstrate that dramatic potential speedups,
compared to various alternative schemes, are attainable.","['Charles Matthews', 'Jonathan Weare', 'Benedict Leimkuhler']","['stat.ME', 'math.NA', 'stat.CO', 'stat.ML']",2016-07-13 23:12:05+00:00
http://arxiv.org/abs/1607.03854v7,The Partially Observable Hidden Markov Model and its Application to Keystroke Dynamics,"The partially observable hidden Markov model is an extension of the hidden
Markov Model in which the hidden state is conditioned on an independent Markov
chain. This structure is motivated by the presence of discrete metadata, such
as an event type, that may partially reveal the hidden state but itself
emanates from a separate process. Such a scenario is encountered in keystroke
dynamics whereby a user's typing behavior is dependent on the text that is
typed. Under the assumption that the user can be in either an active or passive
state of typing, the keyboard key names are event types that partially reveal
the hidden state due to the presence of relatively longer time intervals
between words and sentences than between letters of a word. Using five public
datasets, the proposed model is shown to consistently outperform other anomaly
detectors, including the standard HMM, in biometric identification and
verification tasks and is generally preferred over the HMM in a Monte Carlo
goodness of fit test.","['John V. Monaco', 'Charles C. Tappert']","['cs.IT', 'cs.CR', 'math.IT', 'stat.ML']",2016-07-13 18:30:33+00:00
http://arxiv.org/abs/1607.03849v2,Fitting a Simplicial Complex using a Variation of k-means,"We give a simple and effective two stage algorithm for approximating a point
cloud $\mathcal{S}\subset\mathbb{R}^m$ by a simplicial complex $K$. The first
stage is an iterative fitting procedure that generalizes k-means clustering,
while the second stage involves deleting redundant simplices. A form of
dimension reduction of $\mathcal{S}$ is obtained as a consequence.",['Piotr Beben'],"['cs.LG', 'cs.CG', 'stat.ML']",2016-07-13 18:15:52+00:00
http://arxiv.org/abs/1607.03842v1,Safe Policy Improvement by Minimizing Robust Baseline Regret,"An important problem in sequential decision-making under uncertainty is to
use limited data to compute a safe policy, i.e., a policy that is guaranteed to
perform at least as well as a given baseline strategy. In this paper, we
develop and analyze a new model-based approach to compute a safe policy when we
have access to an inaccurate dynamics model of the system with known accuracy
guarantees. Our proposed robust method uses this (inaccurate) model to directly
minimize the (negative) regret w.r.t. the baseline policy. Contrary to the
existing approaches, minimizing the regret allows one to improve the baseline
policy in states with accurate dynamics and seamlessly fall back to the
baseline policy, otherwise. We show that our formulation is NP-hard and propose
an approximate algorithm. Our empirical results on several domains show that
even this relatively simple approximate algorithm can significantly outperform
standard approaches.","['Marek Petrik', 'Yinlam Chow', 'Mohammad Ghavamzadeh']",['stat.ML'],2016-07-13 17:55:45+00:00
http://arxiv.org/abs/1607.03822v1,Feature Extraction and Automated Classification of Heartbeats by Machine Learning,"We present algorithms for the detection of a class of heart arrhythmias with
the goal of eventual adoption by practicing cardiologists. In clinical
practice, detection is based on a small number of meaningful features extracted
from the heartbeat cycle. However, techniques proposed in the literature use
high dimensional vectors consisting of morphological, and time based features
for detection. Using electrocardiogram (ECG) signals, we found smaller subsets
of features sufficient to detect arrhythmias with high accuracy. The features
were found by an iterative step-wise feature selection method. We depart from
common literature in the following aspects: 1. As opposed to a high dimensional
feature vectors, we use a small set of features with meaningful clinical
interpretation, 2. we eliminate the necessity of short-duration
patient-specific ECG data to append to the global training data for
classification 3. We apply semi-parametric classification procedures (in an
ensemble framework) for arrhythmia detection, and 4. our approach is based on a
reduced sampling rate of ~ 115 Hz as opposed to 360 Hz in standard literature.","['Choudur Lakshminarayan', 'Tony Basil']","['stat.ML', 'cs.LG']",2016-07-13 16:46:55+00:00
http://arxiv.org/abs/1607.03815v2,Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than $O(1/ε)$,"In this paper, we develop a novel {\bf ho}moto{\bf p}y {\bf s}moothing (HOPS)
algorithm for solving a family of non-smooth problems that is composed of a
non-smooth term with an explicit max-structure and a smooth term or a simple
non-smooth term whose proximal mapping is easy to compute. The best known
iteration complexity for solving such non-smooth optimization problems is
$O(1/\epsilon)$ without any assumption on the strong convexity. In this work,
we will show that the proposed HOPS achieved a lower iteration complexity of
$\widetilde O(1/\epsilon^{1-\theta})$\footnote{$\widetilde O()$ suppresses a
logarithmic factor.} with $\theta\in(0,1]$ capturing the local sharpness of the
objective function around the optimal solutions. To the best of our knowledge,
this is the lowest iteration complexity achieved so far for the considered
non-smooth optimization problems without strong convexity assumption. The HOPS
algorithm employs Nesterov's smoothing technique and Nesterov's accelerated
gradient method and runs in stages, which gradually decreases the smoothing
parameter in a stage-wise manner until it yields a sufficiently good
approximation of the original function. We show that HOPS enjoys a linear
convergence for many well-known non-smooth problems (e.g., empirical risk
minimization with a piece-wise linear loss function and $\ell_1$ norm
regularizer, finding a point in a polyhedron, cone programming, etc).
Experimental results verify the effectiveness of HOPS in comparison with
Nesterov's smoothing algorithm and the primal-dual style of first-order
methods.","['Yi Xu', 'Yan Yan', 'Qihang Lin', 'Tianbao Yang']","['math.OC', 'stat.ML']",2016-07-13 16:20:52+00:00
http://arxiv.org/abs/1607.03792v1,Kernel Density Estimation for Dynamical Systems,"We study the density estimation problem with observations generated by
certain dynamical systems that admit a unique underlying invariant Lebesgue
density. Observations drawn from dynamical systems are not independent and
moreover, usual mixing concepts may not be appropriate for measuring the
dependence among these observations. By employing the $\mathcal{C}$-mixing
concept to measure the dependence, we conduct statistical analysis on the
consistency and convergence of the kernel density estimator. Our main results
are as follows: First, we show that with properly chosen bandwidth, the kernel
density estimator is universally consistent under $L_1$-norm; Second, we
establish convergence rates for the estimator with respect to several classes
of dynamical systems under $L_1$-norm. In the analysis, the density function
$f$ is only assumed to be H\""{o}lder continuous which is a weak assumption in
the literature of nonparametric density estimation and also more realistic in
the dynamical system context. Last but not least, we prove that the same
convergence rates of the estimator under $L_\infty$-norm and $L_1$-norm can be
achieved when the density function is H\""{o}lder continuous, compactly
supported and bounded. The bandwidth selection problem of the kernel density
estimator for dynamical system is also discussed in our study via numerical
simulations.","['Hanyuan Hang', 'Ingo Steinwart', 'Yunlong Feng', 'Johan A. K. Suykens']",['stat.ML'],2016-07-13 15:32:35+00:00
http://arxiv.org/abs/1607.03730v1,Learning Shallow Detection Cascades for Wearable Sensor-Based Mobile Health Applications,"The field of mobile health aims to leverage recent advances in wearable
on-body sensing technology and smart phone computing capabilities to develop
systems that can monitor health states and deliver just-in-time adaptive
interventions. However, existing work has largely focused on analyzing
collected data in the off-line setting. In this paper, we propose a novel
approach to learning shallow detection cascades developed explicitly for use in
a real-time wearable-phone or wearable-phone-cloud systems. We apply our
approach to the problem of cigarette smoking detection from a combination of
wrist-worn actigraphy data and respiration chest band data using two and three
stage cascades.","['Hamid Dadkhahi', 'Nazir Saleheen', 'Santosh Kumar', 'Benjamin Marlin']","['stat.ML', 'cs.LG']",2016-07-13 13:47:49+00:00
http://arxiv.org/abs/1607.03615v1,Multiple-Instance Logistic Regression with LASSO Penalty,"In this work, we consider a manufactory process which can be described by a
multiple-instance logistic regression model. In order to compute the maximum
likelihood estimation of the unknown coefficient, an expectation-maximization
algorithm is proposed, and the proposed modeling approach can be extended to
identify the important covariates by adding the coefficient penalty term into
the likelihood function. In addition to essential technical details, we
demonstrate the usefulness of the proposed method by simulations and real
examples.","['Ray-Bing Chen', 'Kuang-Hung Cheng', 'Sheng-Mao Chang', 'Shuen-Lin Jeng', 'Ping-Yang Chen', 'Chun-Hao Yang', 'Chi-Chun Hsia']","['stat.ML', 'stat.AP']",2016-07-13 07:30:57+00:00
http://arxiv.org/abs/1607.03574v4,Effects of Additional Data on Bayesian Clustering,"Hierarchical probabilistic models, such as mixture models, are used for
cluster analysis. These models have two types of variables: observable and
latent. In cluster analysis, the latent variable is estimated, and it is
expected that additional information will improve the accuracy of the
estimation of the latent variable. Many proposed learning methods are able to
use additional data; these include semi-supervised learning and transfer
learning. However, from a statistical point of view, a complex probabilistic
model that encompasses both the initial and additional data might be less
accurate due to having a higher-dimensional parameter. The present paper
presents a theoretical analysis of the accuracy of such a model and clarifies
which factor has the greatest effect on its accuracy, the advantages of
obtaining additional data, and the disadvantages of increasing the complexity.",['Keisuke Yamazaki'],['stat.ML'],2016-07-13 02:41:24+00:00
http://arxiv.org/abs/1607.03559v1,Fast Sampling for Strongly Rayleigh Measures with Application to Determinantal Point Processes,"In this note we consider sampling from (non-homogeneous) strongly Rayleigh
probability measures. As an important corollary, we obtain a fast mixing Markov
Chain sampler for Determinantal Point Processes.","['Chengtao Li', 'Stefanie Jegelka', 'Suvrit Sra']","['cs.LG', 'cs.DS', 'math.PR', 'stat.ML']",2016-07-13 01:22:04+00:00
http://arxiv.org/abs/1607.03516v2,Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation,"In this paper, we propose a novel unsupervised domain adaptation algorithm
based on deep learning for visual object recognition. Specifically, we design a
new model called Deep Reconstruction-Classification Network (DRCN), which
jointly learns a shared encoding representation for two tasks: i) supervised
classification of labeled source data, and ii) unsupervised reconstruction of
unlabeled target data.In this way, the learnt representation not only preserves
discriminability, but also encodes useful information from the target domain.
Our new DRCN model can be optimized by using backpropagation similarly as the
standard neural networks.
  We evaluate the performance of DRCN on a series of cross-domain object
recognition tasks, where DRCN provides a considerable improvement (up to ~8% in
accuracy) over the prior state-of-the-art algorithms. Interestingly, we also
observe that the reconstruction pipeline of DRCN transforms images from the
source domain into images whose appearance resembles the target dataset. This
suggests that DRCN's performance is due to constructing a single composite
representation that encodes information about both the structure of target
images and the classification of source images. Finally, we provide a formal
analysis to justify the algorithm's objective in domain adaptation context.","['Muhammad Ghifary', 'W. Bastiaan Kleijn', 'Mengjie Zhang', 'David Balduzzi', 'Wen Li']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2016-07-12 20:48:58+00:00
http://arxiv.org/abs/1607.03502v1,Natural brain-information interfaces: Recommending information by relevance inferred from human brain signals,"Finding relevant information from large document collections such as the
World Wide Web is a common task in our daily lives. Estimation of a user's
interest or search intention is necessary to recommend and retrieve relevant
information from these collections. We introduce a brain-information interface
used for recommending information by relevance inferred directly from brain
signals. In experiments, participants were asked to read Wikipedia documents
about a selection of topics while their EEG was recorded. Based on the
prediction of word relevance, the individual's search intent was modeled and
successfully used for retrieving new, relevant documents from the whole English
Wikipedia corpus. The results show that the users' interests towards digital
content can be modeled from the brain signals evoked by reading. The introduced
brain-relevance paradigm enables the recommendation of information without any
explicit user interaction, and may be applied across diverse
information-intensive applications.","['Manuel J. A. Eugster', 'Tuukka Ruotsalo', 'Michiel M. Spapé', 'Oswald Barral', 'Niklas Ravaja', 'Giulio Jacucci', 'Samuel Kaski']","['cs.IR', 'cs.HC', 'q-bio.NC', 'stat.ML']",2016-07-12 20:17:00+00:00
http://arxiv.org/abs/1607.03475v1,Nystrom Method for Approximating the GMM Kernel,"The GMM (generalized min-max) kernel was recently proposed (Li, 2016) as a
measure of data similarity and was demonstrated effective in machine learning
tasks. In order to use the GMM kernel for large-scale datasets, the prior work
resorted to the (generalized) consistent weighted sampling (GCWS) to convert
the GMM kernel to linear kernel. We call this approach as ``GMM-GCWS''.
  In the machine learning literature, there is a popular algorithm which we
call ``RBF-RFF''. That is, one can use the ``random Fourier features'' (RFF) to
convert the ``radial basis function'' (RBF) kernel to linear kernel. It was
empirically shown in (Li, 2016) that RBF-RFF typically requires substantially
more samples than GMM-GCWS in order to achieve comparable accuracies.
  The Nystrom method is a general tool for computing nonlinear kernels, which
again converts nonlinear kernels into linear kernels. We apply the Nystrom
method for approximating the GMM kernel, a strategy which we name as
``GMM-NYS''. In this study, our extensive experiments on a set of fairly large
datasets confirm that GMM-NYS is also a strong competitor of RBF-RFF.",['Ping Li'],"['stat.ML', 'cs.LG']",2016-07-12 19:42:40+00:00
http://arxiv.org/abs/1607.03463v2,LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain,"We study $k$-SVD that is to obtain the first $k$ singular vectors of a matrix
$A$. Recently, a few breakthroughs have been discovered on $k$-SVD: Musco and
Musco [1] proved the first gap-free convergence result using the block Krylov
method, Shamir [2] discovered the first variance-reduction stochastic method,
and Bhojanapalli et al. [3] provided the fastest $O(\mathsf{nnz}(A) +
\mathsf{poly}(1/\varepsilon))$-time algorithm using alternating minimization.
  In this paper, we put forward a new and simple LazySVD framework to improve
the above breakthroughs. This framework leads to a faster gap-free method
outperforming [1], and the first accelerated and stochastic method
outperforming [2]. In the $O(\mathsf{nnz}(A) + \mathsf{poly}(1/\varepsilon))$
running-time regime, LazySVD outperforms [3] in certain parameter regimes
without even using alternating minimization.","['Zeyuan Allen-Zhu', 'Yuanzhi Li']","['cs.NA', 'cs.DS', 'cs.LG', 'math.OC', 'stat.ML']",2016-07-12 18:41:52+00:00
http://arxiv.org/abs/1607.03456v1,Incomplete Pivoted QR-based Dimensionality Reduction,"High-dimensional big data appears in many research fields such as image
recognition, biology and collaborative filtering. Often, the exploration of
such data by classic algorithms is encountered with difficulties due to `curse
of dimensionality' phenomenon. Therefore, dimensionality reduction methods are
applied to the data prior to its analysis. Many of these methods are based on
principal components analysis, which is statistically driven, namely they map
the data into a low-dimension subspace that preserves significant statistical
properties of the high-dimensional data. As a consequence, such methods do not
directly address the geometry of the data, reflected by the mutual distances
between multidimensional data point. Thus, operations such as classification,
anomaly detection or other machine learning tasks may be affected.
  This work provides a dictionary-based framework for geometrically driven data
analysis that includes dimensionality reduction, out-of-sample extension and
anomaly detection. It embeds high-dimensional data in a low-dimensional
subspace. This embedding preserves the original high-dimensional geometry of
the data up to a user-defined distortion rate. In addition, it identifies a
subset of landmark data points that constitute a dictionary for the analyzed
dataset. The dictionary enables to have a natural extension of the
low-dimensional embedding to out-of-sample data points, which gives rise to a
distortion-based criterion for anomaly detection. The suggested method is
demonstrated on synthetic and real-world datasets and achieves good results for
classification, anomaly detection and out-of-sample tasks.","['Amit Bermanis', 'Aviv Rotbart', 'Moshe Salhov', 'Amir Averbuch']","['cs.LG', 'stat.ML']",2016-07-12 18:20:23+00:00
http://arxiv.org/abs/1607.03428v3,Learning in Quantum Control: High-Dimensional Global Optimization for Noisy Quantum Dynamics,"Quantum control is valuable for various quantum technologies such as
high-fidelity gates for universal quantum computing, adaptive quantum-enhanced
metrology, and ultra-cold atom manipulation. Although supervised machine
learning and reinforcement learning are widely used for optimizing control
parameters in classical systems, quantum control for parameter optimization is
mainly pursued via gradient-based greedy algorithms. Although the quantum
fitness landscape is often compatible with greedy algorithms, sometimes greedy
algorithms yield poor results, especially for large-dimensional quantum
systems. We employ differential evolution algorithms to circumvent the
stagnation problem of non-convex optimization. We improve quantum control
fidelity for noisy system by averaging over the objective function. To reduce
computational cost, we introduce heuristics for early termination of runs and
for adaptive selection of search subspaces. Our implementation is massively
parallel and vectorized to reduce run time even further. We demonstrate our
methods with two examples, namely quantum phase estimation and quantum gate
design, for which we achieve superior fidelity and scalability than obtained
using greedy algorithms.","['Pantita Palittapongarnpim', 'Peter Wittek', 'Ehsan Zahedinejad', 'Shakib Vedaie', 'Barry C. Sanders']","['cs.LG', 'cs.SY', 'quant-ph', 'stat.ML']",2016-07-12 16:17:38+00:00
http://arxiv.org/abs/1607.03392v3,Statistical power and prediction accuracy in multisite resting-state fMRI connectivity,"Connectivity studies using resting-state functional magnetic resonance
imaging are increasingly pooling data acquired at multiple sites. While this
may allow investigators to speed up recruitment or increase sample size,
multisite studies also potentially introduce systematic biases in connectivity
measures across sites. In this work, we measure the inter-site effect in
connectivity and its impact on our ability to detect individual and group
differences. Our study was based on real, as opposed to simulated, multisite
fMRI datasets collected in N=345 young, healthy subjects across 8 scanning
sites with 3T scanners and heterogeneous scanning protocols, drawn from the
1000 functional connectome project. We first empirically show that typical
functional networks were reliably found at the group level in all sites, and
that the amplitude of the inter-site effects was small to moderate, with a
Cohen's effect size below 0.5 on average across brain connections. We then
implemented a series of Monte-Carlo simulations, based on real data, to
evaluate the impact of the multisite effects on detection power in statistical
tests comparing two groups (with and without the effect) using a general linear
model, as well as on the prediction of group labels with a support-vector
machine. As a reference, we also implemented the same simulations with fMRI
data collected at a single site using an identical sample size. Simulations
revealed that using data from heterogeneous sites only slightly decreased our
ability to detect changes compared to a monosite study with the GLM, and had a
greater impact on prediction accuracy. Taken together, our results support the
feasibility of multisite studies in rs-fMRI provided the sample size is large
enough.","['Christian Dansereau', 'Yassine Benhajali', 'Celine Risterucci', 'Emilio Merlo Pich', 'Pierre Orban', 'Douglas Arnold', 'Pierre Bellec']","['q-bio.QM', 'cs.CE', 'stat.ML']",2016-07-12 15:22:52+00:00
http://arxiv.org/abs/1607.03360v1,Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods,"The well known maximum-entropy principle due to Jaynes, which states that
given mean parameters, the maximum entropy distribution matching them is in an
exponential family, has been very popular in machine learning due to its
""Occam's razor"" interpretation. Unfortunately, calculating the potentials in
the maximum-entropy distribution is intractable \cite{bresler2014hardness}. We
provide computationally efficient versions of this principle when the mean
parameters are pairwise moments: we design distributions that approximately
match given pairwise moments, while having entropy which is comparable to the
maximum entropy distribution matching those moments.
  We additionally provide surprising applications of the approximate maximum
entropy principle to designing provable variational methods for partition
function calculations for Ising models without any assumptions on the
potentials of the model. More precisely, we show that in every temperature, we
can get approximation guarantees for the log-partition function comparable to
those in the low-temperature limit, which is the setting of optimization of
quadratic forms over the hypercube. \cite{alon2006approximating}","['Yuanzhi Li', 'Andrej Risteski']","['cs.LG', 'cs.DS', 'stat.ML']",2016-07-12 14:09:03+00:00
http://arxiv.org/abs/1607.03313v1,Predicting the evolution of stationary graph signals,"An emerging way of tackling the dimensionality issues arising in the modeling
of a multivariate process is to assume that the inherent data structure can be
captured by a graph. Nevertheless, though state-of-the-art graph-based methods
have been successful for many learning tasks, they do not consider
time-evolving signals and thus are not suitable for prediction. Based on the
recently introduced joint stationarity framework for time-vertex processes,
this letter considers multivariate models that exploit the graph topology so as
to facilitate the prediction. The resulting method yields similar accuracy to
the joint (time-graph) mean-squared error estimator but at lower complexity,
and outperforms purely time-based methods.","['Andreas Loukas', 'Nathanael Perraudin']","['stat.ML', 'cs.LG']",2016-07-12 11:30:30+00:00
http://arxiv.org/abs/1607.03300v1,From Dependence to Causation,"Machine learning is the science of discovering statistical dependencies in
data, and the use of those dependencies to perform predictions. During the last
decade, machine learning has made spectacular progress, surpassing human
performance in complex tasks such as object recognition, car driving, and
computer gaming. However, the central role of prediction in machine learning
avoids progress towards general-purpose artificial intelligence. As one way
forward, we argue that causal inference is a fundamental component of human
intelligence, yet ignored by learning algorithms.
  Causal inference is the problem of uncovering the cause-effect relationships
between the variables of a data generating system. Causal structures provide
understanding about how these systems behave under changing, unseen
environments. In turn, knowledge about these causal dynamics allows to answer
""what if"" questions, describing the potential responses of the system under
hypothetical manipulations and interventions. Thus, understanding cause and
effect is one step from machine learning towards machine reasoning and machine
intelligence. But, currently available causal inference algorithms operate in
specific regimes, and rely on assumptions that are difficult to verify in
practice.
  This thesis advances the art of causal inference in three different ways.
First, we develop a framework for the study of statistical dependence based on
copulas and random features. Second, we build on this framework to interpret
the problem of causal inference as the task of distribution classification,
yielding a family of novel causal inference algorithms. Third, we discover
causal structures in convolutional neural network features using our
algorithms. The algorithms presented in this thesis are scalable, exhibit
strong theoretical guarantees, and achieve state-of-the-art performance in a
variety of real-world benchmarks.",['David Lopez-Paz'],['stat.ML'],2016-07-12 10:32:02+00:00
http://arxiv.org/abs/1607.03204v1,Information Projection and Approximate Inference for Structured Sparse Variables,"Approximate inference via information projection has been recently introduced
as a general-purpose approach for efficient probabilistic inference given
sparse variables. This manuscript goes beyond classical sparsity by proposing
efficient algorithms for approximate inference via information projection that
are applicable to any structure on the set of variables that admits enumeration
using a \emph{matroid}. We show that the resulting information projection can
be reduced to combinatorial submodular optimization subject to matroid
constraints. Further, leveraging recent advances in submodular optimization, we
provide an efficient greedy algorithm with strong optimization-theoretic
guarantees. The class of probabilistic models that can be expressed in this way
is quite broad and, as we show, includes group sparse regression, group sparse
principal components analysis and sparse canonical correlation analysis, among
others. Moreover, empirical results on simulated data and high dimensional
neuroimaging data highlight the superior performance of the information
projection approach as compared to established baselines for a range of
probabilistic models.","['Rajiv Khanna', 'Joydeep Ghosh', 'Russell Poldrack', 'Oluwasanmi Koyejo']","['stat.ML', 'cs.LG']",2016-07-12 00:11:59+00:00
http://arxiv.org/abs/1607.03202v1,Rapid Prediction of Player Retention in Free-to-Play Mobile Games,"Predicting and improving player retention is crucial to the success of mobile
Free-to-Play games. This paper explores the problem of rapid retention
prediction in this context. Heuristic modeling approaches are introduced as a
way of building simple rules for predicting short-term retention. Compared to
common classification algorithms, our heuristic-based approach achieves
reasonable and comparable performance using information from the first session,
day, and week of player activity.","['Anders Drachen', 'Eric Thurston Lundquist', 'Yungjen Kung', 'Pranav Simha Rao', 'Diego Klabjan', 'Rafet Sifa', 'Julian Runge']","['stat.ML', 'cs.SI', 'stat.AP']",2016-07-12 00:03:05+00:00
http://arxiv.org/abs/1607.03200v2,Qualitative Judgement of Research Impact: Domain Taxonomy as a Fundamental Framework for Judgement of the Quality of Research,"The appeal of metric evaluation of research impact has attracted considerable
interest in recent times. Although the public at large and administrative
bodies are much interested in the idea, scientists and other researchers are
much more cautious, insisting that metrics are but an auxiliary instrument to
the qualitative peer-based judgement. The goal of this article is to propose
availing of such a well positioned construct as domain taxonomy as a tool for
directly assessing the scope and quality of research. We first show how
taxonomies can be used to analyse the scope and perspectives of a set of
research projects or papers. Then we proceed to define a research team or
researcher's rank by those nodes in the hierarchy that have been created or
significantly transformed by the results of the researcher. An experimental
test of the approach in the data analysis domain is described. Although the
concept of taxonomy seems rather simplistic to describe all the richness of a
research domain, its changes and use can be made transparent and subject to
open discussions.","['Fionn Murtagh', 'Michael Orlov', 'Boris Mirkin']","['cs.DL', 'stat.ML', '68P01', 'H.0, I.5.3, G.3']",2016-07-11 23:42:36+00:00
http://arxiv.org/abs/1607.03191v1,On Deterministic Conditions for Subspace Clustering under Missing Data,"In this paper we present deterministic conditions for success of sparse
subspace clustering (SSC) under missing data, when data is assumed to come from
a Union of Subspaces (UoS) model. We consider two algorithms, which are
variants of SSC with entry-wise zero-filling that differ in terms of the
optimization problems used to find affinity matrix for spectral clustering. For
both the algorithms, we provide deterministic conditions for any pattern of
missing data such that perfect clustering can be achieved. We provide extensive
sets of simulation results for clustering as well as completion of data at
missing entries, under the UoS model. Our experimental results indicate that in
contrast to the full data case, accurate clustering does not imply accurate
subspace identification and completion, indicating the natural order of
relative hardness of these problems.","['Wenqi Wang', 'Shuchin Aeron', 'Vaneet Aggarwal']","['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2016-07-11 22:40:31+00:00
http://arxiv.org/abs/1607.03183v1,How to calculate partition functions using convex programming hierarchies: provable bounds for variational methods,"We consider the problem of approximating partition functions for Ising
models. We make use of recent tools in combinatorial optimization: the
Sherali-Adams and Lasserre convex programming hierarchies, in combination with
variational methods to get algorithms for calculating partition functions in
these families. These techniques give new, non-trivial approximation guarantees
for the partition function beyond the regime of correlation decay. They also
generalize some classical results from statistical physics about the
Curie-Weiss ferromagnetic Ising model, as well as provide a partition function
counterpart of classical results about max-cut on dense graphs
\cite{arora1995polynomial}. With this, we connect techniques from two
apparently disparate research areas -- optimization and counting/partition
function approximations. (i.e. \#-P type of problems).
  Furthermore, we design to the best of our knowledge the first provable,
convex variational methods. Though in the literature there are a host of convex
versions of variational methods \cite{wainwright2003tree, wainwright2005new,
heskes2006convexity, meshi2009convexifying}, they come with no guarantees
(apart from some extremely special cases, like e.g. the graph has a single
cycle \cite{weiss2000correctness}). We consider dense and low threshold rank
graphs, and interestingly, the reason our approach works on these types of
graphs is because local correlations propagate to global correlations --
completely the opposite of algorithms based on correlation decay. In the
process we design novel entropy approximations based on the low-order moments
of a distribution.
  Our proof techniques are very simple and generic, and likely to be applicable
to many other settings other than Ising models.",['Andrej Risteski'],"['cs.LG', 'cs.DS', 'stat.ML']",2016-07-11 22:10:04+00:00
http://arxiv.org/abs/1607.03084v1,Kernel-based methods for bandit convex optimization,"We consider the adversarial convex bandit problem and we build the first
$\mathrm{poly}(T)$-time algorithm with $\mathrm{poly}(n) \sqrt{T}$-regret for
this problem. To do so we introduce three new ideas in the derivative-free
optimization literature: (i) kernel methods, (ii) a generalization of Bernoulli
convolutions, and (iii) a new annealing schedule for exponential weights (with
increasing learning rate). The basic version of our algorithm achieves
$\tilde{O}(n^{9.5} \sqrt{T})$-regret, and we show that a simple variant of this
algorithm can be run in $\mathrm{poly}(n \log(T))$-time per step at the cost of
an additional $\mathrm{poly}(n) T^{o(1)}$ factor in the regret. These results
improve upon the $\tilde{O}(n^{11} \sqrt{T})$-regret and
$\exp(\mathrm{poly}(T))$-time result of the first two authors, and the
$\log(T)^{\mathrm{poly}(n)} \sqrt{T}$-regret and
$\log(T)^{\mathrm{poly}(n)}$-time result of Hazan and Li. Furthermore we
conjecture that another variant of the algorithm could achieve
$\tilde{O}(n^{1.5} \sqrt{T})$-regret, and moreover that this regret is
unimprovable (the current best lower bound being $\Omega(n \sqrt{T})$ and it is
achieved with linear functions). For the simpler situation of zeroth order
stochastic convex optimization this corresponds to the conjecture that the
optimal query complexity is of order $n^3 / \epsilon^2$.","['Sébastien Bubeck', 'Ronen Eldan', 'Yin Tat Lee']","['cs.LG', 'cs.DS', 'math.OC', 'stat.ML']",2016-07-11 19:25:07+00:00
http://arxiv.org/abs/1607.03081v2,Proximal Quasi-Newton Methods for Regularized Convex Optimization with Linear and Accelerated Sublinear Convergence Rates,"In [19], a general, inexact, efficient proximal quasi-Newton algorithm for
composite optimization problems has been proposed and a sublinear global
convergence rate has been established. In this paper, we analyze the
convergence properties of this method, both in the exact and inexact setting,
in the case when the objective function is strongly convex. We also investigate
a practical variant of this method by establishing a simple stopping criterion
for the subproblem optimization. Furthermore, we consider an accelerated
variant, based on FISTA [1], to the proximal quasi-Newton algorithm. A similar
accelerated method has been considered in [7], where the convergence rate
analysis relies on very strong impractical assumptions. We present a modified
analysis while relaxing these assumptions and perform a practical comparison of
the accelerated proximal quasi- Newton algorithm and the regular one. Our
analysis and computational results show that acceleration may not bring any
benefit in the quasi-Newton setting.","['Hiva Ghanbari', 'Katya Scheinberg']","['cs.NA', 'cs.LG', 'math.OC', 'stat.ML']",2016-07-11 19:20:06+00:00
http://arxiv.org/abs/1607.03050v1,Learning a metric for class-conditional KNN,"Naive Bayes Nearest Neighbour (NBNN) is a simple and effective framework
which addresses many of the pitfalls of K-Nearest Neighbour (KNN)
classification. It has yielded competitive results on several computer vision
benchmarks. Its central tenet is that during NN search, a query is not compared
to every example in a database, ignoring class information. Instead, NN
searches are performed within each class, generating a score per class. A key
problem with NN techniques, including NBNN, is that they fail when the data
representation does not capture perceptual (e.g.~class-based) similarity. NBNN
circumvents this by using independent engineered descriptors (e.g.~SIFT). To
extend its applicability outside of image-based domains, we propose to learn a
metric which captures perceptual similarity. Similar to how Neighbourhood
Components Analysis optimizes a differentiable form of KNN classification, we
propose ""Class Conditional"" metric learning (CCML), which optimizes a soft form
of the NBNN selection rule. Typical metric learning algorithms learn either a
global or local metric. However, our proposed method can be adjusted to a
particular level of locality by tuning a single parameter. An empirical
evaluation on classification and retrieval tasks demonstrates that our proposed
method clearly outperforms existing learned distance metrics across a variety
of image and non-image datasets.","['Daniel Jiwoong Im', 'Graham W. Taylor']","['cs.LG', 'stat.ML']",2016-07-11 17:29:19+00:00
http://arxiv.org/abs/1607.03026v1,Retrospective Causal Inference with Machine Learning Ensembles: An Application to Anti-Recidivism Policies in Colombia,"We present new methods to estimate causal effects retrospectively from micro
data with the assistance of a machine learning ensemble. This approach
overcomes two important limitations in conventional methods like regression
modeling or matching: (i) ambiguity about the pertinent retrospective
counterfactuals and (ii) potential misspecification, overfitting, and otherwise
bias-prone or inefficient use of a large identifying covariate set in the
estimation of causal effects. Our method targets the analysis toward a well
defined ``retrospective intervention effect'' (RIE) based on hypothetical
population interventions and applies a machine learning ensemble that allows
data to guide us, in a controlled fashion, on how to use a large identifying
covariate set. We illustrate with an analysis of policy options for reducing
ex-combatant recidivism in Colombia.","['Cyrus Samii', 'Laura Paler', 'Sarah Zukerman Daly']",['stat.ML'],2016-07-11 16:47:47+00:00
http://arxiv.org/abs/1607.02959v2,From Behavior to Sparse Graphical Games: Efficient Recovery of Equilibria,"In this paper we study the problem of exact recovery of the pure-strategy
Nash equilibria (PSNE) set of a graphical game from noisy observations of joint
actions of the players alone. We consider sparse linear influence games --- a
parametric class of graphical games with linear payoffs, and represented by
directed graphs of n nodes (players) and in-degree of at most k. We present an
$\ell_1$-regularized logistic regression based algorithm for recovering the
PSNE set exactly, that is both computationally efficient --- i.e. runs in
polynomial time --- and statistically efficient --- i.e. has logarithmic sample
complexity. Specifically, we show that the sufficient number of samples
required for exact PSNE recovery scales as $\mathcal{O}(\mathrm{poly}(k) \log
n)$. We also validate our theoretical results using synthetic experiments.","['Asish Ghoshal', 'Jean Honorio']","['cs.GT', 'cs.LG', 'stat.ML']",2016-07-11 14:05:16+00:00
http://arxiv.org/abs/1607.02914v1,Minimum Description Length Principle in Supervised Learning with Application to Lasso,"The minimum description length (MDL) principle in supervised learning is
studied. One of the most important theories for the MDL principle is Barron and
Cover's theory (BC theory), which gives a mathematical justification of the MDL
principle. The original BC theory, however, can be applied to supervised
learning only approximately and limitedly. Though Barron et al. recently
succeeded in removing a similar approximation in case of unsupervised learning,
their idea cannot be essentially applied to supervised learning in general. To
overcome this issue, an extension of BC theory to supervised learning is
proposed. The derived risk bound has several advantages inherited from the
original BC theory. First, the risk bound holds for finite sample size. Second,
it requires remarkably few assumptions. Third, the risk bound has a form of
redundancy of the two-stage code for the MDL procedure. Hence, the proposed
extension gives a mathematical justification of the MDL principle to supervised
learning like the original BC theory. As an important example of application,
new risk and (probabilistic) regret bounds of lasso with random design are
derived. The derived risk bound holds for any finite sample size $n$ and
feature number $p$ even if $n\ll p$ without boundedness of features in contrast
to the past work. Behavior of the regret bound is investigated by numerical
simulations. We believe that this is the first extension of BC theory to
general supervised learning with random design without approximation.","['Masanori Kawakita', ""Jun'ichi Takeuchi""]","['cs.IT', 'math.IT', 'stat.ML']",2016-07-11 12:18:20+00:00
http://arxiv.org/abs/1607.02802v1,Mapping distributional to model-theoretic semantic spaces: a baseline,"Word embeddings have been shown to be useful across state-of-the-art systems
in many natural language processing tasks, ranging from question answering
systems to dependency parsing. (Herbelot and Vecchi, 2015) explored word
embeddings and their utility for modeling language semantics. In particular,
they presented an approach to automatically map a standard distributional
semantic space onto a set-theoretic model using partial least squares
regression. We show in this paper that a simple baseline achieves a +51%
relative improvement compared to their model on one of the two datasets they
used, and yields competitive results on the second dataset.",['Franck Dernoncourt'],"['cs.CL', 'cs.AI', 'stat.ML']",2016-07-11 01:20:57+00:00
http://arxiv.org/abs/1607.02801v2,Bounds on the Number of Measurements for Reliable Compressive Classification,"This paper studies the classification of high-dimensional Gaussian signals
from low-dimensional noisy, linear measurements. In particular, it provides
upper bounds (sufficient conditions) on the number of measurements required to
drive the probability of misclassification to zero in the low-noise regime,
both for random measurements and designed ones. Such bounds reveal two
important operational regimes that are a function of the characteristics of the
source: i) when the number of classes is less than or equal to the dimension of
the space spanned by signals in each class, reliable classification is possible
in the low-noise regime by using a one-vs-all measurement design; ii) when the
dimension of the spaces spanned by signals in each class is lower than the
number of classes, reliable classification is guaranteed in the low-noise
regime by using a simple random measurement design. Simulation results both
with synthetic and real data show that our analysis is sharp, in the sense that
it is able to gauge the number of measurements required to drive the
misclassification probability to zero in the low-noise regime.","['Hugo Reboredo', 'Francesco Renna', 'Robert Calderbank', 'Miguel R. D. Rodrigues']","['cs.IT', 'cs.CV', 'math.IT', 'stat.ML']",2016-07-11 01:08:14+00:00
http://arxiv.org/abs/1607.02793v3,On Faster Convergence of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization,"The cyclic block coordinate descent-type (CBCD-type) methods, which performs
iterative updates for a few coordinates (a block) simultaneously throughout the
procedure, have shown remarkable computational performance for solving strongly
convex minimization problems. Typical applications include many popular
statistical machine learning methods such as elastic-net regression, ridge
penalized logistic regression, and sparse additive regression. Existing
optimization literature has shown that for strongly convex minimization, the
CBCD-type methods attain iteration complexity of
$\mathcal{O}(p\log(1/\epsilon))$, where $\epsilon$ is a pre-specified accuracy
of the objective value, and $p$ is the number of blocks. However, such
iteration complexity explicitly depends on $p$, and therefore is at least $p$
times worse than the complexity $\mathcal{O}(\log(1/\epsilon))$ of gradient
descent (GD) methods. To bridge this theoretical gap, we propose an improved
convergence analysis for the CBCD-type methods. In particular, we first show
that for a family of quadratic minimization problems, the iteration complexity
$\mathcal{O}(\log^2(p)\cdot\log(1/\epsilon))$ of the CBCD-type methods matches
that of the GD methods in term of dependency on $p$, up to a $\log^2 p$ factor.
Thus our complexity bounds are sharper than the existing bounds by at least a
factor of $p/\log^2(p)$. We also provide a lower bound to confirm that our
improved complexity bounds are tight (up to a $\log^2 (p)$ factor), under the
assumption that the largest and smallest eigenvalues of the Hessian matrix do
not scale with $p$. Finally, we generalize our analysis to other strongly
convex minimization problems beyond quadratic ones.","['Xingguo Li', 'Tuo Zhao', 'Raman Arora', 'Han Liu', 'Mingyi Hong']","['math.OC', 'cs.LG', 'stat.ML']",2016-07-10 23:15:18+00:00
http://arxiv.org/abs/1607.02763v1,How to Allocate Resources For Features Acquisition?,"We study classification problems where features are corrupted by noise and
where the magnitude of the noise in each feature is influenced by the resources
allocated to its acquisition. This is the case, for example, when multiple
sensors share a common resource (power, bandwidth, attention, etc.). We develop
a method for computing the optimal resource allocation for a variety of
scenarios and derive theoretical bounds concerning the benefit that may arise
by non-uniform allocation. We further demonstrate the effectiveness of the
developed method in simulations.","['Oran Richman', 'Shie Mannor']","['cs.AI', 'cs.LG', 'stat.ML']",2016-07-10 16:19:00+00:00
http://arxiv.org/abs/1607.02738v2,Magnetic Hamiltonian Monte Carlo,"Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct
efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we
present a generalization of HMC which exploits \textit{non-canonical}
Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3
dimensions a subset of the dynamics map onto the mechanics of a charged
particle coupled to a magnetic field. We establish a theoretical basis for the
use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic,
leapfrog-like integrator allowing for the implementation of magnetic HMC.
Finally, we exhibit several examples where these non-canonical dynamics can
lead to improved mixing of magnetic HMC relative to ordinary HMC.","['Nilesh Tripuraneni', 'Mark Rowland', 'Zoubin Ghahramani', 'Richard Turner']",['stat.ML'],2016-07-10 12:19:29+00:00
http://arxiv.org/abs/1607.02676v1,Bayesian quantile additive regression trees,"Ensemble of regression trees have become popular statistical tools for the
estimation of conditional mean given a set of predictors. However, quantile
regression trees and their ensembles have not yet garnered much attention
despite the increasing popularity of the linear quantile regression model. This
work proposes a Bayesian quantile additive regression trees model that shows
very good predictive performance illustrated using simulation studies and real
data applications. Further extension to tackle binary classification problems
is also considered.","['Bereket P. Kindo', 'Hao Wang', 'Timothy Hanson', 'Edsel A. Peña']",['stat.ML'],2016-07-10 01:20:57+00:00
http://arxiv.org/abs/1607.02675v4,Covariate Regularized Community Detection in Sparse Graphs,"In this paper, we investigate community detection in networks in the presence
of node covariates. In many instances, covariates and networks individually
only give a partial view of the cluster structure. One needs to jointly infer
the full cluster structure by considering both. In statistics, an emerging body
of work has been focused on combining information from both the edges in the
network and the node covariates to infer community memberships. However, so far
the theoretical guarantees have been established in the dense regime, where the
network can lead to perfect clustering under a broad parameter regime, and
hence the role of covariates is often not clear. In this paper, we examine
sparse networks in conjunction with finite dimensional sub-gaussian mixtures as
covariates under moderate separation conditions. In this setting each
individual source can only cluster a non-vanishing fraction of nodes correctly.
We propose a simple optimization framework which provably improves clustering
accuracy when the two sources carry partial information about the cluster
memberships, and hence perform poorly on their own. Our optimization problem
can be solved using scalable convex optimization algorithms. Using a variety of
simulated and real data examples, we show that the proposed method outperforms
other existing methodology.","['Bowei Yan', 'Purnamrita Sarkar']","['stat.ME', 'stat.ML']",2016-07-10 01:07:16+00:00
http://arxiv.org/abs/1607.02670v1,Sparse additive Gaussian process with soft interactions,"Additive nonparametric regression models provide an attractive tool for
variable selection in high dimensions when the relationship between the
response and predictors is complex. They offer greater flexibility compared to
parametric non-linear regression models and better interpretability and
scalability than the non-parametric regression models. However, achieving
sparsity simultaneously in the number of nonparametric components as well as in
the variables within each nonparametric component poses a stiff computational
challenge. In this article, we develop a novel Bayesian additive regression
model using a combination of hard and soft shrinkages to separately control the
number of additive components and the variables within each component. An
efficient algorithm is developed to select the importance variables and
estimate the interaction network. Excellent performance is obtained in
simulated and real data examples.","['Garret Vo', 'Debdeep Pati']",['stat.ML'],2016-07-09 22:29:32+00:00
http://arxiv.org/abs/1607.02665v2,Classifier Risk Estimation under Limited Labeling Resources,"In this paper we propose strategies for estimating performance of a
classifier when labels cannot be obtained for the whole test set. The number of
test instances which can be labeled is very small compared to the whole test
data size. The goal then is to obtain a precise estimate of classifier
performance using as little labeling resource as possible. Specifically, we try
to answer, how to select a subset of the large test set for labeling such that
the performance of a classifier estimated on this subset is as close as
possible to the one on the whole test set. We propose strategies based on
stratified sampling for selecting this subset. We show that these strategies
can reduce the variance in estimation of classifier accuracy by a significant
amount compared to simple random sampling (over 65% in several cases). Hence,
our proposed methods are much more precise compared to random sampling for
accuracy estimation under restricted labeling resources. The reduction in
number of samples required (compared to random sampling) to estimate the
classifier accuracy with only 1% error is high as 60% in some cases.","['Anurag Kumar', 'Bhiksha Raj']","['cs.LG', 'stat.AP', 'stat.ML']",2016-07-09 21:18:23+00:00
http://arxiv.org/abs/1607.02654v2,Combining multiple resolutions into hierarchical representations for kernel-based image classification,"Geographic object-based image analysis (GEOBIA) framework has gained
increasing interest recently. Following this popular paradigm, we propose a
novel multiscale classification approach operating on a hierarchical image
representation built from two images at different resolutions. They capture the
same scene with different sensors and are naturally fused together through the
hierarchical representation, where coarser levels are built from a Low Spatial
Resolution (LSR) or Medium Spatial Resolution (MSR) image while finer levels
are generated from a High Spatial Resolution (HSR) or Very High Spatial
Resolution (VHSR) image. Such a representation allows one to benefit from the
context information thanks to the coarser levels, and subregions spatial
arrangement information thanks to the finer levels. Two dedicated structured
kernels are then used to perform machine learning directly on the constructed
hierarchical representation. This strategy overcomes the limits of conventional
GEOBIA classification procedures that can handle only one or very few
pre-selected scales. Experiments run on an urban classification task show that
the proposed approach can highly improve the classification accuracy w.r.t.
conventional approaches working on a single scale.","['Yanwei Cui', 'Sébastien Lefevre', 'Laetitia Chapel', 'Anne Puissant']","['cs.CV', 'stat.ML']",2016-07-09 20:07:37+00:00
http://arxiv.org/abs/1607.02624v1,Beating level-set methods for 3D seismic data interpolation: a primal-dual alternating approach,"Acquisition cost is a crucial bottleneck for seismic workflows, and low-rank
formulations for data interpolation allow practitioners to `fill in' data
volumes from critically subsampled data acquired in the field. Tremendous size
of seismic data volumes required for seismic processing remains a major
challenge for these techniques.
  We propose a new approach to solve residual constrained formulations for
interpolation. We represent the data volume using matrix factors, and build a
block-coordinate algorithm with constrained convex subproblems that are solved
with a primal-dual splitting scheme. The new approach is competitive with state
of the art level-set algorithms that interchange the role of objectives with
constraints. We use the new algorithm to successfully interpolate a large scale
5D seismic data volume, generated from the geologically complex synthetic 3D
Compass velocity model, where 80% of the data has been removed.","['Rajiv Kumar', 'Oscar López', 'Damek Davis', 'Aleksandr Y. Aravkin', 'Felix J. Herrmann']","['math.OC', 'stat.ML', '62F35, 65K10']",2016-07-09 15:38:22+00:00
http://arxiv.org/abs/1607.02533v4,Adversarial examples in the physical world,"Most existing machine learning classifiers are highly vulnerable to
adversarial examples. An adversarial example is a sample of input data which
has been modified very slightly in a way that is intended to cause a machine
learning classifier to misclassify it. In many cases, these modifications can
be so subtle that a human observer does not even notice the modification at
all, yet the classifier still makes a mistake. Adversarial examples pose
security concerns because they could be used to perform an attack on machine
learning systems, even if the adversary has no access to the underlying model.
Up to now, all previous work have assumed a threat model in which the adversary
can feed data directly into the machine learning classifier. This is not always
the case for systems operating in the physical world, for example those which
are using signals from cameras and other sensors as an input. This paper shows
that even in such physical world scenarios, machine learning systems are
vulnerable to adversarial examples. We demonstrate this by feeding adversarial
images obtained from cell-phone camera to an ImageNet Inception classifier and
measuring the classification accuracy of the system. We find that a large
fraction of adversarial examples are classified incorrectly even when perceived
through the camera.","['Alexey Kurakin', 'Ian Goodfellow', 'Samy Bengio']","['cs.CV', 'cs.CR', 'cs.LG', 'stat.ML']",2016-07-08 21:12:11+00:00
http://arxiv.org/abs/1607.02531v2,Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016),"This is the Proceedings of the 2016 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.
  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,
and Hanna Wallach.","['Been Kim', 'Dmitry M. Malioutov', 'Kush R. Varshney']","['stat.ML', 'cs.LG']",2016-07-08 21:07:54+00:00
http://arxiv.org/abs/1607.02516v2,Pseudo-Marginal Hamiltonian Monte Carlo,"Bayesian inference in the presence of an intractable likelihood function is
computationally challenging. When following a Markov chain Monte Carlo (MCMC)
approach to approximate the posterior distribution in this context, one
typically either uses MCMC schemes which target the joint posterior of the
parameters and some auxiliary latent variables, or pseudo-marginal
Metropolis--Hastings (MH) schemes. The latter mimic a MH algorithm targeting
the marginal posterior of the parameters by approximating unbiasedly the
intractable likelihood. However, in scenarios where the parameters and
auxiliary variables are strongly correlated under the posterior and/or this
posterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will
perform poorly and the pseudo-marginal MH algorithm, as any other MH scheme,
will be inefficient for high dimensional parameters. We propose here an
original MCMC algorithm, termed pseudo-marginal HMC, which combines the
advantages of both HMC and pseudo-marginal schemes. Specifically, the
pseudo-marginal HMC method is controlled by a precision parameter N,
controlling the approximation of the likelihood and, for any N, it samples the
marginal posterior of the parameters. Additionally, as N tends to infinity, its
sample trajectories and acceptance probability converge to those of an ideal,
but intractable, HMC algorithm which would have access to the marginal
posterior of parameters and its gradient. We demonstrate through experiments
that pseudo-marginal HMC can outperform significantly both standard HMC and
pseudo-marginal MH schemes.","['Johan Alenlöv', 'Arnaud Doucet', 'Fredrik Lindsten']","['stat.ME', 'stat.ML']",2016-07-08 20:06:43+00:00
http://arxiv.org/abs/1607.02450v2,Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications,"This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning
in Social Good Applications, which was held on June 24, 2016 in New York.",['Kush R. Varshney'],"['stat.ML', 'cs.CY', 'cs.LG']",2016-07-08 16:55:31+00:00
http://arxiv.org/abs/1607.02435v3,Optimal Rates of Statistical Seriation,"Given a matrix the seriation problem consists in permuting its rows in such
way that all its columns have the same shape, for example, they are monotone
increasing. We propose a statistical approach to this problem where the matrix
of interest is observed with noise and study the corresponding minimax rate of
estimation of the matrices. Specifically, when the columns are either unimodal
or monotone, we show that the least squares estimator is optimal up to
logarithmic factors and adapts to matrices with a certain natural structure.
Finally, we propose a computationally efficient estimator in the monotonic case
and study its performance both theoretically and experimentally. Our work is at
the intersection of shape constrained estimation and recent work that involves
permutation learning, such as graph denoising and ranking.","['Nicolas Flammarion', 'Cheng Mao', 'Philippe Rigollet']","['math.ST', 'stat.ML', 'stat.TH', '62G08']",2016-07-08 16:12:47+00:00
http://arxiv.org/abs/1607.02413v2,Lower Bounds on Active Learning for Graphical Model Selection,"We consider the problem of estimating the underlying graph associated with a
Markov random field, with the added twist that the decoding algorithm can
iteratively choose which subsets of nodes to sample based on the previous
samples, resulting in an active learning setting. Considering both Ising and
Gaussian models, we provide algorithm-independent lower bounds for
high-probability recovery within the class of degree-bounded graphs. Our main
results are minimax lower bounds for the active setting that match the best
known lower bounds for the passive setting, which in turn are known to be tight
in several cases of interest. Our analysis is based on Fano's inequality, along
with novel mutual information bounds for the active learning setting, and the
application of restricted graph ensembles. While we consider ensembles that are
similar or identical to those used in the passive setting, we require different
analysis techniques, with a key challenge being bounding a mutual information
quantity associated with observed subsets of nodes, as opposed to full
observations.","['Jonathan Scarlett', 'Volkan Cevher']","['cs.IT', 'cs.LG', 'cs.SI', 'math.IT', 'stat.ML']",2016-07-08 15:25:46+00:00
http://arxiv.org/abs/1607.02387v1,Convergence rates of Kernel Conjugate Gradient for random design regression,"We prove statistical rates of convergence for kernel-based least squares
regression from i.i.d. data using a conjugate gradient algorithm, where
regularization against overfitting is obtained by early stopping. This method
is related to Kernel Partial Least Squares, a regression method that combines
supervised dimensionality reduction with least squares projection. Following
the setting introduced in earlier related literature, we study so-called ""fast
convergence rates"" depending on the regularity of the target regression
function (measured by a source condition in terms of the kernel integral
operator) and on the effective dimensionality of the data mapped into the
kernel space. We obtain upper bounds, essentially matching known minimax lower
bounds, for the $\mathcal{L}^2$ (prediction) norm as well as for the stronger
Hilbert norm, if the true regression function belongs to the reproducing kernel
Hilbert space. If the latter assumption is not fulfilled, we obtain similar
convergence rates for appropriate norms, provided additional unlabeled data are
available.","['Gilles Blanchard', 'Nicole Krämer']","['math.ST', 'stat.ML', 'stat.TH']",2016-07-08 14:41:17+00:00
http://arxiv.org/abs/1607.02188v2,Whole-brain substitute CT generation using Markov random field mixture models,"Computed tomography (CT) equivalent information is needed for attenuation
correction in PET imaging and for dose planning in radiotherapy. Prior work has
shown that Gaussian mixture models can be used to generate a substitute CT
(s-CT) image from a specific set of MRI modalities. This work introduces a more
flexible class of mixture models for s-CT generation, that incorporates spatial
dependency in the data through a Markov random field prior on the latent field
of class memberships associated with a mixture model. Furthermore, the mixture
distributions are extended from Gaussian to normal inverse Gaussian (NIG),
allowing heavier tails and skewness. The amount of data needed to train a model
for s-CT generation is of the order of 100 million voxels. The computational
efficiency of the parameter estimation and prediction methods are hence
paramount, especially when spatial dependency is included in the models. A
stochastic Expectation Maximization (EM) gradient algorithm is proposed in
order to tackle this challenge. The advantages of the spatial model and NIG
distributions are evaluated with a cross-validation study based on data from 14
patients. The study show that the proposed model enhances the predictive
quality of the s-CT images by reducing the mean absolute error with 17.9%.
Also, the distribution of CT values conditioned on the MR images are better
explained by the proposed model as evaluated using continuous ranked
probability scores.","['Anders Hildeman', 'David Bolin', 'Jonas Wallin', 'Adam Johansson', 'Tufve Nyholm', 'Thomas Asklund', 'Jun Yu']","['stat.AP', 'stat.CO', 'stat.ML']",2016-07-07 22:52:36+00:00
http://arxiv.org/abs/1607.02173v1,Single-Channel Multi-Speaker Separation using Deep Clustering,"Deep clustering is a recently introduced deep learning architecture that uses
discriminatively trained embeddings as the basis for clustering. It was
recently applied to spectrogram segmentation, resulting in impressive results
on speaker-independent multi-speaker separation. In this paper we extend the
baseline system with an end-to-end signal approximation objective that greatly
improves performance on a challenging speech separation. We first significantly
improve upon the baseline system performance by incorporating better
regularization, larger temporal context, and a deeper architecture, culminating
in an overall improvement in signal to distortion ratio (SDR) of 10.3 dB
compared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1
dB SDR improvement for three-speaker separation. We then extend the model to
incorporate an enhancement layer to refine the signal estimates, and perform
end-to-end training through both the clustering and enhancement stages to
maximize signal fidelity. We evaluate the results using automatic speech
recognition. The new signal approximation objective, combined with end-to-end
training, produces unprecedented performance, reducing the word error rate
(WER) from 89.1% down to 30.8%. This represents a major advancement towards
solving the cocktail party problem.","['Yusuf Isik', 'Jonathan Le Roux', 'Zhuo Chen', 'Shinji Watanabe', 'John R. Hershey']","['cs.LG', 'cs.SD', 'stat.ML']",2016-07-07 21:06:48+00:00
http://arxiv.org/abs/1607.02109v2,Predicting and Understanding Law-Making with Word Vectors and an Ensemble Model,"Out of nearly 70,000 bills introduced in the U.S. Congress from 2001 to 2015,
only 2,513 were enacted. We developed a machine learning approach to
forecasting the probability that any bill will become law. Starting in 2001
with the 107th Congress, we trained models on data from previous Congresses,
predicted all bills in the current Congress, and repeated until the 113th
Congress served as the test. For prediction we scored each sentence of a bill
with a language model that embeds legislative vocabulary into a
high-dimensional, semantic-laden vector space. This language representation
enables our investigation into which words increase the probability of
enactment for any topic. To test the relative importance of text and context,
we compared the text model to a context-only model that uses variables such as
whether the bill's sponsor is in the majority party. To test the effect of
changes to bills after their introduction on our ability to predict their final
outcome, we compared using the bill text and meta-data available at the time of
introduction with using the most recent data. At the time of introduction
context-only predictions outperform text-only, and with the newest data
text-only outperforms context-only. Combining text and context always performs
best. We conducted a global sensitivity analysis on the combined model to
determine important variables predicting enactment.",['John J. Nay'],"['cs.CL', 'physics.soc-ph', 'stat.AP', 'stat.ML']",2016-07-07 18:08:59+00:00
http://arxiv.org/abs/1607.02085v1,A Classification Framework for Partially Observed Dynamical Systems,"We present a general framework for classifying partially observed dynamical
systems based on the idea of learning in the model space. In contrast to the
existing approaches using model point estimates to represent individual data
items, we employ posterior distributions over models, thus taking into account
in a principled manner the uncertainty due to both the generative
(observational and/or dynamic noise) and observation (sampling in time)
processes. We evaluate the framework on two testbeds - a biological pathway
model and a stochastic double-well system. Crucially, we show that the
classifier performance is not impaired when the model class used for inferring
posterior distributions is much more simple than the observation-generating
model class, provided the reduced complexity inferential model class captures
the essential characteristics needed for the given classification task.","['Yuan Shen', 'Peter Tino', 'Krasimira Tsaneva-Atanasova']",['stat.ML'],2016-07-07 17:17:04+00:00
http://arxiv.org/abs/1607.02066v1,A characterization of product-form exchangeable feature probability functions,"We characterize the class of exchangeable feature allocations assigning
probability $V_{n,k}\prod_{l=1}^{k}W_{m_{l}}U_{n-m_{l}}$ to a feature
allocation of $n$ individuals, displaying $k$ features with counts
$(m_{1},\ldots,m_{k})$ for these features. Each element of this class is
parametrized by a countable matrix $V$ and two sequences $U$ and $W$ of
non-negative weights. Moreover, a consistency condition is imposed to guarantee
that the distribution for feature allocations of $n-1$ individuals is recovered
from that of $n$ individuals, when the last individual is integrated out. In
Theorem 1.1, we prove that the only members of this class satisfying the
consistency condition are mixtures of the Indian Buffet Process over its mass
parameter $\gamma$ and mixtures of the Beta--Bernoulli model over its
dimensionality parameter $N$. Hence, we provide a characterization of these two
models as the only, up to randomization of the parameters, consistent
exchangeable feature allocations having the required product form.","['Marco Battiston', 'Stefano Favaro', 'Daniel M. Roy', 'Yee Whye Teh']","['math.PR', 'math.ST', 'stat.ML', 'stat.TH', '60K99, 60C99']",2016-07-07 16:07:09+00:00
http://arxiv.org/abs/1607.02024v2,Mini-Batch Spectral Clustering,"The cost of computing the spectrum of Laplacian matrices hinders the
application of spectral clustering to large data sets. While approximations
recover computational tractability, they can potentially affect clustering
performance. This paper proposes a practical approach to learn spectral
clustering based on adaptive stochastic gradient optimization. Crucially, the
proposed approach recovers the exact spectrum of Laplacian matrices in the
limit of the iterations, and the cost of each iteration is linear in the number
of samples. Extensive experimental validation on data sets with up to half a
million samples demonstrate its scalability and its ability to outperform
state-of-the-art approximate methods to learn spectral clustering for a given
computational budget.","['Yufei Han', 'Maurizio Filippone']","['stat.ML', 'cs.LG']",2016-07-07 14:06:06+00:00
http://arxiv.org/abs/1607.02011v2,Kernel Bayesian Inference with Posterior Regularization,"We propose a vector-valued regression problem whose solution is equivalent to
the reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior
distribution. This equivalence provides a new understanding of kernel Bayesian
inference. Moreover, the optimization problem induces a new regularization for
the posterior embedding estimator, which is faster and has comparable
performance to the squared regularization in kernel Bayes' rule. This
regularization coincides with a former thresholding approach used in kernel
POMDPs whose consistency remains to be established. Our theoretical work solves
this open problem and provides consistency analysis in regression settings.
Based on our optimizational formulation, we propose a flexible Bayesian
posterior regularization framework which for the first time enables us to put
regularization at the distribution level. We apply this method to nonparametric
state-space filtering tasks with extremely nonlinear dynamics and show
performance gains over all other baselines.","['Yang Song', 'Jun Zhu', 'Yong Ren']",['stat.ML'],2016-07-07 13:44:44+00:00
http://arxiv.org/abs/1607.01981v2,Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent,"We present a unifying framework for adapting the update direction in
gradient-based iterative optimization methods. As natural special cases we
re-derive classical momentum and Nesterov's accelerated gradient method,
lending a new intuitive interpretation to the latter algorithm. We show that a
new algorithm, which we term Regularised Gradient Descent, can converge more
quickly than either Nesterov's algorithm or the classical momentum algorithm.","['Aleksandar Botev', 'Guy Lever', 'David Barber']","['stat.ML', 'cs.LG']",2016-07-07 12:12:11+00:00
