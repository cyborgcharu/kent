id,title,abstract,authors,categories,date
http://arxiv.org/abs/2303.11454v1,How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer,"Randomized neural networks (randomized NNs), where only the terminal layer's
weights are optimized constitute a powerful model class to reduce computational
time in training the neural network model. At the same time, these models
generalize surprisingly well in various regression and classification tasks. In
this paper, we give an exact macroscopic characterization (i.e., a
characterization in function space) of the generalization behavior of
randomized, shallow NNs with ReLU activation (RSNs). We show that RSNs
correspond to a generalized additive model (GAM)-typed regression in which
infinitely many directions are considered: the infinite generalized additive
model (IGAM). The IGAM is formalized as solution to an optimization problem in
function space for a specific regularization functional and a fairly general
loss. This work is an extension to multivariate NNs of prior work, where we
showed how wide RSNs with ReLU activation behave like spline regression under
certain conditions and if the input is one-dimensional.","['Jakob Heiss', 'Josef Teichmann', 'Hanna Wutte']","['cs.LG', 'stat.ML']",2023-03-20 21:05:47+00:00
http://arxiv.org/abs/2303.11453v2,Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing,"Pruning schemes have been widely used in practice to reduce the complexity of
trained models with a massive number of parameters. In fact, several practical
studies have shown that if a pruned model is fine-tuned with some
gradient-based updates it generalizes well to new samples. Although the above
pipeline, which we refer to as pruning + fine-tuning, has been extremely
successful in lowering the complexity of trained models, there is very little
known about the theory behind this success. In this paper, we address this
issue by investigating the pruning + fine-tuning framework on the
overparameterized matrix sensing problem with the ground truth $U_\star \in
\mathbb{R}^{d \times r}$ and the overparameterized model $U \in \mathbb{R}^{d
\times k}$ with $k \gg r$. We study the approximate local minima of the mean
square error, augmented with a smooth version of a group Lasso regularizer,
$\sum_{i=1}^k \| U e_i \|_2$. In particular, we provably show that pruning all
the columns below a certain explicit $\ell_2$-norm threshold results in a
solution $U_{\text{prune}}$ which has the minimum number of columns $r$, yet
close to the ground truth in training loss. Moreover, in the subsequent
fine-tuning phase, gradient descent initialized at $U_{\text{prune}}$ converges
at a linear rate to its limit. While our analysis provides insights into the
role of regularization in pruning, we also show that running gradient descent
in the absence of regularization results in models which {are not suitable for
greedy pruning}, i.e., many columns could have their $\ell_2$ norm comparable
to that of the maximum. To the best of our knowledge, our results provide the
first rigorous insights on why greedy pruning + fine-tuning leads to smaller
models which also generalize well.","['Nived Rajaraman', 'Devvrit', 'Aryan Mokhtari', 'Kannan Ramchandran']","['cs.LG', 'stat.ML']",2023-03-20 21:05:44+00:00
http://arxiv.org/abs/2303.11448v1,Geometrical aspects of lattice gauge equivariant convolutional neural networks,"Lattice gauge equivariant convolutional neural networks (L-CNNs) are a
framework for convolutional neural networks that can be applied to non-Abelian
lattice gauge theories without violating gauge symmetry. We demonstrate how
L-CNNs can be equipped with global group equivariance. This allows us to extend
the formulation to be equivariant not just under translations but under global
lattice symmetries such as rotations and reflections. Additionally, we provide
a geometric formulation of L-CNNs and show how convolutions in L-CNNs arise as
a special case of gauge equivariant neural networks on SU($N$) principal
bundles.","['Jimmy Aronsson', 'David I. MÃ¼ller', 'Daniel Schuh']","['hep-lat', 'cs.LG', 'stat.ML']",2023-03-20 20:49:08+00:00
http://arxiv.org/abs/2303.11379v1,Solving High-Dimensional Inverse Problems with Auxiliary Uncertainty via Operator Learning with Limited Data,"In complex large-scale systems such as climate, important effects are caused
by a combination of confounding processes that are not fully observable. The
identification of sources from observations of system state is vital for
attribution and prediction, which inform critical policy decisions. The
difficulty of these types of inverse problems lies in the inability to isolate
sources and the cost of simulating computational models. Surrogate models may
enable the many-query algorithms required for source identification, but data
challenges arise from high dimensionality of the state and source, limited
ensembles of costly model simulations to train a surrogate model, and few and
potentially noisy state observations for inversion due to measurement
limitations. The influence of auxiliary processes adds an additional layer of
uncertainty that further confounds source identification. We introduce a
framework based on (1) calibrating deep neural network surrogates to the flow
maps provided by an ensemble of simulations obtained by varying sources, and
(2) using these surrogates in a Bayesian framework to identify sources from
observations via optimization. Focusing on an atmospheric dispersion exemplar,
we find that the expressive and computationally efficient nature of the deep
neural network operator surrogates in appropriately reduced dimension allows
for source identification with uncertainty quantification using limited data.
Introducing a variable wind field as an auxiliary process, we find that a
Bayesian approximation error approach is essential for reliable source
inversion when uncertainty due to wind stresses the algorithm.","['Joseph Hart', 'Mamikon Gulian', 'Indu Manickam', 'Laura Swiler']","['stat.ML', 'cs.LG', 'math.OC']",2023-03-20 18:29:23+00:00
http://arxiv.org/abs/2303.11233v1,Sparse Recovery with Shuffled Labels: Statistical Limits and Practical Estimators,"This paper considers the sparse recovery with shuffled labels, i.e., $\by =
\bPitrue \bX \bbetatrue + \bw$, where $\by \in \RR^n$, $\bPi\in \RR^{n\times
n}$, $\bX\in \RR^{n\times p}$, $\bbetatrue\in \RR^p$, $\bw \in \RR^n$ denote
the sensing result, the unknown permutation matrix, the design matrix, the
sparse signal, and the additive noise, respectively. Our goal is to reconstruct
both the permutation matrix $\bPitrue$ and the sparse signal $\bbetatrue$. We
investigate this problem from both the statistical and computational aspects.
From the statistical aspect, we first establish the minimax lower bounds on the
sample number $n$ and the \emph{signal-to-noise ratio} ($\snr$) for the correct
recovery of permutation matrix $\bPitrue$ and the support set
$\supp(\bbetatrue)$, to be more specific, $n \gtrsim k\log p$ and $\log\snr
\gtrsim \log n + \frac{k\log p}{n}$. Then, we confirm the tightness of these
minimax lower bounds by presenting an exhaustive-search based estimator whose
performance matches the lower bounds thereof up to some multiplicative
constants. From the computational aspect, we impose a parsimonious assumption
on the number of permuted rows and propose a computationally-efficient
estimator accordingly. Moreover, we show that our proposed estimator can obtain
the ground-truth $(\bPitrue, \supp(\bbetatrue))$ under mild conditions.
Furthermore, we provide numerical experiments to corroborate our claims.","['Hang Zhang', 'Ping Li']","['cs.IT', 'math.IT', 'stat.ML']",2023-03-20 16:14:58+00:00
http://arxiv.org/abs/2303.11187v1,A Unified Framework of Policy Learning for Contextual Bandit with Confounding Bias and Missing Observations,"We study the offline contextual bandit problem, where we aim to acquire an
optimal policy using observational data. However, this data usually contains
two deficiencies: (i) some variables that confound actions are not observed,
and (ii) missing observations exist in the collected data. Unobserved
confounders lead to a confounding bias and missing observations cause bias and
inefficiency problems. To overcome these challenges and learn the optimal
policy from the observed dataset, we present a new algorithm called
Causal-Adjusted Pessimistic (CAP) policy learning, which forms the reward
function as the solution of an integral equation system, builds a confidence
set, and greedily takes action with pessimism. With mild assumptions on the
data, we develop an upper bound to the suboptimality of CAP for the offline
contextual bandit problem.","['Siyu Chen', 'Yitan Wang', 'Zhaoran Wang', 'Zhuoran Yang']","['cs.LG', 'cs.AI', 'stat.ML']",2023-03-20 15:17:31+00:00
http://arxiv.org/abs/2303.11177v1,Integration of Radiomics and Tumor Biomarkers in Interpretable Machine Learning Models,"Despite the unprecedented performance of deep neural networks (DNNs) in
computer vision, their practical application in the diagnosis and prognosis of
cancer using medical imaging has been limited. One of the critical challenges
for integrating diagnostic DNNs into radiological and oncological applications
is their lack of interpretability, preventing clinicians from understanding the
model predictions. Therefore, we study and propose the integration of
expert-derived radiomics and DNN-predicted biomarkers in interpretable
classifiers which we call ConRad, for computerized tomography (CT) scans of
lung cancer. Importantly, the tumor biomarkers are predicted from a concept
bottleneck model (CBM) such that once trained, our ConRad models do not require
labor-intensive and time-consuming biomarkers. In our evaluation and practical
application, the only input to ConRad is a segmented CT scan. The proposed
model is compared to convolutional neural networks (CNNs) which act as a black
box classifier. We further investigated and evaluated all combinations of
radiomics, predicted biomarkers and CNN features in five different classifiers.
We found the ConRad models using non-linear SVM and the logistic regression
with the Lasso outperform others in five-fold cross-validation, although we
highlight that interpretability of ConRad is its primary advantage. The Lasso
is used for feature selection, which substantially reduces the number of
non-zero weights while increasing the accuracy. Overall, the proposed ConRad
model combines CBM-derived biomarkers and radiomics features in an
interpretable ML model which perform excellently for the lung nodule malignancy
classification.","['Lennart Brocki', 'Neo Christopher Chung']","['cs.LG', 'cs.CV', 'stat.AP', 'stat.ML']",2023-03-20 15:00:52+00:00
http://arxiv.org/abs/2303.11155v1,An ADMM approach for multi-response regression with overlapping groups and interaction effects,"In this paper, we consider the regularized multi-response regression problem
where there exists some structural relation within the responses and also
between the covariates and a set of modifying variables. To handle this
problem, we propose MADMMplasso, a novel regularized regression method. This
method is able to find covariates and their corresponding interactions, with
some joint association with multiple related responses. We allow the
interaction term between covariate and modifying variable to be included in a
(weak) asymmetrical hierarchical manner by first considering whether the
corresponding covariate main term is in the model. For parameter estimation, we
develop an ADMM algorithm that allows us to implement the overlapping groups in
a simple way. The results from the simulations and analysis of a
pharmacogenomic screen data set show that the proposed method has an advantage
in handling correlated responses and interaction effects, both with respect to
prediction and variable selection performance.","['Theophilus Quachie Asenso', 'Manuela Zucknick']","['stat.ME', 'q-bio.QM', 'stat.ML']",2023-03-20 14:43:22+00:00
http://arxiv.org/abs/2303.11138v2,Fault Detection via Occupation Kernel Principal Component Analysis,"The reliable operation of automatic systems is heavily dependent on the
ability to detect faults in the underlying dynamical system. While traditional
model-based methods have been widely used for fault detection, data-driven
approaches have garnered increasing attention due to their ease of deployment
and minimal need for expert knowledge. In this paper, we present a novel
principal component analysis (PCA) method that uses occupation kernels.
Occupation kernels result in feature maps that are tailored to the measured
data, have inherent noise-robustness due to the use of integration, and can
utilize irregularly sampled system trajectories of variable lengths for PCA.
The occupation kernel PCA method is used to develop a reconstruction error
approach to fault detection and its efficacy is validated using numerical
simulations.","['Zachary Morrison', 'Benjamin P. Russo', 'Yingzhao Lian', 'Rushikesh Kamalapurkar']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY', 'math.OC']",2023-03-20 14:15:52+00:00
http://arxiv.org/abs/2303.11835v2,Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian,"We establish a layer-wise parameterization for 1D convolutional neural
networks (CNNs) with built-in end-to-end robustness guarantees. In doing so, we
use the Lipschitz constant of the input-output mapping characterized by a CNN
as a robustness measure. We base our parameterization on the Cayley transform
that parameterizes orthogonal matrices and the controllability Gramian of the
state space representation of the convolutional layers. The proposed
parameterization by design fulfills linear matrix inequalities that are
sufficient for Lipschitz continuity of the CNN, which further enables
unconstrained training of Lipschitz-bounded 1D CNNs. Finally, we train
Lipschitz-bounded 1D CNNs for the classification of heart arrythmia data and
show their improved robustness.","['Patricia Pauli', 'Ruigang Wang', 'Ian R. Manchester', 'Frank AllgÃ¶wer']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2023-03-20 12:25:43+00:00
http://arxiv.org/abs/2303.11060v1,Quantile and moment neural networks for learning functionals of distributions,"We study news neural networks to approximate function of distributions in a
probability space. Two classes of neural networks based on quantile and moment
approximation are proposed to learn these functions and are theoretically
supported by universal approximation theorems. By mixing the quantile and
moment features in other new networks, we develop schemes that outperform
existing networks on numerical test cases involving univariate distributions.
For bivariate distributions, the moment neural network outperforms all other
networks.",['Xavier Warin'],"['stat.ML', 'cs.LG', '68T07']",2023-03-20 12:23:31+00:00
http://arxiv.org/abs/2303.10931v2,Approaching an unknown communication system by latent space exploration and causal inference,"This paper proposes a methodology for discovering meaningful properties in
data by exploring the latent space of unsupervised deep generative models. We
combine manipulation of individual latent variables to extreme values with
methods inspired by causal inference into an approach we call causal
disentanglement with extreme values (CDEV) and show that this method yields
insights for model interpretability. With this, we can test for what properties
of unknown data the model encodes as meaningful, using it to glean insight into
the communication system of sperm whales (Physeter macrocephalus), one of the
most intriguing and understudied animal communication systems. The network
architecture used has been shown to learn meaningful representations of speech;
here, it is used as a learning mechanism to decipher the properties of another
vocal communication system in which case we have no ground truth. The proposed
methodology suggests that sperm whales encode information using the number of
clicks in a sequence, the regularity of their timing, and audio properties such
as the spectral mean and the acoustic regularity of the sequences. Some of
these findings are consistent with existing hypotheses, while others are
proposed for the first time. We also argue that our models uncover rules that
govern the structure of units in the communication system and apply them while
generating innovative data not shown during training. This paper suggests that
an interpretation of the outputs of deep neural networks with causal inference
methodology can be a viable strategy for approaching data about which little is
known and presents another case of how deep learning can limit the hypothesis
space. Finally, the proposed approach can be extended to other architectures
and datasets.","['GaÅ¡per BeguÅ¡', 'Andrej Leban', 'Shane Gero']","['stat.ML', 'cs.LG', 'cs.SD', 'eess.AS']",2023-03-20 08:09:13+00:00
http://arxiv.org/abs/2303.10859v1,Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs,"In reward-free reinforcement learning (RL), an agent explores the environment
first without any reward information, in order to achieve certain learning
goals afterwards for any given reward. In this paper we focus on reward-free RL
under low-rank MDP models, in which both the representation and linear weight
vectors are unknown. Although various algorithms have been proposed for
reward-free low-rank MDPs, the corresponding sample complexity is still far
from being satisfactory. In this work, we first provide the first known sample
complexity lower bound that holds for any algorithm under low-rank MDPs. This
lower bound implies it is strictly harder to find a near-optimal policy under
low-rank MDPs than under linear MDPs. We then propose a novel model-based
algorithm, coined RAFFLE, and show it can both find an $\epsilon$-optimal
policy and achieve an $\epsilon$-accurate system identification via reward-free
exploration, with a sample complexity significantly improving the previous
results. Such a sample complexity matches our lower bound in the dependence on
$\epsilon$, as well as on $K$ in the large $d$ regime, where $d$ and $K$
respectively denote the representation dimension and action space cardinality.
Finally, we provide a planning algorithm (without further interaction with true
environment) for RAFFLE to learn a near-accurate representation, which is the
first known representation learning guarantee under the same setting.","['Yuan Cheng', 'Ruiquan Huang', 'Jing Yang', 'Yingbin Liang']","['cs.LG', 'stat.ML']",2023-03-20 04:39:39+00:00
http://arxiv.org/abs/2303.11958v1,Formulation of Weighted Average Smoothing as a Projection of the Origin onto a Convex Polytope,"Our study focuses on determining the best weight windows for a weighted
moving average smoother under squared loss. We show that there exists an
optimal weight window that is symmetrical around its center. We study the class
of tapered weight windows, which decrease in weight as they move away from the
center. We formulate the corresponding least squares problem as a quadratic
program and finally as a projection of the origin onto a convex polytope.
Additionally, we provide some analytical solutions to the best window when some
conditions are met on the input data.","['Kaan Gokcesu', 'Hakan Gokcesu']","['cs.LG', 'eess.SP', 'math.OC', 'stat.ML']",2023-03-19 22:40:39+00:00
http://arxiv.org/abs/2303.11786v2,Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure,"We introduce a new regression framework designed to deal with large-scale,
complex data that lies around a low-dimensional manifold with noises. Our
approach first constructs a graph representation, referred to as the skeleton,
to capture the underlying geometric structure. We then define metrics on the
skeleton graph and apply nonparametric regression techniques, along with
feature transformations based on the graph, to estimate the regression
function. We also discuss the limitations of some nonparametric regressors with
respect to the general metric space such as the skeleton graph. The proposed
regression framework suggests a novel way to deal with data with underlying
geometric structures and provides additional advantages in handling the union
of multiple manifolds, additive noises, and noisy observations. We provide
statistical guarantees for the proposed method and demonstrate its
effectiveness through simulations and real data examples.","['Zeyu Wei', 'Yen-Chi Chen']","['cs.LG', 'stat.ME', 'stat.ML']",2023-03-19 21:45:40+00:00
http://arxiv.org/abs/2303.10761v1,Calibration of Neural Networks,"Neural networks solving real-world problems are often required not only to
make accurate predictions but also to provide a confidence level in the
forecast. The calibration of a model indicates how close the estimated
confidence is to the true probability. This paper presents a survey of
confidence calibration problems in the context of neural networks and provides
an empirical comparison of calibration methods. We analyze problem statement,
calibration definitions, and different approaches to evaluation: visualizations
and scalar measures that estimate whether the model is well-calibrated. We
review modern calibration techniques: based on post-processing or requiring
changes in training. Empirical experiments cover various datasets and models,
comparing calibration methods according to different criteria.","['Ruslan Vasilev', ""Alexander D'yakonov""]","['cs.NE', 'cs.AI', 'cs.LG', 'stat.ML']",2023-03-19 20:27:51+00:00
http://arxiv.org/abs/2303.10758v2,Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization,"This work studies the generalization error of gradient methods. More
specifically, we focus on how training steps $T$ and step-size $\eta$ might
affect generalization in smooth stochastic convex optimization (SCO) problems.
We first provide tight excess risk lower bounds for Gradient Descent (GD) and
Stochastic Gradient Descent (SGD) under the general non-realizable smooth SCO
setting, suggesting that existing stability analyses are tight in step-size and
iteration dependence, and that overfitting provably happens. Next, we study the
case when the loss is realizable, i.e. an optimal solution minimizes all the
data points. Recent works show better rates can be attained but the improvement
is reduced when training time is long. Our paper examines this observation by
providing excess risk lower bounds for GD and SGD in two realizable settings:
1) $\eta T = \bigO{n}$, and (2) $\eta T = \bigOmega{n}$, where $n$ is the size
of dataset. In the first case $\eta T = \bigOmega{n}$, our lower bounds tightly
match and certify the respective upper bounds. However, for the case $\eta T =
\bigOmega{n}$, our analysis indicates a gap between the lower and upper bounds.
A conjecture is proposed that the gap can be closed by improving upper bounds,
supported by analyses in two special scenarios.","['Peiyuan Zhang', 'Jiaye Teng', 'Jingzhao Zhang']","['cs.LG', 'math.OC', 'stat.ML']",2023-03-19 20:24:33+00:00
http://arxiv.org/abs/2303.10712v3,Mixture of segmentation for heterogeneous functional data,"In this paper we consider functional data with heterogeneity in time and in
population. We propose a mixture model with segmentation of time to represent
this heterogeneity while keeping the functional structure. Maximum likelihood
estimator is considered, proved to be identifiable and consistent. In practice,
an EM algorithm is used, combined with dynamic programming for the maximization
step, to approximate the maximum likelihood estimator. The method is
illustrated on a simulated dataset, and used on a real dataset of electricity
consumption.","['Vincent Brault', 'Ãmilie Devijver', 'Charlotte Laclau']","['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML', '62M10, 62F12, 62-08', 'G.3']",2023-03-19 16:43:17+00:00
http://arxiv.org/abs/2303.10694v1,Improving Uncertainty Quantification of Deep Classifiers via Neighborhood Conformal Prediction: Novel Algorithm and Theoretical Analysis,"Safe deployment of deep neural networks in high-stake real-world applications
requires theoretically sound uncertainty quantification. Conformal prediction
(CP) is a principled framework for uncertainty quantification of deep models in
the form of prediction set for classification tasks with a user-specified
coverage (i.e., true class label is contained with high probability). This
paper proposes a novel algorithm referred to as Neighborhood Conformal
Prediction (NCP) to improve the efficiency of uncertainty quantification from
CP for deep classifiers (i.e., reduce prediction set size). The key idea behind
NCP is to use the learned representation of the neural network to identify k
nearest-neighbors calibration examples for a given testing input and assign
them importance weights proportional to their distance to create adaptive
prediction sets. We theoretically show that if the learned data representation
of the neural network satisfies some mild conditions, NCP will produce smaller
prediction sets than traditional CP algorithms. Our comprehensive experiments
on CIFAR-10, CIFAR-100, and ImageNet datasets using diverse deep neural
networks strongly demonstrate that NCP leads to significant reduction in
prediction set size over prior CP methods.","['Subhankar Ghosh', 'Taha Belkhouja', 'Yan Yan', 'Janardhan Rao Doppa']","['cs.LG', 'stat.ML']",2023-03-19 15:56:50+00:00
http://arxiv.org/abs/2303.10599v2,Convergence Analysis of Stochastic Gradient Descent with MCMC Estimators,"Understanding stochastic gradient descent (SGD) and its variants is essential
for machine learning. However, most of the preceding analyses are conducted
under amenable conditions such as unbiased gradient estimator and bounded
objective functions, which does not encompass many sophisticated applications,
such as variational Monte Carlo, entropy-regularized reinforcement learning and
variational inference. In this paper, we consider the SGD algorithm that employ
the Markov Chain Monte Carlo (MCMC) estimator to compute the gradient, called
MCMC-SGD. Since MCMC reduces the sampling complexity significantly, it is an
asymptotically convergent biased estimator in practice. Moreover, by
incorporating a general class of unbounded functions, it is much more difficult
to analyze the MCMC sampling error. Therefore, we assume that the function is
sub-exponential and use the Bernstein inequality for non-stationary Markov
chains to derive error bounds of the MCMC estimator. Consequently, MCMC-SGD is
proven to have a first order convergence rate $O(\log K/\sqrt{n K})$ with $K$
iterations and a sample size $n$. It partially explains how MCMC influences the
behavior of SGD. Furthermore, we verify the correlated negative curvature
condition under reasonable assumptions. It is shown that MCMC-SGD escapes from
saddle points and reaches $(\epsilon,\epsilon^{1/4})$ approximate second order
stationary points or $\epsilon^{1/2}$-variance points at least
$O(\epsilon^{-11/2}\log^{2}(1/\epsilon) )$ steps with high probability. Our
analysis unveils the convergence pattern of MCMC-SGD across a broad class of
stochastic optimization problems, and interprets the convergence phenomena
observed in practical applications.","['Tianyou Li', 'Fan Chen', 'Huajie Chen', 'Zaiwen Wen']","['stat.ML', 'math.OC']",2023-03-19 08:29:49+00:00
http://arxiv.org/abs/2303.10472v4,Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference,"Understanding the gradient variance of black-box variational inference (BBVI)
is a crucial step for establishing its convergence and developing algorithmic
improvements. However, existing studies have yet to show that the gradient
variance of BBVI satisfies the conditions used to study the convergence of
stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show
that BBVI satisfies a matching bound corresponding to the $ABC$ condition used
in the SGD literature when applied to smooth and quadratically-growing
log-likelihoods. Our results generalize to nonlinear covariance
parameterizations widely used in the practice of BBVI. Furthermore, we show
that the variance of the mean-field parameterization has provably superior
dimensional dependence.","['Kyurae Kim', 'Kaiwen Wu', 'Jisu Oh', 'Jacob R. Gardner']","['cs.LG', 'math.OC', 'stat.CO', 'stat.ML']",2023-03-18 19:07:14+00:00
http://arxiv.org/abs/2303.10322v2,Inverse Cubature and Quadrature Kalman filters,"Recent research in inverse cognition with cognitive radar has led to the
development of inverse stochastic filters that are employed by the target to
infer the information the cognitive radar may have learned. Prior works
addressed this inverse cognition problem by proposing inverse Kalman filter
(I-KF) and inverse extended KF (I-EKF), respectively, for linear and non-linear
Gaussian state-space models. However, in practice, many counter-adversarial
settings involve highly non-linear system models, wherein EKF's linearization
often fails. In this paper, we consider the efficient numerical integration
techniques to address such non-linearities and, to this end, develop inverse
cubature KF (I-CKF), inverse quadrature KF (I-QKF), and inverse
cubature-quadrature KF (I-CQKF). For the unknown system model case, we develop
reproducing kernel Hilbert space (RKHS)-based CKF. We derive the stochastic
stability conditions for the proposed filters in the
exponential-mean-squared-boundedness sense and prove the filters' consistency.
Numerical experiments demonstrate the estimation accuracy of our I-CKF, I-QKF,
and I-CQKF with the recursive Cram\'{e}r-Rao lower bound as a benchmark.","['Himali Singh', 'Kumar Vijay Mishra', 'Arpan Chattopadhyay']","['math.OC', 'cs.SY', 'eess.SP', 'eess.SY', 'stat.ML']",2023-03-18 03:48:39+00:00
http://arxiv.org/abs/2303.10221v1,"A statistical framework for GWAS of high dimensional phenotypes using summary statistics, with application to metabolite GWAS","The recent explosion of genetic and high dimensional biobank and 'omic' data
has provided researchers with the opportunity to investigate the shared genetic
origin (pleiotropy) of hundreds to thousands of related phenotypes. However,
existing methods for multi-phenotype genome-wide association studies (GWAS) do
not model pleiotropy, are only applicable to a small number of phenotypes, or
provide no way to perform inference. To add further complication, raw genetic
and phenotype data are rarely observed, meaning analyses must be performed on
GWAS summary statistics whose statistical properties in high dimensions are
poorly understood. We therefore developed a novel model, theoretical framework,
and set of methods to perform Bayesian inference in GWAS of high dimensional
phenotypes using summary statistics that explicitly model pleiotropy, beget
fast computation, and facilitate the use of biologically informed priors. We
demonstrate the utility of our procedure by applying it to metabolite GWAS,
where we develop new nonparametric priors for genetic effects on metabolite
levels that use known metabolic pathway information and foster interpretable
inference at the pathway level.","['Weiqiong Huang', 'Emily C. Hector', 'Joshua Cape', 'Chris McKennan']","['stat.ME', 'math.ST', 'q-bio.QM', 'stat.ML', 'stat.TH']",2023-03-17 19:33:25+00:00
http://arxiv.org/abs/2303.10167v4,Generalized partitioned local depth,"In this paper we provide a generalization of the concept of cohesion as
introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National
Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the
technique of partitioned local depth by distilling two key probabilistic
concepts: local relevance and support division. Earlier results are extended
within the new context, and examples of applications to revealing communities
in data with uncertainty are included. The work sheds light on the foundations
of partitioned local depth, and extends the original ideas to enable
probabilistic consideration of uncertain, variable and potentially conflicting
information.","['Kenneth S. Berenhaut', 'John D. Foley', 'Liangdongsheng Lyu']","['stat.ML', 'cs.LG', 'cs.SI', 'physics.soc-ph']",2023-03-17 17:54:25+00:00
http://arxiv.org/abs/2303.10165v2,Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs,"We study reward-free reinforcement learning (RL) with linear function
approximation, where the agent works in two phases: (1) in the exploration
phase, the agent interacts with the environment but cannot access the reward;
and (2) in the planning phase, the agent is given a reward function and is
expected to find a near-optimal policy based on samples collected in the
exploration phase. The sample complexities of existing reward-free algorithms
have a polynomial dependence on the planning horizon, which makes them
intractable for long planning horizon RL problems. In this paper, we propose a
new reward-free algorithm for learning linear mixture Markov decision processes
(MDPs), where the transition probability can be parameterized as a linear
combination of known feature mappings. At the core of our algorithm is
uncertainty-weighted value-targeted regression with exploration-driven
pseudo-reward and a high-order moment estimator for the aleatoric and epistemic
uncertainties. When the total reward is bounded by $1$, we show that our
algorithm only needs to explore $\tilde O( d^2\varepsilon^{-2})$ episodes to
find an $\varepsilon$-optimal policy, where $d$ is the dimension of the feature
mapping. The sample complexity of our algorithm only has a polylogarithmic
dependence on the planning horizon and therefore is ""horizon-free"". In
addition, we provide an $\Omega(d^2\varepsilon^{-2})$ sample complexity lower
bound, which matches the sample complexity of our algorithm up to logarithmic
factors, suggesting that our algorithm is optimal.","['Junkai Zhang', 'Weitong Zhang', 'Quanquan Gu']","['cs.LG', 'math.OC', 'stat.ML']",2023-03-17 17:53:28+00:00
http://arxiv.org/abs/2303.10144v1,Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting,"Early stopping based on the validation set performance is a popular approach
to find the right balance between under- and overfitting in the context of
supervised learning. However, in reinforcement learning, even for supervised
sub-problems such as world model learning, early stopping is not applicable as
the dataset is continually evolving. As a solution, we propose a new general
method that dynamically adjusts the update to data (UTD) ratio during training
based on under- and overfitting detection on a small subset of the continuously
collected experience not used for training. We apply our method to DreamerV2, a
state-of-the-art model-based reinforcement learning algorithm, and evaluate it
on the DeepMind Control Suite and the Atari $100$k benchmark. The results
demonstrate that one can better balance under- and overestimation by adjusting
the UTD ratio with our approach compared to the default setting in DreamerV2
and that it is competitive with an extensive hyperparameter search which is not
feasible for many applications. Our method eliminates the need to set the UTD
hyperparameter by hand and even leads to a higher robustness with regard to
other learning-related hyperparameters further reducing the amount of necessary
tuning.","['Nicolai Dorka', 'Tim Welschehold', 'Wolfram Burgard']","['cs.LG', 'stat.ML']",2023-03-17 17:29:02+00:00
http://arxiv.org/abs/2303.10104v1,A Robustness Analysis of Blind Source Separation,"Blind source separation (BSS) aims to recover an unobserved signal $S$ from
its mixture $X=f(S)$ under the condition that the effecting transformation $f$
is invertible but unknown. As this is a basic problem with many practical
applications, a fundamental issue is to understand how the solutions to this
problem behave when their supporting statistical prior assumptions are
violated. In the classical context of linear mixtures, we present a general
framework for analysing such violations and quantifying their impact on the
blind recovery of $S$ from $X$. Modelling $S$ as a multidimensional stochastic
process, we introduce an informative topology on the space of possible causes
underlying a mixture $X$, and show that the behaviour of a generic BSS-solution
in response to general deviations from its defining structural assumptions can
be profitably analysed in the form of explicit continuity guarantees with
respect to this topology. This allows for a flexible and convenient
quantification of general model uncertainty scenarios and amounts to the first
comprehensive robustness framework for BSS. Our approach is entirely
constructive, and we demonstrate its utility with novel theoretical guarantees
for a number of statistical applications.",['Alexander Schell'],"['math.ST', 'stat.ML', 'stat.TH', '62H25, 62G35, 62H05, 60L10, 62M86']",2023-03-17 16:30:51+00:00
http://arxiv.org/abs/2303.10085v1,Robust probabilistic inference via a constrained transport metric,"Flexible Bayesian models are typically constructed using limits of large
parametric models with a multitude of parameters that are often
uninterpretable. In this article, we offer a novel alternative by constructing
an exponentially tilted empirical likelihood carefully designed to concentrate
near a parametric family of distributions of choice with respect to a novel
variant of the Wasserstein metric, which is then combined with a prior
distribution on model parameters to obtain a robustified posterior. The
proposed approach finds applications in a wide variety of robust inference
problems, where we intend to perform inference on the parameters associated
with the centering distribution in presence of outliers. Our proposed transport
metric enjoys great computational simplicity, exploiting the Sinkhorn
regularization for discrete optimal transport problems, and being inherently
parallelizable. We demonstrate superior performance of our methodology when
compared against state-of-the-art robust Bayesian inference methods. We also
demonstrate equivalence of our approach with a nonparametric Bayesian
formulation under a suitable asymptotic framework, testifying to its
flexibility. The constrained entropy maximization that sits at the heart of our
likelihood formulation finds its utility beyond robust Bayesian inference; an
illustration is provided in a trustworthy machine learning application.","['Abhisek Chakraborty', 'Anirban Bhattacharya', 'Debdeep Pati']","['stat.ME', 'cs.LG', 'stat.ML']",2023-03-17 16:10:06+00:00
http://arxiv.org/abs/2303.10019v3,Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices,"This paper presents a new method for combining (or aggregating or ensembling)
multivariate probabilistic forecasts, considering dependencies between
quantiles and marginals through a smoothing procedure that allows for online
learning. We discuss two smoothing methods: dimensionality reduction using
Basis matrices and penalized smoothing. The new online learning algorithm
generalizes the standard CRPS learning framework into multivariate dimensions.
It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic
learning properties. The procedure uses horizontal aggregation, i.e.,
aggregation across quantiles. We provide an in-depth discussion on possible
extensions of the algorithm and several nested cases related to the existing
literature on online forecast combination. We apply the proposed methodology to
forecasting day-ahead electricity prices, which are 24-dimensional
distributional forecasts. The proposed method yields significant improvements
over uniform combination in terms of continuous ranked probability score
(CRPS). We discuss the temporal evolution of the weights and hyperparameters
and present the results of reduced versions of the preferred model. A fast C++
implementation of the proposed algorithm is provided in the open-source
R-Package profoc on CRAN.","['Jonathan Berrisch', 'Florian Ziel']","['stat.ML', 'cs.LG', 'econ.EM', 'q-fin.CP', 'stat.AP']",2023-03-17 14:47:55+00:00
http://arxiv.org/abs/2303.09995v2,Neural-prior stochastic block model,"The stochastic block model (SBM) is widely studied as a benchmark for graph
clustering aka community detection. In practice, graph data often come with
node attributes that bear additional information about the communities.
Previous works modeled such data by considering that the node attributes are
generated from the node community memberships. In this work, motivated by a
recent surge of works in signal processing using deep neural networks as
priors, we propose to model the communities as being determined by the node
attributes rather than the opposite. We define the corresponding model; we call
it the neural-prior SBM. We propose an algorithm, stemming from statistical
physics, based on a combination of belief propagation and approximate message
passing. We analyze the performance of the algorithm as well as the
Bayes-optimal performance. We identify detectability and exact recovery phase
transitions, as well as an algorithmically hard region. The proposed model and
algorithm can be used as a benchmark for both theory and algorithms. To
illustrate this, we compare the optimal performances to the performance of
simple graph neural networks.","['O. Duranthon', 'L. ZdeborovÃ¡']","['cond-mat.dis-nn', 'cs.SI', 'stat.ML']",2023-03-17 14:14:54+00:00
http://arxiv.org/abs/2303.09989v3,Finding Competence Regions in Domain Generalization,"We investigate a ""learning to reject"" framework to address the problem of
silent failures in Domain Generalization (DG), where the test distribution
differs from the training distribution. Assuming a mild distribution shift, we
wish to accept out-of-distribution (OOD) data from a new domain whenever a
model's estimated competence foresees trustworthy responses, instead of
rejecting OOD data outright. Trustworthiness is then predicted via a proxy
incompetence score that is tightly linked to the performance of a classifier.
We present a comprehensive experimental evaluation of existing proxy scores as
incompetence scores for classification and highlight the resulting trade-offs
between rejection rate and accuracy gain. For comparability with prior work, we
focus on standard DG benchmarks and consider the effect of measuring
incompetence via different learned representations in a closed versus an open
world setting. Our results suggest that increasing incompetence scores are
indeed predictive of reduced accuracy, leading to significant improvements of
the average accuracy below a suitable incompetence threshold. However, the
scores are not yet good enough to allow for a favorable accuracy/rejection
trade-off in all tested domains. Surprisingly, our results also indicate that
classifiers optimized for DG robustness do not outperform a naive Empirical
Risk Minimization (ERM) baseline in the competence region, that is, where test
samples elicit low incompetence scores.","['Jens MÃ¼ller', 'Stefan T. Radev', 'Robert Schmier', 'Felix Draxler', 'Carsten Rother', 'Ullrich KÃ¶the']","['cs.LG', 'stat.ML']",2023-03-17 14:04:51+00:00
http://arxiv.org/abs/2303.09877v1,On the Effects of Self-supervision and Contrastive Alignment in Deep Multi-view Clustering,"Self-supervised learning is a central component in recent approaches to deep
multi-view clustering (MVC). However, we find large variations in the
development of self-supervision-based methods for deep MVC, potentially slowing
the progress of the field. To address this, we present DeepMVC, a unified
framework for deep MVC that includes many recent methods as instances. We
leverage our framework to make key observations about the effect of
self-supervision, and in particular, drawbacks of aligning representations with
contrastive learning. Further, we prove that contrastive alignment can
negatively influence cluster separability, and that this effect becomes worse
when the number of views increases. Motivated by our findings, we develop
several new DeepMVC instances with new forms of self-supervision. We conduct
extensive experiments and find that (i) in line with our theoretical findings,
contrastive alignments decreases performance on datasets with many views; (ii)
all methods benefit from some form of self-supervision; and (iii) our new
instances outperform previous methods on several datasets. Based on our
results, we suggest several promising directions for future research. To
enhance the openness of the field, we provide an open-source implementation of
DeepMVC, including recent models and our new instances. Our implementation
includes a consistent evaluation protocol, facilitating fair and accurate
evaluation of methods and components.","['Daniel J. Trosten', 'Sigurd LÃ¸kse', 'Robert Jenssen', 'Michael C. Kampffmeyer']","['stat.ML', 'cs.CV', 'cs.LG']",2023-03-17 10:51:38+00:00
http://arxiv.org/abs/2303.09863v3,Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness,"Autoencoders have demonstrated remarkable success in learning low-dimensional
latent features of high-dimensional data across various applications. Assuming
that data are sampled near a low-dimensional manifold, we employ chart
autoencoders, which encode data into low-dimensional latent features on a
collection of charts, preserving the topology and geometry of the data
manifold. Our paper establishes statistical guarantees on the generalization
error of chart autoencoders, and we demonstrate their denoising capabilities by
considering $n$ noisy training samples, along with their noise-free
counterparts, on a $d$-dimensional manifold. By training autoencoders, we show
that chart autoencoders can effectively denoise the input data with normal
noise. We prove that, under proper network architectures, chart autoencoders
achieve a squared generalization error in the order of $\displaystyle
n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the
manifold and only weakly depends on the ambient dimension and noise level. We
further extend our theory on data with noise containing both normal and
tangential components, where chart autoencoders still exhibit a denoising
effect for the normal component. As a special case, our theory also applies to
classical autoencoders, as long as the data manifold has a global
parametrization. Our results provide a solid theoretical foundation for the
effectiveness of autoencoders, which is further validated through several
numerical experiments.","['Hao Liu', 'Alex Havrilla', 'Rongjie Lai', 'Wenjing Liao']","['stat.ML', 'cs.LG']",2023-03-17 10:01:32+00:00
http://arxiv.org/abs/2303.09842v1,Error Bounds for Kernel-Based Linear System Identification with Unknown Hyperparameters,"The kernel-based method has been successfully applied in linear system
identification using stable kernel designs. From a Gaussian process
perspective, it automatically provides probabilistic error bounds for the
identified models from the posterior covariance, which are useful in robust and
stochastic control. However, the error bounds require knowledge of the true
hyperparameters in the kernel design and are demonstrated to be inaccurate with
estimated hyperparameters for lightly damped systems or in the presence of high
noise. In this work, we provide reliable quantification of the estimation error
when the hyperparameters are unknown. The bounds are obtained by first
constructing a high-probability set for the true hyperparameters from the
marginal likelihood function and then finding the worst-case posterior
covariance within the set. The proposed bound is proven to contain the true
model with a high probability and its validity is verified in numerical
simulation.","['Mingzhou Yin', 'Roy S. Smith']","['eess.SY', 'cs.SY', 'stat.ML']",2023-03-17 08:52:16+00:00
http://arxiv.org/abs/2303.09705v2,Batch Updating of a Posterior Tree Distribution over a Meta-Tree,"Previously, we proposed a probabilistic data generation model represented by
an unobservable tree and a sequential updating method to calculate a posterior
distribution over a set of trees. The set is called a meta-tree. In this paper,
we propose a more efficient batch updating method.","['Yuta Nakahara', 'Toshiyasu Matsushima']","['cs.LG', 'stat.ML']",2023-03-17 00:27:55+00:00
http://arxiv.org/abs/2303.09531v1,GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data,"Vertical federated learning (VFL) is a distributed learning paradigm, where
computing clients collectively train a model based on the partial features of
the same set of samples they possess. Current research on VFL focuses on the
case when samples are independent, but it rarely addresses an emerging scenario
when samples are interrelated through a graph. For graph-structured data, graph
neural networks (GNNs) are competitive machine learning models, but a naive
implementation in the VFL setting causes a significant communication overhead.
Moreover, the analysis of the training is faced with a challenge caused by the
biased stochastic gradients. In this paper, we propose a model splitting method
that splits a backbone GNN across the clients and the server and a
communication-efficient algorithm, GLASU, to train such a model. GLASU adopts
lazy aggregation and stale updates to skip aggregation when evaluating the
model and skip feature exchanges during training, greatly reducing
communication. We offer a theoretical analysis and conduct extensive numerical
experiments on real-world datasets, showing that the proposed algorithm
effectively trains a GNN model, whose performance matches that of the backbone
GNN when trained in a centralized manner.","['Xinwei Zhang', 'Mingyi Hong', 'Jie Chen']","['cs.LG', 'stat.ML']",2023-03-16 17:47:55+00:00
http://arxiv.org/abs/2303.09519v2,PyVBMC: Efficient Bayesian inference in Python,"PyVBMC is a Python implementation of the Variational Bayesian Monte Carlo
(VBMC) algorithm for posterior and model inference for black-box computational
models (Acerbi, 2018, 2020). VBMC is an approximate inference method designed
for efficient parameter estimation and model assessment when model evaluations
are mildly-to-very expensive (e.g., a second or more) and/or noisy.
Specifically, VBMC computes:
  - a flexible (non-Gaussian) approximate posterior distribution of the model
parameters, from which statistics and posterior samples can be easily
extracted;
  - an approximation of the model evidence or marginal likelihood, a metric
used for Bayesian model selection.
  PyVBMC can be applied to any computational or statistical model with up to
roughly 10-15 continuous parameters, with the only requirement that the user
can provide a Python function that computes the target log likelihood of the
model, or an approximation thereof (e.g., an estimate of the likelihood
obtained via simulation or Monte Carlo methods). PyVBMC is particularly
effective when the model takes more than about a second per evaluation, with
dramatic speed-ups of 1-2 orders of magnitude when compared to traditional
approximate inference methods.
  Extensive benchmarks on both artificial test problems and a large number of
real models from the computational sciences, particularly computational and
cognitive neuroscience, show that VBMC generally - and often vastly -
outperforms alternative methods for sample-efficient Bayesian inference, and is
applicable to both exact and simulator-based models (Acerbi, 2018, 2019, 2020).
PyVBMC brings this state-of-the-art inference algorithm to Python, along with
an easy-to-use Pythonic interface for running the algorithm and manipulating
and visualizing its results.","['Bobby Huggins', 'Chengkun Li', 'Marlon Tobaben', 'Mikko J. Aarnos', 'Luigi Acerbi']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2023-03-16 17:37:22+00:00
http://arxiv.org/abs/2303.09491v1,Challenges and Opportunities in Quantum Machine Learning,"At the intersection of machine learning and quantum computing, Quantum
Machine Learning (QML) has the potential of accelerating data analysis,
especially for quantum data, with applications for quantum materials,
biochemistry, and high-energy physics. Nevertheless, challenges remain
regarding the trainability of QML models. Here we review current methods and
applications for QML. We highlight differences between quantum and classical
machine learning, with a focus on quantum neural networks and quantum deep
learning. Finally, we discuss opportunities for quantum advantage with QML.","['M. Cerezo', 'Guillaume Verdon', 'Hsin-Yuan Huang', 'Lukasz Cincio', 'Patrick J. Coles']","['quant-ph', 'cs.LG', 'stat.ML']",2023-03-16 17:10:39+00:00
http://arxiv.org/abs/2303.09474v1,Gradient flow on extensive-rank positive semi-definite matrix denoising,"In this work, we present a new approach to analyze the gradient flow for a
positive semi-definite matrix denoising problem in an extensive-rank and
high-dimensional regime. We use recent linear pencil techniques of random
matrix theory to derive fixed point equations which track the complete time
evolution of the matrix-mean-square-error of the problem. The predictions of
the resulting fixed point equations are validated by numerical experiments. In
this short note we briefly illustrate a few predictions of our formalism by way
of examples, and in particular we uncover continuous phase transitions in the
extensive-rank and high-dimensional regime, which connect to the classical
phase transitions of the low-rank problem in the appropriate limit. The
formalism has much wider applicability than shown in this communication.","['Antoine Bodin', 'Nicolas Macris']","['stat.ML', 'cs.LG']",2023-03-16 16:50:46+00:00
http://arxiv.org/abs/2303.09470v2,Learning with Noisy Labels through Learnable Weighting and Centroid Similarity,"We introduce a novel method for training machine learning models in the
presence of noisy labels, which are prevalent in domains such as medical
diagnosis and autonomous driving and have the potential to degrade a model's
generalization performance. Inspired by established literature that highlights
how deep learning models are prone to overfitting to noisy samples in the later
epochs of training, we propose a strategic approach. This strategy leverages
the distance to class centroids in the latent space and incorporates a
discounting mechanism, aiming to diminish the influence of samples that lie
distant from all class centroids. By doing so, we effectively counteract the
adverse effects of noisy labels. The foundational premise of our approach is
the assumption that samples situated further from their respective class
centroid in the initial stages of training are more likely to be associated
with noise. Our methodology is grounded in robust theoretical principles and
has been validated empirically through extensive experiments on several
benchmark datasets. Our results show that our method consistently outperforms
the existing state-of-the-art techniques, achieving significant improvements in
classification accuracy in the presence of noisy labels. The code for our
proposed loss function and supplementary materials is available at
https://github.com/wanifarooq/NCOD","['Farooq Ahmad Wani', 'Maria Sofia Bucarelli', 'Fabrizio Silvestri']","['cs.LG', 'cs.AI', 'stat.ML']",2023-03-16 16:43:24+00:00
http://arxiv.org/abs/2303.09468v2,On the Existence of a Complexity in Fixed Budget Bandit Identification,"In fixed budget bandit identification, an algorithm sequentially observes
samples from several distributions up to a given final time. It then answers a
query about the set of distributions. A good algorithm will have a small
probability of error. While that probability decreases exponentially with the
final time, the best attainable rate is not known precisely for most
identification tasks. We show that if a fixed budget task admits a complexity,
defined as a lower bound on the probability of error which is attained by the
same algorithm on all bandit problems, then that complexity is determined by
the best non-adaptive sampling procedure for that problem. We show that there
is no such complexity for several fixed budget identification tasks including
Bernoulli best arm identification with two arms: there is no single algorithm
that attains everywhere the best possible rate.",['RÃ©my Degenne'],"['stat.ML', 'cs.LG']",2023-03-16 16:39:00+00:00
http://arxiv.org/abs/2303.09408v2,Distributionally Robust Optimization using Cost-Aware Ambiguity Sets,"We present a novel framework for distributionally robust optimization (DRO),
called cost-aware DRO (CADRO). The key idea of CADRO is to exploit the cost
structure in the design of the ambiguity set to reduce conservatism.
Particularly, the set specifically constrains the worst-case distribution along
the direction in which the expected cost of an approximate solution increases
most rapidly. We prove that CADRO provides both a high-confidence upper bound
and a consistent estimator of the out-of-sample expected cost, and show
empirically that it produces solutions that are substantially less conservative
than existing DRO methods, while providing the same guarantees.","['Mathijs Schuurmans', 'Panagiotis Patrinos']","['math.OC', 'stat.ML']",2023-03-16 15:41:06+00:00
http://arxiv.org/abs/2303.09390v1,On the Interplay Between Misspecification and Sub-optimality Gap in Linear Contextual Bandits,"We study linear contextual bandits in the misspecified setting, where the
expected reward function can be approximated by a linear function class up to a
bounded misspecification level $\zeta>0$. We propose an algorithm based on a
novel data selection scheme, which only selects the contextual vectors with
large uncertainty for online regression. We show that, when the
misspecification level $\zeta$ is dominated by $\tilde O (\Delta / \sqrt{d})$
with $\Delta$ being the minimal sub-optimality gap and $d$ being the dimension
of the contextual vectors, our algorithm enjoys the same gap-dependent regret
bound $\tilde O (d^2/\Delta)$ as in the well-specified setting up to
logarithmic factors. In addition, we show that an existing algorithm SupLinUCB
(Chu et al., 2011) can also achieve a gap-dependent constant regret bound
without the knowledge of sub-optimality gap $\Delta$. Together with a lower
bound adapted from Lattimore et al. (2020), our result suggests an interplay
between misspecification level and the sub-optimality gap: (1) the linear
contextual bandit model is efficiently learnable when $\zeta \leq \tilde
O(\Delta / \sqrt{d})$; and (2) it is not efficiently learnable when $\zeta \geq
\tilde \Omega({\Delta} / {\sqrt{d}})$. Experiments on both synthetic and
real-world datasets corroborate our theoretical results.","['Weitong Zhang', 'Jiafan He', 'Zhiyuan Fan', 'Quanquan Gu']","['cs.LG', 'stat.ML']",2023-03-16 15:24:29+00:00
http://arxiv.org/abs/2303.09350v3,Unsupervised domain adaptation by learning using privileged information,"Successful unsupervised domain adaptation is guaranteed only under strong
assumptions such as covariate shift and overlap between input domains. The
latter is often violated in high-dimensional applications like image
classification which, despite this limitation, continues to serve as
inspiration and benchmark for algorithm development. In this work, we show that
training-time access to side information in the form of auxiliary variables can
help relax restrictions on input variables and increase the sample efficiency
of learning at the cost of collecting a richer variable set. As this
information is assumed available only during training, not in deployment, we
call this problem unsupervised domain adaptation by learning using privileged
information (DALUPI). To solve this problem, we propose a simple two-stage
learning algorithm, inspired by our analysis of the expected error in the
target domain, and a practical end-to-end variant for image classification. We
propose three evaluation tasks based on classification of entities in photos
and anomalies in medical images with different types of available privileged
information (binary attributes and single or multiple regions of interest). We
demonstrate across these tasks that using privileged information in learning
can reduce errors in domain transfer compared to baselines, be robust to
spurious correlations in the source domain, and increase sample efficiency.","['Adam Breitholtz', 'Anton Matsson', 'Fredrik D. Johansson']","['cs.LG', 'stat.ML']",2023-03-16 14:31:50+00:00
http://arxiv.org/abs/2303.09261v1,Orthogonal Directions Constrained Gradient Method: from non-linear equality constraints to Stiefel manifold,"We consider the problem of minimizing a non-convex function over a smooth
manifold $\mathcal{M}$. We propose a novel algorithm, the Orthogonal Directions
Constrained Gradient Method (ODCGM) which only requires computing a projection
onto a vector space. ODCGM is infeasible but the iterates are constantly pulled
towards the manifold, ensuring the convergence of ODCGM towards $\mathcal{M}$.
ODCGM is much simpler to implement than the classical methods which require the
computation of a retraction. Moreover, we show that ODCGM exhibits the
near-optimal oracle complexities $\mathcal{O}(1/\varepsilon^2)$ and
$\mathcal{O}(1/\varepsilon^4)$ in the deterministic and stochastic cases,
respectively. Furthermore, we establish that, under an appropriate choice of
the projection metric, our method recovers the landing algorithm of Ablin and
Peyr\'e (2022), a recently introduced algorithm for optimization over the
Stiefel manifold. As a result, we significantly extend the analysis of Ablin
and Peyr\'e (2022), establishing near-optimal rates both in deterministic and
stochastic frameworks. Finally, we perform numerical experiments which shows
the efficiency of ODCGM in a high-dimensional setting.","['Sholom Schechtman', 'Daniil Tiapkin', 'Michael Muehlebach', 'Eric Moulines']","['math.OC', 'stat.ML']",2023-03-16 12:25:53+00:00
http://arxiv.org/abs/2303.09166v1,Identifiability Results for Multimodal Contrastive Learning,"Contrastive learning is a cornerstone underlying recent progress in
multi-view and multimodal learning, e.g., in representation learning with
image/caption pairs. While its effectiveness is not yet fully understood, a
line of recent work reveals that contrastive learning can invert the data
generating process and recover ground truth latent factors shared between
views. In this work, we present new identifiability results for multimodal
contrastive learning, showing that it is possible to recover shared factors in
a more general setup than the multi-view setting studied previously.
Specifically, we distinguish between the multi-view setting with one generative
mechanism (e.g., multiple cameras of the same type) and the multimodal setting
that is characterized by distinct mechanisms (e.g., cameras and microphones).
Our work generalizes previous identifiability results by redefining the
generative process in terms of distinct mechanisms with modality-specific
latent variables. We prove that contrastive learning can block-identify latent
factors shared between modalities, even when there are nontrivial dependencies
between factors. We empirically verify our identifiability results with
numerical simulations and corroborate our findings on a complex multimodal
dataset of image/text pairs. Zooming out, our work provides a theoretical basis
for multimodal representation learning and explains in which settings
multimodal contrastive learning can be effective in practice.","['Imant Daunhawer', 'Alice Bizeul', 'Emanuele Palumbo', 'Alexander Marx', 'Julia E. Vogt']","['cs.LG', 'stat.ML']",2023-03-16 09:14:26+00:00
http://arxiv.org/abs/2303.09154v1,Bayesian Generalization Error in Linear Neural Networks with Concept Bottleneck Structure and Multitask Formulation,"Concept bottleneck model (CBM) is a ubiquitous method that can interpret
neural networks using concepts. In CBM, concepts are inserted between the
output layer and the last intermediate layer as observable values. This helps
in understanding the reason behind the outputs generated by the neural
networks: the weights corresponding to the concepts from the last hidden layer
to the output layer. However, it has not yet been possible to understand the
behavior of the generalization error in CBM since a neural network is a
singular statistical model in general. When the model is singular, a one to one
map from the parameters to probability distributions cannot be created. This
non-identifiability makes it difficult to analyze the generalization
performance. In this study, we mathematically clarify the Bayesian
generalization error and free energy of CBM when its architecture is
three-layered linear neural networks. We also consider a multitask problem
where the neural network outputs not only the original output but also the
concepts. The results show that CBM drastically changes the behavior of the
parameter region and the Bayesian generalization error in three-layered linear
neural networks as compared with the standard version, whereas the multitask
formulation does not.","['Naoki Hayashi', 'Yoshihide Sawada']","['stat.ML', 'cs.AI', 'cs.LG', 'math.ST', 'stat.TH', '62F15, 62R01, 68T07']",2023-03-16 08:34:56+00:00
http://arxiv.org/abs/2303.09066v1,High-Dimensional Penalized Bernstein Support Vector Machines,"The support vector machines (SVM) is a powerful classifier used for binary
classification to improve the prediction accuracy. However, the
non-differentiability of the SVM hinge loss function can lead to computational
difficulties in high dimensional settings. To overcome this problem, we rely on
Bernstein polynomial and propose a new smoothed version of the SVM hinge loss
called the Bernstein support vector machine (BernSVM), which is suitable for
the high dimension $p >> n$ regime. As the BernSVM objective loss function is
of the class $C^2$, we propose two efficient algorithms for computing the
solution of the penalized BernSVM. The first algorithm is based on coordinate
descent with maximization-majorization (MM) principle and the second one is
IRLS-type algorithm (iterative re-weighted least squares). Under standard
assumptions, we derive a cone condition and a restricted strong convexity to
establish an upper bound for the weighted Lasso BernSVM estimator. Using a
local linear approximation, we extend the latter result to penalized BernSVM
with non convex penalties SCAD and MCP. Our bound holds with high probability
and achieves a rate of order $\sqrt{s\log(p)/n}$, where $s$ is the number of
active features. Simulation studies are considered to illustrate the prediction
accuracy of BernSVM to its competitors and also to compare the performance of
the two algorithms in terms of computational timing and error estimation. The
use of the proposed method is illustrated through analysis of three large-scale
real data examples.","['Rachid Kharoubi', 'Abdallah Mkhadri', 'Karim Oualkacha']","['stat.ML', 'cs.LG']",2023-03-16 03:48:29+00:00
http://arxiv.org/abs/2303.08902v2,Learning ground states of gapped quantum Hamiltonians with Kernel Methods,"Neural network approaches to approximate the ground state of quantum
hamiltonians require the numerical solution of a highly nonlinear optimization
problem. We introduce a statistical learning approach that makes the
optimization trivial by using kernel methods. Our scheme is an approximate
realization of the power method, where supervised learning is used to learn the
next step of the power iteration. We show that the ground state properties of
arbitrary gapped quantum hamiltonians can be reached with polynomial resources
under the assumption that the supervised learning is efficient. Using kernel
ridge regression, we provide numerical evidence that the learning assumption is
verified by applying our scheme to find the ground states of several
prototypical interacting many-body quantum systems, both in one and two
dimensions, showing the flexibility of our approach.","['Clemens Giuliani', 'Filippo Vicentini', 'Riccardo Rossi', 'Giuseppe Carleo']","['quant-ph', 'cs.LG', 'physics.comp-ph', 'stat.ML']",2023-03-15 19:37:33+00:00
http://arxiv.org/abs/2303.08874v2,Bayesian Quadrature for Neural Ensemble Search,"Ensembling can improve the performance of Neural Networks, but existing
approaches struggle when the architecture likelihood surface has dispersed,
narrow peaks. Furthermore, existing methods construct equally weighted
ensembles, and this is likely to be vulnerable to the failure modes of the
weaker architectures. By viewing ensembling as approximately marginalising over
architectures we construct ensembles using the tools of Bayesian Quadrature --
tools which are well suited to the exploration of likelihood surfaces with
dispersed, narrow peaks. Additionally, the resulting ensembles consist of
architectures weighted commensurate with their performance. We show empirically
-- in terms of test likelihood, accuracy, and expected calibration error --
that our method outperforms state-of-the-art baselines, and verify via ablation
studies that its components do so independently.","['Saad Hamid', 'Xingchen Wan', 'Martin JÃ¸rgensen', 'Binxin Ru', 'Michael Osborne']","['stat.ML', 'cs.LG']",2023-03-15 18:37:41+00:00
http://arxiv.org/abs/2303.08816v2,Borda Regret Minimization for Generalized Linear Dueling Bandits,"Dueling bandits are widely used to model preferential feedback prevalent in
many applications such as recommendation systems and ranking. In this paper, we
study the Borda regret minimization problem for dueling bandits, which aims to
identify the item with the highest Borda score while minimizing the cumulative
regret. We propose a rich class of generalized linear dueling bandit models,
which cover many existing models. We first prove a regret lower bound of order
$\Omega(d^{2/3} T^{2/3})$ for the Borda regret minimization problem, where $d$
is the dimension of contextual vectors and $T$ is the time horizon. To attain
this lower bound, we propose an explore-then-commit type algorithm for the
stochastic setting, which has a nearly matching regret upper bound
$\tilde{O}(d^{2/3} T^{2/3})$. We also propose an EXP3-type algorithm for the
adversarial linear setting, where the underlying model parameter can change at
each round. Our algorithm achieves an $\tilde{O}(d^{2/3} T^{2/3})$ regret,
which is also optimal. Empirical evaluations on both synthetic data and a
simulated real-world environment are conducted to corroborate our theoretical
analysis.","['Yue Wu', 'Tao Jin', 'Hao Lou', 'Farzad Farnoud', 'Quanquan Gu']","['cs.LG', 'stat.ML']",2023-03-15 17:59:27+00:00
http://arxiv.org/abs/2303.08806v1,Understanding Post-hoc Explainers: The Case of Anchors,"In many scenarios, the interpretability of machine learning models is a
highly required but difficult task. To explain the individual predictions of
such models, local model-agnostic approaches have been proposed. However, the
process generating the explanations can be, for a user, as mysterious as the
prediction to be explained. Furthermore, interpretability methods frequently
lack theoretical guarantees, and their behavior on simple models is frequently
unknown. While it is difficult, if not impossible, to ensure that an explainer
behaves as expected on a cutting-edge model, we can at least ensure that
everything works on simple, already interpretable models. In this paper, we
present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular
rule-based interpretability method that highlights a small set of words to
explain a text classifier's decision. After formalizing its algorithm and
providing useful insights, we demonstrate mathematically that Anchors produces
meaningful results when used with linear text classifiers on top of a TF-IDF
vectorization. We believe that our analysis framework can aid in the
development of new explainability methods based on solid theoretical
foundations.","['Gianluigi Lopardo', 'Frederic Precioso', 'Damien Garreau']","['stat.ML', 'cs.AI', 'cs.CL', 'cs.LG']",2023-03-15 17:56:34+00:00
http://arxiv.org/abs/2303.08777v2,Distribution-free Deviation Bounds and The Role of Domain Knowledge in Learning via Model Selection with Cross-validation Risk Estimation,"Cross-validation techniques for risk estimation and model selection are
widely used in statistics and machine learning. However, the understanding of
the theoretical properties of learning via model selection with
cross-validation risk estimation is quite low in face of its widespread use. In
this context, this paper presents learning via model selection with
cross-validation risk estimation as a general systematic learning framework
within classical statistical learning theory and establishes distribution-free
deviation bounds in terms of VC dimension, giving detailed proofs of the
results and considering both bounded and unbounded loss functions. In
particular, we investigate how the generalization of learning via model
selection may be increased by modeling the collection of candidate models. We
define the Learning Spaces as a class of candidate models in which the partial
order by inclusion reflects the models complexities, and we formalize a manner
of defining them based on domain knowledge. We illustrate this modeling in a
worst-case scenario of learning a classifier with finite domain and a typical
scenario of linear regression. Through theoretical insights and concrete
examples, we aim to provide guidance on selecting the family of candidate
models based on domain knowledge to increase generalization.","['Diego Marcondes', 'ClÃ¡udia Peixoto']","['stat.ML', 'cs.LG']",2023-03-15 17:18:31+00:00
http://arxiv.org/abs/2303.08732v1,Predicting Individualized Effects of Internet-Based Treatment for Genito-Pelvic Pain/Penetration Disorder: Development and Internal Validation of a Multivariable Decision Tree Model,"Genito-Pelvic Pain/Penetration-Disorder (GPPPD) is a common disorder but
rarely treated in routine care. Previous research documents that GPPPD symptoms
can be treated effectively using internet-based psychological interventions.
However, non-response remains common for all state-of-the-art treatments and it
is unclear which patient groups are expected to benefit most from an
internet-based intervention. Multivariable prediction models are increasingly
used to identify predictors of heterogeneous treatment effects, and to allocate
treatments with the greatest expected benefits. In this study, we developed and
internally validated a multivariable decision tree model that predicts effects
of an internet-based treatment on a multidimensional composite score of GPPPD
symptoms. Data of a randomized controlled trial comparing the internet-based
intervention to a waitlist control group (N =200) was used to develop a
decision tree model using model-based recursive partitioning. Model performance
was assessed by examining the apparent and bootstrap bias-corrected
performance. The final pruned decision tree consisted of one splitting
variable, joint dyadic coping, based on which two response clusters emerged. No
effect was found for patients with low dyadic coping ($n$=33; $d$=0.12; 95% CI:
-0.57-0.80), while large effects ($d$=1.00; 95%CI: 0.68-1.32; $n$=167) are
predicted for those with high dyadic coping at baseline. The
bootstrap-bias-corrected performance of the model was $R^2$=27.74%
(RMSE=13.22).","['Anna-Carlotta Zarski', 'Mathias Harrer', 'Paula Kuper', 'Antonia A. Sprenger', 'Matthias Berking', 'David Daniel Ebert']","['stat.AP', 'stat.ME', 'stat.ML']",2023-03-15 16:16:16+00:00
http://arxiv.org/abs/2303.08720v1,Practicality of generalization guarantees for unsupervised domain adaptation with neural networks,"Understanding generalization is crucial to confidently engineer and deploy
machine learning models, especially when deployment implies a shift in the data
domain. For such domain adaptation problems, we seek generalization bounds
which are tractably computable and tight. If these desiderata can be reached,
the bounds can serve as guarantees for adequate performance in deployment.
However, in applications where deep neural networks are the models of choice,
deriving results which fulfill these remains an unresolved challenge; most
existing bounds are either vacuous or has non-estimable terms, even in
favorable conditions. In this work, we evaluate existing bounds from the
literature with potential to satisfy our desiderata on domain adaptation image
classification tasks, where deep neural networks are preferred. We find that
all bounds are vacuous and that sample generalization terms account for much of
the observed looseness, especially when these terms interact with measures of
domain shift. To overcome this and arrive at the tightest possible results, we
combine each bound with recent data-dependent PAC-Bayes analysis, greatly
improving the guarantees. We find that, when domain overlap can be assumed, a
simple importance weighting extension of previous work provides the tightest
estimable bound. Finally, we study which terms dominate the bounds and identify
possible directions for further improvement.","['Adam Breitholtz', 'Fredrik D. Johansson']","['cs.LG', 'stat.ML']",2023-03-15 16:05:05+00:00
http://arxiv.org/abs/2303.08691v3,Learning to Reconstruct Signals From Binary Measurements,"Recent advances in unsupervised learning have highlighted the possibility of
learning to reconstruct signals from noisy and incomplete linear measurements
alone. These methods play a key role in medical and scientific imaging and
sensing, where ground truth data is often scarce or difficult to obtain.
However, in practice, measurements are not only noisy and incomplete but also
quantized. Here we explore the extreme case of learning from binary
observations and provide necessary and sufficient conditions on the number of
measurements required for identifying a set of signals from incomplete binary
data. Our results are complementary to existing bounds on signal recovery from
binary measurements. Furthermore, we introduce a novel self-supervised learning
approach, which we name SSBM, that only requires binary data for training. We
demonstrate in a series of experiments with real datasets that SSBM performs on
par with supervised learning and outperforms sparse reconstruction methods with
a fixed wavelet basis by a large margin.","['JuliÃ¡n Tachella', 'Laurent Jacques']","['eess.SP', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML', '68U10', 'I.4.5; I.2.10; G.3']",2023-03-15 15:21:41+00:00
http://arxiv.org/abs/2303.08625v1,Interpretable Ensembles of Hyper-Rectangles as Base Models,"A new extremely simple ensemble-based model with the uniformly generated
axis-parallel hyper-rectangles as base models (HRBM) is proposed. Two types of
HRBMs are studied: closed rectangles and corners. The main idea behind HRBM is
to consider and count training examples inside and outside each rectangle. It
is proposed to incorporate HRBMs into the gradient boosting machine (GBM).
Despite simplicity of HRBMs, it turns out that these simple base models allow
us to construct effective ensemble-based models and avoid overfitting. A simple
method for calculating optimal regularization parameters of the ensemble-based
model, which can be modified in the explicit way at each iteration of GBM, is
considered. Moreover, a new regularization called the ""step height penalty"" is
studied in addition to the standard L1 and L2 regularizations. An extremely
simple approach to the proposed ensemble-based model prediction interpretation
by using the well-known method SHAP is proposed. It is shown that GBM with HRBM
can be regarded as a model extending a set of interpretable models for
explaining black-box models. Numerical experiments with real datasets
illustrate the proposed GBM with HRBMs for regression and classification
problems. Experiments also illustrate computational efficiency of the proposed
SHAP modifications. The code of proposed algorithms implementing GBM with HRBM
is publicly available.","['Andrei V. Konstantinov', 'Lev V. Utkin']","['cs.LG', 'cs.AI', 'stat.ML']",2023-03-15 13:50:36+00:00
http://arxiv.org/abs/2303.08622v2,Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer,"Diffusion models have shown great promise in text-guided image style
transfer, but there is a trade-off between style transformation and content
preservation due to their stochastic nature. Existing methods require
computationally expensive fine-tuning of diffusion models or additional neural
network. To address this, here we propose a zero-shot contrastive loss for
diffusion models that doesn't require additional fine-tuning or auxiliary
networks. By leveraging patch-wise contrastive loss between generated samples
and original image embeddings in the pre-trained diffusion model, our method
can generate images with the same semantic content as the source image in a
zero-shot manner. Our approach outperforms existing methods while preserving
content and requiring no additional training, not only for image style transfer
but also for image-to-image translation and manipulation. Our experimental
results validate the effectiveness of our proposed method.","['Serin Yang', 'Hyunmin Hwang', 'Jong Chul Ye']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2023-03-15 13:47:02+00:00
http://arxiv.org/abs/2303.08613v2,Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model,"We study the incentivized information acquisition problem, where a principal
hires an agent to gather information on her behalf. Such a problem is modeled
as a Stackelberg game between the principal and the agent, where the principal
announces a scoring rule that specifies the payment, and then the agent then
chooses an effort level that maximizes her own profit and reports the
information. We study the online setting of such a problem from the principal's
perspective, i.e., designing the optimal scoring rule by repeatedly interacting
with the strategic agent. We design a provably sample efficient algorithm that
tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a
sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a
delicate estimation procedure for the optimal profit of the principal, and a
conservative correction scheme that ensures the desired agent's actions are
incentivized. Furthermore, a key feature of our regret bound is that it is
independent of the number of states of the environment.","['Siyu Chen', 'Jibang Wu', 'Yifan Wu', 'Zhuoran Yang']","['cs.LG', 'cs.AI', 'cs.GT', 'econ.TH', 'stat.ML']",2023-03-15 13:40:16+00:00
http://arxiv.org/abs/2303.08456v2,Statistical learning on measures: an application to persistence diagrams,"We consider a binary supervised learning classification problem where instead
of having data in a finite-dimensional Euclidean space, we observe measures on
a compact space $\mathcal{X}$. Formally, we observe data $D_N = (\mu_1, Y_1),
\ldots, (\mu_N, Y_N)$ where $\mu_i$ is a measure on $\mathcal{X}$ and $Y_i$ is
a label in $\{0, 1\}$. Given a set $\mathcal{F}$ of base-classifiers on
$\mathcal{X}$, we build corresponding classifiers in the space of measures. We
provide upper and lower bounds on the Rademacher complexity of this new class
of classifiers that can be expressed simply in terms of corresponding
quantities for the class $\mathcal{F}$. If the measures $\mu_i$ are uniform
over a finite set, this classification task boils down to a multi-instance
learning problem. However, our approach allows more flexibility and diversity
in the input data we can deal with. While such a framework has many possible
applications, this work strongly emphasizes on classifying data via topological
descriptors called persistence diagrams. These objects are discrete measures on
$\mathbb{R}^2$, where the coordinates of each point correspond to the range of
scales at which a topological feature exists. We will present several
classifiers on measures and show how they can heuristically and theoretically
enable a good classification performance in various settings in the case of
persistence diagrams.","['Olympio Hacquard', 'Gilles Blanchard', 'ClÃ©ment Levrard']","['cs.CG', 'math.ST', 'stat.ML', 'stat.TH']",2023-03-15 09:01:37+00:00
http://arxiv.org/abs/2303.08433v1,The Benefits of Mixup for Feature Learning,"Mixup, a simple data augmentation method that randomly mixes two data points
via linear interpolation, has been extensively applied in various deep learning
applications to gain better generalization. However, the theoretical
underpinnings of its efficacy are not yet fully understood. In this paper, we
aim to seek a fundamental understanding of the benefits of Mixup. We first show
that Mixup using different linear interpolation parameters for features and
labels can still achieve similar performance to the standard Mixup. This
indicates that the intuitive linearity explanation in Zhang et al., (2018) may
not fully explain the success of Mixup. Then we perform a theoretical study of
Mixup from the feature learning perspective. We consider a feature-noise data
model and show that Mixup training can effectively learn the rare features
(appearing in a small fraction of data) from its mixture with the common
features (appearing in a large fraction of data). In contrast, standard
training can only learn the common features but fails to learn the rare
features, thus suffering from bad generalization performance. Moreover, our
theoretical analysis also shows that the benefits of Mixup for feature learning
are mostly gained in the early training phase, based on which we propose to
apply early stopping in Mixup. Experimental results verify our theoretical
findings and demonstrate the effectiveness of the early-stopped Mixup training.","['Difan Zou', 'Yuan Cao', 'Yuanzhi Li', 'Quanquan Gu']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2023-03-15 08:11:47+00:00
http://arxiv.org/abs/2303.08431v4,Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators,"Nonlinear control systems with partial information to the decision maker are
prevalent in a variety of applications. As a step toward studying such
nonlinear systems, this work explores reinforcement learning methods for
finding the optimal policy in the nearly linear-quadratic regulator systems. In
particular, we consider a dynamic system that combines linear and nonlinear
components, and is governed by a policy with the same structure. Assuming that
the nonlinear component comprises kernels with small Lipschitz coefficients, we
characterize the optimization landscape of the cost function. Although the cost
function is nonconvex in general, we establish the local strong convexity and
smoothness in the vicinity of the global optimizer. Additionally, we propose an
initialization mechanism to leverage these properties. Building on the
developments, we design a policy gradient algorithm that is guaranteed to
converge to the globally optimal policy with a linear rate.","['Yinbin Han', 'Meisam Razaviyayn', 'Renyuan Xu']","['cs.LG', 'math.OC', 'stat.ML']",2023-03-15 08:08:02+00:00
http://arxiv.org/abs/2303.08242v1,Optimal Sampling Designs for Multi-dimensional Streaming Time Series with Application to Power Grid Sensor Data,"The Internet of Things (IoT) system generates massive high-speed temporally
correlated streaming data and is often connected with online inference tasks
under computational or energy constraints. Online analysis of these streaming
time series data often faces a trade-off between statistical efficiency and
computational cost. One important approach to balance this trade-off is
sampling, where only a small portion of the sample is selected for the model
fitting and update. Motivated by the demands of dynamic relationship analysis
of IoT system, we study the data-dependent sample selection and online
inference problem for a multi-dimensional streaming time series, aiming to
provide low-cost real-time analysis of high-speed power grid electricity
consumption data. Inspired by D-optimality criterion in design of experiments,
we propose a class of online data reduction methods that achieve an optimal
sampling criterion and improve the computational efficiency of the online
analysis. We show that the optimal solution amounts to a strategy that is a
mixture of Bernoulli sampling and leverage score sampling. The leverage score
sampling involves auxiliary estimations that have a computational advantage
over recursive least squares updates. Theoretical properties of the auxiliary
estimations involved are also discussed. When applied to European power grid
consumption data, the proposed leverage score based sampling methods outperform
the benchmark sampling method in online estimation and prediction. The general
applicability of the sampling-assisted online estimation method is assessed via
simulation studies.","['Rui Xie', 'Shuyang Bai', 'Ping Ma']","['stat.ML', 'cs.LG', 'stat.AP']",2023-03-14 21:26:30+00:00
http://arxiv.org/abs/2303.08230v1,Bayesian Beta-Bernoulli Process Sparse Coding with Deep Neural Networks,"Several approximate inference methods have been proposed for deep discrete
latent variable models. However, non-parametric methods which have previously
been successfully employed for classical sparse coding models have largely been
unexplored in the context of deep models. We propose a non-parametric iterative
algorithm for learning discrete latent representations in such deep models.
Additionally, to learn scale invariant discrete features, we propose local data
scaling variables. Lastly, to encourage sparsity in our representations, we
propose a Beta-Bernoulli process prior on the latent factors. We evaluate our
spare coding model coupled with different likelihood models. We evaluate our
method across datasets with varying characteristics and compare our results to
current amortized approximate inference methods.","['Arunesh Mittal', 'Kai Yang', 'Paul Sajda', 'John Paisley']","['cs.LG', 'stat.ML']",2023-03-14 20:50:12+00:00
http://arxiv.org/abs/2303.08102v2,Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice,"We investigate the problem of bandits with expert advice when the experts are
fixed and known distributions over the actions. Improving on previous analyses,
we show that the regret in this setting is controlled by information-theoretic
quantities that measure the similarity between experts. In some natural special
cases, this allows us to obtain the first regret bound for EXP4 that can get
arbitrarily close to zero if the experts are similar enough. While for a
different algorithm, we provide another bound that describes the similarity
between the experts in terms of the KL-divergence, and we show that this bound
can be smaller than the one of EXP4 in some cases. Additionally, we provide
lower bounds for certain classes of experts showing that the algorithms we
analyzed are nearly optimal in some cases.","['Khaled Eldowa', 'NicolÃ² Cesa-Bianchi', 'Alberto Maria Metelli', 'Marcello Restelli']","['cs.LG', 'stat.ML']",2023-03-14 17:41:31+00:00
http://arxiv.org/abs/2303.08081v2,Explanation Shift: How Did the Distribution Shift Impact the Model?,"As input data distributions evolve, the predictive performance of machine
learning models tends to deteriorate. In practice, new input data tend to come
without target labels. Then, state-of-the-art techniques model input data
distributions or model prediction distributions and try to understand issues
regarding the interactions between learned models and shifting distributions.
We suggest a novel approach that models how explanation characteristics shift
when affected by distribution shifts. We find that the modeling of explanation
shifts can be a better indicator for detecting out-of-distribution model
behaviour than state-of-the-art techniques. We analyze different types of
distribution shifts using synthetic examples and real-world data sets. We
provide an algorithmic method that allows us to inspect the interaction between
data set features and learned models and compare them to the state-of-the-art.
We release our methods in an open-source Python package, as well as the code
used to reproduce our experiments.","['Carlos Mougan', 'Klaus Broelemann', 'David Masip', 'Gjergji Kasneci', 'Thanassis Thiropanis', 'Steffen Staab']","['cs.LG', 'stat.ML']",2023-03-14 17:13:01+00:00
http://arxiv.org/abs/2303.08059v2,Fast Rates for Maximum Entropy Exploration,"We address the challenge of exploration in reinforcement learning (RL) when
the agent operates in an unknown environment with sparse or no rewards. In this
work, we study the maximum entropy exploration problem of two different types.
The first type is visitation entropy maximization previously considered by
Hazan et al.(2019) in the discounted setting. For this type of exploration, we
propose a game-theoretic algorithm that has
$\widetilde{\mathcal{O}}(H^3S^2A/\varepsilon^2)$ sample complexity thus
improving the $\varepsilon$-dependence upon existing results, where $S$ is a
number of states, $A$ is a number of actions, $H$ is an episode length, and
$\varepsilon$ is a desired accuracy. The second type of entropy we study is the
trajectory entropy. This objective function is closely related to the
entropy-regularized MDPs, and we propose a simple algorithm that has a sample
complexity of order
$\widetilde{\mathcal{O}}(\mathrm{poly}(S,A,H)/\varepsilon)$. Interestingly, it
is the first theoretical result in RL literature that establishes the potential
statistical advantage of regularized MDPs for exploration. Finally, we apply
developed regularization techniques to reduce sample complexity of visitation
entropy maximization to $\widetilde{\mathcal{O}}(H^2SA/\varepsilon^2)$,
yielding a statistical separation between maximum entropy exploration and
reward-free exploration.","['Daniil Tiapkin', 'Denis Belomestny', 'Daniele Calandriello', 'Eric Moulines', 'Remi Munos', 'Alexey Naumov', 'Pierre Perrault', 'Yunhao Tang', 'Michal Valko', 'Pierre Menard']","['stat.ML', 'cs.LG']",2023-03-14 16:51:14+00:00
http://arxiv.org/abs/2303.08587v1,Delay-SDE-net: A deep learning approach for time series modelling with memory and uncertainty estimates,"To model time series accurately is important within a wide range of fields.
As the world is generally too complex to be modelled exactly, it is often
meaningful to assess the probability of a dynamical system to be in a specific
state. This paper presents the Delay-SDE-net, a neural network model based on
stochastic delay differential equations (SDDEs). The use of SDDEs with multiple
delays as modelling framework makes it a suitable model for time series with
memory effects, as it includes memory through previous states of the system.
The stochastic part of the Delay-SDE-net provides a basis for estimating
uncertainty in modelling, and is split into two neural networks to account for
aleatoric and epistemic uncertainty. The uncertainty is provided instantly,
making the model suitable for applications where time is sparse. We derive the
theoretical error of the Delay-SDE-net and analyze the convergence rate
numerically. At comparisons with similar models, the Delay-SDE-net has
consistently the best performance, both in predicting time series values and
uncertainties.","['Mari Dahl Eggen', 'Alise Danielle Midtfjord']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2023-03-14 14:31:38+00:00
http://arxiv.org/abs/2303.07773v1,Axiomatic characterization of pointwise Shapley decompositions,"A common problem in various applications is the additive decomposition of the
output of a function with respect to its input variables. Functions with binary
arguments can be axiomatically decomposed by the famous Shapley value. For the
decomposition of functions with real arguments, a popular method is the
pointwise application of the Shapley value on the domain. However, this
pointwise application largely ignores the overall structure of functions. In
this paper, axioms are developed which fully preserve functional structures and
lead to unique decompositions for all Borel measurable functions.",['Marcus C Christiansen'],"['q-fin.MF', 'econ.GN', 'q-fin.EC', 'stat.ML']",2023-03-14 10:24:48+00:00
http://arxiv.org/abs/2303.07768v3,DBSCAN of Multi-Slice Clustering for Third-Order Tensors,"Several methods for triclustering three-dimensional data require the cluster
size or the number of clusters in each dimension to be specified. To address
this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal
slices that lie in a low dimensional subspace for a rank-one tensor dataset in
order to find a cluster based on the threshold similarity. We propose an
extension algorithm called MSC-DBSCAN to extract the different clusters of
slices that lie in the different subspaces from the data if the dataset is a
sum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC
algorithm and can find the same solution for rank-one tensor data as MSC.","['Dina Faneva Andriantsiory', 'Joseph Ben Geloun', 'Mustapha Lebbah']","['cs.LG', 'stat.ML']",2023-03-14 10:18:31+00:00
http://arxiv.org/abs/2303.07757v1,Multiway clustering of 3-order tensor via affinity matrix,"We propose a new method of multiway clustering for 3-order tensors via
affinity matrix (MCAM). Based on a notion of similarity between the tensor
slices and the spread of information of each slice, our model builds an
affinity/similarity matrix on which we apply advanced clustering methods. The
combination of all clusters of the three modes delivers the desired multiway
clustering. Finally, MCAM achieves competitive results compared with other
known algorithms on synthetics and real datasets.","['Dina Faneva Andriantsiory', 'Joseph Ben Geloun', 'Mustapha Lebbah']","['cs.LG', 'stat.ML']",2023-03-14 10:02:52+00:00
http://arxiv.org/abs/2303.07627v1,Best arm identification in rare events,"We consider the best arm identification problem in the stochastic multi-armed
bandit framework where each arm has a tiny probability of realizing large
rewards while with overwhelming probability the reward is zero. A key
application of this framework is in online advertising where click rates of
advertisements could be a fraction of a single percent and final conversion to
sales, while highly profitable, may again be a small fraction of the click
rates. Lately, algorithms for BAI problems have been developed that minimise
sample complexity while providing statistical guarantees on the correct arm
selection. As we observe, these algorithms can be computationally prohibitive.
We exploit the fact that the reward process for each arm is well approximated
by a Compound Poisson process to arrive at algorithms that are faster, with a
small increase in sample complexity. We analyze the problem in an asymptotic
regime as rarity of reward occurrence reduces to zero, and reward amounts
increase to infinity. This helps illustrate the benefits of the proposed
algorithm. It also sheds light on the underlying structure of the optimal BAI
algorithms in the rare event setting.","['Anirban Bhattacharjee', 'Sushant Vijayan', 'Sandeep K Juneja']","['cs.LG', 'stat.ML']",2023-03-14 04:51:24+00:00
http://arxiv.org/abs/2303.07597v1,Fast Regularized Discrete Optimal Transport with Group-Sparse Regularizers,"Regularized discrete optimal transport (OT) is a powerful tool to measure the
distance between two discrete distributions that have been constructed from
data samples on two different domains. While it has a wide range of
applications in machine learning, in some cases the sampled data from only one
of the domains will have class labels such as unsupervised domain adaptation.
In this kind of problem setting, a group-sparse regularizer is frequently
leveraged as a regularization term to handle class labels. In particular, it
can preserve the label structure on the data samples by corresponding the data
samples with the same class label to one group-sparse regularization term. As a
result, we can measure the distance while utilizing label information by
solving the regularized optimization problem with gradient-based algorithms.
However, the gradient computation is expensive when the number of classes or
data samples is large because the number of regularization terms and their
respective sizes also turn out to be large. This paper proposes fast discrete
OT with group-sparse regularizers. Our method is based on two ideas. The first
is to safely skip the computations of the gradients that must be zero. The
second is to efficiently extract the gradients that are expected to be nonzero.
Our method is guaranteed to return the same value of the objective function as
that of the original method. Experiments show that our method is up to 8.6
times faster than the original method without degrading accuracy.","['Yasutoshi Ida', 'Sekitoshi Kanai', 'Kazuki Adachi', 'Atsutoshi Kumagai', 'Yasuhiro Fujiwara']","['cs.LG', 'stat.ML']",2023-03-14 02:40:46+00:00
http://arxiv.org/abs/2303.07487v2,Using VAEs to Learn Latent Variables: Observations on Applications in cryo-EM,"Variational autoencoders (VAEs) are a popular generative model used to
approximate distributions. The encoder part of the VAE is used in amortized
learning of latent variables, producing a latent representation for data
samples. Recently, VAEs have been used to characterize physical and biological
systems. In this case study, we qualitatively examine the amortization
properties of a VAE used in biological applications. We find that in this
application the encoder bears a qualitative resemblance to more traditional
explicit representation of latent variables.","['Daniel G. Edelberg', 'Roy R. Lederman']","['stat.ML', 'cs.LG', 'q-bio.QM']",2023-03-13 21:48:20+00:00
http://arxiv.org/abs/2303.07475v1,General Loss Functions Lead to (Approximate) Interpolation in High Dimensions,"We provide a unified framework, applicable to a general family of convex
losses and across binary and multiclass settings in the overparameterized
regime, to approximately characterize the implicit bias of gradient descent in
closed form. Specifically, we show that the implicit bias is approximated (but
not exactly equal to) the minimum-norm interpolation in high dimensions, which
arises from training on the squared loss. In contrast to prior work which was
tailored to exponentially-tailed losses and used the intermediate
support-vector-machine formulation, our framework directly builds on the
primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new
approximate equivalences for general convex losses through a novel sensitivity
analysis. Our framework also recovers existing exact equivalence results for
exponentially-tailed losses across binary and multiclass settings. Finally, we
provide evidence for the tightness of our techniques, which we use to
demonstrate the effect of certain loss functions designed for
out-of-distribution problems on the closed-form solution.","['Kuo-Wei Lai', 'Vidya Muthukumar']","['stat.ML', 'cs.LG', 'math.OC']",2023-03-13 21:23:12+00:00
http://arxiv.org/abs/2303.07400v1,Tuning support vector machines and boosted trees using optimization algorithms,"Statistical learning methods have been growing in popularity in recent years.
Many of these procedures have parameters that must be tuned for models to
perform well. Research has been extensive in neural networks, but not for many
other learning methods. We looked at the behavior of tuning parameters for
support vector machines, gradient boosting machines, and adaboost in both a
classification and regression setting. We used grid search to identify ranges
of tuning parameters where good models can be found across many different
datasets. We then explored different optimization algorithms to select a model
across the tuning parameter space. Models selected by the optimization
algorithm were compared to the best models obtained through grid search to
select well performing algorithms. This information was used to create an R
package, EZtune, that automatically tunes support vector machines and boosted
trees.",['Jill F. Lundell'],"['stat.ML', 'cs.LG']",2023-03-13 18:26:55+00:00
http://arxiv.org/abs/2303.07392v1,Efficient Bayesian Physics Informed Neural Networks for Inverse Problems via Ensemble Kalman Inversion,"Bayesian Physics Informed Neural Networks (B-PINNs) have gained significant
attention for inferring physical parameters and learning the forward solutions
for problems based on partial differential equations. However, the
overparameterized nature of neural networks poses a computational challenge for
high-dimensional posterior inference. Existing inference approaches, such as
particle-based or variance inference methods, are either computationally
expensive for high-dimensional posterior inference or provide unsatisfactory
uncertainty estimates. In this paper, we present a new efficient inference
algorithm for B-PINNs that uses Ensemble Kalman Inversion (EKI) for
high-dimensional inference tasks. We find that our proposed method can achieve
inference results with informative uncertainty estimates comparable to
Hamiltonian Monte Carlo (HMC)-based B-PINNs with a much reduced computational
cost. These findings suggest that our proposed approach has great potential for
uncertainty quantification in physics-informed machine learning for practical
applications.","['Andrew Pensoneault', 'Xueyu Zhu']","['stat.ML', 'cs.LG']",2023-03-13 18:15:26+00:00
http://arxiv.org/abs/2303.07329v1,Application of targeted maximum likelihood estimation in public health and epidemiological studies: a systematic review,"The Targeted Maximum Likelihood Estimation (TMLE) statistical data analysis
framework integrates machine learning, statistical theory, and statistical
inference to provide a least biased, efficient and robust strategy for
estimation and inference of a variety of statistical and causal parameters. We
describe and evaluate the epidemiological applications that have benefited from
recent methodological developments. We conducted a systematic literature review
in PubMed for articles that applied any form of TMLE in observational studies.
We summarised the epidemiological discipline, geographical location, expertise
of the authors, and TMLE methods over time. We used the Roadmap of Targeted
Learning and Causal Inference to extract key methodological aspects of the
publications. We showcase the contributions to the literature of these TMLE
results. Of the 81 publications included, 25% originated from the University of
California at Berkeley, where the framework was first developed by Professor
Mark van der Laan. By the first half of 2022, 70% of the publications
originated from outside the United States and explored up to 7 different
epidemiological disciplines in 2021-22. Double-robustness, bias reduction and
model misspecification were the main motivations that drew researchers towards
the TMLE framework. Through time, a wide variety of methodological, tutorial
and software-specific articles were cited, owing to the constant growth of
methodological developments around TMLE. There is a clear dissemination trend
of the TMLE framework to various epidemiological disciplines and to increasing
numbers of geographical areas. The availability of R packages, publication of
tutorial papers, and involvement of methodological experts in applied
publications have contributed to an exponential increase in the number of
studies that understood the benefits, and adoption, of TMLE.","['Matthew J. Smith', 'Rachael V. Phillips', 'Miguel Angel Luque-Fernandez', 'Camille Maringe']","['stat.AP', 'stat.ME', 'stat.ML']",2023-03-13 17:50:03+00:00
http://arxiv.org/abs/2303.07287v2,Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm,"In non-asymptotic learning, variance-type parameters of sub-Gaussian
distributions are of paramount importance. However, directly estimating these
parameters using the empirical moment generating function (MGF) is infeasible.
To address this, we suggest using the sub-Gaussian intrinsic moment norm
[Buldygin and Kozachenko (2000), Theorem 1.3] achieved by maximizing a sequence
of normalized moments. Significantly, the suggested norm can not only
reconstruct the exponential moment bounds of MGFs but also provide tighter
sub-Gaussian concentration inequalities. In practice, we provide an intuitive
method for assessing whether data with a finite sample size is sub-Gaussian,
utilizing the sub-Gaussian plot. The intrinsic moment norm can be robustly
estimated via a simple plug-in approach. Our theoretical findings are also
applicable to reinforcement learning, including the multi-armed bandit
scenario.","['Huiming Zhang', 'Haoyu Wei', 'Guang Cheng']","['stat.ML', 'cs.LG', 'econ.EM']",2023-03-13 17:03:19+00:00
http://arxiv.org/abs/2303.07279v1,"Universal coding, intrinsic volumes, and metric complexity","We study sequential probability assignment in the Gaussian setting, where the
goal is to predict, or equivalently compress, a sequence of real-valued
observations almost as well as the best Gaussian distribution with mean
constrained to a given subset of $\mathbf{R}^n$. First, in the case of a convex
constraint set $K$, we express the hardness of the prediction problem (the
minimax regret) in terms of the intrinsic volumes of $K$; specifically, it
equals the logarithm of the Wills functional from convex geometry. We then
establish a comparison inequality for the Wills functional in the general
nonconvex case, which underlines the metric nature of this quantity and
generalizes the Slepian-Sudakov-Fernique comparison principle for the Gaussian
width. Motivated by this inequality, we characterize the exact order of
magnitude of the considered functional for a general nonconvex set, in terms of
global covering numbers and local Gaussian widths. This implies metric
isomorphic estimates for the log-Laplace transform of the intrinsic volume
sequence of a convex body. As part of our analysis, we also characterize the
minimax redundancy for a general constraint set. We finally relate and contrast
our findings with classical asymptotic results in information theory.",['Jaouad Mourtada'],"['cs.IT', 'math.IT', 'math.MG', 'math.ST', 'stat.ML', 'stat.TH', '94A29, 52A39, 62C20, 62B10, 60G15']",2023-03-13 16:54:04+00:00
http://arxiv.org/abs/2303.07170v2,Validation of uncertainty quantification metrics: a primer based on the consistency and adaptivity concepts,"The practice of uncertainty quantification (UQ) validation, notably in
machine learning for the physico-chemical sciences, rests on several graphical
methods (scattering plots, calibration curves, reliability diagrams and
confidence curves) which explore complementary aspects of calibration, without
covering all the desirable ones. For instance, none of these methods deals with
the reliability of UQ metrics across the range of input features (adaptivity).
Based on the complementary concepts of consistency and adaptivity, the toolbox
of common validation methods for variance- and intervals- based UQ metrics is
revisited with the aim to provide a better grasp on their capabilities. This
study is conceived as an introduction to UQ validation, and all methods are
derived from a few basic rules. The methods are illustrated and tested on
synthetic datasets and representative examples extracted from the recent
physico-chemical machine learning UQ literature.",['Pascal Pernot'],"['physics.chem-ph', 'physics.data-an', 'stat.ML']",2023-03-13 15:13:03+00:00
http://arxiv.org/abs/2303.07167v2,When Respondents Don't Care Anymore: Identifying the Onset of Careless Responding,"Questionnaires in the behavioral and organizational sciences tend to be
lengthy: survey measures comprising hundreds of items are the norm rather than
the exception. However, literature suggests that the longer a questionnaire
takes, the higher the probability that participants lose interest and start
responding carelessly. Consequently, in long surveys a large number of
participants may engage in careless responding, posing a major threat to
internal validity. We propose a novel method for identifying the onset of
careless responding (or an absence thereof) for each participant. It is based
on combined measurements of multiple dimensions in which carelessness may
manifest, such as inconsistency and invariability. Since a structural break in
either dimension is potentially indicative of carelessness, the proposed method
searches for evidence for changepoints along the combined measurements. It is
highly flexible, based on machine learning, and provides statistical guarantees
on its performance. An empirical application on data from a seminal study on
the incidence of careless responding reveals that the reported incidence has
likely been substantially underestimated due to the presence of respondents
that were careless for only parts of the questionnaire. In simulation
experiments, we find that the proposed method achieves high reliability in
correctly identifying carelessness onset, discriminates well between careless
and attentive respondents, and captures a variety of careless response types,
even when a large number of careless respondents are present. Furthermore, we
provide freely available open source software to enhance accessibility and
facilitate adoption by empirical researchers.","['Max Welz', 'Andreas Alfons']","['stat.ME', 'stat.AP', 'stat.ML']",2023-03-13 15:10:30+00:00
http://arxiv.org/abs/2303.07160v2,Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond,"We study convergence lower bounds of without-replacement stochastic gradient
descent (SGD) for solving smooth (strongly-)convex finite-sum minimization
problems. Unlike most existing results focusing on final iterate lower bounds
in terms of the number of components $n$ and the number of epochs $K$, we seek
bounds for arbitrary weighted average iterates that are tight in all factors
including the condition number $\kappa$. For SGD with Random Reshuffling, we
present lower bounds that have tighter $\kappa$ dependencies than existing
bounds. Our results are the first to perfectly close the gap between lower and
upper bounds for weighted average iterates in both strongly-convex and convex
cases. We also prove weighted average iterate lower bounds for arbitrary
permutation-based SGD, which apply to all variants that carefully choose the
best permutation. Our bounds improve the existing bounds in factors of $n$ and
$\kappa$ and thereby match the upper bounds shown for a recently proposed
algorithm called GraB.","['Jaeyoung Cha', 'Jaewook Lee', 'Chulhee Yun']","['cs.LG', 'math.OC', 'stat.ML']",2023-03-13 14:35:55+00:00
http://arxiv.org/abs/2303.07154v3,Differential Good Arm Identification,"This paper targets a variant of the stochastic multi-armed bandit problem
called good arm identification (GAI). GAI is a pure-exploration bandit problem
with the goal to output as many good arms using as few samples as possible,
where a good arm is defined as an arm whose expected reward is greater than a
given threshold. In this work, we propose DGAI - a differentiable good arm
identification algorithm to improve the sample complexity of the
state-of-the-art HDoC algorithm in a data-driven fashion. We also showed that
the DGAI can further boost the performance of a general multi-arm bandit (MAB)
problem given a threshold as a prior knowledge to the arm set. Extensive
experiments confirm that our algorithm outperform the baseline algorithms
significantly in both synthetic and real world datasets for both GAI and MAB
tasks.","['Yun-Da Tsai', 'Tzu-Hsien Tsai', 'Shou-De Lin']","['cs.LG', 'stat.ML']",2023-03-13 14:28:21+00:00
http://arxiv.org/abs/2303.07152v1,Score Attack: A Lower Bound Technique for Optimal Differentially Private Learning,"Achieving optimal statistical performance while ensuring the privacy of
personal data is a challenging yet crucial objective in modern data analysis.
However, characterizing the optimality, particularly the minimax lower bound,
under privacy constraints is technically difficult.
  To address this issue, we propose a novel approach called the score attack,
which provides a lower bound on the differential-privacy-constrained minimax
risk of parameter estimation. The score attack method is based on the tracing
attack concept in differential privacy and can be applied to any statistical
model with a well-defined score statistic. It can optimally lower bound the
minimax risk of estimating unknown model parameters, up to a logarithmic
factor, while ensuring differential privacy for a range of statistical
problems. We demonstrate the effectiveness and optimality of this general
method in various examples, such as the generalized linear model in both
classical and high-dimensional sparse settings, the Bradley-Terry-Luce model
for pairwise comparisons, and nonparametric regression over the Sobolev class.","['T. Tony Cai', 'Yichen Wang', 'Linjun Zhang']","['math.ST', 'cs.CR', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH', '62F30, 62J12, 62G05']",2023-03-13 14:26:27+00:00
http://arxiv.org/abs/2303.07139v2,Comparing statistical and machine learning methods for time series forecasting in data-driven logistics -- A simulation study,"Many planning and decision activities in logistics and supply chain
management are based on forecasts of multiple time dependent factors.
Therefore, the quality of planning depends on the quality of the forecasts. We
compare various forecasting methods in terms of out of the box forecasting
performance on a broad set of simulated time series. We simulate various linear
and non-linear time series and look at the one step forecast performance of
statistical learning methods.","['Lena Schmid', 'Moritz Roidl', 'Markus Pauly']","['stat.ML', 'cs.LG']",2023-03-13 14:05:19+00:00
http://arxiv.org/abs/2303.07109v1,Transformer-based World Models Are Happy With 100k Interactions,"Deep neural networks have been successful in many reinforcement learning
settings. However, compared to human learners they are overly data hungry. To
build a sample-efficient world model, we apply a transformer to real-world
episodes in an autoregressive manner: not only the compact latent states and
the taken actions but also the experienced or predicted rewards are fed into
the transformer, so that it can attend flexibly to all three modalities at
different time steps. The transformer allows our world model to access previous
states directly, instead of viewing them through a compressed recurrent state.
By utilizing the Transformer-XL architecture, it is able to learn long-term
dependencies while staying computationally efficient. Our transformer-based
world model (TWM) generates meaningful, new experience, which is used to train
a policy that outperforms previous model-free and model-based reinforcement
learning algorithms on the Atari 100k benchmark.","['Jan Robine', 'Marc HÃ¶ftmann', 'Tobias Uelwer', 'Stefan Harmeling']","['cs.LG', 'cs.AI', 'stat.ML']",2023-03-13 13:43:59+00:00
http://arxiv.org/abs/2303.07874v1,Bayes Complexity of Learners vs Overfitting,"We introduce a new notion of complexity of functions and we show that it has
the following properties: (i) it governs a PAC Bayes-like generalization bound,
(ii) for neural networks it relates to natural notions of complexity of
functions (such as the variation), and (iii) it explains the generalization gap
between neural networks and linear schemes. While there is a large set of
papers which describes bounds that have each such property in isolation, and
even some that have two, as far as we know, this is a first notion that
satisfies all three of them. Moreover, in contrast to previous works, our
notion naturally generalizes to neural networks with several layers.
  Even though the computation of our complexity is nontrivial in general, an
upper-bound is often easy to derive, even for higher number of layers and
functions with structure, such as period functions. An upper-bound we derive
allows to show a separation in the number of samples needed for good
generalization between 2 and 4-layer neural networks for periodic functions.","['Grzegorz GÅuch', 'Rudiger Urbanke']","['cs.LG', 'stat.ML']",2023-03-13 13:07:02+00:00
http://arxiv.org/abs/2303.07053v1,Bandit-supported care planning for older people with complex health and care needs,"Long-term care service for old people is in great demand in most of the aging
societies. The number of nursing homes residents is increasing while the number
of care providers is limited. Due to the care worker shortage, care to
vulnerable older residents cannot be fully tailored to the unique needs and
preference of each individual. This may bring negative impacts on health
outcomes and quality of life among institutionalized older people. To improve
care quality through personalized care planning and delivery with limited care
workforce, we propose a new care planning model assisted by artificial
intelligence. We apply bandit algorithms which optimize the clinical decision
for care planning by adapting to the sequential feedback from the past
decisions. We evaluate the proposed model on empirical data acquired from the
Systems for Person-centered Elder Care (SPEC) study, a ICT-enhanced care
management program.","['Gi-Soo Kim', 'Young Suh Hong', 'Tae Hoon Lee', 'Myunghee Cho Paik', 'Hongsoo Kim']","['stat.ML', 'cs.LG']",2023-03-13 12:22:38+00:00
http://arxiv.org/abs/2303.06993v1,Actor-Critic learning for mean-field control in continuous time,"We study policy gradient for mean-field control in continuous time in a
reinforcement learning setting. By considering randomised policies with entropy
regularisation, we derive a gradient expectation representation of the value
function, which is amenable to actor-critic type algorithms, where the value
functions and the policies are learnt alternately based on observation samples
of the state and model-free estimation of the population state distribution,
either by offline or online learning. In the linear-quadratic mean-field
framework, we obtain an exact parametrisation of the actor and critic functions
defined on the Wasserstein space. Finally, we illustrate the results of our
algorithms with some numerical experiments on concrete examples.","['Noufel Frikha', 'Maximilien Germain', 'Mathieu LauriÃ¨re', 'HuyÃªn Pham', 'Xuanye Song']","['stat.ML', 'math.OC']",2023-03-13 10:49:25+00:00
http://arxiv.org/abs/2303.06992v1,Improving Mutual Information Estimation with Annealed and Energy-Based Bounds,"Mutual information (MI) is a fundamental quantity in information theory and
machine learning. However, direct estimation of MI is intractable, even if the
true joint probability density for the variables of interest is known, as it
involves estimating a potentially high-dimensional log partition function. In
this work, we present a unifying view of existing MI bounds from the
perspective of importance sampling, and propose three novel bounds based on
this approach. Since accurate estimation of MI without density information
requires a sample size exponential in the true MI, we assume either a single
marginal or the full joint density information is known. In settings where the
full joint density is available, we propose Multi-Sample Annealed Importance
Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large
values of MI in our experiments. In settings where only a single marginal
distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds.
Our GIWAE bound unifies variational and contrastive bounds in a single
framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our
MINE-AIS method improves upon existing energy-based methods such as MINE-DV and
MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC
sampling to estimate gradients for training and Multi-Sample AIS for evaluating
the bound. Our methods are particularly suitable for evaluating MI in deep
generative models, since explicit forms of the marginal or joint densities are
often available. We evaluate our bounds on estimating the MI of VAEs and GANs
trained on the MNIST and CIFAR datasets, and showcase significant gains over
existing bounds in these challenging settings with high ground truth MI.","['Rob Brekelmans', 'Sicong Huang', 'Marzyeh Ghassemi', 'Greg Ver Steeg', 'Roger Grosse', 'Alireza Makhzani']","['cs.LG', 'stat.ML']",2023-03-13 10:47:24+00:00
http://arxiv.org/abs/2303.06966v1,A new methodology to predict the oncotype scores based on clinico-pathological data with similar tumor profiles,"Introduction: The Oncotype DX (ODX) test is a commercially available
molecular test for breast cancer assay that provides prognostic and predictive
breast cancer recurrence information for hormone positive, HER2-negative
patients. The aim of this study is to propose a novel methodology to assist
physicians in their decision-making. Methods: A retrospective study between
2012 and 2020 with 333 cases that underwent an ODX assay from three hospitals
in Bourgogne Franche-Comt{\'e} was conducted. Clinical and pathological reports
were used to collect the data. A methodology based on distributional random
forest was developed using 9 clinico-pathological characteristics. This
methodology can be used particularly to identify the patients of the training
cohort that share similarities with the new patient and to predict an estimate
of the distribution of the ODX score. Results: The mean age of participants id
56.9 years old. We have correctly classified 92% of patients in low risk and
40.2% of patients in high risk. The overall accuracy is 79.3%. The proportion
of low risk correct predicted value (PPV) is 82%. The percentage of high risk
correct predicted value (NPV) is approximately 62.3%. The F1-score and the Area
Under Curve (AUC) are of 0.87 and 0.759, respectively. Conclusion: The proposed
methodology makes it possible to predict the distribution of the ODX score for
a patient and provides an explanation of the predicted score. The use of the
methodology with the pathologist's expertise on the different histological and
immunohistochemical characteristics has a clinical impact to help oncologist in
decision-making regarding breast cancer therapy.","['Zeina Al Masry', 'Romain Pic', 'ClÃ©ment Dombry', 'Christine Devalland']","['stat.AP', 'q-bio.QM', 'stat.ML']",2023-03-13 10:08:13+00:00
http://arxiv.org/abs/2303.06825v2,Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm,"The linear bandit problem has been studied for many years in both stochastic
and adversarial settings. Designing an algorithm that can optimize the
environment without knowing the loss type attracts lots of interest.
\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and
then switches between different algorithms specially designed for specific
settings. However, such an approach requires meticulous designs to perform well
in all environments. Follow-the-regularized-leader (FTRL) is another type of
popular algorithm that can adapt to different environments. This algorithm is
of simple design and the regret bounds are shown to be optimal in traditional
multi-armed bandit problems compared with the detect-switch type. Designing an
FTRL-type algorithm for linear bandits is an important question that has been
open for a long time. In this paper, we prove that the FTRL algorithm with a
negative entropy regularizer can achieve the best-of-three-world results for
the linear bandit problem. Our regret bounds achieve the same or nearly the
same order as the previous detect-switch type algorithm but with a much simpler
algorithmic design.","['Fang Kong', 'Canzhe Zhao', 'Shuai Li']","['cs.LG', 'stat.ML']",2023-03-13 02:50:59+00:00
http://arxiv.org/abs/2303.06815v3,"On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee","Model compression is a crucial part of deploying neural networks (NNs),
especially when the memory and storage of computing devices are limited in many
applications. This paper focuses on two model compression techniques: low-rank
approximation and weight pruning in neural networks, which are very popular
nowadays. However, training NN with low-rank approximation and weight pruning
always suffers significant accuracy loss and convergence issues. In this paper,
a holistic framework is proposed for model compression from a novel perspective
of nonconvex optimization by designing an appropriate objective function. Then,
we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the
nonconvex optimization. One advantage of our algorithm is that an efficient
iteration scheme can be derived with closed-form, which is gradient-free.
Therefore, our algorithm will not suffer from vanishing/exploding gradient
problems. Furthermore, with the Kurdyka-{\L}ojasiewicz (K{\L}) property of our
objective function, we show that our algorithm globally converges to a critical
point at the rate of O(1/k), where k denotes the number of iterations. Lastly,
extensive experiments with tensor train decomposition and weight pruning
demonstrate the efficiency and superior performance of the proposed framework.
Our code implementation is available at https://github.com/ChenyangLi-97/NN-BCD","['Chenyang Li', 'Jihoon Chung', 'Mengnan Du', 'Haimin Wang', 'Xianlian Zhou', 'Bo Shen']","['cs.LG', 'stat.ML']",2023-03-13 02:14:42+00:00
http://arxiv.org/abs/2303.06726v1,Global Optimality of Elman-type RNN in the Mean-Field Regime,"We analyze Elman-type Recurrent Reural Networks (RNNs) and their training in
the mean-field regime. Specifically, we show convergence of gradient descent
training dynamics of the RNN to the corresponding mean-field formulation in the
large width limit. We also show that the fixed points of the limiting
infinite-width dynamics are globally optimal, under some assumptions on the
initialization of the weights. Our results establish optimality for
feature-learning with wide RNNs in the mean-field regime","['Andrea Agazzi', 'Jianfeng Lu', 'Sayan Mukherjee']","['stat.ML', 'cs.LG']",2023-03-12 18:44:29+00:00
http://arxiv.org/abs/2303.06614v4,Synthetic Experience Replay,"A key theme in the past decade has been that when large neural networks and
large datasets combine they can produce remarkable results. In deep
reinforcement learning (RL), this paradigm is commonly made possible through
experience replay, whereby a dataset of past experiences is used to train a
policy or value function. However, unlike in supervised or self-supervised
learning, an RL agent has to collect its own data, which is often limited.
Thus, it is challenging to reap the benefits of deep learning, and even small
neural networks can overfit at the start of training. In this work, we leverage
the tremendous recent progress in generative modeling and propose Synthetic
Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an
agent's collected experience. We show that SynthER is an effective method for
training RL agents across offline and online settings, in both proprioceptive
and pixel-based environments. In offline settings, we observe drastic
improvements when upsampling small offline datasets and see that additional
synthetic data also allows us to effectively train larger networks.
Furthermore, SynthER enables online agents to train with a much higher
update-to-data ratio than before, leading to a significant increase in sample
efficiency, without any algorithmic changes. We believe that synthetic training
data could open the door to realizing the full potential of deep learning for
replay-based RL algorithms from limited data. Finally, we open-source our code
at https://github.com/conglu1997/SynthER.","['Cong Lu', 'Philip J. Ball', 'Yee Whye Teh', 'Jack Parker-Holder']","['cs.LG', 'cs.AI', 'stat.ML']",2023-03-12 09:10:45+00:00
http://arxiv.org/abs/2303.06562v2,ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond,"Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks
(GNNs) and Transformers, where performance worsens as the number of layers
increases. Instead of characterizing oversmoothing from the view of complete
collapse in which representations converge to a single point, we dive into a
more general perspective of dimensional collapse in which representations lie
in a narrow cone. Accordingly, inspired by the effectiveness of contrastive
learning in preventing dimensional collapse, we propose a novel normalization
layer called ContraNorm. Intuitively, ContraNorm implicitly shatters
representations in the embedding space, leading to a more uniform distribution
and a slighter dimensional collapse. On the theoretical analysis, we prove that
ContraNorm can alleviate both complete collapse and dimensional collapse under
certain conditions. Our proposed normalization layer can be easily integrated
into GNNs and Transformers with negligible parameter overhead. Experiments on
various real-world datasets demonstrate the effectiveness of our proposed
ContraNorm. Our implementation is available at
https://github.com/PKU-ML/ContraNorm.","['Xiaojun Guo', 'Yifei Wang', 'Tianqi Du', 'Yisen Wang']","['cs.LG', 'cs.CV', 'stat.ML']",2023-03-12 04:04:51+00:00
http://arxiv.org/abs/2303.06561v2,Phase Diagram of Initial Condensation for Two-layer Neural Networks,"The phenomenon of distinct behaviors exhibited by neural networks under
varying scales of initialization remains an enigma in deep learning research.
In this paper, based on the earlier work by Luo et al.~\cite{luo2021phase}, we
present a phase diagram of initial condensation for two-layer neural networks.
Condensation is a phenomenon wherein the weight vectors of neural networks
concentrate on isolated orientations during the training process, and it is a
feature in non-linear learning process that enables neural networks to possess
better generalization abilities. Our phase diagram serves to provide a
comprehensive understanding of the dynamical regimes of neural networks and
their dependence on the choice of hyperparameters related to initialization.
Furthermore, we demonstrate in detail the underlying mechanisms by which small
initialization leads to condensation at the initial training stage.","['Zhengan Chen', 'Yuqing Li', 'Tao Luo', 'Zhangchen Zhou', 'Zhi-Qin John Xu']","['cs.LG', 'cond-mat.dis-nn', 'math.OC', 'stat.ML', '68U99, 90C26, 34A45']",2023-03-12 03:55:38+00:00
http://arxiv.org/abs/2303.06526v1,Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback,"We study the adversarial online learning problem and create a completely
online algorithmic framework that has data dependent regret guarantees in both
full expert feedback and bandit feedback settings. We study the expected
performance of our algorithm against general comparators, which makes it
applicable for a wide variety of problem scenarios. Our algorithm works from a
universal prediction perspective and the performance measure used is the
expected regret against arbitrary comparator sequences, which is the difference
between our losses and a competing loss sequence. The competition class can be
designed to include fixed arm selections, switching bandits, contextual
bandits, periodic bandits or any other competition of interest. The sequences
in the competition class are generally determined by the specific application
at hand and should be designed accordingly. Our algorithm neither uses nor
needs any preliminary information about the loss sequences and is completely
online. Its performance bounds are data dependent, where any affine transform
of the losses has no effect on the normalized regret.","['Kaan Gokcesu', 'Hakan Gokcesu']","['cs.LG', 'cs.IT', 'eess.SP', 'math.IT', 'math.OC', 'stat.ML']",2023-03-12 00:18:46+00:00
http://arxiv.org/abs/2303.06484v2,Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap,"The neural collapse (NC) phenomenon describes an underlying geometric
symmetry for deep neural networks, where both deeply learned features and
classifiers converge to a simplex equiangular tight frame. It has been shown
that both cross-entropy loss and mean square error can provably lead to NC. We
remove NC's key assumption on the feature dimension and the number of classes,
and then present a generalized neural collapse (GNC) hypothesis that
effectively subsumes the original NC. Inspired by how NC characterizes the
training target of neural networks, we decouple GNC into two objectives:
minimal intra-class variability and maximal inter-class separability. We then
use hyperspherical uniformity (which characterizes the degree of uniformity on
the unit hypersphere) as a unified framework to quantify these two objectives.
Finally, we propose a general objective -- hyperspherical uniformity gap (HUG),
which is defined by the difference between inter-class and intra-class
hyperspherical uniformity. HUG not only provably converges to GNC, but also
decouples GNC into two separate objectives. Unlike cross-entropy loss that
couples intra-class compactness and inter-class separability, HUG enjoys more
flexibility and serves as a good alternative loss function. Empirical results
show that HUG works well in terms of generalization and robustness.","['Weiyang Liu', 'Longhui Yu', 'Adrian Weller', 'Bernhard SchÃ¶lkopf']","['cs.LG', 'cs.CV', 'stat.ML']",2023-03-11 19:33:24+00:00
