id,title,abstract,authors,categories,date
http://arxiv.org/abs/1606.07845v1,Robust and scalable Bayesian analysis of spatial neural tuning function data,"A common analytical problem in neuroscience is the interpretation of neural
activity with respect to sensory input or behavioral output. This is typically
achieved by regressing measured neural activity against known stimuli or
behavioral variables to produce a ""tuning function"" for each neuron.
Unfortunately, because this approach handles neurons individually, it cannot
take advantage of simultaneous measurements from spatially adjacent neurons
that often have similar tuning properties. On the other hand, sharing
information between adjacent neurons can errantly degrade estimates of tuning
functions across space if there are sharp discontinuities in tuning between
nearby neurons. In this paper, we develop a computationally efficient block
Gibbs sampler that effectively pools information between neurons to de-noise
tuning function estimates while simultaneously preserving sharp discontinuities
that might exist in the organization of tuning across space. This method is
fully Bayesian and its computational cost per iteration scales
sub-quadratically with total parameter dimensionality. We demonstrate the
robustness and scalability of this approach by applying it to both real and
synthetic datasets. In particular, an application to data from the spinal cord
illustrates that the proposed methods can dramatically decrease the
experimental time required to accurately estimate tuning functions.","['Kamiar Rahnama Rad', 'Timothy A. Machado', 'Liam Paninski']","['stat.CO', 'stat.ML']",2016-06-24 22:15:19+00:00
http://arxiv.org/abs/1606.07840v1,Modeling Group Dynamics Using Probabilistic Tensor Decompositions,"We propose a probabilistic modeling framework for learning the dynamic
patterns in the collective behaviors of social agents and developing profiles
for different behavioral groups, using data collected from multiple information
sources. The proposed model is based on a hierarchical Bayesian process, in
which each observation is a finite mixture of an set of latent groups and the
mixture proportions (i.e., group probabilities) are drawn randomly. Each group
is associated with some distributions over a finite set of outcomes. Moreover,
as time evolves, the structure of these groups also changes; we model the
change in the group structure by a hidden Markov model (HMM) with a fixed
transition probability. We present an efficient inference method based on
tensor decompositions and the expectation-maximization (EM) algorithm for
parameter estimation.","['Lin Li', 'Ananthram Swami', 'Anna Scaglione']",['stat.ML'],2016-06-24 21:51:24+00:00
http://arxiv.org/abs/1606.07792v1,Wide & Deep Learning for Recommender Systems,"Generalized linear models with nonlinear feature transformations are widely
used for large-scale regression and classification problems with sparse inputs.
Memorization of feature interactions through a wide set of cross-product
feature transformations are effective and interpretable, while generalization
requires more feature engineering effort. With less feature engineering, deep
neural networks can generalize better to unseen feature combinations through
low-dimensional dense embeddings learned for the sparse features. However, deep
neural networks with embeddings can over-generalize and recommend less relevant
items when the user-item interactions are sparse and high-rank. In this paper,
we present Wide & Deep learning---jointly trained wide linear models and deep
neural networks---to combine the benefits of memorization and generalization
for recommender systems. We productionized and evaluated the system on Google
Play, a commercial mobile app store with over one billion active users and over
one million apps. Online experiment results show that Wide & Deep significantly
increased app acquisitions compared with wide-only and deep-only models. We
have also open-sourced our implementation in TensorFlow.","['Heng-Tze Cheng', 'Levent Koc', 'Jeremiah Harmsen', 'Tal Shaked', 'Tushar Chandra', 'Hrishi Aradhye', 'Glen Anderson', 'Greg Corrado', 'Wei Chai', 'Mustafa Ispir', 'Rohan Anil', 'Zakaria Haque', 'Lichan Hong', 'Vihan Jain', 'Xiaobing Liu', 'Hemal Shah']","['cs.LG', 'cs.IR', 'stat.ML']",2016-06-24 19:07:02+00:00
http://arxiv.org/abs/1606.07781v1,Harnessing the Power of the Crowd to Increase Capacity for Data Science in the Social Sector,"We present three case studies of organizations using a data science
competition to answer a pressing question. The first is in education where a
nonprofit that creates smart school budgets wanted to automatically tag budget
line items. The second is in public health, where a low-cost, nonprofit women's
health care provider wanted to understand the effect of demographic and
behavioral questions on predicting which services a woman would need. The third
and final example is in government innovation: using online restaurant reviews
from Yelp, competitors built models to forecast which restaurants were most
likely to have hygiene violations when visited by health inspectors. Finally,
we reflect on the unique benefits of the open, public competition model.","['Peter Bull', 'Isaac Slavitt', 'Greg Lipstein']","['cs.HC', 'cs.CY', 'cs.SI', 'stat.ML']",2016-06-24 18:30:35+00:00
http://arxiv.org/abs/1606.07686v2,Gamblets for opening the complexity-bottleneck of implicit schemes for hyperbolic and parabolic ODEs/PDEs with rough coefficients,"Implicit schemes are popular methods for the integration of time dependent
PDEs such as hyperbolic and parabolic PDEs. However the necessity to solve
corresponding linear systems at each time step constitutes a complexity
bottleneck in their application to PDEs with rough coefficients. We present a
generalization of gamblets introduced in \cite{OwhadiMultigrid:2015} enabling
the resolution of these implicit systems in near-linear complexity and provide
rigorous a-priori error bounds on the resulting numerical approximations of
hyperbolic and parabolic PDEs. These generalized gamblets induce a
multiresolution decomposition of the solution space that is adapted to both the
underlying (hyperbolic and parabolic) PDE (and the system of ODEs resulting
from space discretization) and to the time-steps of the numerical scheme.","['Houman Owhadi', 'Lei Zhang']","['math.NA', 'stat.ML', '65T60, 65N55, 65N75, 62C99, 42C40, 62M86']",2016-06-24 13:56:45+00:00
http://arxiv.org/abs/1606.07636v3,Is the Bellman residual a bad proxy?,"This paper aims at theoretically and empirically comparing two standard
optimization criteria for Reinforcement Learning: i) maximization of the mean
value and ii) minimization of the Bellman residual. For that purpose, we place
ourselves in the framework of policy search algorithms, that are usually
designed to maximize the mean value, and derive a method that minimizes the
residual $\|T_* v_\pi - v_\pi\|_{1,\nu}$ over policies. A theoretical analysis
shows how good this proxy is to policy optimization, and notably that it is
better than its value-based counterpart. We also propose experiments on
randomly generated generic Markov decision processes, specifically designed for
studying the influence of the involved concentrability coefficient. They show
that the Bellman residual is generally a bad proxy to policy optimization and
that directly maximizing the mean value is much better, despite the current
lack of deep theoretical analysis. This might seem obvious, as directly
addressing the problem of interest is usually better, but given the prevalence
of (projected) Bellman residual minimization in value-based reinforcement
learning, we believe that this question is worth to be considered.","['Matthieu Geist', 'Bilal Piot', 'Olivier Pietquin']","['cs.LG', 'stat.ML']",2016-06-24 10:54:41+00:00
http://arxiv.org/abs/1606.07578v1,Regression Trees and Random forest based feature selection for malaria risk exposure prediction,"This paper deals with prediction of anopheles number, the main vector of
malaria risk, using environmental and climate variables. The variables
selection is based on an automatic machine learning method using regression
trees, and random forests combined with stratified two levels cross validation.
The minimum threshold of variables importance is accessed using the quadratic
distance of variables importance while the optimal subset of selected variables
is used to perform predictions. Finally the results revealed to be
qualitatively better, at the selection, the prediction , and the CPU time point
of view than those obtained by GLM-Lasso method.",['Bienvenue Kouwayè'],"['stat.ML', 'cs.LG']",2016-06-24 06:34:17+00:00
http://arxiv.org/abs/1606.07575v1,Multipartite Ranking-Selection of Low-Dimensional Instances by Supervised Projection to High-Dimensional Space,"Pruning of redundant or irrelevant instances of data is a key to every
successful solution for pattern recognition. In this paper, we present a novel
ranking-selection framework for low-length but highly correlated instances.
Instead of working in the low-dimensional instance space, we learn a supervised
projection to high-dimensional space spanned by the number of classes in the
dataset under study. Imposing higher distinctions via exposing the notion of
labels to the instances, lets to deploy one versus all ranking for each
individual classes and selecting quality instances via adaptive thresholding of
the overall scores. To prove the efficiency of our paradigm, we employ it for
the purpose of texture understanding which is a hard recognition challenge due
to high similarity of texture pixels and low dimensionality of their color
features. Our experiments show considerable improvements in recognition
performance over other local descriptors on several publicly available
datasets.",['Arash Shahriari'],"['stat.ML', 'cs.CV', 'cs.LG']",2016-06-24 06:15:45+00:00
http://arxiv.org/abs/1606.07545v1,Interactive Semantic Featuring for Text Classification,"In text classification, dictionaries can be used to define
human-comprehensible features. We propose an improvement to dictionary features
called smoothed dictionary features. These features recognize document contexts
instead of n-grams. We describe a principled methodology to solicit dictionary
features from a teacher, and present results showing that models built using
these human-comprehensible features are competitive with models trained with
Bag of Words features.","['Camille Jandot', 'Patrice Simard', 'Max Chickering', 'David Grangier', 'Jina Suh']","['cs.CL', 'stat.ML']",2016-06-24 02:28:24+00:00
http://arxiv.org/abs/1606.07470v1,NN-grams: Unifying neural network and n-gram language models for Speech Recognition,"We present NN-grams, a novel, hybrid language model integrating n-grams and
neural networks (NN) for speech recognition. The model takes as input both word
histories as well as n-gram counts. Thus, it combines the memorization capacity
and scalability of an n-gram model with the generalization ability of neural
networks. We report experiments where the model is trained on 26B words.
NN-grams are efficient at run-time since they do not include an output soft-max
layer. The model is trained using noise contrastive estimation (NCE), an
approach that transforms the estimation problem of neural networks into one of
binary classification between data samples and noise samples. We present
results with noise samples derived from either an n-gram distribution or from
speech recognition lattices. NN-grams outperforms an n-gram model on an Italian
speech recognition dictation task.","['Babak Damavandi', 'Shankar Kumar', 'Noam Shazeer', 'Antoine Bruguier']","['cs.CL', 'stat.ML']",2016-06-23 20:37:06+00:00
http://arxiv.org/abs/1606.07365v1,Parallel SGD: When does averaging help?,"Consider a number of workers running SGD independently on the same pool of
data and averaging the models every once in a while -- a common but not well
understood practice. We study model averaging as a variance-reducing mechanism
and describe two ways in which the frequency of averaging affects convergence.
For convex objectives, we show the benefit of frequent averaging depends on the
gradient variance envelope. For non-convex objectives, we illustrate that this
benefit depends on the presence of multiple globally optimal points. We
complement our findings with multicore experiments on both synthetic and real
data.","['Jian Zhang', 'Christopher De Sa', 'Ioannis Mitliagkas', 'Christopher Ré']","['stat.ML', 'cs.LG']",2016-06-23 16:23:35+00:00
http://arxiv.org/abs/1606.07326v3,DropNeuron: Simplifying the Structure of Deep Neural Networks,"Deep learning using multi-layer neural networks (NNs) architecture manifests
superb power in modern machine learning systems. The trained Deep Neural
Networks (DNNs) are typically large. The question we would like to address is
whether it is possible to simplify the NN during training process to achieve a
reasonable performance within an acceptable computational time. We presented a
novel approach of optimising a deep neural network through regularisation of
net- work architecture. We proposed regularisers which support a simple
mechanism of dropping neurons during a network training process. The method
supports the construction of a simpler deep neural networks with compatible
performance with its simplified version. As a proof of concept, we evaluate the
proposed method with examples including sparse linear regression, deep
autoencoder and convolutional neural network. The valuations demonstrate
excellent performance.
  The code for this work can be found in
http://www.github.com/panweihit/DropNeuron","['Wei Pan', 'Hao Dong', 'Yike Guo']","['cs.CV', 'cs.LG', 'stat.ML']",2016-06-23 14:30:36+00:00
http://arxiv.org/abs/1606.07312v1,Unsupervised preprocessing for Tactile Data,"Tactile information is important for gripping, stable grasp, and in-hand
manipulation, yet the complexity of tactile data prevents widespread use of
such sensors. We make use of an unsupervised learning algorithm that transforms
the complex tactile data into a compact, latent representation without the need
to record ground truth reference data. These compact representations can either
be used directly in a reinforcement learning based controller or can be used to
calibrate the tactile sensor to physical quantities with only a few datapoints.
We show the quality of our latent representation by predicting important
features and with a simple control task.","['Maximilian Karl', 'Justin Bayer', 'Patrick van der Smagt']","['cs.RO', 'cs.LG', 'stat.ML']",2016-06-23 13:44:28+00:00
http://arxiv.org/abs/1606.07298v1,Explaining Predictions of Non-Linear Classifiers in NLP,"Layer-wise relevance propagation (LRP) is a recently proposed technique for
explaining predictions of complex non-linear classifiers in terms of input
variables. In this paper, we apply LRP for the first time to natural language
processing (NLP). More precisely, we use it to explain the predictions of a
convolutional neural network (CNN) trained on a topic categorization task. Our
analysis highlights which words are relevant for a specific prediction of the
CNN. We compare our technique to standard sensitivity analysis, both
qualitatively and quantitatively, using a ""word deleting"" perturbation
experiment, a PCA analysis, and various visualizations. All experiments
validate the suitability of LRP for explaining the CNN predictions, which is
also in line with results reported in recent image classification studies.","['Leila Arras', 'Franziska Horn', 'Grégoire Montavon', 'Klaus-Robert Müller', 'Wojciech Samek']","['cs.CL', 'cs.IR', 'cs.LG', 'cs.NE', 'stat.ML']",2016-06-23 12:53:31+00:00
http://arxiv.org/abs/1606.07289v1,Non-convex regularization in remote sensing,"In this paper, we study the effect of different regularizers and their
implications in high dimensional image classification and sparse linear
unmixing. Although kernelization or sparse methods are globally accepted
solutions for processing data in high dimensions, we present here a study on
the impact of the form of regularization used and its parametrization. We
consider regularization via traditional squared (2) and sparsity-promoting (1)
norms, as well as more unconventional nonconvex regularizers (p and Log Sum
Penalty). We compare their properties and advantages on several classification
and linear unmixing tasks and provide advices on the choice of the best
regularizer for the problem at hand. Finally, we also provide a fully
functional toolbox for the community.","['Devis Tuia', 'Remi Flamary', 'Michel Barlaud']","['stat.ML', 'cs.LG']",2016-06-23 12:36:01+00:00
http://arxiv.org/abs/1606.07285v2,Identifying individual facial expressions by deconstructing a neural network,"This paper focuses on the problem of explaining predictions of psychological
attributes such as attractiveness, happiness, confidence and intelligence from
face photographs using deep neural networks. Since psychological attribute
datasets typically suffer from small sample sizes, we apply transfer learning
with two base models to avoid overfitting. These models were trained on an age
and gender prediction task, respectively. Using a novel explanation method we
extract heatmaps that highlight the parts of the image most responsible for the
prediction. We further observe that the explanation method provides important
insights into the nature of features of the base model, which allow one to
assess the aptitude of the base model for a given transfer learning task.
Finally, we observe that the multiclass model is more feature rich than its
binary counterpart. The experimental evaluation is performed on the 2222 images
from the 10k US faces dataset containing psychological attribute labels as well
as on a subset of KDEF images.","['Farhad Arbabzadah', 'Grégoire Montavon', 'Klaus-Robert Müller', 'Wojciech Samek']","['cs.CV', 'cs.NE', 'stat.ML']",2016-06-23 12:24:45+00:00
http://arxiv.org/abs/1606.07282v5,A review of Gaussian Markov models for conditional independence,"Markov models lie at the interface between statistical independence in a
probability distribution and graph separation properties. We review model
selection and estimation in directed and undirected Markov models with Gaussian
parametrization, emphasizing the main similarities and differences. These two
model classes are similar but not equivalent, although they share a common
intersection. We present the existing results from a historical perspective,
taking into account the amount of literature existing from both the artificial
intelligence and statistics research communities, where these models were
originated. We cover classical topics such as maximum likelihood estimation and
model selection via hypothesis testing, but also more modern approaches like
regularization and Bayesian methods. We also discuss how the Markov models
reviewed fit in the rich hierarchy of other, higher level Markov model classes.
Finally, we close the paper overviewing relaxations of the Gaussian assumption
and pointing out the main areas of application where these Markov models are
nowadays used.","['Irene Córdoba', 'Concha Bielza', 'Pedro Larrañaga']","['stat.ME', 'cs.AI', 'stat.ML']",2016-06-23 12:12:20+00:00
http://arxiv.org/abs/1606.07279v1,Multiclass feature learning for hyperspectral image classification: sparse and hierarchical solutions,"In this paper, we tackle the question of discovering an effective set of
spatial filters to solve hyperspectral classification problems. Instead of
fixing a priori the filters and their parameters using expert knowledge, we let
the model find them within random draws in the (possibly infinite) space of
possible filters. We define an active set feature learner that includes in the
model only features that improve the classifier. To this end, we consider a
fast and linear classifier, multiclass logistic classification, and show that
with a good representation (the filters discovered), such a simple classifier
can reach at least state of the art performances. We apply the proposed active
set learner in four hyperspectral image classification problems, including
agricultural and urban classification at different resolutions, as well as
multimodal data. We also propose a hierarchical setting, which allows to
generate more complex banks of features that can better describe the
nonlinearities present in the data.","['Devis Tuia', 'Rémi Flamary', 'Nicolas Courty']","['stat.ML', 'cs.LG']",2016-06-23 12:05:23+00:00
http://arxiv.org/abs/1606.07268v2,Semi-supervised Inference: General Theory and Estimation of Means,"We propose a general semi-supervised inference framework focused on the
estimation of the population mean. As usual in semi-supervised settings, there
exists an unlabeled sample of covariate vectors and a labeled sample consisting
of covariate vectors along with real-valued responses (""labels""). Otherwise,
the formulation is ""assumption-lean"" in that no major conditions are imposed on
the statistical or functional form of the data. We consider both the ideal
semi-supervised setting where infinitely many unlabeled samples are available,
as well as the ordinary semi-supervised setting in which only a finite number
of unlabeled samples is available.
  Estimators are proposed along with corresponding confidence intervals for the
population mean. Theoretical analysis on both the asymptotic distribution and
$\ell_2$-risk for the proposed procedures are given. Surprisingly, the proposed
estimators, based on a simple form of the least squares method, outperform the
ordinary sample mean. The simple, transparent form of the estimator lends
confidence to the perception that its asymptotic improvement over the ordinary
sample mean also nearly holds even for moderate size samples. The method is
further extended to a nonparametric setting, in which the oracle rate can be
achieved asymptotically. The proposed estimators are further illustrated by
simulation studies and a real data example involving estimation of the homeless
population.","['Anru Zhang', 'Lawrence D. Brown', 'T. Tony Cai']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2016-06-23 10:53:05+00:00
http://arxiv.org/abs/1606.07251v1,Algorithmic Composition of Melodies with Deep Recurrent Neural Networks,"A big challenge in algorithmic composition is to devise a model that is both
easily trainable and able to reproduce the long-range temporal dependencies
typical of music. Here we investigate how artificial neural networks can be
trained on a large corpus of melodies and turned into automated music composers
able to generate new melodies coherent with the style they have been trained
on. We employ gated recurrent unit networks that have been shown to be
particularly efficient in learning complex sequential activations with
arbitrary long time lags. Our model processes rhythm and melody in parallel
while modeling the relation between these two features. Using such an approach,
we were able to generate interesting complete melodies or suggest possible
continuations of a melody fragment that is coherent with the characteristics of
the fragment itself.","['Florian Colombo', 'Samuel P. Muscinelli', 'Alexander Seeholzer', 'Johanni Brea', 'Wulfram Gerstner']","['stat.ML', 'cs.LG']",2016-06-23 09:53:30+00:00
http://arxiv.org/abs/1606.07240v3,PAC-Bayesian Analysis for a two-step Hierarchical Multiview Learning Approach,"We study a two-level multiview learning with more than two views under the
PAC-Bayesian framework. This approach, sometimes referred as late fusion,
consists in learning sequentially multiple view-specific classifiers at the
first level, and then combining these view-specific classifiers at the second
level. Our main theoretical result is a generalization bound on the risk of the
majority vote which exhibits a term of diversity in the predictions of the
view-specific classifiers. From this result it comes out that controlling the
trade-off between diversity and accuracy is a key element for multiview
learning, which complements other results in multiview learning. Finally, we
experiment our principle on multiview datasets extracted from the Reuters
RCV1/RCV2 collection.","['Anil Goyal', 'Emilie Morvant', 'Pascal Germain', 'Massih-Reza Amini']",['stat.ML'],2016-06-23 09:29:53+00:00
http://arxiv.org/abs/1606.07163v1,Interpretable Machine Learning Models for the Digital Clock Drawing Test,"The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular
neuropsychological screening tool for cognitive conditions. The Digital Clock
Drawing Test (dCDT) uses novel software to analyze data from a digitizing
ballpoint pen that reports its position with considerable spatial and temporal
precision, making possible the analysis of both the drawing process and final
product. We developed methodology to analyze pen stroke data from these
drawings, and computed a large collection of features which were then analyzed
with a variety of machine learning techniques. The resulting scoring systems
were designed to be more accurate than the systems currently used by
clinicians, but just as interpretable and easy to use. The systems also allow
us to quantify the tradeoff between accuracy and interpretability. We created
automated versions of the CDT scoring systems currently used by clinicians,
allowing us to benchmark our models, which indicated that our machine learning
models substantially outperformed the existing scoring systems.","['William Souillard-Mandar', 'Randall Davis', 'Cynthia Rudin', 'Rhoda Au', 'Dana Penney']","['stat.ML', 'cs.LG']",2016-06-23 02:08:58+00:00
http://arxiv.org/abs/1606.07153v1,Fast robustness quantification with variational Bayes,"Bayesian hierarchical models are increasing popular in economics. When using
hierarchical models, it is useful not only to calculate posterior expectations,
but also to measure the robustness of these expectations to reasonable
alternative prior choices. We use variational Bayes and linear response methods
to provide fast, accurate posterior means and robustness measures with an
application to measuring the effectiveness of microcredit in the developing
world.","['Ryan Giordano', 'Tamara Broderick', 'Rachael Meager', 'Jonathan Huggins', 'Michael Jordan']","['stat.ML', 'stat.AP', 'stat.ME']",2016-06-23 01:19:17+00:00
http://arxiv.org/abs/1606.07129v1,Explainable Restricted Boltzmann Machines for Collaborative Filtering,"Most accurate recommender systems are black-box models, hiding the reasoning
behind their recommendations. Yet explanations have been shown to increase the
user's trust in the system in addition to providing other benefits such as
scrutability, meaning the ability to verify the validity of recommendations.
This gap between accuracy and transparency or explainability has generated an
interest in automated explanation generation methods. Restricted Boltzmann
Machines (RBM) are accurate models for CF that also lack interpretability. In
this paper, we focus on RBM based collaborative filtering recommendations, and
further assume the absence of any additional data source, such as item content
or user attributes. We thus propose a new Explainable RBM technique that
computes the top-n recommendation list from items that are explainable.
Experimental results show that our method is effective in generating accurate
and explainable recommendations.","['Behnoush Abdollahi', 'Olfa Nasraoui']","['stat.ML', 'cs.LG']",2016-06-22 22:24:30+00:00
http://arxiv.org/abs/1606.07112v1,Visualizing Dynamics: from t-SNE to SEMI-MDPs,"Deep Reinforcement Learning (DRL) is a trending field of research, showing
great promise in many challenging problems such as playing Atari, solving Go
and controlling robots. While DRL agents perform well in practice we are still
missing the tools to analayze their performance and visualize the temporal
abstractions that they learn. In this paper, we present a novel method that
automatically discovers an internal Semi Markov Decision Process (SMDP) model
in the Deep Q Network's (DQN) learned representation. We suggest a novel
visualization method that represents the SMDP model by a directed graph and
visualize it above a t-SNE map. We show how can we interpret the agent's policy
and give evidence for the hierarchical state aggregation that DQNs are learning
automatically. Our algorithm is fully automatic, does not require any domain
specific knowledge and is evaluated by a novel likelihood based evaluation
criteria.","['Nir Ben Zrihem', 'Tom Zahavy', 'Shie Mannor']","['stat.ML', 'cs.LG']",2016-06-22 21:18:50+00:00
http://arxiv.org/abs/1606.07081v1,Finite Sample Prediction and Recovery Bounds for Ordinal Embedding,"The goal of ordinal embedding is to represent items as points in a
low-dimensional Euclidean space given a set of constraints in the form of
distance comparisons like ""item $i$ is closer to item $j$ than item $k$"".
Ordinal constraints like this often come from human judgments. To account for
errors and variation in judgments, we consider the noisy situation in which the
given constraints are independently corrupted by reversing the correct
constraint with some probability. This paper makes several new contributions to
this problem. First, we derive prediction error bounds for ordinal embedding
with noise by exploiting the fact that the rank of a distance matrix of points
in $\mathbb{R}^d$ is at most $d+2$. These bounds characterize how well a
learned embedding predicts new comparative judgments. Second, we investigate
the special case of a known noise model and study the Maximum Likelihood
estimator. Third, knowledge of the noise model enables us to relate prediction
errors to embedding accuracy. This relationship is highly non-trivial since we
show that the linear map corresponding to distance comparisons is
non-invertible, but there exists a nonlinear map that is invertible. Fourth,
two new algorithms for ordinal embedding are proposed and evaluated in
experiments.","['Lalit Jain', 'Kevin Jamieson', 'Robert Nowak']","['stat.ML', 'cs.LG']",2016-06-22 20:06:10+00:00
http://arxiv.org/abs/1606.07043v1,Toward Interpretable Topic Discovery via Anchored Correlation Explanation,"Many predictive tasks, such as diagnosing a patient based on their medical
chart, are ultimately defined by the decisions of human experts. Unfortunately,
encoding experts' knowledge is often time consuming and expensive. We propose a
simple way to use fuzzy and informal knowledge from experts to guide discovery
of interpretable latent topics in text. The underlying intuition of our
approach is that latent factors should be informative about both correlations
in the data and a set of relevance variables specified by an expert.
Mathematically, this approach is a combination of the information bottleneck
and Total Correlation Explanation (CorEx). We give a preliminary evaluation of
Anchored CorEx, showing that it produces more coherent and interpretable topics
on two distinct corpora.","['Kyle Reing', 'David C. Kale', 'Greg Ver Steeg', 'Aram Galstyan']","['stat.ML', 'cs.CL', 'cs.LG']",2016-06-22 19:00:38+00:00
http://arxiv.org/abs/1606.07035v3,Ancestral Causal Inference,"Constraint-based causal discovery from limited data is a notoriously
difficult challenge due to the many borderline independence test decisions.
Several approaches to improve the reliability of the predictions by exploiting
redundancy in the independence information have been proposed recently. Though
promising, existing approaches can still be greatly improved in terms of
accuracy and scalability. We present a novel method that reduces the
combinatorial explosion of the search space by using a more coarse-grained
representation of causal information, drastically reducing computation time.
Additionally, we propose a method to score causal predictions based on their
confidence. Crucially, our implementation also allows one to easily combine
observational and interventional data and to incorporate various types of
available background knowledge. We prove soundness and asymptotic consistency
of our method and demonstrate that it can outperform the state-of-the-art on
synthetic data, achieving a speedup of several orders of magnitude. We
illustrate its practical feasibility by applying it on a challenging protein
data set.","['Sara Magliacane', 'Tom Claassen', 'Joris M. Mooij']","['cs.LG', 'cs.AI', 'stat.ML']",2016-06-22 18:26:27+00:00
http://arxiv.org/abs/1606.07025v1,Efficient Attack Graph Analysis through Approximate Inference,"Attack graphs provide compact representations of the attack paths that an
attacker can follow to compromise network resources by analysing network
vulnerabilities and topology. These representations are a powerful tool for
security risk assessment. Bayesian inference on attack graphs enables the
estimation of the risk of compromise to the system's components given their
vulnerabilities and interconnections, and accounts for multi-step attacks
spreading through the system. Whilst static analysis considers the risk posture
at rest, dynamic analysis also accounts for evidence of compromise, e.g. from
SIEM software or forensic investigation. However, in this context, exact
Bayesian inference techniques do not scale well. In this paper we show how
Loopy Belief Propagation - an approximate inference technique - can be applied
to attack graphs, and that it scales linearly in the number of nodes for both
static and dynamic analysis, making such analyses viable for larger networks.
We experiment with different topologies and network clustering on synthetic
Bayesian attack graphs with thousands of nodes to show that the algorithm's
accuracy is acceptable and converge to a stable solution. We compare sequential
and parallel versions of Loopy Belief Propagation with exact inference
techniques for both static and dynamic analysis, showing the advantages of
approximate inference techniques to scale to larger attack graphs.","['Luis Muñoz-González', 'Daniele Sgandurra', 'Andrea Paudice', 'Emil C. Lupu']","['cs.CR', 'cs.AI', 'stat.ML']",2016-06-22 17:48:17+00:00
http://arxiv.org/abs/1606.06997v4,On the uniqueness and stability of dictionaries for sparse representation of noisy signals,"Learning optimal dictionaries for sparse coding has exposed characteristic
sparse features of many natural signals. However, universal guarantees of the
stability of such features in the presence of noise are lacking. Here, we
provide very general conditions guaranteeing when dictionaries yielding the
sparsest encodings are unique and stable with respect to measurement or
modeling error. We demonstrate that some or all original dictionary elements
are recoverable from noisy data even if the dictionary fails to satisfy the
spark condition, its size is overestimated, or only a polynomial number of
distinct sparse supports appear in the data. Importantly, we derive these
guarantees without requiring any constraints on the recovered dictionary beyond
a natural upper bound on its size. Our results also yield an effective
procedure sufficient to affirm if a proposed solution to the dictionary
learning problem is unique within bounds commensurate with the noise. We
suggest applications to data analysis, engineering, and neuroscience and close
with some remaining challenges left open by our work.","['Charles J. Garfinkle', 'Christopher J. Hillar']","['stat.ML', 'cs.IT', 'math.IT']",2016-06-22 16:06:29+00:00
http://arxiv.org/abs/1606.07369v1,Personalized Prognostic Models for Oncology: A Machine Learning Approach,"We have applied a little-known data transformation to subsets of the
Surveillance, Epidemiology, and End Results (SEER) publically available data of
the National Cancer Institute (NCI) to make it suitable input to standard
machine learning classifiers. This transformation properly treats the
right-censored data in the SEER data and the resulting Random Forest and
Multi-Layer Perceptron models predict full survival curves. Treating the 6, 12,
and 60 months points of the resulting survival curves as 3 binary classifiers,
the 18 resulting classifiers have AUC values ranging from .765 to .885. Further
evidence that the models have generalized well from the training data is
provided by the extremely high levels of agreement between the random forest
and neural network models predictions on the 6, 12, and 60 month binary
classifiers.","['David Dooling', 'Angela Kim', 'Barbara McAneny', 'Jennifer Webster']","['stat.AP', 'cs.LG', 'stat.ML']",2016-06-22 15:55:22+00:00
http://arxiv.org/abs/1606.06962v1,Towards stationary time-vertex signal processing,"Graph-based methods for signal processing have shown promise for the analysis
of data exhibiting irregular structure, such as those found in social,
transportation, and sensor networks. Yet, though these systems are often
dynamic, state-of-the-art methods for signal processing on graphs ignore the
dimension of time, treating successive graph signals independently or taking a
global average. To address this shortcoming, this paper considers the
statistical analysis of time-varying graph signals. We introduce a novel
definition of joint (time-vertex) stationarity, which generalizes the classical
definition of time stationarity and the more recent definition appropriate for
graphs. Joint stationarity gives rise to a scalable Wiener optimization
framework for joint denoising, semi-supervised learning, or more generally
inversing a linear operator, that is provably optimal. Experimental results on
real weather data demonstrate that taking into account graph and time
dimensions jointly can yield significant accuracy improvements in the
reconstruction effort.","['Nathanael Perraudin', 'Andreas Loukas', 'Francesco Grassi', 'Pierre Vandergheynst']","['cs.LG', 'cs.SI', 'stat.ML']",2016-06-22 14:33:15+00:00
http://arxiv.org/abs/1606.06959v2,"Dealing with a large number of classes -- Likelihood, Discrimination or Ranking?","We consider training probabilistic classifiers in the case of a large number
of classes. The number of classes is assumed too large to perform exact
normalisation over all classes. To account for this we consider a simple
approach that directly approximates the likelihood. We show that this simple
approach works well on toy problems and is competitive with recently introduced
alternative non-likelihood based approximations. Furthermore, we relate this
approach to a simple ranking objective. This leads us to suggest a specific
setting for the optimal threshold in the ranking objective.","['David Barber', 'Aleksandar Botev']",['stat.ML'],2016-06-22 14:10:47+00:00
http://arxiv.org/abs/1606.06564v2,An artificial neural network to find correlation patterns in an arbitrary number of variables,"Methods to find correlation among variables are of interest to many
disciplines, including statistics, machine learning, (big) data mining and
neurosciences. Parameters that measure correlation between two variables are of
limited utility when used with multiple variables. In this work, I propose a
simple criterion to measure correlation among an arbitrary number of variables,
based on a data set. The central idea is to i) design a function of the
variables that can take different forms depending on a set of parameters, ii)
calculate the difference between a statistics associated to the function
computed on the data set and the same statistics computed on a randomised
version of the data set, called ""scrambled"" data set, and iii) optimise the
parameters to maximise this difference. Many such functions can be organised in
layers, which can in turn be stacked one on top of the other, forming a neural
network. The function parameters are searched with an enhanced genetic
algortihm called POET and the resulting method is tested on a cancer gene data
set. The method may have potential implications for some issues that affect the
field of neural networks, such as overfitting, the need to process huge amounts
of data for training and the presence of ""adversarial examples"".",['Alessandro Fontana'],"['cs.LG', 'q-bio.NC', 'stat.ML']",2016-06-21 13:35:43+00:00
http://arxiv.org/abs/1606.06439v1,Social-sparsity brain decoders: faster spatial sparsity,"Spatially-sparse predictors are good models for brain decoding: they give
accurate predictions and their weight maps are interpretable as they focus on a
small number of regions. However, the state of the art, based on total
variation or graph-net, is computationally costly. Here we introduce sparsity
in the local neighborhood of each voxel with social-sparsity, a structured
shrinkage operator. We find that, on brain imaging classification problems,
social-sparsity performs almost as well as total-variation models and better
than graph-net, for a fraction of the computational cost. It also very clearly
outlines predictive regions. We give details of the model and the algorithm.","['Gaël Varoquaux', 'Matthieu Kowalski', 'Bertrand Thirion']","['stat.ML', 'cs.CV', 'q-bio.NC']",2016-06-21 06:51:57+00:00
http://arxiv.org/abs/1606.06377v1,Kernel-based Generative Learning in Distortion Feature Space,"This paper presents a novel kernel-based generative classifier which is
defined in a distortion subspace using polynomial series expansion, named
Kernel-Distortion (KD) classifier. An iterative kernel selection algorithm is
developed to steadily improve classification performance by repeatedly removing
and adding kernels. The experimental results on character recognition
application not only show that the proposed generative classifier performs
better than many existing classifiers, but also illustrate that it has
different recognition capability compared to the state-of-the-art
discriminative classifier - deep belief network. The recognition diversity
indicates that a hybrid combination of the proposed generative classifier and
the discriminative classifier could further improve the classification
performance. Two hybrid combination methods, cascading and stacking, have been
implemented to verify the diversity and the improvement of the proposed
classifier.","['Bo Tang', 'Paul M. Baggenstoss', 'Haibo He']","['stat.ML', 'cs.LG']",2016-06-21 00:45:35+00:00
http://arxiv.org/abs/1606.06366v1,FSMJ: Feature Selection with Maximum Jensen-Shannon Divergence for Text Categorization,"In this paper, we present a new wrapper feature selection approach based on
Jensen-Shannon (JS) divergence, termed feature selection with maximum
JS-divergence (FSMJ), for text categorization. Unlike most existing feature
selection approaches, the proposed FSMJ approach is based on real-valued
features which provide more information for discrimination than binary-valued
features used in conventional approaches. We show that the FSMJ is a greedy
approach and the JS-divergence monotonically increases when more features are
selected. We conduct several experiments on real-life data sets, compared with
the state-of-the-art feature selection approaches for text categorization. The
superior performance of the proposed FSMJ approach demonstrates its
effectiveness and further indicates its wide potential applications on data
mining.","['Bo Tang', 'Haibo He']","['stat.ML', 'cs.LG']",2016-06-20 23:58:13+00:00
http://arxiv.org/abs/1606.06364v4,Predicting Student Dropout in Higher Education,"Each year, roughly 30% of first-year students at US baccalaureate
institutions do not return for their second year and over $9 billion is spent
educating these students. Yet, little quantitative research has analyzed the
causes and possible remedies for student attrition. Here, we describe initial
efforts to model student dropout using the largest known dataset on higher
education attrition, which tracks over 32,500 students' demographics and
transcript records at one of the nation's largest public universities. Our
results highlight several early indicators of student attrition and show that
dropout can be accurately predicted even when predictions are based on a single
term of academic transcript data. These results highlight the potential for
machine learning to have an impact on student retention and success while
pointing to several promising directions for future work.","['Lovenoor Aulck', 'Nishant Velagapudi', 'Joshua Blumenstock', 'Jevin West']","['stat.ML', 'cs.CY']",2016-06-20 23:41:19+00:00
http://arxiv.org/abs/1606.06361v2,A Probabilistic Generative Grammar for Semantic Parsing,"Domain-general semantic parsing is a long-standing goal in natural language
processing, where the semantic parser is capable of robustly parsing sentences
from domains outside of which it was trained. Current approaches largely rely
on additional supervision from new domains in order to generalize to those
domains. We present a generative model of natural language utterances and
logical forms and demonstrate its application to semantic parsing. Our approach
relies on domain-independent supervision to generalize to new domains. We
derive and implement efficient algorithms for training, parsing, and sentence
generation. The work relies on a novel application of hierarchical Dirichlet
processes (HDPs) for structured prediction, which we also present in this
manuscript.
  This manuscript is an excerpt of chapter 4 from the Ph.D. thesis of Saparov
(2022), where the model plays a central role in a larger natural language
understanding system.
  This manuscript provides a new simplified and more complete presentation of
the work first introduced in Saparov, Saraswat, and Mitchell (2017). The
description and proofs of correctness of the training algorithm, parsing
algorithm, and sentence generation algorithm are much simplified in this new
presentation. We also describe the novel application of hierarchical Dirichlet
processes for structured prediction. In addition, we extend the earlier work
with a new model of word morphology, which utilizes the comprehensive
morphological data from Wiktionary.",['Abulhair Saparov'],"['cs.CL', 'cs.LG', 'stat.ML']",2016-06-20 23:29:55+00:00
http://arxiv.org/abs/1606.06357v1,Complex Embeddings for Simple Link Prediction,"In statistical relational learning, the link prediction problem is key to
automatically understand the structure of large knowledge bases. As in previous
studies, we propose to solve this problem through latent factorization.
However, here we make use of complex valued embeddings. The composition of
complex embeddings can handle a large variety of binary relations, among them
symmetric and antisymmetric relations. Compared to state-of-the-art models such
as Neural Tensor Network and Holographic Embeddings, our approach based on
complex embeddings is arguably simpler, as it only uses the Hermitian dot
product, the complex counterpart of the standard dot product between real
vectors. Our approach is scalable to large datasets as it remains linear in
both space and time, while consistently outperforming alternative approaches on
standard link prediction benchmarks.","['Théo Trouillon', 'Johannes Welbl', 'Sebastian Riedel', 'Éric Gaussier', 'Guillaume Bouchard']","['cs.AI', 'cs.LG', 'stat.ML']",2016-06-20 22:52:48+00:00
http://arxiv.org/abs/1606.06352v1,Visualizing textual models with in-text and word-as-pixel highlighting,"We explore two techniques which use color to make sense of statistical text
models. One method uses in-text annotations to illustrate a model's view of
particular tokens in particular documents. Another uses a high-level,
""words-as-pixels"" graphic to display an entire corpus. Together, these methods
offer both zoomed-in and zoomed-out perspectives into a model's understanding
of text. We show how these interconnected methods help diagnose a classifier's
poor performance on Twitter slang, and make sense of a topic model on
historical political texts.","['Abram Handler', 'Su Lin Blodgett', ""Brendan O'Connor""]","['stat.ML', 'cs.CL', 'cs.LG']",2016-06-20 22:30:19+00:00
http://arxiv.org/abs/1606.06343v1,Twitter as a Source of Global Mobility Patterns for Social Good,"Data on human spatial distribution and movement is essential for
understanding and analyzing social systems. However existing sources for this
data are lacking in various ways; difficult to access, biased, have poor
geographical or temporal resolution, or are significantly delayed. In this
paper, we describe how geolocation data from Twitter can be used to estimate
global mobility patterns and address these shortcomings. These findings will
inform how this novel data source can be harnessed to address humanitarian and
development efforts.","['Mark Dredze', 'Manuel García-Herranz', 'Alex Rutherford', 'Gideon Mann']","['cs.SI', 'physics.soc-ph', 'stat.ML']",2016-06-20 21:39:51+00:00
http://arxiv.org/abs/1606.06250v1,An Empirical Comparison of Sampling Quality Metrics: A Case Study for Bayesian Nonnegative Matrix Factorization,"In this work, we empirically explore the question: how can we assess the
quality of samples from some target distribution? We assume that the samples
are provided by some valid Monte Carlo procedure, so we are guaranteed that the
collection of samples will asymptotically approximate the true distribution.
Most current evaluation approaches focus on two questions: (1) Has the chain
mixed, that is, is it sampling from the distribution? and (2) How independent
are the samples (as MCMC procedures produce correlated samples)? Focusing on
the case of Bayesian nonnegative matrix factorization, we empirically evaluate
standard metrics of sampler quality as well as propose new metrics to capture
aspects that these measures fail to expose. The aspect of sampling that is of
particular interest to us is the ability (or inability) of sampling methods to
move between multiple optima in NMF problems. As a proxy, we propose and study
a number of metrics that might quantify the diversity of a set of NMF
factorizations obtained by a sampler through quantifying the coverage of the
posterior distribution. We compare the performance of a number of standard
sampling methods for NMF in terms of these new metrics.","['Arjumand Masood', 'Weiwei Pan', 'Finale Doshi-Velez']","['cs.LG', 'stat.ML']",2016-06-20 18:59:34+00:00
http://arxiv.org/abs/1606.06237v4,Online and Differentially-Private Tensor Decomposition,"In this paper, we resolve many of the key algorithmic questions regarding
robustness, memory efficiency, and differential privacy of tensor
decomposition. We propose simple variants of the tensor power method which
enjoy these strong properties. We present the first guarantees for online
tensor power method which has a linear memory requirement. Moreover, we present
a noise calibrated tensor power method with efficient privacy guarantees. At
the heart of all these guarantees lies a careful perturbation analysis derived
in this paper which improves up on the existing results significantly.","['Yining Wang', 'Animashree Anandkumar']","['stat.ML', 'cs.LG']",2016-06-20 18:30:10+00:00
http://arxiv.org/abs/1606.06179v2,On the prediction loss of the lasso in the partially labeled setting,"In this paper we revisit the risk bounds of the lasso estimator in the
context of transductive and semi-supervised learning. In other terms, the
setting under consideration is that of regression with random design under
partial labeling. The main goal is to obtain user-friendly bounds on the
off-sample prediction risk. To this end, the simple setting of bounded response
variable and bounded (high-dimensional) covariates is considered. We propose
some new adaptations of the lasso to these settings and establish oracle
inequalities both in expectation and in deviation. These results provide
non-asymptotic upper bounds on the risk that highlight the interplay between
the bias due to the mis-specification of the linear model, the bias due to the
approximate sparsity and the variance. They also demonstrate that the presence
of a large number of unlabeled features may have significant positive impact in
the situations where the restricted eigenvalue of the design matrix vanishes or
is very small.","['Pierre C. Bellec', 'Arnak S. Dalalyan', 'Edwin Grappin', 'Quentin Paris']","['math.ST', 'stat.ML', 'stat.TH']",2016-06-20 15:38:59+00:00
http://arxiv.org/abs/1606.06126v3,Bootstrapping with Models: Confidence Intervals for Off-Policy Evaluation,"For an autonomous agent, executing a poor policy may be costly or even
dangerous. For such agents, it is desirable to determine confidence interval
lower bounds on the performance of any given policy without executing said
policy. Current methods for exact high confidence off-policy evaluation that
use importance sampling require a substantial amount of data to achieve a tight
lower bound. Existing model-based methods only address the problem in discrete
state spaces. Since exact bounds are intractable for many domains we trade off
strict guarantees of safety for more data-efficient approximate bounds. In this
context, we propose two bootstrapping off-policy evaluation methods which use
learned MDP transition models in order to estimate lower confidence bounds on
policy performance with limited data in both continuous and discrete state
spaces. Since direct use of a model may introduce bias, we derive a theoretical
upper bound on model bias for when the model transition function is estimated
with i.i.d. trajectories. This bound broadens our understanding of the
conditions under which model-based methods have high bias. Finally, we
empirically evaluate our proposed methods and analyze the settings in which
different bootstrapping off-policy confidence interval methods succeed and
fail.","['Josiah P. Hanna', 'Peter Stone', 'Scott Niekum']","['cs.AI', 'cs.LG', 'stat.ML']",2016-06-20 14:06:22+00:00
http://arxiv.org/abs/1606.06121v1,Quantifying and Reducing Stereotypes in Word Embeddings,"Machine learning algorithms are optimized to model statistical properties of
the training data. If the input data reflects stereotypes and biases of the
broader society, then the output of the learning algorithm also captures these
stereotypes. In this paper, we initiate the study of gender stereotypes in {\em
word embedding}, a popular framework to represent text data. As their use
becomes increasingly common, applications can inadvertently amplify unwanted
stereotypes. We show across multiple datasets that the embeddings contain
significant gender stereotypes, especially with regard to professions. We
created a novel gender analogy task and combined it with crowdsourcing to
systematically quantify the gender bias in a given embedding. We developed an
efficient algorithm that reduces gender stereotype using just a handful of
training examples while preserving the useful geometric properties of the
embedding. We evaluated our algorithm on several metrics. While we focus on
male/female stereotypes, our framework may be applicable to other types of
embedding biases.","['Tolga Bolukbasi', 'Kai-Wei Chang', 'James Zou', 'Venkatesh Saligrama', 'Adam Kalai']","['cs.CL', 'cs.LG', 'stat.ML']",2016-06-20 13:58:45+00:00
http://arxiv.org/abs/1606.05988v3,Continuum directions for supervised dimension reduction,"Dimension reduction of multivariate data supervised by auxiliary information
is considered. A series of basis for dimension reduction is obtained as
minimizers of a novel criterion. The proposed method is akin to continuum
regression, and the resulting basis is called continuum directions. With a
presence of binary supervision data, these directions continuously bridge the
principal component, mean difference and linear discriminant directions, thus
ranging from unsupervised to fully supervised dimension reduction.
High-dimensional asymptotic studies of continuum directions for binary
supervision reveal several interesting facts. The conditions under which the
sample continuum directions are inconsistent, but their classification
performance is good, are specified. While the proposed method can be directly
used for binary and multi-category classification, its generalizations to
incorporate any form of auxiliary data are also presented. The proposed method
enjoys fast computation, and the performance is better or on par with more
computer-intensive alternatives.",['Sungkyu Jung'],"['stat.ME', 'cs.LG', 'stat.ML']",2016-06-20 06:52:41+00:00
http://arxiv.org/abs/1606.05925v1,Graph based manifold regularized deep neural networks for automatic speech recognition,"Deep neural networks (DNNs) have been successfully applied to a wide variety
of acoustic modeling tasks in recent years. These include the applications of
DNNs either in a discriminative feature extraction or in a hybrid acoustic
modeling scenario. Despite the rapid progress in this area, a number of
challenges remain in training DNNs. This paper presents an effective way of
training DNNs using a manifold learning based regularization framework. In this
framework, the parameters of the network are optimized to preserve underlying
manifold based relationships between speech feature vectors while minimizing a
measure of loss between network outputs and targets. This is achieved by
incorporating manifold based locality constraints in the objective criterion of
DNNs. Empirical evidence is provided to demonstrate that training a network
with manifold constraints preserves structural compactness in the hidden layers
of the network. Manifold regularization is applied to train bottleneck DNNs for
feature extraction in hidden Markov model (HMM) based speech recognition. The
experiments in this work are conducted on the Aurora-2 spoken digits and the
Aurora-4 read news large vocabulary continuous speech recognition tasks. The
performance is measured in terms of word error rate (WER) on these tasks. It is
shown that the manifold regularized DNNs result in up to 37% reduction in WER
relative to standard DNNs.","['Vikrant Singh Tomar', 'Richard C. Rose']","['stat.ML', 'cs.CL', 'cs.LG']",2016-06-19 23:40:51+00:00
http://arxiv.org/abs/1606.05908v3,Tutorial on Variational Autoencoders,"In just three years, Variational Autoencoders (VAEs) have emerged as one of
the most popular approaches to unsupervised learning of complicated
distributions. VAEs are appealing because they are built on top of standard
function approximators (neural networks), and can be trained with stochastic
gradient descent. VAEs have already shown promise in generating many kinds of
complicated data, including handwritten digits, faces, house numbers, CIFAR
images, physical models of scenes, segmentation, and predicting the future from
static images. This tutorial introduces the intuitions behind VAEs, explains
the mathematics behind them, and describes some empirical behavior. No prior
knowledge of variational Bayesian methods is assumed.",['Carl Doersch'],"['stat.ML', 'cs.LG']",2016-06-19 21:02:30+00:00
http://arxiv.org/abs/1606.05896v1,Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation,"A good clustering can help a data analyst to explore and understand a data
set, but what constitutes a good clustering may depend on domain-specific and
application-specific criteria. These criteria can be difficult to formalize,
even when it is easy for an analyst to know a good clustering when they see
one. We present a new approach to interactive clustering for data exploration
called TINDER, based on a particularly simple feedback mechanism, in which an
analyst can reject a given clustering and request a new one, which is chosen to
be different from the previous clustering while fitting the data well. We
formalize this interaction in a Bayesian framework as a method for prior
elicitation, in which each different clustering is produced by a prior
distribution that is modified to discourage previously rejected clusterings. We
show that TINDER successfully produces a diverse set of clusterings, each of
equivalent quality, that are much more diverse than would be obtained by
randomized restarts.","['Akash Srivastava', 'James Zou', 'Ryan P. Adams', 'Charles Sutton']","['stat.ML', 'cs.LG']",2016-06-19 18:07:15+00:00
http://arxiv.org/abs/1606.05889v2,Tight Performance Bounds for Compressed Sensing With Conventional and Group Sparsity,"In this paper, we study the problem of recovering a group sparse vector from
a small number of linear measurements. In the past the common approach has been
to use various ""group sparsity-inducing"" norms such as the Group LASSO norm for
this purpose. By using the theory of convex relaxations, we show that it is
also possible to use $\ell_1$-norm minimization for group sparse recovery. We
introduce a new concept called group robust null space property (GRNSP), and
show that, under suitable conditions, a group version of the restricted
isometry property (GRIP) implies the GRNSP, and thus leads to group sparse
recovery. When all groups are of equal size, our bounds are less conservative
than known bounds. Moreover, our results apply even to situations where where
the groups have different sizes. When specialized to conventional sparsity, our
bounds reduce to one of the well-known ""best possible"" conditions for sparse
recovery. This relationship between GRNSP and GRIP is new even for conventional
sparsity, and substantially streamlines the proofs of some known results. Using
this relationship, we derive bounds on the $\ell_p$-norm of the residual error
vector for all $p \in [1,2]$, and not just when $p = 2$. When the measurement
matrix consists of random samples of a sub-Gaussian random variable, we present
bounds on the number of measurements, which are less conservative than
currently known bounds.","['Shashank Ranjan', 'Mathukumalli Vidyasagar']",['stat.ML'],2016-06-19 17:01:31+00:00
http://arxiv.org/abs/1606.05850v2,Guaranteed bounds on the Kullback-Leibler divergence of univariate mixtures using piecewise log-sum-exp inequalities,"Information-theoretic measures such as the entropy, cross-entropy and the
Kullback-Leibler divergence between two mixture models is a core primitive in
many signal processing tasks. Since the Kullback-Leibler divergence of mixtures
provably does not admit a closed-form formula, it is in practice either
estimated using costly Monte-Carlo stochastic integration, approximated, or
bounded using various techniques. We present a fast and generic method that
builds algorithmically closed-form lower and upper bounds on the entropy, the
cross-entropy and the Kullback-Leibler divergence of mixtures. We illustrate
the versatile method by reporting on our experiments for approximating the
Kullback-Leibler divergence between univariate exponential mixtures, Gaussian
mixtures, Rayleigh mixtures, and Gamma mixtures.","['Frank Nielsen', 'Ke Sun']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2016-06-19 09:39:30+00:00
http://arxiv.org/abs/1606.05819v1,Building an Interpretable Recommender via Loss-Preserving Transformation,"We propose a method for building an interpretable recommender system for
personalizing online content and promotions. Historical data available for the
system consists of customer features, provided content (promotions), and user
responses. Unlike in a standard multi-class classification setting,
misclassification costs depend on both recommended actions and customers. Our
method transforms such a data set to a new set which can be used with standard
interpretable multi-class classification algorithms. The transformation has the
desirable property that minimizing the standard misclassification penalty in
this new space is equivalent to minimizing the custom cost function.","['Amit Dhurandhar', 'Sechan Oh', 'Marek Petrik']","['stat.ML', 'cs.LG']",2016-06-19 01:37:01+00:00
http://arxiv.org/abs/1606.05798v1,Interpretable Two-level Boolean Rule Learning for Classification,"As a contribution to interpretable machine learning research, we develop a
novel optimization framework for learning accurate and sparse two-level Boolean
rules. We consider rules in both conjunctive normal form (AND-of-ORs) and
disjunctive normal form (OR-of-ANDs). A principled objective function is
proposed to trade classification accuracy and interpretability, where we use
Hamming loss to characterize accuracy and sparsity to characterize
interpretability. We propose efficient procedures to optimize these objectives
based on linear programming (LP) relaxation, block coordinate descent, and
alternating minimization. Experiments show that our new algorithms provide very
good tradeoffs between accuracy and interpretability.","['Guolong Su', 'Dennis Wei', 'Kush R. Varshney', 'Dmitry M. Malioutov']","['stat.ML', 'cs.LG']",2016-06-18 19:37:26+00:00
http://arxiv.org/abs/1606.05725v1,An Efficient Large-scale Semi-supervised Multi-label Classifier Capable of Handling Missing labels,"Multi-label classification has received considerable interest in recent
years. Multi-label classifiers have to address many problems including:
handling large-scale datasets with many instances and a large set of labels,
compensating missing label assignments in the training set, considering
correlations between labels, as well as exploiting unlabeled data to improve
prediction performance. To tackle datasets with a large set of labels,
embedding-based methods have been proposed which seek to represent the label
assignments in a low-dimensional space. Many state-of-the-art embedding-based
methods use a linear dimensionality reduction to represent the label
assignments in a low-dimensional space. However, by doing so, these methods
actually neglect the tail labels - labels that are infrequently assigned to
instances. We propose an embedding-based method that non-linearly embeds the
label vectors using an stochastic approach, thereby predicting the tail labels
more accurately. Moreover, the proposed method have excellent mechanisms for
handling missing labels, dealing with large-scale datasets, as well as
exploiting unlabeled data. With the best of our knowledge, our proposed method
is the first multi-label classifier that simultaneously addresses all of the
mentioned challenges. Experiments on real-world datasets show that our method
outperforms stateof-the-art multi-label classifiers by a large margin, in terms
of prediction performance, as well as training time.","['Amirhossein Akbarnejad', 'Mahdieh Soleymani Baghshah']","['cs.LG', 'cs.AI', 'stat.ML']",2016-06-18 07:49:13+00:00
http://arxiv.org/abs/1606.05693v1,Structured Stochastic Linear Bandits,"The stochastic linear bandit problem proceeds in rounds where at each round
the algorithm selects a vector from a decision set after which it receives a
noisy linear loss parameterized by an unknown vector. The goal in such a
problem is to minimize the (pseudo) regret which is the difference between the
total expected loss of the algorithm and the total expected loss of the best
fixed vector in hindsight. In this paper, we consider settings where the
unknown parameter has structure, e.g., sparse, group sparse, low-rank, which
can be captured by a norm, e.g., $L_1$, $L_{(1,2)}$, nuclear norm. We focus on
constructing confidence ellipsoids which contain the unknown parameter across
all rounds with high-probability. We show the radius of such ellipsoids depend
on the Gaussian width of sets associated with the norm capturing the structure.
Such characterization leads to tighter confidence ellipsoids and, therefore,
sharper regret bounds compared to bounds in the existing literature which are
based on the ambient dimensionality.","['Nicholas Johnson', 'Vidyashankar Sivakumar', 'Arindam Banerjee']","['stat.ML', 'cs.LG']",2016-06-17 22:31:01+00:00
http://arxiv.org/abs/1606.05685v2,Using Visual Analytics to Interpret Predictive Machine Learning Models,"It is commonly believed that increasing the interpretability of a machine
learning model may decrease its predictive power. However, inspecting
input-output relationships of those models using visual analytics, while
treating them as black-box, can help to understand the reasoning behind
outcomes without sacrificing predictive quality. We identify a space of
possible solutions and provide two examples of where such techniques have been
successfully used in practice.","['Josua Krause', 'Adam Perer', 'Enrico Bertini']","['stat.ML', 'cs.LG']",2016-06-17 21:56:43+00:00
http://arxiv.org/abs/1606.05672v1,Interpretability in Linear Brain Decoding,"Improving the interpretability of brain decoding approaches is of primary
interest in many neuroimaging studies. Despite extensive studies of this type,
at present, there is no formal definition for interpretability of brain
decoding models. As a consequence, there is no quantitative measure for
evaluating the interpretability of different brain decoding methods. In this
paper, we present a simple definition for interpretability of linear brain
decoding models. Then, we propose to combine the interpretability and the
performance of the brain decoding into a new multi-objective criterion for
model selection. Our preliminary results on the toy data show that optimizing
the hyper-parameters of the regularized linear classifier based on the proposed
criterion results in more informative linear models. The presented definition
provides the theoretical background for quantitative evaluation of
interpretability in linear brain decoding.","['Seyed Mostafa Kia', 'Andrea Passerini']",['stat.ML'],2016-06-17 20:34:04+00:00
http://arxiv.org/abs/1606.05642v2,Balancing New Against Old Information: The Role of Surprise in Learning,"Surprise describes a range of phenomena from unexpected events to behavioral
responses. We propose a measure of surprise and use it for surprise-driven
learning. Our surprise measure takes into account data likelihood as well as
the degree of commitment to a belief via the entropy of the belief
distribution. We find that surprise-minimizing learning dynamically adjusts the
balance between new and old information without the need of knowledge about the
temporal statistics of the environment. We apply our framework to a dynamic
decision-making task and a maze exploration task. Our surprise minimizing
framework is suitable for learning in complex environments, even if the
environment undergoes gradual or sudden changes and could eventually provide a
framework to study the behavior of humans and animals encountering surprising
events.","['Mohammadjavad Faraji', 'Kerstin Preuschoff', 'Wulfram Gerstner']","['stat.ML', 'cs.LG', 'q-bio.NC']",2016-06-17 19:54:43+00:00
http://arxiv.org/abs/1606.05596v1,Ground Truth Bias in External Cluster Validity Indices,"It has been noticed that some external CVIs exhibit a preferential bias
towards a larger or smaller number of clusters which is monotonic (directly or
inversely) in the number of clusters in candidate partitions. This type of bias
is caused by the functional form of the CVI model. For example, the popular
Rand index (RI) exhibits a monotone increasing (NCinc) bias, while the Jaccard
Index (JI) index suffers from a monotone decreasing (NCdec) bias. This type of
bias has been previously recognized in the literature. In this work, we
identify a new type of bias arising from the distribution of the ground truth
(reference) partition against which candidate partitions are compared. We call
this new type of bias ground truth (GT) bias. This type of bias occurs if a
change in the reference partition causes a change in the bias status (e.g.,
NCinc, NCdec) of a CVI. For example, NCinc bias in the RI can be changed to
NCdec bias by skewing the distribution of clusters in the ground truth
partition. It is important for users to be aware of this new type of biased
behaviour, since it may affect the interpretations of CVI results. The
objective of this article is to study the empirical and theoretical
implications of GT bias. To the best of our knowledge, this is the first
extensive study of such a property for external cluster validity indices.","['Yang Lei', 'James C. Bezdek', 'Simone Romano', 'Nguyen Xuan Vinh', 'Jeffrey Chan', 'James Bailey']","['stat.ML', 'cs.LG']",2016-06-17 17:31:51+00:00
http://arxiv.org/abs/1606.05589v1,Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?,"We conduct large-scale studies on `human attention' in Visual Question
Answering (VQA) to understand where humans choose to look to answer questions
about images. We design and test multiple game-inspired novel
attention-annotation interfaces that require the subject to sharpen regions of
a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human
ATtention) dataset. We evaluate attention maps generated by state-of-the-art
VQA models against human attention both qualitatively (via visualizations) and
quantitatively (via rank-order correlation). Overall, our experiments show that
current attention models in VQA do not seem to be looking at the same regions
as humans.","['Abhishek Das', 'Harsh Agrawal', 'C. Lawrence Zitnick', 'Devi Parikh', 'Dhruv Batra']","['stat.ML', 'cs.CV']",2016-06-17 17:00:02+00:00
http://arxiv.org/abs/1606.05579v3,Early Visual Concept Learning with Unsupervised Deep Learning,"Automated discovery of early visual concepts from raw image data is a major
open challenge in AI research. Addressing this problem, we propose an
unsupervised approach for learning disentangled representations of the
underlying factors of variation. We draw inspiration from neuroscience, and
show how this can be achieved in an unsupervised generative model by applying
the same learning pressures as have been suggested to act in the ventral visual
stream in the brain. By enforcing redundancy reduction, encouraging statistical
independence, and exposure to data with transform continuities analogous to
those to which human infants are exposed, we obtain a variational autoencoder
(VAE) framework capable of learning disentangled factors. Our approach makes
few assumptions and works well across a wide variety of datasets. Furthermore,
our solution has useful emergent properties, such as zero-shot inference and an
intuitive understanding of ""objectness"".","['Irina Higgins', 'Loic Matthey', 'Xavier Glorot', 'Arka Pal', 'Benigno Uria', 'Charles Blundell', 'Shakir Mohamed', 'Alexander Lerchner']","['stat.ML', 'cs.LG', 'q-bio.NC']",2016-06-17 16:19:46+00:00
http://arxiv.org/abs/1606.05572v1,Learning Interpretable Musical Compositional Rules and Traces,"Throughout music history, theorists have identified and documented
interpretable rules that capture the decisions of composers. This paper asks,
""Can a machine behave like a music theorist?"" It presents MUS-ROVER, a
self-learning system for automatically discovering rules from symbolic music.
MUS-ROVER performs feature learning via $n$-gram models to extract
compositional rules --- statistical patterns over the resulting features. We
evaluate MUS-ROVER on Bach's (SATB) chorales, demonstrating that it can recover
known rules, as well as identify new, characteristic patterns for further
study. We discuss how the extracted rules can be used in both machine and human
composition.","['Haizi Yu', 'Lav R. Varshney', 'Guy E. Garnett', 'Ranjitha Kumar']","['stat.ML', 'cs.LG']",2016-06-17 15:58:24+00:00
http://arxiv.org/abs/1606.05492v3,PSF : Introduction to R Package for Pattern Sequence Based Forecasting Algorithm,"This paper discusses about an R package that implements the Pattern Sequence
based Forecasting (PSF) algorithm, which was developed for univariate time
series forecasting. This algorithm has been successfully applied to many
different fields. The PSF algorithm consists of two major parts: clustering and
prediction. The clustering part includes selection of the optimum number of
clusters. It labels time series data with reference to such clusters. The
prediction part includes functions like optimum window size selection for
specific patterns and prediction of future values with reference to past
pattern sequences. The PSF package consists of various functions to implement
the PSF algorithm. It also contains a function which automates all other
functions to obtain optimized prediction results. The aim of this package is to
promote the PSF algorithm and to ease its implementation with minimum efforts.
This paper describes all the functions in the PSF package with their syntax. It
also provides a simple example of usage. Finally, the usefulness of this
package is discussed by comparing it to auto.arima and ets, well-known time
series forecasting functions available on CRAN repository.","['Neeraj Bokde', 'Gualberto Asencio-Cortés', 'Francisco Martínez-Álvarez', 'Kishore Kulat']",['stat.ML'],2016-06-17 12:00:21+00:00
http://arxiv.org/abs/1606.05400v1,"Complex systems: features, similarity and connectivity","The increasing interest in complex networks research has been a consequence
of several intrinsic features of this area, such as the generality of the
approach to represent and model virtually any discrete system, and the
incorporation of concepts and methods deriving from many areas, from
statistical physics to sociology, which are often used in an independent way.
Yet, for this same reason, it would be desirable to integrate these various
aspects into a more coherent and organic framework, which would imply in
several benefits normally allowed by the systematization in science, including
the identification of new types of problems and the cross-fertilization between
fields. More specifically, the identification of the main areas to which the
concepts frequently used in complex networks can be applied paves the way to
adopting and applying a larger set of concepts and methods deriving from those
respective areas. Among the several areas that have been used in complex
networks research, pattern recognition, optimization, linear algebra, and time
series analysis seem to play a more basic and recurrent role. In the present
manuscript, we propose a systematic way to integrate the concepts from these
diverse areas regarding complex networks research. In order to do so, we start
by grouping the multidisciplinary concepts into three main groups, namely
features, similarity, and network connectivity. Then we show that several of
the analysis and modeling approaches to complex networks can be thought as a
composition of maps between these three groups, with emphasis on nine main
types of mappings, which are presented and illustrated. Such a systematization
of principles and approaches also provides an opportunity to review some of the
most closely related works in the literature, which is also developed in this
article.","['Cesar H. Comin', 'Thomas K. DM. Peron', 'Filipi N. Silva', 'Diego R. Amancio', 'Francisco A. Rodrigues', 'Luciano da F. Costa']","['physics.soc-ph', 'physics.data-an', 'stat.ML']",2016-06-17 01:48:16+00:00
http://arxiv.org/abs/1606.05390v1,Making Tree Ensembles Interpretable,"Tree ensembles, such as random forest and boosted trees, are renowned for
their high prediction performance, whereas their interpretability is critically
limited. In this paper, we propose a post processing method that improves the
model interpretability of tree ensembles. After learning a complex tree
ensembles in a standard way, we approximate it by a simpler model that is
interpretable for human. To obtain the simpler model, we derive the EM
algorithm minimizing the KL divergence from the complex ensemble. A synthetic
experiment showed that a complicated tree ensemble was approximated reasonably
as interpretable.","['Satoshi Hara', 'Kohei Hayashi']",['stat.ML'],2016-06-17 00:33:03+00:00
http://arxiv.org/abs/1606.05386v1,Model-Agnostic Interpretability of Machine Learning,"Understanding why machine learning models behave the way they do empowers
both system designers and end-users in many ways: in model selection, feature
engineering, in order to trust and act upon the predictions, and in more
intuitive user interfaces. Thus, interpretability has become a vital concern in
machine learning, and work in the area of interpretable models has found
renewed interest. In some applications, such models are as accurate as
non-interpretable ones, and thus are preferred for their transparency. Even
when they are not accurate, they may still be preferred when interpretability
is of paramount importance. However, restricting machine learning to
interpretable models is often a severe limitation. In this paper we argue for
explaining machine learning predictions using model-agnostic approaches. By
treating the machine learning models as black-box functions, these approaches
provide crucial flexibility in the choice of models, explanations, and
representations, improving debugging, comparison, and interfaces for a variety
of users and models. We also outline the main challenges for such methods, and
review a recently-introduced model-agnostic explanation approach (LIME) that
addresses these challenges.","['Marco Tulio Ribeiro', 'Sameer Singh', 'Carlos Guestrin']","['stat.ML', 'cs.LG']",2016-06-16 23:39:41+00:00
http://arxiv.org/abs/1606.05382v3,Sampling Method for Fast Training of Support Vector Data Description,"Support Vector Data Description (SVDD) is a popular outlier detection
technique which constructs a flexible description of the input data. SVDD
computation time is high for large training datasets which limits its use in
big-data process-monitoring applications. We propose a new iterative
sampling-based method for SVDD training. The method incrementally learns the
training data description at each iteration by computing SVDD on an independent
random sample selected with replacement from the training data set. The
experimental results indicate that the proposed method is extremely fast and
provides a good data description .","['Arin Chaudhuri', 'Deovrat Kakde', 'Maria Jahja', 'Wei Xiao', 'Hansi Jiang', 'Seunghyun Kong', 'Sergiy Peredriy']","['cs.LG', 'stat.AP', 'stat.ML']",2016-06-16 23:18:23+00:00
http://arxiv.org/abs/1606.05363v1,Predicting Ambulance Demand: Challenges and Methods,"Predicting ambulance demand accurately at a fine resolution in time and space
(e.g., every hour and 1 km$^2$) is critical for staff / fleet management and
dynamic deployment. There are several challenges: though the dataset is
typically large-scale, demand per time period and locality is almost always
zero. The demand arises from complex urban geography and exhibits complex
spatio-temporal patterns, both of which need to captured and exploited. To
address these challenges, we propose three methods based on Gaussian mixture
models, kernel density estimation, and kernel warping. These methods provide
spatio-temporal predictions for Toronto and Melbourne that are significantly
more accurate than the current industry practice.",['Zhengyi Zhou'],"['stat.ML', 'stat.AP']",2016-06-16 20:25:47+00:00
http://arxiv.org/abs/1606.05340v2,Exponential expressivity in deep neural networks through transient chaos,"We combine Riemannian geometry with the mean field theory of high dimensional
chaos to study the nature of signal propagation in generic, deep neural
networks with random weights. Our results reveal an order-to-chaos expressivity
phase transition, with networks in the chaotic phase computing nonlinear
functions whose global curvature grows exponentially with depth but not width.
We prove this generic class of deep random functions cannot be efficiently
computed by any shallow network, going beyond prior work restricted to the
analysis of single functions. Moreover, we formalize and quantitatively
demonstrate the long conjectured idea that deep networks can disentangle highly
curved manifolds in input space into flat manifolds in hidden space. Our
theoretical analysis of the expressive power of deep networks broadly applies
to arbitrary nonlinearities, and provides a quantitative underpinning for
previously abstract notions about the geometry of deep functions.","['Ben Poole', 'Subhaneil Lahiri', 'Maithra Raghu', 'Jascha Sohl-Dickstein', 'Surya Ganguli']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG']",2016-06-16 19:59:57+00:00
http://arxiv.org/abs/1606.05336v6,On the Expressive Power of Deep Neural Networks,"We propose a new approach to the problem of neural network expressivity,
which seeks to characterize how structural properties of a neural network
family affect the functions it is able to compute. Our approach is based on an
interrelated set of measures of expressivity, unified by the novel notion of
trajectory length, which measures how the output of a network changes as the
input sweeps along a one-dimensional path. Our findings can be summarized as
follows:
  (1) The complexity of the computed function grows exponentially with depth.
  (2) All weights are not equal: trained networks are more sensitive to their
lower (initial) layer weights.
  (3) Regularizing on trajectory length (trajectory regularization) is a
simpler alternative to batch normalization, with the same performance.","['Maithra Raghu', 'Ben Poole', 'Jon Kleinberg', 'Surya Ganguli', 'Jascha Sohl-Dickstein']","['stat.ML', 'cs.AI', 'cs.LG']",2016-06-16 19:55:29+00:00
http://arxiv.org/abs/1606.05325v1,ACDC: $α$-Carving Decision Chain for Risk Stratification,"In many healthcare settings, intuitive decision rules for risk stratification
can help effective hospital resource allocation. This paper introduces a novel
variant of decision tree algorithms that produces a chain of decisions, not a
general tree. Our algorithm, $\alpha$-Carving Decision Chain (ACDC),
sequentially carves out ""pure"" subsets of the majority class examples. The
resulting chain of decision rules yields a pure subset of the minority class
examples. Our approach is particularly effective in exploring large and
class-imbalanced health datasets. Moreover, ACDC provides an interactive
interpretation in conjunction with visual performance metrics such as Receiver
Operating Characteristics curve and Lift chart.","['Yubin Park', 'Joyce Ho', 'Joydeep Ghosh']","['stat.ML', 'cs.LG']",2016-06-16 19:36:51+00:00
http://arxiv.org/abs/1606.05320v2,Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models,"As deep neural networks continue to revolutionize various application
domains, there is increasing interest in making these powerful models more
understandable and interpretable, and narrowing down the causes of good and bad
predictions. We focus on recurrent neural networks (RNNs), state of the art
models in speech recognition and translation. Our approach to increasing
interpretability is by combining an RNN with a hidden Markov model (HMM), a
simpler and more transparent model. We explore various combinations of RNNs and
HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained
first, then a small LSTM is given HMM state distributions and trained to fill
in gaps in the HMM's performance; and a jointly trained hybrid model. We find
that the LSTM and HMM learn complementary information about the features in the
text.","['Viktoriya Krakovna', 'Finale Doshi-Velez']","['stat.ML', 'cs.CL', 'cs.LG']",2016-06-16 19:13:52+00:00
http://arxiv.org/abs/1606.05313v1,Unsupervised Risk Estimation Using Only Conditional Independence Structure,"We show how to estimate a model's test error from unlabeled data, on
distributions very different from the training distribution, while assuming
only that certain conditional independencies are preserved between train and
test. We do not need to assume that the optimal predictor is the same between
train and test, or that the true distribution lies in any parametric family. We
can also efficiently differentiate the error estimate to perform unsupervised
discriminative learning. Our technical tool is the method of moments, which
allows us to exploit conditional independencies in the absence of a
fully-specified model. Our framework encompasses a large family of losses
including the log and exponential loss, and extends to structured output
settings such as hidden Markov models.","['Jacob Steinhardt', 'Percy Liang']","['cs.LG', 'cs.AI', 'stat.ML']",2016-06-16 18:48:51+00:00
http://arxiv.org/abs/1606.05286v1,Spectral decomposition method of dialog state tracking via collective matrix factorization,"The task of dialog management is commonly decomposed into two sequential
subtasks: dialog state tracking and dialog policy learning. In an end-to-end
dialog system, the aim of dialog state tracking is to accurately estimate the
true dialog state from noisy observations produced by the speech recognition
and the natural language understanding modules. The state tracking task is
primarily meant to support a dialog policy. From a probabilistic perspective,
this is achieved by maintaining a posterior distribution over hidden dialog
states composed of a set of context dependent variables. Once a dialog policy
is learned, it strives to select an optimal dialog act given the estimated
dialog state and a defined reward function. This paper introduces a novel
method of dialog state tracking based on a bilinear algebric decomposition
model that provides an efficient inference schema through collective matrix
factorization. We evaluate the proposed approach on the second Dialog State
Tracking Challenge (DSTC-2) dataset and we show that the proposed tracker gives
encouraging results compared to the state-of-the-art trackers that participated
in this standard benchmark. Finally, we show that the prediction schema is
computationally efficient in comparison to the previous approaches.",['Julien Perez'],"['cs.CL', 'stat.ML']",2016-06-16 17:31:13+00:00
http://arxiv.org/abs/1606.05275v1,Designing Intelligent Automation based Solutions for Complex Social Problems,"Deciding effective and timely preventive measures against complex social
problems affecting relatively low income geographies is a difficult challenge.
There is a strong need to adopt intelligent automation based solutions with low
cost imprints to tackle these problems at larger scales. Starting with the
hypothesis that analytical modelling and analysis of social phenomena with high
accuracy is in general inherently hard, in this paper we propose design
framework to enable data-driven machine learning based adaptive solution
approach towards enabling more effective preventive measures. We use survey
data collected from a socio-economically backward region of India about
adolescent girls to illustrate the design approach.","['Sanjay Podder', 'Janardan Misra', 'Senthil Kumaresan', 'Neville Dubash', 'Indrani Bhattacharya']","['stat.ML', 'cs.CY']",2016-06-16 17:15:45+00:00
http://arxiv.org/abs/1606.05273v1,The Effect of Heteroscedasticity on Regression Trees,"Regression trees are becoming increasingly popular as omnibus predicting
tools and as the basis of numerous modern statistical learning ensembles. Part
of their popularity is their ability to create a regression prediction without
ever specifying a structure for the mean model. However, the method implicitly
assumes homogeneous variance across the entire explanatory-variable space. It
is unknown how the algorithm behaves when faced with heteroscedastic data. In
this study, we assess the performance of the most popular regression-tree
algorithm in a single-variable setting under a very simple step-function model
for heteroscedasticity. We use simulation to show that the locations of splits,
and hence the ability to accurately predict means, are both adversely
influenced by the change in variance. We identify the pruning algorithm as the
main concern, although the effects on the splitting algorithm may be meaningful
in some applications.","['Will Ruth', 'Thomas Loughin']",['stat.ML'],2016-06-16 17:11:27+00:00
http://arxiv.org/abs/1606.05241v1,The Mondrian Kernel,"We introduce the Mondrian kernel, a fast random feature approximation to the
Laplace kernel. It is suitable for both batch and online learning, and admits a
fast kernel-width-selection procedure as the random features can be re-used
efficiently for all kernel widths. The features are constructed by sampling
trees via a Mondrian process [Roy and Teh, 2009], and we highlight the
connection to Mondrian forests [Lakshminarayanan et al., 2014], where trees are
also sampled via a Mondrian process, but fit independently. This link provides
a new insight into the relationship between kernel methods and random forests.","['Matej Balog', 'Balaji Lakshminarayanan', 'Zoubin Ghahramani', 'Daniel M. Roy', 'Yee Whye Teh']",['stat.ML'],2016-06-16 16:14:32+00:00
http://arxiv.org/abs/1606.05560v1,Estimation of matrix trace using machine learning,"We present a new trace estimator of the matrix whose explicit form is not
given but its matrix multiplication to a vector is available. The form of the
estimator is similar to the Hutchison stochastic trace estimator, but instead
of the random noise vectors in Hutchison estimator, we use small number of
probing vectors determined by machine learning. Evaluation of the quality of
estimates and bias correction are discussed. An unbiased estimator is proposed
for the calculation of the expectation value of a function of traces. In the
numerical experiments with random matrices, it is shown that the precision of
trace estimates with $\mathcal{O}(10)$ probing vectors determined by the
machine learning is similar to that with $\mathcal{O}(10000)$ random noise
vectors.",['Boram Yoon'],"['stat.ML', 'math.NA']",2016-06-16 15:50:25+00:00
http://arxiv.org/abs/1606.05229v2,Estimating mutual information in high dimensions via classification error,"Multivariate pattern analyses approaches in neuroimaging are fundamentally
concerned with investigating the quantity and type of information processed by
various regions of the human brain; typically, estimates of classification
accuracy are used to quantify information. While a extensive and powerful
library of methods can be applied to train and assess classifiers, it is not
always clear how to use the resulting measures of classification performance to
draw scientific conclusions: e.g. for the purpose of evaluating redundancy
between brain regions. An additional confound for interpreting classification
performance is the dependence of the error rate on the number and choice of
distinct classes obtained for the classification task. In contrast, mutual
information is a quantity defined independently of the experimental design, and
has ideal properties for comparative analyses. Unfortunately, estimating the
mutual information based on observations becomes statistically infeasible in
high dimensions without some kind of assumption or prior.
  In this paper, we construct a novel classification-based estimator of mutual
information based on high-dimensional asymptotics. We show that in a particular
limiting regime, the mutual information is an invertible function of the
expected $k$-class Bayes error. While the theory is based on a large-sample,
high-dimensional limit, we demonstrate through simulations that our proposed
estimator has superior performance to the alternatives in problems of moderate
dimensionality.","['Charles Y. Zheng', 'Yuval Benjamini']","['stat.ML', 'cs.IT', 'math.IT']",2016-06-16 15:40:21+00:00
http://arxiv.org/abs/1606.05228v1,How many faces can be recognized? Performance extrapolation for multi-class classification,"The difficulty of multi-class classification generally increases with the
number of classes. Using data from a subset of the classes, can we predict how
well a classifier will scale with an increased number of classes? Under the
assumption that the classes are sampled exchangeably, and under the assumption
that the classifier is generative (e.g. QDA or Naive Bayes), we show that the
expected accuracy when the classifier is trained on $k$ classes is the $k-1$st
moment of a \emph{conditional accuracy distribution}, which can be estimated
from data. This provides the theoretical foundation for performance
extrapolation based on pseudolikelihood, unbiased estimation, and
high-dimensional asymptotics. We investigate the robustness of our methods to
non-generative classifiers in simulations and one optical character recognition
example.","['Charles Y. Zheng', 'Rakesh Achanta', 'Yuval Benjamini']","['stat.ML', 'cs.CV', 'cs.IT', 'cs.LG', 'math.IT']",2016-06-16 15:38:20+00:00
http://arxiv.org/abs/1606.05201v2,"Assessing and tuning brain decoders: cross-validation, caveats, and guidelines","Decoding, ie prediction from brain images or signals, calls for empirical
evaluation of its predictive power. Such evaluation is achieved via
cross-validation, a method also used to tune decoders' hyper-parameters. This
paper is a review on cross-validation procedures for decoding in neuroimaging.
It includes a didactic overview of the relevant theoretical considerations.
Practical aspects are highlighted with an extensive empirical study of the
common decoders in within-and across-subject predictions, on multiple datasets
--anatomical and functional MRI and MEG-- and simulations. Theory and
experiments outline that the popular "" leave-one-out "" strategy leads to
unstable and biased estimates, and a repeated random splits method should be
preferred. Experiments outline the large error bars of cross-validation in
neuroimaging settings: typical confidence intervals of 10%. Nested
cross-validation can tune decoders' parameters while avoiding circularity bias.
However we find that it can be more favorable to use sane defaults, in
particular for non-sparse decoders.","['Gaël Varoquaux', 'Pradeep Reddy Raamana', 'Denis Engemann', 'Andrés Hoyos-Idrobo', 'Yannick Schwartz', 'Bertrand Thirion']",['stat.ML'],2016-06-16 14:29:28+00:00
http://arxiv.org/abs/1606.05158v2,CLEAR: Covariant LEAst-square Re-fitting with applications to image restoration,"In this paper, we propose a new framework to remove parts of the systematic
errors affecting popular restoration algorithms, with a special focus for image
processing tasks. Generalizing ideas that emerged for $\ell_1$ regularization,
we develop an approach re-fitting the results of standard methods towards the
input data. Total variation regularizations and non-local means are special
cases of interest. We identify important covariant information that should be
preserved by the re-fitting method, and emphasize the importance of preserving
the Jacobian (w.r.t. the observed signal) of the original estimator. Then, we
provide an approach that has a ""twicing"" flavor and allows re-fitting the
restored signal by adding back a local affine transformation of the residual
term. We illustrate the benefits of our method on numerical simulations for
image restoration tasks.","['C-A. Deledalle', 'N. Papadakis', 'J. Salmon', 'S. Vaiter']","['math.ST', 'cs.CV', 'stat.ML', 'stat.TH']",2016-06-16 12:23:55+00:00
http://arxiv.org/abs/1606.05110v1,Machine Learning meets Data-Driven Journalism: Boosting International Understanding and Transparency in News Coverage,"Migration crisis, climate change or tax havens: Global challenges need global
solutions. But agreeing on a joint approach is difficult without a common
ground for discussion. Public spheres are highly segmented because news are
mainly produced and received on a national level. Gain- ing a global view on
international debates about important issues is hindered by the enormous
quantity of news and by language barriers. Media analysis usually focuses only
on qualitative re- search. In this position statement, we argue that it is
imperative to pool methods from machine learning, journalism studies and
statistics to help bridging the segmented data of the international public
sphere, using the Transatlantic Trade and Investment Partnership (TTIP) as a
case study.","['Elena Erdmann', 'Karin Boczek', 'Lars Koppers', 'Gerret von Nordheim', 'Christian Pölitz', 'Alejandro Molina', 'Katharina Morik', 'Henrik Müller', 'Jörg Rahnenführer', 'Kristian Kersting']","['stat.ML', 'cs.CY']",2016-06-16 09:31:12+00:00
http://arxiv.org/abs/1606.05105v1,Machine Learning Across Cultures: Modeling the Adoption of Financial Services for the Poor,"Recently, mobile operators in many developing economies have launched ""Mobile
Money"" platforms that deliver basic financial services over the mobile phone
network. While many believe that these services can improve the lives of the
poor, a consistent difficulty has been identifying individuals most likely to
benefit from access to the new technology. Here, we combine terabyte-scale data
from three different mobile phone operators from Ghana, Pakistan, and Zambia,
to better understand the behavioral determinants of mobile money adoption. Our
supervised learning models provide insight into the best predictors of adoption
in three very distinct cultures. We find that models fit on one population fail
to generalize to another, and in general are highly context-dependent. These
findings highlight the need for a nuanced approach to understanding the role
and potential of financial services for the poor.","['Muhammad Raza Khan', 'Joshua E. Blumenstock']","['stat.ML', 'cs.CY']",2016-06-16 09:21:55+00:00
http://arxiv.org/abs/1606.05060v1,Pruning Random Forests for Prediction on a Budget,"We propose to prune a random forest (RF) for resource-constrained prediction.
We first construct a RF and then prune it to optimize expected feature cost &
accuracy. We pose pruning RFs as a novel 0-1 integer program with linear
constraints that encourages feature re-use. We establish total unimodularity of
the constraint set to prove that the corresponding LP relaxation solves the
original integer program. We then exploit connections to combinatorial
optimization and develop an efficient primal-dual algorithm, scalable to large
datasets. In contrast to our bottom-up approach, which benefits from good RF
initialization, conventional methods are top-down acquiring features based on
their utility value and is generally intractable, requiring heuristics.
Empirically, our pruning algorithm outperforms existing state-of-the-art
resource-constrained algorithms.","['Feng Nan', 'Joseph Wang', 'Venkatesh Saligrama']","['stat.ML', 'cs.LG']",2016-06-16 05:56:36+00:00
http://arxiv.org/abs/1606.05027v2,Learning Optimal Interventions,"Our goal is to identify beneficial interventions from observational data. We
consider interventions that are narrowly focused (impacting few covariates) and
may be tailored to each individual or globally enacted over a population. For
applications where harmful intervention is drastically worse than proposing no
change, we propose a conservative definition of the optimal intervention.
Assuming the underlying relationship remains invariant under intervention, we
develop efficient algorithms to identify the optimal intervention policy from
limited data and provide theoretical guarantees for our approach in a Gaussian
Process setting. Although our methods assume covariates can be precisely
adjusted, they remain capable of improving outcomes in misspecified settings
where interventions incur unintentional downstream effects. Empirically, our
approach identifies good interventions in two practical applications: gene
perturbation and writing improvement.","['Jonas Mueller', 'David N. Reshef', 'George Du', 'Tommi Jaakkola']","['stat.ML', 'cs.LG']",2016-06-16 01:55:33+00:00
http://arxiv.org/abs/1606.05018v1,Improving Power Generation Efficiency using Deep Neural Networks,"Recently there has been significant research on power generation,
distribution and transmission efficiency especially in the case of renewable
resources. The main objective is reduction of energy losses and this requires
improvements on data acquisition and analysis. In this paper we address these
concerns by using consumers' electrical smart meter readings to estimate
network loading and this information can then be used for better capacity
planning. We compare Deep Neural Network (DNN) methods with traditional methods
for load forecasting. Our results indicate that DNN methods outperform most
traditional methods. This comes at the cost of additional computational
complexity but this can be addressed with the use of cloud resources. We also
illustrate how these results can be used to better support dynamic pricing.","['Stefan Hosein', 'Patrick Hosein']","['stat.ML', 'cs.LG', 'cs.NE']",2016-06-16 00:53:56+00:00
http://arxiv.org/abs/1606.04995v1,Joint Data Compression and MAC Protocol Design for Smartgrids with Renewable Energy,"In this paper, we consider the joint design of data compression and
802.15.4-based medium access control (MAC) protocol for smartgrids with
renewable energy. We study the setting where a number of nodes, each of which
comprises electricity load and/or renewable sources, report periodically their
injected powers to a data concentrator. Our design exploits the correlation of
the reported data in both time and space to efficiently design the data
compression using the compressed sensing (CS) technique and theMAC protocol so
that the reported data can be recovered reliably within minimum reporting time.
Specifically, we perform the following design tasks: i) we employ the
two-dimensional (2D) CS technique to compress the reported data in the
distributed manner; ii) we propose to adapt the 802.15.4 MAC protocol frame
structure to enable efficient data transmission and reliable data
reconstruction; and iii) we develop an analytical model based on which we can
obtain efficient MAC parameter configuration to minimize the reporting delay.
Finally, numerical results are presented to demonstrate the effectiveness of
our proposed framework compared to existing solutions.","['Le Thanh Tan', 'Long Bao Le']","['cs.NI', 'cs.IT', 'math.IT', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",2016-06-15 22:14:15+00:00
http://arxiv.org/abs/1606.04991v1,A Class of Parallel Doubly Stochastic Algorithms for Large-Scale Learning,"We consider learning problems over training sets in which both, the number of
training examples and the dimension of the feature vectors, are large. To solve
these problems we propose the random parallel stochastic algorithm (RAPSA). We
call the algorithm random parallel because it utilizes multiple parallel
processors to operate on a randomly chosen subset of blocks of the feature
vector. We call the algorithm stochastic because processors choose training
subsets uniformly at random. Algorithms that are parallel in either of these
dimensions exist, but RAPSA is the first attempt at a methodology that is
parallel in both the selection of blocks and the selection of elements of the
training set. In RAPSA, processors utilize the randomly chosen functions to
compute the stochastic gradient component associated with a randomly chosen
block. The technical contribution of this paper is to show that this minimally
coordinated algorithm converges to the optimal classifier when the training
objective is convex. Moreover, we present an accelerated version of RAPSA
(ARAPSA) that incorporates the objective function curvature information by
premultiplying the descent direction by a Hessian approximation matrix. We
further extend the results for asynchronous settings and show that if the
processors perform their updates without any coordination the algorithms are
still convergent to the optimal argument. RAPSA and its extensions are then
numerically evaluated on a linear estimation problem and a binary image
classification task using the MNIST handwritten digit dataset.","['Aryan Mokhtari', 'Alec Koppel', 'Alejandro Ribeiro']","['cs.LG', 'math.OC', 'stat.ML']",2016-06-15 21:34:46+00:00
http://arxiv.org/abs/1606.04988v2,Logarithmic Time One-Against-Some,"We create a new online reduction of multiclass classification to binary
classification for which training and prediction time scale logarithmically
with the number of classes. Compared to previous approaches, we obtain
substantially better statistical performance for two reasons: First, we prove a
tighter and more complete boosting theorem, and second we translate the results
more directly into an algorithm. We show that several simple techniques give
rise to an algorithm that can compete with one-against-all in both space and
predictive power while offering exponential improvements in speed when the
number of classes is large.","['Hal Daume III', 'Nikos Karampatziakis', 'John Langford', 'Paul Mineiro']","['stat.ML', 'cs.LG']",2016-06-15 21:27:43+00:00
http://arxiv.org/abs/1606.04985v1,Combining multiscale features for classification of hyperspectral images: a sequence based kernel approach,"Nowadays, hyperspectral image classification widely copes with spatial
information to improve accuracy. One of the most popular way to integrate such
information is to extract hierarchical features from a multiscale segmentation.
In the classification context, the extracted features are commonly concatenated
into a long vector (also called stacked vector), on which is applied a
conventional vector-based machine learning technique (e.g. SVM with Gaussian
kernel). In this paper, we rather propose to use a sequence structured kernel:
the spectrum kernel. We show that the conventional stacked vector-based kernel
is actually a special case of this kernel. Experiments conducted on various
publicly available hyperspectral datasets illustrate the improvement of the
proposed kernel w.r.t. conventional ones using the same hierarchical spatial
features.","['Yanwei Cui', 'Laetitia Chapel', 'Sébastien Lefèvre']","['cs.CV', 'cs.LG', 'stat.ML']",2016-06-15 21:19:54+00:00
http://arxiv.org/abs/1606.04934v2,Improving Variational Inference with Inverse Autoregressive Flow,"The framework of normalizing flows provides a general strategy for flexible
variational inference of posteriors over latent variables. We propose a new
type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast
to earlier published flows, scales well to high-dimensional latent spaces. The
proposed flow consists of a chain of invertible transformations, where each
transformation is based on an autoregressive neural network. In experiments, we
show that IAF significantly improves upon diagonal Gaussian approximate
posteriors. In addition, we demonstrate that a novel type of variational
autoencoder, coupled with IAF, is competitive with neural autoregressive models
in terms of attained log-likelihood on natural images, while allowing
significantly faster synthesis.","['Diederik P. Kingma', 'Tim Salimans', 'Rafal Jozefowicz', 'Xi Chen', 'Ilya Sutskever', 'Max Welling']","['cs.LG', 'stat.ML']",2016-06-15 19:46:36+00:00
http://arxiv.org/abs/1606.06177v1,Understanding Innovation to Drive Sustainable Development,"Innovation is among the key factors driving a country's economic and social
growth. But what are the factors that make a country innovative? How do they
differ across different parts of the world and different stages of development?
In this work done in collaboration with the World Economic Forum (WEF), we
analyze the scores obtained through executive opinion surveys that constitute
the WEF's Global Competitiveness Index in conjunction with other country-level
metrics and indicators to identify actionable levers of innovation. The
findings can help country leaders and organizations shape the policies to drive
developmental activities and increase the capacity of innovation.","['Prasanna Sattigeri', 'Aurélie Lozano', 'Aleksandra Mojsilović', 'Kush R. Varshney', 'Mahmoud Naghshineh']","['cs.CY', 'stat.ML']",2016-06-15 19:14:20+00:00
http://arxiv.org/abs/1606.04838v3,Optimization Methods for Large-Scale Machine Learning,"This paper provides a review and commentary on the past, present, and future
of numerical optimization algorithms in the context of machine learning
applications. Through case studies on text classification and the training of
deep neural networks, we discuss how optimization problems arise in machine
learning and what makes them challenging. A major theme of our study is that
large-scale machine learning represents a distinctive setting in which the
stochastic gradient (SG) method has traditionally played a central role while
conventional gradient-based nonlinear optimization techniques typically falter.
Based on this viewpoint, we present a comprehensive theory of a
straightforward, yet versatile SG algorithm, discuss its practical behavior,
and highlight opportunities for designing algorithms with improved performance.
This leads to a discussion about the next generation of optimization methods
for large-scale machine learning, including an investigation of two main
streams of research on techniques that diminish noise in the stochastic
directions and methods that make use of second-order derivative approximations.","['Léon Bottou', 'Frank E. Curtis', 'Jorge Nocedal']","['stat.ML', 'cs.LG', 'math.OC']",2016-06-15 16:15:53+00:00
http://arxiv.org/abs/1606.04820v2,Understanding Probabilistic Sparse Gaussian Process Approximations,"Good sparse approximations are essential for practical inference in Gaussian
Processes as the computational cost of exact methods is prohibitive for large
datasets. The Fully Independent Training Conditional (FITC) and the Variational
Free Energy (VFE) approximations are two recent popular methods. Despite
superficial similarities, these approximations have surprisingly different
theoretical properties and behave differently in practice. We thoroughly
investigate the two methods for regression both analytically and through
illustrative examples, and draw conclusions to guide practical application.","['Matthias Bauer', 'Mark van der Wilk', 'Carl Edward Rasmussen']",['stat.ML'],2016-06-15 15:33:27+00:00
http://arxiv.org/abs/1606.04809v3,ASAGA: Asynchronous Parallel SAGA,"We describe ASAGA, an asynchronous parallel version of the incremental
gradient algorithm SAGA that enjoys fast linear convergence rates. Through a
novel perspective, we revisit and clarify a subtle but important technical
issue present in a large fraction of the recent convergence rate proofs for
asynchronous parallel optimization algorithms, and propose a simplification of
the recently introduced ""perturbed iterate"" framework that resolves it. We
thereby prove that ASAGA can obtain a theoretical linear speedup on multi-core
systems even without sparsity assumptions. We present results of an
implementation on a 40-core architecture illustrating the practical speedup as
well as the hardware overhead.","['Rémi Leblond', 'Fabian Pedregosa', 'Simon Lacoste-Julien']","['math.OC', 'cs.LG', 'stat.ML']",2016-06-15 15:12:01+00:00
http://arxiv.org/abs/1606.04789v2,Network Maximal Correlation,"We introduce Network Maximal Correlation (NMC) as a multivariate measure of
nonlinear association among random variables. NMC is defined via an
optimization that infers transformations of variables by maximizing aggregate
inner products between transformed variables. For finite discrete and jointly
Gaussian random variables, we characterize a solution of the NMC optimization
using basis expansion of functions over appropriate basis functions. For finite
discrete variables, we propose an algorithm based on alternating conditional
expectation to determine NMC. Moreover we propose a distributed algorithm to
compute an approximation of NMC for large and dense graphs using graph
partitioning. For finite discrete variables, we show that the probability of
discrepancy greater than any given level between NMC and NMC computed using
empirical distributions decays exponentially fast as the sample size grows. For
jointly Gaussian variables, we show that under some conditions the NMC
optimization is an instance of the Max-Cut problem. We then illustrate an
application of NMC in inference of graphical model for bijective functions of
jointly Gaussian variables. Finally, we show NMC's utility in a data
application of learning nonlinear dependencies among genes in a cancer dataset.","['Soheil Feizi', 'Ali Makhdoumi', 'Ken Duffy', 'Muriel Medard', 'Manolis Kellis']",['stat.ML'],2016-06-15 14:45:16+00:00
http://arxiv.org/abs/1606.04753v2,Safe Exploration in Finite Markov Decision Processes with Gaussian Processes,"In classical reinforcement learning, when exploring an environment, agents
accept arbitrary short term loss for long term gain. This is infeasible for
safety critical applications, such as robotics, where even a single unsafe
action may cause system failure. In this paper, we address the problem of
safely exploring finite Markov decision processes (MDP). We define safety in
terms of an, a priori unknown, safety constraint that depends on states and
actions. We aim to explore the MDP under this constraint, assuming that the
unknown function satisfies regularity conditions expressed via a Gaussian
process prior. We develop a novel algorithm for this task and prove that it is
able to completely explore the safely reachable part of the MDP without
violating the safety constraint. To achieve this, it cautiously explores safe
states and actions in order to gain statistical confidence about the safety of
unvisited state-action pairs from noisy observations collected while navigating
the environment. Moreover, the algorithm explicitly considers reachability when
exploring the MDP, ensuring that it does not get stuck in any state with no
safe way out. We demonstrate our method on digital terrain models for the task
of exploring an unknown map with a rover.","['Matteo Turchetta', 'Felix Berkenkamp', 'Andreas Krause']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2016-06-15 13:18:30+00:00
