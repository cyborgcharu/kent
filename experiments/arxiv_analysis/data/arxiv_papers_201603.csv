id,title,abstract,authors,categories,date
http://arxiv.org/abs/1604.07464v2,Nonparametric Bayesian Negative Binomial Factor Analysis,"A common approach to analyze a covariate-sample count matrix, an element of
which represents how many times a covariate appears in a sample, is to
factorize it under the Poisson likelihood. We show its limitation in capturing
the tendency for a covariate present in a sample to both repeat itself and
excite related ones. To address this limitation, we construct negative binomial
factor analysis (NBFA) to factorize the matrix under the negative binomial
likelihood, and relate it to a Dirichlet-multinomial distribution based
mixed-membership model. To support countably infinite factors, we propose the
hierarchical gamma-negative binomial process. By exploiting newly proved
connections between discrete distributions, we construct two blocked and a
collapsed Gibbs sampler that all adaptively truncate their number of factors,
and demonstrate that the blocked Gibbs sampler developed under a compound
Poisson representation converges fast and has low computational complexity.
Example results show that NBFA has a distinct mechanism in adjusting its number
of inferred factors according to the sample lengths, and provides clear
advantages in parsimonious representation, predictive power, and computational
complexity over previously proposed discrete latent variable models, which
either completely ignore burstiness, or model only the burstiness of the
covariates but not that of the factors.",['Mingyuan Zhou'],"['stat.ME', 'stat.ML']",2016-04-25 22:27:25+00:00
http://arxiv.org/abs/1604.07463v1,Dynamic Pricing with Demand Covariates,"We consider a firm that sells products over $T$ periods without knowing the
demand function. The firm sequentially sets prices to earn revenue and to learn
the underlying demand function simultaneously. A natural heuristic for this
problem, commonly used in practice, is greedy iterative least squares (GILS).
At each time period, GILS estimates the demand as a linear function of the
price by applying least squares to the set of prior prices and realized
demands. Then a price that maximizes the revenue, given the estimated demand
function, is used for the next time period. The performance is measured by the
regret, which is the expected revenue loss from the optimal (oracle) pricing
policy when the demand function is known. Recently, den Boer and Zwart (2014)
and Keskin and Zeevi (2014) demonstrated that GILS is sub-optimal. They
introduced algorithms which integrate forced price dispersion with GILS and
achieve asymptotically optimal performance.
  In this paper, we consider this dynamic pricing problem in a data-rich
environment. In particular, we assume that the firm knows the expected demand
under a particular price from historical data, and in each period, before
setting the price, the firm has access to extra information (demand covariates)
which may be predictive of the demand. We prove that in this setting GILS
achieves asymptotically optimal regret of order $\log(T)$. We also show the
following surprising result: in the original dynamic pricing problem of den
Boer and Zwart (2014) and Keskin and Zeevi (2014), inclusion of any set of
covariates in GILS as potential demand covariates (even though they could carry
no information) would make GILS asymptotically optimal. We validate our results
via extensive numerical simulations on synthetic and real data sets.","['Sheng Qiang', 'Mohsen Bayati']",['stat.ML'],2016-04-25 22:22:11+00:00
http://arxiv.org/abs/1604.07451v3,Learning Local Dependence In Ordered Data,"In many applications, data come with a natural ordering. This ordering can
often induce local dependence among nearby variables. However, in complex data,
the width of this dependence may vary, making simple assumptions such as a
constant neighborhood size unrealistic. We propose a framework for learning
this local dependence based on estimating the inverse of the Cholesky factor of
the covariance matrix. Penalized maximum likelihood estimation of this matrix
yields a simple regression interpretation for local dependence in which
variables are predicted by their neighbors. Our proposed method involves
solving a convex, penalized Gaussian likelihood problem with a hierarchical
group lasso penalty. The problem decomposes into independent subproblems which
can be solved efficiently in parallel using first-order methods. Our method
yields a sparse, symmetric, positive definite estimator of the precision
matrix, encoding a Gaussian graphical model. We derive theoretical results not
found in existing methods attaining this structure. In particular, our
conditions for signed support recovery and estimation consistency rates in
multiple norms are as mild as those in a regression problem. Empirical results
show our method performing favorably compared to existing methods. We apply our
method to genomic data to flexibly model linkage disequilibrium. Our method is
also applied to improve the performance of discriminant analysis in sound
recording classification.","['Guo Yu', 'Jacob Bien']","['math.ST', 'stat.CO', 'stat.ME', 'stat.ML', 'stat.TH']",2016-04-25 21:20:51+00:00
http://arxiv.org/abs/1604.07407v1,Conversational Markers of Constructive Discussions,"Group discussions are essential for organizing every aspect of modern life,
from faculty meetings to senate debates, from grant review panels to papal
conclaves. While costly in terms of time and organization effort, group
discussions are commonly seen as a way of reaching better decisions compared to
solutions that do not require coordination between the individuals (e.g.
voting)---through discussion, the sum becomes greater than the parts. However,
this assumption is not irrefutable: anecdotal evidence of wasteful discussions
abounds, and in our own experiments we find that over 30% of discussions are
unproductive.
  We propose a framework for analyzing conversational dynamics in order to
determine whether a given task-oriented discussion is worth having or not. We
exploit conversational patterns reflecting the flow of ideas and the balance
between the participants, as well as their linguistic choices. We apply this
framework to conversations naturally occurring in an online collaborative world
exploration game developed and deployed to support this research. Using this
setting, we show that linguistic cues and conversational patterns extracted
from the first 20 seconds of a team discussion are predictive of whether it
will be a wasteful or a productive one.","['Vlad Niculae', 'Cristian Danescu-Niculescu-Mizil']","['cs.CL', 'cs.AI', 'cs.SI', 'physics.soc-ph', 'stat.ML']",2016-04-25 20:00:02+00:00
http://arxiv.org/abs/1604.07356v1,Fast nonlinear embeddings via structured matrices,"We present a new paradigm for speeding up randomized computations of several
frequently used functions in machine learning. In particular, our paradigm can
be applied for improving computations of kernels based on random embeddings.
Above that, the presented framework covers multivariate randomized functions.
As a byproduct, we propose an algorithmic approach that also leads to a
significant reduction of space complexity. Our method is based on careful
recycling of Gaussian vectors into structured matrices that share properties of
fully random matrices. The quality of the proposed structured approach follows
from combinatorial properties of the graphs encoding correlations between rows
of these structured matrices. Our framework covers as special cases already
known structured approaches such as the Fast Johnson-Lindenstrauss Transform,
but is much more general since it can be applied also to highly nonlinear
embeddings. We provide strong concentration results showing the quality of the
presented paradigm.","['Krzysztof Choromanski', 'Francois Fagan']","['stat.ML', 'cs.LG', 'G.3']",2016-04-25 18:33:59+00:00
http://arxiv.org/abs/1604.07178v1,Weighted Spectral Cluster Ensemble,"Clustering explores meaningful patterns in the non-labeled data sets. Cluster
Ensemble Selection (CES) is a new approach, which can combine individual
clustering results for increasing the performance of the final results.
Although CES can achieve better final results in comparison with individual
clustering algorithms and cluster ensemble methods, its performance can be
dramatically affected by its consensus diversity metric and thresholding
procedure. There are two problems in CES: 1) most of the diversity metrics is
based on heuristic Shannon's entropy and 2) estimating threshold values are
really hard in practice. The main goal of this paper is proposing a robust
approach for solving the above mentioned problems. Accordingly, this paper
develops a novel framework for clustering problems, which is called Weighted
Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community
detection arena and graph based clustering. Under this framework, a new version
of spectral clustering, which is called Two Kernels Spectral Clustering, is
used for generating graphs based individual clustering results. Further, by
using modularity, which is a famous metric in the community detection, on the
transformed graph representation of individual clustering results, our approach
provides an effective diversity estimation for individual clustering results.
Moreover, this paper introduces a new approach for combining the evaluated
individual clustering results without the procedure of thresholding.
Experimental study on varied data sets demonstrates that the prosed approach
achieves superior performance to state-of-the-art methods.","['Muhammad Yousefnezhad', 'Daoqiang Zhang']","['cs.LG', 'cs.AI', 'stat.ML']",2016-04-25 09:29:21+00:00
http://arxiv.org/abs/1604.07143v2,Neural Random Forests,"Given an ensemble of randomized regression trees, it is possible to
restructure them as a collection of multilayered neural networks with
particular connection weights. Following this principle, we reformulate the
random forest method of Breiman (2001) into a neural network setting, and in
turn propose two new hybrid procedures that we call neural random forests. Both
predictors exploit prior knowledge of regression trees for their architecture,
have less parameters to tune than standard networks, and less restrictions on
the geometry of the decision boundaries than trees. Consistency results are
proved, and substantial numerical evidence is provided on both synthetic and
real data sets to assess the excellent performance of our methods in a large
variety of prediction problems.","['Gérard Biau', 'Erwan Scornet', 'Johannes Welbl']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2016-04-25 06:43:47+00:00
http://arxiv.org/abs/1604.07101v2,Double Thompson Sampling for Dueling Bandits,"In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for
dueling bandit problems. As indicated by its name, D-TS selects both the first
and the second candidates according to Thompson Sampling. Specifically, D-TS
maintains a posterior distribution for the preference matrix, and chooses the
pair of arms for comparison by sampling twice from the posterior distribution.
This simple algorithm applies to general Copeland dueling bandits, including
Condorcet dueling bandits as its special case. For general Copeland dueling
bandits, we show that D-TS achieves $O(K^2 \log T)$ regret. For Condorcet
dueling bandits, we further simplify the D-TS algorithm and show that the
simplified D-TS algorithm achieves $O(K \log T + K^2 \log \log T)$ regret.
Simulation results based on both synthetic and real-world data demonstrate the
efficiency of the proposed D-TS algorithm.","['Huasen Wu', 'Xin Liu']","['cs.LG', 'stat.ML']",2016-04-25 00:38:16+00:00
http://arxiv.org/abs/1604.07093v1,Semi-supervised Vocabulary-informed Learning,"Despite significant progress in object categorization, in recent years, a
number of important challenges remain, mainly, ability to learn from limited
labeled data and ability to recognize object classes within large, potentially
open, set of labels. Zero-shot learning is one way of addressing these
challenges, but it has only been shown to work with limited sized class
vocabularies and typically requires separation between supervised and
unsupervised classes, allowing former to inform the latter but not vice versa.
We propose the notion of semi-supervised vocabulary-informed learning to
alleviate the above mentioned challenges and address problems of supervised,
zero-shot and open set recognition using a unified framework. Specifically, we
propose a maximum margin framework for semantic manifold-based recognition that
incorporates distance constraints from (both supervised and unsupervised)
vocabulary atoms, ensuring that labeled samples are projected closest to their
correct prototypes, in the embedding space, than to others. We show that
resulting model shows improvements in supervised, zero-shot, and large open set
recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.","['Yanwei Fu', 'Leonid Sigal']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.AP', 'stat.ML']",2016-04-24 23:36:36+00:00
http://arxiv.org/abs/1604.07070v3,Stochastic Variance-Reduced ADMM,"The alternating direction method of multipliers (ADMM) is a powerful
optimization solver in machine learning. Recently, stochastic ADMM has been
integrated with variance reduction methods for stochastic gradient, leading to
SAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration
complexities. However, their space requirements can still be high. In this
paper, we propose an integration of ADMM with the method of stochastic variance
reduced gradient (SVRG). Unlike another recent integration attempt called
SCAS-ADMM, the proposed algorithm retains the fast convergence benefits of
SAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage
requirement is very low, even independent of the sample size $n$. We also
extend the proposed method for nonconvex problems, and obtain a convergence
rate of $O(1/T)$. Experimental results demonstrate that it is as fast as
SAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much
bigger data sets.","['Shuai Zheng', 'James T. Kwok']","['cs.LG', 'math.OC', 'stat.ML']",2016-04-24 18:50:58+00:00
http://arxiv.org/abs/1604.06968v2,Agnostic Estimation of Mean and Covariance,"We consider the problem of estimating the mean and covariance of a
distribution from iid samples in $\mathbb{R}^n$, in the presence of an $\eta$
fraction of malicious noise; this is in contrast to much recent work where the
noise itself is assumed to be from a distribution of known type. The agnostic
problem includes many interesting special cases, e.g., learning the parameters
of a single Gaussian (or finding the best-fit Gaussian) when $\eta$ fraction of
data is adversarially corrupted, agnostically learning a mixture of Gaussians,
agnostic ICA, etc. We present polynomial-time algorithms to estimate the mean
and covariance with error guarantees in terms of information-theoretic lower
bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value
Decomposition.","['Kevin A. Lai', 'Anup B. Rao', 'Santosh Vempala']","['cs.DS', 'cs.LG', 'stat.ML']",2016-04-24 00:23:51+00:00
http://arxiv.org/abs/1604.06952v3,"Visualization of Jacques Lacan's Registers of the Psychoanalytic Field, and Discovery of Metaphor and of Metonymy. Analytical Case Study of Edgar Allan Poe's ""The Purloined Letter""","We start with a description of Lacan's work that we then take into our
analytics methodology. In a first investigation, a Lacan-motivated template of
the Poe story is fitted to the data. A segmentation of the storyline is used in
order to map out the diachrony. Based on this, it will be shown how synchronous
aspects, potentially related to Lacanian registers, can be sought. This
demonstrates the effectiveness of an approach based on a model template of the
storyline narrative. In a second and more comprehensive investigation, we
develop an approach for revealing, that is, uncovering, Lacanian register
relationships. Objectives of this work include the wide and general application
of our methodology. This methodology is strongly based on the ""letting the data
speak"" Correspondence Analysis analytics platform of Jean-Paul Benz\'ecri, that
is also the geometric data analysis, both qualitative and quantitative
analytics, developed by Pierre Bourdieu.","['Fionn Murtagh', 'Giuseppe Iurato']","['cs.CL', 'stat.ML', '62H25, 62H30', 'I.5.3; I.5.4; I.2; G.2.2; G.3']",2016-04-23 20:48:50+00:00
http://arxiv.org/abs/1604.06815v2,Non-convex Global Minimization and False Discovery Rate Control for the TREX,"The TREX is a recently introduced method for performing sparse
high-dimensional regression. Despite its statistical promise as an alternative
to the lasso, square-root lasso, and scaled lasso, the TREX is computationally
challenging in that it requires solving a non-convex optimization problem. This
paper shows a remarkable result: despite the non-convexity of the TREX problem,
there exists a polynomial-time algorithm that is guaranteed to find the global
minimum. This result adds the TREX to a very short list of non-convex
optimization problems that can be globally optimized (principal components
analysis being a famous example). After deriving and developing this new
approach, we demonstrate that (i) the ability of the preexisting TREX heuristic
to reach the global minimum is strongly dependent on the difficulty of the
underlying statistical problem, (ii) the new polynomial-time algorithm for TREX
permits a novel variable ranking and selection scheme, (iii) this scheme can be
incorporated into a rule that controls the false discovery rate (FDR) of
included features in the model. To achieve this last aim, we provide an
extension of the results of Barber & Candes (2015) to establish that the
knockoff filter framework can be applied to the TREX. This investigation thus
provides both a rare case study of a heuristic for non-convex optimization and
a novel way of exploiting non-convexity for statistical inference.","['Jacob Bien', 'Irina Gaynanova', 'Johannes Lederer', 'Christian Müller']","['stat.ML', 'cs.OH', 'stat.CO', 'stat.ME']",2016-04-22 20:28:55+00:00
http://arxiv.org/abs/1604.06749v3,Learning a Tree-Structured Ising Model in Order to Make Predictions,"We study the problem of learning a tree Ising model from samples such that
subsequent predictions made using the model are accurate. The prediction task
considered in this paper is that of predicting the values of a subset of
variables given values of some other subset of variables. Virtually all
previous work on graphical model learning has focused on recovering the true
underlying graph. We define a distance (""small set TV"" or ssTV) between
distributions $P$ and $Q$ by taking the maximum, over all subsets $\mathcal{S}$
of a given size, of the total variation between the marginals of $P$ and $Q$ on
$\mathcal{S}$; this distance captures the accuracy of the prediction task of
interest. We derive non-asymptotic bounds on the number of samples needed to
get a distribution (from the same class) with small ssTV relative to the one
generating the samples. One of the main messages of this paper is that far
fewer samples are needed than for recovering the underlying tree, which means
that accurate predictions are possible using the wrong tree.","['Guy Bresler', 'Mina Karzand']","['math.ST', 'cs.IT', 'math.IT', 'math.PR', 'stat.ML', 'stat.TH']",2016-04-22 16:57:30+00:00
http://arxiv.org/abs/1604.06730v1,Developing an ICU scoring system with interaction terms using a genetic algorithm,"ICU mortality scoring systems attempt to predict patient mortality using
predictive models with various clinical predictors. Examples of such systems
are APACHE, SAPS and MPM. However, most such scoring systems do not actively
look for and include interaction terms, despite physicians intuitively taking
such interactions into account when making a diagnosis. One barrier to
including such terms in predictive models is the difficulty of using most
variable selection methods in high-dimensional datasets. A genetic algorithm
framework for variable selection with logistic regression models is used to
search for two-way interaction terms in a clinical dataset of adult ICU
patients, with separate models being built for each category of diagnosis upon
admittance to the ICU. The models had good discrimination across all
categories, with a weighted average AUC of 0.84 (>0.90 for several categories)
and the genetic algorithm was able to find several significant interaction
terms, which may be able to provide greater insight into mortality prediction
for health practitioners. The GA selected models had improved performance
against stepwise selection and random forest models, and provides greater
flexibility in terms of variable selection by being able to optimize over any
modeler-defined model performance metric instead of a specific variable
importance metric.","['Chee Chun Gan', 'Gerard Learmonth']","['cs.NE', 'cs.LG', 'stat.ML']",2016-04-22 16:20:29+00:00
http://arxiv.org/abs/1604.06727v1,An improved chromosome formulation for genetic algorithms applied to variable selection with the inclusion of interaction terms,"Genetic algorithms are a well-known method for tackling the problem of
variable selection. As they are non-parametric and can use a large variety of
fitness functions, they are well-suited as a variable selection wrapper that
can be applied to many different models. In almost all cases, the chromosome
formulation used in these genetic algorithms consists of a binary vector of
length n for n potential variables indicating the presence or absence of the
corresponding variables. While the aforementioned chromosome formulation has
exhibited good performance for relatively small n, there are potential problems
when the size of n grows very large, especially when interaction terms are
considered. We introduce a modification to the standard chromosome formulation
that allows for better scalability and model sparsity when interaction terms
are included in the predictor search space. Experimental results show that the
indexed chromosome formulation demonstrates improved computational efficiency
and sparsity on high-dimensional datasets with interaction terms compared to
the standard chromosome formulation.","['Chee Chun Gan', 'Gerard Learmonth']","['stat.ML', 'cs.NE']",2016-04-22 16:14:55+00:00
http://arxiv.org/abs/1604.06637v3,Robust and Sparse Regression via $γ$-divergence,"In high-dimensional data, many sparse regression methods have been proposed.
However, they may not be robust against outliers. Recently, the use of density
power weight has been studied for robust parameter estimation and the
corresponding divergences have been discussed. One of such divergences is the
$\gamma$-divergence and the robust estimator using the $\gamma$-divergence is
known for having a strong robustness. In this paper, we consider the robust and
sparse regression based on $\gamma$-divergence. We extend the
$\gamma$-divergence to the regression problem and show that it has a strong
robustness under heavy contamination even when outliers are heterogeneous. The
loss function is constructed by an empirical estimate of the
$\gamma$-divergence with sparse regularization and the parameter estimate is
defined as the minimizer of the loss function. To obtain the robust and sparse
estimate, we propose an efficient update algorithm which has a monotone
decreasing property of the loss function. Particularly, we discuss a linear
regression problem with $L_1$ regularization in detail. In numerical
experiments and real data analyses, we see that the proposed method outperforms
past robust and sparse methods.","['Takayuki Kawashima', 'Hironori Fujisawa']","['stat.ME', 'stat.ML']",2016-04-22 12:53:27+00:00
http://arxiv.org/abs/1604.06626v2,The Mean Partition Theorem of Consensus Clustering,"To devise efficient solutions for approximating a mean partition in consensus
clustering, Dimitriadou et al. [3] presented a necessary condition of
optimality for a consensus function based on least square distances. We show
that their result is pivotal for deriving interesting properties of consensus
clustering beyond optimization. For this, we present the necessary condition of
optimality in a slightly stronger form in terms of the Mean Partition Theorem
and extend it to the Expected Partition Theorem. To underpin its versatility,
we show three examples that apply the Mean Partition Theorem: (i) equivalence
of the mean partition and optimal multiple alignment, (ii) construction of
profiles and motifs, and (iii) relationship between consensus clustering and
cluster stability.",['Brijnesh J. Jain'],"['cs.LG', 'cs.CV', 'stat.ML']",2016-04-22 12:32:37+00:00
http://arxiv.org/abs/1604.06518v4,Approximation Vector Machines for Large-scale Online Learning,"One of the most challenging problems in kernel online learning is to bound
the model size and to promote the model sparsity. Sparse models not only
improve computation and memory usage, but also enhance the generalization
capacity, a principle that concurs with the law of parsimony. However,
inappropriate sparsity modeling may also significantly degrade the performance.
In this paper, we propose Approximation Vector Machine (AVM), a model that can
simultaneously encourage the sparsity and safeguard its risk in compromising
the performance. When an incoming instance arrives, we approximate this
instance by one of its neighbors whose distance to it is less than a predefined
threshold. Our key intuition is that since the newly seen instance is expressed
by its nearby neighbor the optimal performance can be analytically formulated
and maintained. We develop theoretical foundations to support this intuition
and further establish an analysis to characterize the gap between the
approximation and optimal solutions. This gap crucially depends on the
frequency of approximation and the predefined threshold. We perform the
convergence analysis for a wide spectrum of loss functions including Hinge,
smooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and
$\epsilon$-insensitive for regression task. We conducted extensive experiments
for classification task in batch and online modes, and regression task in
online mode over several benchmark datasets. The results show that our proposed
AVM achieved a comparable predictive performance with current state-of-the-art
methods while simultaneously achieving significant computational speed-up due
to the ability of the proposed AVM in maintaining the model size.","['Trung Le', 'Tu Dinh Nguyen', 'Vu Nguyen', 'Dinh Phung']","['cs.LG', 'stat.ML']",2016-04-22 01:57:01+00:00
http://arxiv.org/abs/1604.06498v3,Stabilized Sparse Online Learning for Sparse Data,"Stochastic gradient descent (SGD) is commonly used for optimization in
large-scale machine learning problems. Langford et al. (2009) introduce a
sparse online learning method to induce sparsity via truncated gradient. With
high-dimensional sparse data, however, the method suffers from slow convergence
and high variance due to the heterogeneity in feature sparsity. To mitigate
this issue, we introduce a stabilized truncated stochastic gradient descent
algorithm. We employ a soft-thresholding scheme on the weight vector where the
imposed shrinkage is adaptive to the amount of information available in each
feature. The variability in the resulted sparse weight vector is further
controlled by stability selection integrated with the informative truncation.
To facilitate better convergence, we adopt an annealing strategy on the
truncation rate, which leads to a balanced trade-off between exploration and
exploitation in learning a sparse weight vector. Numerical experiments show
that our algorithm compares favorably with the original algorithm in terms of
prediction accuracy, achieved sparsity and stability.","['Yuting Ma', 'Tian Zheng']","['stat.ML', 'cs.LG']",2016-04-21 21:34:34+00:00
http://arxiv.org/abs/1604.06443v2,Robust Estimators in High Dimensions without the Computational Intractability,"We study high-dimensional distribution learning in an agnostic setting where
an adversary is allowed to arbitrarily corrupt an $\varepsilon$-fraction of the
samples. Such questions have a rich history spanning statistics, machine
learning and theoretical computer science. Even in the most basic settings, the
only known approaches are either computationally inefficient or lose
dimension-dependent factors in their error guarantees. This raises the
following question:Is high-dimensional agnostic distribution learning even
possible, algorithmically?
  In this work, we obtain the first computationally efficient algorithms with
dimension-independent error guarantees for agnostically learning several
fundamental classes of high-dimensional distributions: (1) a single Gaussian,
(2) a product distribution on the hypercube, (3) mixtures of two product
distributions (under a natural balancedness condition), and (4) mixtures of
spherical Gaussians. Our algorithms achieve error that is independent of the
dimension, and in many cases scales nearly-linearly with the fraction of
adversarially corrupted samples. Moreover, we develop a general recipe for
detecting and correcting corruptions in high-dimensions, that may be applicable
to many other problems.","['Ilias Diakonikolas', 'Gautam Kamath', 'Daniel Kane', 'Jerry Li', 'Ankur Moitra', 'Alistair Stewart']","['cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2016-04-21 19:54:24+00:00
http://arxiv.org/abs/1604.06335v1,Markov models for ocular fixation locations in the presence and absence of colour,"We propose to model the fixation locations of the human eye when observing a
still image by a Markovian point process in R 2 . Our approach is data driven
using k-means clustering of the fixation locations to identify distinct salient
regions of the image, which in turn correspond to the states of our Markov
chain. Bayes factors are computed as model selection criterion to determine the
number of clusters. Furthermore, we demonstrate that the behaviour of the human
eye differs from this model when colour information is removed from the given
image.","['Adam B. Kashlak', 'Eoin Devane', 'Helge Dietert', 'Henry Jackson']","['stat.AP', 'stat.ML', '60J20 (primary), 62P10 (Secondary)']",2016-04-21 14:50:27+00:00
http://arxiv.org/abs/1604.06194v1,Dynamic matrix factorization with social influence,"Matrix factorization is a key component of collaborative filtering-based
recommendation systems because it allows us to complete sparse user-by-item
ratings matrices under a low-rank assumption that encodes the belief that
similar users give similar ratings and that similar items garner similar
ratings. This paradigm has had immeasurable practical success, but it is not
the complete story for understanding and inferring the preferences of people.
First, peoples' preferences and their observable manifestations as ratings
evolve over time along general patterns of trajectories. Second, an individual
person's preferences evolve over time through influence of their social
connections. In this paper, we develop a unified process model for both types
of dynamics within a state space approach, together with an efficient
optimization scheme for estimation within that model. The model combines
elements from recent developments in dynamic matrix factorization, opinion
dynamics and social learning, and trust-based recommendation. The estimation
builds upon recent advances in numerical nonlinear optimization. Empirical
results on a large-scale data set from the Epinions website demonstrate
consistent reduction in root mean squared error by consideration of the two
types of dynamics.","['Aleksandr Y. Aravkin', 'Kush R. Varshney', 'Liu Yang']","['stat.ML', 'cs.IR', 'cs.SI', 'math.OC', '90C06, 81P50, 65K10, 62F35, 47N30']",2016-04-21 06:51:22+00:00
http://arxiv.org/abs/1604.06057v2,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,"Learning goal-directed behavior in environments with sparse feedback is a
major challenge for reinforcement learning algorithms. The primary difficulty
arises due to insufficient exploration, resulting in an agent being unable to
learn robust value functions. Intrinsically motivated agents can explore new
behavior for its own sake rather than to directly solve problems. Such
intrinsic behaviors could eventually help the agent solve tasks posed by the
environment. We present hierarchical-DQN (h-DQN), a framework to integrate
hierarchical value functions, operating at different temporal scales, with
intrinsically motivated deep reinforcement learning. A top-level value function
learns a policy over intrinsic goals, and a lower-level function learns a
policy over atomic actions to satisfy the given goals. h-DQN allows for
flexible goal specifications, such as functions over entities and relations.
This provides an efficient space for exploration in complicated environments.
We demonstrate the strength of our approach on two problems with very sparse,
delayed feedback: (1) a complex discrete stochastic decision process, and (2)
the classic ATARI game `Montezuma's Revenge'.","['Tejas D. Kulkarni', 'Karthik R. Narasimhan', 'Ardavan Saeedi', 'Joshua B. Tenenbaum']","['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE', 'stat.ML']",2016-04-20 18:47:48+00:00
http://arxiv.org/abs/1604.06036v1,Random Projection Estimation of Discrete-Choice Models with Large Choice Sets,"We introduce sparse random projection, an important dimension-reduction tool
from machine learning, for the estimation of discrete-choice models with
high-dimensional choice sets. Initially, high-dimensional data are compressed
into a lower-dimensional Euclidean space using random projections.
Subsequently, estimation proceeds using cyclic monotonicity moment inequalities
implied by the multinomial choice model; the estimation procedure is
semi-parametric and does not require explicit distributional assumptions to be
made regarding the random utility errors. The random projection procedure is
justified via the Johnson-Lindenstrauss Lemma -- the pairwise distances between
data points are preserved during data compression, which we exploit to show
convergence of our estimator. The estimator works well in simulations and in an
application to a supermarket scanner dataset.","['Khai X. Chiong', 'Matthew Shum']",['stat.ML'],2016-04-20 17:11:44+00:00
http://arxiv.org/abs/1604.06020v1,Constructive Preference Elicitation by Setwise Max-margin Learning,"In this paper we propose an approach to preference elicitation that is
suitable to large configuration spaces beyond the reach of existing
state-of-the-art approaches. Our setwise max-margin method can be viewed as a
generalization of max-margin learning to sets, and can produce a set of
""diverse"" items that can be used to ask informative queries to the user.
Moreover, the approach can encourage sparsity in the parameter space, in order
to favor the assessment of utility towards combinations of weights that
concentrate on just few features. We present a mixed integer linear programming
formulation and show how our approach compares favourably with Bayesian
preference elicitation alternatives and easily scales to realistic datasets.","['Stefano Teso', 'Andrea Passerini', 'Paolo Viappiani']","['stat.ML', 'cs.AI', 'cs.LG', '68T05']",2016-04-20 16:22:01+00:00
http://arxiv.org/abs/1604.05976v1,Computational Drug Repositioning Using Continuous Self-controlled Case Series,"Computational Drug Repositioning (CDR) is the task of discovering potential
new indications for existing drugs by mining large-scale heterogeneous
drug-related data sources. Leveraging the patient-level temporal ordering
information between numeric physiological measurements and various drug
prescriptions provided in Electronic Health Records (EHRs), we propose a
Continuous Self-controlled Case Series (CSCCS) model for CDR. As an initial
evaluation, we look for drugs that can control Fasting Blood Glucose (FBG)
level in our experiments. Applying CSCCS to the Marshfield Clinic EHR,
well-known drugs that are indicated for controlling blood glucose level are
rediscovered. Furthermore, some drugs with recent literature support for the
potential effect of blood glucose level control are also identified.","['Zhaobin Kuang', 'James Thomson', 'Michael Caldwell', 'Peggy Peissig', 'Ron Stewart', 'David Page']","['stat.AP', 'stat.ML']",2016-04-20 14:28:44+00:00
http://arxiv.org/abs/1604.05878v1,A Factorization Machine Framework for Testing Bigram Embeddings in Knowledgebase Completion,"Embedding-based Knowledge Base Completion models have so far mostly combined
distributed representations of individual entities or relations to compute
truth scores of missing links. Facts can however also be represented using
pairwise embeddings, i.e. embeddings for pairs of entities and relations. In
this paper we explore such bigram embeddings with a flexible Factorization
Machine model and several ablations from it. We investigate the relevance of
various bigram types on the fb15k237 dataset and find relative improvements
compared to a compositional model.","['Johannes Welbl', 'Guillaume Bouchard', 'Sebastian Riedel']","['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']",2016-04-20 09:58:56+00:00
http://arxiv.org/abs/1604.05819v1,Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive Models,"Predictive models are finding an increasing number of applications in many
industries. As a result, a practical means for trading-off the cost of
deploying a model versus its effectiveness is needed. Our work is motivated by
risk prediction problems in healthcare. Cost-structures in domains such as
healthcare are quite complex, posing a significant challenge to existing
approaches. We propose a novel framework for designing cost-sensitive
structured regularizers that is suitable for problems with complex cost
dependencies. We draw upon a surprising connection to boolean circuits. In
particular, we represent the problem costs as a multi-layer boolean circuit,
and then use properties of boolean circuits to define an extended feature
vector and a group regularizer that exactly captures the underlying cost
structure. The resulting regularizer may then be combined with a fidelity
function to perform model prediction, for example. For the challenging
real-world application of risk prediction for sepsis in intensive care units,
the use of our regularizer leads to models that are in harmony with the
underlying cost structure and thus provide an excellent prediction accuracy
versus cost tradeoff.","['Daniel P. Robinson', 'Suchi Saria']","['stat.ML', 'cs.LG']",2016-04-20 04:59:08+00:00
http://arxiv.org/abs/1604.05449v1,Streaming Label Learning for Modeling Labels on the Fly,"It is challenging to handle a large volume of labels in multi-label learning.
However, existing approaches explicitly or implicitly assume that all the
labels in the learning process are given, which could be easily violated in
changing environments. In this paper, we define and study streaming label
learning (SLL), i.e., labels are arrived on the fly, to model newly arrived
labels with the help of the knowledge learned from past labels. The core of SLL
is to explore and exploit the relationships between new labels and past labels
and then inherit the relationship into hypotheses of labels to boost the
performance of new classifiers. In specific, we use the label
self-representation to model the label relationship, and SLL will be divided
into two steps: a regression problem and a empirical risk minimization (ERM)
problem. Both problems are simple and can be efficiently solved. We further
show that SLL can generate a tighter generalization error bound for new labels
than the general ERM framework with trace norm or Frobenius norm
regularization. Finally, we implement extensive experiments on various
benchmark datasets to validate the new setting. And results show that SLL can
effectively handle the constantly emerging new labels and provides excellent
classification performance.","['Shan You', 'Chang Xu', 'Yunhe Wang', 'Chao Xu', 'Dacheng Tao']","['stat.ML', 'cs.LG']",2016-04-19 07:12:29+00:00
http://arxiv.org/abs/1604.05417v3,Triplet Probabilistic Embedding for Face Verification and Clustering,"Despite significant progress made over the past twenty five years,
unconstrained face verification remains a challenging problem. This paper
proposes an approach that couples a deep CNN-based approach with a
low-dimensional discriminative embedding learned using triplet probability
constraints to solve the unconstrained face verification problem. Aside from
yielding performance improvements, this embedding provides significant
advantages in terms of memory and for post-processing operations like subject
specific clustering. Experiments on the challenging IJB-A dataset show that the
proposed algorithm performs comparably or better than the state of the art
methods in verification and identification metrics, while requiring much less
training data and training time. The superior performance of the proposed
method on the CFP dataset shows that the representation learned by our deep CNN
is robust to extreme pose variation. Furthermore, we demonstrate the robustness
of the deep features to challenges including age, pose, blur and clutter by
performing simple clustering experiments on both IJB-A and LFW datasets.","['Swami Sankaranarayanan', 'Azadeh Alavi', 'Carlos Castillo', 'Rama Chellappa']","['cs.CV', 'cs.LG', 'stat.ML']",2016-04-19 03:29:56+00:00
http://arxiv.org/abs/1604.05377v1,Churn analysis using deep convolutional neural networks and autoencoders,"Customer temporal behavioral data was represented as images in order to
perform churn prediction by leveraging deep learning architectures prominent in
image classification. Supervised learning was performed on labeled data of over
6 million customers using deep convolutional neural networks, which achieved an
AUC of 0.743 on the test dataset using no more than 12 temporal features for
each customer. Unsupervised learning was conducted using autoencoders to better
understand the reasons for customer churn. Images that maximally activate the
hidden units of an autoencoder trained with churned customers reveal ample
opportunities for action to be taken to prevent churn among strong data, no
voice users.","['Artit Wangperawong', 'Cyrille Brun', 'Olav Laudy', 'Rujikorn Pavasuthipaisit']","['stat.ML', 'cs.LG', 'cs.NE']",2016-04-18 23:18:23+00:00
http://arxiv.org/abs/1604.05266v7,Finding Common Characteristics Among NBA Playoff and Championship Teams: A Machine Learning Approach,"In this paper, we employ machine learning techniques to analyze seventeen
seasons (1999-2000 to 2015-2016) of NBA regular season data from every team to
determine the common characteristics among NBA playoff teams. Each team was
characterized by 26 predictor variables and one binary response variable taking
on a value of ""TRUE"" if a team had made the playoffs, and value of ""FALSE"" if a
team had missed the playoffs. After fitting an initial classification tree to
this problem, this tree was then pruned which decreased the test error rate.
Further to this, a random forest of classification trees was grown which
provided a very accurate model from which a variable importance plot was
generated to determine which predictor variables had the greatest influence on
the response variable. The result of this work was the conclusion that the most
important factors in characterizing a team's playoff eligibility are a team's
opponent number of assists per game, a team's opponent number of made two point
shots per game, and a team's number of steals per game. This seems to suggest
that defensive factors as opposed to offensive factors are the most important
characteristics shared among NBA playoff teams. We then use neural networks to
classify championship teams based on regular season data. From this, we show
that the most important factor in a team not winning a championship is that
team's opponent number of made three-point shots per game. This once again
implies that defensive characteristics are of great importance in not only
determining a team's playoff eligibility, but certainly, one can conclude that
a lack of perimeter defense negatively impacts a team's championship chances in
a given season. Further, it is shown that made two-point shots and defensive
rebounding are by far the most important factor in a team's chances at winning
a championship in a given season.",['Ikjyot Singh Kohli'],"['stat.ML', 'stat.AP']",2016-04-18 17:57:21+00:00
http://arxiv.org/abs/1604.05263v1,Chained Gaussian Processes,"Gaussian process models are flexible, Bayesian non-parametric approaches to
regression. Properties of multivariate Gaussians mean that they can be combined
linearly in the manner of additive models and via a link function (like in
generalized linear models) to handle non-Gaussian data. However, the link
function formalism is restrictive, link functions are always invertible and
must convert a parameter of interest to a linear combination of the underlying
processes. There are many likelihoods and models where a non-linear combination
is more appropriate. We term these more general models Chained Gaussian
Processes: the transformation of the GPs to the likelihood parameters will not
generally be invertible, and that implies that linearisation would only be
possible with multiple (localized) links, i.e. a chain. We develop an
approximate inference procedure for Chained GPs that is scalable and applicable
to any factorized likelihood. We demonstrate the approximation on a range of
likelihood functions.","['Alan D. Saul', 'James Hensman', 'Aki Vehtari', 'Neil D. Lawrence']","['stat.ML', 'cs.LG']",2016-04-18 17:46:23+00:00
http://arxiv.org/abs/1604.05251v2,"Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions","Kernel mean embeddings have recently attracted the attention of the machine
learning community. They map measures $\mu$ from some set $M$ to functions in a
reproducing kernel Hilbert space (RKHS) with kernel $k$. The RKHS distance of
two mapped measures is a semi-metric $d_k$ over $M$. We study three questions.
(I) For a given kernel, what sets $M$ can be embedded? (II) When is the
embedding injective over $M$ (in which case $d_k$ is a metric)? (III) How does
the $d_k$-induced topology compare to other topologies on $M$? The existing
machine learning literature has addressed these questions in cases where $M$ is
(a subset of) the finite regular Borel measures. We unify, improve and
generalise those results. Our approach naturally leads to continuous and
possibly even injective embeddings of (Schwartz-) distributions, i.e.,
generalised measures, but the reader is free to focus on measures only. In
particular, we systemise and extend various (partly known) equivalences between
different notions of universal, characteristic and strictly positive definite
kernels, and show that on an underlying locally compact Hausdorff space, $d_k$
metrises the weak convergence of probability measures if and only if $k$ is
continuous and characteristic.","['Carl-Johann Simon-Gabriel', 'Bernhard Schölkopf']","['stat.ML', 'math.FA', 'math.PR', 'G.3']",2016-04-18 17:16:22+00:00
http://arxiv.org/abs/1604.05307v1,Learning Sparse Additive Models with Interactions in High Dimensions,"A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is referred to as a
Sparse Additive Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in
\mathcal{S}}\phi_{l}(x_l)$, where $\mathcal{S} \subset [d]$, $|\mathcal{S}| \ll
d$. Assuming $\phi_l$'s and $\mathcal{S}$ to be unknown, the problem of
estimating $f$ from its samples has been studied extensively. In this work, we
consider a generalized SPAM, allowing for second order interaction terms. For
some $\mathcal{S}_1 \subset [d], \mathcal{S}_2 \subset {[d] \choose 2}$, the
function $f$ is assumed to be of the form: $$f(\mathbf{x}) = \sum_{p \in
\mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in
\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_{l},x_{l^{\prime}}).$$ Assuming
$\phi_{p},\phi_{(l,l^{\prime})}$, $\mathcal{S}_1$ and, $\mathcal{S}_2$ to be
unknown, we provide a randomized algorithm that queries $f$ and exactly
recovers $\mathcal{S}_1,\mathcal{S}_2$. Consequently, this also enables us to
estimate the underlying $\phi_p, \phi_{(l,l^{\prime})}$. We derive sample
complexity bounds for our scheme and also extend our analysis to include the
situation where the queries are corrupted with noise -- either stochastic, or
arbitrary but bounded. Lastly, we provide simulation results on synthetic data,
that validate our theoretical findings.","['Hemant Tyagi', 'Anastasios Kyrillidis', 'Bernd Gärtner', 'Andreas Krause']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2016-04-18 17:09:48+00:00
http://arxiv.org/abs/1604.05198v1,Locally Imposing Function for Generalized Constraint Neural Networks - A Study on Equality Constraints,"This work is a further study on the Generalized Constraint Neural Network
(GCNN) model [1], [2]. Two challenges are encountered in the study, that is, to
embed any type of prior information and to select its imposing schemes. The
work focuses on the second challenge and studies a new constraint imposing
scheme for equality constraints. A new method called locally imposing function
(LIF) is proposed to provide a local correction to the GCNN prediction
function, which therefore falls within Locally Imposing Scheme (LIS). In
comparison, the conventional Lagrange multiplier method is considered as
Globally Imposing Scheme (GIS) because its added constraint term exhibits a
global impact to its objective function. Two advantages are gained from LIS
over GIS. First, LIS enables constraints to fire locally and explicitly in the
domain only where they need on the prediction function. Second, constraints can
be implemented within a network setting directly. We attempt to interpret
several constraint methods graphically from a viewpoint of the locality
principle. Numerical examples confirm the advantages of the proposed method. In
solving boundary value problems with Dirichlet and Neumann constraints, the
GCNN model with LIF is possible to achieve an exact satisfaction of the
constraints.","['Linlin Cao', 'Ran He', 'Bao-Gang Hu']","['cs.NE', 'cs.LG', 'stat.ML']",2016-04-18 15:11:13+00:00
http://arxiv.org/abs/1604.05129v2,Memory shapes time perception and intertemporal choices,"There is a consensus that human and non-human subjects experience temporal
distortions in many stages of their perceptual and decision-making systems.
Similarly, intertemporal choice research has shown that decision-makers
undervalue future outcomes relative to immediate ones. Here we combine
techniques from information theory and artificial intelligence to show how both
temporal distortions and intertemporal choice preferences can be explained as a
consequence of the coding efficiency of sensorimotor representation. In
particular, the model implies that interactions that constrain future behavior
are perceived as being both longer in duration and more valuable. Furthermore,
using simulations of artificial agents, we investigate how memory constraints
enforce a renormalization of the perceived timescales. Our results show that
qualitatively different discount functions, such as exponential and hyperbolic
discounting, arise as a consequence of an agent's probabilistic model of the
world.","['Pedro A. Ortega', 'Naftali Tishby']","['q-bio.NC', 'cs.AI', 'stat.ML']",2016-04-18 13:17:55+00:00
http://arxiv.org/abs/1604.04960v1,Gaussian Copula Variational Autoencoders for Mixed Data,"The variational autoencoder (VAE) is a generative model with continuous
latent variables where a pair of probabilistic encoder (bottom-up) and decoder
(top-down) is jointly learned by stochastic gradient variational Bayes. We
first elaborate Gaussian VAE, approximating the local covariance matrix of the
decoder as an outer product of the principal direction at a position determined
by a sample drawn from Gaussian distribution. We show that this model, referred
to as VAE-ROC, better captures the data manifold, compared to the standard
Gaussian VAE where independent multivariate Gaussian was used to model the
decoder. Then we extend the VAE-ROC to handle mixed categorical and continuous
data. To this end, we employ Gaussian copula to model the local dependency in
mixed categorical and continuous data, leading to {\em Gaussian copula
variational autoencoder} (GCVAE). As in VAE-ROC, we use the rank-one
approximation for the covariance in the Gaussian copula, to capture the local
dependency structure in the mixed data. Experiments on various datasets
demonstrate the useful behaviour of VAE-ROC and GCVAE, compared to the standard
VAE.","['Suwon Suh', 'Seungjin Choi']","['stat.ML', 'cs.LG']",2016-04-18 02:14:07+00:00
http://arxiv.org/abs/1604.04942v4,Identifying global optimality for dictionary learning,"Learning new representations of input observations in machine learning is
often tackled using a factorization of the data. For many such problems,
including sparse coding and matrix completion, learning these factorizations
can be difficult, in terms of efficiency and to guarantee that the solution is
a global minimum. Recently, a general class of objectives have been
introduced-which we term induced dictionary learning models (DLMs)-that have an
induced convex form that enables global optimization. Though attractive
theoretically, this induced form is impractical, particularly for large or
growing datasets. In this work, we investigate the use of practical alternating
minimization algorithms for induced DLMs, that ensure convergence to global
optima. We characterize the stationary points of these models, and, using these
insights, highlight practical choices for the objectives. We then provide
theoretical and empirical evidence that alternating minimization, from a random
initialization, converges to global minima for a large subclass of induced
DLMs. In particular, we take advantage of the existence of the (potentially
unknown) convex induced form, to identify when stationary points are global
minima for the dictionary learning objective. We then provide an empirical
investigation into practical optimization choices for using alternating
minimization for induced DLMs, for both batch and stochastic gradient descent.","['Lei Le', 'Martha White']","['stat.ML', 'cs.LG']",2016-04-17 23:46:04+00:00
http://arxiv.org/abs/1604.04939v1,Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis,"Factor analysis aims to determine latent factors, or traits, which summarize
a given data set. Inter-battery factor analysis extends this notion to multiple
views of the data. In this paper we show how a nonlinear, nonparametric version
of these models can be recovered through the Gaussian process latent variable
model. This gives us a flexible formalism for multi-view learning where the
latent variables can be used both for exploratory purposes and for learning
representations that enable efficient inference for ambiguous estimation tasks.
Learning is performed in a Bayesian manner through the formulation of a
variational compression scheme which gives a rigorous lower bound on the log
likelihood. Our Bayesian framework provides strong regularization during
training, allowing the structure of the latent space to be determined
efficiently and automatically. We demonstrate this by producing the first (to
our knowledge) published results of learning from dozens of views, even when
data is scarce. We further show experimental results on several different types
of multi-view data sets and for different kinds of tasks, including exploratory
data analysis, generation, ambiguity modelling through latent priors and
classification.","['Andreas Damianou', 'Neil D. Lawrence', 'Carl Henrik Ek']","['stat.ML', 'cs.LG', 'math.PR', '60G15 (Primary) 58E30, 62-09', 'G.3; G.1.2; I.2.6; I.5.4']",2016-04-17 23:13:50+00:00
http://arxiv.org/abs/1604.04931v1,Regularizing Solutions to the MEG Inverse Problem Using Space-Time Separable Covariance Functions,"In magnetoencephalography (MEG) the conventional approach to source
reconstruction is to solve the underdetermined inverse problem independently
over time and space. Here we present how the conventional approach can be
extended by regularizing the solution in space and time by a Gaussian process
(Gaussian random field) model. Assuming a separable covariance function in
space and time, the computational complexity of the proposed model becomes
(without any further assumptions or restrictions) $\mathcal{O}(t^3 + n^3 +
m^2n)$, where $t$ is the number of time steps, $m$ is the number of sources,
and $n$ is the number of sensors. We apply the method to both simulated and
empirical data, and demonstrate the efficiency and generality of our Bayesian
source reconstruction approach which subsumes various classical approaches in
the literature.","['Arno Solin', 'Pasi Jylänki', 'Jaakko Kauramäki', 'Tom Heskes', 'Marcel A. J. van Gerven', 'Simo Särkkä']","['stat.AP', 'stat.ML']",2016-04-17 21:16:37+00:00
http://arxiv.org/abs/1604.04834v1,"Probabilistic Receiver Architecture Combining BP, MF, and EP for Multi-Signal Detection","Receiver algorithms which combine belief propagation (BP) with the mean field
(MF) approximation are well-suited for inference of both continuous and
discrete random variables. In wireless scenarios involving detection of
multiple signals, the standard construction of the combined BP-MF framework
includes the equalization or multi-user detection functions within the MF
subgraph. In this paper, we show that the MF approximation is not particularly
effective for multi-signal detection. We develop a new factor graph
construction for application of the BP-MF framework to problems involving the
detection of multiple signals. We then develop a low-complexity variant to the
proposed construction in which Gaussian BP is applied to the equalization
factors. In this case, the factor graph of the joint probability distribution
is divided into three subgraphs: (i) a MF subgraph comprised of the observation
factors and channel estimation, (ii) a Gaussian BP subgraph which is applied to
multi-signal detection, and (iii) a discrete BP subgraph which is applied to
demodulation and decoding. Expectation propagation is used to approximate
discrete distributions with a Gaussian distribution and links the discrete BP
and Gaussian BP subgraphs. The result is a probabilistic receiver architecture
with strong theoretical justification which can be applied to multi-signal
detection.","['Daniel J. Jakubisin', 'R. Michael Buehrer', 'Claudio R. C. M. da Silva']","['cs.IT', 'math.IT', 'stat.ML']",2016-04-17 06:50:42+00:00
http://arxiv.org/abs/1604.04741v1,Smoothed Hierarchical Dirichlet Process: A Non-Parametric Approach to Constraint Measures,"Time-varying mixture densities occur in many scenarios, for example, the
distributions of keywords that appear in publications may evolve from year to
year, video frame features associated with multiple targets may evolve in a
sequence. Any models that realistically cater to this phenomenon must exhibit
two important properties: the underlying mixture densities must have an unknown
number of mixtures, and there must be some ""smoothness"" constraints in place
for the adjacent mixture densities. The traditional Hierarchical Dirichlet
Process (HDP) may be suited to the first property, but certainly not the
second. This is due to how each random measure in the lower hierarchies is
sampled independent of each other and hence does not facilitate any temporal
correlations. To overcome such shortcomings, we proposed a new Smoothed
Hierarchical Dirichlet Process (sHDP). The key novelty of this model is that we
place a temporal constraint amongst the nearby discrete measures $\{G_j\}$ in
the form of symmetric Kullback-Leibler (KL) Divergence with a fixed bound $B$.
Although the constraint we place only involves a single scalar value, it
nonetheless allows for flexibility in the corresponding successive measures.
Remarkably, it also led us to infer the model within the stick-breaking process
where the traditional Beta distribution used in stick-breaking is now replaced
by a new constraint calculated from $B$. We present the inference algorithm and
elaborate on its solutions. Our experiment using NIPS keywords has shown the
desirable effect of the model.","['Cheng Luo', 'Yang Xiang', 'Richard Yi Da Xu']",['stat.ML'],2016-04-16 12:35:30+00:00
http://arxiv.org/abs/1604.04706v7,DS-MLR: Exploiting Double Separability for Scaling up Distributed Multinomial Logistic Regression,"Scaling multinomial logistic regression to datasets with very large number of
data points and classes is challenging. This is primarily because one needs to
compute the log-partition function on every data point. This makes distributing
the computation hard. In this paper, we present a distributed stochastic
gradient descent based optimization method (DS-MLR) for scaling up multinomial
logistic regression problems to massive scale datasets without hitting any
storage constraints on the data and model parameters. Our algorithm exploits
double-separability, an attractive property that allows us to achieve both data
as well as model parallelism simultaneously. In addition, we introduce a
non-blocking and asynchronous variant of our algorithm that avoids
bulk-synchronization. We demonstrate the versatility of DS-MLR to various
scenarios in data and model parallelism, through an extensive empirical study
using several real-world datasets. In particular, we demonstrate the
scalability of DS-MLR by solving an extreme multi-class classification problem
on the Reddit dataset (159 GB data, 358 GB parameters) where, to the best of
our knowledge, no other existing methods apply.","['Parameswaran Raman', 'Sriram Srinivasan', 'Shin Matsushima', 'Xinhua Zhang', 'Hyokun Yun', 'S. V. N. Vishwanathan']","['cs.LG', 'stat.ML']",2016-04-16 07:26:58+00:00
http://arxiv.org/abs/1604.04661v2,Parallelizing Word2Vec in Shared and Distributed Memory,"Word2Vec is a widely used algorithm for extracting low-dimensional vector
representations of words. It generated considerable excitement in the machine
learning and natural language processing (NLP) communities recently due to its
exceptional performance in many NLP applications such as named entity
recognition, sentiment analysis, machine translation and question answering.
State-of-the-art algorithms including those by Mikolov et al. have been
parallelized for multi-core CPU architectures but are based on vector-vector
operations that are memory-bandwidth intensive and do not efficiently use
computational resources. In this paper, we improve reuse of various data
structures in the algorithm through the use of minibatching, hence allowing us
to express the problem using matrix multiply operations. We also explore
different techniques to distribute word2vec computation across nodes in a
compute cluster, and demonstrate good strong scalability up to 32 nodes. In
combination, these techniques allow us to scale up the computation near
linearly across cores and nodes, and process hundreds of millions of words per
second, which is the fastest word2vec implementation to the best of our
knowledge.","['Shihao Ji', 'Nadathur Satish', 'Sheng Li', 'Pradeep Dubey']","['cs.DC', 'cs.CL', 'stat.ML']",2016-04-15 23:40:04+00:00
http://arxiv.org/abs/1604.04615v1,On deterministic conditions for subspace clustering under missing data,"In this paper we present deterministic analysis of sufficient conditions for
sparse subspace clustering under missing data, when data is assumed to come
from a Union of Subspaces (UoS) model. In this context we consider two cases,
namely Case I when all the points are sampled at the same co-ordinates, and
Case II when points are sampled at different locations. We show that results
for Case I directly follow from several existing results in the literature,
while results for Case II are not as straightforward and we provide a set of
dual conditions under which, perfect clustering holds true. We provide
extensive set of simulation results for clustering as well as completion of
data under missing entries, under the UoS model. Our experimental results
indicate that in contrast to the full data case, accurate clustering does not
imply accurate subspace identification and completion, indicating the natural
order of relative hardness of these problems.","['Wenqi Wang', 'Shuchin Aeron', 'Vaneet Aggarwal']","['cs.IT', 'math.IT', 'stat.ML']",2016-04-15 19:47:25+00:00
http://arxiv.org/abs/1604.04600v1,Estimation of low rank density matrices: bounds in Schatten norms and other distances,"Let ${\mathcal S}_m$ be the set of all $m\times m$ density matrices
(Hermitian positively semi-definite matrices of unit trace). Consider a problem
of estimation of an unknown density matrix $\rho\in {\mathcal S}_m$ based on
outcomes of $n$ measurements of observables $X_1,\dots, X_n\in {\mathbb H}_m$
(${\mathbb H}_m$ being the space of $m\times m$ Hermitian matrices) for a
quantum system identically prepared $n$ times in state $\rho.$ Outcomes
$Y_1,\dots, Y_n$ of such measurements could be described by a trace regression
model in which ${\mathbb E}_{\rho}(Y_j|X_j)={\rm tr}(\rho X_j), j=1,\dots, n.$
The design variables $X_1,\dots, X_n$ are often sampled at random from the
uniform distribution in an orthonormal basis $\{E_1,\dots, E_{m^2}\}$ of
${\mathbb H}_m$ (such as Pauli basis). The goal is to estimate the unknown
density matrix $\rho$ based on the data $(X_1,Y_1), \dots, (X_n,Y_n).$ Let $$
\hat Z:=\frac{m^2}{n}\sum_{j=1}^n Y_j X_j $$ and let $\check \rho$ be the
projection of $\hat Z$ onto the convex set ${\mathcal S}_m$ of density
matrices. It is shown that for estimator $\check \rho$ the minimax lower bounds
in classes of low rank density matrices (established earlier) are attained up
logarithmic factors for all Schatten $p$-norm distances, $p\in [1,\infty]$ and
for Bures version of quantum Hellinger distance. Moreover, for a slightly
modified version of estimator $\check \rho$ the same property holds also for
quantum relative entropy (Kullback-Leibler) distance between density matrices.","['Dong Xia', 'Vladimir Koltchinskii']","['stat.ML', 'math.ST', 'stat.TH']",2016-04-15 18:56:35+00:00
http://arxiv.org/abs/1604.04562v3,A Network-based End-to-End Trainable Task-oriented Dialogue System,"Teaching machines to accomplish tasks by conversing naturally with humans is
challenging. Currently, developing task-oriented dialogue systems requires
creating multiple components and typically this involves either a large amount
of handcrafting, or acquiring costly labelled datasets to solve a statistical
learning problem for each component. In this work we introduce a neural
network-based text-in, text-out end-to-end trainable goal-oriented dialogue
system along with a new way of collecting dialogue data based on a novel
pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue
systems easily and without making too many assumptions about the task at hand.
The results show that the model can converse with human subjects naturally
whilst helping them to accomplish tasks in a restaurant search domain.","['Tsung-Hsien Wen', 'David Vandyke', 'Nikola Mrksic', 'Milica Gasic', 'Lina M. Rojas-Barahona', 'Pei-Hao Su', 'Stefan Ultes', 'Steve Young']","['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']",2016-04-15 16:40:49+00:00
http://arxiv.org/abs/1604.04505v1,A short note on extension theorems and their connection to universal consistency in machine learning,"Statistical machine learning plays an important role in modern statistics and
computer science. One main goal of statistical machine learning is to provide
universally consistent algorithms, i.e., the estimator converges in probability
or in some stronger sense to the Bayes risk or to the Bayes decision function.
Kernel methods based on minimizing the regularized risk over a reproducing
kernel Hilbert space (RKHS) belong to these statistical machine learning
methods. It is in general unknown which kernel yields optimal results for a
particular data set or for the unknown probability measure. Hence various
kernel learning methods were proposed to choose the kernel and therefore also
its RKHS in a data adaptive manner. Nevertheless, many practitioners often use
the classical Gaussian RBF kernel or certain Sobolev kernels with good success.
The goal of this short note is to offer one possible theoretical explanation
for this empirical fact.","['Andreas Christmann', 'Florian Dumpert', 'Dao-Hong Xiang']","['stat.ML', 'cs.LG']",2016-04-15 13:51:57+00:00
http://arxiv.org/abs/1604.04451v2,Delta divergence: A novel decision cognizant measure of classifier incongruence,"Disagreement between two classifiers regarding the class membership of an
observation in pattern recognition can be indicative of an anomaly and its
nuance. As in general classifiers base their decision on class aposteriori
probabilities, the most natural approach to detecting classifier incongruence
is to use divergence. However, existing divergences are not particularly
suitable to gauge classifier incongruence. In this paper, we postulate the
properties that a divergence measure should satisfy and propose a novel
divergence measure, referred to as Delta divergence. In contrast to existing
measures, it is decision cognizant. The focus in Delta divergence on the
dominant hypotheses has a clutter reducing property, the significance of which
grows with increasing number of classes. The proposed measure satisfies other
important properties such as symmetry, and independence of classifier
confidence. The relationship of the proposed divergence to some baseline
measures is demonstrated experimentally, showing its superiority.","['Josef Kittler', 'Cemre Zor']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2016-04-15 12:06:48+00:00
http://arxiv.org/abs/1604.04434v1,Bayesian linear regression with Student-t assumptions,"As an automatic method of determining model complexity using the training
data alone, Bayesian linear regression provides us a principled way to select
hyperparameters. But one often needs approximation inference if distribution
assumption is beyond Gaussian distribution. In this paper, we propose a
Bayesian linear regression model with Student-t assumptions (BLRS), which can
be inferred exactly. In this framework, both conjugate prior and expectation
maximization (EM) algorithm are generalized. Meanwhile, we prove that the
maximum likelihood solution is equivalent to the standard Bayesian linear
regression with Gaussian assumptions (BLRG). The $q$-EM algorithm for BLRS is
nearly identical to the EM algorithm for BLRG. It is showed that $q$-EM for
BLRS can converge faster than EM for BLRG for the task of predicting online
news popularity.","['Chaobing Song', 'Shu-Tao Xia']","['cs.LG', 'stat.ML']",2016-04-15 11:21:27+00:00
http://arxiv.org/abs/1604.04348v3,Positive Definite Estimation of Large Covariance Matrix Using Generalized Nonconvex Penalties,"This work addresses the issue of large covariance matrix estimation in
high-dimensional statistical analysis. Recently, improved iterative algorithms
with positive-definite guarantee have been developed. However, these algorithms
cannot be directly extended to use a nonconvex penalty for sparsity inducing.
Generally, a nonconvex penalty has the capability of ameliorating the bias
problem of the popular convex lasso penalty, and thus is more advantageous. In
this work, we propose a class of positive-definite covariance estimators using
generalized nonconvex penalties. We develop a first-order algorithm based on
the alternating direction method framework to solve the nonconvex optimization
problem efficiently. The convergence of this algorithm has been proved.
Further, the statistical properties of the new estimators have been analyzed
for generalized nonconvex penalties. Moreover, extension of this algorithm to
covariance estimation from sketched measurements has been considered. The
performances of the new estimators have been demonstrated by both a simulation
study and a gene clustering example for tumor tissues. Code for the proposed
estimators is available at https://github.com/FWen/Nonconvex-PDLCE.git.","['Fei Wen', 'Yuan Yang', 'Peilin Liu', 'Robert C. Qiu']","['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2016-04-15 03:50:57+00:00
http://arxiv.org/abs/1604.04280v2,"Variational inference for rare variant detection in deep, heterogeneous next-generation sequencing data","The detection of rare variants is important for understanding the genetic
heterogeneity in mixed samples. Recently, next-generation sequencing (NGS)
technologies have enabled the identification of single nucleotide variants
(SNVs) in mixed samples with high resolution. Yet, the noise inherent in the
biological processes involved in next-generation sequencing necessitates the
use of statistical methods to identify true rare variants. We propose a novel
Bayesian statistical model and a variational expectation-maximization (EM)
algorithm to estimate non-reference allele frequency (NRAF) and identify SNVs
in heterogeneous cell populations. We demonstrate that our variational EM
algorithm has comparable sensitivity and specificity compared with a Markov
Chain Monte Carlo (MCMC) sampling inference algorithm, and is more
computationally efficient on tests of low coverage ($27\times$ and $298\times$)
data. Furthermore, we show that our model with a variational EM inference
algorithm has higher specificity than many state-of-the-art algorithms. In an
analysis of a directed evolution longitudinal yeast data set, we are able to
identify a time-series trend in non-reference allele frequency and detect novel
variants that have not yet been reported. Our model also detects the emergence
of a beneficial variant earlier than was previously shown, and a pair of
concomitant variants.","['Fan Zhang', 'Patrick Flaherty']","['q-bio.GN', 'stat.ML']",2016-04-14 19:58:38+00:00
http://arxiv.org/abs/1604.04198v4,Estimating parameters of nonlinear systems using the elitist particle filter based on evolutionary strategies,"In this article, we present the elitist particle filter based on evolutionary
strategies (EPFES) as an efficient approach for nonlinear system
identification. The EPFES is derived from the frequently-employed state-space
model, where the relevant information of the nonlinear system is captured by an
unknown state vector. Similar to classical particle filtering, the EPFES
consists of a set of particles and respective weights which represent different
realizations of the latent state vector and their likelihood of being the
solution of the optimization problem. As main innovation, the EPFES includes an
evolutionary elitist-particle selection which combines long-term information
with instantaneous sampling from an approximated continuous posterior
distribution. In this article, we propose two advancements of the
previously-published elitist-particle selection process. Further, the EPFES is
shown to be a generalization of the widely-used Gaussian particle filter and
thus evaluated with respect to the latter for two completely different
scenarios: First, we consider the so-called univariate nonstationary growth
model with time-variant latent state variable, where the evolutionary selection
of elitist particles is evaluated for non-recursively calculated particle
weights. Second, the problem of nonlinear acoustic echo cancellation is
addressed in a simulated scenario with speech as input signal: By using
long-term fitness measures, we highlight the efficacy of the well-generalizing
EPFES in estimating the nonlinear system even for large search spaces. Finally,
we illustrate similarities between the EPFES and evolutionary algorithms to
outline future improvements by fusing the achievements of both fields of
research.","['Christian Huemmer', 'Christian Hofmann', 'Roland Maas', 'Walter Kellermann']",['stat.ML'],2016-04-14 15:59:35+00:00
http://arxiv.org/abs/1604.04191v1,1-bit Matrix Completion: PAC-Bayesian Analysis of a Variational Approximation,"Due to challenging applications such as collaborative filtering, the matrix
completion problem has been widely studied in the past few years. Different
approaches rely on different structure assumptions on the matrix in hand. Here,
we focus on the completion of a (possibly) low-rank matrix with binary entries,
the so-called 1-bit matrix completion problem. Our approach relies on tools
from machine learning theory: empirical risk minimization and its convex
relaxations. We propose an algorithm to compute a variational approximation of
the pseudo-posterior. Thanks to the convex relaxation, the corresponding
minimization problem is bi-convex, and thus the method behaves well in
practice. We also study the performance of this variational approximation
through PAC-Bayesian learning bounds. On the contrary to previous works that
focused on upper bounds on the estimation error of M with various matrix norms,
we are able to derive from this analysis a PAC bound on the prediction error of
our algorithm.
  We focus essentially on convex relaxation through the hinge loss, for which
we present the complete analysis, a complete simulation study and a test on the
MovieLens data set. However, we also discuss a variational approximation to
deal with the logistic loss.","['Vincent Cottet', 'Pierre Alquier']",['stat.ML'],2016-04-14 15:42:03+00:00
http://arxiv.org/abs/1604.04182v1,Consistently Estimating Markov Chains with Noisy Aggregate Data,"We address the problem of estimating the parameters of a time-homogeneous
Markov chain given only noisy, aggregate data. This arises when a population of
individuals behave independently according to a Markov chain, but individual
sample paths cannot be observed due to limitations of the observation process
or the need to protect privacy. Instead, only population-level counts of the
number of individuals in each state at each time step are available. When these
counts are exact, a conditional least squares (CLS) estimator is known to be
consistent and asymptotically normal. We initiate the study of method of
moments estimators for this problem to handle the more realistic case when
observations are additionally corrupted by noise. We show that CLS can be
interpreted as a simple ""plug-in"" method of moments estimator. However, when
observations are noisy, it is not consistent because it fails to account for
additional variance introduced by the noise. We develop a new, simpler method
of moments estimator that bypasses this problem and is consistent under noisy
observations.","['Garrett Bernstein', 'Daniel Sheldon']","['cs.LG', 'stat.ML']",2016-04-14 15:13:06+00:00
http://arxiv.org/abs/1604.04173v2,Distribution-Free Predictive Inference For Regression,"We develop a general framework for distribution-free predictive inference in
regression, using conformal inference. The proposed methodology allows for the
construction of a prediction band for the response variable using any estimator
of the regression function. The resulting prediction band preserves the
consistency properties of the original estimator under standard assumptions,
while guaranteeing finite-sample marginal coverage even when these assumptions
do not hold. We analyze and compare, both empirically and theoretically, the
two major variants of our conformal framework: full conformal inference and
split conformal inference, along with a related jackknife method. These methods
offer different tradeoffs between statistical accuracy (length of resulting
prediction intervals) and computational efficiency. As extensions, we develop a
method for constructing valid in-sample prediction intervals called {\it
rank-one-out} conformal inference, which has essentially the same computational
efficiency as split conformal inference. We also describe an extension of our
procedures for producing prediction bands with locally varying length, in order
to adapt to heteroskedascity in the data. Finally, we propose a model-free
notion of variable importance, called {\it leave-one-covariate-out} or LOCO
inference. Accompanying this paper is an R package {\tt conformalInference}
that implements all of the proposals we have introduced. In the spirit of
reproducibility, all of our empirical results can also be easily (re)generated
using this package.","['Jing Lei', ""Max G'Sell"", 'Alessandro Rinaldo', 'Ryan J. Tibshirani', 'Larry Wasserman']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2016-04-14 14:46:16+00:00
http://arxiv.org/abs/1604.04054v1,Optimal Rates For Regularization Of Statistical Inverse Learning Problems,"We consider a statistical inverse learning problem, where we observe the
image of a function $f$ through a linear operator $A$ at i.i.d. random design
points $X_i$, superposed with an additive noise. The distribution of the design
points is unknown and can be very general. We analyze simultaneously the direct
(estimation of $Af$) and the inverse (estimation of $f$) learning problems. In
this general framework, we obtain strong and weak minimax optimal rates of
convergence (as the number of observations $n$ grows large) for a large class
of spectral regularization methods over regularity classes defined through
appropriate source conditions. This improves on or completes previous results
obtained in related settings. The optimality of the obtained rates is shown not
only in the exponent in $n$ but also in the explicit dependency of the constant
factor in the variance of the noise and the radius of the source condition set.","['Gilles Blanchard', 'Nicole Mücke']",['stat.ML'],2016-04-14 07:23:56+00:00
http://arxiv.org/abs/1604.03930v2,Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis,"This paper considers the problem of canonical-correlation analysis (CCA)
(Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a
pair of symmetric matrices. These are two fundamental problems in data analysis
and scientific computing with numerous applications in machine learning and
statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).
  We provide simple iterative algorithms, with improved runtimes, for solving
these problems that are globally linearly convergent with moderate dependencies
on the condition numbers and eigenvalue gaps of the matrices involved.
  We obtain our results by reducing CCA to the top-$k$ generalized eigenvector
problem. We solve this problem through a general framework that simply requires
black box access to an approximate linear system solver. Instantiating this
framework with accelerated gradient descent we obtain a running time of
$O(\frac{z k \sqrt{\kappa}}{\rho} \log(1/\epsilon) \log
\left(k\kappa/\rho\right))$ where $z$ is the total number of nonzero entries,
$\kappa$ is the condition number and $\rho$ is the relative eigenvalue gap of
the appropriate matrices.
  Our algorithm is linear in the input size and the number of components $k$ up
to a $\log(k)$ factor. This is essential for handling large-scale matrices that
appear in practice. To the best of our knowledge this is the first such
algorithm with global linear convergence. We hope that our results prompt
further research and ultimately improve the practical running time for
performing these important data analysis procedures on large data sets.","['Rong Ge', 'Chi Jin', 'Sham M. Kakade', 'Praneeth Netrapalli', 'Aaron Sidford']","['cs.LG', 'math.OC', 'stat.ML']",2016-04-13 19:57:46+00:00
http://arxiv.org/abs/1604.03912v1,Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics,"Inverse Reinforcement Learning (IRL) describes the problem of learning an
unknown reward function of a Markov Decision Process (MDP) from observed
behavior of an agent. Since the agent's behavior originates in its policy and
MDP policies depend on both the stochastic system dynamics as well as the
reward function, the solution of the inverse problem is significantly
influenced by both. Current IRL approaches assume that if the transition model
is unknown, additional samples from the system's dynamics are accessible, or
the observed behavior provides enough samples of the system's dynamics to solve
the inverse problem accurately. These assumptions are often not satisfied. To
overcome this, we present a gradient-based IRL approach that simultaneously
estimates the system's dynamics. By solving the combined optimization problem,
our approach takes into account the bias of the demonstrations, which stems
from the generating policy. The evaluation on a synthetic MDP and a transfer
learning task shows improvements regarding the sample efficiency as well as the
accuracy of the estimated reward functions and transition models.","['Michael Herman', 'Tobias Gindele', 'Jörg Wagner', 'Felix Schmitt', 'Wolfram Burgard']","['cs.AI', 'cs.LG', 'cs.SY', 'stat.ML']",2016-04-13 19:06:41+00:00
http://arxiv.org/abs/1604.03887v8,Algorithms for stochastic optimization with functional or expectation constraints,"This paper considers the problem of minimizing an expectation function over a
closed convex set, coupled with a {\color{black} functional or expectation}
constraint on either decision variables or problem parameters. We first present
a new stochastic approximation (SA) type algorithm, namely the cooperative SA
(CSA), to handle problems with the constraint on devision variables. We show
that this algorithm exhibits the optimal ${\cal O}(1/\epsilon^2)$ rate of
convergence, in terms of both optimality gap and constraint violation, when the
objective and constraint functions are generally convex, where $\epsilon$
denotes the optimality gap and infeasibility. Moreover, we show that this rate
of convergence can be improved to ${\cal O}(1/\epsilon)$ if the objective and
constraint functions are strongly convex. We then present a variant of CSA,
namely the cooperative stochastic parameter approximation (CSPA) algorithm, to
deal with the situation when the constraint is defined over problem parameters
and show that it exhibits similar optimal rate of convergence to CSA. It is
worth noting that CSA and CSPA are primal methods which do not require the
iterations on the dual space and/or the estimation on the size of the dual
variables. To the best of our knowledge, this is the first time that such
optimal SA methods for solving functional or expectation constrained stochastic
optimization are presented in the literature.","['Guanghui Lan', 'Zhiqiang Zhou']","['math.OC', 'stat.ML']",2016-04-13 17:49:30+00:00
http://arxiv.org/abs/1604.03853v2,Hierarchical Compound Poisson Factorization,"Non-negative matrix factorization models based on a hierarchical
Gamma-Poisson structure capture user and item behavior effectively in extremely
sparse data sets, making them the ideal choice for collaborative filtering
applications. Hierarchical Poisson factorization (HPF) in particular has proved
successful for scalable recommendation systems with extreme sparsity. HPF,
however, suffers from a tight coupling of sparsity model (absence of a rating)
and response model (the value of the rating), which limits the expressiveness
of the latter. Here, we introduce hierarchical compound Poisson factorization
(HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to
high-dimensional extremely sparse matrices. More importantly, HCPF decouples
the sparsity model from the response model, allowing us to choose the most
suitable distribution for the response. HCPF can capture binary, non-negative
discrete, non-negative continuous, and zero-inflated continuous responses. We
compare HCPF with HPF on nine discrete and three continuous data sets and
conclude that HCPF captures the relationship between sparsity and response
better than HPF.","['Mehmet E. Basbug', 'Barbara E. Engelhardt']","['cs.LG', 'cs.AI', 'stat.ML']",2016-04-13 16:12:01+00:00
http://arxiv.org/abs/1604.03763v3,A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization,"In modern large-scale machine learning applications, the training data are
often partitioned and stored on multiple machines. It is customary to employ
the ""data parallelism"" approach, where the aggregated training loss is
minimized without moving data across machines. In this paper, we introduce a
novel distributed dual formulation for regularized loss minimization problems
that can directly handle data parallelism in the distributed setting. This
formulation allows us to systematically derive dual coordinate optimization
procedures, which we refer to as Distributed Alternating Dual Maximization
(DADM). The framework extends earlier studies described in (Boyd et al., 2011;
Ma et al., 2015a; Jaggi et al., 2014; Yang, 2013) and has rigorous theoretical
analyses. Moreover with the help of the new formulation, we develop the
accelerated version of DADM (Acc-DADM) by generalizing the acceleration
technique from (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We
also provide theoretical results for the proposed accelerated version and the
new result improves previous ones (Yang, 2013; Ma et al., 2015a) whose runtimes
grow linearly on the condition number. Our empirical studies validate our
theory and show that our accelerated approach significantly improves the
previous state-of-the-art distributed dual coordinate optimization algorithms.","['Shun Zheng', 'Jialei Wang', 'Fen Xia', 'Wei Xu', 'Tong Zhang']","['cs.LG', 'cs.DC', 'math.OC', 'stat.ML']",2016-04-13 13:33:32+00:00
http://arxiv.org/abs/1604.03744v2,Variational Bayesian Inference of Line Spectra,"In this paper, we address the fundamental problem of line spectral estimation
in a Bayesian framework. We target model order and parameter estimation via
variational inference in a probabilistic model in which the frequencies are
continuous-valued, i.e., not restricted to a grid; and the coefficients are
governed by a Bernoulli-Gaussian prior model turning model order selection into
binary sequence detection. Unlike earlier works which retain only point
estimates of the frequencies, we undertake a more complete Bayesian treatment
by estimating the posterior probability density functions (pdfs) of the
frequencies and computing expectations over them. Thus, we additionally capture
and operate with the uncertainty of the frequency estimates. Aiming to maximize
the model evidence, variational optimization provides analytic approximations
of the posterior pdfs and also gives estimates of the additional parameters. We
propose an accurate representation of the pdfs of the frequencies by mixtures
of von Mises pdfs, which yields closed-form expectations. We define the
algorithm VALSE in which the estimates of the pdfs and parameters are
iteratively updated. VALSE is a gridless, convergent method, does not require
parameter tuning, can easily include prior knowledge about the frequencies and
provides approximate posterior pdfs based on which the uncertainty in line
spectral estimation can be quantified. Simulation results show that accounting
for the uncertainty of frequency estimates, rather than computing just point
estimates, significantly improves the performance. The performance of VALSE is
superior to that of state-of-the-art methods and closely approaches the
Cram\'er-Rao bound computed for the true model order.","['Mihai-Alin Badiu', 'Thomas Lundgaard Hansen', 'Bernard Henri Fleury']","['cs.IT', 'math.IT', 'stat.ML']",2016-04-13 12:51:51+00:00
http://arxiv.org/abs/1604.03736v1,A Differentiable Transition Between Additive and Multiplicative Neurons,"Existing approaches to combine both additive and multiplicative neural units
either use a fixed assignment of operations or require discrete optimization to
determine what function a neuron should perform. However, this leads to an
extensive increase in the computational complexity of the training procedure.
  We present a novel, parameterizable transfer function based on the
mathematical concept of non-integer functional iteration that allows the
operation each neuron performs to be smoothly and, most importantly,
differentiablely adjusted between addition and multiplication. This allows the
decision between addition and multiplication to be integrated into the standard
backpropagation training procedure.","['Wiebke Köpp', 'Patrick van der Smagt', 'Sebastian Urban']","['cs.LG', 'stat.ML']",2016-04-13 12:38:27+00:00
http://arxiv.org/abs/1604.03601v1,Community Detection with Node Attributes and its Generalization,"Community detection algorithms are fundamental tools to understand
organizational principles in social networks. With the increasing power of
social media platforms, when detecting communities there are two possi- ble
sources of information one can use: the structure of social network and node
attributes. However structure of social networks and node attributes are often
interpreted separately in the research of community detection. When these two
sources are interpreted simultaneously, one common as- sumption shared by
previous studies is that nodes attributes are correlated with communities. In
this paper, we present a model that is capable of combining topology
information and nodes attributes information with- out assuming correlation.
This new model can recover communities with higher accuracy even when node
attributes and communities are uncorre- lated. We derive the detectability
threshold for this model and use Belief Propagation (BP) to make inference.
This algorithm is optimal in the sense that it can recover community all the
way down to the threshold. This new model is also with the potential to handle
edge content and dynamic settings.",['Yuan Li'],"['cs.SI', 'physics.soc-ph', 'stat.ML']",2016-04-12 22:09:02+00:00
http://arxiv.org/abs/1604.03492v1,Structured Matrix Recovery via the Generalized Dantzig Selector,"In recent years, structured matrix recovery problems have gained considerable
attention for its real world applications, such as recommender systems and
computer vision. Much of the existing work has focused on matrices with
low-rank structure, and limited progress has been made matrices with other
types of structure. In this paper we present non-asymptotic analysis for
estimation of generally structured matrices via the generalized Dantzig
selector under generic sub-Gaussian measurements. We show that the estimation
error can always be succinctly expressed in terms of a few geometric measures
of suitable sets which only depend on the structure of the underlying true
matrix. In addition, we derive the general bounds on these geometric measures
for structures characterized by unitarily invariant norms, which is a large
family covering most matrix norms of practical interest. Examples are provided
to illustrate the utility of our theoretical development.","['Sheng Chen', 'Arindam Banerjee']",['stat.ML'],2016-04-12 17:54:14+00:00
http://arxiv.org/abs/1604.03463v2,The Matrix Generalized Inverse Gaussian Distribution: Properties and Applications,"While the Matrix Generalized Inverse Gaussian ($\mathcal{MGIG}$) distribution
arises naturally in some settings as a distribution over symmetric positive
semi-definite matrices, certain key properties of the distribution and
effective ways of sampling from the distribution have not been carefully
studied. In this paper, we show that the $\mathcal{MGIG}$ is unimodal, and the
mode can be obtained by solving an Algebraic Riccati Equation (ARE) equation
[7]. Based on the property, we propose an importance sampling method for the
$\mathcal{MGIG}$ where the mode of the proposal distribution matches that of
the target. The proposed sampling method is more efficient than existing
approaches [32, 33], which use proposal distributions that may have the mode
far from the $\mathcal{MGIG}$'s mode. Further, we illustrate that the the
posterior distribution in latent factor models, such as probabilistic matrix
factorization (PMF) [25], when marginalized over one latent factor has the
$\mathcal{MGIG}$ distribution. The characterization leads to a novel Collapsed
Monte Carlo (CMC) inference algorithm for such latent factor models. We
illustrate that CMC has a lower log loss or perplexity than MCMC, and needs
fewer samples.","['Farideh Fazayeli', 'Arindam Banerjee']",['stat.ML'],2016-04-12 16:03:31+00:00
http://arxiv.org/abs/1604.03373v1,A Convex Surrogate Operator for General Non-Modular Loss Functions,"Empirical risk minimization frequently employs convex surrogates to
underlying discrete loss functions in order to achieve computational
tractability during optimization. However, classical convex surrogates can only
tightly bound modular loss functions, sub-modular functions or supermodular
functions separately while maintaining polynomial time computation. In this
work, a novel generic convex surrogate for general non-modular loss functions
is introduced, which provides for the first time a tractable solution for loss
functions that are neither super-modular nor submodular. This convex surro-gate
is based on a submodular-supermodular decomposition for which the existence and
uniqueness is proven in this paper. It takes the sum of two convex surrogates
that separately bound the supermodular component and the submodular component
using slack-rescaling and the Lov{\'a}sz hinge, respectively. It is further
proven that this surrogate is convex , piecewise linear, an extension of the
loss function, and for which subgradient computation is polynomial time.
Empirical results are reported on a non-submodular loss based on the
S{{\o}}rensen-Dice difference function, and a real-world face track dataset
with tens of thousands of frames, demonstrating the improved performance,
efficiency, and scalabil-ity of the novel convex surrogate.","['Jiaqian Yu', 'Matthew Blaschko']","['stat.ML', 'cs.LG']",2016-04-12 12:31:59+00:00
http://arxiv.org/abs/1604.03343v1,Loss Bounds and Time Complexity for Speed Priors,"This paper establishes for the first time the predictive performance of speed
priors and their computational complexity. A speed prior is essentially a
probability distribution that puts low probability on strings that are not
efficiently computable. We propose a variant to the original speed prior
(Schmidhuber, 2002), and show that our prior can predict sequences drawn from
probability measures that are estimable in polynomial time. Our speed prior is
computable in doubly-exponential time, but not in polynomial time. On a
polynomial time computable sequence our speed prior is computable in
exponential time. We show better upper complexity bounds for Schmidhuber's
speed prior under the same conditions, and that it predicts deterministic
sequences that are computable in polynomial time; however, we also show that it
is not computable in polynomial time, and the question of its predictive
properties for stochastic sequences remains open.","['Daniel Filan', 'Marcus Hutter', 'Jan Leike']","['cs.LG', 'stat.ML']",2016-04-12 11:26:12+00:00
http://arxiv.org/abs/1604.03278v1,Confidence Decision Trees via Online and Active Learning for Streaming (BIG) Data,"Decision tree classifiers are a widely used tool in data stream mining. The
use of confidence intervals to estimate the gain associated with each split
leads to very effective methods, like the popular Hoeffding tree algorithm.
From a statistical viewpoint, the analysis of decision tree classifiers in a
streaming setting requires knowing when enough new information has been
collected to justify splitting a leaf. Although some of the issues in the
statistical analysis of Hoeffding trees have been already clarified, a general
and rigorous study of confidence intervals for splitting criteria is missing.
We fill this gap by deriving accurate confidence intervals to estimate the
splitting gain in decision tree learning with respect to three criteria:
entropy, Gini index, and a third index proposed by Kearns and Mansour. Our
confidence intervals depend in a more detailed way on the tree parameters. We
also extend our confidence analysis to a selective sampling setting, in which
the decision tree learner adaptively decides which labels to query in the
stream. We furnish theoretical guarantee bounding the probability that the
classification is non-optimal learning the decision tree via our selective
sampling strategy. Experiments on real and synthetic data in a streaming
setting show that our trees are indeed more accurate than trees with the same
number of leaves generated by other techniques and our active learning module
permits to save labeling cost. In addition, comparing our labeling strategy
with recent methods, we show that our approach is more robust and consistent
respect all the other techniques applied to incremental decision trees.",['Rocco De Rosa'],"['stat.ML', 'cs.LG']",2016-04-12 07:59:55+00:00
http://arxiv.org/abs/1604.03257v2,Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization,"Recently, {\it stochastic momentum} methods have been widely adopted in
training deep neural networks. However, their convergence analysis is still
underexplored at the moment, in particular for non-convex optimization. This
paper fills the gap between practice and theory by developing a basic
convergence analysis of two stochastic momentum methods, namely stochastic
heavy-ball method and the stochastic variant of Nesterov's accelerated gradient
method. We hope that the basic convergence results developed in this paper can
serve the reference to the convergence of stochastic momentum methods and also
serve the baselines for comparison in future development of stochastic momentum
methods. The novelty of convergence analysis presented in this paper is a
unified framework, revealing more insights about the similarities and
differences between different stochastic momentum methods and stochastic
gradient method. The unified framework exhibits a continuous change from the
gradient method to Nesterov's accelerated gradient method and finally the
heavy-ball method incurred by a free parameter, which can help explain a
similar change observed in the testing error convergence behavior for deep
learning. Furthermore, our empirical results for optimizing deep neural
networks demonstrate that the stochastic variant of Nesterov's accelerated
gradient method achieves a good tradeoff (between speed of convergence in
training error and robustness of convergence in testing error) among the three
stochastic methods.","['Tianbao Yang', 'Qihang Lin', 'Zhe Li']","['math.OC', 'stat.ML']",2016-04-12 06:24:19+00:00
http://arxiv.org/abs/1604.03227v1,Recurrent Attentional Networks for Saliency Detection,"Convolutional-deconvolution networks can be adopted to perform end-to-end
saliency detection. But, they do not work well with objects of multiple scales.
To overcome such a limitation, in this work, we propose a recurrent attentional
convolutional-deconvolution network (RACDNN). Using spatial transformer and
recurrent network units, RACDNN is able to iteratively attend to selected image
sub-regions to perform saliency refinement progressively. Besides tackling the
scale problem, RACDNN can also learn context-aware features from past
iterations to enhance saliency refinement in future iterations. Experiments on
several challenging saliency detection datasets validate the effectiveness of
RACDNN, and show that RACDNN outperforms state-of-the-art saliency detection
methods.","['Jason Kuen', 'Zhenhua Wang', 'Gang Wang']","['cs.CV', 'cs.LG', 'stat.ML']",2016-04-12 03:03:04+00:00
http://arxiv.org/abs/1604.03159v4,Phase Transitions and a Model Order Selection Criterion for Spectral Graph Clustering,"One of the longstanding open problems in spectral graph clustering (SGC) is
the so-called model order selection problem: automated selection of the correct
number of clusters. This is equivalent to the problem of finding the number of
connected components or communities in an undirected graph. We propose
automated model order selection (AMOS), a solution to the SGC model selection
problem under a random interconnection model (RIM) using a novel selection
criterion that is based on an asymptotic phase transition analysis. AMOS can
more generally be applied to discovering hidden block diagonal structure in
symmetric non-negative matrices. Numerical experiments on simulated graphs
validate the phase transition analysis, and real-world network data is used to
validate the performance of the proposed model selection procedure.","['Pin-Yu Chen', 'Alfred O. Hero']","['cs.SI', 'stat.ML']",2016-04-11 21:42:30+00:00
http://arxiv.org/abs/1604.03114v1,Conversational flow in Oxford-style debates,"Public debates are a common platform for presenting and juxtaposing diverging
views on important issues. In this work we propose a methodology for tracking
how ideas flow between participants throughout a debate. We use this approach
in a case study of Oxford-style debates---a competitive format where the winner
is determined by audience votes---and show how the outcome of a debate depends
on aspects of conversational flow. In particular, we find that winners tend to
make better use of a debate's interactive component than losers, by actively
pursuing their opponents' points rather than promoting their own ideas over the
course of the conversation.","['Justine Zhang', 'Ravi Kumar', 'Sujith Ravi', 'Cristian Danescu-Niculescu-Mizil']","['cs.CL', 'cs.AI', 'cs.SI', 'physics.soc-ph', 'stat.ML']",2016-04-11 20:00:04+00:00
http://arxiv.org/abs/1604.03053v5,Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains,"When governed by underlying low-dimensional dynamics, the interdependence of
simultaneously recorded population of neurons can be explained by a small
number of shared factors, or a low-dimensional trajectory. Recovering these
latent trajectories, particularly from single-trial population recordings, may
help us understand the dynamics that drive neural computation. However, due to
the biophysical constraints and noise in the spike trains, inferring
trajectories from data is a challenging statistical problem in general. Here,
we propose a practical and efficient inference method, called the variational
latent Gaussian process (vLGP). The vLGP combines a generative model with a
history-dependent point process observation together with a smoothness prior on
the latent trajectories. The vLGP improves upon earlier methods for recovering
latent trajectories, which assume either observation models inappropriate for
point processes or linear dynamics. We compare and validate vLGP on both
simulated datasets and population recordings from the primary visual cortex. In
the V1 dataset, we find that vLGP achieves substantially higher performance
than previous methods for predicting omitted spike trains, as well as capturing
both the toroidal topology of visual stimuli space, and the noise-correlation.
These results show that vLGP is a robust method with a potential to reveal
hidden neural dynamics from large-scale neural recordings.","['Yuan Zhao', 'Il Memming Park']","['stat.ML', 'q-bio.NC']",2016-04-11 18:25:09+00:00
http://arxiv.org/abs/1604.03427v1,In the mood: the dynamics of collective sentiments on Twitter,"We study the relationship between the sentiment levels of Twitter users and
the evolving network structure that the users created by @-mentioning each
other. We use a large dataset of tweets to which we apply three sentiment
scoring algorithms, including the open source SentiStrength program.
Specifically we make three contributions. Firstly we find that people who have
potentially the largest communication reach (according to a dynamic centrality
measure) use sentiment differently than the average user: for example they use
positive sentiment more often and negative sentiment less often. Secondly we
find that when we follow structurally stable Twitter communities over a period
of months, their sentiment levels are also stable, and sudden changes in
community sentiment from one day to the next can in most cases be traced to
external events affecting the community. Thirdly, based on our findings, we
create and calibrate a simple agent-based model that is capable of reproducing
measures of emotive response comparable to those obtained from our empirical
dataset.","['Nathaniel Charlton', 'Colin Singleton', 'Danica Vukadinović Greetham']","['cs.SI', 'stat.ML']",2016-04-11 16:24:22+00:00
http://arxiv.org/abs/1604.03006v2,Demystifying Fixed k-Nearest Neighbor Information Estimators,"Estimating mutual information from i.i.d. samples drawn from an unknown joint
density function is a basic statistical problem of broad interest with
multitudinous applications. The most popular estimator is one proposed by
Kraskov and St\""ogbauer and Grassberger (KSG) in 2004, and is nonparametric and
based on the distances of each sample to its $k^{\rm th}$ nearest neighboring
sample, where $k$ is a fixed small integer. Despite its widespread use (part of
scientific software packages), theoretical properties of this estimator have
been largely unexplored. In this paper we demonstrate that the estimator is
consistent and also identify an upper bound on the rate of convergence of the
bias as a function of number of samples. We argue that the superior performance
benefits of the KSG estimator stems from a curious ""correlation boosting""
effect and build on this intuition to modify the KSG estimator in novel ways to
construct a superior estimator. As a byproduct of our investigations, we obtain
nearly tight rates of convergence of the $\ell_2$ error of the well known fixed
$k$ nearest neighbor estimator of differential entropy by Kozachenko and
Leonenko.","['Weihao Gao', 'Sewoong Oh', 'Pramod Viswanath']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2016-04-11 15:47:05+00:00
http://arxiv.org/abs/1604.02917v2,Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis,"We present a novel approach for supervised domain adaptation that is based
upon the probabilistic framework of Gaussian processes (GPs). Specifically, we
introduce domain-specific GPs as local experts for facial expression
classification from face images. The adaptation of the classifier is
facilitated in probabilistic fashion by conditioning the target expert on
multiple source experts. Furthermore, in contrast to existing adaptation
approaches, we also learn a target expert from available target data solely.
Then, a single and confident classifier is obtained by combining the
predictions from multiple experts based on their confidence. Learning of the
model is efficient and requires no retraining/reweighting of the source
classifiers. We evaluate the proposed approach on two publicly available
datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression
classification. To this end, we perform adaptation of two contextual factors:
'where' (view) and 'who' (subject). We show in our experiments that the
proposed approach consistently outperforms both source and target classifiers,
while using as few as 30 target examples. It also outperforms the
state-of-the-art approaches for supervised domain adaptation.","['Stefanos Eleftheriadis', 'Ognjen Rudovic', 'Marc P. Deisenroth', 'Maja Pantic']","['stat.ML', 'cs.CV', 'cs.LG']",2016-04-11 12:37:36+00:00
http://arxiv.org/abs/1604.03392v1,A statistical learning strategy for closed-loop control of fluid flows,"This work discusses a closed-loop control strategy for complex systems
utilizing scarce and streaming data. A discrete embedding space is first built
using hash functions applied to the sensor measurements from which a Markov
process model is derived, approximating the complex system's dynamics. A
control strategy is then learned using reinforcement learning once rewards
relevant with respect to the control objective are identified. This method is
designed for experimental configurations, requiring no computations nor prior
knowledge of the system, and enjoys intrinsic robustness. It is illustrated on
two systems: the control of the transitions of a Lorenz 63 dynamical system,
and the control of the drag of a cylinder flow. The method is shown to perform
well.","['Florimond Guéniat', 'Lionel Mathelin', 'M. Yousuff Hussaini']","['stat.ML', 'math.OC', 'physics.flu-dyn']",2016-04-11 11:13:42+00:00
http://arxiv.org/abs/1604.02855v1,Active Learning for Online Recognition of Human Activities from Streaming Videos,"Recognising human activities from streaming videos poses unique challenges to
learning algorithms: predictive models need to be scalable, incrementally
trainable, and must remain bounded in size even when the data stream is
arbitrarily long. Furthermore, as parameter tuning is problematic in a
streaming setting, suitable approaches should be parameterless, and make no
assumptions on what class labels may occur in the stream. We present here an
approach to the recognition of human actions from streaming data which meets
all these requirements by: (1) incrementally learning a model which adaptively
covers the feature space with simple local classifiers; (2) employing an active
learning strategy to reduce annotation requests; (3) achieving promising
accuracy within a fixed model size. Extensive experiments on standard
benchmarks show that our approach is competitive with state-of-the-art
non-incremental methods, and outperforms the existing active incremental
baselines.","['Rocco De Rosa', 'Ilaria Gori', 'Fabio Cuzzolin', 'Barbara Caputo', 'Nicolò Cesa-Bianchi']","['stat.ML', 'cs.CV', 'cs.LG']",2016-04-11 09:32:51+00:00
http://arxiv.org/abs/1604.02737v2,Correlated Equilibria for Approximate Variational Inference in MRFs,"Almost all of the work in graphical models for game theory has mirrored
previous work in probabilistic graphical models. Our work considers the
opposite direction: Taking advantage of recent advances in equilibrium
computation for probabilistic inference. We present formulations of inference
problems in Markov random fields (MRFs) as computation of equilibria in a
certain class of game-theoretic graphical models. We concretely establishes the
precise connection between variational probabilistic inference in MRFs and
correlated equilibria. No previous work exploits recent theoretical and
empirical results from the literature on algorithmic and computational game
theory on the tractable, polynomial-time computation of exact or approximate
correlated equilibria in graphical games with arbitrary, loopy graph structure.
We discuss how to design new algorithms with equally tractable guarantees for
the computation of approximate variational inference in MRFs. Also, inspired by
a previously stated game-theoretic view of state-of-the-art tree-reweighed
(TRW) message-passing techniques for belief inference as zero-sum game, we
propose a different, general-sum potential game to design approximate
fictitious-play techniques. We perform synthetic experiments evaluating our
proposed approximation algorithms with standard methods and TRW on several
classes of classical Ising models (i.e., with binary random variables). We also
evaluate the algorithms using Ising models learned from the MNIST dataset. Our
experiments show that our global approach is competitive, particularly shinning
in a class of Ising models with constant, ""highly attractive"" edge-weights, in
which it is often better than all other alternatives we evaluated. With a
notable exception, our more local approach was not as effective. Yet, in
fairness, almost all of the alternatives are often no better than a simple
baseline: estimate 0.5.","['Luis E. Ortiz', 'Boshen Wang', 'Ze Gong']","['cs.AI', 'cs.GT', 'stat.ML']",2016-04-10 21:21:00+00:00
http://arxiv.org/abs/1604.02668v1,Distance for Functional Data Clustering Based on Smoothing Parameter Commutation,"We propose a novel method to determine the dissimilarity between subjects for
functional data clustering. Spline smoothing or interpolation is common to deal
with data of such type. Instead of estimating the best-representing curve for
each subject as fixed during clustering, we measure the dissimilarity between
subjects based on varying curve estimates with commutation of smoothing
parameters pair-by-pair (of subjects). The intuitions are that smoothing
parameters of smoothing splines reflect inverse signal-to-noise ratios and that
applying an identical smoothing parameter the smoothed curves for two similar
subjects are expected to be close. The effectiveness of our proposal is shown
through simulations comparing to other dissimilarity measures. It also has
several pragmatic advantages. First, missing values or irregular time points
can be handled directly, thanks to the nature of smoothing splines. Second,
conventional clustering method based on dissimilarity can be employed
straightforward, and the dissimilarity also serves as a useful tool for outlier
detection. Third, the implementation is almost handy since subroutines for
smoothing splines and numerical integration are widely available. Fourth, the
computational complexity does not increase and is parallel with that in
calculating Euclidean distance between curves estimated by smoothing splines.","['ShengLi Tzeng', 'Christian Hennig', 'Yu-Fen Li', 'Chien-Ju Lin']","['stat.ME', 'stat.AP', 'stat.ML']",2016-04-10 10:55:52+00:00
http://arxiv.org/abs/1604.02634v2,Online Nonnegative Matrix Factorization with Outliers,"We propose a unified and systematic framework for performing online
nonnegative matrix factorization in the presence of outliers. Our framework is
particularly suited to large-scale data. We propose two solvers based on
projected gradient descent and the alternating direction method of multipliers.
We prove that the sequence of objective values converges almost surely by
appealing to the quasi-martingale convergence theorem. We also show the
sequence of learned dictionaries converges to the set of stationary points of
the expected loss function almost surely. In addition, we extend our basic
problem formulation to various settings with different constraints and
regularizers. We also adapt the solvers and analyses to each setting. We
perform extensive experiments on both synthetic and real datasets. These
experiments demonstrate the computational efficiency and efficacy of our
algorithms on tasks such as (parts-based) basis learning, image denoising,
shadow removal and foreground-background separation.","['Renbo Zhao', 'Vincent Y. F. Tan']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC', 'stat.ME']",2016-04-10 04:02:57+00:00
http://arxiv.org/abs/1604.02631v1,Grid Based Nonlinear Filtering Revisited: Recursive Estimation & Asymptotic Optimality,"We revisit the development of grid based recursive approximate filtering of
general Markov processes in discrete time, partially observed in conditionally
Gaussian noise. The grid based filters considered rely on two types of state
quantization: The \textit{Markovian} type and the \textit{marginal} type. We
propose a set of novel, relaxed sufficient conditions, ensuring strong and
fully characterized pathwise convergence of these filters to the respective
MMSE state estimator. In particular, for marginal state quantizations, we
introduce the notion of \textit{conditional regularity of stochastic kernels},
which, to the best of our knowledge, constitutes the most relaxed condition
proposed, under which asymptotic optimality of the respective grid based
filters is guaranteed. Further, we extend our convergence results, including
filtering of bounded and continuous functionals of the state, as well as
recursive approximate state prediction. For both Markovian and marginal
quantizations, the whole development of the respective grid based filters
relies more on linear-algebraic techniques and less on measure theoretic
arguments, making the presentation considerably shorter and technically
simpler.","['Dionysios S. Kalogerias', 'Athina P. Petropulu']","['math.ST', 'cs.IT', 'math.IT', 'math.OC', 'stat.ME', 'stat.ML', 'stat.TH']",2016-04-10 03:06:23+00:00
http://arxiv.org/abs/1604.02606v2,A General Retraining Framework for Scalable Adversarial Classification,"Traditional classification algorithms assume that training and test data come
from similar distributions. This assumption is violated in adversarial
settings, where malicious actors modify instances to evade detection. A number
of custom methods have been developed for both adversarial evasion attacks and
robust learning. We propose the first systematic and general-purpose retraining
framework which can: a) boost robustness of an \emph{arbitrary} learning
algorithm, in the face of b) a broader class of adversarial models than any
prior methods. We show that, under natural conditions, the retraining framework
minimizes an upper bound on optimal adversarial risk, and show how to extend
this result to account for approximations of evasion attacks. Extensive
experimental evaluation demonstrates that our retraining methods are nearly
indistinguishable from state-of-the-art algorithms for optimizing adversarial
risk, but are more general and far more scalable. The experiments also confirm
that without retraining, our adversarial framework dramatically reduces the
effectiveness of learning. In contrast, retraining significantly boosts
robustness to evasion attacks without significantly compromising overall
accuracy.","['Bo Li', 'Yevgeniy Vorobeychik', 'Xinyun Chen']","['cs.GT', 'cs.LG', 'stat.ML']",2016-04-09 20:14:36+00:00
http://arxiv.org/abs/1604.02492v5,Challenges in Bayesian Adaptive Data Analysis,"Traditional statistical analysis requires that the analysis process and data
are independent. By contrast, the new field of adaptive data analysis hopes to
understand and provide algorithms and accuracy guarantees for research as it is
commonly performed in practice, as an iterative process of interacting
repeatedly with the same data set, such as repeated tests against a holdout
set. Previous work has defined a model with a rather strong lower bound on
sample complexity in terms of the number of queries, $n\sim\sqrt q$, arguing
that adaptive data analysis is much harder than static data analysis, where
$n\sim\log q$ is possible. Instead, we argue that those strong lower bounds
point to a limitation of the previous model in that it must consider wildly
asymmetric scenarios which do not hold in typical applications.
  To better understand other difficulties of adaptivity, we propose a new
Bayesian version of the problem that mandates symmetry. Since the other lower
bound techniques are ruled out, we can more effectively see difficulties that
might otherwise be overshadowed. As a first contribution to this model, we
produce a new problem using error-correcting codes on which a large family of
methods, including all previously proposed algorithms, require roughly
$n\sim\sqrt[4]q$. These early results illustrate new difficulties in adaptive
data analysis regarding slightly correlated queries on problems with
concentrated uncertainty.",['Sam Elder'],"['cs.LG', 'stat.ML']",2016-04-08 21:56:24+00:00
http://arxiv.org/abs/1604.02275v1,Online Open World Recognition,"As we enter into the big data age and an avalanche of images have become
readily available, recognition systems face the need to move from close, lab
settings where the number of classes and training data are fixed, to dynamic
scenarios where the number of categories to be recognized grows continuously
over time, as well as new data providing useful information to update the
system. Recent attempts, like the open world recognition framework, tried to
inject dynamics into the system by detecting new unknown classes and adding
them incrementally, while at the same time continuously updating the models for
the known classes. incrementally adding new classes and detecting instances
from unknown classes, while at the same time continuously updating the models
for the known classes. In this paper we argue that to properly capture the
intrinsic dynamic of open world recognition, it is necessary to add to these
aspects (a) the incremental learning of the underlying metric, (b) the
incremental estimate of confidence thresholds for the unknown classes, and (c)
the use of local learning to precisely describe the space of classes. We extend
three existing metric learning algorithms towards these goals by using online
metric learning. Experimentally we validate our approach on two large-scale
datasets in different learning scenarios. For all these scenarios our proposed
methods outperform their non-online counterparts. We conclude that local and
online learning is important to capture the full dynamics of open world
recognition.","['Rocco De Rosa', 'Thomas Mensink', 'Barbara Caputo']","['cs.CV', 'cs.LG', 'stat.ML']",2016-04-08 08:43:15+00:00
http://arxiv.org/abs/1604.02218v3,A Low Complexity Algorithm with $O(\sqrt{T})$ Regret and $O(1)$ Constraint Violations for Online Convex Optimization with Long Term Constraints,"This paper considers online convex optimization over a complicated constraint
set, which typically consists of multiple functional constraints and a set
constraint. The conventional online projection algorithm (Zinkevich, 2003) can
be difficult to implement due to the potentially high computation complexity of
the projection operation. In this paper, we relax the functional constraints by
allowing them to be violated at each round but still requiring them to be
satisfied in the long term. This type of relaxed online convex optimization
(with long term constraints) was first considered in Mahdavi et al. (2012).
That prior work proposes an algorithm to achieve $O(\sqrt{T})$ regret and
$O(T^{3/4})$ constraint violations for general problems and another algorithm
to achieve an $O(T^{2/3})$ bound for both regret and constraint violations when
the constraint set can be described by a finite number of linear constraints. A
recent extension in \citet{Jenatton16ICML} can achieve
$O(T^{\max\{\theta,1-\theta\}})$ regret and $O(T^{1-\theta/2})$ constraint
violations where $\theta\in (0,1)$. The current paper proposes a new simple
algorithm that yields improved performance in comparison to prior works. The
new algorithm achieves an $O(\sqrt{T})$ regret bound with $O(1)$ constraint
violations.","['Hao Yu', 'Michael J. Neely']","['math.OC', 'cs.LG', 'stat.ML']",2016-04-08 03:37:52+00:00
http://arxiv.org/abs/1604.02181v6,A Unified Framework for Sparse Non-Negative Least Squares using Multiplicative Updates and the Non-Negative Matrix Factorization Problem,"We study the sparse non-negative least squares (S-NNLS) problem. S-NNLS
occurs naturally in a wide variety of applications where an unknown,
non-negative quantity must be recovered from linear measurements. We present a
unified framework for S-NNLS based on a rectified power exponential scale
mixture prior on the sparse codes. We show that the proposed framework
encompasses a large class of S-NNLS algorithms and provide a computationally
efficient inference procedure based on multiplicative update rules. Such update
rules are convenient for solving large sets of S-NNLS problems simultaneously,
which is required in contexts like sparse non-negative matrix factorization
(S-NMF). We provide theoretical justification for the proposed approach by
showing that the local minima of the objective function being optimized are
sparse and the S-NNLS algorithms presented are guaranteed to converge to a set
of stationary points of the objective function. We then extend our framework to
S-NMF, showing that our framework leads to many well known S-NMF algorithms
under specific choices of prior and providing a guarantee that a popular
subclass of the proposed algorithms converges to a set of stationary points of
the objective function. Finally, we study the performance of the proposed
approaches on synthetic and real-world data.","['Igor Fedorov', 'Alican Nalci', 'Ritwik Giri', 'Bhaskar D. Rao', 'Truong Q. Nguyen', 'Harinath Garudadri']",['stat.ML'],2016-04-07 21:35:42+00:00
http://arxiv.org/abs/1604.02123v1,Multilevel Weighted Support Vector Machine for Classification on Healthcare Data with Missing Values,"This work is motivated by the needs of predictive analytics on healthcare
data as represented by Electronic Medical Records. Such data is invariably
problematic: noisy, with missing entries, with imbalance in classes of
interests, leading to serious bias in predictive modeling. Since standard data
mining methods often produce poor performance measures, we argue for
development of specialized techniques of data-preprocessing and classification.
In this paper, we propose a new method to simultaneously classify large
datasets and reduce the effects of missing values. It is based on a multilevel
framework of the cost-sensitive SVM and the expected maximization imputation
method for missing values, which relies on iterated regression analyses. We
compare classification results of multilevel SVM-based algorithms on public
benchmark datasets with imbalanced classes and missing values as well as real
data in health applications, and show that our multilevel SVM-based method
produces fast, and more accurate and robust classification results.","['Talayeh Razzaghi', 'Oleg Roderick', 'Ilya Safro', 'Nicholas Marko']","['stat.ML', 'cs.LG', 'stat.AP']",2016-04-07 19:19:52+00:00
http://arxiv.org/abs/1604.02027v2,Combinatorial Topic Models using Small-Variance Asymptotics,"Topic models have emerged as fundamental tools in unsupervised machine
learning. Most modern topic modeling algorithms take a probabilistic view and
derive inference algorithms based on Latent Dirichlet Allocation (LDA) or its
variants. In contrast, we study topic modeling as a combinatorial optimization
problem, and propose a new objective function derived from LDA by passing to
the small-variance limit. We minimize the derived objective by using ideas from
combinatorial optimization, which results in a new, fast, and high-quality
topic modeling algorithm. In particular, we show that our results are
competitive with popular LDA-based topic modeling approaches, and also discuss
the (dis)similarities between our approach and its probabilistic counterparts.","['Ke Jiang', 'Suvrit Sra', 'Brian Kulis']","['cs.LG', 'cs.CL', 'stat.ML']",2016-04-07 15:04:16+00:00
http://arxiv.org/abs/1604.01999v2,Online Optimization of Smoothed Piecewise Constant Functions,"We study online optimization of smoothed piecewise constant functions over
the domain [0, 1). This is motivated by the problem of adaptively picking
parameters of learning algorithms as in the recently introduced framework by
Gupta and Roughgarden (2016). Majority of the machine learning literature has
focused on Lipschitz-continuous functions or functions with bounded gradients.
1 This is with good reason---any learning algorithm suffers linear regret even
against piecewise constant functions that are chosen adversarially, arguably
the simplest of non-Lipschitz continuous functions. The smoothed setting we
consider is inspired by the seminal work of Spielman and Teng (2004) and the
recent work of Gupta and Roughgarden---in this setting, the sequence of
functions may be chosen by an adversary, however, with some uncertainty in the
location of discontinuities. We give algorithms that achieve sublinear regret
in the full information and bandit settings.","['Vincent Cohen-Addad', 'Varun Kanade']","['cs.LG', 'stat.ML']",2016-04-07 13:52:47+00:00
http://arxiv.org/abs/1604.01972v2,An Adaptive Resample-Move Algorithm for Estimating Normalizing Constants,"The estimation of normalizing constants is a fundamental step in
probabilistic model comparison. Sequential Monte Carlo methods may be used for
this task and have the advantage of being inherently parallelizable. However,
the standard choice of using a fixed number of particles at each iteration is
suboptimal because some steps will contribute disproportionately to the
variance of the estimate. We introduce an adaptive version of the Resample-Move
algorithm, in which the particle set is adaptively expanded whenever a better
approximation of an intermediate distribution is needed. The algorithm builds
on the expression for the optimal number of particles and the corresponding
minimum variance found under ideal conditions. Benchmark results on challenging
Gaussian Process Classification and Restricted Boltzmann Machine applications
show that Adaptive Resample-Move (ARM) estimates the normalizing constant with
a smaller variance, using less computational resources, than either
Resample-Move with a fixed number of particles or Annealed Importance Sampling.
A further advantage over Annealed Importance Sampling is that ARM is easier to
tune.","['Marco Fraccaro', 'Ulrich Paquet', 'Ole Winther']",['stat.ML'],2016-04-07 12:15:54+00:00
http://arxiv.org/abs/1604.01955v1,Monitoring Chinese Population Migration in Consecutive Weekly Basis from Intra-city scale to Inter-province scale by Didi's Bigdata,"Population migration is valuable information which leads to proper decision
in urban-planning strategy, massive investment, and many other fields. For
instance, inter-city migration is a posterior evidence to see if the
government's constrain of population works, and inter-community immigration
might be a prior evidence of real estate price hike. With timely data, it is
also impossible to compare which city is more favorable for the people, suppose
the cities release different new regulations, we could also compare the
customers of different real estate development groups, where they come from,
where they probably will go. Unfortunately these data was not available.
  In this paper, leveraging the data generated by positioning team in Didi, we
propose a novel approach that timely monitoring population migration from
community scale to provincial scale. Migration can be detected as soon as in a
week. It could be faster, the setting of a week is for statistical purpose. A
monitoring system is developed, then applied nation wide in China, some
observations derived from the system will be presented in this paper.
  This new method of migration perception is origin from the insight that
nowadays people mostly moving with their personal Access Point (AP), also known
as WiFi hotspot. Assume that the ratio of AP moving to the migration of
population is constant, analysis of comparative population migration would be
feasible. More exact quantitative research would also be done with few sample
research and model regression.
  The procedures of processing data includes many steps: eliminating the impact
of pseudo-migration AP, for instance pocket WiFi, and second-hand traded
router; distinguishing moving of population with moving of companies;
identifying shifting of AP by the finger print clusters, etc..",['Renyu Zhao'],['stat.ML'],2016-04-07 11:08:45+00:00
http://arxiv.org/abs/1604.01952v1,Deep Online Convex Optimization with Gated Games,"Methods from convex optimization are widely used as building blocks for deep
learning algorithms. However, the reasons for their empirical success are
unclear, since modern convolutional networks (convnets), incorporating
rectifier units and max-pooling, are neither smooth nor convex. Standard
guarantees therefore do not apply. This paper provides the first convergence
rates for gradient descent on rectifier convnets. The proof utilizes the
particular structure of rectifier networks which consists in binary
active/inactive gates applied on top of an underlying linear network. The
approach generalizes to max-pooling, dropout and maxout. In other words, to
precisely the neural networks that perform best empirically. The key step is to
introduce gated games, an extension of convex games with similar convergence
properties that capture the gating function of rectifiers. The main result is
that rectifier convnets converge to a critical point at a rate controlled by
the gated-regret of the units in the network. Corollaries of the main result
include: (i) a game-theoretic description of the representations learned by a
neural network; (ii) a logarithmic-regret algorithm for training neural nets;
and (iii) a formal setting for analyzing conditional computation in neural nets
that can be applied to recently developed models of attention.",['David Balduzzi'],"['cs.LG', 'cs.GT', 'cs.NE', 'stat.ML']",2016-04-07 10:46:54+00:00
http://arxiv.org/abs/1604.01854v2,Building Ensembles of Adaptive Nested Dichotomies with Random-Pair Selection,"A system of nested dichotomies is a method of decomposing a multi-class
problem into a collection of binary problems. Such a system recursively splits
the set of classes into two subsets, and trains a binary classifier to
distinguish between each subset. Even though ensembles of nested dichotomies
with random structure have been shown to perform well in practice, using a more
sophisticated class subset selection method can be used to improve
classification accuracy. We investigate an approach to this problem called
random-pair selection, and evaluate its effectiveness compared to other
published methods of subset selection. We show that our method outperforms
other methods in many cases when forming ensembles of nested dichotomies, and
is at least on par in all other cases.","['Tim Leathart', 'Bernhard Pfahringer', 'Eibe Frank']","['stat.ML', 'cs.LG']",2016-04-07 02:56:19+00:00
http://arxiv.org/abs/1604.01733v1,A U-statistic Approach to Hypothesis Testing for Structure Discovery in Undirected Graphical Models,"Structure discovery in graphical models is the determination of the topology
of a graph that encodes conditional independence properties of the joint
distribution of all variables in the model. For some class of probability
distributions, an edge between two variables is present if and only if the
corresponding entry in the precision matrix is non-zero. For a finite sample
estimate of the precision matrix, entries close to zero may be due to low
sample effects, or due to an actual association between variables; these two
cases are not readily distinguishable. %Fisher provided a hypothesis test based
on a parametric approximation to the distribution of an entry in the precision
matrix of a Gaussian distribution, but this may not provide valid upper bounds
on $p$-values for non-Gaussian distributions. Many related works on this topic
consider potentially restrictive distributional or sparsity assumptions that
may not apply to a data sample of interest, and direct estimation of the
uncertainty of an estimate of the precision matrix for general distributions
remains challenging. Consequently, we make use of results for $U$-statistics
and apply them to the covariance matrix. By probabilistically bounding the
distortion of the covariance matrix, we can apply Weyl's theorem to bound the
distortion of the precision matrix, yielding a conservative, but sound test
threshold for a much wider class of distributions than considered in previous
works. The resulting test enables one to answer with statistical significance
whether an edge is present in the graph, and convergence results are known for
a wide range of distributions. The computational complexities is linear in the
sample size enabling the application of the test to large data samples for
which computation time becomes a limiting factor. We experimentally validate
the correctness and scalability of the test on multivariate distributions for
which the distributional assumptions of competing tests result in
underestimates of the false positive ratio. By contrast, the proposed test
remains sound, promising to be a useful tool for hypothesis testing for diverse
real-world problems.","['Wacha Bounliphone', 'Matthew Blaschko']","['stat.ML', 'math.ST', 'stat.TH']",2016-04-06 19:05:34+00:00
http://arxiv.org/abs/1604.01662v4,A Survey on Bayesian Deep Learning,"A comprehensive artificial intelligence system needs to not only perceive the
environment with different `senses' (e.g., seeing and hearing) but also infer
the world's conditional (or even causal) relations and corresponding
uncertainty. The past decade has seen major advances in many perception tasks
such as visual object recognition and speech recognition using deep learning
models. For higher-level inference, however, probabilistic graphical models
with their Bayesian nature are still more powerful and flexible. In recent
years, Bayesian deep learning has emerged as a unified probabilistic framework
to tightly integrate deep learning and Bayesian models. In this general
framework, the perception of text or images using deep learning can boost the
performance of higher-level inference and in turn, the feedback from the
inference process is able to enhance the perception of text or images. This
survey provides a comprehensive introduction to Bayesian deep learning and
reviews its recent applications on recommender systems, topic models, control,
etc. Besides, we also discuss the relationship and differences between Bayesian
deep learning and other related topics such as Bayesian treatment of neural
networks. For a constantly updating project page, please refer to
https://github.com/js05212/BayesianDeepLearning-Survey.","['Hao Wang', 'Dit-Yan Yeung']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.NE']",2016-04-06 15:35:08+00:00
