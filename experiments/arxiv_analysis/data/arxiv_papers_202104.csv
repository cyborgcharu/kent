id,title,abstract,authors,categories,date
http://arxiv.org/abs/2105.14368v1,Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation,"In the past decade the mathematical theory of machine learning has lagged far
behind the triumphs of deep neural networks on practical challenges. However,
the gap between theory and practice is gradually starting to close. In this
paper I will attempt to assemble some pieces of the remarkable and still
incomplete mathematical mosaic emerging from the efforts to understand the
foundations of deep learning. The two key themes will be interpolation, and its
sibling, over-parameterization. Interpolation corresponds to fitting data, even
noisy data, exactly. Over-parameterization enables interpolation and provides
flexibility to select a right interpolating model.
  As we will see, just as a physical prism separates colors mixed within a ray
of light, the figurative prism of interpolation helps to disentangle
generalization and optimization properties within the complex picture of modern
Machine Learning. This article is written with belief and hope that clearer
understanding of these issues brings us a step closer toward a general theory
of deep learning and machine learning.",['Mikhail Belkin'],"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2021-05-29 20:15:53+00:00
http://arxiv.org/abs/2105.14367v3,Deconvolutional Density Network: Modeling Free-Form Conditional Distributions,"Conditional density estimation (CDE) is the task of estimating the
probability of an event conditioned on some inputs. A neural network (NN) can
also be used to compute the output distribution for continuous-domain, which
can be viewed as an extension of regression task. Nevertheless, it is difficult
to explicitly approximate a distribution without knowing the information of its
general form a priori. In order to fit an arbitrary conditional distribution,
discretizing the continuous domain into bins is an effective strategy, as long
as we have sufficiently narrow bins and very large data. However, collecting
enough data is often hard to reach and falls far short of that ideal in many
circumstances, especially in multivariate CDE for the curse of dimensionality.
In this paper, we demonstrate the benefits of modeling free-form conditional
distributions using a deconvolution-based neural net framework, coping with
data deficiency problems in discretization. It has the advantage of being
flexible but also takes advantage of the hierarchical smoothness offered by the
deconvolution layers. We compare our method to a number of other
density-estimation approaches and show that our Deconvolutional Density Network
(DDN) outperforms the competing methods on many univariate and multivariate
tasks. The code of DDN is available at https://github.com/NBICLAB/DDN.","['Bing Chen', 'Mazharul Islam', 'Jisuo Gao', 'Lin Wang']","['cs.LG', 'stat.ML']",2021-05-29 20:09:25+00:00
http://arxiv.org/abs/2105.14363v3,On the Theory of Reinforcement Learning with Once-per-Episode Feedback,"We study a theory of reinforcement learning (RL) in which the learner
receives binary feedback only once at the end of an episode. While this is an
extreme test case for theory, it is also arguably more representative of
real-world applications than the traditional requirement in RL practice that
the learner receive feedback at every time step. Indeed, in many real-world
applications of reinforcement learning, such as self-driving cars and robotics,
it is easier to evaluate whether a learner's complete trajectory was either
""good"" or ""bad,"" but harder to provide a reward signal at each step. To show
that learning is possible in this more challenging setting, we study the case
where trajectory labels are generated by an unknown parametric model, and
provide a statistically and computationally efficient algorithm that achieves
sublinear regret.","['Niladri S. Chatterji', 'Aldo Pacchiano', 'Peter L. Bartlett', 'Michael I. Jordan']","['cs.LG', 'cs.AI', 'stat.ML']",2021-05-29 19:48:51+00:00
http://arxiv.org/abs/2106.00543v2,MARL with General Utilities via Decentralized Shadow Reward Actor-Critic,"We posit a new mechanism for cooperation in multi-agent reinforcement
learning (MARL) based upon any nonlinear function of the team's long-term
state-action occupancy measure, i.e., a \emph{general utility}. This subsumes
the cumulative return but also allows one to incorporate risk-sensitivity,
exploration, and priors. % We derive the {\bf D}ecentralized {\bf S}hadow
Reward {\bf A}ctor-{\bf C}ritic (DSAC) in which agents alternate between policy
evaluation (critic), weighted averaging with neighbors (information mixing),
and local gradient updates for their policy parameters (actor). DSAC augments
the classic critic step by requiring agents to (i) estimate their local
occupancy measure in order to (ii) estimate the derivative of the local utility
with respect to their occupancy measure, i.e., the ""shadow reward"". DSAC
converges to $\epsilon$-stationarity in $\mathcal{O}(1/\epsilon^{2.5})$
(Theorem \ref{theorem:final}) or faster $\mathcal{O}(1/\epsilon^{2})$
(Corollary \ref{corollary:communication}) steps with high probability,
depending on the amount of communications. We further establish the
non-existence of spurious stationary points for this problem, that is, DSAC
finds the globally optimal policy (Corollary \ref{corollary:global}).
Experiments demonstrate the merits of goals beyond the cumulative return in
cooperative MARL.","['Junyu Zhang', 'Amrit Singh Bedi', 'Mengdi Wang', 'Alec Koppel']","['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']",2021-05-29 19:05:48+00:00
http://arxiv.org/abs/2105.14337v2,Optimal transport with $f$-divergence regularization and generalized Sinkhorn algorithm,"Entropic regularization provides a generalization of the original optimal
transport problem. It introduces a penalty term defined by the Kullback-Leibler
divergence, making the problem more tractable via the celebrated Sinkhorn
algorithm. Replacing the Kullback-Leibler divergence with a general
$f$-divergence leads to a natural generalization. The case of divergences
defined by superlinear functions was recently studied by Di Marino and Gerolin.
Using convex analysis, we extend the theory developed so far to include all
$f$-divergences defined by functions of Legendre type, and prove that under
some mild conditions, strong duality holds, optimums in both the primal and
dual problems are attained, the generalization of the $c$-transform is
well-defined, and we give sufficient conditions for the generalized Sinkhorn
algorithm to converge to an optimal solution. We propose a practical algorithm
for computing an approximate solution of the optimal transport problem with
$f$-divergence regularization via the generalized Sinkhorn algorithm. Finally,
we present experimental results on synthetic 2-dimensional data, demonstrating
the effects of using different $f$-divergences for regularization, which
influences convergence speed, numerical stability and sparsity of the optimal
coupling.","['Dávid Terjék', 'Diego González-Sánchez']","['math.OC', 'cs.IT', 'cs.LG', 'math.FA', 'math.IT', 'stat.ML', '49K30 (Primary), 49Q22, 49M29 (Secondary)']",2021-05-29 16:37:31+00:00
http://arxiv.org/abs/2105.14328v4,Transfer Learning under High-dimensional Generalized Linear Models,"In this work, we study the transfer learning problem under high-dimensional
generalized linear models (GLMs), which aim to improve the fit on target data
by borrowing information from useful source data. Given which sources to
transfer, we propose a transfer learning algorithm on GLM, and derive its
$\ell_1/\ell_2$-estimation error bounds as well as a bound for a prediction
error measure. The theoretical analysis shows that when the target and source
are sufficiently close to each other, these bounds could be improved over those
of the classical penalized estimator using only target data under mild
conditions. When we don't know which sources to transfer, an algorithm-free
transferable source detection approach is introduced to detect informative
sources. The detection consistency is proved under the high-dimensional GLM
transfer learning setting. We also propose an algorithm to construct confidence
intervals of each coefficient component, and the corresponding theories are
provided. Extensive simulations and a real-data experiment verify the
effectiveness of our algorithms. We implement the proposed GLM transfer
learning algorithms in a new R package glmtrans, which is available on CRAN.","['Ye Tian', 'Yang Feng']","['stat.ML', 'cs.LG', 'stat.ME']",2021-05-29 15:39:43+00:00
http://arxiv.org/abs/2105.14301v2,A Theory of Neural Tangent Kernel Alignment and Its Influence on Training,"The training dynamics and generalization properties of neural networks (NN)
can be precisely characterized in function space via the neural tangent kernel
(NTK). Structural changes to the NTK during training reflect feature learning
and underlie the superior performance of networks outside of the static kernel
regime. In this work, we seek to theoretically understand kernel alignment, a
prominent and ubiquitous structural change that aligns the NTK with the target
function. We first study a toy model of kernel evolution in which the NTK
evolves to accelerate training and show that alignment naturally emerges from
this demand. We then study alignment mechanism in deep linear networks and two
layer ReLU networks. These theories provide good qualitative descriptions of
kernel alignment and specialization in practical networks and identify factors
in network architecture and data structure that drive kernel alignment. In
nonlinear networks with multiple outputs, we identify the phenomenon of kernel
specialization, where the kernel function for each output head preferentially
aligns to its own target function. Together, our results provide a mechanistic
explanation of how kernel alignment emerges during NN training and a normative
explanation of how it benefits training.","['Haozhe Shan', 'Blake Bordelon']","['stat.ML', 'cs.LG']",2021-05-29 13:50:03+00:00
http://arxiv.org/abs/2105.14267v1,Information Directed Sampling for Sparse Linear Bandits,"Stochastic sparse linear bandits offer a practical model for high-dimensional
online decision-making problems and have a rich information-regret structure.
In this work we explore the use of information-directed sampling (IDS), which
naturally balances the information-regret trade-off. We develop a class of
information-theoretic Bayesian regret bounds that nearly match existing lower
bounds on a variety of problem instances, demonstrating the adaptivity of IDS.
To efficiently implement sparse IDS, we propose an empirical Bayesian approach
for sparse posterior sampling using a spike-and-slab Gaussian-Laplace prior.
Numerical results demonstrate significant regret reductions by sparse IDS
relative to several baselines.","['Botao Hao', 'Tor Lattimore', 'Wei Deng']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2021-05-29 10:26:23+00:00
http://arxiv.org/abs/2105.14260v2,Understanding Bandits with Graph Feedback,"The bandit problem with graph feedback, proposed in [Mannor and Shamir,
NeurIPS 2011], is modeled by a directed graph $G=(V,E)$ where $V$ is the
collection of bandit arms, and once an arm is triggered, all its incident arms
are observed. A fundamental question is how the structure of the graph affects
the min-max regret. We propose the notions of the fractional weak domination
number $\delta^*$ and the $k$-packing independence number capturing upper bound
and lower bound for the regret respectively. We show that the two notions are
inherently connected via aligning them with the linear program of the weakly
dominating set and its dual -- the fractional vertex packing set respectively.
Based on this connection, we utilize the strong duality theorem to prove a
general regret upper bound $O\left(\left( \delta^*\log
|V|\right)^{\frac{1}{3}}T^{\frac{2}{3}}\right)$ and a lower bound
$\Omega\left(\left(\delta^*/\alpha\right)^{\frac{1}{3}}T^{\frac{2}{3}}\right)$
where $\alpha$ is the integrality gap of the dual linear program. Therefore,
our bounds are tight up to a $\left(\log |V|\right)^{\frac{1}{3}}$ factor on
graphs with bounded integrality gap for the vertex packing problem including
trees and graphs with bounded degree. Moreover, we show that for several
special families of graphs, we can get rid of the $\left(\log
|V|\right)^{\frac{1}{3}}$ factor and establish optimal regret.","['Houshuang Chen', 'Zengfeng Huang', 'Shuai Li', 'Chihao Zhang']","['cs.LG', 'cs.DS', 'stat.ML']",2021-05-29 09:35:28+00:00
http://arxiv.org/abs/2105.14244v1,Learning Graphon Autoencoders for Generative Graph Modeling,"Graphon is a nonparametric model that generates graphs with arbitrary sizes
and can be induced from graphs easily. Based on this model, we propose a novel
algorithmic framework called \textit{graphon autoencoder} to build an
interpretable and scalable graph generative model. This framework treats
observed graphs as induced graphons in functional space and derives their
latent representations by an encoder that aggregates Chebshev graphon filters.
A linear graphon factorization model works as a decoder, leveraging the latent
representations to reconstruct the induced graphons (and the corresponding
observed graphs). We develop an efficient learning algorithm to learn the
encoder and the decoder, minimizing the Wasserstein distance between the model
and data distributions. This algorithm takes the KL divergence of the graph
distributions conditioned on different graphons as the underlying distance and
leads to a reward-augmented maximum likelihood estimation. The graphon
autoencoder provides a new paradigm to represent and generate graphs, which has
good generalizability and transferability.","['Hongteng Xu', 'Peilin Zhao', 'Junzhou Huang', 'Dixin Luo']","['cs.LG', 'cs.SI', 'stat.ML']",2021-05-29 08:11:40+00:00
http://arxiv.org/abs/2105.14203v4,Understanding Instance-based Interpretability of Variational Auto-Encoders,"Instance-based interpretation methods have been widely studied for supervised
learning methods as they help explain how black box neural networks predict.
However, instance-based interpretations remain ill-understood in the context of
unsupervised learning. In this paper, we investigate influence functions [Koh
and Liang, 2017], a popular instance-based interpretation method, for a class
of deep generative models called variational auto-encoders (VAE). We formally
frame the counter-factual question answered by influence functions in this
setting, and through theoretical analysis, examine what they reveal about the
impact of training samples on classical unsupervised learning methods. We then
introduce VAE- TracIn, a computationally efficient and theoretically sound
solution based on Pruthi et al. [2020], for VAEs. Finally, we evaluate
VAE-TracIn on several real world datasets with extensive quantitative and
qualitative analysis.","['Zhifeng Kong', 'Kamalika Chaudhuri']","['cs.LG', 'stat.ML']",2021-05-29 04:03:09+00:00
http://arxiv.org/abs/2105.14172v2,A Stochastic Alternating Balance $k$-Means Algorithm for Fair Clustering,"In the application of data clustering to human-centric decision-making
systems, such as loan applications and advertisement recommendations, the
clustering outcome might discriminate against people across different
demographic groups, leading to unfairness. A natural conflict occurs between
the cost of clustering (in terms of distance to cluster centers) and the
balance representation of all demographic groups across the clusters, leading
to a bi-objective optimization problem that is nonconvex and nonsmooth. To
determine the complete trade-off between these two competing goals, we design a
novel stochastic alternating balance fair $k$-means (SAfairKM) algorithm, which
consists of alternating classical mini-batch $k$-means updates and group swap
updates. The number of $k$-means updates and the number of swap updates
essentially parameterize the weight put on optimizing each objective function.
Our numerical experiments show that the proposed SAfairKM algorithm is robust
and computationally efficient in constructing well-spread and high-quality
Pareto fronts both on synthetic and real datasets.","['Suyun Liu', 'Luis Nunes Vicente']","['cs.LG', 'stat.ML']",2021-05-29 01:47:15+00:00
http://arxiv.org/abs/2105.14166v1,Rejection sampling from shape-constrained distributions in sublinear time,"We consider the task of generating exact samples from a target distribution,
known up to normalization, over a finite alphabet. The classical algorithm for
this task is rejection sampling, and although it has been used in practice for
decades, there is surprisingly little study of its fundamental limitations. In
this work, we study the query complexity of rejection sampling in a minimax
framework for various classes of discrete distributions. Our results provide
new algorithms for sampling whose complexity scales sublinearly with the
alphabet size. When applied to adversarial bandits, we show that a slight
modification of the Exp3 algorithm reduces the per-iteration complexity from
$\mathcal O(K)$ to $\mathcal O(\log^2 K)$, where $K$ is the number of arms.","['Sinho Chewi', 'Patrik Gerber', 'Chen Lu', 'Thibaut Le Gouic', 'Philippe Rigollet']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2021-05-29 01:00:42+00:00
http://arxiv.org/abs/2105.14146v1,Deep Fair Discriminative Clustering,"Deep clustering has the potential to learn a strong representation and hence
better clustering performance compared to traditional clustering methods such
as $k$-means and spectral clustering. However, this strong representation
learning ability may make the clustering unfair by discovering surrogates for
protected information which we empirically show in our experiments. In this
work, we study a general notion of group-level fairness for both binary and
multi-state protected status variables (PSVs). We begin by formulating the
group-level fairness problem as an integer linear programming formulation whose
totally unimodular constraint matrix means it can be efficiently solved via
linear programming. We then show how to inject this solver into a
discriminative deep clustering backbone and hence propose a refinement learning
algorithm to combine the clustering goal with the fairness objective to learn
fair clusters adaptively. Experimental results on real-world datasets
demonstrate that our model consistently outperforms state-of-the-art fair
clustering algorithms. Our framework shows promising results for novel
clustering tasks including flexible fairness constraints, multi-state PSVs and
predictive clustering.","['Hongjing Zhang', 'Ian Davidson']","['cs.LG', 'cs.CY', 'stat.ML']",2021-05-28 23:50:48+00:00
http://arxiv.org/abs/2105.14141v1,ARMS: Antithetic-REINFORCE-Multi-Sample Gradient for Binary Variables,"Estimating the gradients for binary variables is a task that arises
frequently in various domains, such as training discrete latent variable
models. What has been commonly used is a REINFORCE based Monte Carlo estimation
method that uses either independent samples or pairs of negatively correlated
samples. To better utilize more than two samples, we propose ARMS, an
Antithetic REINFORCE-based Multi-Sample gradient estimator. ARMS uses a copula
to generate any number of mutually antithetic samples. It is unbiased, has low
variance, and generalizes both DisARM, which we show to be ARMS with two
samples, and the leave-one-out REINFORCE (LOORF) estimator, which is ARMS with
uncorrelated samples. We evaluate ARMS on several datasets for training
generative models, and our experimental results show that it outperforms
competing methods. We also develop a version of ARMS for optimizing the
multi-sample variational bound, and show that it outperforms both VIMCO and
DisARM. The code is publicly available.","['Alek Dimitriev', 'Mingyuan Zhou']","['cs.LG', 'stat.ML']",2021-05-28 23:19:54+00:00
http://arxiv.org/abs/2105.14119v2,Towards optimally abstaining from prediction with OOD test examples,"A common challenge across all areas of machine learning is that training data
is not distributed like test data, due to natural shifts, ""blind spots,"" or
adversarial examples; such test examples are referred to as out-of-distribution
(OOD) test examples. We consider a model where one may abstain from predicting,
at a fixed cost. In particular, our transductive abstention algorithm takes
labeled training examples and unlabeled test examples as input, and provides
predictions with optimal prediction loss guarantees. The loss bounds match
standard generalization bounds when test examples are i.i.d. from the training
distribution, but add an additional term that is the cost of abstaining times
the statistical distance between the train and test distribution (or the
fraction of adversarial examples). For linear regression, we give a
polynomial-time algorithm based on Celis-Dennis-Tapia optimization algorithms.
For binary classification, we show how to efficiently implement it using a
proper agnostic learner (i.e., an Empirical Risk Minimizer) for the class of
interest. Our work builds on a recent abstention algorithm of Goldwasser,
Kalais, and Montasser (2020) for transductive binary classification.","['Adam Tauman Kalai', 'Varun Kanade']","['cs.LG', 'cs.AI', 'cs.DS', 'stat.ML']",2021-05-28 21:44:48+00:00
http://arxiv.org/abs/2105.14114v1,Asymptotically Optimal Bandits under Weighted Information,"We study the problem of regret minimization in a multi-armed bandit setup
where the agent is allowed to play multiple arms at each round by spreading the
resources usually allocated to only one arm. At each iteration the agent
selects a normalized power profile and receives a Gaussian vector as outcome,
where the unknown variance of each sample is inversely proportional to the
power allocated to that arm. The reward corresponds to a linear combination of
the power profile and the outcomes, resembling a linear bandit. By spreading
the power, the agent can choose to collect information much faster than in a
traditional multi-armed bandit at the price of reducing the accuracy of the
samples. This setup is fundamentally different from that of a linear bandit --
the regret is known to scale as $\Theta(\sqrt{T})$ for linear bandits, while in
this setup the agent receives a much more detailed feedback, for which we
derive a tight $\log(T)$ problem-dependent lower-bound. We propose a
Thompson-Sampling-based strategy, called Weighted Thompson Sampling (\WTS),
that designs the power profile as its posterior belief of each arm being the
best arm, and show that its upper bound matches the derived logarithmic lower
bound. Finally, we apply this strategy to a problem of control and system
identification, where the goal is to estimate the maximum gain (also called
$\mathcal{H}_\infty$-norm) of a linear dynamical system based on batches of
input-output samples.","['Matias I. Müller', 'Cristian R. Rojas']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2021-05-28 21:28:44+00:00
http://arxiv.org/abs/2105.14099v2,Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot Meta-Learning,"Despite recent advances in its theoretical understanding, there still remains
a significant gap in the ability of existing PAC-Bayesian theories on
meta-learning to explain performance improvements in the few-shot learning
setting, where the number of training examples in the target tasks is severely
limited. This gap originates from an assumption in the existing theories which
supposes that the number of training examples in the observed tasks and the
number of training examples in the target tasks follow the same distribution,
an assumption that rarely holds in practice. By relaxing this assumption, we
develop two PAC-Bayesian bounds tailored for the few-shot learning setting and
show that two existing meta-learning algorithms (MAML and Reptile) can be
derived from our bounds, thereby bridging the gap between practice and
PAC-Bayesian theories. Furthermore, we derive a new computationally-efficient
PACMAML algorithm, and show it outperforms existing meta-learning algorithms on
several few-shot benchmark datasets.","['Nan Ding', 'Xi Chen', 'Tomer Levinboim', 'Sebastian Goodman', 'Radu Soricut']","['cs.LG', 'stat.ML']",2021-05-28 20:40:40+00:00
http://arxiv.org/abs/2105.14095v2,Weighted Training for Cross-Task Learning,"In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted
training algorithm for cross-task learning based on minimizing a
representation-based task distance between the source and target tasks. We show
that TAWT is easy to implement, is computationally efficient, requires little
hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees.
The effectiveness of TAWT is corroborated through extensive experiments with
BERT on four sequence tagging tasks in natural language processing (NLP),
including part-of-speech (PoS) tagging, chunking, predicate detection, and
named entity recognition (NER). As a byproduct, the proposed
representation-based task distance allows one to reason in a theoretically
principled way about several critical aspects of cross-task learning, such as
the choice of the source data and the impact of fine-tuning.","['Shuxiao Chen', 'Koby Crammer', 'Hangfeng He', 'Dan Roth', 'Weijie J. Su']","['cs.LG', 'cs.CL', 'stat.ML']",2021-05-28 20:27:02+00:00
http://arxiv.org/abs/2105.14084v2,Support vector machines and linear regression coincide with very high-dimensional features,"The support vector machine (SVM) and minimum Euclidean norm least squares
regression are two fundamentally different approaches to fitting linear models,
but they have recently been connected in models for very high-dimensional data
through a phenomenon of support vector proliferation, where every training
example used to fit an SVM becomes a support vector. In this paper, we explore
the generality of this phenomenon and make the following contributions. First,
we prove a super-linear lower bound on the dimension (in terms of sample size)
required for support vector proliferation in independent feature models,
matching the upper bounds from previous works. We further identify a sharp
phase transition in Gaussian feature models, bound the width of this
transition, and give experimental support for its universality. Finally, we
hypothesize that this phase transition occurs only in much higher-dimensional
settings in the $\ell_1$ variant of the SVM, and we present a new geometric
characterization of the problem that may elucidate this phenomenon for the
general $\ell_p$ case.","['Navid Ardeshir', 'Clayton Sanford', 'Daniel Hsu']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2021-05-28 20:06:21+00:00
http://arxiv.org/abs/2105.14083v2,Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial Awareness,"Most studies on learning from noisy labels rely on unrealistic models of
i.i.d. label noise, such as class-conditional transition matrices. More recent
work on instance-dependent noise models are more realistic, but assume a single
generative process for label noise across the entire dataset. We propose a more
principled model of label noise that generalizes instance-dependent noise to
multiple labelers, based on the observation that modern datasets are typically
annotated using distributed crowdsourcing methods. Under our labeler-dependent
model, label noise manifests itself under two modalities: natural error of
good-faith labelers, and adversarial labels provided by malicious actors. We
present two adversarial attack vectors that more accurately reflect the label
noise that may be encountered in real-world settings, and demonstrate that
under our multimodal noisy labels model, state-of-the-art approaches for
learning from noisy labels are defeated by adversarial label attacks. Finally,
we propose a multi-stage, labeler-aware, model-agnostic framework that reliably
filters noisy labels by leveraging knowledge about which data partitions were
labeled by which labeler, and show that our proposed framework remains robust
even in the presence of extreme adversarial label noise.","['Glenn Dawson', 'Robi Polikar']","['cs.LG', 'cs.AI', 'cs.HC', 'stat.ML']",2021-05-28 19:58:18+00:00
http://arxiv.org/abs/2105.14080v1,Gotta Go Fast When Generating Data with Score-Based Models,"Score-based (denoising diffusion) generative models have recently gained a
lot of success in generating realistic and diverse data. These approaches
define a forward diffusion process for transforming data to noise and generate
data by reversing it (thereby going from noise to data). Unfortunately, current
score-based models generate data very slowly due to the sheer number of score
network evaluations required by numerical SDE solvers.
  In this work, we aim to accelerate this process by devising a more efficient
SDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which
uses a fixed step size. We found that naively replacing it with other SDE
solvers fares poorly - they either result in low-quality samples or become
slower than EM. To get around this issue, we carefully devise an SDE solver
with adaptive step sizes tailored to score-based generative models piece by
piece. Our solver requires only two score function evaluations, rarely rejects
samples, and leads to high-quality samples. Our approach generates data 2 to 10
times faster than EM while achieving better or equal sample quality. For
high-resolution images, our method leads to significantly higher quality
samples than all other methods tested. Our SDE solver has the benefit of
requiring no step size tuning.","['Alexia Jolicoeur-Martineau', 'Ke Li', 'Rémi Piché-Taillefer', 'Tal Kachman', 'Ioannis Mitliagkas']","['cs.LG', 'cs.CV', 'math.OC', 'stat.ML']",2021-05-28 19:48:51+00:00
http://arxiv.org/abs/2105.14035v2,DeepMoM: Robust Deep Learning With Median-of-Means,"Data used in deep learning is notoriously problematic. For example, data are
usually combined from diverse sources, rarely cleaned and vetted thoroughly,
and sometimes corrupted on purpose. Intentional corruption that targets the
weak spots of algorithms has been studied extensively under the label of
""adversarial attacks."" In contrast, the arguably much more common case of
corruption that reflects the limited quality of data has been studied much
less. Such ""random"" corruptions are due to measurement errors, unreliable
sources, convenience sampling, and so forth. These kinds of corruption are
common in deep learning, because data are rarely collected according to strict
protocols -- in strong contrast to the formalized data collection in some parts
of classical statistics. This paper concerns such corruption. We introduce an
approach motivated by very recent insights into median-of-means and Le Cam's
principle, we show that the approach can be readily implemented, and we
demonstrate that it performs very well in practice. In conclusion, we believe
that our approach is a very promising alternative to standard parameter
training based on least-squares and cross-entropy loss.","['Shih-Ting Huang', 'Johannes Lederer']","['stat.ML', 'cs.LG', 'stat.CO']",2021-05-28 18:07:32+00:00
http://arxiv.org/abs/2105.14027v3,The Dark Machines Anomaly Score Challenge: Benchmark Data and Model Independent Event Classification for the Large Hadron Collider,"We describe the outcome of a data challenge conducted as part of the Dark
Machines Initiative and the Les Houches 2019 workshop on Physics at TeV
colliders. The challenged aims at detecting signals of new physics at the LHC
using unsupervised machine learning algorithms. First, we propose how an
anomaly score could be implemented to define model-independent signal regions
in LHC searches. We define and describe a large benchmark dataset, consisting
of >1 Billion simulated LHC events corresponding to $10~\rm{fb}^{-1}$ of
proton-proton collisions at a center-of-mass energy of 13 TeV. We then review a
wide range of anomaly detection and density estimation algorithms, developed in
the context of the data challenge, and we measure their performance in a set of
realistic analysis environments. We draw a number of useful conclusions that
will aid the development of unsupervised new physics searches during the third
run of the LHC, and provide our benchmark dataset for future studies at
https://www.phenoMLdata.org. Code to reproduce the analysis is provided at
https://github.com/bostdiek/DarkMachines-UnsupervisedChallenge.","['T. Aarrestad', 'M. van Beekveld', 'M. Bona', 'A. Boveia', 'S. Caron', 'J. Davies', 'A. De Simone', 'C. Doglioni', 'J. M. Duarte', 'A. Farbin', 'H. Gupta', 'L. Hendriks', 'L. Heinrich', 'J. Howarth', 'P. Jawahar', 'A. Jueid', 'J. Lastow', 'A. Leinweber', 'J. Mamuzic', 'E. Merényi', 'A. Morandini', 'P. Moskvitina', 'C. Nellist', 'J. Ngadiuba', 'B. Ostdiek', 'M. Pierini', 'B. Ravina', 'R. Ruiz de Austri', 'S. Sekmen', 'M. Touranakou', 'M. Vaškevičiūte', 'R. Vilalta', 'J. R. Vlimant', 'R. Verheyen', 'M. White', 'E. Wulff', 'E. Wallin', 'K. A. Wozniak', 'Z. Zhang']","['hep-ph', 'hep-ex', 'physics.data-an', 'stat.ML']",2021-05-28 18:00:02+00:00
http://arxiv.org/abs/2105.14016v3,Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs with a Generative Model,"The curse of dimensionality is a widely known issue in reinforcement learning
(RL). In the tabular setting where the state space $\mathcal{S}$ and the action
space $\mathcal{A}$ are both finite, to obtain a nearly optimal policy with
sampling access to a generative model, the minimax optimal sample complexity
scales linearly with $|\mathcal{S}|\times|\mathcal{A}|$, which can be
prohibitively large when $\mathcal{S}$ or $\mathcal{A}$ is large. This paper
considers a Markov decision process (MDP) that admits a set of state-action
features, which can linearly express (or approximate) its probability
transition kernel. We show that a model-based approach (resp.$~$Q-learning)
provably learns an $\varepsilon$-optimal policy (resp.$~$Q-function) with high
probability as soon as the sample size exceeds the order of
$\frac{K}{(1-\gamma)^{3}\varepsilon^{2}}$
(resp.$~$$\frac{K}{(1-\gamma)^{4}\varepsilon^{2}}$), up to some logarithmic
factor. Here $K$ is the feature dimension and $\gamma\in(0,1)$ is the discount
factor of the MDP. Both sample complexity bounds are provably tight, and our
result for the model-based approach matches the minimax lower bound. Our
results show that for arbitrarily large-scale MDP, both the model-based
approach and Q-learning are sample-efficient when $K$ is relatively small, and
hence the title of this paper.","['Bingyan Wang', 'Yuling Yan', 'Jianqing Fan']","['cs.LG', 'cs.IT', 'math.IT', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",2021-05-28 17:49:39+00:00
http://arxiv.org/abs/2105.13975v1,Relation Matters in Sampling: A Scalable Multi-Relational Graph Neural Network for Drug-Drug Interaction Prediction,"Sampling is an established technique to scale graph neural networks to large
graphs. Current approaches however assume the graphs to be homogeneous in terms
of relations and ignore relation types, critically important in biomedical
graphs. Multi-relational graphs contain various types of relations that usually
come with variable frequency and have different importance for the problem at
hand. We propose an approach to modeling the importance of relation types for
neighborhood sampling in graph neural networks and show that we can learn the
right balance: relation-type probabilities that reflect both frequency and
importance. Our experiments on drug-drug interaction prediction show that
state-of-the-art graph neural networks profit from relation-dependent sampling
in terms of both accuracy and efficiency.","['Arthur Feeney', 'Rishabh Gupta', 'Veronika Thost', 'Rico Angell', 'Gayathri Chandu', 'Yash Adhikari', 'Tengfei Ma']","['cs.LG', 'cs.AI', 'stat.ML']",2021-05-28 16:55:09+00:00
http://arxiv.org/abs/2105.13949v1,Latent Space Exploration Using Generative Kernel PCA,"Kernel PCA is a powerful feature extractor which recently has seen a
reformulation in the context of Restricted Kernel Machines (RKMs). These RKMs
allow for a representation of kernel PCA in terms of hidden and visible units
similar to Restricted Boltzmann Machines. This connection has led to insights
on how to use kernel PCA in a generative procedure, called generative kernel
PCA. In this paper, the use of generative kernel PCA for exploring latent
spaces of datasets is investigated. New points can be generated by gradually
moving in the latent space, which allows for an interpretation of the
components. Firstly, examples of this feature space exploration on three
datasets are shown with one of them leading to an interpretable representation
of ECG signals. Afterwards, the use of the tool in combination with novelty
detection is shown, where the latent space around novel patterns in the data is
explored. This helps in the interpretation of why certain points are considered
as novel.","['David Winant', 'Joachim Schreurs', 'Johan A. K. Suykens']","['cs.LG', 'stat.ML']",2021-05-28 16:17:37+00:00
http://arxiv.org/abs/2105.13942v1,Towards Deterministic Diverse Subset Sampling,"Determinantal point processes (DPPs) are well known models for diverse subset
selection problems, including recommendation tasks, document summarization and
image search. In this paper, we discuss a greedy deterministic adaptation of
k-DPP. Deterministic algorithms are interesting for many applications, as they
provide interpretability to the user by having no failure probability and
always returning the same results. First, the ability of the method to yield
low-rank approximations of kernel matrices is evaluated by comparing the
accuracy of the Nystr\""om approximation on multiple datasets. Afterwards, we
demonstrate the usefulness of the model on an image search task.","['Joachim Schreurs', 'Michaël Fanuel', 'Johan A. K. Suykens']","['cs.LG', 'stat.ML']",2021-05-28 16:05:58+00:00
http://arxiv.org/abs/2105.13939v2,Efficient Online-Bandit Strategies for Minimax Learning Problems,"Several learning problems involve solving min-max problems, e.g., empirical
distributional robust learning or learning with non-standard aggregated losses.
More specifically, these problems are convex-linear problems where the
minimization is carried out over the model parameters $w\in\mathcal{W}$ and the
maximization over the empirical distribution $p\in\mathcal{K}$ of the training
set indexes, where $\mathcal{K}$ is the simplex or a subset of it. To design
efficient methods, we let an online learning algorithm play against a
(combinatorial) bandit algorithm. We argue that the efficiency of such
approaches critically depends on the structure of $\mathcal{K}$ and propose two
properties of $\mathcal{K}$ that facilitate designing efficient algorithms. We
focus on a specific family of sets $\mathcal{S}_{n,k}$ encompassing various
learning applications and provide high-probability convergence guarantees to
the minimax values.","['Christophe Roux', 'Elias Wirth', 'Sebastian Pokutta', 'Thomas Kerdreux']","['cs.LG', 'math.OC', 'stat.ML']",2021-05-28 16:01:42+00:00
http://arxiv.org/abs/2105.13937v3,Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks,"We present a new class of Langevin based algorithms, which overcomes many of
the known shortcomings of popular adaptive optimizers that are currently used
for the fine tuning of deep learning models. Its underpinning theory relies on
recent advances of Euler's polygonal approximations for stochastic differential
equations (SDEs) with monotone coefficients. As a result, it inherits the
stability properties of tamed algorithms, while it addresses other known
issues, e.g. vanishing gradients in neural networks. In particular, we provide
a nonasymptotic analysis and full theoretical guarantees for the convergence
properties of an algorithm of this novel class, which we named TH$\varepsilon$O
POULA (or, simply, TheoPouLa). Finally, several experiments are presented with
different types of deep learning models, which show the superior performance of
TheoPouLa over many popular adaptive optimization algorithms.","['Dong-Young Lim', 'Sotirios Sabanis']","['cs.LG', 'math.OC', 'math.PR', 'stat.ML']",2021-05-28 15:58:48+00:00
http://arxiv.org/abs/2105.13922v2,Discretization Drift in Two-Player Games,"Gradient-based methods for two-player games produce rich dynamics that can
solve challenging problems, yet can be difficult to stabilize and understand.
Part of this complexity originates from the discrete update steps given by
simultaneous or alternating gradient descent, which causes each player to drift
away from the continuous gradient flow -- a phenomenon we call discretization
drift. Using backward error analysis, we derive modified continuous dynamical
systems that closely follow the discrete dynamics. These modified dynamics
provide an insight into the notorious challenges associated with zero-sum
games, including Generative Adversarial Networks. In particular, we identify
distinct components of the discretization drift that can alter performance and
in some cases destabilize the game. Finally, quantifying discretization drift
allows us to identify regularizers that explicitly cancel harmful forms of
drift or strengthen beneficial forms of drift, and thus improve performance of
GAN training.","['Mihaela Rosca', 'Yan Wu', 'Benoit Dherin', 'David G. T. Barrett']","['stat.ML', 'cs.LG']",2021-05-28 15:38:34+00:00
http://arxiv.org/abs/2105.13913v8,Scalable Frank-Wolfe on Generalized Self-concordant Functions via Simple Steps,"Generalized self-concordance is a key property present in the objective
function of many important learning problems. We establish the convergence rate
of a simple Frank-Wolfe variant that uses the open-loop step size strategy
$\gamma_t = 2/(t+2)$, obtaining a $\mathcal{O}(1/t)$ convergence rate for this
class of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the
iteration count. This avoids the use of second-order information or the need to
estimate local smoothness parameters of previous work. We also show improved
convergence rates for various common cases, e.g., when the feasible region
under consideration is uniformly convex or polyhedral.","['Alejandro Carderera', 'Mathieu Besançon', 'Sebastian Pokutta']","['math.OC', 'cs.LG', 'stat.ML']",2021-05-28 15:26:36+00:00
http://arxiv.org/abs/2105.13859v4,"Generative Network-Based Reduced-Order Model for Prediction, Data Assimilation and Uncertainty Quantification","We propose a new method in which a generative network (GN) integrate into a
reduced-order model (ROM) framework is used to solve inverse problems for
partial differential equations (PDE). The aim is to match available
measurements and estimate the corresponding uncertainties associated with the
states and parameters of a numerical physical simulation. The GN is trained
using only unconditional simulations of the discretized PDE model. We compare
the proposed method with the golden standard Markov chain Monte Carlo. We apply
the proposed approaches to a spatio-temporal compartmental model in
epidemiology. The results show that the proposed GN-based ROM can efficiently
quantify uncertainty and accurately match the measurements and the golden
standard, using only a few unconditional simulations of the full-order
numerical PDE model.","['Vinicius L. S. Silva', 'Claire E. Heaney', 'Nenko Nenov', 'Christopher C. Pain']","['cs.LG', 'stat.ML']",2021-05-28 14:12:45+00:00
http://arxiv.org/abs/2105.13850v1,pRSL: Interpretable Multi-label Stacking by Learning Probabilistic Rules,"A key task in multi-label classification is modeling the structure between
the involved classes. Modeling this structure by probabilistic and
interpretable means enables application in a broad variety of tasks such as
zero-shot learning or learning from incomplete data. In this paper, we present
the probabilistic rule stacking learner (pRSL) which uses probabilistic
propositional logic rules and belief propagation to combine the predictions of
several underlying classifiers. We derive algorithms for exact and approximate
inference and learning, and show that pRSL reaches state-of-the-art performance
on various benchmark datasets.
  In the process, we introduce a novel multicategorical generalization of the
noisy-or gate. Additionally, we report simulation results on the quality of
loopy belief propagation algorithms for approximate inference in bipartite
noisy-or networks.","['Michael Kirchhof', 'Lena Schmid', 'Christopher Reining', 'Michael ten Hompel', 'Markus Pauly']","['stat.ML', 'cs.LG', 'stat.CO']",2021-05-28 14:06:21+00:00
http://arxiv.org/abs/2105.13841v2,A General Taylor Framework for Unifying and Revisiting Attribution Methods,"Attribution methods provide an insight into the decision-making process of
machine learning models, especially deep neural networks, by assigning
contribution scores to each individual feature. However, the attribution
problem has not been well-defined, which lacks a unified guideline to the
contribution assignment process. Furthermore, existing attribution methods
often built upon various empirical intuitions and heuristics. There still lacks
a general theoretical framework that not only can offer a good description of
the attribution problem, but also can be applied to unifying and revisiting
existing attribution methods. To bridge the gap, in this paper, we propose a
Taylor attribution framework, which models the attribution problem as how to
decide individual payoffs in a coalition. Then, we reformulate fourteen
mainstream attribution methods into the Taylor framework and analyze these
attribution methods in terms of rationale, fidelity, and limitation in the
framework. Moreover, we establish three principles for a good attribution in
the Taylor attribution framework, i.e., low approximation error, correct Taylor
contribution assignment, and unbiased baseline selection. Finally, we
empirically validate the Taylor reformulations and reveal a positive
correlation between the attribution performance and the number of principles
followed by the attribution method via benchmarking on real-world datasets.","['Huiqi Deng', 'Na Zou', 'Mengnan Du', 'Weifu Chen', 'Guocan Feng', 'Xia Hu']","['cs.LG', 'cs.AI', 'stat.ML']",2021-05-28 13:57:16+00:00
http://arxiv.org/abs/2105.13831v2,Implicit Regularization in Matrix Sensing via Mirror Descent,"We study discrete-time mirror descent applied to the unregularized empirical
risk in matrix sensing. In both the general case of rectangular matrices and
the particular case of positive semidefinite matrices, a simple potential-based
analysis in terms of the Bregman divergence allows us to establish convergence
of mirror descent -- with different choices of the mirror maps -- to a matrix
that, among all global minimizers of the empirical risk, minimizes a quantity
explicitly related to the nuclear norm, the Frobenius norm, and the von Neumann
entropy. In both cases, this characterization implies that mirror descent, a
first-order algorithm minimizing the unregularized empirical risk, recovers
low-rank matrices under the same set of assumptions that are sufficient to
guarantee recovery for nuclear-norm minimization. When the sensing matrices are
symmetric and commute, we show that gradient descent with full-rank factorized
parametrization is a first-order approximation to mirror descent, in which case
we obtain an explicit characterization of the implicit bias of gradient flow as
a by-product.","['Fan Wu', 'Patrick Rebeschini']","['stat.ML', 'cs.LG']",2021-05-28 13:46:47+00:00
http://arxiv.org/abs/2105.13810v1,A Survey on Anomaly Detection for Technical Systems using LSTM Networks,"Anomalies represent deviations from the intended system operation and can
lead to decreased efficiency as well as partial or complete system failure. As
the causes of anomalies are often unknown due to complex system dynamics,
efficient anomaly detection is necessary. Conventional detection approaches
rely on statistical and time-invariant methods that fail to address the complex
and dynamic nature of anomalies. With advances in artificial intelligence and
increasing importance for anomaly detection and prevention in various domains,
artificial neural network approaches enable the detection of more complex
anomaly types while considering temporal and contextual characteristics. In
this article, a survey on state-of-the-art anomaly detection using deep neural
and especially long short-term memory networks is conducted. The investigated
approaches are evaluated based on the application scenario, data and anomaly
types as well as further metrics. To highlight the potential of upcoming
anomaly detection techniques, graph-based and transfer learning approaches are
also included in the survey, enabling the analysis of heterogeneous data as
well as compensating for its shortage and improving the handling of dynamic
processes.","['Benjamin Lindemann', 'Benjamin Maschler', 'Nada Sahlab', 'Michael Weyrich']","['cs.LG', 'cs.AI', 'stat.ML']",2021-05-28 13:24:40+00:00
http://arxiv.org/abs/2105.13745v1,Robust Regularization with Adversarial Labelling of Perturbed Samples,"Recent researches have suggested that the predictive accuracy of neural
network may contend with its adversarial robustness. This presents challenges
in designing effective regularization schemes that also provide strong
adversarial robustness. Revisiting Vicinal Risk Minimization (VRM) as a
unifying regularization principle, we propose Adversarial Labelling of
Perturbed Samples (ALPS) as a regularization scheme that aims at improving the
generalization ability and adversarial robustness of the trained model. ALPS
trains neural networks with synthetic samples formed by perturbing each
authentic input sample towards another one along with an adversarially assigned
label. The ALPS regularization objective is formulated as a min-max problem, in
which the outer problem is minimizing an upper-bound of the VRM loss, and the
inner problem is L$_1$-ball constrained adversarial labelling on perturbed
sample. The analytic solution to the induced inner maximization problem is
elegantly derived, which enables computational efficiency. Experiments on the
SVHN, CIFAR-10, CIFAR-100 and Tiny-ImageNet datasets show that the ALPS has a
state-of-the-art regularization performance while also serving as an effective
adversarial training scheme.","['Xiaohui Guo', 'Richong Zhang', 'Yaowei Zheng', 'Yongyi Mao']","['cs.LG', 'cs.AI', 'stat.ML']",2021-05-28 11:26:49+00:00
http://arxiv.org/abs/2105.13727v3,Slow Momentum with Fast Reversion: A Trading Strategy Using Deep Learning and Changepoint Detection,"Momentum strategies are an important part of alternative investments and are
at the heart of commodity trading advisors (CTAs). These strategies have,
however, been found to have difficulties adjusting to rapid changes in market
conditions, such as during the 2020 market crash. In particular, immediately
after momentum turning points, where a trend reverses from an uptrend
(downtrend) to a downtrend (uptrend), time-series momentum (TSMOM) strategies
are prone to making bad bets. To improve the response to regime change, we
introduce a novel approach, where we insert an online changepoint detection
(CPD) module into a Deep Momentum Network (DMN) [1904.04912] pipeline, which
uses an LSTM deep-learning architecture to simultaneously learn both trend
estimation and position sizing. Furthermore, our model is able to optimise the
way in which it balances 1) a slow momentum strategy which exploits persisting
trends, but does not overreact to localised price moves, and 2) a fast
mean-reversion strategy regime by quickly flipping its position, then swapping
it back again to exploit localised price moves. Our CPD module outputs a
changepoint location and severity score, allowing our model to learn to respond
to varying degrees of disequilibrium, or smaller and more localised
changepoints, in a data driven manner. Back-testing our model over the period
1995-2020, the addition of the CPD module leads to an improvement in Sharpe
ratio of one-third. The module is especially beneficial in periods of
significant nonstationarity, and in particular, over the most recent years
tested (2015-2020) the performance boost is approximately two-thirds. This is
interesting as traditional momentum strategies have been underperforming in
this period.","['Kieran Wood', 'Stephen Roberts', 'Stefan Zohren']","['stat.ML', 'cs.LG', 'q-fin.TR']",2021-05-28 10:46:53+00:00
http://arxiv.org/abs/2105.13669v1,Measuring global properties of neural generative model outputs via generating mathematical objects,"We train deep generative models on datasets of reflexive polytopes. This
enables us to compare how well the models have picked up on various global
properties of generated samples. Our datasets are complete in the sense that
every single example, up to changes of coordinate, is included in the dataset.
Using this property we also perform tests checking to what extent the models
are merely memorizing the data. We also train models on the same dataset
represented in two different ways, enabling us to measure which form is easiest
to learn from. We use these experiments to show that deep generative models can
learn to generate geometric objects with non-trivial global properties, and
that the models learn some underlying properties of the objects rather than
simply memorizing the data.",['Bernt Ivar Utstøl Nødland'],"['cs.LG', 'math.CO', 'stat.ML']",2021-05-28 08:38:18+00:00
http://arxiv.org/abs/2105.13655v3,Scheduling Jobs with Stochastic Holding Costs,"We study a single-server scheduling problem for the objective of minimizing
the expected cumulative holding cost incurred by jobs, where parameters
defining stochastic job holding costs are unknown to the scheduler. We consider
a general setting allowing for different job classes, where jobs of the same
class have statistically identical holding costs and service times, with an
arbitrary number of jobs across classes. In each time step, the server can
process a job and observes random holding costs of the jobs that are yet to be
completed. We consider a learning-based $c\mu$ rule scheduling which starts
with a preemption period of fixed duration, serving as a learning phase, and
having gathered data about jobs, it switches to nonpreemptive scheduling. Our
algorithms are designed to handle instances with large and small gaps in mean
job holding costs and achieve near-optimal performance guarantees. The
performance of algorithms is evaluated by regret, where the benchmark is the
minimum possible total holding cost attained by the $c\mu$ rule scheduling
policy when the parameters of jobs are known. We show regret lower bounds and
algorithms that achieve nearly matching regret upper bounds. Our numerical
results demonstrate the efficacy of our algorithms and show that our regret
analysis is nearly tight.","['Dabeen Lee', 'Milan Vojnovic']","['cs.LG', 'cs.DS', 'math.OC', 'stat.ML']",2021-05-28 08:04:06+00:00
http://arxiv.org/abs/2105.13504v2,Lattice partition recovery with dyadic CART,"We study piece-wise constant signals corrupted by additive Gaussian noise
over a $d$-dimensional lattice. Data of this form naturally arise in a host of
applications, and the tasks of signal detection or testing, de-noising and
estimation have been studied extensively in the statistical and signal
processing literature. In this paper we consider instead the problem of
partition recovery, i.e.~of estimating the partition of the lattice induced by
the constancy regions of the unknown signal, using the
computationally-efficient dyadic classification and regression tree (DCART)
methodology proposed by \citep{donoho1997cart}. We prove that, under
appropriate regularity conditions on the shape of the partition elements, a
DCART-based procedure consistently estimates the underlying partition at a rate
of order $\sigma^2 k^* \log (N)/\kappa^2$, where $k^*$ is the minimal number of
rectangular sub-graphs obtained using recursive dyadic partitions supporting
the signal partition, $\sigma^2$ is the noise variance, $\kappa$ is the minimal
magnitude of the signal difference among contiguous elements of the partition
and $N$ is the size of the lattice. Furthermore, under stronger assumptions,
our method attains a sharper estimation error of order
$\sigma^2\log(N)/\kappa^2$, independent of $k^*$, which we show to be minimax
rate optimal. Our theoretical guarantees further extend to the partition
estimator based on the optimal regression tree estimator (ORT) of
\cite{chatterjee2019adaptive} and to the one obtained through an NP-hard
exhaustive search method. We corroborate our theoretical findings and the
effectiveness of DCART for partition recovery in simulations.","['Oscar Hernan Madrid Padilla', 'Yi Yu', 'Alessandro Rinaldo']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2021-05-27 23:41:01+00:00
http://arxiv.org/abs/2105.13493v3,Efficient and Accurate Gradients for Neural SDEs,"Neural SDEs combine many of the best qualities of both RNNs and SDEs: memory
efficient training, high-capacity function approximation, and strong priors on
model space. This makes them a natural choice for modelling many types of
temporal dynamics. Training a Neural SDE (either as a VAE or as a GAN) requires
backpropagating through an SDE solve. This may be done by solving a
backwards-in-time SDE whose solution is the desired parameter gradients.
However, this has previously suffered from severe speed and accuracy issues,
due to high computational cost and numerical truncation errors. Here, we
overcome these issues through several technical innovations. First, we
introduce the \textit{reversible Heun method}. This is a new SDE solver that is
\textit{algebraically reversible}: eliminating numerical gradient errors, and
the first such solver of which we are aware. Moreover it requires half as many
function evaluations as comparable solvers, giving up to a $1.98\times$
speedup. Second, we introduce the \textit{Brownian Interval}: a new, fast,
memory efficient, and exact way of sampling \textit{and reconstructing}
Brownian motion. With this we obtain up to a $10.6\times$ speed improvement
over previous techniques, which in contrast are both approximate and relatively
slow. Third, when specifically training Neural SDEs as GANs (Kidger et al.
2021), we demonstrate how SDE-GANs may be trained through careful weight
clipping and choice of activation function. This reduces computational cost
(giving up to a $1.87\times$ speedup) and removes the numerical truncation
errors associated with gradient penalty. Altogether, we outperform the
state-of-the-art by substantial margins, with respect to training speed, and
with respect to classification, prediction, and MMD test metrics. We have
contributed implementations of all of our techniques to the torchsde library to
help facilitate their adoption.","['Patrick Kidger', 'James Foster', 'Xuechen Li', 'Terry Lyons']","['cs.LG', 'cs.AI', 'math.DS', 'stat.ML']",2021-05-27 22:59:36+00:00
http://arxiv.org/abs/2105.13483v2,"Causal, Bayesian, & Non-parametric Modeling of the SARS-CoV-2 Viral Load Distribution vs. Patient's Age","The viral load of patients infected with SARS-CoV-2 varies on logarithmic
scales and possibly with age. Controversial claims have been made in the
literature regarding whether the viral load distribution actually depends on
the age of the patients. Such a dependence would have implications for the
COVID-19 spreading mechanism, the age-dependent immune system reaction, and
thus for policymaking. We hereby develop a method to analyze viral-load
distribution data as a function of the patients' age within a flexible,
non-parametric, hierarchical, Bayesian, and causal model. The causal nature of
the developed reconstruction additionally allows to test for bias in the data.
This could be due to, e.g., bias in patient-testing and data collection or
systematic errors in the measurement of the viral load. We perform these tests
by calculating the Bayesian evidence for each implied possible causal
direction. The possibility of testing for bias in data collection and
identifying causal directions can be very useful in other contexts as well. For
this reason we make our model freely available. When applied to publicly
available age and SARS-CoV-2 viral load data, we find a statistically
significant increase in the viral load with age, but only for one of the two
analyzed datasets. If we consider this dataset, and based on the current
understanding of viral load's impact on patients' infectivity, we expect a
non-negligible difference in the infectivity of different age groups. This
difference is nonetheless too small to justify considering any age group as
noninfectious.","['Matteo Guardiani', 'Philipp Frank', 'Andrija Kostić', 'Gordian Edenhofer', 'Jakob Roth', 'Berit Uhlmann', 'Torsten Enßlin']","['stat.AP', 'stat.ME', 'stat.ML']",2021-05-27 22:35:02+00:00
http://arxiv.org/abs/2105.13440v2,Non-negative matrix factorization algorithms greatly improve topic model fits,"We report on the potential for using algorithms for non-negative matrix
factorization (NMF) to improve parameter estimation in topic models. While
several papers have studied connections between NMF and topic models, none have
suggested leveraging these connections to develop new algorithms for fitting
topic models. NMF avoids the ""sum-to-one"" constraints on the topic model
parameters, resulting in an optimization problem with simpler structure and
more efficient computations. Building on recent advances in optimization
algorithms for NMF, we show that first solving the NMF problem then recovering
the topic model fit can produce remarkably better fits, and in less time, than
standard algorithms for topic models. While we focus primarily on maximum
likelihood estimation, we show that this approach also has the potential to
improve variational inference for topic models. Our methods are implemented in
the R package fastTopics.","['Peter Carbonetto', 'Abhishek Sarkar', 'Zihao Wang', 'Matthew Stephens']","['stat.ML', 'cs.LG', 'stat.CO']",2021-05-27 20:34:46+00:00
http://arxiv.org/abs/2105.13420v1,Model Selection for Production System via Automated Online Experiments,"A challenge that machine learning practitioners in the industry face is the
task of selecting the best model to deploy in production. As a model is often
an intermediate component of a production system, online controlled experiments
such as A/B tests yield the most reliable estimation of the effectiveness of
the whole system, but can only compare two or a few models due to budget
constraints. We propose an automated online experimentation mechanism that can
efficiently perform model selection from a large pool of models with a small
number of online experiments. We derive the probability distribution of the
metric of interest that contains the model uncertainty from our Bayesian
surrogate model trained using historical logs. Our method efficiently
identifies the best model by sequentially selecting and deploying a list of
models from the candidate set that balance exploration-exploitation. Using
simulations based on real data, we demonstrate the effectiveness of our method
on two different tasks.","['Zhenwen Dai', 'Praveen Chandar', 'Ghazal Fazelnia', 'Ben Carterette', 'Mounia Lalmas-Roelleke']","['stat.ML', 'cs.AI', 'cs.LG']",2021-05-27 19:48:23+00:00
http://arxiv.org/abs/2105.13302v2,Characterizing the SLOPE Trade-off: A Variational Perspective and the Donoho-Tanner Limit,"Sorted l1 regularization has been incorporated into many methods for solving
high-dimensional statistical estimation problems, including the SLOPE estimator
in linear regression. In this paper, we study how this relatively new
regularization technique improves variable selection by characterizing the
optimal SLOPE trade-off between the false discovery proportion (FDP) and true
positive proportion (TPP) or, equivalently, between measures of type I error
and power. Assuming a regime of linear sparsity and working under Gaussian
random designs, we obtain an upper bound on the optimal trade-off for SLOPE,
showing its capability of breaking the Donoho-Tanner power limit. To put it
into perspective, this limit is the highest possible power that the Lasso,
which is perhaps the most popular l1-based method, can achieve even with
arbitrarily strong effect sizes. Next, we derive a tight lower bound that
delineates the fundamental limit of sorted l1 regularization in optimally
trading the FDP off for the TPP. Finally, we show that on any problem instance,
SLOPE with a certain regularization sequence outperforms the Lasso, in the
sense of having a smaller FDP, larger TPP and smaller l2 estimation risk
simultaneously. Our proofs are based on a novel technique that reduces a
calculus of variations problem to a class of infinite-dimensional convex
optimization problems and a very recent result from approximate message passing
theory.","['Zhiqi Bu', 'Jason Klusowski', 'Cynthia Rush', 'Weijie J. Su']","['math.ST', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT', 'stat.ML', 'stat.TH']",2021-05-27 16:56:42+00:00
http://arxiv.org/abs/2105.13283v2,Deep Ensembles from a Bayesian Perspective,"Deep ensembles can be considered as the current state-of-the-art for
uncertainty quantification in deep learning. While the approach was originally
proposed as a non-Bayesian technique, arguments supporting its Bayesian footing
have been put forward as well. We show that deep ensembles can be viewed as an
approximate Bayesian method by specifying the corresponding assumptions. Our
findings lead to an improved approximation which results in an enlarged
epistemic part of the uncertainty. Numerical examples suggest that the improved
approximation can lead to more reliable uncertainties. Analytical derivations
ensure easy calculation of results.","['Lara Hoffmann', 'Clemens Elster']","['cs.LG', 'cs.AI', 'stat.ML']",2021-05-27 16:30:52+00:00
http://arxiv.org/abs/2105.13251v1,An Impossibility Theorem for Node Embedding,"With the increasing popularity of graph-based methods for dimensionality
reduction and representation learning, node embedding functions have become
important objects of study in the literature. In this paper, we take an
axiomatic approach to understanding node embedding methods, first stating three
properties for embedding dissimilarity networks, then proving that all three
cannot be satisfied simultaneously by any node embedding method. Similar to
existing results on the impossibility of clustering under certain axiomatic
assumptions, this points to fundamental difficulties inherent to node embedding
tasks. Once these difficulties are identified, we then relax these axioms to
allow for certain node embedding methods to be admissible in our framework.","['T. Mitchell Roddenberry', 'Yu Zhu', 'Santiago Segarra']","['cs.LG', 'cs.DM', 'stat.ML']",2021-05-27 15:48:41+00:00
http://arxiv.org/abs/2105.13245v1,Bayesian Optimisation for Constrained Problems,"Many real-world optimisation problems such as hyperparameter tuning in
machine learning or simulation-based optimisation can be formulated as
expensive-to-evaluate black-box functions. A popular approach to tackle such
problems is Bayesian optimisation (BO), which builds a response surface model
based on the data collected so far, and uses the mean and uncertainty predicted
by the model to decide what information to collect next. In this paper, we
propose a novel variant of the well-known Knowledge Gradient acquisition
function that allows it to handle constraints. We empirically compare the new
algorithm with four other state-of-the-art constrained Bayesian optimisation
algorithms and demonstrate its superior performance. We also prove theoretical
convergence in the infinite budget limit.","['Juan Ungredda', 'Juergen Branke']","['cs.LG', 'stat.ML']",2021-05-27 15:43:09+00:00
http://arxiv.org/abs/2105.13099v2,On the Universality of Graph Neural Networks on Large Random Graphs,"We study the approximation power of Graph Neural Networks (GNNs) on latent
position random graphs. In the large graph limit, GNNs are known to converge to
certain ""continuous"" models known as c-GNNs, which directly enables a study of
their approximation power on random graph models. In the absence of input node
features however, just as GNNs are limited by the Weisfeiler-Lehman isomorphism
test, c-GNNs will be severely limited on simple random graph models. For
instance, they will fail to distinguish the communities of a well-separated
Stochastic Block Model (SBM) with constant degree function. Thus, we consider
recently proposed architectures that augment GNNs with unique node identifiers,
referred to as Structural GNNs here (SGNNs). We study the convergence of SGNNs
to their continuous counterpart (c-SGNNs) in the large random graph limit,
under new conditions on the node identifiers. We then show that c-SGNNs are
strictly more powerful than c-GNNs in the continuous limit, and prove their
universality on several random graph models of interest, including most SBMs
and a large class of random geometric graphs. Our results cover both
permutation-invariant and permutation-equivariant architectures.","['Nicolas Keriven', 'Alberto Bietti', 'Samuel Vaiter']","['stat.ML', 'cs.LG']",2021-05-27 12:52:36+00:00
http://arxiv.org/abs/2105.13093v1,Towards Understanding Knowledge Distillation,"Knowledge distillation, i.e., one classifier being trained on the outputs of
another classifier, is an empirically very successful technique for knowledge
transfer between classifiers. It has even been observed that classifiers learn
much faster and more reliably if trained with the outputs of another classifier
as soft labels, instead of from ground truth data. So far, however, there is no
satisfactory theoretical explanation of this phenomenon. In this work, we
provide the first insights into the working mechanisms of distillation by
studying the special case of linear and deep linear classifiers. Specifically,
we prove a generalization bound that establishes fast convergence of the
expected risk of a distillation-trained linear classifier. From the bound and
its proof we extract three key factors that determine the success of
distillation: * data geometry -- geometric properties of the data distribution,
in particular class separation, has a direct influence on the convergence speed
of the risk; * optimization bias -- gradient descent optimization finds a very
favorable minimum of the distillation objective; and * strong monotonicity --
the expected risk of the student classifier always decreases when the size of
the training set grows.","['Mary Phuong', 'Christoph H. Lampert']","['cs.LG', 'stat.ML']",2021-05-27 12:45:08+00:00
http://arxiv.org/abs/2105.13059v3,Efficient and Generalizable Tuning Strategies for Stochastic Gradient MCMC,"Stochastic gradient Markov chain Monte Carlo (SGMCMC) is a popular class of
algorithms for scalable Bayesian inference. However, these algorithms include
hyperparameters such as step size or batch size that influence the accuracy of
estimators based on the obtained posterior samples. As a result, these
hyperparameters must be tuned by the practitioner and currently no principled
and automated way to tune them exists. Standard MCMC tuning methods based on
acceptance rates cannot be used for SGMCMC, thus requiring alternative tools
and diagnostics. We propose a novel bandit-based algorithm that tunes the
SGMCMC hyperparameters by minimizing the Stein discrepancy between the true
posterior and its Monte Carlo approximation. We provide theoretical results
supporting this approach and assess various Stein-based discrepancies. We
support our results with experiments on both simulated and real datasets, and
find that this method is practical for a wide range of applications.","['Jeremie Coullon', 'Leah South', 'Christopher Nemeth']","['stat.CO', 'stat.ME', 'stat.ML']",2021-05-27 11:00:31+00:00
http://arxiv.org/abs/2105.13052v3,A generalization of the randomized singular value decomposition,"The randomized singular value decomposition (SVD) is a popular and effective
algorithm for computing a near-best rank $k$ approximation of a matrix $A$
using matrix-vector products with standard Gaussian vectors. Here, we
generalize the randomized SVD to multivariate Gaussian vectors, allowing one to
incorporate prior knowledge of $A$ into the algorithm. This enables us to
explore the continuous analogue of the randomized SVD for Hilbert--Schmidt (HS)
operators using operator-function products with functions drawn from a Gaussian
process (GP). We then construct a new covariance kernel for GPs, based on
weighted Jacobi polynomials, which allows us to rapidly sample the GP and
control the smoothness of the randomly generated functions. Numerical examples
on matrices and HS operators demonstrate the applicability of the algorithm.","['Nicolas Boullé', 'Alex Townsend']","['math.NA', 'cs.LG', 'cs.NA', 'stat.ML']",2021-05-27 10:39:37+00:00
http://arxiv.org/abs/2105.13011v2,Neural Network Training Using $\ell_1$-Regularization and Bi-fidelity Data,"With the capability of accurately representing a functional relationship
between the inputs of a physical system's model and output quantities of
interest, neural networks have become popular for surrogate modeling in
scientific applications. However, as these networks are over-parameterized,
their training often requires a large amount of data. To prevent overfitting
and improve generalization error, regularization based on, e.g., $\ell_1$- and
$\ell_2$-norms of the parameters is applied. Similarly, multiple connections of
the network may be pruned to increase sparsity in the network parameters. In
this paper, we explore the effects of sparsity promoting
$\ell_1$-regularization on training neural networks when only a small training
dataset from a high-fidelity model is available. As opposed to standard
$\ell_1$-regularization that is known to be inadequate, we consider two
variants of $\ell_1$-regularization informed by the parameters of an identical
network trained using data from lower-fidelity models of the problem at hand.
These bi-fidelity strategies are generalizations of transfer learning of neural
networks that uses the parameters learned from a large low-fidelity dataset to
efficiently train networks for a small high-fidelity dataset. We also compare
the bi-fidelity strategies with two $\ell_1$-regularization methods that only
use the high-fidelity dataset. Three numerical examples for propagating
uncertainty through physical systems are used to show that the proposed
bi-fidelity $\ell_1$-regularization strategies produce errors that are one
order of magnitude smaller than those of networks trained only using datasets
from the high-fidelity models.","['Subhayan De', 'Alireza Doostan']","['stat.ML', 'cs.LG']",2021-05-27 08:56:17+00:00
http://arxiv.org/abs/2105.13010v5,An error analysis of generative adversarial networks for learning distributions,"This paper studies how well generative adversarial networks (GANs) learn
probability distributions from finite samples. Our main results establish the
convergence rates of GANs under a collection of integral probability metrics
defined through H\""older classes, including the Wasserstein distance as a
special case. We also show that GANs are able to adaptively learn data
distributions with low-dimensional structures or have H\""older densities, when
the network architectures are chosen properly. In particular, for distributions
concentrated around a low-dimensional set, we show that the learning rates of
GANs do not depend on the high ambient dimension, but on the lower intrinsic
dimension. Our analysis is based on a new oracle inequality decomposing the
estimation error into the generator and discriminator approximation error and
the statistical error, which may be of independent interest.","['Jian Huang', 'Yuling Jiao', 'Zhen Li', 'Shiao Liu', 'Yang Wang', 'Yunfei Yang']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2021-05-27 08:55:19+00:00
http://arxiv.org/abs/2105.12978v2,A Non-asymptotic Approach to Best-Arm Identification for Gaussian Bandits,"We propose a new strategy for best-arm identification with fixed confidence
of Gaussian variables with bounded means and unit variance. This strategy,
called Exploration-Biased Sampling, is not only asymptotically optimal: it is
to the best of our knowledge the first strategy with non-asymptotic bounds that
asymptotically matches the sample complexity.But the main advantage over other
algorithms like Track-and-Stop is an improved behavior regarding exploration:
Exploration-Biased Sampling is biased towards exploration in a subtle but
natural way that makes it more stable and interpretable. These improvements are
allowed by a new analysis of the sample complexity optimization problem, which
yields a faster numerical resolution scheme and several quantitative regularity
results that we believe of high independent interest.","['Antoine Barrier', 'Aurélien Garivier', 'Tomáš Kocák']","['math.ST', 'stat.ML', 'stat.TH']",2021-05-27 07:42:49+00:00
http://arxiv.org/abs/2105.12941v3,CrystalCandle: A User-Facing Model Explainer for Narrative Explanations,"Predictive machine learning models often lack interpretability, resulting in
low trust from model end users despite having high predictive performance.
While many model interpretation approaches return top important features to
help interpret model predictions, these top features may not be well-organized
or intuitive to end users, which limits model adoption rates. In this paper, we
propose CrystalCandle, a user-facing model explainer that creates
user-digestible interpretations and insights reflecting the rationale behind
model predictions. CrystalCandle builds an end-to-end pipeline from machine
learning platforms to end user platforms, and provides users with an interface
for implementing model interpretation approaches and for customizing narrative
insights. CrystalCandle is a platform consisting of four components: Model
Importer, Model Interpreter, Narrative Generator, and Narrative Exporter. We
describe these components, and then demonstrate the effectiveness of
CrystalCandle through use cases at LinkedIn. Quantitative performance analyses
indicate that CrystalCandle's narrative insights lead to lifts in adoption
rates of predictive model recommendations, as well as to increases in
downstream key metrics such as revenue when compared to previous approaches,
while qualitative analyses indicate positive feedback from end users.","['Jilei Yang', 'Diana Negoescu', 'Parvez Ahammad']","['stat.ML', 'cs.LG']",2021-05-27 05:11:47+00:00
http://arxiv.org/abs/2105.12937v2,Towards a Better Understanding of Linear Models for Recommendation,"Recently, linear regression models, such as EASE and SLIM, have shown to
often produce rather competitive results against more sophisticated deep
learning models. On the other side, the (weighted) matrix factorization
approaches have been popular choices for recommendation in the past and widely
adopted in the industry. In this work, we aim to theoretically understand the
relationship between these two approaches, which are the cornerstones of
model-based recommendations. Through the derivation and analysis of the
closed-form solutions for two basic regression and matrix factorization
approaches, we found these two approaches are indeed inherently related but
also diverge in how they ""scale-down"" the singular values of the original
user-item interaction matrix. This analysis also helps resolve the questions
related to the regularization parameter range and model complexities. We
further introduce a new learning algorithm in searching (hyper)parameters for
the closed-form solution and utilize it to discover the nearby models of the
existing solutions. The experimental results demonstrate that the basic models
and their closed-form solutions are indeed quite competitive against the
state-of-the-art models, thus, confirming the validity of studying the basic
models. The effectiveness of exploring the nearby models are also
experimentally validated.","['Ruoming Jin', 'Dong Li', 'Jing Gao', 'Zhi Liu', 'Li Chen', 'Yang Zhou']","['cs.IR', 'cs.LG', 'stat.ML']",2021-05-27 04:17:04+00:00
http://arxiv.org/abs/2105.12916v1,Robust learning from corrupted EEG with dynamic spatial filtering,"Building machine learning models using EEG recorded outside of the laboratory
setting requires methods robust to noisy data and randomly missing channels.
This need is particularly great when working with sparse EEG montages (1-6
channels), often encountered in consumer-grade or mobile EEG devices. Neither
classical machine learning models nor deep neural networks trained end-to-end
on EEG are typically designed or tested for robustness to corruption, and
especially to randomly missing channels. While some studies have proposed
strategies for using data with missing channels, these approaches are not
practical when sparse montages are used and computing power is limited (e.g.,
wearables, cell phones). To tackle this problem, we propose dynamic spatial
filtering (DSF), a multi-head attention module that can be plugged in before
the first layer of a neural network to handle missing EEG channels by learning
to focus on good channels and to ignore bad ones. We tested DSF on public EEG
data encompassing ~4,000 recordings with simulated channel corruption and on a
private dataset of ~100 at-home recordings of mobile EEG with natural
corruption. Our proposed approach achieves the same performance as baseline
models when no noise is applied, but outperforms baselines by as much as 29.4%
accuracy when significant channel corruption is present. Moreover, DSF outputs
are interpretable, making it possible to monitor channel importance in
real-time. This approach has the potential to enable the analysis of EEG in
challenging settings where channel corruption hampers the reading of brain
signals.","['Hubert Banville', 'Sean U. N. Wood', 'Chris Aimone', 'Denis-Alexander Engemann', 'Alexandre Gramfort']","['cs.LG', 'eess.SP', 'q-bio.NC', 'q-bio.QM', 'stat.ML']",2021-05-27 02:33:16+00:00
http://arxiv.org/abs/2105.12909v3,Deconditional Downscaling with Gaussian Processes,"Refining low-resolution (LR) spatial fields with high-resolution (HR)
information, often known as statistical downscaling, is challenging as the
diversity of spatial datasets often prevents direct matching of observations.
Yet, when LR samples are modeled as aggregate conditional means of HR samples
with respect to a mediating variable that is globally observed, the recovery of
the underlying fine-grained field can be framed as taking an ""inverse"" of the
conditional expectation, namely a deconditioning problem. In this work, we
propose a Bayesian formulation of deconditioning which naturally recovers the
initial reproducing kernel Hilbert space formulation from Hsu and Ramos (2019).
We extend deconditioning to a downscaling setup and devise efficient
conditional mean embedding estimator for multiresolution data. By treating
conditional expectations as inter-domain features of the underlying field, a
posterior for the latent field can be established as a solution to the
deconditioning problem. Furthermore, we show that this solution can be viewed
as a two-staged vector-valued kernel ridge regressor and show that it has a
minimax optimal convergence rate under mild assumptions. Lastly, we demonstrate
its proficiency in a synthetic and a real-world atmospheric field downscaling
problem, showing substantial improvements over existing methods.","['Siu Lun Chau', 'Shahine Bouabid', 'Dino Sejdinovic']","['cs.LG', 'stat.ML']",2021-05-27 02:10:22+00:00
http://arxiv.org/abs/2105.12898v1,Stochastic Intervention for Causal Effect Estimation,"Causal inference methods are widely applied in various decision-making
domains such as precision medicine, optimal policy and economics. Central to
these applications is the treatment effect estimation of intervention
strategies. Current estimation methods are mostly restricted to the
deterministic treatment, which however, is unable to address the stochastic
space treatment policies. Moreover, previous methods can only make binary
yes-or-no decisions based on the treatment effect, lacking the capability of
providing fine-grained effect estimation degree to explain the process of
decision making. In our study, we therefore advance the causal inference
research to estimate stochastic intervention effect by devising a new
stochastic propensity score and stochastic intervention effect estimator (SIE).
Meanwhile, we design a customized genetic algorithm specific to stochastic
intervention effect (Ge-SIO) with the aim of providing causal evidence for
decision making. We provide the theoretical analysis and conduct an empirical
study to justify that our proposed measures and algorithms can achieve a
significant performance lift in comparison with state-of-the-art baselines.","['Tri Dung Duong', 'Qian Li', 'Guandong Xu']","['cs.AI', 'cs.LG', 'stat.ML']",2021-05-27 01:12:03+00:00
http://arxiv.org/abs/2105.12894v3,MAGI-X: Manifold-Constrained Gaussian Process Inference for Unknown System Dynamics,"Ordinary differential equations (ODEs), commonly used to characterize the
dynamic systems, are difficult to propose in closed-form for many complicated
scientific applications, even with the help of domain expert. We propose a fast
and accurate data-driven method, MAGI-X, to learn the unknown dynamic from the
observation data in a non-parametric fashion, without the need of any domain
knowledge. Unlike the existing methods that mainly rely on the costly numerical
integration, MAGI-X utilizes the powerful functional approximator of neural
network to learn the unknown nonlinear dynamic within the MAnifold-constrained
Gaussian process Inference (MAGI) framework that completely circumvents the
numerical integration. Comparing against the state-of-the-art methods on three
realistic examples, MAGI-X achieves competitive accuracy in both fitting and
forecasting while only taking a fraction of computational time. Moreover,
MAGI-X provides practical solution for the inference of partial observed
systems, which no previous method is able to handle.","['Chaofan Huang', 'Simin Ma', 'Shihao Yang']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2021-05-27 01:01:40+00:00
http://arxiv.org/abs/2105.12866v2,Augmented KRnet for density estimation and approximation,"In this work, we have proposed augmented KRnets including both discrete and
continuous models. One difficulty in flow-based generative modeling is to
maintain the invertibility of the transport map, which is often a trade-off
between effectiveness and robustness. The exact invertibility has been achieved
in the real NVP using a specific pattern to exchange information between two
separated groups of dimensions. KRnet has been developed to enhance the
information exchange among data dimensions by incorporating the
Knothe-Rosenblatt rearrangement into the structure of the transport map. Due to
the maintenance of exact invertibility, a full nonlinear update of all data
dimensions needs three iterations in KRnet. To alleviate this issue, we will
add augmented dimensions that act as a channel for communications among the
data dimensions. In the augmented KRnet, a fully nonlinear update is achieved
in two iterations. We also show that the augmented KRnet can be reformulated as
the discretization of a neural ODE, where the exact invertibility is kept such
that the adjoint method can be formulated with respect to the discretized ODE
to obtain the exact gradient. Numerical experiments have been implemented to
demonstrate the effectiveness of our models.","['Xiaoliang Wan', 'Kejun Tang']","['stat.ML', 'cs.LG']",2021-05-26 22:20:16+00:00
http://arxiv.org/abs/2105.12837v3,Fooling Partial Dependence via Data Poisoning,"Many methods have been developed to understand complex predictive models and
high expectations are placed on post-hoc model explainability. It turns out
that such explanations are not robust nor trustworthy, and they can be fooled.
This paper presents techniques for attacking Partial Dependence (plots,
profiles, PDP), which are among the most popular methods of explaining any
predictive model trained on tabular data. We showcase that PD can be
manipulated in an adversarial manner, which is alarming, especially in
financial or medical applications where auditability became a must-have trait
supporting black-box machine learning. The fooling is performed via poisoning
the data to bend and shift explanations in the desired direction using genetic
and gradient algorithms. We believe this to be the first work using a genetic
algorithm for manipulating explanations, which is transferable as it
generalizes both ways: in a model-agnostic and an explanation-agnostic manner.","['Hubert Baniecki', 'Wojciech Kretowicz', 'Przemyslaw Biecek']","['cs.LG', 'stat.ML']",2021-05-26 20:58:04+00:00
http://arxiv.org/abs/2105.12806v4,A Universal Law of Robustness via Isoperimetry,"Classically, data interpolation with a parametrized model class is possible
as long as the number of parameters is larger than the number of equations to
be satisfied. A puzzling phenomenon in deep learning is that models are trained
with many more parameters than what this classical theory would suggest. We
propose a partial theoretical explanation for this phenomenon. We prove that
for a broad class of data distributions and model classes, overparametrization
is necessary if one wants to interpolate the data smoothly. Namely we show that
smooth interpolation requires $d$ times more parameters than mere
interpolation, where $d$ is the ambient data dimension. We prove this universal
law of robustness for any smoothly parametrized function class with polynomial
size weights, and any covariate distribution verifying isoperimetry. In the
case of two-layers neural networks and Gaussian covariates, this law was
conjectured in prior work by Bubeck, Li and Nagaraj. We also give an
interpretation of our result as an improved generalization bound for model
classes consisting of smooth functions.","['Sébastien Bubeck', 'Mark Sellke']","['cs.LG', 'stat.ML']",2021-05-26 19:49:47+00:00
http://arxiv.org/abs/2105.12778v1,Statistical Depth Meets Machine Learning: Kernel Mean Embeddings and Depth in Functional Data Analysis,"Statistical depth is the act of gauging how representative a point is
compared to a reference probability measure. The depth allows introducing
rankings and orderings to data living in multivariate, or function spaces.
Though widely applied and with much experimental success, little theoretical
progress has been made in analysing functional depths. This article highlights
how the common $h$-depth and related statistical depths for functional data can
be viewed as a kernel mean embedding, a technique used widely in statistical
machine learning. This connection facilitates answers to open questions
regarding statistical properties of functional depths, as well as it provides a
link between the depth and empirical characteristic function based procedures
for functional data.","['George Wynne', 'Stanislav Nagy']","['math.ST', 'stat.ML', 'stat.TH']",2021-05-26 18:22:33+00:00
http://arxiv.org/abs/2105.12769v4,Clustered Federated Learning via Generalized Total Variation Minimization,"We study optimization methods to train local (or personalized) models for
decentralized collections of local datasets with an intrinsic network
structure. This network structure arises from domain-specific notions of
similarity between local datasets. Examples for such notions include
spatio-temporal proximity, statistical dependencies or functional relations.
Our main conceptual contribution is to formulate federated learning as
generalized total variation (GTV) minimization. This formulation unifies and
considerably extends existing federated learning methods. It is highly flexible
and can be combined with a broad range of parametric models, including
generalized linear models or deep neural networks. Our main algorithmic
contribution is a fully decentralized federated learning algorithm. This
algorithm is obtained by applying an established primal-dual method to solve
GTV minimization. It can be implemented as message passing and is robust
against inexact computations that arise from limited computational resources
including processing time or bandwidth. Our main analytic contribution is an
upper bound on the deviation between the local model parameters learnt by our
algorithm and an oracle-based clustered federated learning method. This upper
bound reveals conditions on the local models and the network structure of local
datasets such that GTV minimization is able to pool (nearly) homogeneous local
datasets.","['Yasmin SarcheshmehPour', 'Yu Tian', 'Linli Zhang', 'Alexander Jung']","['cs.LG', 'cs.DC', 'stat.ML', 'I.5.1; I.5.2; I.5.3; I.5.4; E.1']",2021-05-26 18:07:19+00:00
http://arxiv.org/abs/2105.12639v4,"Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness","Neural network ensembles, such as Bayesian neural networks (BNNs), have shown
success in the areas of uncertainty estimation and robustness. However, a
crucial challenge prohibits their use in practice. BNNs require a large number
of predictions to produce reliable results, leading to a significant increase
in computational cost. To alleviate this issue, we propose spatial smoothing, a
method that spatially ensembles neighboring feature map points of convolutional
neural networks. By simply adding a few blur layers to the models, we
empirically show that spatial smoothing improves accuracy, uncertainty
estimation, and robustness of BNNs across a whole range of ensemble sizes. In
particular, BNNs incorporating spatial smoothing achieve high predictive
performance merely with a handful of ensembles. Moreover, this method also can
be applied to canonical deterministic neural networks to improve the
performances. A number of evidences suggest that the improvements can be
attributed to the stabilized feature maps and the smoothing of the loss
landscape. In addition, we provide a fundamental explanation for prior works -
namely, global average pooling, pre-activation, and ReLU6 - by addressing them
as special cases of spatial smoothing. These not only enhance accuracy, but
also improve uncertainty estimation and robustness by making the loss landscape
smoother in the same manner as spatial smoothing. The code is available at
https://github.com/xxxnell/spatial-smoothing.","['Namuk Park', 'Songkuk Kim']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2021-05-26 15:58:11+00:00
http://arxiv.org/abs/2105.13189v2,Sparse recovery based on the generalized error function,"In this paper, we propose a novel sparse recovery method based on the
generalized error function. The penalty function introduced involves both the
shape and the scale parameters, making it very flexible. The theoretical
analysis results in terms of the null space property, the spherical section
property and the restricted invertibility factor are established for both
constrained and unconstrained models. The practical algorithms via both the
iteratively reweighted $\ell_1$ and the difference of convex functions
algorithms are presented. Numerical experiments are conducted to illustrate the
improvement provided by the proposed approach in various scenarios. Its
practical application in magnetic resonance imaging (MRI) reconstruction is
studied as well.",['Zhiyong Zhou'],"['math.NA', 'cs.LG', 'cs.NA', 'eess.IV', 'stat.ML']",2021-05-26 11:36:01+00:00
http://arxiv.org/abs/2105.12478v1,"The ""given data"" paradigm undermines both cultures","Breiman organizes ""Statistical modeling: The two cultures"" around a simple
visual. Data, to the far right, are compelled into a ""black box"" with an arrow
and then catapulted left by a second arrow, having been transformed into an
output. Breiman then posits two interpretations of this visual as encapsulating
a distinction between two cultures in statistics. The divide, he argues is
about what happens in the ""black box."" In this comment, I argue for a broader
perspective on statistics and, in doing so, elevate questions from ""before"" and
""after"" the box as fruitful areas for statistical innovation and practice.",['Tyler McCormick'],"['stat.ML', 'cs.LG']",2021-05-26 11:22:06+00:00
http://arxiv.org/abs/2105.12356v2,The Graph Cut Kernel for Ranked Data,"Many algorithms for ranked data become computationally intractable as the
number of objects grows due to the complex geometric structure induced by
rankings. An additional challenge is posed by partial rankings, i.e. rankings
in which the preference is only known for a subset of all objects. For these
reasons, state-of-the-art methods cannot scale to real-world applications, such
as recommender systems. We address this challenge by exploiting the geometric
structure of ranked data and additional available information about the objects
to derive a kernel for ranking based on the graph cut function. The graph cut
kernel combines the efficiency of submodular optimization with the theoretical
properties of kernel-based methods. The graph cut kernel combines the
efficiency of submodular optimization with the theoretical properties of
kernel-based methods.","['Michelangelo Conserva', 'Marc Peter Deisenroth', 'K S Sesh Kumar']","['cs.LG', 'stat.ML']",2021-05-26 06:42:06+00:00
http://arxiv.org/abs/2105.12342v3,A data-driven approach to beating SAA out-of-sample,"While solutions of Distributionally Robust Optimization (DRO) problems can
sometimes have a higher out-of-sample expected reward than the Sample Average
Approximation (SAA), there is no guarantee. In this paper, we introduce a class
of Distributionally Optimistic Optimization (DOO) models, and show that it is
always possible to ``beat"" SAA out-of-sample if we consider not just worst-case
(DRO) models but also best-case (DOO) ones. We also show, however, that this
comes at a cost: Optimistic solutions are more sensitive to model error than
either worst-case or SAA optimizers, and hence are less robust and calibrating
the worst- or best-case model to outperform SAA may be difficult when data is
limited.","['Jun-ya Gotoh', 'Michael Jong Kim', 'Andrew E. B. Lim']","['math.OC', 'cs.LG', 'cs.SY', 'econ.EM', 'eess.SY', 'stat.ML', '90C17, 90C31, 93B35, 90C47, 90B50, 62G35, 62K25,']",2021-05-26 06:10:12+00:00
http://arxiv.org/abs/2105.12290v1,Block Dense Weighted Networks with Augmented Degree Correction,"Dense networks with weighted connections often exhibit a community like
structure, where although most nodes are connected to each other, different
patterns of edge weights may emerge depending on each node's community
membership. We propose a new framework for generating and estimating dense
weighted networks with potentially different connectivity patterns across
different communities. The proposed model relies on a particular class of
functions which map individual node characteristics to the edges connecting
those nodes, allowing for flexibility while requiring a small number of
parameters relative to the number of edges. By leveraging the estimation
techniques, we also develop a bootstrap methodology for generating new networks
on the same set of vertices, which may be useful in circumstances where
multiple data sets cannot be collected. Performance of these methods are
analyzed in theory, simulations, and real data.","['Benjamin Leinwand', 'Vladas Pipiras']","['stat.ML', 'cs.LG']",2021-05-26 01:25:07+00:00
http://arxiv.org/abs/2105.12286v1,An algorithm-based multiple detection influence measure for high dimensional regression using expectile,"The identification of influential observations is an important part of data
analysis that can prevent erroneous conclusions drawn from biased estimators.
However, in high dimensional data, this identification is challenging.
Classical and recently-developed methods often perform poorly when there are
multiple influential observations in the same dataset. In particular, current
methods can fail when there is masking several influential observations with
similar characteristics, or swamping when the influential observations are near
the boundary of the space spanned by well-behaved observations. Therefore, we
propose an algorithm-based, multi-step, multiple detection procedure to
identify influential observations that addresses current limitations. Our
three-step algorithm to identify and capture undesirable variability in the
data, $\asymMIP,$ is based on two complementary statistics, inspired by
asymmetric correlations, and built on expectiles. Simulations demonstrate
higher detection power than competing methods. Use of the resulting asymptotic
distribution leads to detection of influential observations without the need
for computationally demanding procedures such as the bootstrap. The application
of our method to the Autism Brain Imaging Data Exchange neuroimaging dataset
resulted in a more balanced and accurate prediction of brain maturity based on
cortical thickness. See our GitHub for a free R package that implements our
algorithm: \texttt{asymMIP} (\url{github.com/AmBarry/hidetify}).","['Amadou Barry', 'Nikhil Bhagwat', 'Bratislav Misic', 'Jean-Baptiste Poline', 'Celia M. T. Greenwood']","['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']",2021-05-26 01:16:24+00:00
http://arxiv.org/abs/2105.12271v1,SG-PALM: a Fast Physically Interpretable Tensor Graphical Model,"We propose a new graphical model inference procedure, called SG-PALM, for
learning conditional dependency structure of high-dimensional tensor-variate
data. Unlike most other tensor graphical models the proposed model is
interpretable and computationally scalable to high dimension. Physical
interpretability follows from the Sylvester generative (SG) model on which
SG-PALM is based: the model is exact for any observation process that is a
solution of a partial differential equation of Poisson type. Scalability
follows from the fast proximal alternating linearized minimization (PALM)
procedure that SG-PALM uses during training. We establish that SG-PALM
converges linearly (i.e., geometric convergence rate) to a global optimum of
its objective function. We demonstrate the scalability and accuracy of SG-PALM
for an important but challenging climate prediction problem: spatio-temporal
forecasting of solar flares from multimodal imaging data.","['Yu Wang', 'Alfred Hero']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.CO']",2021-05-26 00:24:25+00:00
http://arxiv.org/abs/2105.12257v1,Rank-one matrix estimation: analytic time evolution of gradient descent dynamics,"We consider a rank-one symmetric matrix corrupted by additive noise. The
rank-one matrix is formed by an $n$-component unknown vector on the sphere of
radius $\sqrt{n}$, and we consider the problem of estimating this vector from
the corrupted matrix in the high dimensional limit of $n$ large, by gradient
descent for a quadratic cost function on the sphere. Explicit formulas for the
whole time evolution of the overlap between the estimator and unknown vector,
as well as the cost, are rigorously derived. In the long time limit we recover
the well known spectral phase transition, as a function of the signal-to-noise
ratio. The explicit formulas also allow to point out interesting transient
features of the time evolution. Our analysis technique is based on recent
progress in random matrix theory and uses local versions of the semi-circle
law.","['Antoine Bodin', 'Nicolas Macris']","['stat.ML', 'cs.LG', 'math.PR']",2021-05-25 23:31:08+00:00
http://arxiv.org/abs/2105.12247v4,GraphVICRegHSIC: Towards improved self-supervised representation learning for graphs with a hyrbid loss function,"Self-supervised learning and pre-training strategieshave developed over the
last few years especiallyfor Convolutional Neural Networks (CNNs). Re-cently
application of such methods can also be no-ticed for Graph Neural Networks
(GNNs) . In thispaper, we have used a graph based self-supervisedlearning
strategy with different loss functions (Bar-low Twins[Zbontaret al., 2021],
HSIC[Tsaiet al.,2021], VICReg[Bardeset al., 2021]) which haveshown promising
results when applied with CNNspreviously. We have also proposed a hybrid
lossfunction combining the advantages of VICReg andHSIC and called it as
VICRegHSIC. The perfor-mance of these aforementioned methods have beencompared
when applied to 7 different datasets suchas MUTAG, PROTEINS, IMDB-Binary, etc.
Ex-periments showed that our hybrid loss function per-formed better than the
remaining ones in 4 out of7 cases. Moreover, the impact of different
batchsizes, projector dimensions and data augmentationstrategies have also been
explored.",['Sayan Nag'],"['cs.LG', 'cs.AI', 'cs.CG', 'cs.CV', 'stat.ML']",2021-05-25 22:34:19+00:00
http://arxiv.org/abs/2105.12245v2,Scaling Properties of Deep Residual Networks,"Residual networks (ResNets) have displayed impressive results in pattern
recognition and, recently, have garnered considerable theoretical interest due
to a perceived link with neural ordinary differential equations (neural ODEs).
This link relies on the convergence of network weights to a smooth function as
the number of layers increases. We investigate the properties of weights
trained by stochastic gradient descent and their scaling with network depth
through detailed numerical experiments. We observe the existence of scaling
regimes markedly different from those assumed in neural ODE literature.
Depending on certain features of the network architecture, such as the
smoothness of the activation function, one may obtain an alternative ODE limit,
a stochastic differential equation or neither of these. These findings cast
doubts on the validity of the neural ODE model as an adequate asymptotic
description of deep ResNets and point to an alternative class of differential
equations as a better description of the deep network limit.","['Alain-Sam Cohen', 'Rama Cont', 'Alain Rossier', 'Renyuan Xu']","['cs.LG', 'cs.NA', 'cs.NE', 'math.NA', 'stat.ML']",2021-05-25 22:31:30+00:00
http://arxiv.org/abs/2105.12237v1,Practical Convex Formulation of Robust One-hidden-layer Neural Network Training,"Recent work has shown that the training of a one-hidden-layer, scalar-output
fully-connected ReLU neural network can be reformulated as a finite-dimensional
convex program. Unfortunately, the scale of such a convex program grows
exponentially in data size. In this work, we prove that a stochastic procedure
with a linear complexity well approximates the exact formulation. Moreover, we
derive a convex optimization approach to efficiently solve the ""adversarial
training"" problem, which trains neural networks that are robust to adversarial
input perturbations. Our method can be applied to binary classification and
regression, and provides an alternative to the current adversarial training
methods, such as Fast Gradient Sign Method (FGSM) and Projected Gradient
Descent (PGD). We demonstrate in experiments that the proposed method achieves
a noticeably better adversarial robustness and performance than the existing
methods.","['Yatong Bai', 'Tanmay Gautam', 'Yu Gai', 'Somayeh Sojoudi']","['cs.LG', 'cs.CC', 'stat.ML']",2021-05-25 22:06:27+00:00
http://arxiv.org/abs/2105.12152v3,Density estimation on low-dimensional manifolds: an inflation-deflation approach,"Normalizing Flows (NFs) are universal density estimators based on Neural
Networks. However, this universality is limited: the density's support needs to
be diffeomorphic to a Euclidean space. In this paper, we propose a novel method
to overcome this limitation without sacrificing universality. The proposed
method inflates the data manifold by adding noise in the normal space, trains
an NF on this inflated manifold, and, finally, deflates the learned density.
Our main result provides sufficient conditions on the manifold and the specific
choice of noise under which the corresponding estimator is exact. Our method
has the same computational complexity as NFs and does not require computing an
inverse flow. We also show that, if the embedding dimension is much larger than
the manifold dimension, noise in the normal space can be well approximated by
Gaussian noise. This allows using our method for approximating arbitrary
densities on unknown manifolds provided that the manifold dimension is known.","['Christian Horvat', 'Jean-Pascal Pfister']","['cs.LG', 'stat.ML']",2021-05-25 18:08:09+00:00
http://arxiv.org/abs/2105.12092v2,Trajectory Modeling via Random Utility Inverse Reinforcement Learning,"We consider the problem of modeling trajectories of drivers in a road network
from the perspective of inverse reinforcement learning. Cars are detected by
sensors placed on sparsely distributed points on the street network of a city.
As rational agents, drivers are trying to maximize some reward function unknown
to an external observer. We apply the concept of random utility from
econometrics to model the unknown reward function as a function of observed and
unobserved features. In contrast to current inverse reinforcement learning
approaches, we do not assume that agents act according to a stochastic policy;
rather, we assume that agents act according to a deterministic optimal policy
and show that randomness in data arises because the exact rewards are not fully
observed by an external observer. We introduce the concept of extended state to
cope with unobserved features and develop a Markov decision process formulation
of drivers decisions. We present theoretical results which guarantee the
existence of solutions and show that maximum entropy inverse reinforcement
learning is a particular case of our approach. Finally, we illustrate Bayesian
inference on model parameters through a case study with real trajectory data
from a large city in Brazil.","['Anselmo R. Pitombeira-Neto', 'Helano P. Santos', 'Ticiana L. Coelho da Silva', 'José Antonio F. de Macedo']","['cs.AI', 'cs.LG', 'stat.ML']",2021-05-25 17:19:09+00:00
http://arxiv.org/abs/2105.12089v1,Investigating Manifold Neighborhood size for Nonlinear Analysis of LIBS Amino Acid Spectra,"Classification and identification of amino acids in aqueous solutions is
important in the study of biomacromolecules. Laser Induced Breakdown
Spectroscopy (LIBS) uses high energy laser-pulses for ablation of chemical
compounds whose radiated spectra are captured and recorded to reveal molecular
structure. Spectral peaks and noise from LIBS are impacted by experimental
protocols. Current methods for LIBS spectral analysis achieves promising
results using PCA, a linear method. It is well-known that the underlying
physical processes behind LIBS are highly nonlinear. Our work set out to
understand the impact of LIBS spectra on suitable neighborhood size over which
to consider pattern phenomena, if nonlinear methods capture pattern phenomena
with increased efficacy, and how they improve classification and identification
of compounds. We analyzed four amino acids, polysaccharide, and a control
group, water. We developed an information theoretic method for measurement of
LIBS energy spectra, implemented manifold methods for nonlinear dimensionality
reduction, and found while clustering results were not statistically
significantly different, nonlinear methods lead to increased classification
accuracy. Moreover, our approach uncovered the contribution of micro-wells
(experimental protocol) in LIBS spectra. To the best of our knowledge, ours is
the first application of Manifold methods to LIBS amino-acid analysis in the
research literature.","['Piyush K. Sharma', 'Gary Holness', 'Poopalasingam Sivakumar', 'Yuri Markushin', 'Noureddine Melikechi']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2021-05-25 17:17:00+00:00
http://arxiv.org/abs/2105.12081v3,Group selection and shrinkage: Structured sparsity for semiparametric additive models,"Sparse regression and classification estimators that respect group structures
have application to an assortment of statistical and machine learning problems,
from multitask learning to sparse additive modeling to hierarchical selection.
This work introduces structured sparse estimators that combine group subset
selection with shrinkage. To accommodate sophisticated structures, our
estimators allow for arbitrary overlap between groups. We develop an
optimization framework for fitting the nonconvex regularization surface and
present finite-sample error bounds for estimation of the regression function.
As an application requiring structure, we study sparse semiparametric additive
modeling, a procedure that allows the effect of each predictor to be zero,
linear, or nonlinear. For this task, the new estimators improve across several
metrics on synthetic data compared to alternatives. Finally, we demonstrate
their efficacy in modeling supermarket foot traffic and economic recessions
using many predictors. These demonstrations suggest sparse semiparametric
additive models, fit using the new estimators, are an excellent compromise
between fully linear and fully nonparametric alternatives. All of our
algorithms are made available in the scalable implementation grpsel.","['Ryan Thompson', 'Farshid Vahid']","['stat.ME', 'stat.ML']",2021-05-25 17:00:25+00:00
http://arxiv.org/abs/2105.12062v2,Practical Schemes for Finding Near-Stationary Points of Convex Finite-Sums,"In convex optimization, the problem of finding near-stationary points has not
been adequately studied yet, unlike other optimality measures such as the
function value. Even in the deterministic case, the optimal method (OGM-G, due
to Kim and Fessler (2021)) has just been discovered recently. In this work, we
conduct a systematic study of algorithmic techniques for finding
near-stationary points of convex finite-sums. Our main contributions are
several algorithmic discoveries: (1) we discover a memory-saving variant of
OGM-G based on the performance estimation problem approach (Drori and Teboulle,
2014); (2) we design a new accelerated SVRG variant that can simultaneously
achieve fast rates for minimizing both the gradient norm and function value;
(3) we propose an adaptively regularized accelerated SVRG variant, which does
not require the knowledge of some unknown initial constants and achieves
near-optimal complexities. We put an emphasis on the simplicity and
practicality of the new schemes, which could facilitate future work.","['Kaiwen Zhou', 'Lai Tian', 'Anthony Man-Cho So', 'James Cheng']","['math.OC', 'cs.LG', 'stat.ML']",2021-05-25 16:46:35+00:00
http://arxiv.org/abs/2105.12033v3,TNet: A Model-Constrained Tikhonov Network Approach for Inverse Problems,"Deep Learning (DL), in particular deep neural networks (DNN), by default is
purely data-driven and in general does not require physics. This is the
strength of DL but also one of its key limitations when applied to science and
engineering problems in which underlying physical properties and desired
accuracy need to be achieved. DL methods in their original forms are not
capable of respecting the underlying mathematical models or achieving desired
accuracy even in big-data regimes. However, many data-driven science and
engineering problems, such as inverse problems, typically have limited
experimental or observational data, and DL would overfit the data in this case.
Leveraging information encoded in the underlying mathematical models, we argue,
not only compensates missing information in low data regimes but also provides
opportunities to equip DL methods with the underlying physics, hence promoting
better generalization. This paper develops a model-constrained deep learning
approach and its variant TNet that are capable of learning information hidden
in both the training data and the underlying mathematical models to solve
inverse problems governed by partial differential equations. We provide the
constructions and some theoretical results for the proposed approaches. We show
that data randomization can enhance the smoothness of the networks and their
generalizations. Comprehensive numerical results not only confirm the
theoretical findings but also show that with even as little as 20 training data
samples for 1D deconvolution, 50 for inverse 2D heat conductivity problem, 100
and 50 for inverse initial conditions for time-dependent 2D Burgers' equation
and 2D Navier-Stokes equations, respectively. TNet solutions can be as accurate
as Tikhonov solutions while being several orders of magnitude faster. This is
possible owing to the model-constrained term, replications, and randomization.","['Hai V. Nguyen', 'Tan Bui-Thanh']","['stat.ML', 'cs.LG', 'math.OC']",2021-05-25 16:12:39+00:00
http://arxiv.org/abs/2105.12022v1,Principal Component Hierarchy for Sparse Quadratic Programs,"We propose a novel approximation hierarchy for cardinality-constrained,
convex quadratic programs that exploits the rank-dominating eigenvectors of the
quadratic matrix. Each level of approximation admits a min-max characterization
whose objective function can be optimized over the binary variables
analytically, while preserving convexity in the continuous variables.
Exploiting this property, we propose two scalable optimization algorithms,
coined as the ""best response"" and the ""dual program"", that can efficiently
screen the potential indices of the nonzero elements of the original program.
We show that the proposed methods are competitive with the existing screening
methods in the current sparse regression literature, and it is particularly
fast on instances with high number of measurements in experiments with both
synthetic and real datasets.","['Robbie Vreugdenhil', 'Viet Anh Nguyen', 'Armin Eftekhari', 'Peyman Mohajerin Esfahani']","['math.OC', 'cs.LG', 'stat.ML']",2021-05-25 15:45:16+00:00
http://arxiv.org/abs/2105.12005v1,Hierarchical Subspace Learning for Dimensionality Reduction to Improve Classification Accuracy in Large Data Sets,"Manifold learning is used for dimensionality reduction, with the goal of
finding a projection subspace to increase and decrease the inter- and
intraclass variances, respectively. However, a bottleneck for subspace learning
methods often arises from the high dimensionality of datasets. In this paper, a
hierarchical approach is proposed to scale subspace learning methods, with the
goal of improving classification in large datasets by a range of 3% to 10%.
Different combinations of methods are studied. We assess the proposed method on
five publicly available large datasets, for different eigen-value based
subspace learning methods such as linear discriminant analysis, principal
component analysis, generalized discriminant analysis, and reconstruction
independent component analysis. To further examine the effect of the proposed
method on various classification methods, we fed the generated result to linear
discriminant analysis, quadratic linear analysis, k-nearest neighbor, and
random forest classifiers. The resulting classification accuracies are compared
to show the effectiveness of the hierarchical approach, reporting results of an
average of 5% increase in classification accuracy.","['Parisa Abdolrahim Poorheravi', 'Vincent Gaudet']","['cs.LG', 'stat.ML', 'I.5.2; I.5.1']",2021-05-25 15:15:12+00:00
http://arxiv.org/abs/2105.11982v2,Quantifying Uncertainty in Deep Spatiotemporal Forecasting,"Deep learning is gaining increasing popularity for spatiotemporal
forecasting. However, prior works have mostly focused on point estimates
without quantifying the uncertainty of the predictions. In high stakes domains,
being able to generate probabilistic forecasts with confidence intervals is
critical to risk assessment and decision making. Hence, a systematic study of
uncertainty quantification (UQ) methods for spatiotemporal forecasting is
missing in the community. In this paper, we describe two types of
spatiotemporal forecasting problems: regular grid-based and graph-based. Then
we analyze UQ methods from both the Bayesian and the frequentist point of view,
casting in a unified framework via statistical decision theory. Through
extensive experiments on real-world road network traffic, epidemics, and air
quality forecasting tasks, we reveal the statistical and computational
trade-offs for different UQ methods: Bayesian methods are typically more robust
in mean prediction, while confidence levels obtained from frequentist methods
provide more extensive coverage over data variations. Computationally, quantile
regression type methods are cheaper for a single confidence interval but
require re-training for different intervals. Sampling based methods generate
samples that can form multiple confidence intervals, albeit at a higher
computational cost.","['Dongxia Wu', 'Liyao Gao', 'Xinyue Xiong', 'Matteo Chinazzi', 'Alessandro Vespignani', 'Yi-An Ma', 'Rose Yu']","['cs.AI', 'cs.LG', 'stat.AP', 'stat.ML']",2021-05-25 14:35:46+00:00
http://arxiv.org/abs/2105.11964v1,Model Mismatch Trade-offs in LMMSE Estimation,"We consider a linear minimum mean squared error (LMMSE) estimation framework
with model mismatch where the assumed model order is smaller than that of the
underlying linear system which generates the data used in the estimation
process. By modelling the regressors of the underlying system as random
variables, we analyze the average behaviour of the mean squared error (MSE).
Our results quantify how the MSE depends on the interplay between the number of
samples and the number of parameters in the underlying system and in the
assumed model. In particular, if the number of samples is not sufficiently
large, neither increasing the number of samples nor the assumed model
complexity is sufficient to guarantee a performance improvement.","['Martin Hellkvist', 'Ayça Özçelikkale']","['eess.SP', 'cs.LG', 'stat.ML']",2021-05-25 14:16:45+00:00
http://arxiv.org/abs/2105.11886v2,Conformal Anomaly Detection on Spatio-Temporal Observations with Missing Data,"We develop a distribution-free, unsupervised anomaly detection method called
ECAD, which wraps around any regression algorithm and sequentially detects
anomalies. Rooted in conformal prediction, ECAD does not require data
exchangeability but approximately controls the Type-I error when data are
normal. Computationally, it involves no data-splitting and efficiently trains
ensemble predictors to increase statistical power. We demonstrate the superior
performance of ECAD on detecting anomalous spatio-temporal traffic flow.","['Chen Xu', 'Yao Xie']","['stat.AP', 'stat.ME', 'stat.ML']",2021-05-25 12:44:14+00:00
http://arxiv.org/abs/2105.11839v3,DiBS: Differentiable Bayesian Structure Learning,"Bayesian structure learning allows inferring Bayesian network structure from
data while reasoning about the epistemic uncertainty -- a key element towards
enabling active causal discovery and designing interventions in real world
systems. In this work, we propose a general, fully differentiable framework for
Bayesian structure learning (DiBS) that operates in the continuous space of a
latent probabilistic graph representation. Contrary to existing work, DiBS is
agnostic to the form of the local conditional distributions and allows for
joint posterior inference of both the graph structure and the conditional
distribution parameters. This makes our formulation directly applicable to
posterior inference of complex Bayesian network models, e.g., with nonlinear
dependencies encoded by neural networks. Using DiBS, we devise an efficient,
general purpose variational inference method for approximating distributions
over structural models. In evaluations on simulated and real-world data, our
method significantly outperforms related approaches to joint posterior
inference.","['Lars Lorch', 'Jonas Rothfuss', 'Bernhard Schölkopf', 'Andreas Krause']","['cs.LG', 'stat.ML']",2021-05-25 11:23:08+00:00
http://arxiv.org/abs/2105.11818v2,SGD with Coordinate Sampling: Theory and Practice,"While classical forms of stochastic gradient descent algorithm treat the
different coordinates in the same way, a framework allowing for adaptive (non
uniform) coordinate sampling is developed to leverage structure in data. In a
non-convex setting and including zeroth order gradient estimate, almost sure
convergence as well as non-asymptotic bounds are established. Within the
proposed framework, we develop an algorithm, MUSKETEER, based on a
reinforcement strategy: after collecting information on the noisy gradients, it
samples the most promising coordinate (all for one); then it moves along the
one direction yielding an important decrease of the objective (one for all).
Numerical experiments on both synthetic and real data examples confirm the
effectiveness of MUSKETEER in large scale problems.","['Rémi Leluc', 'François Portier']","['stat.ML', 'cs.LG']",2021-05-25 10:37:50+00:00
http://arxiv.org/abs/2105.11815v1,"Hashing embeddings of optimal dimension, with applications to linear least squares","The aim of this paper is two-fold: firstly, to present subspace embedding
properties for $s$-hashing sketching matrices, with $s\geq 1$, that are optimal
in the projection dimension $m$ of the sketch, namely, $m=\mathcal{O}(d)$,
where $d$ is the dimension of the subspace. A diverse set of results are
presented that address the case when the input matrix has sufficiently low
coherence (thus removing the $\log^2 d$ factor dependence in $m$, in the
low-coherence result of Bourgain et al (2015) at the expense of a smaller
coherence requirement); how this coherence changes with the number $s$ of
column nonzeros (allowing a scaling of $\sqrt{s}$ of the coherence bound), or
is reduced through suitable transformations (when considering hashed -- instead
of subsampled -- coherence reducing transformations such as randomised
Hadamard). Secondly, we apply these general hashing sketching results to the
special case of Linear Least Squares (LLS), and develop Ski-LLS, a generic
software package for these problems, that builds upon and improves the
Blendenpik solver on dense input and the (sequential) LSRN performance on
sparse problems. In addition to the hashing sketching improvements, we add
suitable linear algebra tools for rank-deficient and for sparse problems that
lead Ski-LLS to outperform not only sketching-based routines on randomly
generated input, but also state of the art direct solver SPQR and iterative
code HSL on certain subsets of the sparse Florida matrix collection; namely, on
least squares problems that are significantly overdetermined, or moderately
sparse, or difficult.","['Coralia Cartis', 'Jan Fiala', 'Zhen Shao']","['math.NA', 'cs.NA', 'math.OC', 'stat.ML', '65K05, 93E24, 65F08, 65F10, 65F20, 65F50, 62J05']",2021-05-25 10:35:13+00:00
http://arxiv.org/abs/2105.11802v2,Bias-Robust Bayesian Optimization via Dueling Bandits,"We consider Bayesian optimization in settings where observations can be
adversarially biased, for example by an uncontrolled hidden confounder. Our
first contribution is a reduction of the confounded setting to the dueling
bandit model. Then we propose a novel approach for dueling bandits based on
information-directed sampling (IDS). Thereby, we obtain the first efficient
kernelized algorithm for dueling bandits that comes with cumulative regret
guarantees. Our analysis further generalizes a previously proposed
semi-parametric linear bandit model to non-linear reward functions, and
uncovers interesting links to doubly-robust estimation.","['Johannes Kirschner', 'Andreas Krause']","['stat.ML', 'cs.LG']",2021-05-25 10:08:41+00:00
http://arxiv.org/abs/2105.11724v3,SHAFF: Fast and consistent SHApley eFfect estimates via random Forests,"Interpretability of learning algorithms is crucial for applications involving
critical decisions, and variable importance is one of the main interpretation
tools. Shapley effects are now widely used to interpret both tree ensembles and
neural networks, as they can efficiently handle dependence and interactions in
the data, as opposed to most other variable importance measures. However,
estimating Shapley effects is a challenging task, because of the computational
complexity and the conditional expectation estimates. Accordingly, existing
Shapley algorithms have flaws: a costly running time, or a bias when input
variables are dependent. Therefore, we introduce SHAFF, SHApley eFfects via
random Forests, a fast and accurate Shapley effect estimate, even when input
variables are dependent. We show SHAFF efficiency through both a theoretical
analysis of its consistency, and the practical performance improvements over
competitors with extensive experiments. An implementation of SHAFF in C++ and R
is available online.","['Clément Bénard', 'Gérard Biau', 'Sébastien da Veiga', 'Erwan Scornet']","['stat.ML', 'cs.LG']",2021-05-25 07:48:07+00:00
http://arxiv.org/abs/2105.11558v3,Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems,"We consider the setting of vector valued non-linear dynamical systems
$X_{t+1} = \phi(A^* X_t) + \eta_t$, where $\eta_t$ is unbiased noise and $\phi
: \mathbb{R} \to \mathbb{R}$ is a known link function that satisfies certain
{\em expansivity property}. The goal is to learn $A^*$ from a single trajectory
$X_1,\cdots,X_T$ of {\em dependent or correlated} samples. While the problem is
well-studied in the linear case, where $\phi$ is identity, with optimal error
rates even for non-mixing systems, existing results in the non-linear case hold
only for mixing systems. In this work, we improve existing results for learning
nonlinear systems in a number of ways: a) we provide the first offline
algorithm that can learn non-linear dynamical systems without the mixing
assumption, b) we significantly improve upon the sample complexity of existing
results for mixing systems, c) in the much harder one-pass, streaming setting
we study a SGD with Reverse Experience Replay ($\mathsf{SGD-RER}$) method, and
demonstrate that for mixing systems, it achieves the same sample complexity as
our offline algorithm, d) we justify the expansivity assumption by showing that
for the popular ReLU link function -- a non-expansive but easy to learn link
function with i.i.d. samples -- any method would require exponentially many
samples (with respect to dimension of $X_t$) from the dynamical system. We
validate our results via. simulations and demonstrate that a naive application
of SGD can be highly sub-optimal. Indeed, our work demonstrates that for
correlated data, specialized methods designed for the dependency structure in
data can significantly outperform standard SGD based methods.","['Prateek Jain', 'Suhas S Kowshik', 'Dheeraj Nagaraj', 'Praneeth Netrapalli']","['cs.LG', 'math.OC', 'stat.ML']",2021-05-24 22:14:26+00:00
http://arxiv.org/abs/2105.11535v2,Scalable Cross Validation Losses for Gaussian Process Models,"We introduce a simple and scalable method for training Gaussian process (GP)
models that exploits cross-validation and nearest neighbor truncation. To
accommodate binary and multi-class classification we leverage P\`olya-Gamma
auxiliary variables and variational inference. In an extensive empirical
comparison with a number of alternative methods for scalable GP regression and
classification, we find that our method offers fast training and excellent
predictive performance. We argue that the good predictive performance can be
traced to the non-parametric nature of the resulting predictive distributions
as well as to the cross-validation loss, which provides robustness against
model mis-specification.","['Martin Jankowiak', 'Geoff Pleiss']","['stat.ML', 'cs.LG']",2021-05-24 21:01:47+00:00
http://arxiv.org/abs/2105.11522v2,Unbiased Estimation of the Gradient of the Log-Likelihood for a Class of Continuous-Time State-Space Models,"In this paper, we consider static parameter estimation for a class of
continuous-time state-space models. Our goal is to obtain an unbiased estimate
of the gradient of the log-likelihood (score function), which is an estimate
that is unbiased even if the stochastic processes involved in the model must be
discretized in time. To achieve this goal, we apply a doubly randomized scheme,
that involves a novel coupled conditional particle filter (CCPF) on the second
level of randomization. Our novel estimate helps facilitate the application of
gradient-based estimation algorithms, such as stochastic-gradient Langevin
descent. We illustrate our methodology in the context of stochastic gradient
descent (SGD) in several numerical examples and compare with the Rhee & Glynn
estimator.","['Marco Ballesio', 'Ajay Jasra']","['stat.ML', 'cs.LG', 'cs.NA', 'math.NA']",2021-05-24 20:31:48+00:00
http://arxiv.org/abs/2105.11447v1,True Few-Shot Learning with Language Models,"Pretrained language models (LMs) perform well on many tasks even when
learning from a few examples, but prior work uses many held-out examples to
tune various aspects of learning, such as hyperparameters, training objectives,
and natural language templates (""prompts""). Here, we evaluate the few-shot
ability of LMs when such held-out examples are unavailable, a setting we call
true few-shot learning. We test two model selection criteria, cross-validation
and minimum description length, for choosing LM prompts and hyperparameters in
the true few-shot setting. On average, both marginally outperform random
selection and greatly underperform selection based on held-out examples.
Moreover, selection criteria often prefer models that perform significantly
worse than randomly-selected ones. We find similar results even when taking
into account our uncertainty in a model's true performance during selection, as
well as when varying the amount of computation and number of examples used for
selection. Overall, our findings suggest that prior work significantly
overestimated the true few-shot ability of LMs given the difficulty of few-shot
model selection.","['Ethan Perez', 'Douwe Kiela', 'Kyunghyun Cho']","['cs.CL', 'cs.LG', 'stat.ML']",2021-05-24 17:55:51+00:00
