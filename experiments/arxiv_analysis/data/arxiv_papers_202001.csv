id,title,abstract,authors,categories,date
http://arxiv.org/abs/2002.01576v1,Large Batch Training Does Not Need Warmup,"Training deep neural networks using a large batch size has shown promising
results and benefits many real-world applications. However, the optimizer
converges slowly at early epochs and there is a gap between large-batch deep
learning optimization heuristics and theoretical underpinnings. In this paper,
we propose a novel Complete Layer-wise Adaptive Rate Scaling (CLARS) algorithm
for large-batch training. We also analyze the convergence rate of the proposed
method by introducing a new fine-grained analysis of gradient-based methods.
Based on our analysis, we bridge the gap and illustrate the theoretical
insights for three popular large-batch training techniques, including linear
learning rate scaling, gradual warmup, and layer-wise adaptive rate scaling.
Extensive experiments demonstrate that the proposed algorithm outperforms
gradual warmup technique by a large margin and defeats the convergence of the
state-of-the-art large-batch optimizer in training advanced deep neural
networks (ResNet, DenseNet, MobileNet) on ImageNet dataset.","['Zhouyuan Huo', 'Bin Gu', 'Heng Huang']","['cs.LG', 'stat.ML']",2020-02-04 23:03:12+00:00
http://arxiv.org/abs/2002.01569v2,Uncertainty Quantification for Bayesian Optimization,"Bayesian optimization is a class of global optimization techniques. In
Bayesian optimization, the underlying objective function is modeled as a
realization of a Gaussian process. Although the Gaussian process assumption
implies a random distribution of the Bayesian optimization outputs,
quantification of this uncertainty is rarely studied in the literature. In this
work, we propose a novel approach to assess the output uncertainty of Bayesian
optimization algorithms, which proceeds by constructing confidence regions of
the maximum point (or value) of the objective function. These regions can be
computed efficiently, and their confidence levels are guaranteed by the uniform
error bounds for sequential Gaussian process regression newly developed in the
present work. Our theory provides a unified uncertainty quantification
framework for all existing sequential sampling policies and stopping criteria.","['Rui Tuo', 'Wenjia Wang']","['math.ST', 'stat.ML', 'stat.TH']",2020-02-04 22:48:07+00:00
http://arxiv.org/abs/2002.01568v1,DVNet: A Memory-Efficient Three-Dimensional CNN for Large-Scale Neurovascular Reconstruction,"Maps of brain microarchitecture are important for understanding neurological
function and behavior, including alterations caused by chronic conditions such
as neurodegenerative disease. Techniques such as knife-edge scanning microscopy
(KESM) provide the potential for whole organ imaging at sub-cellular
resolution. However, multi-terabyte data sizes make manual annotation
impractical and automatic segmentation challenging. Densely packed cells
combined with interconnected microvascular networks are a challenge for current
segmentation algorithms. The massive size of high-throughput microscopy data
necessitates fast and largely unsupervised algorithms. In this paper, we
investigate a fully-convolutional, deep, and densely-connected encoder-decoder
for pixel-wise semantic segmentation. The excessive memory complexity often
encountered with deep and dense networks is mitigated using skip connections,
resulting in fewer parameters and enabling a significant performance increase
over prior architectures. The proposed network provides superior performance
for semantic segmentation problems applied to open-source benchmarks. We
finally demonstrate our network for cellular and microvascular segmentation,
enabling quantitative metrics for organ-scale neurovascular analysis.","['Leila Saadatifard', 'Aryan Mobiny', 'Pavel Govyadinov', 'Hien Nguyen', 'David Mayerich']","['eess.IV', 'cs.LG', 'stat.ML']",2020-02-04 22:39:58+00:00
http://arxiv.org/abs/2002.01547v1,Accelerating Psychometric Screening Tests With Bayesian Active Differential Selection,"Classical methods for psychometric function estimation either require
excessive measurements or produce only a low-resolution approximation of the
target psychometric function. In this paper, we propose a novel solution for
rapid screening for a change in the psychometric function estimation of a given
patient. We use Bayesian active model selection to perform an automated
pure-tone audiogram test with the goal of quickly finding if the current
audiogram will be different from a previous audiogram. We validate our approach
using audiometric data from the National Institute for Occupational Safety and
Health NIOSH. Initial results show that with a few tones we can detect if the
patient's audiometric function has changed between the two test sessions with
high confidence.","['Trevor J. Larsen', 'Gustavo Malkomes', 'Dennis L. Barbour']","['cs.LG', 'stat.ML']",2020-02-04 21:35:03+00:00
http://arxiv.org/abs/2002.01523v3,A Deep Conditioning Treatment of Neural Networks,"We study the role of depth in training randomly initialized overparameterized
neural networks. We give a general result showing that depth improves
trainability of neural networks by improving the conditioning of certain kernel
matrices of the input data. This result holds for arbitrary non-linear
activation functions under a certain normalization. We provide versions of the
result that hold for training just the top layer of the neural network, as well
as for training all layers, via the neural tangent kernel. As applications of
these general results, we provide a generalization of the results of Das et al.
(2019) showing that learnability of deep random neural networks with a large
class of non-linear activations degrades exponentially with depth. We also show
how benign overfitting can occur in deep neural networks via the results of
Bartlett et al. (2019b). We also give experimental evidence that normalized
versions of ReLU are a viable alternative to more complex operations like Batch
Normalization in training deep neural networks.","['Naman Agarwal', 'Pranjal Awasthi', 'Satyen Kale']","['cs.LG', 'stat.ML']",2020-02-04 20:21:36+00:00
http://arxiv.org/abs/2002.01927v8,Self-Directed Online Machine Learning for Topology Optimization,"Topology optimization by optimally distributing materials in a given domain
requires non-gradient optimizers to solve highly complicated problems. However,
with hundreds of design variables or more involved, solving such problems would
require millions of Finite Element Method (FEM) calculations whose
computational cost is huge and impractical. Here we report Self-directed Online
Learning Optimization (SOLO) which integrates Deep Neural Network (DNN) with
FEM calculations. A DNN learns and substitutes the objective as a function of
design variables. A small number of training data is generated dynamically
based on the DNN's prediction of the optimum. The DNN adapts to the new
training data and gives better prediction in the region of interest until
convergence. The optimum predicted by the DNN is proved to converge to the true
global optimum through iterations. Our algorithm was tested by four types of
problems including compliance minimization, fluid-structure optimization, heat
transfer enhancement and truss optimization. It reduced the computational time
by 2 ~ 5 orders of magnitude compared with directly using heuristic methods,
and outperformed all state-of-the-art algorithms tested in our experiments.
This approach enables solving large multi-dimensional optimization problems.","['Changyu Deng', 'Yizhou Wang', 'Can Qin', 'Yun Fu', 'Wei Lu']","['cs.CE', 'cs.LG', 'stat.ML']",2020-02-04 20:00:28+00:00
http://arxiv.org/abs/2002.01464v1,Visual Concept-Metaconcept Learning,"Humans reason with concepts and metaconcepts: we recognize red and green from
visual input; we also understand that they describe the same property of
objects (i.e., the color). In this paper, we propose the visual
concept-metaconcept learner (VCML) for joint learning of concepts and
metaconcepts from images and associated question-answer pairs. The key is to
exploit the bidirectional connection between visual concepts and metaconcepts.
Visual representations provide grounding cues for predicting relations between
unseen pairs of concepts. Knowing that red and green describe the same property
of objects, we generalize to the fact that cube and sphere also describe the
same property of objects, since they both categorize the shape of objects.
Meanwhile, knowledge about metaconcepts empowers visual concept learning from
limited, noisy, and even biased data. From just a few examples of purple cubes
we can understand a new color purple, which resembles the hue of the cubes
instead of the shape of them. Evaluation on both synthetic and real-world
datasets validates our claims.","['Chi Han', 'Jiayuan Mao', 'Chuang Gan', 'Joshua B. Tenenbaum', 'Jiajun Wu']","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'stat.ML']",2020-02-04 18:42:30+00:00
http://arxiv.org/abs/2002.01444v6,Learning of Linear Dynamical Systems as a Non-Commutative Polynomial Optimization Problem,"There has been much recent progress in forecasting the next observation of a
linear dynamical system (LDS), which is known as the improper learning, as well
as in the estimation of its system matrices, which is known as the proper
learning of LDS. We present an approach to proper learning of LDS, which in
spite of the non-convexity of the problem, guarantees global convergence of
numerical solutions to a least-squares estimator. We present promising
computational results.","['Quan Zhou', 'Jakub Marecek']","['math.OC', 'cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2020-02-04 18:08:49+00:00
http://arxiv.org/abs/2002.01441v2,A Generalized Flow for B2B Sales Predictive Modeling: An Azure Machine Learning Approach,"Predicting the outcome of sales opportunities is a core part of successful
business management. Conventionally, making this prediction has relied mostly
on subjective human evaluations in the process of sales decision making. In
this paper, we addressed the problem of forecasting the outcome of business to
business (B2B) sales by proposing a thorough data-driven Machine Learning (ML)
workflow on a cloud-based computing platform: Microsoft Azure Machine Learning
Service (Azure ML). This workflow consists of two pipelines: (1) An ML pipeline
to train probabilistic predictive models on the historical sales opportunities
data. In this pipeline, data is enriched with an extensive feature enhancement
step and then used to train an ensemble of ML classification models in
parallel. (2) A prediction pipeline to utilize the trained ML model and infer
the likelihood of winning new sales opportunities along with calculating
optimal decision boundaries. The effectiveness of the proposed workflow was
evaluated on a real sales dataset of a major global B2B consulting firm. Our
results implied that decision-making based on the ML predictions is more
accurate and brings a higher monetary value.",['Alireza Rezazadeh'],"['cs.LG', 'stat.ML']",2020-02-04 18:01:24+00:00
http://arxiv.org/abs/2002.01428v1,Learning Task-Driven Control Policies via Information Bottlenecks,"This paper presents a reinforcement learning approach to synthesizing
task-driven control policies for robotic systems equipped with rich sensory
modalities (e.g., vision or depth). Standard reinforcement learning algorithms
typically produce policies that tightly couple control actions to the entirety
of the system's state and rich sensor observations. As a consequence, the
resulting policies can often be sensitive to changes in task-irrelevant
portions of the state or observations (e.g., changing background colors). In
contrast, the approach we present here learns to create a task-driven
representation that is used to compute control actions. Formally, this is
achieved by deriving a policy gradient-style algorithm that creates an
information bottleneck between the states and the task-driven representation;
this constrains actions to only depend on task-relevant information. We
demonstrate our approach in a thorough set of simulation results on multiple
examples including a grasping task that utilizes depth images and a
ball-catching task that utilizes RGB images. Comparisons with a standard policy
gradient approach demonstrate that the task-driven policies produced by our
algorithm are often significantly more robust to sensor noise and
task-irrelevant changes in the environment.","['Vincent Pacelli', 'Anirudha Majumdar']","['cs.LG', 'cs.RO', 'math.OC', 'stat.ML']",2020-02-04 17:50:06+00:00
http://arxiv.org/abs/2002.03898v2,Self-supervised ECG Representation Learning for Emotion Recognition,"We exploit a self-supervised deep multi-task learning framework for
electrocardiogram (ECG) -based emotion recognition. The proposed solution
consists of two stages of learning a) learning ECG representations and b)
learning to classify emotions. ECG representations are learned by a signal
transformation recognition network. The network learns high-level abstract
representations from unlabeled ECG data. Six different signal transformations
are applied to the ECG signals, and transformation recognition is performed as
pretext tasks. Training the model on pretext tasks helps the network learn
spatiotemporal representations that generalize well across different datasets
and different emotion categories. We transfer the weights of the
self-supervised network to an emotion recognition network, where the
convolutional layers are kept frozen and the dense layers are trained with
labelled ECG data. We show that the proposed solution considerably improves the
performance compared to a network trained using fully-supervised learning. New
state-of-the-art results are set in classification of arousal, valence,
affective states, and stress for the four utilized datasets. Extensive
experiments are performed, providing interesting insights into the impact of
using a multi-task self-supervised structure instead of a single-task model, as
well as the optimum level of difficulty required for the pretext
self-supervised tasks.","['Pritam Sarkar', 'Ali Etemad']","['eess.SP', 'cs.LG', 'stat.ML']",2020-02-04 17:15:37+00:00
http://arxiv.org/abs/2002.01408v1,Apportioned Margin Approach for Cost Sensitive Large Margin Classifiers,"We consider the problem of cost sensitive multiclass classification, where we
would like to increase the sensitivity of an important class at the expense of
a less important one. We adopt an {\em apportioned margin} framework to address
this problem, which enables an efficient margin shift between classes that
share the same boundary. The decision boundary between all pairs of classes
divides the margin between them in accordance to a given prioritization vector,
which yields a tighter error bound for the important classes while also
reducing the overall out-of-sample error. In addition to demonstrating an
efficient implementation of our framework, we derive generalization bounds,
demonstrate Fisher consistency, adapt the framework to Mercer's kernel and to
neural networks, and report promising empirical results on all accounts.","['Lee-Ad Gottlieb', 'Eran Kaufman', 'Aryeh Kontorovich']","['cs.LG', 'stat.ML']",2020-02-04 17:00:30+00:00
http://arxiv.org/abs/2002.01953v1,BOFFIN TTS: Few-Shot Speaker Adaptation by Bayesian Optimization,"We present BOFFIN TTS (Bayesian Optimization For FIne-tuning Neural Text To
Speech), a novel approach for few-shot speaker adaptation. Here, the task is to
fine-tune a pre-trained TTS model to mimic a new speaker using a small corpus
of target utterances. We demonstrate that there does not exist a
one-size-fits-all adaptation strategy, with convincing synthesis requiring a
corpus-specific configuration of the hyper-parameters that control fine-tuning.
By using Bayesian optimization to efficiently optimize these hyper-parameter
values for a target speaker, we are able to perform adaptation with an average
30% improvement in speaker similarity over standard techniques. Results
indicate, across multiple corpora, that BOFFIN TTS can learn to synthesize new
speakers using less than ten minutes of audio, achieving the same naturalness
as produced for the speakers used to train the base model.","['Henry B. Moss', 'Vatsal Aggarwal', 'Nishant Prateek', 'Javier González', 'Roberto Barra-Chicote']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2020-02-04 16:37:52+00:00
http://arxiv.org/abs/2002.01370v1,Bootstrapping a DQN Replay Memory with Synthetic Experiences,"An important component of many Deep Reinforcement Learning algorithms is the
Experience Replay which serves as a storage mechanism or memory of made
experiences. These experiences are used for training and help the agent to
stably find the perfect trajectory through the problem space. The classic
Experience Replay however makes only use of the experiences it actually made,
but the stored samples bear great potential in form of knowledge about the
problem that can be extracted. We present an algorithm that creates synthetic
experiences in a nondeterministic discrete environment to assist the learner.
The Interpolated Experience Replay is evaluated on the FrozenLake environment
and we show that it can support the agent to learn faster and even better than
the classic version.","['Wenzel Baron Pilar von Pilchau', 'Anthony Stein', 'Jörg Hähner']","['cs.LG', 'stat.ML']",2020-02-04 15:36:36+00:00
http://arxiv.org/abs/2002.01368v8,Open-set learning with augmented categories by exploiting unlabelled data,"Novel categories are commonly defined as those unobserved during training but
present during testing. However, partially labelled training datasets can
contain unlabelled training samples that belong to novel categories, meaning
these can be present in training and testing. This research is the first to
generalise between what we call observed-novel and unobserved-novel categories
within a new learning policy called open-set learning with augmented category
by exploiting unlabelled data or Open-LACU. After surveying existing learning
policies, we introduce Open-LACU as a unified policy of positive and unlabelled
learning, semi-supervised learning and open-set recognition. Subsequently, we
develop the first Open-LACU model using an algorithmic training process of the
relevant research fields. The proposed Open-LACU classifier achieves
state-of-the-art and first-of-its-kind results.","['Emile R. Engelbrecht', 'Johan A. du Preez']","['stat.ML', 'cs.CV', 'cs.LG']",2020-02-04 15:32:23+00:00
http://arxiv.org/abs/2002.01335v4,Structural Inductive Biases in Emergent Communication,"In order to communicate, humans flatten a complex representation of ideas and
their attributes into a single word or a sentence. We investigate the impact of
representation learning in artificial agents by developing graph referential
games. We empirically show that agents parametrized by graph neural networks
develop a more compositional language compared to bag-of-words and sequence
models, which allows them to systematically generalize to new combinations of
familiar features.","['Agnieszka Słowik', 'Abhinav Gupta', 'William L. Hamilton', 'Mateja Jamnik', 'Sean B. Holden', 'Christopher Pal']","['cs.CL', 'cs.AI', 'cs.LG', 'cs.MA', 'stat.ML']",2020-02-04 14:59:08+00:00
http://arxiv.org/abs/2002.01268v1,Finite Time Analysis of Linear Two-timescale Stochastic Approximation with Markovian Noise,"Linear two-timescale stochastic approximation (SA) scheme is an important
class of algorithms which has become popular in reinforcement learning (RL),
particularly for the policy evaluation problem. Recently, a number of works
have been devoted to establishing the finite time analysis of the scheme,
especially under the Markovian (non-i.i.d.) noise settings that are ubiquitous
in practice. In this paper, we provide a finite-time analysis for linear two
timescale SA. Our bounds show that there is no discrepancy in the convergence
rate between Markovian and martingale noise, only the constants are affected by
the mixing time of the Markov chain. With an appropriate step size schedule,
the transient term in the expected error bound is $o(1/k^c)$ and the
steady-state term is ${\cal O}(1/k)$, where $c>1$ and $k$ is the iteration
number. Furthermore, we present an asymptotic expansion of the expected error
with a matching lower bound of $\Omega(1/k)$. A simple numerical experiment is
presented to support our theory.","['Maxim Kaledin', 'Eric Moulines', 'Alexey Naumov', 'Vladislav Tadic', 'Hoi-To Wai']","['stat.ML', 'cs.LG']",2020-02-04 13:03:17+00:00
http://arxiv.org/abs/2002.01256v1,Minimax Defense against Gradient-based Adversarial Attacks,"State-of-the-art adversarial attacks are aimed at neural network classifiers.
By default, neural networks use gradient descent to minimize their loss
function. The gradient of a classifier's loss function is used by
gradient-based adversarial attacks to generate adversarially perturbed images.
We pose the question whether another type of optimization could give neural
network classifiers an edge. Here, we introduce a novel approach that uses
minimax optimization to foil gradient-based adversarial attacks. Our minimax
classifier is the discriminator of a generative adversarial network (GAN) that
plays a minimax game with the GAN generator. In addition, our GAN generator
projects all points onto a manifold that is different from the original
manifold since the original manifold might be the cause of adversarial attacks.
To measure the performance of our minimax defense, we use adversarial attacks -
Carlini Wagner (CW), DeepFool, Fast Gradient Sign Method (FGSM) - on three
datasets: MNIST, CIFAR-10 and German Traffic Sign (TRAFFIC). Against CW
attacks, our minimax defense achieves 98.07% (MNIST-default 98.93%), 73.90%
(CIFAR-10-default 83.14%) and 94.54% (TRAFFIC-default 96.97%). Against DeepFool
attacks, our minimax defense achieves 98.87% (MNIST), 76.61% (CIFAR-10) and
94.57% (TRAFFIC). Against FGSM attacks, we achieve 97.01% (MNIST), 76.79%
(CIFAR-10) and 81.41% (TRAFFIC). Our Minimax adversarial approach presents a
significant shift in defense strategy for neural network classifiers.","['Blerta Lindqvist', 'Rauf Izmailov']","['cs.LG', 'cs.GT', 'stat.ML']",2020-02-04 12:33:13+00:00
http://arxiv.org/abs/2002.01245v1,A Regression Tsetlin Machine with Integer Weighted Clauses for Compact Pattern Representation,"The Regression Tsetlin Machine (RTM) addresses the lack of interpretability
impeding state-of-the-art nonlinear regression models. It does this by using
conjunctive clauses in propositional logic to capture the underlying non-linear
frequent patterns in the data. These, in turn, are combined into a continuous
output through summation, akin to a linear regression function, however, with
non-linear components and unity weights. Although the RTM has solved non-linear
regression problems with competitive accuracy, the resolution of the output is
proportional to the number of clauses employed. This means that computation
cost increases with resolution. To reduce this problem, we here introduce
integer weighted RTM clauses. Our integer weighted clause is a compact
representation of multiple clauses that capture the same sub-pattern-N
repeating clauses are turned into one, with an integer weight N. This reduces
computation cost N times, and increases interpretability through a sparser
representation. We further introduce a novel learning scheme that allows us to
simultaneously learn both the clauses and their weights, taking advantage of
so-called stochastic searching on the line. We evaluate the potential of the
integer weighted RTM empirically using six artificial datasets. The results
show that the integer weighted RTM is able to acquire on par or better accuracy
using significantly less computational resources compared to regular RTMs. We
further show that integer weights yield improved accuracy over real-valued
ones.","['K. Darshana Abeyrathna', 'Ole-Christoffer Granmo', 'Morten Goodwin']","['cs.LG', 'cs.AI', 'stat.ML']",2020-02-04 12:06:16+00:00
http://arxiv.org/abs/2002.01244v1,Machine Learning Techniques to Detect and Characterise Whistler Radio Waves,"Lightning strokes create powerful electromagnetic pulses that routinely cause
very low frequency (VLF) waves to propagate across hemispheres along
geomagnetic field lines. VLF antenna receivers can be used to detect these
whistler waves generated by these lightning strokes. The particular
time/frequency dependence of the received whistler wave enables the estimation
of electron density in the plasmasphere region of the magnetosphere. Therefore
the identification and characterisation of whistlers are important tasks to
monitor the plasmasphere in real-time and to build large databases of events to
be used for statistical studies. The current state of the art in detecting
whistler is the Automatic Whistler Detection (AWD) method developed by
Lichtenberger (2009). This method is based on image correlation in 2 dimensions
and requires significant computing hardware situated at the VLF receiver
antennas (e.g. in Antarctica). The aim of this work is to develop a machine
learning-based model capable of automatically detecting whistlers in the data
provided by the VLF receivers. The approach is to use a combination of image
classification and localisation on the spectrogram data generated by the VLF
receivers to identify and localise each whistler. The data at hand has around
2300 events identified by AWD at SANAE and Marion and will be used as training,
validation, and testing data. Three detector designs have been proposed. The
first one using a similar method to AWD, the second using image classification
on regions of interest extracted from a spectrogram, and the last one using
YOLO, the current state of the art in object detection. It has been shown that
these detectors can achieve a misdetection and false alarm of less than 15% on
Marion's dataset.","['Othniel J. E. Y. Konan', 'Amit Kumar Mishra', 'Stefan Lotz']","['eess.SP', 'cs.LG', 'stat.ML']",2020-02-04 12:05:44+00:00
http://arxiv.org/abs/2002.02402v1,Neural network with data augmentation in multi-objective prediction of multi-stage pump,"A multi-objective prediction method of multi-stage pump method based on
neural network with data augmentation is proposed. In order to study the highly
nonlinear relationship between key design variables and centrifugal pump
external characteristic values (head and power), the neural network model (NN)
is built in comparison with the quadratic response surface model (RSF), the
radial basis Gaussian response surface model (RBF), and the Kriging model
(KRG). The numerical model validation experiment of another type of single
stage centrifugal pump showed that numerical model based on CFD is quite
accurate and fair. All of prediction models are trained by 60 samples under the
different combination of three key variables in design range respectively. The
accuracy of the head and power based on the four predictions models are
analyzed comparing with the CFD simulation values. The results show that the
neural network model has better performance in all external characteristic
values comparing with other three surrogate models. Finally, a neural network
model based on data augmentation (NNDA) is proposed for the reason that
simulation cost is too high and data is scarce in mechanical simulation field
especially in CFD problems. The model with data augmentation can triple the
data by interpolation at each sample point of different attributes. It shows
that the performance of neural network model with data augmentation is better
than former neural network model. Therefore, the prediction ability of NN is
enhanced without more simulation costs. With data augmentation it can be a
better prediction model used in solving the optimization problems of multistage
pump for next optimization and generalized to finite element analysis
optimization problems in future.",['Hang Zhao'],"['eess.SP', 'stat.ML']",2020-02-04 11:23:42+00:00
http://arxiv.org/abs/2002.01227v1,ALPINE: Active Link Prediction using Network Embedding,"Many real-world problems can be formalized as predicting links in a partially
observed network. Examples include Facebook friendship suggestions,
consumer-product recommendations, and the identification of hidden interactions
between actors in a crime network. Several link prediction algorithms, notably
those recently introduced using network embedding, are capable of doing this by
just relying on the observed part of the network. Often, the link status of a
node pair can be queried, which can be used as additional information by the
link prediction algorithm. Unfortunately, such queries can be expensive or
time-consuming, mandating the careful consideration of which node pairs to
query. In this paper we estimate the improvement in link prediction accuracy
after querying any particular node pair, to use in an active learning setup.
Specifically, we propose ALPINE (Active Link Prediction usIng Network
Embedding), the first method to achieve this for link prediction based on
network embedding. To this end, we generalized the notion of V-optimality from
experimental design to this setting, as well as more basic active learning
heuristics originally developed in standard classification settings. Empirical
results on real data show that ALPINE is scalable, and boosts link prediction
accuracy with far fewer queries.","['Xi Chen', 'Bo Kang', 'Jefrey Lijffijt', 'Tijl De Bie']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2020-02-04 11:09:03+00:00
http://arxiv.org/abs/2002.01197v2,Selfish Robustness and Equilibria in Multi-Player Bandits,"Motivated by cognitive radios, stochastic multi-player multi-armed bandits
gained a lot of interest recently. In this class of problems, several players
simultaneously pull arms and encounter a collision - with 0 reward - if some of
them pull the same arm at the same time. While the cooperative case where
players maximize the collective reward (obediently following some fixed
protocol) has been mostly considered, robustness to malicious players is a
crucial and challenging concern. Existing approaches consider only the case of
adversarial jammers whose objective is to blindly minimize the collective
reward. We shall consider instead the more natural class of selfish players
whose incentives are to maximize their individual rewards, potentially at the
expense of the social welfare. We provide the first algorithm robust to selfish
players (a.k.a. Nash equilibrium) with a logarithmic regret, when the arm
performance is observed. When collisions are also observed, Grim Trigger type
of strategies enable some implicit communication-based algorithms and we
construct robust algorithms in two different settings: the homogeneous (with a
regret comparable to the centralized optimal one) and heterogeneous cases (for
an adapted and relevant notion of regret). We also provide impossibility
results when only the reward is observed or when arm means vary arbitrarily
among players.","['Etienne Boursier', 'Vianney Perchet']","['cs.LG', 'stat.ML']",2020-02-04 09:50:28+00:00
http://arxiv.org/abs/2002.01184v1,tfp.mcmc: Modern Markov Chain Monte Carlo Tools Built for Modern Hardware,"Markov chain Monte Carlo (MCMC) is widely regarded as one of the most
important algorithms of the 20th century. Its guarantees of asymptotic
convergence, stability, and estimator-variance bounds using only unnormalized
probability functions make it indispensable to probabilistic programming. In
this paper, we introduce the TensorFlow Probability MCMC toolkit, and discuss
some of the considerations that motivated its design.","['Junpeng Lao', 'Christopher Suter', 'Ian Langmore', 'Cyril Chimisov', 'Ashish Saxena', 'Pavel Sountsov', 'Dave Moore', 'Rif A. Saurous', 'Matthew D. Hoffman', 'Joshua V. Dillon']","['stat.CO', 'cs.PL', 'stat.ML']",2020-02-04 09:27:26+00:00
http://arxiv.org/abs/2002.01182v1,Learning bounded subsets of $L_p$,"We study learning problems in which the underlying class is a bounded subset
of $L_p$ and the target $Y$ belongs to $L_p$. Previously, minimax sample
complexity estimates were known under such boundedness assumptions only when
$p=\infty$. We present a sharp sample complexity estimate that holds for any $p
> 4$. It is based on a learning procedure that is suited for heavy-tailed
problems.",['Shahar Mendelson'],"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2020-02-04 09:25:34+00:00
http://arxiv.org/abs/2002.01180v3,Robust Generative Restricted Kernel Machines using Weighted Conjugate Feature Duality,"Interest in generative models has grown tremendously in the past decade.
However, their training performance can be adversely affected by contamination,
where outliers are encoded in the representation of the model. This results in
the generation of noisy data. In this paper, we introduce weighted conjugate
feature duality in the framework of Restricted Kernel Machines (RKMs). The RKM
formulation allows for an easy integration of methods from classical robust
statistics. This formulation is used to fine-tune the latent space of
generative RKMs using a weighting function based on the Minimum Covariance
Determinant, which is a highly robust estimator of multivariate location and
scatter. Experiments show that the weighted RKM is capable of generating clean
images when contamination is present in the training data. We further show that
the robust method also preserves uncorrelated feature learning through
qualitative and quantitative experiments on standard datasets.","['Arun Pandey', 'Joachim Schreurs', 'Johan A. K. Suykens']","['stat.ML', 'cs.CV', 'cs.LG']",2020-02-04 09:23:25+00:00
http://arxiv.org/abs/2002.01171v2,Towards a Fast Steady-State Visual Evoked Potentials (SSVEP) Brain-Computer Interface (BCI),"Steady-state visual evoked potentials (SSVEP) brain-computer interface (BCI)
provides reliable responses leading to high accuracy and information
throughput. But achieving high accuracy typically requires a relatively long
time window of one second or more. Various methods were proposed to improve
sub-second response accuracy through subject-specific training and calibration.
Substantial performance improvements were achieved with tedious calibration and
subject-specific training; resulting in the user's discomfort. So, we propose a
training-free method by combining spatial-filtering and temporal alignment
(CSTA) to recognize SSVEP responses in sub-second response time. CSTA exploits
linear correlation and non-linear similarity between steady-state responses and
stimulus templates with complementary fusion to achieve desirable performance
improvements. We evaluated the performance of CSTA in terms of accuracy and
Information Transfer Rate (ITR) in comparison with both training-based and
training-free methods using two SSVEP data-sets. We observed that CSTA achieves
the maximum mean accuracy of 97.43$\pm$2.26 % and 85.71$\pm$13.41 % with
four-class and forty-class SSVEP data-sets respectively in sub-second response
time in offline analysis. CSTA yields significantly higher mean performance
(p<0.001) than the training-free method on both data-sets. Compared with
training-based methods, CSTA shows 29.33$\pm$19.65 % higher mean accuracy with
statistically significant differences in time window less than 0.5 s. In longer
time windows, CSTA exhibits either better or comparable performance though not
statistically significantly better than training-based methods. We show that
the proposed method brings advantages of subject-independent SSVEP
classification without requiring training while enabling high target
recognition performance in sub-second response time.","['Aung Aung Phyo Wai', 'Yangsong Zhang', 'Heng Guo', 'Ying Chi', 'Lei Zhang', 'Xian-Sheng Hua', 'Seong Whan Lee', 'Cuntai Guan']","['cs.LG', 'cs.HC', 'eess.SP', 'stat.ML']",2020-02-04 08:48:36+00:00
http://arxiv.org/abs/2002.01169v1,Graph Representation Learning via Graphical Mutual Information Maximization,"The richness in the content of various information networks such as social
networks and communication networks provides the unprecedented potential for
learning high-quality expressive representations without external supervision.
This paper investigates how to preserve and extract the abundant information
from graph-structured data into embedding space in an unsupervised manner. To
this end, we propose a novel concept, Graphical Mutual Information (GMI), to
measure the correlation between input graphs and high-level hidden
representations. GMI generalizes the idea of conventional mutual information
computations from vector space to the graph domain where measuring mutual
information from two aspects of node features and topological structure is
indispensable. GMI exhibits several benefits: First, it is invariant to the
isomorphic transformation of input graphs---an inevitable constraint in many
existing graph representation learning algorithms; Besides, it can be
efficiently estimated and maximized by current mutual information estimation
methods such as MINE; Finally, our theoretical analysis confirms its
correctness and rationality. With the aid of GMI, we develop an unsupervised
learning model trained by maximizing GMI between the input and output of a
graph neural encoder. Considerable experiments on transductive as well as
inductive node classification and link prediction demonstrate that our method
outperforms state-of-the-art unsupervised counterparts, and even sometimes
exceeds the performance of supervised ones.","['Zhen Peng', 'Wenbing Huang', 'Minnan Luo', 'Qinghua Zheng', 'Yu Rong', 'Tingyang Xu', 'Junzhou Huang']","['cs.LG', 'cs.AI', 'stat.ML']",2020-02-04 08:33:49+00:00
http://arxiv.org/abs/2002.01136v1,On Positive-Unlabeled Classification in GAN,"This paper defines a positive and unlabeled classification problem for
standard GANs, which then leads to a novel technique to stabilize the training
of the discriminator in GANs. Traditionally, real data are taken as positive
while generated data are negative. This positive-negative classification
criterion was kept fixed all through the learning process of the discriminator
without considering the gradually improved quality of generated data, even if
they could be more realistic than real data at times. In contrast, it is more
reasonable to treat the generated data as unlabeled, which could be positive or
negative according to their quality. The discriminator is thus a classifier for
this positive and unlabeled classification problem, and we derive a new
Positive-Unlabeled GAN (PUGAN). We theoretically discuss the global optimality
the proposed model will achieve and the equivalent optimization goal.
Empirically, we find that PUGAN can achieve comparable or even better
performance than those sophisticated discriminator stabilization methods.","['Tianyu Guo', 'Chang Xu', 'Jiajun Huang', 'Yunhe Wang', 'Boxin Shi', 'Chao Xu', 'Dacheng Tao']","['cs.LG', 'stat.ML']",2020-02-04 05:59:37+00:00
http://arxiv.org/abs/2002.01129v3,Bayesian Meta-Prior Learning Using Empirical Bayes,"Adding domain knowledge to a learning system is known to improve results. In
multi-parameter Bayesian frameworks, such knowledge is incorporated as a prior.
On the other hand, various model parameters can have different learning rates
in real-world problems, especially with skewed data. Two often-faced challenges
in Operation Management and Management Science applications are the absence of
informative priors, and the inability to control parameter learning rates. In
this study, we propose a hierarchical Empirical Bayes approach that addresses
both challenges, and that can generalize to any Bayesian framework. Our method
learns empirical meta-priors from the data itself and uses them to decouple the
learning rates of first-order and second-order features (or any other given
feature grouping) in a Generalized Linear Model. As the first-order features
are likely to have a more pronounced effect on the outcome, focusing on
learning first-order weights first is likely to improve performance and
convergence time. Our Empirical Bayes method clamps features in each group
together and uses the deployed model's observed data to empirically compute a
hierarchical prior in hindsight. We report theoretical results for the
unbiasedness, strong consistency, and optimal frequentist cumulative regret
properties of our meta-prior variance estimator. We apply our method to a
standard supervised learning optimization problem, as well as an online
combinatorial optimization problem in a contextual bandit setting implemented
in an Amazon production system. Both during simulations and live experiments,
our method shows marked improvements, especially in cases of small traffic. Our
findings are promising, as optimizing over sparse data is often a challenge.","['Sareh Nabi', 'Houssam Nassif', 'Joseph Hong', 'Hamed Mamani', 'Guido Imbens']","['cs.LG', 'stat.ML']",2020-02-04 05:08:17+00:00
http://arxiv.org/abs/2002.01119v1,Improving Efficiency in Large-Scale Decentralized Distributed Training,"Decentralized Parallel SGD (D-PSGD) and its asynchronous variant Asynchronous
Parallel SGD (AD-PSGD) is a family of distributed learning algorithms that have
been demonstrated to perform well for large-scale deep learning tasks. One
drawback of (A)D-PSGD is that the spectral gap of the mixing matrix decreases
when the number of learners in the system increases, which hampers convergence.
In this paper, we investigate techniques to accelerate (A)D-PSGD based training
by improving the spectral gap while minimizing the communication cost. We
demonstrate the effectiveness of our proposed techniques by running experiments
on the 2000-hour Switchboard speech recognition task and the ImageNet computer
vision task. On an IBM P9 supercomputer, our system is able to train an LSTM
acoustic model in 2.28 hours with 7.5% WER on the Hub5-2000 Switchboard (SWB)
test set and 13.3% WER on the CallHome (CH) test set using 64 V100 GPUs and in
1.98 hours with 7.7% WER on SWB and 13.3% WER on CH using 128 V100 GPUs, the
fastest training time reported to date.","['Wei Zhang', 'Xiaodong Cui', 'Abdullah Kayi', 'Mingrui Liu', 'Ulrich Finkler', 'Brian Kingsbury', 'George Saon', 'Youssef Mroueh', 'Alper Buyuktosunoglu', 'Payel Das', 'David Kung', 'Michael Picheny']","['cs.LG', 'cs.DC', 'stat.ML']",2020-02-04 04:29:09+00:00
http://arxiv.org/abs/2002.01113v1,Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform,"Strictly enforcing orthonormality constraints on parameter matrices has been
shown advantageous in deep learning. This amounts to Riemannian optimization on
the Stiefel manifold, which, however, is computationally expensive. To address
this challenge, we present two main contributions: (1) A new efficient
retraction map based on an iterative Cayley transform for optimization updates,
and (2) An implicit vector transport mechanism based on the combination of a
projection of the momentum and the Cayley transform on the Stiefel manifold. We
specify two new optimization algorithms: Cayley SGD with momentum, and Cayley
ADAM on the Stiefel manifold. Convergence of Cayley SGD is theoretically
analyzed. Our experiments for CNN training demonstrate that both algorithms:
(a) Use less running time per iteration relative to existing approaches that
enforce orthonormality of CNN parameters; and (b) Achieve faster convergence
rates than the baseline SGD and ADAM algorithms without compromising the
performance of the CNN. Cayley SGD and Cayley ADAM are also shown to reduce the
training time for optimizing the unitary transition matrices in RNNs.","['Jun Li', 'Li Fuxin', 'Sinisa Todorovic']","['cs.LG', 'stat.ML']",2020-02-04 04:01:51+00:00
http://arxiv.org/abs/2002.01100v1,"Efficient, Noise-Tolerant, and Private Learning via Boosting","We introduce a simple framework for designing private boosting algorithms. We
give natural conditions under which these algorithms are differentially
private, efficient, and noise-tolerant PAC learners. To demonstrate our
framework, we use it to construct noise-tolerant and private PAC learners for
large-margin halfspaces whose sample complexity does not depend on the
dimension.
  We give two sample complexity bounds for our large-margin halfspace learner.
One bound is based only on differential privacy, and uses this guarantee as an
asset for ensuring generalization. This first bound illustrates a general
methodology for obtaining PAC learners from privacy, which may be of
independent interest. The second bound uses standard techniques from the theory
of large-margin classification (the fat-shattering dimension) to match the best
known sample complexity for differentially private learning of large-margin
halfspaces, while additionally tolerating random label noise.","['Mark Bun', 'Marco Leandro Carmosino', 'Jessica Sorrell']","['cs.LG', 'stat.ML']",2020-02-04 03:16:37+00:00
http://arxiv.org/abs/2002.02779v2,Generating Digital Twins with Multiple Sclerosis Using Probabilistic Neural Networks,"Multiple Sclerosis (MS) is a neurodegenerative disorder characterized by a
complex set of clinical assessments. We use an unsupervised machine learning
model called a Conditional Restricted Boltzmann Machine (CRBM) to learn the
relationships between covariates commonly used to characterize subjects and
their disease progression in MS clinical trials. A CRBM is capable of
generating digital twins, which are simulated subjects having the same baseline
data as actual subjects. Digital twins allow for subject-level statistical
analyses of disease progression. The CRBM is trained using data from 2395
subjects enrolled in the placebo arms of clinical trials across the three
primary subtypes of MS. We discuss how CRBMs are trained and show that digital
twins generated by the model are statistically indistinguishable from their
actual subject counterparts along a number of measures.","['Jonathan R. Walsh', 'Aaron M. Smith', 'Yannick Pouliot', 'David Li-Bland', 'Anton Loukianov', 'Charles K. Fisher']","['stat.ML', 'cs.LG', 'q-bio.QM']",2020-02-04 02:57:08+00:00
http://arxiv.org/abs/2002.01093v2,On the interaction between supervision and self-play in emergent communication,"A promising approach for teaching artificial agents to use natural language
involves using human-in-the-loop training. However, recent work suggests that
current machine learning methods are too data inefficient to be trained in this
way from scratch. In this paper, we investigate the relationship between two
categories of learning signals with the ultimate goal of improving sample
efficiency: imitating human language data via supervised learning, and
maximizing reward in a simulated multi-agent environment via self-play (as done
in emergent communication), and introduce the term supervised self-play (S2P)
for algorithms using both of these signals. We find that first training agents
via supervised learning on human data followed by self-play outperforms the
converse, suggesting that it is not beneficial to emerge languages from
scratch. We then empirically investigate various S2P schedules that begin with
supervised learning in two environments: a Lewis signaling game with symbolic
inputs, and an image-based referential game with natural language descriptions.
Lastly, we introduce population based approaches to S2P, which further improves
the performance over single-agent methods.","['Ryan Lowe', 'Abhinav Gupta', 'Jakob Foerster', 'Douwe Kiela', 'Joelle Pineau']","['cs.CL', 'cs.AI', 'cs.LG', 'cs.MA', 'stat.ML']",2020-02-04 02:35:19+00:00
http://arxiv.org/abs/2002.05646v3,Adversarial Machine Learning -- Industry Perspectives,"Based on interviews with 28 organizations, we found that industry
practitioners are not equipped with tactical and strategic tools to protect,
detect and respond to attacks on their Machine Learning (ML) systems. We
leverage the insights from the interviews and we enumerate the gaps in
perspective in securing machine learning systems when viewed in the context of
traditional software security development. We write this paper from the
perspective of two personas: developers/ML engineers and security incident
responders who are tasked with securing ML systems as they are designed,
developed and deployed ML systems. The goal of this paper is to engage
researchers to revise and amend the Security Development Lifecycle for
industrial-grade software in the adversarial ML era.","['Ram Shankar Siva Kumar', 'Magnus Nyström', 'John Lambert', 'Andrew Marshall', 'Mario Goertzel', 'Andi Comissoneru', 'Matt Swann', 'Sharon Xia']","['cs.CY', 'cs.CR', 'cs.LG', 'stat.ML']",2020-02-04 02:28:34+00:00
http://arxiv.org/abs/2002.02804v4,A deep learning approach for the computation of curvature in the level-set method,"We propose a deep learning strategy to estimate the mean curvature of
two-dimensional implicit interfaces in the level-set method. Our approach is
based on fitting feed-forward neural networks to synthetic data sets
constructed from circular interfaces immersed in uniform grids of various
resolutions. These multilayer perceptrons process the level-set values from
mesh points next to the free boundary and output the dimensionless curvature at
their closest locations on the interface. Accuracy analyses involving irregular
interfaces, in both uniform and adaptive grids, show that our models are
competitive with traditional numerical schemes in the $L^1$ and $L^2$ norms. In
particular, our neural networks approximate curvature with comparable precision
in coarse resolutions, when the interface features steep curvature regions, and
when the number of iterations to reinitialize the level-set function is small.
Although the conventional numerical approach is more robust than our framework,
our results have unveiled the potential of machine learning for dealing with
computational tasks where the level-set method is known to experience
difficulties. We also establish that an application-dependent map of local
resolutions to neural models can be devised to estimate mean curvature more
effectively than a universal neural network.","['Luis Ángel Larios-Cárdenas', 'Frederic Gibou']","['math.NA', 'cs.LG', 'cs.NA', 'stat.ML', '68T99 (primary), 65Z05 (secondary), 65N06', 'I.2.6; G.1.8']",2020-02-04 00:49:47+00:00
http://arxiv.org/abs/2002.01066v2,On the Sample Complexity and Optimization Landscape for Quadratic Feasibility Problems,"We consider the problem of recovering a complex vector $\mathbf{x}\in
\mathbb{C}^n$ from $m$ quadratic measurements $\{\langle A_i\mathbf{x},
\mathbf{x}\rangle\}_{i=1}^m$. This problem, known as quadratic feasibility,
encompasses the well known phase retrieval problem and has applications in a
wide range of important areas including power system state estimation and x-ray
crystallography. In general, not only is the the quadratic feasibility problem
NP-hard to solve, but it may in fact be unidentifiable. In this paper, we
establish conditions under which this problem becomes {identifiable}, and
further prove isometry properties in the case when the matrices
$\{A_i\}_{i=1}^m$ are Hermitian matrices sampled from a complex Gaussian
distribution. Moreover, we explore a nonconvex {optimization} formulation of
this problem, and establish salient features of the associated optimization
landscape that enables gradient algorithms with an arbitrary initialization to
converge to a \emph{globally optimal} point with a high probability. Our
results also reveal sample complexity requirements for successfully identifying
a feasible solution in these contexts.","['Parth Thaker', 'Gautam Dasarathy', 'Angelia Nedić']","['eess.SP', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC', 'stat.ML']",2020-02-04 00:35:09+00:00
http://arxiv.org/abs/2002.01060v1,Transfer Learning for HVAC System Fault Detection,"Faults in HVAC systems degrade thermal comfort and energy efficiency in
buildings and have received significant attention from the research community,
with data driven methods gaining in popularity. Yet the lack of labeled data,
such as normal versus faulty operational status, has slowed the application of
machine learning to HVAC systems. In addition, for any particular building,
there may be an insufficient number of observed faults over a reasonable amount
of time for training. To overcome these challenges, we present a transfer
methodology for a novel Bayesian classifier designed to distinguish between
normal operations and faulty operations. The key is to train this classifier on
a building with a large amount of sensor and fault data (for example, via
simulation or standard test data) then transfer the classifier to a new
building using a small amount of normal operations data from the new building.
We demonstrate a proof-of-concept for transferring a classifier between
architecturally similar buildings in different climates and show few samples
are required to maintain classification precision and recall.","['Chase P. Dowling', 'Baosen Zhang']","['cs.LG', 'stat.ML']",2020-02-04 00:06:48+00:00
http://arxiv.org/abs/2002.02007v2,Defending Adversarial Attacks via Semantic Feature Manipulation,"Machine learning models have demonstrated vulnerability to adversarial
attacks, more specifically misclassification of adversarial examples. In this
paper, we propose a one-off and attack-agnostic Feature Manipulation
(FM)-Defense to detect and purify adversarial examples in an interpretable and
efficient manner. The intuition is that the classification result of a normal
image is generally resistant to non-significant intrinsic feature changes,
e.g., varying thickness of handwritten digits. In contrast, adversarial
examples are sensitive to such changes since the perturbation lacks
transferability. To enable manipulation of features, a combo-variational
autoencoder is applied to learn disentangled latent codes that reveal semantic
features. The resistance to classification change over the morphs, derived by
varying and reconstructing latent codes, is used to detect suspicious inputs.
Further, combo-VAE is enhanced to purify the adversarial examples with good
quality by considering both class-shared and class-unique features. We
empirically demonstrate the effectiveness of detection and the quality of
purified instance. Our experiments on three datasets show that FM-Defense can
detect nearly $100\%$ of adversarial examples produced by different
state-of-the-art adversarial attacks. It achieves more than $99\%$ overall
purification accuracy on the suspicious instances that close the manifold of
normal examples.","['Shuo Wang', 'Tianle Chen', 'Surya Nepal', 'Carsten Rudolph', 'Marthie Grobler', 'Shangyu Chen']","['cs.LG', 'cs.CR', 'stat.ML']",2020-02-03 23:24:32+00:00
http://arxiv.org/abs/2002.01044v2,Optimal Confidence Regions for the Multinomial Parameter,"Construction of tight confidence regions and intervals is central to
statistical inference and decision making. This paper develops new theory
showing minimum average volume confidence regions for categorical data. More
precisely, consider an empirical distribution $\widehat{\boldsymbol{p}}$
generated from $n$ iid realizations of a random variable that takes one of $k$
possible values according to an unknown distribution $\boldsymbol{p}$. This is
analogous to a single draw from a multinomial distribution. A confidence region
is a subset of the probability simplex that depends on
$\widehat{\boldsymbol{p}}$ and contains the unknown $\boldsymbol{p}$ with a
specified confidence. This paper shows how one can construct minimum average
volume confidence regions, answering a long standing question. We also show the
optimality of the regions directly translates to optimal confidence intervals
of linear functionals such as the mean, implying sample complexity and regret
improvements for adaptive machine learning algorithms.","['Matthew L. Malloy', 'Ardhendu Tripathy', 'Robert D. Nowak']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",2020-02-03 23:00:16+00:00
http://arxiv.org/abs/2002.03763v2,Learning Numerical Observers using Unsupervised Domain Adaptation,"Medical imaging systems are commonly assessed by use of objective image
quality measures. Supervised deep learning methods have been investigated to
implement numerical observers for task-based image quality assessment. However,
labeling large amounts of experimental data to train deep neural networks is
tedious, expensive, and prone to subjective errors. Computer-simulated image
data can potentially be employed to circumvent these issues; however, it is
often difficult to computationally model complicated anatomical structures,
noise sources, and the response of real world imaging systems. Hence, simulated
image data will generally possess physical and statistical differences from the
experimental image data they seek to emulate. Within the context of machine
learning, these differences between the sets of two images is referred to as
domain shift. In this study, we propose and investigate the use of an
adversarial domain adaptation method to mitigate the deleterious effects of
domain shift between simulated and experimental image data for deep
learning-based numerical observers (DL-NOs) that are trained on simulated
images but applied to experimental ones. In the proposed method, a DL-NO will
initially be trained on computer-simulated image data and subsequently adapted
for use with experimental image data, without the need for any labeled
experimental images. As a proof of concept, a binary signal detection task is
considered. The success of this strategy as a function of the degree of domain
shift present between the simulated and experimental image data is
investigated.","['Shenghua He', 'Weimin Zhou', 'Hua Li', 'Mark A. Anastasio']","['cs.CV', 'cs.LG', 'stat.ML']",2020-02-03 22:58:28+00:00
http://arxiv.org/abs/2002.01029v1,PDE-NetGen 1.0: from symbolic PDE representations of physical processes to trainable neural network representations,"Bridging physics and deep learning is a topical challenge. While deep
learning frameworks open avenues in physical science, the design of
physically-consistent deep neural network architectures is an open issue. In
the spirit of physics-informed NNs, PDE-NetGen package provides new means to
automatically translate physical equations, given as PDEs, into neural network
architectures. PDE-NetGen combines symbolic calculus and a neural network
generator. The later exploits NN-based implementations of PDE solvers using
Keras. With some knowledge of a problem, PDE-NetGen is a plug-and-play tool to
generate physics-informed NN architectures. They provide
computationally-efficient yet compact representations to address a variety of
issues, including among others adjoint derivation, model calibration,
forecasting, data assimilation as well as uncertainty quantification. As an
illustration, the workflow is first presented for the 2D diffusion equation,
then applied to the data-driven and physics-informed identification of
uncertainty dynamics for the Burgers equation.","['Olivier Pannekoucke', 'Ronan Fablet']","['physics.comp-ph', 'cs.LG', 'stat.ML']",2020-02-03 22:11:13+00:00
http://arxiv.org/abs/2002.01020v1,Bending Loss Regularized Network for Nuclei Segmentation in Histopathology Images,"Separating overlapped nuclei is a major challenge in histopathology image
analysis. Recently published approaches have achieved promising overall
performance on public datasets; however, their performance in segmenting
overlapped nuclei are limited. To address the issue, we propose the bending
loss regularized network for nuclei segmentation. The proposed bending loss
defines high penalties to contour points with large curvatures, and applies
small penalties to contour points with small curvature. Minimizing the bending
loss can avoid generating contours that encompass multiple nuclei. The proposed
approach is validated on the MoNuSeg dataset using five quantitative metrics.
It outperforms six state-of-the-art approaches on the following metrics:
Aggregate Jaccard Index, Dice, Recognition Quality, and Pan-optic Quality.","['Haotian Wang', 'Min Xian', 'Aleksandar Vakanski']","['eess.IV', 'cs.LG', 'stat.ML']",2020-02-03 21:20:50+00:00
http://arxiv.org/abs/2002.00995v1,Learning from Noisy Similar and Dissimilar Data,"With the widespread use of machine learning for classification, it becomes
increasingly important to be able to use weaker kinds of supervision for tasks
in which it is hard to obtain standard labeled data. One such kind of
supervision is provided pairwise---in the form of Similar (S) pairs (if two
examples belong to the same class) and Dissimilar (D) pairs (if two examples
belong to different classes). This kind of supervision is realistic in
privacy-sensitive domains. Although this problem has been looked at recently,
it is unclear how to learn from such supervision under label noise, which is
very common when the supervision is crowd-sourced. In this paper, we close this
gap and demonstrate how to learn a classifier from noisy S and D labeled data.
We perform a detailed investigation of this problem under two realistic noise
models and propose two algorithms to learn from noisy S-D data. We also show
important connections between learning from such pairwise supervision data and
learning from ordinary class-labeled data. Finally, we perform experiments on
synthetic and real world datasets and show our noise-informed algorithms
outperform noise-blind baselines in learning from noisy pairwise data.","['Soham Dan', 'Han Bao', 'Masashi Sugiyama']","['cs.LG', 'stat.ML']",2020-02-03 19:59:16+00:00
http://arxiv.org/abs/2002.02533v1,Understanding the dynamics of message passing algorithms: a free probability heuristics,"We use freeness assumptions of random matrix theory to analyze the dynamical
behavior of inference algorithms for probabilistic models with dense coupling
matrices in the limit of large systems. For a toy Ising model, we are able to
recover previous results such as the property of vanishing effective memories
and the analytical convergence rate of the algorithm.","['Manfred Opper', 'Burak Çakmak']","['cond-mat.stat-mech', 'cond-mat.dis-nn', 'cs.LG', 'stat.ML']",2020-02-03 19:50:31+00:00
http://arxiv.org/abs/2002.01891v1,A neural network model that learns differences in diagnosis strategies among radiologists has an improved area under the curve for aneurysm status classification in magnetic resonance angiography image series,"Purpose: To construct a neural network model that can learn the different
diagnosing strategies of radiologists to better classify aneurysm status in
magnetic resonance angiography images. Materials and methods: This
retrospective study included 3423 time-of-flight brain magnetic resonance
angiography image series (subjects: male 1843 [mean age, 50.2 +/- 11.7 years],
female 1580 [50.8 +/- 11.3 years]) recorded from November 2017 through January
2019. The image series were read independently for aneurysm status by one of
four board-certified radiologists, who were assisted by an established deep
learning-based computer-assisted diagnosis (CAD) system. The constructed neural
networks were trained to classify the aneurysm status of zero to five
aneurysm-suspicious areas suggested by the CAD system for each image series,
and any additional aneurysm areas added by the radiologists, and this
classification was compared with the judgment of the annotating radiologist.
Image series were randomly allocated to training and testing data in an 8:2
ratio. The accuracy of the classification was compared by receiver operating
characteristic analysis between the control model that accepted only image data
as input and the proposed model that additionally accepted the information of
who the annotating radiologist was. The DeLong test was used to compare areas
under the curves (P < 0.05 was considered significant). Results: The area under
the curve was larger in the proposed model (0.845) than in the control model
(0.793), and the difference was significant (P < 0.0001). Conclusion: The
proposed model improved classification accuracy by learning the diagnosis
strategies of individual annotating radiologists.","['Yasuhiko Tachibana', 'Masataka Nishimori', 'Naoyuki Kitamura', 'Kensuke Umehara', 'Junko Ota', 'Takayuki Obata', 'Tatsuya Higashi']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2020-02-03 19:19:57+00:00
http://arxiv.org/abs/2002.00941v2,Quantifying Hypothesis Space Misspecification in Learning from Human-Robot Demonstrations and Physical Corrections,"Human input has enabled autonomous systems to improve their capabilities and
achieve complex behaviors that are otherwise challenging to generate
automatically. Recent work focuses on how robots can use such input - like
demonstrations or corrections - to learn intended objectives. These techniques
assume that the human's desired objective already exists within the robot's
hypothesis space. In reality, this assumption is often inaccurate: there will
always be situations where the person might care about aspects of the task that
the robot does not know about. Without this knowledge, the robot cannot infer
the correct objective. Hence, when the robot's hypothesis space is
misspecified, even methods that keep track of uncertainty over the objective
fail because they reason about which hypothesis might be correct, and not
whether any of the hypotheses are correct. In this paper, we posit that the
robot should reason explicitly about how well it can explain human inputs given
its hypothesis space and use that situational confidence to inform how it
should incorporate human input. We demonstrate our method on a 7
degree-of-freedom robot manipulator in learning from two important types of
human input: demonstrations of manipulation tasks, and physical corrections
during the robot's task execution.","['Andreea Bobu', 'Andrea Bajcsy', 'Jaime F. Fisac', 'Sampada Deglurkar', 'Anca D. Dragan']","['cs.RO', 'cs.AI', 'cs.HC', 'cs.LG', 'stat.ML']",2020-02-03 18:59:23+00:00
http://arxiv.org/abs/2002.00937v1,Radioactive data: tracing through training,"We want to detect whether a particular image dataset has been used to train a
model. We propose a new technique, \emph{radioactive data}, that makes
imperceptible changes to this dataset such that any model trained on it will
bear an identifiable mark. The mark is robust to strong variations such as
different architectures or optimization methods. Given a trained model, our
technique detects the use of radioactive data and provides a level of
confidence (p-value). Our experiments on large-scale benchmarks (Imagenet),
using standard architectures (Resnet-18, VGG-16, Densenet-121) and training
procedures, show that we can detect usage of radioactive data with high
confidence (p<10^-4) even when only 1% of the data used to trained our model is
radioactive. Our method is robust to data augmentation and the stochasticity of
deep network optimization. As a result, it offers a much higher signal-to-noise
ratio than data poisoning and backdoor methods.","['Alexandre Sablayrolles', 'Matthijs Douze', 'Cordelia Schmid', 'Hervé Jégou']","['stat.ML', 'cs.CR', 'cs.CV', 'cs.LG']",2020-02-03 18:41:08+00:00
http://arxiv.org/abs/2002.00915v2,Complexity Guarantees for Polyak Steps with Momentum,"In smooth strongly convex optimization, knowledge of the strong convexity
parameter is critical for obtaining simple methods with accelerated rates. In
this work, we study a class of methods, based on Polyak steps, where this
knowledge is substituted by that of the optimal value, $f_*$. We first show
slightly improved convergence bounds than previously known for the classical
case of simple gradient descent with Polyak steps, we then derive an
accelerated gradient method with Polyak steps and momentum, along with
convergence guarantees.","['Mathieu Barré', 'Adrien Taylor', ""Alexandre d'Aspremont""]","['math.OC', 'cs.NA', 'math.NA', 'stat.ML']",2020-02-03 17:50:28+00:00
http://arxiv.org/abs/2002.11192v1,End-to-End Models for the Analysis of System 1 and System 2 Interactions based on Eye-Tracking Data,"While theories postulating a dual cognitive system take hold, quantitative
confirmations are still needed to understand and identify interactions between
the two systems or conflict events. Eye movements are among the most direct
markers of the individual attentive load and may serve as an important proxy of
information. In this work we propose a computational method, within a modified
visual version of the well-known Stroop test, for the identification of
different tasks and potential conflicts events between the two systems through
the collection and processing of data related to eye movements. A statistical
analysis shows that the selected variables can characterize the variation of
attentive load within different scenarios. Moreover, we show that Machine
Learning techniques allow to distinguish between different tasks with a good
classification accuracy and to investigate more in depth the gaze dynamics.","['Alessandro Rossi', 'Sara Ermini', 'Dario Bernabini', 'Dario Zanca', 'Marino Todisco', 'Alessandro Genovese', 'Antonio Rizzo']","['q-bio.NC', 'cs.CV', 'cs.LG', 'stat.ML']",2020-02-03 17:46:13+00:00
http://arxiv.org/abs/2002.00909v1,Towards Explainable Bit Error Tolerance of Resistive RAM-Based Binarized Neural Networks,"Non-volatile memory, such as resistive RAM (RRAM), is an emerging
energy-efficient storage, especially for low-power machine learning models on
the edge. It is reported, however, that the bit error rate of RRAMs can be up
to 3.3% in the ultra low-power setting, which might be crucial for many use
cases. Binary neural networks (BNNs), a resource efficient variant of neural
networks (NNs), can tolerate a certain percentage of errors without a loss in
accuracy and demand lower resources in computation and storage. The bit error
tolerance (BET) in BNNs can be achieved by flipping the weight signs during
training, as proposed by Hirtzlin et al., but their method has a significant
drawback, especially for fully connected neural networks (FCNN): The FCNNs
overfit to the error rate used in training, which leads to low accuracy under
lower error rates. In addition, the underlying principles of BET are not
investigated. In this work, we improve the training for BET of BNNs and aim to
explain this property. We propose straight-through gradient approximation to
improve the weight-sign-flip training, by which BNNs adapt less to the bit
error rates. To explain the achieved robustness, we define a metric that aims
to measure BET without fault injection. We evaluate the metric and find that it
correlates with accuracy over error rate for all FCNNs tested. Finally, we
explore the influence of a novel regularizer that optimizes with respect to
this metric, with the aim of providing a configurable trade-off in accuracy and
BET.","['Sebastian Buschjäger', 'Jian-Jia Chen', 'Kuan-Hsun Chen', 'Mario Günzel', 'Christian Hakert', 'Katharina Morik', 'Rodion Novkin', 'Lukas Pfahler', 'Mikail Yayla']","['cs.LG', 'cs.ET', 'stat.ML', '68T05', 'I.2.6; B.7.1']",2020-02-03 17:38:45+00:00
http://arxiv.org/abs/2002.00897v1,Modular Simulation Framework for Process Variation Analysis of MRAM-based Deep Belief Networks,"Magnetic Random-Access Memory (MRAM) based p-bit neuromorphic computing
devices are garnering increasing interest as a means to compactly and
efficiently realize machine learning operations in Restricted Boltzmann
Machines (RBMs). When embedded within an RBM resistive crossbar array, the
p-bit based neuron realizes a tunable sigmoidal activation function. Since the
stochasticity of activation is dependent on the energy barrier of the MRAM
device, it is essential to assess the impact of process variation on the
voltage-dependent behavior of the sigmoid function. Other influential
performance factors arise from varying energy barriers on power consumption
requiring a simulation environment to facilitate the multi-objective
optimization of device and network parameters. Herein, transportable Python
scripts are developed to analyze the output variation under changes in device
dimensions on the accuracy of machine learning applications. Evaluation with
RBM circuits using the MNIST dataset reveal impacts and limits for processing
variation of device fabrication in terms of the resulting energy vs. accuracy
tradeoffs, and the resulting simulation framework is available via a Creative
Commons license.","['Paul Wood', 'Hossein Pourmeidani', 'Ronald F. DeMara']","['cs.ET', 'cs.LG', 'stat.ML']",2020-02-03 17:20:21+00:00
http://arxiv.org/abs/2002.00952v2,Improved inter-scanner MS lesion segmentation by adversarial training on longitudinal data,"The evaluation of white matter lesion progression is an important biomarker
in the follow-up of MS patients and plays a crucial role when deciding the
course of treatment. Current automated lesion segmentation algorithms are
susceptible to variability in image characteristics related to MRI scanner or
protocol differences. We propose a model that improves the consistency of MS
lesion segmentations in inter-scanner studies. First, we train a CNN base model
to approximate the performance of icobrain, an FDA-approved clinically
available lesion segmentation software. A discriminator model is then trained
to predict if two lesion segmentations are based on scans acquired using the
same scanner type or not, achieving a 78% accuracy in this task. Finally, the
base model and the discriminator are trained adversarially on multi-scanner
longitudinal data to improve the inter-scanner consistency of the base model.
The performance of the models is evaluated on an unseen dataset containing
manual delineations. The inter-scanner variability is evaluated on test-retest
data, where the adversarial network produces improved results over the base
model and the FDA-approved solution.","['Mattias Billast', 'Maria Ines Meyer', 'Diana M. Sima', 'David Robben']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2020-02-03 16:56:05+00:00
http://arxiv.org/abs/2002.00876v1,Torch-Struct: Deep Structured Prediction Library,"The literature on structured prediction for NLP describes a rich collection
of distributions and algorithms over sequences, segmentations, alignments, and
trees; however, these algorithms are difficult to utilize in deep learning
frameworks. We introduce Torch-Struct, a library for structured prediction
designed to take advantage of and integrate with vectorized,
auto-differentiation based frameworks. Torch-Struct includes a broad collection
of probabilistic structures accessed through a simple and flexible
distribution-based API that connects to any deep learning model. The library
utilizes batched, vectorized operations and exploits auto-differentiation to
produce readable, fast, and testable code. Internally, we also include a number
of general-purpose optimizations to provide cross-algorithm efficiency.
Experiments show significant performance gains over fast baselines and
case-studies demonstrate the benefits of the library. Torch-Struct is available
at https://github.com/harvardnlp/pytorch-struct.",['Alexander M. Rush'],"['cs.CL', 'cs.NE', 'stat.ML']",2020-02-03 16:43:02+00:00
http://arxiv.org/abs/2002.00874v6,Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes,"Stochastic Approximation (SA) is a popular approach for solving fixed-point
equations where the information is corrupted by noise. In this paper, we
consider an SA involving a contraction mapping with respect to an arbitrary
norm, and show its finite-sample error bounds while using different stepsizes.
The idea is to construct a smooth Lyapunov function using the generalized
Moreau envelope, and show that the iterates of SA have negative drift with
respect to that Lyapunov function. Our result is applicable in Reinforcement
Learning (RL). In particular, we use it to establish the first-known
convergence rate of the V-trace algorithm for off-policy TD-learning. Moreover,
we also use it to study TD-learning in the on-policy setting, and recover the
existing state-of-the-art results for $Q$-learning. Importantly, our
construction results in only a logarithmic dependence of the convergence bound
on the size of the state-space.","['Zaiwei Chen', 'Siva Theja Maguluri', 'Sanjay Shakkottai', 'Karthikeyan Shanmugam']","['cs.LG', 'math.OC', 'stat.ML']",2020-02-03 16:42:01+00:00
http://arxiv.org/abs/2002.00865v3,Designing GANs: A Likelihood Ratio Approach,"We are interested in the design of generative networks. The training of these
mathematical structures is mostly performed with the help of adversarial
(min-max) optimization problems. We propose a simple methodology for
constructing such problems assuring, at the same time, consistency of the
corresponding solution. We give characteristic examples developed by our
method, some of which can be recognized from other applications, and some are
introduced here for the first time. We present a new metric, the likelihood
ratio, that can be employed online to examine the convergence and stability
during the training of different Generative Adversarial Networks (GANs).
Finally, we compare various possibilities by applying them to well-known
datasets using neural networks of different configurations and sizes.","['Kalliopi Basioti', 'George V. Moustakides']","['cs.LG', 'stat.ML']",2020-02-03 16:19:08+00:00
http://arxiv.org/abs/2002.00852v1,The exponentially weighted average forecaster in geodesic spaces of non-positive curvature,"This paper addresses the problem of prediction with expert advice for
outcomes in a geodesic space with non-positive curvature in the sense of
Alexandrov. Via geometric considerations, and in particular the notion of
barycenters, we extend to this setting the definition and analysis of the
classical exponentially weighted average forecaster. We also adapt the
principle of online to batch conversion to this setting. We shortly discuss the
application of these results in the context of aggregation and for the problem
of barycenter estimation.",['Quentin Paris'],"['math.ST', 'cs.GT', 'cs.LG', 'math.MG', 'stat.ML', 'stat.TH']",2020-02-03 15:59:42+00:00
http://arxiv.org/abs/2002.00833v1,Detection of Obstructive Sleep Apnoea Using Features Extracted from Segmented Time-Series ECG Signals Using a One Dimensional Convolutional Neural Network,"The study in this paper presents a one-dimensional convolutional neural
network (1DCNN) model, designed for the automated detection of obstructive
Sleep Apnoea (OSA) captured from single-channel electrocardiogram (ECG)
signals. The system provides mechanisms in clinical practice that help diagnose
patients suffering with OSA. Using the state-of-the-art in 1DCNNs, a model is
constructed using convolutional, max pooling layers and a fully connected
Multilayer Perceptron (MLP) consisting of a hidden layer and SoftMax output for
classification. The 1DCNN extracts prominent features, which are used to train
an MLP. The model is trained using segmented ECG signals grouped into 5 unique
datasets of set window sizes. 35 ECG signal recordings were selected from an
annotated database containing 70 night-time ECG recordings. (Group A = a01 to
a20 (Apnoea breathing), Group B = b01 to b05 (moderate), and Group C = c01 to
c10 (normal). A total of 6514 minutes of Apnoea was recorded. Evaluation of the
model is performed using a set of standard metrics which show the proposed
model achieves high classification results in both training and validation
using our windowing strategy, particularly W=500 (Sensitivity 0.9705,
Specificity 0.9725, F1 Score 0.9717, Kappa Score 0.9430, Log Loss 0.0836,
ROCAUC 0.9945). This demonstrates the model can identify the presence of Apnoea
with a high degree of accuracy.","['Steven Thompson', 'Paul Fergus', 'Carl Chalmers', 'Denis Reilly']","['eess.SP', 'cs.LG', 'stat.ML', 'I.2.0; I.5.1']",2020-02-03 15:47:00+00:00
http://arxiv.org/abs/2002.00819v4,Knowledge Graph Embedding for Link Prediction: A Comparative Analysis,"Knowledge Graphs (KGs) have found many applications in industry and academic
settings, which in turn, have motivated considerable research efforts towards
large-scale information extraction from a variety of sources. Despite such
efforts, it is well known that even state-of-the-art KGs suffer from
incompleteness. Link Prediction (LP), the task of predicting missing facts
among entities already a KG, is a promising and widely studied task aimed at
addressing KG incompleteness. Among the recent LP techniques, those based on KG
embeddings have achieved very promising performances in some benchmarks.
Despite the fast growing literature in the subject, insufficient attention has
been paid to the effect of the various design choices in those methods.
Moreover, the standard practice in this area is to report accuracy by
aggregating over a large number of test facts in which some entities are
over-represented; this allows LP methods to exhibit good performance by just
attending to structural properties that include such entities, while ignoring
the remaining majority of the KG. This analysis provides a comprehensive
comparison of embedding-based LP methods, extending the dimensions of analysis
beyond what is commonly available in the literature. We experimentally compare
effectiveness and efficiency of 16 state-of-the-art methods, consider a
rule-based baseline, and report detailed analysis over the most popular
benchmarks in the literature.","['Andrea Rossi', 'Donatella Firmani', 'Antonio Matinata', 'Paolo Merialdo', 'Denilson Barbosa']","['cs.LG', 'cs.DB', 'stat.ML']",2020-02-03 15:21:25+00:00
http://arxiv.org/abs/2002.00818v3,Linearly Constrained Gaussian Processes with Boundary Conditions,"One goal in Bayesian machine learning is to encode prior knowledge into prior
distributions, to model data efficiently. We consider prior knowledge from
systems of linear partial differential equations together with their boundary
conditions. We construct multi-output Gaussian process priors with realizations
in the solution set of such systems, in particular only such solutions can be
represented by Gaussian process regression. The construction is fully
algorithmic via Gr\""obner bases and it does not employ any approximation. It
builds these priors combining two parametrizations via a pullback: the first
parametrizes the solutions for the system of differential equations and the
second parametrizes all functions adhering to the boundary conditions.",['Markus Lange-Hegermann'],"['cs.LG', 'cs.SC', 'math.AC', 'stat.ML', '13P10, 13P20, 18-04, 47F05, 60G15, 60-08, 62G05, 68T01', 'G.3; I.1.2; I.1.4; I.2.6; I.5.1']",2020-02-03 15:19:03+00:00
http://arxiv.org/abs/2002.00949v1,Profit-oriented sales forecasting: a comparison of forecasting techniques from a business perspective,"Choosing the technique that is the best at forecasting your data, is a
problem that arises in any forecasting application. Decades of research have
resulted into an enormous amount of forecasting methods that stem from
statistics, econometrics and machine learning (ML), which leads to a very
difficult and elaborate choice to make in any forecasting exercise. This paper
aims to facilitate this process for high-level tactical sales forecasts by
comparing a large array of techniques for 35 times series that consist of both
industry data from the Coca-Cola Company and publicly available datasets.
However, instead of solely focusing on the accuracy of the resulting forecasts,
this paper introduces a novel and completely automated profit-driven approach
that takes into account the expected profit that a technique can create during
both the model building and evaluation process. The expected profit function
that is used for this purpose, is easy to understand and adaptable to any
situation by combining forecasting accuracy with business expertise.
Furthermore, we examine the added value of ML techniques, the inclusion of
external factors and the use of seasonal models in order to ascertain which
type of model works best in tactical sales forecasting. Our findings show that
simple seasonal time series models consistently outperform other methodologies
and that the profit-driven approach can lead to selecting a different
forecasting model.","['Tine Van Calster', 'Filip Van den Bossche', 'Bart Baesens', 'Wilfried Lemahieu']","['econ.EM', 'cs.LG', 'stat.ML']",2020-02-03 14:50:24+00:00
http://arxiv.org/abs/2002.00797v3,Stochastic geometry to generalize the Mondrian Process,"The stable under iterated tessellation (STIT) process is a stochastic process
that produces a recursive partition of space with cut directions drawn
independently from a distribution over the sphere. The case of random
axis-aligned cuts is known as the Mondrian process. Random forests and Laplace
kernel approximations built from the Mondrian process have led to efficient
online learning methods and Bayesian optimization. In this work, we utilize
tools from stochastic geometry to resolve some fundamental questions concerning
STIT processes in machine learning. First, we show that a STIT process with cut
directions drawn from a discrete distribution can be efficiently simulated by
lifting to a higher dimensional axis-aligned Mondrian process. Second, we
characterize all possible kernels that stationary STIT processes and their
mixtures can approximate. We also give a uniform convergence rate for the
approximation error of the STIT kernels to the targeted kernels, generalizing
the work of [3] for the Mondrian case. Third, we obtain consistency results for
STIT forests in density estimation and regression. Finally, we give a formula
for the density estimator arising from an infinite STIT random forest. This
allows for precise comparisons between the Mondrian forest, the Mondrian kernel
and the Laplace kernel in density estimation. Our paper calls for further
developments at the novel intersection of stochastic geometry and machine
learning.","[""Eliza O'Reilly"", 'Ngoc Tran']","['stat.ML', 'cs.LG', 'math.PR', '60D05, 62G07']",2020-02-03 14:47:26+00:00
http://arxiv.org/abs/2002.00784v2,Elaborating on Learned Demonstrations with Temporal Logic Specifications,"Most current methods for learning from demonstrations assume that those
demonstrations alone are sufficient to learn the underlying task. This is often
untrue, especially if extra safety specifications exist which were not present
in the original demonstrations. In this paper, we allow an expert to elaborate
on their original demonstration with additional specification information using
linear temporal logic (LTL). Our system converts LTL specifications into a
differentiable loss. This loss is then used to learn a dynamic movement
primitive that satisfies the underlying specification, while remaining close to
the original demonstration. Further, by leveraging adversarial training, our
system learns to robustly satisfy the given LTL specification on unseen inputs,
not just those seen in training. We show that our method is expressive enough
to work across a variety of common movement specification patterns such as
obstacle avoidance, patrolling, keeping steady, and speed limitation. In
addition, we show that our system can modify a base demonstration with complex
specifications by incrementally composing multiple simpler specifications. We
also implement our system on a PR-2 robot to show how a demonstrator can start
with an initial (sub-optimal) demonstration, then interactively improve task
success by including additional specifications enforced with our differentiable
LTL loss.","['Craig Innes', 'Subramanian Ramamoorthy']","['cs.LG', 'cs.RO', 'stat.ML']",2020-02-03 14:33:38+00:00
http://arxiv.org/abs/2002.00751v1,Separation of target anatomical structure and occlusions in chest radiographs,"Chest radiographs are commonly performed low-cost exams for screening and
diagnosis. However, radiographs are 2D representations of 3D structures causing
considerable clutter impeding visual inspection and automated image analysis.
Here, we propose a Fully Convolutional Network to suppress, for a specific
task, undesired visual structure from radiographs while retaining the relevant
image information such as lung-parenchyma. The proposed algorithm creates
reconstructed radiographs and ground-truth data from high resolution CT-scans.
Results show that removing visual variation that is irrelevant for a
classification task improves the performance of a classifier when only limited
training data are available. This is particularly relevant because a low number
of ground-truth cases is common in medical imaging.","['Johannes Hofmanninger', 'Sebastian Roehrich', 'Helmut Prosch', 'Georg Langs']","['physics.med-ph', 'cs.CV', 'cs.LG', 'eess.IV', 'stat.ML', 'J.3; I.4.3; I.5']",2020-02-03 14:01:06+00:00
http://arxiv.org/abs/2002.00727v2,Distance Metric Learning for Graph Structured Data,"Graphs are versatile tools for representing structured data. As a result, a
variety of machine learning methods have been studied for graph data analysis.
Although many such learning methods depend on the measurement of differences
between input graphs, defining an appropriate distance metric for graphs
remains a controversial issue. Hence, we propose a supervised distance metric
learning method for the graph classification problem. Our method, named
interpretable graph metric learning (IGML), learns discriminative metrics in a
subgraph-based feature space, which has a strong graph representation
capability. By introducing a sparsity-inducing penalty on the weight of each
subgraph, IGML can identify a small number of important subgraphs that can
provide insight into the given classification task. Because our formulation has
a large number of optimization variables, an efficient algorithm that uses
pruning techniques based on safe screening and working set selection methods is
also proposed. An important property of IGML is that solution optimality is
guaranteed because the problem is formulated as a convex problem and our
pruning strategies only discard unnecessary subgraphs. Furthermore, we show
that IGML is also applicable to other structured data such as itemset and
sequence data, and that it can incorporate vertex-label similarity by using a
transportation-based subgraph feature. We empirically evaluate the
computational efficiency and classification performance of IGML on several
benchmark datasets and provide some illustrative examples of how IGML
identifies important subgraphs from a given graph dataset.","['Tomoki Yoshida', 'Ichiro Takeuchi', 'Masayuki Karasuyama']","['stat.ML', 'cs.LG']",2020-02-03 13:42:43+00:00
http://arxiv.org/abs/2002.00721v1,Evolutionary algorithms for constructing an ensemble of decision trees,"Most decision tree induction algorithms are based on a greedy top-down
recursive partitioning strategy for tree growth. In this paper, we propose
several methods for induction of decision trees and their ensembles based on
evolutionary algorithms. The main difference of our approach is using
real-valued vector representation of decision tree that allows to use a large
number of different optimization algorithms, as well as optimize the whole tree
or ensemble for avoiding local optima. Differential evolution and evolution
strategies were chosen as optimization algorithms, as they have good results in
reinforcement learning problems. We test the predictive performance of this
methods using several public UCI data sets, and the proposed methods show
better quality than classical methods.","['Evgeny Dolotov', 'Nikolai Zolotykh']","['cs.NE', 'cs.LG', 'stat.ML']",2020-02-03 13:38:50+00:00
http://arxiv.org/abs/2002.00717v2,Error-feedback stochastic modeling strategy for time series forecasting with convolutional neural networks,"Despite the superiority of convolutional neural networks demonstrated in time
series modeling and forecasting, it has not been fully explored on the design
of the neural network architecture and the tuning of the hyper-parameters.
Inspired by the incremental construction strategy for building a random
multilayer perceptron, we propose a novel Error-feedback Stochastic Modeling
(ESM) strategy to construct a random Convolutional Neural Network (ESM-CNN) for
time series forecasting task, which builds the network architecture adaptively.
The ESM strategy suggests that random filters and neurons of the error-feedback
fully connected layer are incrementally added to steadily compensate the
prediction error during the construction process, and then a filter selection
strategy is introduced to enable ESM-CNN to extract the different size of
temporal features, providing helpful information at each iterative process for
the prediction. The performance of ESM-CNN is justified on its prediction
accuracy of one-step-ahead and multi-step-ahead forecasting tasks respectively.
Comprehensive experiments on both the synthetic and real-world datasets show
that the proposed ESM-CNN not only outperforms the state-of-art random neural
networks, but also exhibits stronger predictive power and less computing
overhead in comparison to trained state-of-art deep neural network models.","['Xinze Zhang', 'Kun He', 'Yukun Bao']","['cs.LG', 'stat.ML']",2020-02-03 13:30:29+00:00
http://arxiv.org/abs/2002.00695v1,FAE: A Fairness-Aware Ensemble Framework,"Automated decision making based on big data and machine learning (ML)
algorithms can result in discriminatory decisions against certain protected
groups defined upon personal data like gender, race, sexual orientation etc.
Such algorithms designed to discover patterns in big data might not only pick
up any encoded societal biases in the training data, but even worse, they might
reinforce such biases resulting in more severe discrimination. The majority of
thus far proposed fairness-aware machine learning approaches focus solely on
the pre-, in- or post-processing steps of the machine learning process, that
is, input data, learning algorithms or derived models, respectively. However,
the fairness problem cannot be isolated to a single step of the ML process.
Rather, discrimination is often a result of complex interactions between big
data and algorithms, and therefore, a more holistic approach is required. The
proposed FAE (Fairness-Aware Ensemble) framework combines fairness-related
interventions at both pre- and postprocessing steps of the data analysis
process. In the preprocessing step, we tackle the problems of
under-representation of the protected group (group imbalance) and of
class-imbalance by generating balanced training samples. In the post-processing
step, we tackle the problem of class overlapping by shifting the decision
boundary in the direction of fairness.","['Vasileios Iosifidis', 'Besnik Fetahu', 'Eirini Ntoutsi']","['cs.AI', 'cs.LG', 'stat.ML']",2020-02-03 13:05:18+00:00
http://arxiv.org/abs/2002.03774v1,Machine Learning Based Channel Modeling for Vehicular Visible Light Communication,"Optical Wireless Communication (OWC) propagation channel characterization
plays a key role on the design and performance analysis of Vehicular Visible
Light Communication (VVLC) systems. Current OWC channel models based on
deterministic and stochastic methods, fail to address mobility induced ambient
light, optical turbulence and road reflection effects on channel
characterization. Therefore, alternative machine learning (ML) based schemes,
considering ambient light, optical turbulence, road reflection effects in
addition to intervehicular distance and geometry, are proposed to obtain
accurate VVLC channel loss and channel frequency response (CFR). This work
demonstrates synthesis of ML based VVLC channel model frameworks through multi
layer perceptron feed-forward neural network (MLP), radial basis function
neural network (RBF-NN) and Random Forest ensemble learning algorithms.
Predictor and response variables, collected through practical road
measurements, are employed to train and validate proposed models for various
conditions. Additionally, the importance of different predictor variables on
channel loss and CFR is assessed, normalized importance of features for
measured VVLC channel is introduced. We show that RBF-NN, Random Forest and MLP
based models yield more accurate channel loss estimations with 3.53 dB, 3.81
dB, 3.95 dB root mean square error (RMSE), respectively, when compared to
fitting curve based VVLC channel model with 7 dB RMSE. Moreover, RBF-NN and MLP
models are demonstrated to predict VVLC CFR with respect to distance, ambient
light and receiver inclination angle predictor variables with 3.78 dB and 3.60
dB RMSE respectively.","['Bugra Turan', 'Sinem Coleri']","['eess.SP', 'cs.LG', 'stat.ML', '94', 'I.m']",2020-02-03 12:38:57+00:00
http://arxiv.org/abs/2002.01427v4,On the impact of selected modern deep-learning techniques to the performance and celerity of classification models in an experimental high-energy physics use case,"Beginning from a basic neural-network architecture, we test the potential
benefits offered by a range of advanced techniques for machine learning, in
particular deep learning, in the context of a typical classification problem
encountered in the domain of high-energy physics, using a well-studied dataset:
the 2014 Higgs ML Kaggle dataset. The advantages are evaluated in terms of both
performance metrics and the time required to train and apply the resulting
models. Techniques examined include domain-specific data-augmentation, learning
rate and momentum scheduling, (advanced) ensembling in both model-space and
weight-space, and alternative architectures and connection methods. Following
the investigation, we arrive at a model which achieves equal performance to the
winning solution of the original Kaggle challenge, whilst being significantly
quicker to train and apply, and being suitable for use with both GPU and CPU
hardware setups. These reductions in timing and hardware requirements
potentially allow the use of more powerful algorithms in HEP analyses, where
models must be retrained frequently, sometimes at short notice, by small groups
of researchers with limited hardware resources. Additionally, a new wrapper
library for PyTorch called LUMIN is presented, which incorporates all of the
techniques studied.",['Giles Chatham Strong'],"['physics.data-an', 'cs.LG', 'hep-ex', 'stat.ML']",2020-02-03 12:29:59+00:00
http://arxiv.org/abs/2002.00658v2,Linear predictor on linearly-generated data with missing values: non consistency and solutions,"We consider building predictors when the data have missing values. We study
the seemingly-simple case where the target to predict is a linear function of
the fully-observed data and we show that, in the presence of missing values,
the optimal predictor may not be linear. In the particular Gaussian case, it
can be written as a linear function of multiway interactions between the
observed data and the various missing-value indicators. Due to its intrinsic
complexity, we study a simple approximation and prove generalization bounds
with finite samples, highlighting regimes for which each method performs best.
We then show that multilayer perceptrons with ReLU activation functions can be
consistent, and can explore good trade-offs between the true model and
approximations. Our study highlights the interesting family of models that are
beneficial to fit with missing values depending on the amount of data
available.","['Marine Le Morvan', 'Nicolas Prost', 'Julie Josse', 'Erwan Scornet', 'Gaël Varoquaux']","['cs.LG', 'cs.AI', 'stat.ML']",2020-02-03 11:49:35+00:00
http://arxiv.org/abs/2002.00655v3,Dynamic Parameter Allocation in Parameter Servers,"To keep up with increasing dataset sizes and model complexity, distributed
training has become a necessity for large machine learning tasks. Parameter
servers ease the implementation of distributed parameter management---a key
concern in distributed training---, but can induce severe communication
overhead. To reduce communication overhead, distributed machine learning
algorithms use techniques to increase parameter access locality (PAL),
achieving up to linear speed-ups. We found that existing parameter servers
provide only limited support for PAL techniques, however, and therefore prevent
efficient training. In this paper, we explore whether and to what extent PAL
techniques can be supported, and whether such support is beneficial. We propose
to integrate dynamic parameter allocation into parameter servers, describe an
efficient implementation of such a parameter server called Lapse, and
experimentally compare its performance to existing parameter servers across a
number of machine learning tasks. We found that Lapse provides near-linear
scaling and can be orders of magnitude faster than existing parameter servers.","['Alexander Renz-Wieland', 'Rainer Gemulla', 'Steffen Zeuch', 'Volker Markl']","['cs.LG', 'cs.DB', 'cs.DC', 'stat.ML']",2020-02-03 11:37:54+00:00
http://arxiv.org/abs/2002.00643v3,Automatic structured variational inference,"Stochastic variational inference offers an attractive option as a default
method for differentiable probabilistic programming. However, the performance
of the variational approach depends on the choice of an appropriate variational
family. Here, we introduce automatic structured variational inference (ASVI), a
fully automated method for constructing structured variational families,
inspired by the closed-form update in conjugate Bayesian models. These
convex-update families incorporate the forward pass of the input probabilistic
program and can therefore capture complex statistical dependencies.
Convex-update families have the same space and time complexity as the input
probabilistic program and are therefore tractable for a very large family of
models including both continuous and discrete variables. We validate our
automatic variational method on a wide range of low- and high-dimensional
inference problems. We find that ASVI provides a clear improvement in
performance when compared with other popular approaches such as the mean-field
approach and inverse autoregressive flows. We provide an open source
implementation of ASVI in TensorFlow Probability.","['Luca Ambrogioni', 'Kate Lin', 'Emily Fertig', 'Sharad Vikram', 'Max Hinne', 'Dave Moore', 'Marcel van Gerven']","['stat.ML', 'cs.LG']",2020-02-03 10:52:30+00:00
http://arxiv.org/abs/2002.00632v3,Effective Diversity in Population Based Reinforcement Learning,"Exploration is a key problem in reinforcement learning, since agents can only
learn from data they acquire in the environment. With that in mind, maintaining
a population of agents is an attractive method, as it allows data be collected
with a diverse set of behaviors. This behavioral diversity is often boosted via
multi-objective loss functions. However, those approaches typically leverage
mean field updates based on pairwise distances, which makes them susceptible to
cycling behaviors and increased redundancy. In addition, explicitly boosting
diversity often has a detrimental impact on optimizing already fruitful
behaviors for rewards. As such, the reward-diversity trade off typically relies
on heuristics. Finally, such methods require behavioral representations, often
handcrafted and domain specific. In this paper, we introduce an approach to
optimize all members of a population simultaneously. Rather than using pairwise
distance, we measure the volume of the entire population in a behavioral
manifold, defined by task-agnostic behavioral embeddings. In addition, our
algorithm Diversity via Determinants (DvD), adapts the degree of diversity
during training using online learning techniques. We introduce both
evolutionary and gradient-based instantiations of DvD and show they effectively
improve exploration without reducing performance when better exploration is not
required.","['Jack Parker-Holder', 'Aldo Pacchiano', 'Krzysztof Choromanski', 'Stephen Roberts']","['cs.LG', 'stat.ML']",2020-02-03 10:09:16+00:00
http://arxiv.org/abs/2002.00585v1,Proving the Lottery Ticket Hypothesis: Pruning is All You Need,"The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a
randomly-initialized network contains a small subnetwork such that, when
trained in isolation, can compete with the performance of the original network.
We prove an even stronger hypothesis (as was also conjectured in Ramanujan et
al., 2019), showing that for every bounded distribution and every target
network with bounded weights, a sufficiently over-parameterized neural network
with random weights contains a subnetwork with roughly the same accuracy as the
target network, without any further training.","['Eran Malach', 'Gilad Yehudai', 'Shai Shalev-Shwartz', 'Ohad Shamir']","['cs.LG', 'stat.ML']",2020-02-03 07:23:11+00:00
http://arxiv.org/abs/2002.00577v2,Prophet: Proactive Candidate-Selection for Federated Learning by Predicting the Qualities of Training and Reporting Phases,"Although the challenge of the device connection is much relieved in 5G
networks, the training latency is still an obstacle preventing Federated
Learning (FL) from being largely adopted. One of the most fundamental problems
that lead to large latency is the bad candidate-selection for FL. In the
dynamic environment, the mobile devices selected by the existing reactive
candidate-selection algorithms very possibly fail to complete the training and
reporting phases of FL, because the FL parameter server only knows the
currently-observed resources of all candidates. To this end, we study the
proactive candidate-selection for FL in this paper. We first let each candidate
device predict the qualities of both its training and reporting phases locally
using LSTM. Then, the proposed candidateselection algorithm is implemented by
the Deep Reinforcement Learning (DRL) framework. Finally, the real-world
trace-driven experiments prove that the proposed approach outperforms the
existing reactive algorithms","['Huawei Huang', 'Kangying Lin', 'Song Guo', 'Pan Zhou', 'Zibin Zheng']","['cs.LG', 'cs.DC', 'stat.ML']",2020-02-03 06:40:04+00:00
http://arxiv.org/abs/2002.00573v1,Revisiting Meta-Learning as Supervised Learning,"Recent years have witnessed an abundance of new publications and approaches
on meta-learning. This community-wide enthusiasm has sparked great insights but
has also created a plethora of seemingly different frameworks, which can be
hard to compare and evaluate. In this paper, we aim to provide a principled,
unifying framework by revisiting and strengthening the connection between
meta-learning and traditional supervised learning. By treating pairs of
task-specific data sets and target models as (feature, label) samples, we can
reduce many meta-learning algorithms to instances of supervised learning. This
view not only unifies meta-learning into an intuitive and practical framework
but also allows us to transfer insights from supervised learning directly to
improve meta-learning. For example, we obtain a better understanding of
generalization properties, and we can readily transfer well-understood
techniques, such as model ensemble, pre-training, joint training, data
augmentation, and even nearest neighbor based methods. We provide an intuitive
analogy of these methods in the context of meta-learning and show that they
give rise to significant improvements in model performance on few-shot
learning.","['Wei-Lun Chao', 'Han-Jia Ye', 'De-Chuan Zhan', 'Mark Campbell', 'Kilian Q. Weinberger']","['cs.LG', 'cs.CV', 'stat.ML']",2020-02-03 06:13:01+00:00
http://arxiv.org/abs/2002.00557v2,Bertrand-DR: Improving Text-to-SQL using a Discriminative Re-ranker,"To access data stored in relational databases, users need to understand the
database schema and write a query using a query language such as SQL. To
simplify this task, text-to-SQL models attempt to translate a user's natural
language question to corresponding SQL query. Recently, several generative
text-to-SQL models have been developed. We propose a novel discriminative
re-ranker to improve the performance of generative text-to-SQL models by
extracting the best SQL query from the beam output predicted by the text-to-SQL
generator, resulting in improved performance in the cases where the best query
was in the candidate list, but not at the top of the list. We build the
re-ranker as a schema agnostic BERT fine-tuned classifier. We analyze relative
strengths of the text-to-SQL and re-ranker models across different query
hardness levels, and suggest how to combine the two models for optimal
performance. We demonstrate the effectiveness of the re-ranker by applying it
to two state-of-the-art text-to-SQL models, and achieve top 4 score on the
Spider leaderboard at the time of writing this article.","['Amol Kelkar', 'Rohan Relan', 'Vaishali Bhardwaj', 'Saurabh Vaichal', 'Chandra Khatri', 'Peter Relan']","['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']",2020-02-03 04:52:47+00:00
http://arxiv.org/abs/2002.00526v3,DANCE: Enhancing saliency maps using decoys,"Saliency methods can make deep neural network predictions more interpretable
by identifying a set of critical features in an input sample, such as pixels
that contribute most strongly to a prediction made by an image classifier.
Unfortunately, recent evidence suggests that many saliency methods poorly
perform, especially in situations where gradients are saturated, inputs contain
adversarial perturbations, or predictions rely upon inter-feature dependence.
To address these issues, we propose a framework that improves the robustness of
saliency methods by following a two-step procedure. First, we introduce a
perturbation mechanism that subtly varies the input sample without changing its
intermediate representations. Using this approach, we can gather a corpus of
perturbed data samples while ensuring that the perturbed and original input
samples follow the same distribution. Second, we compute saliency maps for the
perturbed samples and propose a new method to aggregate saliency maps. With
this design, we offset the gradient saturation influence upon interpretation.
From a theoretical perspective, we show the aggregated saliency map could not
only capture inter-feature dependence but, more importantly, robustify
interpretation against previously described adversarial perturbation methods.
Following our theoretical analysis, we present experimental results suggesting
that, both qualitatively and quantitatively, our saliency method outperforms
existing methods.","['Yang Lu', 'Wenbo Guo', 'Xinyu Xing', 'William Stafford Noble']","['cs.LG', 'stat.ML']",2020-02-03 01:21:48+00:00
http://arxiv.org/abs/2002.00498v2,DYNOTEARS: Structure Learning from Time-Series Data,"We revisit the structure learning problem for dynamic Bayesian networks and
propose a method that simultaneously estimates contemporaneous (intra-slice)
and time-lagged (inter-slice) relationships between variables in a time-series.
Our approach is score-based, and revolves around minimizing a penalized loss
subject to an acyclicity constraint. To solve this problem, we leverage a
recent algebraic result characterizing the acyclicity constraint as a smooth
equality constraint. The resulting algorithm, which we call DYNOTEARS,
outperforms other methods on simulated data, especially in high-dimensions as
the number of variables increases. We also apply this algorithm on real
datasets from two different domains, finance and molecular biology, and analyze
the resulting output. Compared to state-of-the-art methods for learning dynamic
Bayesian networks, our method is both scalable and accurate on real data. The
simple formulation and competitive performance of our method make it suitable
for a variety of problems where one seeks to learn connections between
variables across time.","['Roxana Pamfil', 'Nisara Sriwattanaworachai', 'Shaan Desai', 'Philip Pilgerstorfer', 'Paul Beaumont', 'Konstantinos Georgatzis', 'Bryon Aragam']","['stat.ML', 'cs.LG']",2020-02-02 21:47:48+00:00
http://arxiv.org/abs/2002.00497v2,Accelerating Cooperative Planning for Automated Vehicles with Learned Heuristics and Monte Carlo Tree Search,"Efficient driving in urban traffic scenarios requires foresight. The
observation of other traffic participants and the inference of their possible
next actions depending on the own action is considered cooperative prediction
and planning. Humans are well equipped with the capability to predict the
actions of multiple interacting traffic participants and plan accordingly,
without the need to directly communicate with others. Prior work has shown that
it is possible to achieve effective cooperative planning without the need for
explicit communication. However, the search space for cooperative plans is so
large that most of the computational budget is spent on exploring the search
space in unpromising regions that are far away from the solution. To accelerate
the planning process, we combined learned heuristics with a cooperative
planning method to guide the search towards regions with promising actions,
yielding better solutions at lower computational costs.","['Karl Kurzer', 'Marcus Fechner', 'J. Marius Zöllner']","['cs.LG', 'cs.MA', 'cs.RO', 'stat.ML']",2020-02-02 21:41:35+00:00
http://arxiv.org/abs/2002.00495v2,Active Learning for Identification of Linear Dynamical Systems,"We propose an algorithm to actively estimate the parameters of a linear
dynamical system. Given complete control over the system's input, our algorithm
adaptively chooses the inputs to accelerate estimation. We show a finite time
bound quantifying the estimation rate our algorithm attains and prove matching
upper and lower bounds which guarantee its asymptotic optimality, up to
constants. In addition, we show that this optimal rate is unattainable when
using Gaussian noise to excite the system, even with optimally tuned
covariance, and analyze several examples where our algorithm provably improves
over rates obtained by playing noise. Our analysis critically relies on a novel
result quantifying the error in estimating the parameters of a dynamical system
when arbitrary periodic inputs are being played. We conclude with numerical
examples that illustrate the effectiveness of our algorithm in practice.","['Andrew Wagenmaker', 'Kevin Jamieson']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2020-02-02 21:30:38+00:00
http://arxiv.org/abs/2003.03229v5,Non-linear Neurons with Human-like Apical Dendrite Activations,"In order to classify linearly non-separable data, neurons are typically
organized into multi-layer neural networks that are equipped with at least one
hidden layer. Inspired by some recent discoveries in neuroscience, we propose a
new model of artificial neuron along with a novel activation function enabling
the learning of nonlinear decision boundaries using a single neuron. We show
that a standard neuron followed by our novel apical dendrite activation (ADA)
can learn the XOR logical function with 100% accuracy. Furthermore, we conduct
experiments on six benchmark data sets from computer vision, signal processing
and natural language processing, i.e. MOROCO, UTKFace, CREMA-D, Fashion-MNIST,
Tiny ImageNet and ImageNet, showing that the ADA and the leaky ADA functions
provide superior results to Rectified Linear Units (ReLU), leaky ReLU, RBF and
Swish, for various neural network architectures, e.g. one-hidden-layer or
two-hidden-layer multi-layer perceptrons (MLPs) and convolutional neural
networks (CNNs) such as LeNet, VGG, ResNet and Character-level CNN. We obtain
further performance improvements when we change the standard model of the
neuron with our pyramidal neuron with apical dendrite activations (PyNADA). Our
code is available at: https://github.com/raduionescu/pynada.","['Mariana-Iuliana Georgescu', 'Radu Tudor Ionescu', 'Nicolae-Catalin Ristea', 'Nicu Sebe']","['cs.NE', 'cs.CV', 'cs.LG', 'stat.ML']",2020-02-02 21:09:39+00:00
http://arxiv.org/abs/2002.00492v2,"Overfitting Can Be Harmless for Basis Pursuit, But Only to a Degree","Recently, there have been significant interests in studying the so-called
""double-descent"" of the generalization error of linear regression models under
the overparameterized and overfitting regime, with the hope that such analysis
may provide the first step towards understanding why overparameterized deep
neural networks (DNN) still generalize well. However, to date most of these
studies focused on the min $\ell_2$-norm solution that overfits the data. In
contrast, in this paper we study the overfitting solution that minimizes the
$\ell_1$-norm, which is known as Basis Pursuit (BP) in the compressed sensing
literature. Under a sparse true linear regression model with $p$ i.i.d.
Gaussian features, we show that for a large range of $p$ up to a limit that
grows exponentially with the number of samples $n$, with high probability the
model error of BP is upper bounded by a value that decreases with $p$. To the
best of our knowledge, this is the first analytical result in the literature
establishing the double-descent of overfitting BP for finite $n$ and $p$.
Further, our results reveal significant differences between the double-descent
of BP and min $\ell_2$-norm solutions. Specifically, the double-descent
upper-bound of BP is independent of the signal strength, and for high SNR and
sparse models the descent-floor of BP can be much lower and wider than that of
min $\ell_2$-norm solutions.","['Peizhong Ju', 'Xiaojun Lin', 'Jia Liu']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2020-02-02 20:48:39+00:00
http://arxiv.org/abs/2002.00469v3,WeatherBench: A benchmark dataset for data-driven weather forecasting,"Data-driven approaches, most prominently deep learning, have become powerful
tools for prediction in many domains. A natural question to ask is whether
data-driven methods could also be used to predict global weather patterns days
in advance. First studies show promise but the lack of a common dataset and
evaluation metrics make inter-comparison between studies difficult. Here we
present a benchmark dataset for data-driven medium-range weather forecasting, a
topic of high scientific interest for atmospheric and computer scientists
alike. We provide data derived from the ERA5 archive that has been processed to
facilitate the use in machine learning models. We propose simple and clear
evaluation metrics which will enable a direct comparison between different
methods. Further, we provide baseline scores from simple linear regression
techniques, deep learning models, as well as purely physical forecasting
models. The dataset is publicly available at
https://github.com/pangeo-data/WeatherBench and the companion code is
reproducible with tutorials for getting started. We hope that this dataset will
accelerate research in data-driven weather forecasting.","['Stephan Rasp', 'Peter D. Dueben', 'Sebastian Scher', 'Jonathan A. Weyn', 'Soukayna Mouatadid', 'Nils Thuerey']","['physics.ao-ph', 'stat.ML']",2020-02-02 19:20:46+00:00
http://arxiv.org/abs/2002.00421v2,Choice Set Optimization Under Discrete Choice Models of Group Decisions,"The way that people make choices or exhibit preferences can be strongly
affected by the set of available alternatives, often called the choice set.
Furthermore, there are usually heterogeneous preferences, either at an
individual level within small groups or within sub-populations of large groups.
Given the availability of choice data, there are now many models that capture
this behavior in order to make effective predictions--however, there is little
work in understanding how directly changing the choice set can be used to
influence the preferences of a collection of decision-makers. Here, we use
discrete choice modeling to develop an optimization framework of such
interventions for several problems of group influence, namely maximizing
agreement or disagreement and promoting a particular choice. We show that these
problems are NP-hard in general, but imposing restrictions reveals a
fundamental boundary: promoting a choice can be easier than encouraging
consensus or sowing discord. We design approximation algorithms for the hard
problems and show that they work well on real-world choice data.","['Kiran Tomlinson', 'Austin R. Benson']","['cs.GT', 'cs.LG', 'stat.ML']",2020-02-02 15:59:58+00:00
http://arxiv.org/abs/2002.00416v2,Using Machine Learning for Model Physics: an Overview,"In the overview, a generic mathematical object (mapping) is introduced, and
its relation to model physics parameterization is explained. Machine learning
(ML) tools that can be used to emulate and/or approximate mappings are
introduced. Applications of ML to emulate existing parameterizations, to
develop new parameterizations, to ensure physical constraints, and control the
accuracy of developed applications are described. Some ML approaches that allow
developers to go beyond the standard parameterization paradigm are discussed.","['Vladimir Krasnopolsky', 'Aleksei A. Belochitski']","['physics.ao-ph', 'stat.ML']",2020-02-02 15:29:23+00:00
http://arxiv.org/abs/2002.00413v1,Fast Generating A Large Number of Gumbel-Max Variables,"The well-known Gumbel-Max Trick for sampling elements from a categorical
distribution (or more generally a nonnegative vector) and its variants have
been widely used in areas such as machine learning and information retrieval.
To sample a random element $i$ (or a Gumbel-Max variable $i$) in proportion to
its positive weight $v_i$, the Gumbel-Max Trick first computes a Gumbel random
variable $g_i$ for each positive weight element $i$, and then samples the
element $i$ with the largest value of $g_i+\ln v_i$. Recently, applications
including similarity estimation and graph embedding require to generate $k$
independent Gumbel-Max variables from high dimensional vectors. However, it is
computationally expensive for a large $k$ (e.g., hundreds or even thousands)
when using the traditional Gumbel-Max Trick. To solve this problem, we propose
a novel algorithm, \emph{FastGM}, that reduces the time complexity from
$O(kn^+)$ to $O(k \ln k + n^+)$, where $n^+$ is the number of positive elements
in the vector of interest. Instead of computing $k$ independent Gumbel random
variables directly, we find that there exists a technique to generate these
variables in descending order. Using this technique, our method FastGM computes
variables $g_i+\ln v_i$ for all positive elements $i$ in descending order. As a
result, FastGM significantly reduces the computation time because we can stop
the procedure of Gumbel random variables computing for many elements especially
for those with small weights. Experiments on a variety of real-world datasets
show that FastGM is orders of magnitude faster than state-of-the-art methods
without sacrificing accuracy and incurring additional expenses.","['Yiyan Qi', 'Pinghui Wang', 'Yuanming Zhang', 'Junzhou Zhao', 'Guangjian Tian', 'Xiaohong Guan']","['stat.CO', 'cs.LG', 'stat.ML']",2020-02-02 15:15:44+00:00
http://arxiv.org/abs/2002.00412v1,Combating False Negatives in Adversarial Imitation Learning,"In adversarial imitation learning, a discriminator is trained to
differentiate agent episodes from expert demonstrations representing the
desired behavior. However, as the trained policy learns to be more successful,
the negative examples (the ones produced by the agent) become increasingly
similar to expert ones. Despite the fact that the task is successfully
accomplished in some of the agent's trajectories, the discriminator is trained
to output low values for them. We hypothesize that this inconsistent training
signal for the discriminator can impede its learning, and consequently leads to
worse overall performance of the agent. We show experimental evidence for this
hypothesis and that the 'False Negatives' (i.e. successful agent episodes)
significantly hinder adversarial imitation learning, which is the first
contribution of this paper. Then, we propose a method to alleviate the impact
of false negatives and test it on the BabyAI environment. This method
consistently improves sample efficiency over the baselines by at least an order
of magnitude.","['Konrad Zolna', 'Chitwan Saharia', 'Leonard Boussioux', 'David Yu-Tung Hui', 'Maxime Chevalier-Boisvert', 'Dzmitry Bahdanau', 'Yoshua Bengio']","['cs.LG', 'cs.AI', 'stat.ML']",2020-02-02 14:56:39+00:00
http://arxiv.org/abs/2002.00401v1,Provable Noisy Sparse Subspace Clustering using Greedy Neighbor Selection: A Coherence-Based Perspective,"Sparse subspace clustering (SSC) using greedy-based neighbor selection, such
as matching pursuit (MP) and orthogonal matching pursuit (OMP), has been known
as a popular computationally-efficient alternative to the conventional
L1-minimization based methods. Under deterministic bounded noise corruption, in
this paper we derive coherence-based sufficient conditions guaranteeing correct
neighbor identification using MP/OMP. Our analyses exploit the maximum/minimum
inner product between two noisy data points subject to a known upper bound on
the noise level. The obtained sufficient condition clearly reveals the impact
of noise on greedy-based neighbor recovery. Specifically, it asserts that, as
long as noise is sufficiently small so that the resultant perturbed residual
vectors stay close to the desired subspace, both MP and OMP succeed in
returning a correct neighbor subset. A striking finding is that, when the
ground truth subspaces are well-separated from each other and noise is not
large, MP-based iterations, while enjoying lower algorithmic complexity, yield
smaller perturbation of residuals, thereby better able to identify correct
neighbors and, in turn, achieving higher global data clustering accuracy.
Extensive numerical experiments are used to corroborate our theoretical study.","['Jwo-Yuh Wu', 'Wen-Hsuan Li', 'Liang-Chi Huang', 'Yen-Ping Lin', 'Chun-Hung Liu', 'Rung-Hung Gau']","['stat.ML', 'cs.LG']",2020-02-02 14:28:35+00:00
http://arxiv.org/abs/2002.00372v1,Interpretability of Blackbox Machine Learning Models through Dataview Extraction and Shadow Model creation,"Deep learning models trained using massive amounts of data tend to capture
one view of the data and its associated mapping. Different deep learning models
built on the same training data may capture different views of the data based
on the underlying techniques used. For explaining the decisions arrived by
blackbox deep learning models, we argue that it is essential to reproduce that
model's view of the training data faithfully. This faithful reproduction can
then be used for explanation generation. We investigate two methods for data
view extraction: hill-climbing approach and a GAN-driven approach. We then use
this synthesized data for creating shadow models for explanation generation:
Decision-Tree model and Formal Concept Analysis based model. We evaluate these
approaches on a Blackbox model trained on public datasets and show its
usefulness in explanation generation.","['Rupam Patir', 'Shubham Singhal', 'C. Anantaram', 'Vikram Goyal']","['cs.AI', 'cs.LG', 'stat.ML', 'I.2; I.2.6']",2020-02-02 11:47:15+00:00
http://arxiv.org/abs/2002.02758v1,Neural Machine Translation System of Indic Languages -- An Attention based Approach,"Neural machine translation (NMT) is a recent and effective technique which
led to remarkable improvements in comparison of conventional machine
translation techniques. Proposed neural machine translation model developed for
the Gujarati language contains encoder-decoder with attention mechanism. In
India, almost all the languages are originated from their ancestral language -
Sanskrit. They are having inevitable similarities including lexical and named
entity similarity. Translating into Indic languages is always be a challenging
task. In this paper, we have presented the neural machine translation system
(NMT) that can efficiently translate Indic languages like Hindi and Gujarati
that together covers more than 58.49 percentage of total speakers in the
country. We have compared the performance of our NMT model with automatic
evaluation matrices such as BLEU, perplexity and TER matrix. The comparison of
our network with Google translate is also presented where it outperformed with
a margin of 6 BLEU score on English-Gujarati translation.","['Parth Shah', 'Vishvajit Bakrola']","['cs.CL', 'cs.LG', 'stat.ML']",2020-02-02 07:15:18+00:00
http://arxiv.org/abs/2002.00343v1,SQWA: Stochastic Quantized Weight Averaging for Improving the Generalization Capability of Low-Precision Deep Neural Networks,"Designing a deep neural network (DNN) with good generalization capability is
a complex process especially when the weights are severely quantized. Model
averaging is a promising approach for achieving the good generalization
capability of DNNs, especially when the loss surface for training contains many
sharp minima. We present a new quantized neural network optimization approach,
stochastic quantized weight averaging (SQWA), to design low-precision DNNs with
good generalization capability using model averaging. The proposed approach
includes (1) floating-point model training, (2) direct quantization of weights,
(3) capturing multiple low-precision models during retraining with cyclical
learning rates, (4) averaging the captured models, and (5) re-quantizing the
averaged model and fine-tuning it with low-learning rates. Additionally, we
present a loss-visualization technique on the quantized weight domain to
clearly elucidate the behavior of the proposed method. Visualization results
indicate that a quantized DNN (QDNN) optimized with the proposed approach is
located near the center of the flat minimum in the loss surface. With SQWA
training, we achieved state-of-the-art results for 2-bit QDNNs on CIFAR-100 and
ImageNet datasets. Although we only employed a uniform quantization scheme for
the sake of implementation in VLSI or low-precision neural processing units,
the performance achieved exceeded those of previous studies employing
non-uniform quantization.","['Sungho Shin', 'Yoonho Boo', 'Wonyong Sung']","['cs.LG', 'stat.ML']",2020-02-02 07:02:51+00:00
http://arxiv.org/abs/2002.00329v2,The EM Algorithm gives Sample-Optimality for Learning Mixtures of Well-Separated Gaussians,"We consider the problem of spherical Gaussian Mixture models with $k \geq 3$
components when the components are well separated. A fundamental previous
result established that separation of $\Omega(\sqrt{\log k})$ is necessary and
sufficient for identifiability of the parameters with polynomial sample
complexity (Regev and Vijayaraghavan, 2017). In the same context, we show that
$\tilde{O} (kd/\epsilon^2)$ samples suffice for any $\epsilon \lesssim 1/k$,
closing the gap from polynomial to linear, and thus giving the first optimal
sample upper bound for the parameter estimation of well-separated Gaussian
mixtures. We accomplish this by proving a new result for the
Expectation-Maximization (EM) algorithm: we show that EM converges locally,
under separation $\Omega(\sqrt{\log k})$. The previous best-known guarantee
required $\Omega(\sqrt{k})$ separation (Yan, et al., 2017). Unlike prior work,
our results do not assume or use prior knowledge of the (potentially different)
mixing weights or variances of the Gaussian components. Furthermore, our
results show that the finite-sample error of EM does not depend on
non-universal quantities such as pairwise distances between means of Gaussian
components.","['Jeongyeol Kwon', 'Constantine Caramanis']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2020-02-02 05:09:26+00:00
http://arxiv.org/abs/2002.00315v2,A Closer Look at Small-loss Bounds for Bandits with Graph Feedback,"We study small-loss bounds for adversarial multi-armed bandits with graph
feedback, that is, adaptive regret bounds that depend on the loss of the best
arm or related quantities, instead of the total number of rounds. We derive the
first small-loss bound for general strongly observable graphs, resolving an
open problem of Lykouris et al. (2018). Specifically, we develop an algorithm
with regret $\mathcal{\tilde{O}}(\sqrt{\kappa L_*})$ where $\kappa$ is the
clique partition number and $L_*$ is the loss of the best arm, and for the
special case of self-aware graphs where every arm has a self-loop, we improve
the regret to $\mathcal{\tilde{O}}(\min\{\sqrt{\alpha T}, \sqrt{\kappa L_*}\})$
where $\alpha \leq \kappa$ is the independence number. Our results
significantly improve and extend those by Lykouris et al. (2018) who only
consider self-aware undirected graphs.
  Furthermore, we also take the first attempt at deriving small-loss bounds for
weakly observable graphs. We first prove that no typical small-loss bounds are
achievable in this case, and then propose algorithms with alternative
small-loss bounds in terms of the loss of some specific subset of arms. A
surprising side result is that $\mathcal{\tilde{O}}(\sqrt{T})$ regret is
achievable even for weakly observable graphs as long as the best arm has a
self-loop.
  Our algorithms are based on the Online Mirror Descent framework but require a
suite of novel techniques that might be of independent interest. Moreover, all
our algorithms can be made parameter-free without the knowledge of the
environment.","['Chung-Wei Lee', 'Haipeng Luo', 'Mengxiao Zhang']","['cs.LG', 'stat.ML']",2020-02-02 03:48:01+00:00
http://arxiv.org/abs/2002.00306v3,Brainstorming Generative Adversarial Networks (BGANs): Towards Multi-Agent Generative Models with Distributed Private Datasets,"To achieve a high learning accuracy, generative adversarial networks (GANs)
must be fed by large datasets that adequately represent the data space.
However, in many scenarios, the available datasets may be limited and
distributed across multiple agents, each of which is seeking to learn the
distribution of the data on its own. In such scenarios, the agents often do not
wish to share their local data as it can cause communication overhead for large
datasets. In this paper, to address this multi-agent GAN problem, a novel
brainstorming GAN (BGAN) architecture is proposed using which multiple agents
can generate real-like data samples while operating in a fully distributed
manner. BGAN allows the agents to gain information from other agents without
sharing their real datasets but by ``brainstorming'' via the sharing of their
generated data samples. In contrast to existing distributed GAN solutions, the
proposed BGAN architecture is designed to be fully distributed, and it does not
need any centralized controller. Moreover, BGANs are shown to be scalable and
not dependent on the hyperparameters of the agents' deep neural networks (DNNs)
thus enabling the agents to have different DNN architectures. Theoretically,
the interactions between BGAN agents are analyzed as a game whose unique Nash
equilibrium is derived. Experimental results show that BGAN can generate
real-like data samples with higher quality and lower Jensen-Shannon divergence
(JSD) and Fr\`echet Inception distance (FID) compared to other distributed GAN
architectures.","['Aidin Ferdowsi', 'Walid Saad']","['cs.LG', 'cs.DC', 'stat.ML']",2020-02-02 02:58:32+00:00
http://arxiv.org/abs/2002.00291v3,Oracle Lower Bounds for Stochastic Gradient Sampling Algorithms,"We consider the problem of sampling from a strongly log-concave density in
$\mathbb{R}^d$, and prove an information theoretic lower bound on the number of
stochastic gradient queries of the log density needed. Several popular sampling
algorithms (including many Markov chain Monte Carlo methods) operate by using
stochastic gradients of the log density to generate a sample; our results
establish an information theoretic limit for all these algorithms.
  We show that for every algorithm, there exists a well-conditioned strongly
log-concave target density for which the distribution of points generated by
the algorithm would be at least $\varepsilon$ away from the target in total
variation distance if the number of gradient queries is less than
$\Omega(\sigma^2 d/\varepsilon^2)$, where $\sigma^2 d$ is the variance of the
stochastic gradient. Our lower bound follows by combining the ideas of Le Cam
deficiency routinely used in the comparison of statistical experiments along
with standard information theoretic tools used in lower bounding Bayes risk
functions. To the best of our knowledge our results provide the first
nontrivial dimension-dependent lower bound for this problem.","['Niladri S. Chatterji', 'Peter L. Bartlett', 'Philip M. Long']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2020-02-01 23:46:35+00:00
http://arxiv.org/abs/2002.00288v1,The Sylvester Graphical Lasso (SyGlasso),"This paper introduces the Sylvester graphical lasso (SyGlasso) that captures
multiway dependencies present in tensor-valued data. The model is based on the
Sylvester equation that defines a generative model. The proposed model
complements the tensor graphical lasso (Greenewald et al., 2019) that imposes a
Kronecker sum model for the inverse covariance matrix by providing an
alternative Kronecker sum model that is generative and interpretable. A
nodewise regression approach is adopted for estimating the conditional
independence relationships among variables. The statistical convergence of the
method is established, and empirical studies are provided to demonstrate the
recovery of meaningful conditional dependency graphs. We apply the SyGlasso to
an electroencephalography (EEG) study to compare the brain connectivity of
alcoholic and nonalcoholic subjects. We demonstrate that our model can
simultaneously estimate both the brain connectivity and its temporal
dependencies.","['Yu Wang', 'Byoungwook Jang', 'Alfred Hero']","['stat.ML', 'cs.LG', 'stat.ME']",2020-02-01 22:57:45+00:00
http://arxiv.org/abs/2002.00287v3,Efficient and Robust Algorithms for Adversarial Linear Contextual Bandits,"We consider an adversarial variant of the classic $K$-armed linear contextual
bandit problem where the sequence of loss functions associated with each arm
are allowed to change without restriction over time. Under the assumption that
the $d$-dimensional contexts are generated i.i.d.~at random from a known
distributions, we develop computationally efficient algorithms based on the
classic Exp3 algorithm. Our first algorithm, RealLinExp3, is shown to achieve a
regret guarantee of $\widetilde{O}(\sqrt{KdT})$ over $T$ rounds, which matches
the best available bound for this problem. Our second algorithm, RobustLinExp3,
is shown to be robust to misspecification, in that it achieves a regret bound
of $\widetilde{O}((Kd)^{1/3}T^{2/3}) + \varepsilon \sqrt{d} T$ if the true
reward function is linear up to an additive nonlinear error uniformly bounded
in absolute value by $\varepsilon$. To our knowledge, our performance
guarantees constitute the very first results on this problem setting.","['Gergely Neu', 'Julia Olkhovskaya']","['cs.LG', 'stat.ML']",2020-02-01 22:49:46+00:00
