id,title,abstract,authors,categories,date
http://arxiv.org/abs/1708.05768v1,Data-Driven Tree Transforms and Metrics,"We consider the analysis of high dimensional data given in the form of a
matrix with columns consisting of observations and rows consisting of features.
Often the data is such that the observations do not reside on a regular grid,
and the given order of the features is arbitrary and does not convey a notion
of locality. Therefore, traditional transforms and metrics cannot be used for
data organization and analysis. In this paper, our goal is to organize the data
by defining an appropriate representation and metric such that they respect the
smoothness and structure underlying the data. We also aim to generalize the
joint clustering of observations and features in the case the data does not
fall into clear disjoint groups. For this purpose, we propose multiscale
data-driven transforms and metrics based on trees. Their construction is
implemented in an iterative refinement procedure that exploits the
co-dependencies between features and observations. Beyond the organization of a
single dataset, our approach enables us to transfer the organization learned
from one dataset to another and to integrate several datasets together. We
present an application to breast cancer gene expression analysis: learning
metrics on the genes to cluster the tumor samples into cancer sub-types and
validating the joint organization of both the genes and the samples. We
demonstrate that using our approach to combine information from multiple gene
expression cohorts, acquired by different profiling technologies, improves the
clustering of tumor samples.","['Gal Mishne', 'Ronen Talmon', 'Israel Cohen', 'Ronald R. Coifman', 'Yuval Kluger']","['stat.ML', 'cs.LG', 'q-bio.QM']",2017-08-18 21:32:34+00:00
http://arxiv.org/abs/1708.05757v1,Identification of individual coherent sets associated with flow trajectories using Coherent Structure Coloring,"We present a method for identifying the coherent structures associated with
individual Lagrangian flow trajectories even where only sparse particle
trajectory data is available. The method, based on techniques in spectral graph
theory, uses the Coherent Structure Coloring vector and associated eigenvectors
to analyze the distance in higher-dimensional eigenspace between a selected
reference trajectory and other tracer trajectories in the flow. By analyzing
this distance metric in a hierarchical clustering, the coherent structure of
which the reference particle is a member can be identified. This algorithm is
proven successful in identifying coherent structures of varying complexities in
canonical unsteady flows. Additionally, the method is able to assess the
relative coherence of the associated structure in comparison to the surrounding
flow. Although the method is demonstrated here in the context of fluid flow
kinematics, the generality of the approach allows for its potential application
to other unsupervised clustering problems in dynamical systems such as neuronal
activity, gene expression, or social networks.","['Kristy L. Schlueter-Kuck', 'John O. Dabiri']","['physics.flu-dyn', 'math.DS', 'stat.ML']",2017-08-18 20:35:54+00:00
http://arxiv.org/abs/1708.05715v3,The Stochastic Replica Approach to Machine Learning: Stability and Parameter Optimization,"We introduce a statistical physics inspired supervised machine learning
algorithm for classification and regression problems. The method is based on
the invariances or stability of predicted results when known data is
represented as expansions in terms of various stochastic functions. The
algorithm predicts the classification/regression values of new data by
combining (via voting) the outputs of these numerous linear expansions in
randomly chosen functions. The few parameters (typically only one parameter is
used in all studied examples) that this model has may be automatically
optimized. The algorithm has been tested on 10 diverse training data sets of
various types and feature space dimensions. It has been shown to consistently
exhibit high accuracy and readily allow for optimization of parameters, while
simultaneously avoiding pitfalls of existing algorithms such as those
associated with class imbalance. We very briefly speculate on whether spatial
coordinates in physical theories may be viewed as emergent ""features"" that
enable a robust machine learning type description of data with generic low
order smooth functions.","['Patrick Chao', 'Tahereh Mazaheri', 'Bo Sun', 'Nicholas B. Weingartner', 'Zohar Nussinov']","['stat.ML', 'cond-mat.stat-mech', 'physics.data-an']",2017-08-18 18:00:00+00:00
http://arxiv.org/abs/1708.05629v1,Learning to Transfer,"Transfer learning borrows knowledge from a source domain to facilitate
learning in a target domain. Two primary issues to be addressed in transfer
learning are what and how to transfer. For a pair of domains, adopting
different transfer learning algorithms results in different knowledge
transferred between them. To discover the optimal transfer learning algorithm
that maximally improves the learning performance in the target domain,
researchers have to exhaustively explore all existing transfer learning
algorithms, which is computationally intractable. As a trade-off, a sub-optimal
algorithm is selected, which requires considerable expertise in an ad-hoc way.
Meanwhile, it is widely accepted in educational psychology that human beings
improve transfer learning skills of deciding what to transfer through
meta-cognitive reflection on inductive transfer learning practices. Motivated
by this, we propose a novel transfer learning framework known as Learning to
Transfer (L2T) to automatically determine what and how to transfer are the best
by leveraging previous transfer learning experiences. We establish the L2T
framework in two stages: 1) we first learn a reflection function encrypting
transfer learning skills from experiences; and 2) we infer what and how to
transfer for a newly arrived pair of domains by optimizing the reflection
function. Extensive experiments demonstrate the L2T's superiority over several
state-of-the-art transfer learning algorithms and its effectiveness on
discovering more transferable knowledge.","['Ying Wei', 'Yu Zhang', 'Qiang Yang']","['cs.AI', 'cs.LG', 'stat.ML']",2017-08-18 14:36:29+00:00
http://arxiv.org/abs/1708.09022v1,Deep Convolutional Neural Networks for Raman Spectrum Recognition: A Unified Solution,"Machine learning methods have found many applications in Raman spectroscopy,
especially for the identification of chemical species. However, almost all of
these methods require non-trivial preprocessing such as baseline correction
and/or PCA as an essential step. Here we describe our unified solution for the
identification of chemical species in which a convolutional neural network is
trained to automatically identify substances according to their Raman spectrum
without the need of ad-hoc preprocessing steps. We evaluated our approach using
the RRUFF spectral database, comprising mineral sample data. Superior
classification performance is demonstrated compared with other frequently used
machine learning algorithms including the popular support vector machine.","['Jinchao Liu', 'Margarita Osadchy', 'Lorna Ashton', 'Michael Foster', 'Christopher J. Solomon', 'Stuart J. Gibson']","['cs.LG', 'stat.ML']",2017-08-18 14:06:10+00:00
http://arxiv.org/abs/1708.05594v1,Statistical Latent Space Approach for Mixed Data Modelling and Applications,"The analysis of mixed data has been raising challenges in statistics and
machine learning. One of two most prominent challenges is to develop new
statistical techniques and methodologies to effectively handle mixed data by
making the data less heterogeneous with minimum loss of information. The other
challenge is that such methods must be able to apply in large-scale tasks when
dealing with huge amount of mixed data. To tackle these challenges, we
introduce parameter sharing and balancing extensions to our recent model, the
mixed-variate restricted Boltzmann machine (MV.RBM) which can transform
heterogeneous data into homogeneous representation. We also integrate
structured sparsity and distance metric learning into RBM-based models. Our
proposed methods are applied in various applications including latent patient
profile modelling in medical data analysis and representation learning for
image retrieval. The experimental results demonstrate the models perform better
than baseline methods in medical data and outperform state-of-the-art rivals in
image dataset.","['Tu Dinh Nguyen', 'Truyen Tran', 'Dinh Phung', 'Svetha Venkatesh']","['cs.LG', 'stat.ML']",2017-08-18 13:17:57+00:00
http://arxiv.org/abs/1708.05573v1,Two provably consistent divide and conquer clustering algorithms for large networks,"In this article, we advance divide-and-conquer strategies for solving the
community detection problem in networks. We propose two algorithms which
perform clustering on a number of small subgraphs and finally patches the
results into a single clustering. The main advantage of these algorithms is
that they bring down significantly the computational cost of traditional
algorithms, including spectral clustering, semi-definite programs, modularity
based methods, likelihood based methods etc., without losing on accuracy and
even improving accuracy at times. These algorithms are also, by nature,
parallelizable. Thus, exploiting the facts that most traditional algorithms are
accurate and the corresponding optimization problems are much simpler in small
problems, our divide-and-conquer methods provide an omnibus recipe for scaling
traditional algorithms up to large networks. We prove consistency of these
algorithms under various subgraph selection procedures and perform extensive
simulations and real-data analysis to understand the advantages of the
divide-and-conquer approach in various settings.","['Soumendu Sundar Mukherjee', 'Purnamrita Sarkar', 'Peter J. Bickel']","['stat.ML', 'math.ST', 'stat.CO', 'stat.ME', 'stat.TH']",2017-08-18 12:09:10+00:00
http://arxiv.org/abs/1708.05569v2,Community detection in networks via nonlinear modularity eigenvectors,"Revealing a community structure in a network or dataset is a central problem
arising in many scientific areas. The modularity function $Q$ is an established
measure quantifying the quality of a community, being identified as a set of
nodes having high modularity. In our terminology, a set of nodes with positive
modularity is called a \textit{module} and a set that maximizes $Q$ is thus
called \textit{leading module}. Finding a leading module in a network is an
important task, however the dimension of real-world problems makes the
maximization of $Q$ unfeasible. This poses the need of approximation techniques
which are typically based on a linear relaxation of $Q$, induced by the
spectrum of the modularity matrix $M$. In this work we propose a nonlinear
relaxation which is instead based on the spectrum of a nonlinear modularity
operator $\mathcal M$. We show that extremal eigenvalues of $\mathcal M$
provide an exact relaxation of the modularity measure $Q$, however at the price
of being more challenging to be computed than those of $M$. Thus we extend the
work made on nonlinear Laplacians, by proposing a computational scheme, named
\textit{generalized RatioDCA}, to address such extremal eigenvalues. We show
monotonic ascent and convergence of the method. We finally apply the new method
to several synthetic and real-world data sets, showing both effectiveness of
the model and performance of the method.","['Francesco Tudisco', 'Pedro Mercado', 'Matthias Hein']","['cs.SI', 'math.OC', 'stat.ML', '05C50, 05C70, 47H30, 68R10']",2017-08-18 11:43:21+00:00
http://arxiv.org/abs/1708.06250v1,Pillar Networks++: Distributed non-parametric deep and wide networks,"In recent work, it was shown that combining multi-kernel based support vector
machines (SVMs) can lead to near state-of-the-art performance on an action
recognition dataset (HMDB-51 dataset). This was 0.4\% lower than frameworks
that used hand-crafted features in addition to the deep convolutional feature
extractors. In the present work, we show that combining distributed Gaussian
Processes with multi-stream deep convolutional neural networks (CNN) alleviate
the need to augment a neural network with hand-crafted features. In contrast to
prior work, we treat each deep neural convolutional network as an expert
wherein the individual predictions (and their respective uncertainties) are
combined into a Product of Experts (PoE) framework.","['Biswa Sengupta', 'Yu Qian']","['cs.CV', 'cs.NE', 'stat.CO', 'stat.ML']",2017-08-18 07:51:43+00:00
http://arxiv.org/abs/1708.05512v1,Large Margin Learning in Set to Set Similarity Comparison for Person Re-identification,"Person re-identification (Re-ID) aims at matching images of the same person
across disjoint camera views, which is a challenging problem in multimedia
analysis, multimedia editing and content-based media retrieval communities. The
major challenge lies in how to preserve similarity of the same person across
video footages with large appearance variations, while discriminating different
individuals. To address this problem, conventional methods usually consider the
pairwise similarity between persons by only measuring the point to point (P2P)
distance. In this paper, we propose to use deep learning technique to model a
novel set to set (S2S) distance, in which the underline objective focuses on
preserving the compactness of intra-class samples for each camera view, while
maximizing the margin between the intra-class set and inter-class set. The S2S
distance metric is consisted of three terms, namely the class-identity term,
the relative distance term and the regularization term. The class-identity term
keeps the intra-class samples within each camera view gathering together, the
relative distance term maximizes the distance between the intra-class class set
and inter-class set across different camera views, and the regularization term
smoothness the parameters of deep convolutional neural network (CNN). As a
result, the final learned deep model can effectively find out the matched
target to the probe object among various candidates in the video gallery by
learning discriminative and stable feature representations. Using the CUHK01,
CUHK03, PRID2011 and Market1501 benchmark datasets, we extensively conducted
comparative evaluations to demonstrate the advantages of our method over the
state-of-the-art approaches.","['Sanping Zhou', 'Jinjun Wang', 'Rui Shi', 'Qiqi Hou', 'Yihong Gong', 'Nanning Zheng']","['cs.CV', 'cs.LG', 'stat.ML']",2017-08-18 05:19:01+00:00
http://arxiv.org/abs/1708.06246v2,Comparative Benchmarking of Causal Discovery Techniques,"In this paper we present a comprehensive view of prominent causal discovery
algorithms, categorized into two main categories (1) assuming acyclic and no
latent variables, and (2) allowing both cycles and latent variables, along with
experimental results comparing them from three perspectives: (a) structural
accuracy, (b) standard predictive accuracy, and (c) accuracy of counterfactual
inference. For (b) and (c) we train causal Bayesian networks with structures as
predicted by each causal discovery technique to carry out counterfactual or
standard predictive inference. We compare causal algorithms on two pub- licly
available and one simulated datasets having different sample sizes: small,
medium and large. Experiments show that structural accuracy of a technique does
not necessarily correlate with higher accuracy of inferencing tasks. Fur- ther,
surveyed structure learning algorithms do not perform well in terms of
structural accuracy in case of datasets having large number of variables.","['Karamjit Singh', 'Garima Gupta', 'Vartika Tewari', 'Gautam Shroff']","['cs.AI', 'stat.ML']",2017-08-18 04:18:30+00:00
http://arxiv.org/abs/1708.05487v2,Debiased distributed learning for sparse partial linear models in high dimensions,"Although various distributed machine learning schemes have been proposed
recently for pure linear models and fully nonparametric models, little
attention has been paid on distributed optimization for semi-paramemetric
models with multiple-level structures (e.g. sparsity, linearity and
nonlinearity). To address these issues, the current paper proposes a new
communication-efficient distributed learning algorithm for partially sparse
linear models with an increasing number of features. The proposed method is
based on the classical divide and conquer strategy for handing big data and
each sub-method defined on each subsample consists of a debiased estimation of
the double-regularized least squares approach. With the proposed method, we
theoretically prove that our global parametric estimator can achieve optimal
parametric rate in our semi-parametric model given an appropriate partition on
the total data. Specially, the choice of data partition relies on the
underlying smoothness of the nonparametric component, but it is adaptive to the
sparsity parameter. Even under the non-distributed setting, we develop a new
and easily-read proof for optimal estimation of the parametric error in high
dimensional partial linear model. Finally, several simulated experiments are
implemented to indicate comparable empirical performance of our debiased
technique under the distributed setting.","['Shaogao Lv', 'Heng Lian']",['stat.ML'],2017-08-18 02:35:30+00:00
http://arxiv.org/abs/1708.05472v1,Consistency of Dirichlet Partitions,"A Dirichlet $k$-partition of a domain $U \subseteq \mathbb{R}^d$ is a
collection of $k$ pairwise disjoint open subsets such that the sum of their
first Laplace-Dirichlet eigenvalues is minimal. A discrete version of Dirichlet
partitions has been posed on graphs with applications in data analysis. Both
versions admit variational formulations: solutions are characterized by
minimizers of the Dirichlet energy of mappings from $U$ into a singular space
$\Sigma_k \subseteq \mathbb{R}^k$. In this paper, we extend results of N.\
Garc\'ia Trillos and D.\ Slep\v{c}ev to show that there exist solutions of the
continuum problem arising as limits to solutions of a sequence of discrete
problems. Specifically, a sequence of points $\{x_i\}_{i \in \mathbb{N}}$ from
$U$ is sampled i.i.d.\ with respect to a given probability measure $\nu$ on $U$
and for all $n \in \mathbb{N}$, a geometric graph $G_n$ is constructed from the
first $n$ points $x_1, x_2, \ldots, x_n$ and the pairwise distances between the
points. With probability one with respect to the choice of points $\{x_i\}_{i
\in \mathbb{N}}$, we show that as $n \to \infty$ the discrete Dirichlet
energies for functions $G_n \to \Sigma_k$ $\Gamma$-converge to (a scalar
multiple of) the continuum Dirichlet energy for functions $U \to \Sigma_k$ with
respect to a metric coming from the theory of optimal transport. This, along
with a compactness property for the aforementioned energies that we prove,
implies the convergence of minimizers. When $\nu$ is the uniform distribution,
our results also imply the statistical consistency statement that Dirichlet
partitions of geometric graphs converge to partitions of the sampled space in
the Hausdorff sense.","['Braxton Osting', 'Todd Harry Reeb']","['math.ST', 'math.OC', 'stat.ML', 'stat.TH', '62H30, 62G20, 49J55, 68R10, 60D05']",2017-08-18 00:19:17+00:00
http://arxiv.org/abs/1708.05712v1,Extensions of Morse-Smale Regression with Application to Actuarial Science,"The problem of subgroups is ubiquitous in scientific research (ex. disease
heterogeneity, spatial distributions in ecology...), and piecewise regression
is one way to deal with this phenomenon. Morse-Smale regression offers a way to
partition the regression function based on level sets of a defined function and
that function's basins of attraction. This topologically-based piecewise
regression algorithm has shown promise in its initial applications, but the
current implementation in the literature has been limited to elastic net and
generalized linear regression. It is possible that nonparametric methods, such
as random forest or conditional inference trees, may provide better prediction
and insight through modeling interaction terms and other nonlinear
relationships between predictors and a given outcome.
  This study explores the use of several machine learning algorithms within a
Morse-Smale piecewise regression framework, including boosted regression with
linear baselearners, homotopy-based LASSO, conditional inference trees, random
forest, and a wide neural network framework called extreme learning machines.
Simulations on Tweedie regression problems with varying Tweedie parameter and
dispersion suggest that many machine learning approaches to Morse-Smale
piecewise regression improve the original algorithm's performance, particularly
for outcomes with lower dispersion and linear or a mix of linear and nonlinear
predictor relationships. On a real actuarial problem, several of these new
algorithms perform as good as or better than the original Morse-Smale
regression algorithm, and most provide information on the nature of predictor
relationships within each partition to provide insight into differences between
dataset partitions.",['Colleen M. Farrelly'],"['stat.ML', 'stat.AP']",2017-08-17 22:33:35+00:00
http://arxiv.org/abs/1708.05446v1,Robust Contextual Bandit via the Capped-$\ell_{2}$ norm,"This paper considers the actor-critic contextual bandit for the mobile health
(mHealth) intervention. The state-of-the-art decision-making methods in mHealth
generally assume that the noise in the dynamic system follows the Gaussian
distribution. Those methods use the least-square-based algorithm to estimate
the expected reward, which is prone to the existence of outliers. To deal with
the issue of outliers, we propose a novel robust actor-critic contextual bandit
method for the mHealth intervention. In the critic updating, the
capped-$\ell_{2}$ norm is used to measure the approximation error, which
prevents outliers from dominating our objective. A set of weights could be
achieved from the critic updating. Considering them gives a weighted objective
for the actor updating. It provides the badly noised sample in the critic
updating with zero weights for the actor updating. As a result, the robustness
of both actor-critic updating is enhanced. There is a key parameter in the
capped-$\ell_{2}$ norm. We provide a reliable method to properly set it by
making use of one of the most fundamental definitions of outliers in
statistics. Extensive experiment results demonstrate that our method can
achieve almost identical results compared with the state-of-the-art methods on
the dataset without outliers and dramatically outperform them on the datasets
noised by outliers.","['Feiyun Zhu', 'Xinliang Zhu', 'Sheng Wang', 'Jiawen Yao', 'Junzhou Huang']","['cs.LG', 'stat.ML']",2017-08-17 21:44:36+00:00
http://arxiv.org/abs/1708.06243v1,General Backpropagation Algorithm for Training Second-order Neural Networks,"The artificial neural network is a popular framework in machine learning. To
empower individual neurons, we recently suggested that the current type of
neurons could be upgraded to 2nd order counterparts, in which the linear
operation between inputs to a neuron and the associated weights is replaced
with a nonlinear quadratic operation. A single 2nd order neurons already has a
strong nonlinear modeling ability, such as implementing basic fuzzy logic
operations. In this paper, we develop a general backpropagation (BP) algorithm
to train the network consisting of 2nd-order neurons. The numerical studies are
performed to verify of the generalized BP algorithm.","['Fenglei Fan', 'Wenxiang Cong', 'Ge Wang']","['cs.LG', 'stat.ML']",2017-08-17 21:42:22+00:00
http://arxiv.org/abs/1708.05357v2,Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems,"We propose a generic algorithmic building block to accelerate training of
machine learning models on heterogeneous compute systems. Our scheme allows to
efficiently employ compute accelerators such as GPUs and FPGAs for the training
of large-scale machine learning models, when the training data exceeds their
memory capacity. Also, it provides adaptivity to any system's memory hierarchy
in terms of size and processing speed. Our technique is built upon novel
theoretical insights regarding primal-dual coordinate methods, and uses duality
gap information to dynamically decide which part of the data should be made
available for fast processing. To illustrate the power of our approach we
demonstrate its performance for training of generalized linear models on a
large-scale dataset exceeding the memory size of a modern GPU, showing an
order-of-magnitude speedup over existing approaches.","['Celestine Dünner', 'Thomas Parnell', 'Martin Jaggi']","['cs.LG', 'cs.DC', 'math.OC', 'stat.ML', '90C25, 68W15, 68W10', 'G.1.6; C.1.4']",2017-08-17 16:33:20+00:00
http://arxiv.org/abs/1708.05258v1,Comprehensive Feature-Based Landscape Analysis of Continuous and Constrained Optimization Problems Using the R-Package flacco,"Choosing the best-performing optimizer(s) out of a portfolio of optimization
algorithms is usually a difficult and complex task. It gets even worse, if the
underlying functions are unknown, i.e., so-called Black-Box problems, and
function evaluations are considered to be expensive. In the case of continuous
single-objective optimization problems, Exploratory Landscape Analysis (ELA) -
a sophisticated and effective approach for characterizing the landscapes of
such problems by means of numerical values before actually performing the
optimization task itself - is advantageous. Unfortunately, until now it has
been quite complicated to compute multiple ELA features simultaneously, as the
corresponding code has been - if at all - spread across multiple platforms or
at least across several packages within these platforms.
  This article presents a broad summary of existing ELA approaches and
introduces flacco, an R-package for feature-based landscape analysis of
continuous and constrained optimization problems. Although its functions
neither solve the optimization problem itself nor the related ""Algorithm
Selection Problem (ASP)"", it offers easy access to an essential ingredient of
the ASP by providing a wide collection of ELA features on a single platform -
even within a single package. In addition, flacco provides multiple
visualization techniques, which enhance the understanding of some of these
numerical features, and thereby make certain landscape properties more
comprehensible. On top of that, we will introduce the package's build-in, as
well as web-hosted and hence platform-independent, graphical user interface
(GUI), which facilitates the usage of the package - especially for people who
are not familiar with R - making it a very convenient toolbox when working
towards algorithm selection of continuous single-objective optimization
problems.",['Pascal Kerschke'],['stat.ML'],2017-08-17 13:27:21+00:00
http://arxiv.org/abs/1708.05257v1,Auxiliary Variables for Multi-Dirichlet Priors,"Bayesian models that mix multiple Dirichlet prior parameters, called
Multi-Dirichlet priors (MD) in this paper, are gaining popularity. Inferring
mixing weights and parameters of mixed prior distributions seems tricky, as
sums over Dirichlet parameters complicate the joint distribution of model
parameters.
  This paper shows a novel auxiliary variable scheme which helps to simplify
the inference for models involving hierarchical MDs and MDPs. Using this
scheme, it is easy to derive fully collapsed inference schemes which allow for
an efficient inference.",['Christoph Carl Kling'],['stat.ML'],2017-08-17 13:24:36+00:00
http://arxiv.org/abs/1708.05254v3,Adaptive Clustering Using Kernel Density Estimators,"We derive and analyze a generic, recursive algorithm for estimating all
splits in a finite cluster tree as well as the corresponding clusters. We
further investigate statistical properties of this generic clustering algorithm
when it receives level set estimates from a kernel density estimator. In
particular, we derive finite sample guarantees, consistency, rates of
convergence, and an adaptive data-driven strategy for choosing the kernel
bandwidth. For these results we do not need continuity assumptions on the
density such as H\""{o}lder continuity, but only require intuitive geometric
assumptions of non-parametric nature.","['Ingo Steinwart', 'Bharath K. Sriperumbudur', 'Philipp Thomann']","['stat.ML', 'stat.ME']",2017-08-17 13:19:16+00:00
http://arxiv.org/abs/1708.05239v3,Pseudo-extended Markov chain Monte Carlo,"Sampling from posterior distributions using Markov chain Monte Carlo (MCMC)
methods can require an exhaustive number of iterations, particularly when the
posterior is multi-modal as the MCMC sampler can become trapped in a local mode
for a large number of iterations. In this paper, we introduce the
pseudo-extended MCMC method as a simple approach for improving the mixing of
the MCMC sampler for multi-modal posterior distributions. The pseudo-extended
method augments the state-space of the posterior using pseudo-samples as
auxiliary variables. On the extended space, the modes of the posterior are
connected, which allows the MCMC sampler to easily move between well-separated
posterior modes. We demonstrate that the pseudo-extended approach delivers
improved MCMC sampling over the Hamiltonian Monte Carlo algorithm on
multi-modal posteriors, including Boltzmann machines and models with
sparsity-inducing priors.","['Christopher Nemeth', 'Fredrik Lindsten', 'Maurizio Filippone', 'James Hensman']","['stat.ME', 'stat.CO', 'stat.ML']",2017-08-17 12:45:07+00:00
http://arxiv.org/abs/1708.05207v3,Learning Universal Adversarial Perturbations with Generative Models,"Neural networks are known to be vulnerable to adversarial examples, inputs
that have been intentionally perturbed to remain visually similar to the source
input, but cause a misclassification. It was recently shown that given a
dataset and classifier, there exists so called universal adversarial
perturbations, a single perturbation that causes a misclassification when
applied to any input. In this work, we introduce universal adversarial
networks, a generative network that is capable of fooling a target classifier
when it's generated output is added to a clean sample from a dataset. We show
that this technique improves on known universal adversarial attacks.","['Jamie Hayes', 'George Danezis']","['cs.CR', 'cs.LG', 'stat.ML']",2017-08-17 11:25:39+00:00
http://arxiv.org/abs/1708.05200v1,Towards life cycle identification of malaria parasites using machine learning and Riemannian geometry,"Malaria is a serious infectious disease that is responsible for over half
million deaths yearly worldwide. The major cause of these mortalities is late
or inaccurate diagnosis. Manual microscopy is currently considered as the
dominant diagnostic method for malaria. However, it is time consuming and prone
to human errors. The aim of this paper is to automate the diagnosis process and
minimize the human intervention. We have developed the hardware and software
for a cost-efficient malaria diagnostic system. This paper describes the
manufactured hardware and also proposes novel software to handle parasite
detection and life-stage identification. A motorized microscope is developed to
take images from Giemsa-stained blood smears. A patch-based unsupervised
statistical clustering algorithm is proposed which offers a novel method for
classification of different regions within blood images. The proposed method
provides better robustness against different imaging settings. The core of the
proposed algorithm is a model called Mixture of Independent Component Analysis.
A manifold based optimization method is proposed that facilitates the
application of the model for high dimensional data usually acquired in medical
microscopy. The method was tested on 600 blood slides with various imaging
conditions. The speed of the method is higher than current supervised systems
while its accuracy is comparable to or better than them.",['Arash Mehrjou'],['stat.ML'],2017-08-17 10:55:05+00:00
http://arxiv.org/abs/1708.05123v1,Deep & Cross Network for Ad Click Predictions,"Feature engineering has been the key to the success of many prediction
models. However, the process is non-trivial and often requires manual feature
engineering or exhaustive searching. DNNs are able to automatically learn
feature interactions; however, they generate all the interactions implicitly,
and are not necessarily efficient in learning all types of cross features. In
this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits
of a DNN model, and beyond that, it introduces a novel cross network that is
more efficient in learning certain bounded-degree feature interactions. In
particular, DCN explicitly applies feature crossing at each layer, requires no
manual feature engineering, and adds negligible extra complexity to the DNN
model. Our experimental results have demonstrated its superiority over the
state-of-art algorithms on the CTR prediction dataset and dense classification
dataset, in terms of both model accuracy and memory usage.","['Ruoxi Wang', 'Bin Fu', 'Gang Fu', 'Mingliang Wang']","['cs.LG', 'stat.ML']",2017-08-17 03:28:04+00:00
http://arxiv.org/abs/1708.05106v2,The Mean and Median Criterion for Automatic Kernel Bandwidth Selection for Support Vector Data Description,"Support vector data description (SVDD) is a popular technique for detecting
anomalies. The SVDD classifier partitions the whole space into an inlier
region, which consists of the region near the training data, and an outlier
region, which consists of points away from the training data. The computation
of the SVDD classifier requires a kernel function, and the Gaussian kernel is a
common choice for the kernel function. The Gaussian kernel has a bandwidth
parameter, whose value is important for good results. A small bandwidth leads
to overfitting, and the resulting SVDD classifier overestimates the number of
anomalies. A large bandwidth leads to underfitting, and the classifier fails to
detect many anomalies. In this paper we present a new automatic, unsupervised
method for selecting the Gaussian kernel bandwidth. The selected value can be
computed quickly, and it is competitive with existing bandwidth selection
methods.","['Arin Chaudhuri', 'Deovrat Kakde', 'Carol Sadek', 'Laura Gonzalez', 'Seunghyun Kong']","['cs.LG', 'cs.AI', 'stat.ML', 'I.2.7']",2017-08-16 23:38:35+00:00
http://arxiv.org/abs/1708.05094v1,An Ensemble Quadratic Echo State Network for Nonlinear Spatio-Temporal Forecasting,"Spatio-temporal data and processes are prevalent across a wide variety of
scientific disciplines. These processes are often characterized by nonlinear
time dynamics that include interactions across multiple scales of spatial and
temporal variability. The data sets associated with many of these processes are
increasing in size due to advances in automated data measurement, management,
and numerical simulator output. Non- linear spatio-temporal models have only
recently seen interest in statistics, but there are many classes of such models
in the engineering and geophysical sciences. Tradi- tionally, these models are
more heuristic than those that have been presented in the statistics
literature, but are often intuitive and quite efficient computationally. We
show here that with fairly simple, but important, enhancements, the echo state
net- work (ESN) machine learning approach can be used to generate long-lead
forecasts of nonlinear spatio-temporal processes, with reasonable uncertainty
quantification, and at only a fraction of the computational expense of a
traditional parametric nonlinear spatio-temporal models.","['Patrick L. McDermott', 'Christopher K. Wikle']","['stat.ML', 'stat.AP']",2017-08-16 22:08:25+00:00
http://arxiv.org/abs/1708.05033v2,Corrupt Bandits for Preserving Local Privacy,"We study a variant of the stochastic multi-armed bandit (MAB) problem in
which the rewards are corrupted. In this framework, motivated by privacy
preservation in online recommender systems, the goal is to maximize the sum of
the (unobserved) rewards, based on the observation of transformation of these
rewards through a stochastic corruption process with known parameters. We
provide a lower bound on the expected regret of any bandit algorithm in this
corrupted setting. We devise a frequentist algorithm, KLUCB-CF, and a Bayesian
algorithm, TS-CF and give upper bounds on their regret. We also provide the
appropriate corruption parameters to guarantee a desired level of local privacy
and analyze how this impacts the regret. Finally, we present some experimental
results that confirm our analysis.","['Pratik Gajane', 'Tanguy Urvoy', 'Emilie Kaufmann']","['cs.LG', 'stat.ML']",2017-08-16 18:32:24+00:00
http://arxiv.org/abs/1708.04975v2,Training-image based geostatistical inversion using a spatial generative adversarial neural network,"Probabilistic inversion within a multiple-point statistics framework is often
computationally prohibitive for high-dimensional problems. To partly address
this, we introduce and evaluate a new training-image based inversion approach
for complex geologic media. Our approach relies on a deep neural network of the
generative adversarial network (GAN) type. After training using a training
image (TI), our proposed spatial GAN (SGAN) can quickly generate 2D and 3D
unconditional realizations. A key characteristic of our SGAN is that it defines
a (very) low-dimensional parameterization, thereby allowing for efficient
probabilistic inversion using state-of-the-art Markov chain Monte Carlo (MCMC)
methods. In addition, available direct conditioning data can be incorporated
within the inversion. Several 2D and 3D categorical TIs are first used to
analyze the performance of our SGAN for unconditional geostatistical
simulation. Training our deep network can take several hours. After training,
realizations containing a few millions of pixels/voxels can be produced in a
matter of seconds. This makes it especially useful for simulating many
thousands of realizations (e.g., for MCMC inversion) as the relative cost of
the training per realization diminishes with the considered number of
realizations. Synthetic inversion case studies involving 2D steady-state flow
and 3D transient hydraulic tomography with and without direct conditioning data
are used to illustrate the effectiveness of our proposed SGAN-based inversion.
For the 2D case, the inversion rapidly explores the posterior model
distribution. For the 3D case, the inversion recovers model realizations that
fit the data close to the target level and visually resemble the true model
well.","['Eric Laloy', 'Romain Hérault', 'Diederik Jacques', 'Niklas Linde']","['stat.ML', 'cs.CV', 'physics.geo-ph']",2017-08-16 17:04:52+00:00
http://arxiv.org/abs/1708.04970v2,Adaptive Threshold Sampling,"Sampling is a fundamental problem in computer science and statistics.
However, for a given task and stream, it is often not possible to choose good
sampling probabilities in advance. We derive a general framework for adaptively
changing the sampling probabilities via a collection of thresholds.In general,
adaptive sampling procedures introduce dependence amongst the sampled points,
making it difficult to compute expectations and ensure estimators are unbiased
or consistent. Our framework address this issue and further shows when adaptive
thresholds can be treated as if they were fixed thresholds which samples items
independently. This makes our adaptive sampling schemes simple to apply as
there is no need to create custom estimators for the sampling method.
  Using our framework, we derive new samplers that can address a broad range of
new and existing problems including sampling with memory rather than sample
size budgets, stratified samples, multiple objectives, distinct counting, and
sliding windows. In particular, we design a sampling procedure for the top-K
problem where, unlike in the heavy-hitter problem, the sketch size and sampling
probabilities are adaptively chosen.",['Daniel Ting'],"['stat.ML', 'cs.LG']",2017-08-16 16:45:47+00:00
http://arxiv.org/abs/1708.04922v1,Optimal Alarms for Vehicular Collision Detection,"An important application of intelligent vehicles is advance detection of
dangerous events such as collisions. This problem is framed as a problem of
optimal alarm choice given predictive models for vehicle location and motion.
Techniques for real-time collision detection are surveyed and grouped into
three classes: random Monte Carlo sampling, faster deterministic
approximations, and machine learning models trained by simulation. Theoretical
guarantees on the performance of these collision detection techniques are
provided where possible, and empirical analysis is provided for two example
scenarios. Results validate Monte Carlo sampling as a robust solution despite
its simplicity.","['Michael Motro', 'Joydeep Ghosh', 'Chandra Bhat']","['stat.ML', 'cs.RO']",2017-08-16 15:09:09+00:00
http://arxiv.org/abs/1708.04825v1,Ultra-Fast Reactive Transport Simulations When Chemical Reactions Meet Machine Learning: Chemical Equilibrium,"During reactive transport modeling, the computational cost associated with
chemical reaction calculations is often 10-100 times higher than that of
transport calculations. Most of these costs results from chemical equilibrium
calculations that are performed at least once in every mesh cell and at every
time step of the simulation. Calculating chemical equilibrium is an iterative
process, where each iteration is in general so computationally expensive that
even if every calculation converged in a single iteration, the resulting
speedup would not be significant. Thus, rather than proposing a fast-converging
numerical method for solving chemical equilibrium equations, we present a
machine learning method that enables new equilibrium states to be quickly and
accurately estimated, whenever a previous equilibrium calculation with similar
input conditions has been performed. We demonstrate the use of this smart
chemical equilibrium method in a reactive transport modeling example and show
that, even at early simulation times, the majority of all equilibrium
calculations are quickly predicted and, after some time steps, the
machine-learning-accelerated chemical solver has been fully trained to rapidly
perform all subsequent equilibrium calculations, resulting in speedups of
almost two orders of magnitude. We remark that our new on-demand machine
learning method can be applied to any case in which a massive number of
sequential/parallel evaluations of a computationally expensive function $f$
needs to be done, $y=f(x)$. We remark, that, in contrast to traditional machine
learning algorithms, our on-demand training approach does not require a
statistics-based training phase before the actual simulation of interest
commences. The introduced on-demand training scheme requires, however, the
first-order derivatives $\partial f/\partial x$ for later smart predictions.","['Allan M. M. Leal', 'Dmitrii A. Kulik', 'Martin O. Saar']","['math.OC', 'physics.chem-ph', 'physics.flu-dyn', 'stat.ML']",2017-08-16 09:48:21+00:00
http://arxiv.org/abs/1708.04801v1,Weighted parallel SGD for distributed unbalanced-workload training system,"Stochastic gradient descent (SGD) is a popular stochastic optimization method
in machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel
SGD, often require all nodes to have the same performance or to consume equal
quantities of data. However, these requirements are difficult to satisfy when
the parallel SGD algorithms run in a heterogeneous computing environment;
low-performance nodes will exert a negative influence on the final result. In
this paper, we propose an algorithm called weighted parallel SGD (WP-SGD).
WP-SGD combines weighted model parameters from different nodes in the system to
produce the final output. WP-SGD makes use of the reduction in standard
deviation to compensate for the loss from the inconsistency in performance of
nodes in the cluster, which means that WP-SGD does not require that all nodes
consume equal quantities of data. We also analyze the theoretical feasibility
of running two other parallel SGD algorithms combined with WP-SGD in a
heterogeneous environment. The experimental results show that WP-SGD
significantly outperforms the traditional parallel SGD algorithms on
distributed training systems with an unbalanced workload.","['Cheng Daning', 'Li Shigang', 'Zhang Yunquan']","['cs.LG', 'cs.AI', 'stat.ML']",2017-08-16 08:29:23+00:00
http://arxiv.org/abs/1708.04788v3,BitNet: Bit-Regularized Deep Neural Networks,"We present a novel optimization strategy for training neural networks which
we call ""BitNet"". The parameters of neural networks are usually unconstrained
and have a dynamic range dispersed over all real values. Our key idea is to
limit the expressive power of the network by dynamically controlling the range
and set of values that the parameters can take. We formulate this idea using a
novel end-to-end approach that circumvents the discrete parameter space by
optimizing a relaxed continuous and differentiable upper bound of the typical
classification loss function. The approach can be interpreted as a
regularization inspired by the Minimum Description Length (MDL) principle. For
each layer of the network, our approach optimizes real-valued translation and
scaling factors and arbitrary precision integer-valued parameters (weights). We
empirically compare BitNet to an equivalent unregularized model on the MNIST
and CIFAR-10 datasets. We show that BitNet converges faster to a superior
quality solution. Additionally, the resulting model has significant savings in
memory due to the use of integer-valued parameters.","['Aswin Raghavan', 'Mohamed Amer', 'Sek Chai', 'Graham Taylor']","['cs.LG', 'stat.ML']",2017-08-16 06:51:23+00:00
http://arxiv.org/abs/1708.04781v1,Racing Thompson: an Efficient Algorithm for Thompson Sampling with Non-conjugate Priors,"Thompson sampling has impressive empirical performance for many multi-armed
bandit problems. But current algorithms for Thompson sampling only work for the
case of conjugate priors since these algorithms require to infer the posterior,
which is often computationally intractable when the prior is not conjugate. In
this paper, we propose a novel algorithm for Thompson sampling which only
requires to draw samples from a tractable distribution, so our algorithm is
efficient even when the prior is non-conjugate. To do this, we reformulate
Thompson sampling as an optimization problem via the Gumbel-Max trick. After
that we construct a set of random variables and our goal is to identify the one
with highest mean. Finally, we solve it with techniques in best arm
identification.","['Yichi Zhou', 'Jun Zhu', 'Jingwei Zhuo']","['cs.LG', 'stat.ML']",2017-08-16 06:20:40+00:00
http://arxiv.org/abs/1708.04764v1,Active Orthogonal Matching Pursuit for Sparse Subspace Clustering,"Sparse Subspace Clustering (SSC) is a state-of-the-art method for clustering
high-dimensional data points lying in a union of low-dimensional subspaces.
However, while $\ell_1$ optimization-based SSC algorithms suffer from high
computational complexity, other variants of SSC, such as Orthogonal Matching
Pursuit-based SSC (OMP-SSC), lose clustering accuracy in pursuit of improving
time efficiency. In this letter, we propose a novel Active OMP-SSC, which
improves clustering accuracy of OMP-SSC by adaptively updating data points and
randomly dropping data points in the OMP process, while still enjoying the low
computational complexity of greedy pursuit algorithms. We provide heuristic
analysis of our approach, and explain how these two active steps achieve a
better tradeoff between connectivity and separation. Numerical results on both
synthetic data and real-world data validate our analyses and show the
advantages of the proposed active algorithm.","['Yanxi Chen', 'Gen Li', 'Yuantao Gu']","['cs.LG', 'cs.CV', 'cs.IT', 'math.IT', 'stat.ML']",2017-08-16 04:15:37+00:00
http://arxiv.org/abs/1708.04757v1,Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction,"Missing data and noisy observations pose significant challenges for reliably
predicting events from irregularly sampled multivariate time series
(longitudinal) data. Imputation methods, which are typically used for
completing the data prior to event prediction, lack a principled mechanism to
account for the uncertainty due to missingness. Alternatively, state-of-the-art
joint modeling techniques can be used for jointly modeling the longitudinal and
event data and compute event probabilities conditioned on the longitudinal
observations. These approaches, however, make strong parametric assumptions and
do not easily scale to multivariate signals with many observations. Our
proposed approach consists of several key innovations. First, we develop a
flexible and scalable joint model based upon sparse multiple-output Gaussian
processes. Unlike state-of-the-art joint models, the proposed model can explain
highly challenging structure including non-Gaussian noise while scaling to
large data. Second, we derive an optimal policy for predicting events using the
distribution of the event occurrence estimated by the joint model. The derived
policy trades-off the cost of a delayed detection versus incorrect assessments
and abstains from making decisions when the estimated event probability does
not satisfy the derived confidence criteria. Experiments on a large dataset
show that the proposed framework significantly outperforms state-of-the-art
techniques in event prediction.","['Hossein Soleimani', 'James Hensman', 'Suchi Saria']","['stat.ML', 'cs.AI', 'cs.LG']",2017-08-16 03:27:25+00:00
http://arxiv.org/abs/1708.04753v1,Frequentist coverage and sup-norm convergence rate in Gaussian process regression,"Gaussian process (GP) regression is a powerful interpolation technique due to
its flexibility in capturing non-linearity. In this paper, we provide a general
framework for understanding the frequentist coverage of point-wise and
simultaneous Bayesian credible sets in GP regression. As an intermediate
result, we develop a Bernstein von-Mises type result under supremum norm in
random design GP regression. Identifying both the mean and covariance function
of the posterior distribution of the Gaussian process as regularized
$M$-estimators, we show that the sampling distribution of the posterior mean
function and the centered posterior distribution can be respectively
approximated by two population level GPs. By developing a comparison inequality
between two GPs, we provide exact characterization of frequentist coverage
probabilities of Bayesian point-wise credible intervals and simultaneous
credible bands of the regression function. Our results show that inference
based on GP regression tends to be conservative; when the prior is
under-smoothed, the resulting credible intervals and bands have minimax-optimal
sizes, with their frequentist coverage converging to a non-degenerate value
between their nominal level and one. As a byproduct of our theory, we show that
the GP regression also yields minimax-optimal posterior contraction rate
relative to the supremum norm, which provides a positive evidence to the long
standing problem on optimal supremum norm contraction rate in GP regression.","['Yun Yang', 'Anirban Bhattacharya', 'Debdeep Pati']","['math.ST', 'stat.CO', 'stat.ML', 'stat.TH']",2017-08-16 03:07:54+00:00
http://arxiv.org/abs/1708.04733v2,Geometric Enclosing Networks,"Training model to generate data has increasingly attracted research attention
and become important in modern world applications. We propose in this paper a
new geometry-based optimization approach to address this problem. Orthogonal to
current state-of-the-art density-based approaches, most notably VAE and GAN, we
present a fresh new idea that borrows the principle of minimal enclosing ball
to train a generator G\left(\bz\right) in such a way that both training and
generated data, after being mapped to the feature space, are enclosed in the
same sphere. We develop theory to guarantee that the mapping is bijective so
that its inverse from feature space to data space results in expressive
nonlinear contours to describe the data manifold, hence ensuring data generated
are also lying on the data manifold learned from training data. Our model
enjoys a nice geometric interpretation, hence termed Geometric Enclosing
Networks (GEN), and possesses some key advantages over its rivals, namely
simple and easy-to-control optimization formulation, avoidance of mode
collapsing and efficiently learn data manifold representation in a completely
unsupervised manner. We conducted extensive experiments on synthesis and
real-world datasets to illustrate the behaviors, strength and weakness of our
proposed GEN, in particular its ability to handle multi-modal data and quality
of generated data.","['Trung Le', 'Hung Vu', 'Tu Dinh Nguyen', 'Dinh Phung']","['cs.LG', 'cs.AI', 'stat.ML']",2017-08-16 01:10:49+00:00
http://arxiv.org/abs/1708.04729v3,Deconvolutional Paragraph Representation Learning,"Learning latent representations from long text sequences is an important
first step in many natural language processing applications. Recurrent Neural
Networks (RNNs) have become a cornerstone for this challenging task. However,
the quality of sentences during RNN-based decoding (reconstruction) decreases
with the length of the text. We propose a sequence-to-sequence, purely
convolutional and deconvolutional autoencoding framework that is free of the
above issue, while also being computationally efficient. The proposed method is
simple, easy to implement and can be leveraged as a building block for many
applications. We show empirically that compared to RNNs, our framework is
better at reconstructing and correcting long paragraphs. Quantitative
evaluation on semi-supervised text classification and summarization tasks
demonstrate the potential for better utilization of long unlabeled text data.","['Yizhe Zhang', 'Dinghan Shen', 'Guoyin Wang', 'Zhe Gan', 'Ricardo Henao', 'Lawrence Carin']","['cs.CL', 'cs.LG', 'stat.ML']",2017-08-16 00:52:32+00:00
http://arxiv.org/abs/1708.04692v2,GANs for Biological Image Synthesis,"In this paper, we propose a novel application of Generative Adversarial
Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy.
Compared to natural images, cells tend to have a simpler and more geometric
global structure that facilitates image generation. However, the correlation
between the spatial pattern of different fluorescent proteins reflects
important biological functions, and synthesized images have to capture these
relationships to be relevant for biological applications. We adapt GANs to the
task at hand and propose new models with casual dependencies between image
channels that can generate multi-channel images, which would be impossible to
obtain experimentally. We evaluate our approach using two independent
techniques and compare it against sensible baselines. Finally, we demonstrate
that by interpolating across the latent space we can mimic the known changes in
protein localization that occur through time during the cell cycle, allowing us
to predict temporal evolution from static images.","['Anton Osokin', 'Anatole Chessel', 'Rafael E. Carazo Salas', 'Federico Vaggi']","['cs.CV', 'cs.LG', 'stat.ML']",2017-08-15 21:04:11+00:00
http://arxiv.org/abs/1708.04649v1,Machine Learning for Survival Analysis: A Survey,"Accurately predicting the time of occurrence of an event of interest is a
critical problem in longitudinal data analysis. One of the main challenges in
this context is the presence of instances whose event outcomes become
unobservable after a certain time point or when some instances do not
experience any event during the monitoring period. Such a phenomenon is called
censoring which can be effectively handled using survival analysis techniques.
Traditionally, statistical approaches have been widely developed in the
literature to overcome this censoring issue. In addition, many machine learning
algorithms are adapted to effectively handle survival data and tackle other
challenging problems that arise in real-world data. In this survey, we provide
a comprehensive and structured review of the representative statistical methods
along with the machine learning techniques used in survival analysis and
provide a detailed taxonomy of the existing methods. We also discuss several
topics that are closely related to survival analysis and illustrate several
successful applications in various real-world application domains. We hope that
this paper will provide a more thorough understanding of the recent advances in
survival analysis and offer some guidelines on applying these approaches to
solve new problems that arise in applications with censored data.","['Ping Wang', 'Yan Li', 'Chandan K. Reddy']","['cs.LG', 'stat.ML']",2017-08-15 19:03:36+00:00
http://arxiv.org/abs/1708.04622v1,Deep Learning the Ising Model Near Criticality,"It is well established that neural networks with deep architectures perform
better than shallow networks for many tasks in machine learning. In statistical
physics, while there has been recent interest in representing physical data
with generative modelling, the focus has been on shallow neural networks. A
natural question to ask is whether deep neural networks hold any advantage over
shallow networks in representing such data. We investigate this question by
using unsupervised, generative graphical models to learn the probability
distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep
belief networks, and deep restricted Boltzmann networks are trained on thermal
spin configurations from this system, and compared to the shallow architecture
of the restricted Boltzmann machine. We benchmark the models, focussing on the
accuracy of generating energetic observables near the phase transition, where
these quantities are most difficult to approximate. Interestingly, after
training the generative networks, we observe that the accuracy essentially
depends only on the number of neurons in the first hidden layer of the network,
and not on other model details such as network depth or model type. This is
evidence that shallow networks are more efficient than deep networks at
representing physical probability distributions associated with Ising systems
near criticality.","['Alan Morningstar', 'Roger G. Melko']","['cond-mat.dis-nn', 'cs.LG', 'stat.ML']",2017-08-15 18:00:01+00:00
http://arxiv.org/abs/1708.04557v1,"Database of Parliamentary Speeches in Ireland, 1919-2013","We present a database of parliamentary debates that contains the complete
record of parliamentary speeches from D\'ail \'Eireann, the lower house and
principal chamber of the Irish parliament, from 1919 to 2013. In addition, the
database contains background information on all TDs (Teachta D\'ala, members of
parliament), such as their party affiliations, constituencies and office
positions. The current version of the database includes close to 4.5 million
speeches from 1,178 TDs. The speeches were downloaded from the official
parliament website and further processed and parsed with a Python script.
Background information on TDs was collected from the member database of the
parliament website. Data on cabinet positions (ministers and junior ministers)
was collected from the official website of the government. A record linkage
algorithm and human coders were used to match TDs and ministers.","['Alexander Herzog', 'Slava J. Mikhaylov']","['cs.CL', 'cs.SI', 'stat.ML']",2017-08-15 15:34:33+00:00
http://arxiv.org/abs/1708.04527v1,The Trimmed Lasso: Sparsity and Robustness,"Nonconvex penalty methods for sparse modeling in linear regression have been
a topic of fervent interest in recent years. Herein, we study a family of
nonconvex penalty functions that we call the trimmed Lasso and that offers
exact control over the desired level of sparsity of estimators. We analyze its
structural properties and in doing so show the following:
  1) Drawing parallels between robust statistics and robust optimization, we
show that the trimmed-Lasso-regularized least squares problem can be viewed as
a generalized form of total least squares under a specific model of
uncertainty. In contrast, this same model of uncertainty, viewed instead
through a robust optimization lens, leads to the convex SLOPE (or OWL) penalty.
  2) Further, in relating the trimmed Lasso to commonly used sparsity-inducing
penalty functions, we provide a succinct characterization of the connection
between trimmed-Lasso- like approaches and penalty functions that are
coordinate-wise separable, showing that the trimmed penalties subsume existing
coordinate-wise separable penalties, with strict containment in general.
  3) Finally, we describe a variety of exact and heuristic algorithms, both
existing and new, for trimmed Lasso regularized estimation problems. We include
a comparison between the different approaches and an accompanying
implementation of the algorithms.","['Dimitris Bertsimas', 'Martin S. Copenhaver', 'Rahul Mazumder']","['stat.ME', 'math.OC', 'math.ST', 'stat.CO', 'stat.ML', 'stat.TH']",2017-08-15 14:56:28+00:00
http://arxiv.org/abs/1708.04465v1,Actively Learning what makes a Discrete Sequence Valid,"Deep learning techniques have been hugely successful for traditional
supervised and unsupervised machine learning problems. In large part, these
techniques solve continuous optimization problems. Recently however, discrete
generative deep learning models have been successfully used to efficiently
search high-dimensional discrete spaces. These methods work by representing
discrete objects as sequences, for which powerful sequence-based deep models
can be employed. Unfortunately, these techniques are significantly hindered by
the fact that these generative models often produce invalid sequences. As a
step towards solving this problem, we propose to learn a deep recurrent
validator model. Given a partial sequence, our model learns the probability of
that sequence occurring as the beginning of a full valid sequence. Thus this
identifies valid versus invalid sequences and crucially it also provides
insight about how individual sequence elements influence the validity of
discrete objects. To learn this model we propose an approach inspired by
seminal work in Bayesian active learning. On a synthetic dataset, we
demonstrate the ability of our model to distinguish valid and invalid
sequences. We believe this is a key step toward learning generative models that
faithfully produce valid discrete objects.","['David Janz', 'Jos van der Westhuizen', 'José Miguel Hernández-Lobato']","['stat.ML', 'cs.LG']",2017-08-15 11:52:35+00:00
http://arxiv.org/abs/1708.04403v1,Theoretical Foundation of Co-Training and Disagreement-Based Algorithms,"Disagreement-based approaches generate multiple classifiers and exploit the
disagreement among them with unlabeled data to improve learning performance.
Co-training is a representative paradigm of them, which trains two classifiers
separately on two sufficient and redundant views; while for the applications
where there is only one view, several successful variants of co-training with
two different classifiers on single-view data instead of two views have been
proposed. For these disagreement-based approaches, there are several important
issues which still are unsolved, in this article we present theoretical
analyses to address these issues, which provides a theoretical foundation of
co-training and disagreement-based approaches.","['Wei Wang', 'Zhi-Hua Zhou']","['cs.LG', 'cs.AI', 'stat.ML']",2017-08-15 06:00:33+00:00
http://arxiv.org/abs/1708.04357v1,Graph Classification via Deep Learning with Virtual Nodes,"Learning representation for graph classification turns a variable-size graph
into a fixed-size vector (or matrix). Such a representation works nicely with
algebraic manipulations. Here we introduce a simple method to augment an
attributed graph with a virtual node that is bidirectionally connected to all
existing nodes. The virtual node represents the latent aspects of the graph,
which are not immediately available from the attributes and local connectivity
structures. The expanded graph is then put through any node representation
method. The representation of the virtual node is then the representation of
the entire graph. In this paper, we use the recently introduced Column Network
for the expanded graph, resulting in a new end-to-end graph classification
model dubbed Virtual Column Network (VCN). The model is validated on two tasks:
(i) predicting bio-activity of chemical compounds, and (ii) finding software
vulnerability from source code. Results demonstrate that VCN is competitive
against well-established rivals.","['Trang Pham', 'Truyen Tran', 'Hoa Dam', 'Svetha Venkatesh']","['cs.LG', 'cs.AI', 'stat.ML']",2017-08-14 23:47:02+00:00
http://arxiv.org/abs/1708.04887v1,Fixed effects testing in high-dimensional linear mixed models,"Many scientific and engineering challenges -- ranging from pharmacokinetic
drug dosage allocation and personalized medicine to marketing mix (4Ps)
recommendations -- require an understanding of the unobserved heterogeneity in
order to develop the best decision making-processes. In this paper, we develop
a hypothesis test and the corresponding p-value for testing for the
significance of the homogeneous structure in linear mixed models. A robust
matching moment construction is used for creating a test that adapts to the
size of the model sparsity. When unobserved heterogeneity at a cluster level is
constant, we show that our test is both consistent and unbiased even when the
dimension of the model is extremely high. Our theoretical results rely on a new
family of adaptive sparse estimators of the fixed effects that do not require
consistent estimation of the random effects. Moreover, our inference results do
not require consistent model selection. We showcase that moment matching can be
extended to nonlinear mixed effects models and to generalized linear mixed
effects models. In numerical and real data experiments, we find that the
developed method is extremely accurate, that it adapts to the size of the
underlying model and is decidedly powerful in the presence of irrelevant
covariates.","['Jelena Bradic', 'Gerda Claeskens', 'Thomas Gueuning']","['stat.ME', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2017-08-14 21:48:46+00:00
http://arxiv.org/abs/1708.04312v1,Collaborative Filtering using Denoising Auto-Encoders for Market Basket Data,"Recommender systems (RS) help users navigate large sets of items in the
search for ""interesting"" ones. One approach to RS is Collaborative Filtering
(CF), which is based on the idea that similar users are interested in similar
items. Most model-based approaches to CF seek to train a
machine-learning/data-mining model based on sparse data; the model is then used
to provide recommendations. While most of the proposed approaches are effective
for small-size situations, the combinatorial nature of the problem makes it
impractical for medium-to-large instances. In this work we present a novel
approach to CF that works by training a Denoising Auto-Encoder (DAE) on
corrupted baskets, i.e., baskets from which one or more items have been
removed. The DAE is then forced to learn to reconstruct the original basket
given its corrupted input. Due to recent advancements in optimization and other
technologies for training neural-network models (such as DAE), the proposed
method results in a scalable and practical approach to CF. The contribution of
this work is twofold: (1) to identify missing items in observed baskets and,
thus, directly providing a CF model; and, (2) to construct a generative model
of baskets which may be used, for instance, in simulation analysis or as part
of a more complex analytical method.","['Andres G. Abad', 'Luis I. Reyes-Castro']","['stat.ML', 'cs.LG']",2017-08-14 20:32:35+00:00
http://arxiv.org/abs/1708.04106v3,Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net,"Models applied on real time response task, like click-through rate (CTR)
prediction model, require high accuracy and rigorous response time. Therefore,
top-performing deep models of high depth and complexity are not well suited for
these applications with the limitations on the inference time. In order to
further improve the neural networks' performance given the time and
computational limitations, we propose an approach that exploits a cumbersome
net to help train the lightweight net for prediction. We dub the whole process
rocket launching, where the cumbersome booster net is used to guide the
learning of the target light net throughout the whole training process. We
analyze different loss functions aiming at pushing the light net to behave
similarly to the booster net, and adopt the loss with best performance in our
experiments. We use one technique called gradient block to improve the
performance of the light net and booster net further. Experiments on benchmark
datasets and real-life industrial advertisement data present that our light
model can get performance only previously achievable with more complex models.","['Guorui Zhou', 'Ying Fan', 'Runpeng Cui', 'Weijie Bian', 'Xiaoqiang Zhu', 'Kun Gai']","['stat.ML', 'cs.LG', 'I.2.6']",2017-08-14 13:06:15+00:00
http://arxiv.org/abs/1709.06537v1,DC-Prophet: Predicting Catastrophic Machine Failures in DataCenters,"When will a server fail catastrophically in an industrial datacenter? Is it
possible to forecast these failures so preventive actions can be taken to
increase the reliability of a datacenter? To answer these questions, we have
studied what are probably the largest, publicly available datacenter traces,
containing more than 104 million events from 12,500 machines. Among these
samples, we observe and categorize three types of machine failures, all of
which are catastrophic and may lead to information loss, or even worse,
reliability degradation of a datacenter. We further propose a two-stage
framework-DC-Prophet-based on One-Class Support Vector Machine and Random
Forest. DC-Prophet extracts surprising patterns and accurately predicts the
next failure of a machine. Experimental results show that DC-Prophet achieves
an AUC of 0.93 in predicting the next machine failure, and a F3-score of 0.88
(out of 1). On average, DC-Prophet outperforms other classical machine learning
methods by 39.45% in F3-score.","['You-Luen Lee', 'Da-Cheng Juan', 'Xuan-An Tseng', 'Yu-Ting Chen', 'Shih-Chieh Chang']","['cs.DC', 'stat.ML']",2017-08-14 07:46:47+00:00
http://arxiv.org/abs/1708.03999v2,ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models,"Deep neural networks (DNNs) are one of the most prominent technologies of our
time, as they achieve state-of-the-art performance in many machine learning
tasks, including but not limited to image classification, text mining, and
speech processing. However, recent research on DNNs has indicated
ever-increasing concern on the robustness to adversarial examples, especially
for security-critical tasks such as traffic sign identification for autonomous
driving. Studies have unveiled the vulnerability of a well-trained DNN by
demonstrating the ability of generating barely noticeable (to both human and
machines) adversarial images that lead to misclassification. Furthermore,
researchers have shown that these adversarial images are highly transferable by
simply training and attacking a substitute model built upon the target model,
known as a black-box attack to DNNs.
  Similar to the setting of training substitute models, in this paper we
propose an effective black-box attack that also only has access to the input
(images) and the output (confidence scores) of a targeted DNN. However,
different from leveraging attack transferability from substitute models, we
propose zeroth order optimization (ZOO) based attacks to directly estimate the
gradients of the targeted DNN for generating adversarial examples. We use
zeroth order stochastic coordinate descent along with dimension reduction,
hierarchical attack and importance sampling techniques to efficiently attack
black-box models. By exploiting zeroth order optimization, improved attacks to
the targeted DNN can be accomplished, sparing the need for training substitute
models and avoiding the loss in attack transferability. Experimental results on
MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective
as the state-of-the-art white-box attack and significantly outperforms existing
black-box attacks via substitute models.","['Pin-Yu Chen', 'Huan Zhang', 'Yash Sharma', 'Jinfeng Yi', 'Cho-Jui Hsieh']","['stat.ML', 'cs.CR', 'cs.LG']",2017-08-14 03:48:03+00:00
http://arxiv.org/abs/1708.03995v1,Sentiment Analysis by Joint Learning of Word Embeddings and Classifier,"Word embeddings are representations of individual words of a text document in
a vector space and they are often use- ful for performing natural language pro-
cessing tasks. Current state of the art al- gorithms for learning word
embeddings learn vector representations from large corpora of text documents in
an unsu- pervised fashion. This paper introduces SWESA (Supervised Word
Embeddings for Sentiment Analysis), an algorithm for sentiment analysis via
word embeddings. SWESA leverages document label infor- mation to learn vector
representations of words from a modest corpus of text doc- uments by solving an
optimization prob- lem that minimizes a cost function with respect to both word
embeddings as well as classification accuracy. Analysis re- veals that SWESA
provides an efficient way of estimating the dimension of the word embeddings
that are to be learned. Experiments on several real world data sets show that
SWESA has superior per- formance when compared to previously suggested
approaches to word embeddings and sentiment analysis tasks.","['Prathusha Kameswara Sarma', 'Bill Sethares']","['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']",2017-08-14 02:40:20+00:00
http://arxiv.org/abs/1708.03951v2,"Optimization of Ensemble Supervised Learning Algorithms for Increased Sensitivity, Specificity, and AUC of Population-Based Colorectal Cancer Screenings","Over 150,000 new people in the United States are diagnosed with colorectal
cancer each year. Nearly a third die from it (American Cancer Society). The
only approved noninvasive diagnosis tools currently involve fecal blood count
tests (FOBTs) or stool DNA tests. Fecal blood count tests take only five
minutes and are available over the counter for as low as \$15. They are highly
specific, yet not nearly as sensitive, yielding a high percentage (25%) of
false negatives (Colon Cancer Alliance). Moreover, FOBT results are far too
generalized, meaning that a positive result could mean much more than just
colorectal cancer, and could just as easily mean hemorrhoids, anal fissure,
proctitis, Crohn's disease, diverticulosis, ulcerative colitis, rectal ulcer,
rectal prolapse, ischemic colitis, angiodysplasia, rectal trauma, proctitis
from radiation therapy, and others. Stool DNA tests, the modern benchmark for
CRC screening, have a much higher sensitivity and specificity, but also cost
\$600, take two weeks to process, and are not for high-risk individuals or
people with a history of polyps. To yield a cheap and effective CRC screening
alternative, a unique ensemble-based classification algorithm is put in place
that considers the FIT result, BMI, smoking history, and diabetic status of
patients. This method is tested under ten-fold cross validation to have a .95
AUC, 92% specificity, 89% sensitivity, .88 F1, and 90% precision. Once
clinically validated, this test promises to be cheaper, faster, and potentially
more accurate when compared to a stool DNA test.","['Anirudh Kamath', 'Aditya Singh', 'Raj Ramnani', 'Ayush Vyas', 'Jay Shenoy']","['stat.ML', 'cs.AI', 'q-bio.QM']",2017-08-13 18:48:58+00:00
http://arxiv.org/abs/1708.03914v1,Mahalanonbis Distance Informed by Clustering,"A fundamental question in data analysis, machine learning and signal
processing is how to compare between data points. The choice of the distance
metric is specifically challenging for high-dimensional data sets, where the
problem of meaningfulness is more prominent (e.g. the Euclidean distance
between images). In this paper, we propose to exploit a property of
high-dimensional data that is usually ignored - which is the structure stemming
from the relationships between the coordinates. Specifically we show that
organizing similar coordinates in clusters can be exploited for the
construction of the Mahalanobis distance between samples. When the observable
samples are generated by a nonlinear transformation of hidden variables, the
Mahalanobis distance allows the recovery of the Euclidean distances in the
hidden space.We illustrate the advantage of our approach on a synthetic example
where the discovery of clusters of correlated coordinates improves the
estimation of the principal directions of the samples. Our method was applied
to real data of gene expression for lung adenocarcinomas (lung cancer). By
using the proposed metric we found a partition of subjects to risk groups with
a good separation between their Kaplan-Meier survival plot.","['Almog Lahav', 'Ronen Talmon', 'Yuval Kluger']",['stat.ML'],2017-08-13 14:30:03+00:00
http://arxiv.org/abs/1708.03845v2,Quantifying multivariate redundancy with maximum entropy decompositions of mutual information,"Williams and Beer (2010) proposed a nonnegative mutual information
decomposition, based on the construction of redundancy lattices, which allows
separating the information that a set of variables contains about a target
variable into nonnegative components interpretable as the unique information of
some variables not provided by others as well as redundant and synergistic
components. However, the definition of multivariate measures of redundancy that
comply with nonnegativity and conform to certain axioms that capture
conceptually desirable properties of redundancy has proven to be elusive. We
here present a procedure to determine nonnegative multivariate redundancy
measures, within the maximum entropy framework. In particular, we generalize
existing bivariate maximum entropy measures of redundancy and unique
information, defining measures of the redundant information that a group of
variables has about a target, and of the unique redundant information that a
group of variables has about a target that is not redundant with information
from another group. The two key ingredients for this approach are: First, the
identification of a type of constraints on entropy maximization that allows
isolating components of redundancy and unique redundancy by mirroring them to
synergy components. Second, the construction of rooted tree-based
decompositions of the mutual information, which conform to the axioms of the
redundancy lattice by the local implementation at each tree node of binary
unfoldings of the information using hierarchically related maximum entropy
constraints. Altogether, the proposed measures quantify the different
multivariate redundancy contributions of a nonnegative mutual information
decomposition consistent with the redundancy lattice.",['Daniel Chicharro'],"['physics.data-an', 'q-bio.NC', 'stat.ML', '94A15, 94A17']",2017-08-13 03:29:57+00:00
http://arxiv.org/abs/1708.04232v1,Encoding Multi-Resolution Brain Networks Using Unsupervised Deep Learning,"The main goal of this study is to extract a set of brain networks in multiple
time-resolutions to analyze the connectivity patterns among the anatomic
regions for a given cognitive task. We suggest a deep architecture which learns
the natural groupings of the connectivity patterns of human brain in multiple
time-resolutions. The suggested architecture is tested on task data set of
Human Connectome Project (HCP) where we extract multi-resolution networks, each
of which corresponds to a cognitive task. At the first level of this
architecture, we decompose the fMRI signal into multiple sub-bands using
wavelet decompositions. At the second level, for each sub-band, we estimate a
brain network extracted from short time windows of the fMRI signal. At the
third level, we feed the adjacency matrices of each mesh network at each
time-resolution into an unsupervised deep learning algorithm, namely, a Stacked
De- noising Auto-Encoder (SDAE). The outputs of the SDAE provide a compact
connectivity representation for each time window at each sub-band of the fMRI
signal. We concatenate the learned representations of all sub-bands at each
window and cluster them by a hierarchical algorithm to find the natural
groupings among the windows. We observe that each cluster represents a
cognitive task with a performance of 93% Rand Index and 71% Adjusted Rand
Index. We visualize the mean values and the precisions of the networks at each
component of the cluster mixture. The mean brain networks at cluster centers
show the variations among cognitive tasks and the precision of each cluster
shows the within cluster variability of networks, across the subjects.","['Arash Rahnama', 'Abdullah Alchihabi', 'Vijay Gupta', 'Panos Antsaklis', 'Fatos T. Yarman Vural']","['stat.ML', 'cs.CV', 'q-bio.NC']",2017-08-13 01:43:11+00:00
http://arxiv.org/abs/1708.03788v1,Direct-Manipulation Visualization of Deep Networks,"The recent successes of deep learning have led to a wave of interest from
non-experts. Gaining an understanding of this technology, however, is
difficult. While the theory is important, it is also helpful for novices to
develop an intuitive feel for the effect of different hyperparameters and
structural variations. We describe TensorFlow Playground, an interactive, open
sourced visualization that allows users to experiment via direct manipulation
rather than coding, enabling them to quickly build an intuition about neural
nets.","['Daniel Smilkov', 'Shan Carter', 'D. Sculley', 'Fernanda B. Viégas', 'Martin Wattenberg']","['cs.LG', 'cs.HC', 'stat.ML']",2017-08-12 15:36:26+00:00
http://arxiv.org/abs/1708.03741v1,Online Convex Optimization with Stochastic Constraints,"This paper considers online convex optimization (OCO) with stochastic
constraints, which generalizes Zinkevich's OCO over a known simple fixed set by
introducing multiple stochastic functional constraints that are i.i.d.
generated at each round and are disclosed to the decision maker only after the
decision is made. This formulation arises naturally when decisions are
restricted by stochastic environments or deterministic environments with noisy
observations. It also includes many important problems as special cases, such
as OCO with long term constraints, stochastic constrained convex optimization,
and deterministic constrained convex optimization. To solve this problem, this
paper proposes a new algorithm that achieves $O(\sqrt{T})$ expected regret and
constraint violations and $O(\sqrt{T}\log(T))$ high probability regret and
constraint violations. Experiments on a real-world data center scheduling
problem further verify the performance of the new algorithm.","['Hao Yu', 'Michael J. Neely', 'Xiaohan Wei']","['math.OC', 'stat.ML']",2017-08-12 04:11:04+00:00
http://arxiv.org/abs/1708.03735v2,Sparse Coding and Autoencoders,"In ""Dictionary Learning"" one tries to recover incoherent matrices $A^* \in
\mathbb{R}^{n \times h}$ (typically overcomplete and whose columns are assumed
to be normalized) and sparse vectors $x^* \in \mathbb{R}^h$ with a small
support of size $h^p$ for some $0 <p < 1$ while having access to observations
$y \in \mathbb{R}^n$ where $y = A^*x^*$. In this work we undertake a rigorous
analysis of whether gradient descent on the squared loss of an autoencoder can
solve the dictionary learning problem. The ""Autoencoder"" architecture we
consider is a $\mathbb{R}^n \rightarrow \mathbb{R}^n$ mapping with a single
ReLU activation layer of size $h$.
  Under very mild distributional assumptions on $x^*$, we prove that the norm
of the expected gradient of the standard squared loss function is
asymptotically (in sparse code dimension) negligible for all points in a small
neighborhood of $A^*$. This is supported with experimental evidence using
synthetic data. We also conduct experiments to suggest that $A^*$ is a local
minimum. Along the way we prove that a layer of ReLU gates can be set up to
automatically recover the support of the sparse codes. This property holds
independent of the loss function. We believe that it could be of independent
interest.","['Akshay Rangamani', 'Anirbit Mukherjee', 'Amitabh Basu', 'Tejaswini Ganapathy', 'Ashish Arora', 'Sang Chin', 'Trac D. Tran']","['cs.LG', 'math.OC', 'stat.ML']",2017-08-12 01:02:47+00:00
http://arxiv.org/abs/1708.03731v3,OpenML Benchmarking Suites,"Machine learning research depends on objectively interpretable, comparable,
and reproducible algorithm benchmarks. We advocate the use of curated,
comprehensive suites of machine learning tasks to standardize the setup,
execution, and reporting of benchmarks. We enable this through software tools
that help to create and leverage these benchmarking suites. These are
seamlessly integrated into the OpenML platform, and accessible through
interfaces in Python, Java, and R. OpenML benchmarking suites (a) are easy to
use through standardized data formats, APIs, and client libraries; (b) come
with extensive meta-information on the included datasets; and (c) allow
benchmarks to be shared and reused in future studies. We then present a first,
carefully curated and practical benchmarking suite for classification: the
OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18). Finally,
we discuss use cases and applications which demonstrate the usefulness of
OpenML benchmarking suites and the OpenML-CC18 in particular.","['Bernd Bischl', 'Giuseppe Casalicchio', 'Matthias Feurer', 'Pieter Gijsbers', 'Frank Hutter', 'Michel Lang', 'Rafael G. Mantovani', 'Jan N. van Rijn', 'Joaquin Vanschoren']","['stat.ML', 'cs.LG']",2017-08-11 23:28:48+00:00
http://arxiv.org/abs/1708.03704v1,Deep Incremental Boosting,"This paper introduces Deep Incremental Boosting, a new technique derived from
AdaBoost, specifically adapted to work with Deep Learning methods, that reduces
the required training time and improves generalisation. We draw inspiration
from Transfer of Learning approaches to reduce the start-up time to training
each incremental Ensemble member. We show a set of experiments that outlines
some preliminary results on some common Deep Learning datasets and discuss the
potential improvements Deep Incremental Boosting brings to traditional Ensemble
methods in Deep Learning.","['Alan Mosca', 'George D Magoulas']","['stat.ML', 'cs.CV', 'cs.LG']",2017-08-11 21:05:58+00:00
http://arxiv.org/abs/1708.03665v1,Time Series Anomaly Detection; Detection of anomalous drops with limited features and sparse examples in noisy highly periodic data,"Google uses continuous streams of data from industry partners in order to
deliver accurate results to users. Unexpected drops in traffic can be an
indication of an underlying issue and may be an early warning that remedial
action may be necessary. Detecting such drops is non-trivial because streams
are variable and noisy, with roughly regular spikes (in many different shapes)
in traffic data. We investigated the question of whether or not we can predict
anomalies in these data streams. Our goal is to utilize Machine Learning and
statistical approaches to classify anomalous drops in periodic, but noisy,
traffic patterns. Since we do not have a large body of labeled examples to
directly apply supervised learning for anomaly classification, we approached
the problem in two parts. First we used TensorFlow to train our various models
including DNNs, RNNs, and LSTMs to perform regression and predict the expected
value in the time series. Secondly we created anomaly detection rules that
compared the actual values to predicted values. Since the problem requires
finding sustained anomalies, rather than just short delays or momentary
inactivity in the data, our two detection methods focused on continuous
sections of activity rather than just single points. We tried multiple
combinations of our models and rules and found that using the intersection of
our two anomaly detection methods proved to be an effective method of detecting
anomalies on almost all of our models. In the process we also found that not
all data fell within our experimental assumptions, as one data stream had no
periodicity, and therefore no time based model could predict it.","['Dominique T. Shipmon', 'Jason M. Gurevitch', 'Paolo M. Piselli', 'Stephen T. Edwards']","['stat.ML', 'cs.LG']",2017-08-11 19:04:53+00:00
http://arxiv.org/abs/1708.04680v1,Augmentor: An Image Augmentation Library for Machine Learning,"The generation of artificial data based on existing observations, known as
data augmentation, is a technique used in machine learning to improve model
accuracy, generalisation, and to control overfitting. Augmentor is a software
package, available in both Python and Julia versions, that provides a high
level API for the expansion of image data using a stochastic, pipeline-based
approach which effectively allows for images to be sampled from a distribution
of augmented images at runtime. Augmentor provides methods for most standard
augmentation practices as well as several advanced features such as
label-preserving, randomised elastic distortions, and provides many helper
functions for typical augmentation tasks used in machine learning.","['Marcus D. Bloice', 'Christof Stocker', 'Andreas Holzinger']","['cs.CV', 'cs.LG', 'stat.ML']",2017-08-11 11:19:44+00:00
http://arxiv.org/abs/1708.03498v2,Neural Expectation Maximization,"Many real world tasks such as reasoning and physical interaction require
identification and manipulation of conceptual entities. A first step towards
solving these tasks is the automated discovery of distributed symbol-like
representations. In this paper, we explicitly formalize this problem as
inference in a spatial mixture model where each component is parametrized by a
neural network. Based on the Expectation Maximization framework we then derive
a differentiable clustering method that simultaneously learns how to group and
represent individual entities. We evaluate our method on the (sequential)
perceptual grouping task and find that it is able to accurately recover the
constituent objects. We demonstrate that the learned representations are useful
for next-step prediction.","['Klaus Greff', 'Sjoerd van Steenkiste', 'Jürgen Schmidhuber']","['cs.LG', 'cs.NE', 'stat.ML', 'I.2.6']",2017-08-11 10:17:23+00:00
http://arxiv.org/abs/1708.04529v1,Learning from Noisy Label Distributions,"In this paper, we consider a novel machine learning problem, that is,
learning a classifier from noisy label distributions. In this problem, each
instance with a feature vector belongs to at least one group. Then, instead of
the true label of each instance, we observe the label distribution of the
instances associated with a group, where the label distribution is distorted by
an unknown noise. Our goals are to (1) estimate the true label of each
instance, and (2) learn a classifier that predicts the true label of a new
instance. We propose a probabilistic model that considers true label
distributions of groups and parameters that represent the noise as hidden
variables. The model can be learned based on a variational Bayesian method. In
numerical experiments, we show that the proposed model outperforms existing
methods in terms of the estimation of the true labels of instances.",['Yuya Yoshikawa'],"['cs.LG', 'cs.AI', 'stat.ML']",2017-08-11 03:25:46+00:00
http://arxiv.org/abs/1708.03392v1,Jumping across biomedical contexts using compressive data fusion,"Motivation: The rapid growth of diverse biological data allows us to consider
interactions between a variety of objects, such as genes, chemicals, molecular
signatures, diseases, pathways and environmental exposures. Often, any pair of
objects--such as a gene and a disease--can be related in different ways, for
example, directly via gene-disease associations or indirectly via functional
annotations, chemicals and pathways. Different ways of relating these objects
carry different semantic meanings. However, traditional methods disregard these
semantics and thus cannot fully exploit their value in data modeling.
  Results: We present Medusa, an approach to detect size-k modules of objects
that, taken together, appear most significant to another set of objects. Medusa
operates on large-scale collections of heterogeneous data sets and explicitly
distinguishes between diverse data semantics. It advances research along two
dimensions: it builds on collective matrix factorization to derive different
semantics, and it formulates the growing of the modules as a submodular
optimization program. Medusa is flexible in choosing or combining semantic
meanings and provides theoretical guarantees about detection quality. In a
systematic study on 310 complex diseases, we show the effectiveness of Medusa
in associating genes with diseases and detecting disease modules. We
demonstrate that in predicting gene-disease associations Medusa compares
favorably to methods that ignore diverse semantic meanings. We find that the
utility of different semantics depends on disease categories and that, overall,
Medusa recovers disease modules more accurately when combining different
semantics.","['Marinka Zitnik', 'Blaz Zupan']","['cs.LG', 'q-bio.MN', 'stat.ML']",2017-08-10 21:39:54+00:00
http://arxiv.org/abs/1708.03288v4,Subset Selection with Shrinkage: Sparse Linear Modeling when the SNR is low,"We study a seemingly unexpected and relatively less understood overfitting
aspect of a fundamental tool in sparse linear modeling - best subset selection,
which minimizes the residual sum of squares subject to a constraint on the
number of nonzero coefficients. While the best subset selection procedure is
often perceived as the ""gold standard"" in sparse learning when the signal to
noise ratio (SNR) is high, its predictive performance deteriorates when the SNR
is low. In particular, it is outperformed by continuous shrinkage methods, such
as ridge regression and the Lasso. We investigate the behavior of best subset
selection in the high-noise regimes and propose an alternative approach based
on a regularized version of the least-squares criterion. Our proposed
estimators (a) mitigate, to a large extent, the poor predictive performance of
best subset selection in the high-noise regimes; and (b) perform favorably,
while generally delivering substantially sparser models, relative to the best
predictive models available via ridge regression and the Lasso. We conduct an
extensive theoretical analysis of the predictive properties of the proposed
approach and provide justification for its superior predictive performance
relative to best subset selection when the noise-level is high. Our estimators
can be expressed as solutions to mixed integer second order conic optimization
problems and, hence, are amenable to modern computational tools from
mathematical optimization.","['Rahul Mazumder', 'Peter Radchenko', 'Antoine Dedieu']","['stat.ME', 'math.OC', 'math.ST', 'stat.CO', 'stat.ML', 'stat.TH']",2017-08-10 16:28:39+00:00
http://arxiv.org/abs/1708.03229v1,Automatic Selection of t-SNE Perplexity,"t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely
used dimensionality reduction methods for data visualization, but it has a
perplexity hyperparameter that requires manual selection. In practice, proper
tuning of t-SNE perplexity requires users to understand the inner working of
the method as well as to have hands-on experience. We propose a model selection
objective for t-SNE perplexity that requires negligible extra computation
beyond that of the t-SNE itself. We empirically validate that the perplexity
settings found by our approach are consistent with preferences elicited from
human experts across a number of datasets. The similarities of our approach to
Bayesian information criteria (BIC) and minimum description length (MDL) are
also analyzed.","['Yanshuai Cao', 'Luyu Wang']","['cs.AI', 'cs.LG', 'stat.AP', 'stat.ML']",2017-08-10 14:19:20+00:00
http://arxiv.org/abs/1709.06622v1,Distributed Training Large-Scale Deep Architectures,"Scale of data and scale of computation infrastructures together enable the
current deep learning renaissance. However, training large-scale deep
architectures demands both algorithmic improvement and careful system
configuration. In this paper, we focus on employing the system approach to
speed up large-scale training. Via lessons learned from our routine
benchmarking effort, we first identify bottlenecks and overheads that hinter
data parallelism. We then devise guidelines that help practitioners to
configure an effective system and fine-tune parameters to achieve desired
speedup. Specifically, we develop a procedure for setting minibatch size and
choosing computation algorithms. We also derive lemmas for determining the
quantity of key components such as the number of GPUs and parameter servers.
Experiments and examples show that these guidelines help effectively speed up
large-scale deep learning training.","['Shang-Xuan Zou', 'Chun-Yen Chen', 'Jui-Lin Wu', 'Chun-Nan Chou', 'Chia-Chin Tsao', 'Kuan-Chieh Tung', 'Ting-Wei Lin', 'Cheng-Lung Sung', 'Edward Y. Chang']","['cs.DC', 'cs.LG', 'stat.ML']",2017-08-10 09:24:27+00:00
http://arxiv.org/abs/1708.03131v1,Hypotheses testing on infinite random graphs,"Drawing on some recent results that provide the formalism necessary to
definite stationarity for infinite random graphs, this paper initiates the
study of statistical and learning questions pertaining to these objects.
Specifically, a criterion for the existence of a consistent test for complex
hypotheses is presented, generalizing the corresponding results on time series.
As an application, it is shown how one can test that a tree has the Markov
property, or, more generally, to estimate its memory.",['Daniil Ryabko'],"['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2017-08-10 09:11:31+00:00
http://arxiv.org/abs/1708.03052v1,Communication-Free Parallel Supervised Topic Models,"Embarrassingly (communication-free) parallel Markov chain Monte Carlo (MCMC)
methods are commonly used in learning graphical models. However, MCMC cannot be
directly applied in learning topic models because of the quasi-ergodicity
problem caused by multimodal distribution of topics. In this paper, we develop
an embarrassingly parallel MCMC algorithm for sLDA. Our algorithm works by
switching the order of sampled topics combination and labeling variable
prediction in sLDA, in which it overcomes the quasi-ergodicity problem because
high-dimension topics that follow a multimodal distribution are projected into
one-dimension document labels that follow a unimodal distribution. Our
empirical experiments confirm that the out-of-sample prediction performance
using our embarrassingly parallel algorithm is comparable to non-parallel sLDA
while the computation time is significantly reduced.","['Lee Gao', 'Ronghuo Zheng']","['cs.LG', 'cs.CL', 'cs.IR', 'stat.ML']",2017-08-10 02:03:52+00:00
http://arxiv.org/abs/1708.03046v2,When Is the First Spurious Variable Selected by Sequential Regression Procedures?,"Applied statisticians use sequential regression procedures to produce a
ranking of explanatory variables and, in settings of low correlations between
variables and strong true effect sizes, expect that variables at the very top
of this ranking are truly relevant to the response. In a regime of certain
sparsity levels, however, three examples of sequential procedures--forward
stepwise, the lasso, and least angle regression--are shown to include the first
spurious variable unexpectedly early. We derive a rigorous, sharp prediction of
the rank of the first spurious variable for these three procedures,
demonstrating that the first spurious variable occurs earlier and earlier as
the regression coefficients become denser. This counterintuitive phenomenon
persists for statistically independent Gaussian random designs and an
arbitrarily large magnitude of the true effects. We gain a better understanding
of the phenomenon by identifying the underlying cause and then leverage the
insights to introduce a simple visualization tool termed the double-ranking
diagram to improve on sequential methods. As a byproduct of these findings, we
obtain the first provable result certifying the exact equivalence between the
lasso and least angle regression in the early stages of solution paths beyond
orthogonal designs. This equivalence can seamlessly carry over many important
model selection results concerning the lasso to least angle regression.",['Weijie J. Su'],"['math.ST', 'stat.ML', 'stat.TH']",2017-08-10 01:56:16+00:00
http://arxiv.org/abs/1708.03027v1,Using Deep Neural Networks to Automate Large Scale Statistical Analysis for Big Data Applications,"Statistical analysis (SA) is a complex process to deduce population
properties from analysis of data. It usually takes a well-trained analyst to
successfully perform SA, and it becomes extremely challenging to apply SA to
big data applications. We propose to use deep neural networks to automate the
SA process. In particular, we propose to construct convolutional neural
networks (CNNs) to perform automatic model selection and parameter estimation,
two most important SA tasks. We refer to the resulting CNNs as the neural model
selector and the neural model estimator, respectively, which can be properly
trained using labeled data systematically generated from candidate models.
Simulation study shows that both the selector and estimator demonstrate
excellent performances. The idea and proposed framework can be further extended
to automate the entire SA process and have the potential to revolutionize how
SA is performed in big data analytics.","['Rongrong Zhang', 'Wei Deng', 'Michael Yu Zhu']","['stat.ML', 'cs.LG', 'stat.CO']",2017-08-09 22:34:36+00:00
http://arxiv.org/abs/1708.03020v3,"Non-stationary Stochastic Optimization under $L_{p,q}$-Variation Measures","We consider a non-stationary sequential stochastic optimization problem, in
which the underlying cost functions change over time under a variation budget
constraint. We propose an $L_{p,q}$-variation functional to quantify the
change, which yields less variation for dynamic function sequences whose
changes are constrained to short time periods or small subsets of input domain.
Under the $L_{p,q}$-variation constraint, we derive both upper and matching
lower regret bounds for smooth and strongly convex function sequences, which
generalize previous results in Besbes et al. (2015). Furthermore, we provide an
upper bound for general convex function sequences with noisy gradient feedback,
which matches the optimal rate as $p\to\infty$. Our results reveal some
surprising phenomena under this general variation functional, such as the curse
of dimensionality of the function domain. The key technical novelties in our
analysis include affinity lemmas that characterize the distance of the
minimizers of two convex functions with bounded Lp difference, and a cubic
spline based construction that attains matching lower bounds.","['Xi Chen', 'Yining Wang', 'Yu-Xiang Wang']","['stat.ML', 'cs.LG']",2017-08-09 21:36:38+00:00
http://arxiv.org/abs/1708.02979v1,Tikhonov Regularization for Long Short-Term Memory Networks,"It is a well-known fact that adding noise to the input data often improves
network performance. While the dropout technique may be a cause of memory loss,
when it is applied to recurrent connections, Tikhonov regularization, which can
be regarded as the training with additive noise, avoids this issue naturally,
though it implies regularizer derivation for different architectures. In case
of feedforward neural networks this is straightforward, while for networks with
recurrent connections and complicated layers it leads to some difficulties. In
this paper, a Tikhonov regularizer is derived for Long-Short Term Memory (LSTM)
networks. Although it is independent of time for simplicity, it considers
interaction between weights of the LSTM unit, which in theory makes it possible
to regularize the unit with complicated dependences by using only one parameter
that measures the input data perturbation. The regularizer that is proposed in
this paper has three parameters: one to control the regularization process, and
other two to maintain computation stability while the network is being trained.
The theory developed in this paper can be applied to get such regularizers for
different recurrent neural networks with Hadamard products and Lipschitz
continuous functions.",['Andrei Turkin'],"['cs.LG', 'cs.NE', 'stat.ML']",2017-08-09 19:34:26+00:00
http://arxiv.org/abs/1708.02975v3,Anomaly Detection on Graph Time Series,"In this paper, we use variational recurrent neural network to investigate the
anomaly detection problem on graph time series. The temporal correlation is
modeled by the combination of recurrent neural network (RNN) and variational
inference (VI), while the spatial information is captured by the graph
convolutional network. In order to incorporate external factors, we use feature
extractor to augment the transition of latent variables, which can learn the
influence of external factors. With the target function as accumulative ELBO,
it is easy to extend this model to on-line method. The experimental study on
traffic flow data shows the detection capability of the proposed method.",['Daniel Hsu'],"['cs.LG', 'cs.NE', 'stat.ML']",2017-08-09 19:15:56+00:00
http://arxiv.org/abs/1708.02949v3,Classification without labels: Learning from mixed samples in high energy physics,"Modern machine learning techniques can be used to construct powerful models
for difficult collider physics problems. In many applications, however, these
models are trained on imperfect simulations due to a lack of truth-level
information in the data, which risks the model learning artifacts of the
simulation. In this paper, we introduce the paradigm of classification without
labels (CWoLa) in which a classifier is trained to distinguish statistical
mixtures of classes, which are common in collider physics. Crucially, neither
individual labels nor class proportions are required, yet we prove that the
optimal classifier in the CWoLa paradigm is also the optimal classifier in the
traditional fully-supervised case where all label information is available.
After demonstrating the power of this method in an analytical toy example, we
consider a realistic benchmark for collider physics: distinguishing quark-
versus gluon-initiated jets using mixed quark/gluon training samples. More
generally, CWoLa can be applied to any classification problem where labels or
class proportions are unknown or simulations are unreliable, but statistical
mixtures of the classes are available.","['Eric M. Metodiev', 'Benjamin Nachman', 'Jesse Thaler']","['hep-ph', 'hep-ex', 'stat.ML']",2017-08-09 18:00:06+00:00
http://arxiv.org/abs/1708.02918v2,The Tensor Memory Hypothesis,"We discuss memory models which are based on tensor decompositions using
latent representations of entities and events. We show how episodic memory and
semantic memory can be realized and discuss how new memory traces can be
generated from sensory input: Existing memories are the basis for perception
and new memories are generated via perception. We relate our mathematical
approach to the hippocampal memory indexing theory. We describe the first
detailed mathematical models for the complete processing pipeline from sensory
input and its semantic decoding, i.e., perception, to the formation of episodic
and semantic memories and their declarative semantic decodings. Our main
hypothesis is that perception includes an active semantic decoding process,
which relies on latent representations of entities and predicates, and that
episodic and semantic memories depend on the same decoding process. We
contribute to the debate between the leading memory consolidation theories,
i.e., the standard consolidation theory (SCT) and the multiple trace theory
(MTT). The latter is closely related to the complementary learning systems
(CLS) framework. In particular, we show explicitly how episodic memory can
teach the neocortex to form a semantic memory, which is a core issue in MTT and
CLS.","['Volker Tresp', 'Yunpu Ma']","['cs.AI', 'cs.LG', 'q-bio.NC', 'stat.ML']",2017-08-09 17:22:18+00:00
http://arxiv.org/abs/1708.02900v1,How Do People Differ? A Social Media Approach,"Research from a variety of fields including psychology and linguistics have
found correlations and patterns in personal attributes and behavior, but
efforts to understand the broader heterogeneity in human behavior have not yet
integrated these approaches and perspectives with a cohesive methodology. Here
we extract patterns in behavior and relate those patterns together in a
high-dimensional picture. We use dimension reduction to analyze word usage in
text data from the online discussion platform Reddit. We find that pronouns can
be used to characterize the space of the two most prominent dimensions that
capture the greatest differences in word usage, even though pronouns were not
included in the determination of those dimensions. These patterns overlap with
patterns of topics of discussion to reveal relationships between pronouns and
topics that can describe the user population. This analysis corroborates
findings from past research that have identified word use differences across
populations and synthesizes them relative to one another. We believe this is a
step toward understanding how differences between people are related to each
other.","['Vincent Wong', 'Yaneer Bar-Yam']","['physics.soc-ph', 'cs.SI', 'nlin.AO', 'physics.data-an', 'stat.ML']",2017-08-09 16:30:37+00:00
http://arxiv.org/abs/1708.02883v3,Maximum Volume Inscribed Ellipsoid: A New Simplex-Structured Matrix Factorization Framework via Facet Enumeration and Convex Optimization,"Consider a structured matrix factorization model where one factor is
restricted to have its columns lying in the unit simplex. This
simplex-structured matrix factorization (SSMF) model and the associated
factorization techniques have spurred much interest in research topics over
different areas, such as hyperspectral unmixing in remote sensing, topic
discovery in machine learning, to name a few. In this paper we develop a new
theoretical SSMF framework whose idea is to study a maximum volume ellipsoid
inscribed in the convex hull of the data points. This maximum volume inscribed
ellipsoid (MVIE) idea has not been attempted in prior literature, and we show a
sufficient condition under which the MVIE framework guarantees exact recovery
of the factors. The sufficient recovery condition we show for MVIE is much more
relaxed than that of separable non-negative matrix factorization (or pure-pixel
search); coincidentally it is also identical to that of minimum volume
enclosing simplex, which is known to be a powerful SSMF framework for
non-separable problem instances. We also show that MVIE can be practically
implemented by performing facet enumeration and then by solving a convex
optimization problem. The potential of the MVIE framework is illustrated by
numerical results.","['Chia-Hsiang Lin', 'Ruiyuan Wu', 'Wing-Kin Ma', 'Chong-Yung Chi', 'Yue Wang']",['stat.ML'],2017-08-09 15:48:58+00:00
http://arxiv.org/abs/1708.02867v1,Simulated Annealing with Levy Distribution for Fast Matrix Factorization-Based Collaborative Filtering,"Matrix factorization is one of the best approaches for collaborative
filtering, because of its high accuracy in presenting users and items latent
factors. The main disadvantages of matrix factorization are its complexity, and
being very hard to be parallelized, specially with very large matrices. In this
paper, we introduce a new method for collaborative filtering based on Matrix
Factorization by combining simulated annealing with levy distribution. By using
this method, good solutions are achieved in acceptable time with low
computations, compared to other methods like stochastic gradient descent,
alternating least squares, and weighted non-negative matrix factorization.","['Mostafa A. Shehata', 'Mohammad Nassef', 'Amr A. Badr']","['cs.LG', 'cs.IR', 'stat.ML']",2017-08-09 15:14:54+00:00
http://arxiv.org/abs/1708.02735v1,Gaussian Prototypical Networks for Few-Shot Learning on Omniglot,"We propose a novel architecture for $k$-shot classification on the Omniglot
dataset. Building on prototypical networks, we extend their architecture to
what we call Gaussian prototypical networks. Prototypical networks learn a map
between images and embedding vectors, and use their clustering for
classification. In our model, a part of the encoder output is interpreted as a
confidence region estimate about the embedding point, and expressed as a
Gaussian covariance matrix. Our network then constructs a direction and class
dependent distance metric on the embedding space, using uncertainties of
individual data points as weights. We show that Gaussian prototypical networks
are a preferred architecture over vanilla prototypical networks with an
equivalent number of parameters. We report state-of-the-art performance in
1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot
5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.
We explore artificially down-sampling a fraction of images in the training set,
which improves our performance even further. We therefore hypothesize that
Gaussian prototypical networks might perform better in less homogeneous,
noisier datasets, which are commonplace in real world applications.",['Stanislav Fort'],"['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2017-08-09 06:53:31+00:00
http://arxiv.org/abs/1708.02691v3,Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations,"This article concerns the expressive power of depth in neural nets with ReLU
activations and bounded width. We are particularly interested in the following
questions: what is the minimal width $w_{\text{min}}(d)$ so that ReLU nets of
width $w_{\text{min}}(d)$ (and arbitrary depth) can approximate any continuous
function on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this
minimal width, what can one say about the depth necessary to approximate a
given function? Our approach to this paper is based on the observation that,
due to the convexity of the ReLU activation, ReLU nets are particularly
well-suited for representing convex functions. In particular, we prove that
ReLU nets with width $d+1$ can approximate any continuous convex function of
$d$ variables arbitrarily well. These results then give quantitative depth
estimates for the rate of approximation of any continuous scalar function on
the $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$",['Boris Hanin'],"['stat.ML', 'cs.CG', 'cs.LG', 'math.FA', 'math.ST', 'stat.TH']",2017-08-09 01:37:21+00:00
http://arxiv.org/abs/1708.03218v2,Improved Fixed-Rank Nyström Approximation via QR Decomposition: Practical and Theoretical Aspects,"The Nystrom method is a popular technique that uses a small number of
landmark points to compute a fixed-rank approximation of large kernel matrices
that arise in machine learning problems. In practice, to ensure high quality
approximations, the number of landmark points is chosen to be greater than the
target rank. However, for simplicity the standard Nystrom method uses a
sub-optimal procedure for rank reduction. In this paper, we examine the
drawbacks of the standard Nystrom method in terms of poor performance and lack
of theoretical guarantees. To address these issues, we present an efficient
modification for generating improved fixed-rank Nystrom approximations.
Theoretical analysis and numerical experiments are provided to demonstrate the
advantages of the modified method over the standard Nystrom method. Overall,
the aim of this paper is to convince researchers to use the modified method, as
it has nearly identical computational complexity, is easy to code, has greatly
improved accuracy in many cases, and is optimal in a sense that we make
precise.","['Farhad Pourkamali-Anaraki', 'Stephen Becker']","['stat.ML', 'cs.CV', 'cs.LG']",2017-08-08 23:52:53+00:00
http://arxiv.org/abs/1708.02666v1,Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017),"This is the Proceedings of the 2017 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,
2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.","['Been Kim', 'Dmitry M. Malioutov', 'Kush R. Varshney', 'Adrian Weller']","['stat.ML', 'cs.LG']",2017-08-08 22:21:11+00:00
http://arxiv.org/abs/1708.02663v1,Gradient-enhanced kriging for high-dimensional problems,"Surrogate models provide a low computational cost alternative to evaluating
expensive functions. The construction of accurate surrogate models with large
numbers of independent variables is currently prohibitive because it requires a
large number of function evaluations. Gradient-enhanced kriging has the
potential to reduce the number of function evaluations for the desired accuracy
when efficient gradient computation, such as an adjoint method, is available.
However, current gradient-enhanced kriging methods do not scale well with the
number of sampling points due to the rapid growth in the size of the
correlation matrix where new information is added for each sampling point in
each direction of the design space. They do not scale well with the number of
independent variables either due to the increase in the number of
hyperparameters that needs to be estimated. To address this issue, we develop a
new gradient-enhanced surrogate model approach that drastically reduced the
number of hyperparameters through the use of the partial-least squares method
that maintains accuracy. In addition, this method is able to control the size
of the correlation matrix by adding only relevant points defined through the
information provided by the partial-least squares method. To validate our
method, we compare the global accuracy of the proposed method with conventional
kriging surrogate models on two analytic functions with up to 100 dimensions,
as well as engineering problems of varied complexity with up to 15 dimensions.
We show that the proposed method requires fewer sampling points than
conventional methods to obtain the desired accuracy, or provides more accuracy
for a fixed budget of sampling points. In some cases, we get over 3 times more
accurate models than a bench of surrogate models from the literature, and also
over 3200 times faster than standard gradient-enhanced kriging models.","['Mohamed Amine Bouhlel', 'Joaquim R. R. A. Martins']","['cs.LG', 'stat.ML']",2017-08-08 21:58:49+00:00
http://arxiv.org/abs/1708.05070v2,Data-driven Advice for Applying Machine Learning to Bioinformatics Problems,"As the bioinformatics field grows, it must keep pace not only with new data
but with new algorithms. Here we contribute a thorough analysis of 13
state-of-the-art, commonly used machine learning algorithms on a set of 165
publicly available classification problems in order to provide data-driven
algorithm recommendations to current researchers. We present a number of
statistical and visual comparisons of algorithm performance and quantify the
effect of model selection and algorithm tuning for each algorithm and dataset.
The analysis culminates in the recommendation of five algorithms with
hyperparameters that maximize classifier performance across the tested
problems, as well as general guidelines for applying machine learning to
supervised classification problems.","['Randal S. Olson', 'William La Cava', 'Zairah Mustahsan', 'Akshay Varik', 'Jason H. Moore']","['q-bio.QM', 'cs.LG', 'stat.ML']",2017-08-08 21:41:48+00:00
http://arxiv.org/abs/1708.02635v2,Anomaly Detection in Multivariate Non-stationary Time Series for Automatic DBMS Diagnosis,"Anomaly detection in database management systems (DBMSs) is difficult because
of increasing number of statistics (stat) and event metrics in big data system.
In this paper, I propose an automatic DBMS diagnosis system that detects
anomaly periods with abnormal DB stat metrics and finds causal events in the
periods. Reconstruction error from deep autoencoder and statistical process
control approach are applied to detect time period with anomalies. Related
events are found using time series similarity measures between events and
abnormal stat metrics. After training deep autoencoder with DBMS metric data,
efficacy of anomaly detection is investigated from other DBMSs containing
anomalies. Experiment results show effectiveness of proposed model, especially,
batch temporal normalization layer. Proposed model is used for publishing
automatic DBMS diagnosis reports in order to determine DBMS configuration and
SQL tuning.",['Doyup Lee'],"['stat.ML', 'cs.LG', 'stat.AP']",2017-08-08 20:04:19+00:00
http://arxiv.org/abs/1708.02620v1,Multilayer Spectral Graph Clustering via Convex Layer Aggregation: Theory and Algorithms,"Multilayer graphs are commonly used for representing different relations
between entities and handling heterogeneous data processing tasks. Non-standard
multilayer graph clustering methods are needed for assigning clusters to a
common multilayer node set and for combining information from each layer. This
paper presents a multilayer spectral graph clustering (SGC) framework that
performs convex layer aggregation. Under a multilayer signal plus noise model,
we provide a phase transition analysis of clustering reliability. Moreover, we
use the phase transition criterion to propose a multilayer iterative model
order selection algorithm (MIMOSA) for multilayer SGC, which features automated
cluster assignment and layer weight adaptation, and provides statistical
clustering reliability guarantees. Numerical simulations on synthetic
multilayer graphs verify the phase transition analysis, and experiments on
real-world multilayer graphs show that MIMOSA is competitive or better than
other clustering methods.","['Pin-Yu Chen', 'Alfred O. Hero']","['stat.ML', 'cs.SI']",2017-08-08 19:40:09+00:00
http://arxiv.org/abs/1708.02582v3,Cascade Adversarial Machine Learning Regularized with a Unified Embedding,"Injecting adversarial examples during training, known as adversarial
training, can improve robustness against one-step attacks, but not for unknown
iterative attacks. To address this challenge, we first show iteratively
generated adversarial images easily transfer between networks trained with the
same strategy. Inspired by this observation, we propose cascade adversarial
training, which transfers the knowledge of the end results of adversarial
training. We train a network from scratch by injecting iteratively generated
adversarial images crafted from already defended networks in addition to
one-step adversarial images from the network being trained. We also propose to
utilize embedding space for both classification and low-level (pixel-level)
similarity learning to ignore unknown pixel level perturbation. During
training, we inject adversarial images without replacing their corresponding
clean images and penalize the distance between the two embeddings (clean and
adversarial). Experimental results show that cascade adversarial training
together with our proposed low-level similarity learning efficiently enhances
the robustness against iterative attacks, but at the expense of decreased
robustness against one-step attacks. We show that combining those two
techniques can also improve robustness under the worst case black box attack
scenario.","['Taesik Na', 'Jong Hwan Ko', 'Saibal Mukhopadhyay']","['stat.ML', 'cs.LG']",2017-08-08 17:58:40+00:00
http://arxiv.org/abs/1708.02581v1,"Belief Propagation, Bethe Approximation and Polynomials","Factor graphs are important models for succinctly representing probability
distributions in machine learning, coding theory, and statistical physics.
Several computational problems, such as computing marginals and partition
functions, arise naturally when working with factor graphs. Belief propagation
is a widely deployed iterative method for solving these problems. However,
despite its significant empirical success, not much is known about the
correctness and efficiency of belief propagation.
  Bethe approximation is an optimization-based framework for approximating
partition functions. While it is known that the stationary points of the Bethe
approximation coincide with the fixed points of belief propagation, in general,
the relation between the Bethe approximation and the partition function is not
well understood. It has been observed that for a few classes of factor graphs,
the Bethe approximation always gives a lower bound to the partition function,
which distinguishes them from the general case, where neither a lower bound,
nor an upper bound holds universally. This has been rigorously proved for
permanents and for attractive graphical models.
  Here we consider bipartite normal factor graphs and show that if the local
constraints satisfy a certain analytic property, the Bethe approximation is a
lower bound to the partition function. We arrive at this result by viewing
factor graphs through the lens of polynomials. In this process, we reformulate
the Bethe approximation as a polynomial optimization problem. Our sufficient
condition for the lower bound property to hold is inspired by recent
developments in the theory of real stable polynomials. We believe that this way
of viewing factor graphs and its connection to real stability might lead to a
better understanding of belief propagation and factor graphs in general.","['Damian Straszak', 'Nisheeth K. Vishnoi']","['cs.LG', 'cs.DS', 'cs.IT', 'math.IT', 'stat.ML']",2017-08-08 17:56:15+00:00
http://arxiv.org/abs/1708.02556v4,Multi-Generator Generative Adversarial Nets,"We propose a new approach to train the Generative Adversarial Nets (GANs)
with a mixture of generators to overcome the mode collapsing problem. The main
intuition is to employ multiple generators, instead of using a single one as in
the original GAN. The idea is simple, yet proven to be extremely effective at
covering diverse data modes, easily overcoming the mode collapse and delivering
state-of-the-art results. A minimax formulation is able to establish among a
classifier, a discriminator, and a set of generators in a similar spirit with
GAN. Generators create samples that are intended to come from the same
distribution as the training data, whilst the discriminator determines whether
samples are true data or generated by generators, and the classifier specifies
which generator a sample comes from. The distinguishing feature is that
internal samples are created from multiple generators, and then one of them
will be randomly selected as final output similar to the mechanism of a
probabilistic mixture model. We term our method Mixture GAN (MGAN). We develop
theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon
divergence (JSD) between the mixture of generators' distributions and the
empirical data distribution is minimal, whilst the JSD among generators'
distributions is maximal, hence effectively avoiding the mode collapse. By
utilizing parameter sharing, our proposed model adds minimal computational cost
to the standard GAN, and thus can also efficiently scale to large-scale
datasets. We conduct extensive experiments on synthetic 2D data and natural
image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior
performance of our MGAN in achieving state-of-the-art Inception scores over
latest baselines, generating diverse and appealing recognizable objects at
different resolutions, and specializing in capturing different types of objects
by generators.","['Quan Hoang', 'Tu Dinh Nguyen', 'Trung Le', 'Dinh Phung']","['cs.LG', 'cs.AI', 'stat.ML']",2017-08-08 16:48:35+00:00
http://arxiv.org/abs/1708.02999v1,Demixing Structured Superposition Signals from Periodic and Aperiodic Nonlinear Observations,"We consider the demixing problem of two (or more) structured high-dimensional
vectors from a limited number of nonlinear observations where this nonlinearity
is due to either a periodic or an aperiodic function. We study certain families
of structured superposition models, and propose a method which provably
recovers the components given (nearly) $m = \mathcal{O}(s)$ samples where $s$
denotes the sparsity level of the underlying components. This strictly improves
upon previous nonlinear demixing techniques and asymptotically matches the best
possible sample complexity. We also provide a range of simulations to
illustrate the performance of the proposed algorithms.","['Mohammadreza Soltani', 'Chinmay Hegde']",['stat.ML'],2017-08-08 16:16:17+00:00
http://arxiv.org/abs/1708.02544v2,Stochastic Optimization with Bandit Sampling,"Many stochastic optimization algorithms work by estimating the gradient of
the cost function on the fly by sampling datapoints uniformly at random from a
training set. However, the estimator might have a large variance, which
inadvertently slows down the convergence rate of the algorithms. One way to
reduce this variance is to sample the datapoints from a carefully selected
non-uniform distribution. In this work, we propose a novel non-uniform sampling
approach that uses the multi-armed bandit framework. Theoretically, we show
that our algorithm asymptotically approximates the optimal variance within a
factor of 3. Empirically, we show that using this datapoint-selection technique
results in a significant reduction in the convergence time and variance of
several stochastic optimization algorithms such as SGD, SVRG and SAGA. This
approach for sampling datapoints is general, and can be used in conjunction
with any algorithm that uses an unbiased gradient estimation -- we expect it to
have broad applicability beyond the specific examples explored in this work.","['Farnood Salehi', 'L. Elisa Celis', 'Patrick Thiran']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2017-08-08 16:15:26+00:00
http://arxiv.org/abs/1708.02511v4,Parametric Adversarial Divergences are Good Losses for Generative Modeling,"Parametric adversarial divergences, which are a generalization of the losses
used to train generative adversarial networks (GANs), have often been described
as being approximations of their nonparametric counterparts, such as the
Jensen-Shannon divergence, which can be derived under the so-called optimal
discriminator assumption. In this position paper, we argue that despite being
""non-optimal"", parametric divergences have distinct properties from their
nonparametric counterparts which can make them more suitable for learning
high-dimensional distributions. A key property is that parametric divergences
are only sensitive to certain aspects/moments of the distribution, which depend
on the architecture of the discriminator and the loss it was trained with. In
contrast, nonparametric divergences such as the Kullback-Leibler divergence are
sensitive to moments ignored by the discriminator, but they do not necessarily
correlate with sample quality (Theis et al., 2016). Similarly, we show that
mutual information can lead to unintuitive interpretations, and explore more
intuitive alternatives based on parametric divergences. We conclude that
parametric divergences are a flexible framework for defining statistical
quantities relevant to a specific modeling task.","['Gabriel Huang', 'Hugo Berard', 'Ahmed Touati', 'Gauthier Gidel', 'Pascal Vincent', 'Simon Lacoste-Julien']","['cs.LG', 'stat.ML']",2017-08-08 15:01:55+00:00
http://arxiv.org/abs/1708.02497v1,Learning non-parametric Markov networks with mutual information,"We propose a method for learning Markov network structures for continuous
data without invoking any assumptions about the distribution of the variables.
The method makes use of previous work on a non-parametric estimator for mutual
information which is used to create a non-parametric test for multivariate
conditional independence. This independence test is then combined with an
efficient constraint-based algorithm for learning the graph structure. The
performance of the method is evaluated on several synthetic data sets and it is
shown to learn considerably more accurate structures than competing methods
when the dependencies between the variables involve non-linearities.","['Janne Leppä-aho', 'Santeri Räisänen', 'Xiao Yang', 'Teemu Roos']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2017-08-08 14:10:55+00:00
http://arxiv.org/abs/1708.02455v2,Fast Low-Rank Bayesian Matrix Completion with Hierarchical Gaussian Prior Models,"The problem of low rank matrix completion is considered in this paper. To
exploit the underlying low-rank structure of the data matrix, we propose a
hierarchical Gaussian prior model, where columns of the low-rank matrix are
assumed to follow a Gaussian distribution with zero mean and a common precision
matrix, and a Wishart distribution is specified as a hyperprior over the
precision matrix. We show that such a hierarchical Gaussian prior has the
potential to encourage a low-rank solution. Based on the proposed hierarchical
prior model, a variational Bayesian method is developed for matrix completion,
where the generalized approximate massage passing (GAMP) technique is embedded
into the variational Bayesian inference in order to circumvent cumbersome
matrix inverse operations. Simulation results show that our proposed method
demonstrates superiority over existing state-of-the-art matrix completion
methods.","['Linxiao Yang', 'Jun Fang', 'Huiping Duan', 'Hongbin Li', 'Bing Zeng']","['cs.LG', 'stat.ML']",2017-08-08 11:59:10+00:00
http://arxiv.org/abs/1708.02363v1,Beyond the technical challenges for deploying Machine Learning solutions in a software company,"Recently software development companies started to embrace Machine Learning
(ML) techniques for introducing a series of advanced functionality in their
products such as personalisation of the user experience, improved search,
content recommendation and automation. The technical challenges for tackling
these problems are heavily researched in literature. A less studied area is a
pragmatic approach to the role of humans in a complex modern industrial
environment where ML based systems are developed. Key stakeholders affect the
system from inception and up to operation and maintenance. Product managers
want to embed ""smart"" experiences for their users and drive the decisions on
what should be built next; software engineers are challenged to build or
utilise ML software tools that require skills that are well outside of their
comfort zone; legal and risk departments may influence design choices and data
access; operations teams are requested to maintain ML systems which are
non-stationary in their nature and change behaviour over time; and finally ML
practitioners should communicate with all these stakeholders to successfully
build a reliable system. This paper discusses some of the challenges we faced
in Atlassian as we started investing more in the ML space.",['Ilias Flaounas'],"['cs.HC', 'cs.AI', 'cs.SE', 'stat.ML']",2017-08-08 03:59:09+00:00
http://arxiv.org/abs/1708.02230v2,Delayed acceptance ABC-SMC,"Approximate Bayesian computation (ABC) is now an established technique for
statistical inference used in cases where the likelihood function is
computationally expensive or not available. It relies on the use of a~model
that is specified in the form of a~simulator, and approximates the likelihood
at a~parameter value $\theta$ by simulating auxiliary data sets $x$ and
evaluating the distance of $x$ from the true data $y$. However, ABC is not
computationally feasible in cases where using the simulator for each $\theta$
is very expensive. This paper investigates this situation in cases where
a~cheap, but approximate, simulator is available. The approach is to employ
delayed acceptance Markov chain Monte Carlo (MCMC) within an ABC sequential
Monte Carlo (SMC) sampler in order to, in a~first stage of the kernel, use the
cheap simulator to rule out parts of the parameter space that are not worth
exploring, so that the ``true'' simulator is only run (in the second stage of
the kernel) where there is a~reasonable chance of accepting proposed values of
$\theta$. We show that this approach can be used quite automatically, with few
tuning parameters. Applications to stochastic differential equation models and
latent doubly intractable distributions are presented.","['Richard G. Everitt', 'Paulina A. Rowińska']","['stat.CO', 'physics.data-an', 'stat.AP', 'stat.ME', 'stat.ML']",2017-08-07 17:52:04+00:00
