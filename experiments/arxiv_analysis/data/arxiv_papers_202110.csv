id,title,abstract,authors,categories,date
http://arxiv.org/abs/2111.13235v1,Outlier Detection for Trajectories via Flow-embeddings,"We propose a method to detect outliers in empirically observed trajectories
on a discrete or discretized manifold modeled by a simplicial complex. Our
approach is similar to spectral embeddings such as diffusion-maps and Laplacian
eigenmaps, that construct vertex embeddings from the eigenvectors of the graph
Laplacian associated with low eigenvalues. Here we consider trajectories as
edge-flow vectors defined on a simplicial complex, a higher-order
generalization of graphs, and use the Hodge 1-Laplacian of the simplicial
complex to derive embeddings of these edge-flows. By projecting trajectory
vectors onto the eigenspace of the Hodge 1-Laplacian associated to small
eigenvalues, we can characterize the behavior of the trajectories relative to
the homology of the complex, which corresponds to holes in the underlying
space. This enables us to classify trajectories based on simply interpretable,
low-dimensional statistics. We show how this technique can single out
trajectories that behave (topologically) different compared to typical
trajectories, and illustrate the performance of our approach with both
synthetic and empirical data.","['Florian Frantzen', 'Jean-Baptiste Seby', 'Michael T. Schaub']","['cs.SI', 'math.AT', 'physics.soc-ph', 'stat.AP', 'stat.ML']",2021-11-25 19:58:48+00:00
http://arxiv.org/abs/2111.13229v2,Generalizing Clinical Trials with Convex Hulls,"Randomized clinical trials eliminate confounding but impose strict exclusion
criteria that limit recruitment to a subset of the population. Observational
datasets are more inclusive but suffer from confounding -- often providing
overly optimistic estimates of treatment response over time due to partially
optimized physician prescribing patterns. We therefore assume that the
unconfounded treatment response lies somewhere in-between the observational
estimate before and the observational estimate after treatment assignment. This
assumption allows us to extrapolate results from exclusive trials to the
broader population by analyzing observational and trial data simultaneously
using an algorithm called Optimum in Convex Hulls (OCH). OCH represents the
treatment effect either in terms of convex hulls of conditional expectations or
convex hulls (also known as mixtures) of conditional densities. The algorithm
first learns the component expectations or densities using the observational
data and then learns the linear mixing coefficients using trial data in order
to approximate the true treatment effect; theory importantly explains why this
linear combination should hold. OCH estimates the treatment effect in terms
both expectations and densities with state of the art accuracy.","['Eric V. Strobl', 'Thomas A. Lasko']","['stat.ML', 'cs.LG', 'stat.ME']",2021-11-25 19:27:03+00:00
http://arxiv.org/abs/2111.13226v4,A Kernel Test for Causal Association via Noise Contrastive Backdoor Adjustment,"Causal inference grows increasingly complex as the number of confounders
increases. Given treatments $X$, confounders $Z$ and outcomes $Y$, we develop a
non-parametric method to test the \textit{do-null} hypothesis $H_0:\;
p(y|\text{\it do}(X=x))=p(y)$ against the general alternative. Building on the
Hilbert Schmidt Independence Criterion (HSIC) for marginal independence
testing, we propose backdoor-HSIC (bd-HSIC) and demonstrate that it is
calibrated and has power for both binary and continuous treatments under a
large number of confounders. Additionally, we establish convergence properties
of the estimators of covariance operators used in bd-HSIC. We investigate the
advantages and disadvantages of bd-HSIC against parametric tests as well as the
importance of using the do-null testing in contrast to marginal independence
testing or conditional independence testing. A complete implementation can be
found at
\hyperlink{https://github.com/MrHuff/kgformula}{\texttt{https://github.com/MrHuff/kgformula}}.","['Robert Hu', 'Dino Sejdinovic', 'Robin J. Evans']","['stat.ME', 'stat.ML']",2021-11-25 19:12:37+00:00
http://arxiv.org/abs/2111.13219v5,Differentially private stochastic expectation propagation (DP-SEP),"We are interested in privatizing an approximate posterior inference algorithm
called Expectation Propagation (EP). EP approximates the posterior by
iteratively refining approximations to the local likelihoods, and is known to
provide better posterior uncertainties than those by variational inference
(VI). However, EP needs a large memory to maintain all local approximates
associated with each datapoint in the training data. To overcome this
challenge, stochastic expectation propagation (SEP) considers a single unique
local factor that captures the average effect of each likelihood term to the
posterior and refines it in a way analogous to EP. In terms of privacy, SEP is
more tractable than EP because at each refining step of a factor, the remaining
factors are fixed and do not depend on other datapoints as in EP, which makes
the sensitivity analysis straightforward. We provide a theoretical analysis of
the privacy-accuracy trade-off in the posterior estimates under our method,
called differentially private stochastic expectation propagation (DP-SEP).
Furthermore, we demonstrate the performance of our DP-SEP algorithm evaluated
on both synthetic and real-world datasets in terms of the quality of posterior
estimates at different levels of guaranteed privacy.","['Margarita Vinaroz', 'Mijung Park']","['cs.LG', 'stat.ML']",2021-11-25 18:59:35+00:00
http://arxiv.org/abs/2111.13185v1,Learning Conditional Invariance through Cycle Consistency,"Identifying meaningful and independent factors of variation in a dataset is a
challenging learning task frequently addressed by means of deep latent variable
models. This task can be viewed as learning symmetry transformations preserving
the value of a chosen property along latent dimensions. However, existing
approaches exhibit severe drawbacks in enforcing the invariance property in the
latent space. We address these shortcomings with a novel approach to cycle
consistency. Our method involves two separate latent subspaces for the target
property and the remaining input information, respectively. In order to enforce
invariance as well as sparsity in the latent space, we incorporate semantic
knowledge by using cycle consistency constraints relying on property side
information. The proposed method is based on the deep information bottleneck
and, in contrast to other approaches, allows using continuous target properties
and provides inherent model selection capabilities. We demonstrate on synthetic
and molecular data that our approach identifies more meaningful factors which
lead to sparser and more interpretable models with improved invariance
properties.","['Maxim Samarin', 'Vitali Nesterov', 'Mario Wieser', 'Aleksander Wieczorek', 'Sonali Parbhoo', 'Volker Roth']","['cs.LG', 'stat.ML']",2021-11-25 17:33:12+00:00
http://arxiv.org/abs/2111.13180v4,Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data,"Statistical models are central to machine learning with broad applicability
across a range of downstream tasks. The models are controlled by free
parameters that are typically estimated from data by maximum-likelihood
estimation or approximations thereof. However, when faced with real-world data
sets many of the models run into a critical issue: they are formulated in terms
of fully-observed data, whereas in practice the data sets are plagued with
missing data. The theory of statistical model estimation from incomplete data
is conceptually similar to the estimation of latent-variable models, where
powerful tools such as variational inference (VI) exist. However, in contrast
to standard latent-variable models, parameter estimation with incomplete data
often requires estimating exponentially-many conditional distributions of the
missing variables, hence making standard VI methods intractable. We address
this gap by introducing variational Gibbs inference (VGI), a new
general-purpose method to estimate the parameters of statistical models from
incomplete data. We validate VGI on a set of synthetic and real-world
estimation tasks, estimating important machine learning models such as
variational autoencoders and normalising flows from incomplete data. The
proposed method, whilst general-purpose, achieves competitive or better
performance than existing model-specific estimation methods.","['Vaidotas Simkus', 'Benjamin Rhodes', 'Michael U. Gutmann']","['cs.LG', 'stat.ME', 'stat.ML', '62D10', 'I.2.6; G.3']",2021-11-25 17:22:22+00:00
http://arxiv.org/abs/2111.13171v1,"Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks","Disobeying the classical wisdom of statistical learning theory, modern deep
neural networks generalize well even though they typically contain millions of
parameters. Recently, it has been shown that the trajectories of iterative
optimization algorithms can possess fractal structures, and their
generalization error can be formally linked to the complexity of such fractals.
This complexity is measured by the fractal's intrinsic dimension, a quantity
usually much smaller than the number of parameters in the network. Even though
this perspective provides an explanation for why overparametrized networks
would not overfit, computing the intrinsic dimension (e.g., for monitoring
generalization during training) is a notoriously difficult task, where existing
methods typically fail even in moderate ambient dimensions. In this study, we
consider this problem from the lens of topological data analysis (TDA) and
develop a generic computational tool that is built on rigorous mathematical
foundations. By making a novel connection between learning theory and TDA, we
first illustrate that the generalization error can be equivalently bounded in
terms of a notion called the 'persistent homology dimension' (PHD), where,
compared with prior work, our approach does not require any additional
geometrical or statistical assumptions on the training dynamics. Then, by
utilizing recently established theoretical results and TDA tools, we develop an
efficient algorithm to estimate PHD in the scale of modern deep neural networks
and further provide visualization tools to help understand generalization in
deep learning. Our experiments show that the proposed approach can efficiently
compute a network's intrinsic dimension in a variety of settings, which is
predictive of the generalization error.","['Tolga Birdal', 'Aaron Lou', 'Leonidas Guibas', 'Umut Şimşekli']","['cs.LG', 'cs.AI', 'cs.CV', 'math.GN', 'stat.ML']",2021-11-25 17:06:15+00:00
http://arxiv.org/abs/2111.13164v6,Neural network stochastic differential equation models with applications to financial data forecasting,"In this article, we employ a collection of stochastic differential equations
with drift and diffusion coefficients approximated by neural networks to
predict the trend of chaotic time series which has big jump properties. Our
contributions are, first, we propose a model called L\'evy induced stochastic
differential equation network, which explores compounded stochastic
differential equations with $\alpha$-stable L\'evy motion to model complex time
series data and solve the problem through neural network approximation. Second,
we theoretically prove that the numerical solution through our algorithm
converges in probability to the solution of corresponding stochastic
differential equation, without curse of dimensionality. Finally, we illustrate
our method by applying it to real financial time series data and find the
accuracy increases through the use of non-Gaussian L\'evy processes. We also
present detailed comparisons in terms of data patterns, various models,
different shapes of L\'evy motion and the prediction lengths.","['Luxuan Yang', 'Ting Gao', 'Yubin Lu', 'Jinqiao Duan', 'Tao Liu']","['cs.LG', 'q-fin.MF', 'stat.ML']",2021-11-25 16:49:01+00:00
http://arxiv.org/abs/2111.13162v1,Randomized Stochastic Gradient Descent Ascent,"An increasing number of machine learning problems, such as robust or
adversarial variants of existing algorithms, require minimizing a loss function
that is itself defined as a maximum. Carrying a loop of stochastic gradient
ascent (SGA) steps on the (inner) maximization problem, followed by an SGD step
on the (outer) minimization, is known as Epoch Stochastic Gradient
\textit{Descent Ascent} (ESGDA). While successful in practice, the theoretical
analysis of ESGDA remains challenging, with no clear guidance on choices for
the inner loop size nor on the interplay between inner/outer step sizes. We
propose RSGDA (Randomized SGDA), a variant of ESGDA with stochastic loop size
with a simpler theoretical analysis. RSGDA comes with the first (among SGDA
algorithms) almost sure convergence rates when used on nonconvex
min/strongly-concave max settings. RSGDA can be parameterized using optimal
loop sizes that guarantee the best convergence rates known to hold for SGDA. We
test RSGDA on toy and larger scale problems, using distributionally robust
optimization and single-cell data matching using optimal transport as a
testbed.","['Othmane Sebbouh', 'Marco Cuturi', 'Gabriel Peyré']","['cs.LG', 'math.OC', 'stat.ML']",2021-11-25 16:44:19+00:00
http://arxiv.org/abs/2111.13139v2,Group equivariant neural posterior estimation,"Simulation-based inference with conditional neural density estimators is a
powerful approach to solving inverse problems in science. However, these
methods typically treat the underlying forward model as a black box, with no
way to exploit geometric properties such as equivariances. Equivariances are
common in scientific models, however integrating them directly into expressive
inference networks (such as normalizing flows) is not straightforward. We here
describe an alternative method to incorporate equivariances under joint
transformations of parameters and data. Our method -- called group equivariant
neural posterior estimation (GNPE) -- is based on self-consistently
standardizing the ""pose"" of the data while estimating the posterior over
parameters. It is architecture-independent, and applies both to exact and
approximate equivariances. As a real-world application, we use GNPE for
amortized inference of astrophysical binary black hole systems from
gravitational-wave observations. We show that GNPE achieves state-of-the-art
accuracy while reducing inference times by three orders of magnitude.","['Maximilian Dax', 'Stephen R. Green', 'Jonathan Gair', 'Michael Deistler', 'Bernhard Schölkopf', 'Jakob H. Macke']","['cs.LG', 'astro-ph.IM', 'gr-qc', 'stat.ML']",2021-11-25 15:50:01+00:00
http://arxiv.org/abs/2111.13089v1,GeomNet: A Neural Network Based on Riemannian Geometries of SPD Matrix Space and Cholesky Space for 3D Skeleton-Based Interaction Recognition,"In this paper, we propose a novel method for representation and
classification of two-person interactions from 3D skeleton sequences. The key
idea of our approach is to use Gaussian distributions to capture statistics on
R n and those on the space of symmetric positive definite (SPD) matrices. The
main challenge is how to parametrize those distributions. Towards this end, we
develop methods for embedding Gaussian distributions in matrix groups based on
the theory of Lie groups and Riemannian symmetric spaces. Our method relies on
the Riemannian geometry of the underlying manifolds and has the advantage of
encoding high-order statistics from 3D joint positions. We show that the
proposed method achieves competitive results in two-person interaction
recognition on three benchmarks for 3D human activity understanding.",['Xuan Son Nguyen'],"['cs.CV', 'cs.LG', 'stat.ML']",2021-11-25 13:57:43+00:00
http://arxiv.org/abs/2111.13037v2,"Learning dynamical systems from data: A simple cross-validation perspective, part III: Irregularly-Sampled Time Series","A simple and interpretable way to learn a dynamical system from data is to
interpolate its vector-field with a kernel. In particular, this strategy is
highly efficient (both in terms of accuracy and complexity) when the kernel is
data-adapted using Kernel Flows (KF)\cite{Owhadi19} (which uses gradient-based
optimization to learn a kernel based on the premise that a kernel is good if
there is no significant loss in accuracy if half of the data is used for
interpolation). Despite its previous successes, this strategy (based on
interpolating the vector field driving the dynamical system) breaks down when
the observed time series is not regularly sampled in time. In this work, we
propose to address this problem by directly approximating the vector field of
the dynamical system by incorporating time differences between observations in
the (KF) data-adapted kernels. We compare our approach with the classical one
over different benchmark dynamical systems and show that it significantly
improves the forecasting accuracy while remaining simple, fast, and robust.","['Jonghyeon Lee', 'Edward De Brouwer', 'Boumediene Hamzi', 'Houman Owhadi']","['stat.ML', 'cs.LG', 'math.DS', 'stat.CO']",2021-11-25 11:45:40+00:00
http://arxiv.org/abs/2111.13026v1,Bandit problems with fidelity rewards,"The fidelity bandits problem is a variant of the $K$-armed bandit problem in
which the reward of each arm is augmented by a fidelity reward that provides
the player with an additional payoff depending on how 'loyal' the player has
been to that arm in the past. We propose two models for fidelity. In the
loyalty-points model the amount of extra reward depends on the number of times
the arm has previously been played. In the subscription model the additional
reward depends on the current number of consecutive draws of the arm. We
consider both stochastic and adversarial problems. Since single-arm strategies
are not always optimal in stochastic problems, the notion of regret in the
adversarial setting needs careful adjustment. We introduce three possible
notions of regret and investigate which can be bounded sublinearly. We study in
detail the special cases of increasing, decreasing and coupon (where the player
gets an additional reward after every $m$ plays of an arm) fidelity rewards.
For the models which do not necessarily enjoy sublinear regret, we provide a
worst case lower bound. For those models which exhibit sublinear regret, we
provide algorithms and bound their regret.","['Gábor Lugosi', 'Ciara Pike-Burke', 'Pierre-André Savalle']","['stat.ML', 'cs.LG']",2021-11-25 11:09:43+00:00
http://arxiv.org/abs/2111.12981v2,Efficient Mean Estimation with Pure Differential Privacy via a Sum-of-Squares Exponential Mechanism,"We give the first polynomial-time algorithm to estimate the mean of a
$d$-variate probability distribution with bounded covariance from
$\tilde{O}(d)$ independent samples subject to pure differential privacy. Prior
algorithms for this problem either incur exponential running time, require
$\Omega(d^{1.5})$ samples, or satisfy only the weaker concentrated or
approximate differential privacy conditions. In particular, all prior
polynomial-time algorithms require $d^{1+\Omega(1)}$ samples to guarantee small
privacy loss with ""cryptographically"" high probability, $1-2^{-d^{\Omega(1)}}$,
while our algorithm retains $\tilde{O}(d)$ sample complexity even in this
stringent setting.
  Our main technique is a new approach to use the powerful Sum of Squares
method (SoS) to design differentially private algorithms. SoS proofs to
algorithms is a key theme in numerous recent works in high-dimensional
algorithmic statistics -- estimators which apparently require exponential
running time but whose analysis can be captured by low-degree Sum of Squares
proofs can be automatically turned into polynomial-time algorithms with the
same provable guarantees. We demonstrate a similar proofs to private algorithms
phenomenon: instances of the workhorse exponential mechanism which apparently
require exponential time but which can be analyzed with low-degree SoS proofs
can be automatically turned into polynomial-time differentially private
algorithms. We prove a meta-theorem capturing this phenomenon, which we expect
to be of broad use in private algorithm design.
  Our techniques also draw new connections between differentially private and
robust statistics in high dimensions. In particular, viewed through our
proofs-to-private-algorithms lens, several well-studied SoS proofs from recent
works in algorithmic robust statistics directly yield key components of our
differentially private mean estimation algorithm.","['Samuel B. Hopkins', 'Gautam Kamath', 'Mahbod Majid']","['cs.DS', 'cs.CR', 'cs.IT', 'math.IT', 'stat.ML']",2021-11-25 09:31:15+00:00
http://arxiv.org/abs/2111.12963v1,Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU Neural Networks,"Among the several paradigms of artificial intelligence (AI) or machine
learning (ML), a remarkably successful paradigm is deep learning. Deep
learning's phenomenal success has been hoped to be interpreted via fundamental
research on the theory of deep learning. Accordingly, applied research on deep
learning has spurred the theory of deep learning-oriented depth and breadth of
developments. Inspired by such developments, we pose these fundamental
questions: can we accurately approximate an arbitrary matrix-vector product
using deep rectified linear unit (ReLU) feedforward neural networks (FNNs)? If
so, can we bound the resulting approximation error? In light of these
questions, we derive error bounds in Lebesgue and Sobolev norms that comprise
our developed deep approximation theory. Guided by this theory, we have
successfully trained deep ReLU FNNs whose test results justify our developed
theory. The developed theory is also applicable for guiding and easing the
training of teacher deep ReLU FNNs in view of the emerging teacher-student AI
or ML paradigms that are essential for solving several AI or ML problems in
wireless communications and signal processing; network science and graph signal
processing; and network neuroscience and brain physics.",['Tilahun M. Getu'],"['cs.LG', 'stat.ML']",2021-11-25 08:14:55+00:00
http://arxiv.org/abs/2111.12945v2,Low-rank variational Bayes correction to the Laplace method,"Approximate inference methods like the Laplace method, Laplace approximations
and variational methods, amongst others, are popular methods when exact
inference is not feasible due to the complexity of the model or the abundance
of data. In this paper we propose a hybrid approximate method called Low-Rank
Variational Bayes correction (VBC), that uses the Laplace method and
subsequently a Variational Bayes correction in a lower dimension, to the joint
posterior mean. The cost is essentially that of the Laplace method which
ensures scalability of the method, in both model complexity and data size.
Models with fixed and unknown hyperparameters are considered, for simulated and
real examples, for small and large datasets.","['Janet van Niekerk', 'Haavard Rue']","['stat.ME', 'cs.LG', 'stat.ML']",2021-11-25 07:01:06+00:00
http://arxiv.org/abs/2111.12876v1,Time-independent Generalization Bounds for SGLD in Non-convex Settings,"We establish generalization error bounds for stochastic gradient Langevin
dynamics (SGLD) with constant learning rate under the assumptions of
dissipativity and smoothness, a setting that has received increased attention
in the sampling/optimization literature. Unlike existing bounds for SGLD in
non-convex settings, ours are time-independent and decay to zero as the sample
size increases. Using the framework of uniform stability, we establish
time-independent bounds by exploiting the Wasserstein contraction property of
the Langevin diffusion, which also allows us to circumvent the need to bound
gradients using Lipschitz-like assumptions. Our analysis also supports variants
of SGLD that use different discretization methods, incorporate Euclidean
projections, or use non-isotropic noise.","['Tyler Farghly', 'Patrick Rebeschini']","['stat.ML', 'cs.LG', 'math.OC', 'math.PR']",2021-11-25 02:31:52+00:00
http://arxiv.org/abs/2111.12823v2,Fairness for AUC via Feature Augmentation,"We study fairness in the context of classification where the performance is
measured by the area under the curve (AUC) of the receiver operating
characteristic. AUC is commonly used to measure the performance of prediction
models. The same classifier can have significantly varying AUCs for different
protected groups and, in real-world applications, it is often desirable to
reduce such cross-group differences. We address the problem of how to acquire
additional features to most greatly improve AUC for the disadvantaged group. We
develop a novel approach, fairAUC, based on feature augmentation (adding
features) to mitigate bias between identifiable groups. The approach requires
only a few summary statistics to offer provable guarantees on AUC improvement,
and allows managers flexibility in determining where in the fairness-accuracy
tradeoff they would like to be. We evaluate fairAUC on synthetic and real-world
datasets and find that it significantly improves AUC for the disadvantaged
group relative to benchmarks maximizing overall AUC and minimizing bias between
groups.","['Hortense Fong', 'Vineet Kumar', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['cs.LG', 'cs.AI', 'cs.CY', 'stat.ML']",2021-11-24 22:32:19+00:00
http://arxiv.org/abs/2111.12786v1,Differentially Private Nonparametric Regression Under a Growth Condition,"Given a real-valued hypothesis class $\mathcal{H}$, we investigate under what
conditions there is a differentially private algorithm which learns an optimal
hypothesis from $\mathcal{H}$ given i.i.d. data. Inspired by recent results for
the related setting of binary classification (Alon et al., 2019; Bun et al.,
2020), where it was shown that online learnability of a binary class is
necessary and sufficient for its private learnability, Jung et al. (2020)
showed that in the setting of regression, online learnability of $\mathcal{H}$
is necessary for private learnability. Here online learnability of
$\mathcal{H}$ is characterized by the finiteness of its $\eta$-sequential fat
shattering dimension, ${\rm sfat}_\eta(\mathcal{H})$, for all $\eta > 0$. In
terms of sufficient conditions for private learnability, Jung et al. (2020)
showed that $\mathcal{H}$ is privately learnable if $\lim_{\eta \downarrow 0}
{\rm sfat}_\eta(\mathcal{H})$ is finite, which is a fairly restrictive
condition. We show that under the relaxed condition $\lim \inf_{\eta \downarrow
0} \eta \cdot {\rm sfat}_\eta(\mathcal{H}) = 0$, $\mathcal{H}$ is privately
learnable, establishing the first nonparametric private learnability guarantee
for classes $\mathcal{H}$ with ${\rm sfat}_\eta(\mathcal{H})$ diverging as
$\eta \downarrow 0$. Our techniques involve a novel filtering procedure to
output stable hypotheses for nonparametric function classes.",['Noah Golowich'],"['cs.LG', 'cs.CR', 'stat.ML']",2021-11-24 20:36:01+00:00
http://arxiv.org/abs/2111.12664v2,MIO : Mutual Information Optimization using Self-Supervised Binary Contrastive Learning,"Self-supervised contrastive learning frameworks have progressed rapidly over
the last few years. In this paper, we propose a novel mutual information
optimization-based loss function for contrastive learning. We model our
pre-training task as a binary classification problem to induce an implicit
contrastive effect and predict whether a pair is positive or negative. We
further improve the n\""aive loss function using the Majorize-Minimizer
principle and such improvement helps us to track the problem mathematically.
Unlike the existing methods, the proposed loss function optimizes the mutual
information in both positive and negative pairs. We also present a closed-form
expression for the parameter gradient flow and compare the behavior of the
proposed loss function using its Hessian eigen-spectrum to analytically study
the convergence of SSL frameworks. The proposed method outperforms the SOTA
contrastive self-supervised frameworks on benchmark datasets like CIFAR-10,
CIFAR-100, STL-10, and Tiny-ImageNet. After 200 epochs of pre-training with
ResNet-18 as the backbone, the proposed model achieves an accuracy of 86.2\%,
58.18\%, 77.49\%, and 30.87\% on CIFAR-10, CIFAR-100, STL-10, and Tiny-ImageNet
datasets, respectively, and surpasses the SOTA contrastive baseline by 1.23\%,
3.57\%, 2.00\%, and 0.33\%, respectively.","['Siladittya Manna', 'Umapada Pal', 'Saumik Bhattacharya']","['cs.CV', 'stat.ML']",2021-11-24 17:51:29+00:00
http://arxiv.org/abs/2111.12604v1,State-space deep Gaussian processes with applications,"This thesis is mainly concerned with state-space approaches for solving deep
(temporal) Gaussian process (DGP) regression problems. More specifically, we
represent DGPs as hierarchically composed systems of stochastic differential
equations (SDEs), and we consequently solve the DGP regression problem by using
state-space filtering and smoothing methods. The resulting state-space DGP
(SS-DGP) models generate a rich class of priors compatible with modelling a
number of irregular signals/functions. Moreover, due to their Markovian
structure, SS-DGPs regression problems can be solved efficiently by using
Bayesian filtering and smoothing methods. The second contribution of this
thesis is that we solve continuous-discrete Gaussian filtering and smoothing
problems by using the Taylor moment expansion (TME) method. This induces a
class of filters and smoothers that can be asymptotically exact in predicting
the mean and covariance of stochastic differential equations (SDEs) solutions.
Moreover, the TME method and TME filters and smoothers are compatible with
simulating SS-DGPs and solving their regression problems. Lastly, this thesis
features a number of applications of state-space (deep) GPs. These applications
mainly include, (i) estimation of unknown drift functions of SDEs from
partially observed trajectories and (ii) estimation of spectro-temporal
features of signals.",['Zheng Zhao'],"['stat.ME', 'eess.SP', 'stat.ML']",2021-11-24 16:25:43+00:00
http://arxiv.org/abs/2111.12594v2,Conditional Object-Centric Learning from Video,"Object-centric representations are a promising path toward more systematic
generalization by providing flexible abstractions upon which compositional
world models can be built. Recent work on simple 2D and 3D datasets has shown
that models with object-centric inductive biases can learn to segment and
represent meaningful objects from the statistical structure of the data alone
without the need for any supervision. However, such fully-unsupervised methods
still fail to scale to diverse realistic data, despite the use of increasingly
complex inductive biases such as priors for the size of objects or the 3D
geometry of the scene. In this paper, we instead take a weakly-supervised
approach and focus on how 1) using the temporal dynamics of video data in the
form of optical flow and 2) conditioning the model on simple object location
cues can be used to enable segmenting and tracking objects in significantly
more realistic synthetic data. We introduce a sequential extension to Slot
Attention which we train to predict optical flow for realistic looking
synthetic scenes and show that conditioning the initial state of this model on
a small set of hints, such as center of mass of objects in the first frame, is
sufficient to significantly improve instance segmentation. These benefits
generalize beyond the training distribution to novel objects, novel
backgrounds, and to longer video sequences. We also find that such
initial-state-conditioning can be used during inference as a flexible interface
to query the model for specific objects or parts of objects, which could pave
the way for a range of weakly-supervised approaches and allow more effective
interaction with trained models.","['Thomas Kipf', 'Gamaleldin F. Elsayed', 'Aravindh Mahendran', 'Austin Stone', 'Sara Sabour', 'Georg Heigold', 'Rico Jonschkowski', 'Alexey Dosovitskiy', 'Klaus Greff']","['cs.CV', 'cs.LG', 'stat.ML']",2021-11-24 16:10:46+00:00
http://arxiv.org/abs/2111.12577v2,A Method for Evaluating Deep Generative Models of Images via Assessing the Reproduction of High-order Spatial Context,"Deep generative models (DGMs) have the potential to revolutionize diagnostic
imaging. Generative adversarial networks (GANs) are one kind of DGM which are
widely employed. The overarching problem with deploying GANs, and other DGMs,
in any application that requires domain expertise in order to actually use the
generated images is that there generally is not adequate or automatic means of
assessing the domain-relevant quality of generated images. In this work, we
demonstrate several objective tests of images output by two popular GAN
architectures. We designed several stochastic context models (SCMs) of distinct
image features that can be recovered after generation by a trained GAN. Several
of these features are high-order, algorithmic pixel-arrangement rules which are
not readily expressed in covariance matrices. We designed and validated
statistical classifiers to detect specific effects of the known arrangement
rules. We then tested the rates at which two different GANs correctly
reproduced the feature context under a variety of training scenarios, and
degrees of feature-class similarity. We found that ensembles of generated
images can appear largely accurate visually, and show high accuracy in ensemble
measures, while not exhibiting the known spatial arrangements. Furthermore,
GANs trained on a spectrum of distinct spatial orders did not respect the given
prevalence of those orders in the training data. The main conclusion is that
SCMs can be engineered to quantify numerous errors, per image, that may not be
captured in ensemble statistics but plausibly can affect subsequent use of the
GAN-generated images.","['Rucha Deshpande', 'Mark A. Anastasio', 'Frank J. Brooks']","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",2021-11-24 15:58:10+00:00
http://arxiv.org/abs/2111.12526v1,Mining Meta-indicators of University Ranking: A Machine Learning Approach Based on SHAP,"University evaluation and ranking is an extremely complex activity. Major
universities are struggling because of increasingly complex indicator systems
of world university rankings. So can we find the meta-indicators of the index
system by simplifying the complexity? This research discovered three
meta-indicators based on interpretable machine learning. The first one is time,
to be friends with time, and believe in the power of time, and accumulate
historical deposits; the second one is space, to be friends with city, and grow
together by co-develop; the third one is relationships, to be friends with
alumni, and strive for more alumni donations without ceiling.","['Shudong Yang', 'Miaomiao Liu']","['stat.AP', 'cs.LG', 'stat.ML', 'J.4']",2021-11-24 14:49:19+00:00
http://arxiv.org/abs/2111.12482v1,One More Step Towards Reality: Cooperative Bandits with Imperfect Communication,"The cooperative bandit problem is increasingly becoming relevant due to its
applications in large-scale decision-making. However, most research for this
problem focuses exclusively on the setting with perfect communication, whereas
in most real-world distributed settings, communication is often over stochastic
networks, with arbitrary corruptions and delays. In this paper, we study
cooperative bandit learning under three typical real-world communication
scenarios, namely, (a) message-passing over stochastic time-varying networks,
(b) instantaneous reward-sharing over a network with random delays, and (c)
message-passing with adversarially corrupted rewards, including byzantine
communication. For each of these environments, we propose decentralized
algorithms that achieve competitive performance, along with near-optimal
guarantees on the incurred group regret as well. Furthermore, in the setting
with perfect communication, we present an improved delayed-update algorithm
that outperforms the existing state-of-the-art on various network topologies.
Finally, we present tight network-dependent minimax lower bounds on the group
regret. Our proposed algorithms are straightforward to implement and obtain
competitive empirical performance.","['Udari Madhushani', 'Abhimanyu Dubey', 'Naomi Ehrich Leonard', 'Alex Pentland']","['stat.ML', 'cs.LG']",2021-11-24 13:19:11+00:00
http://arxiv.org/abs/2111.12460v3,ViCE: Improving Dense Representation Learning by Superpixelization and Contrasting Cluster Assignment,"Recent self-supervised models have demonstrated equal or better performance
than supervised methods, opening for AI systems to learn visual representations
from practically unlimited data. However, these methods are typically
classification-based and thus ineffective for learning high-resolution feature
maps that preserve precise spatial information. This work introduces
superpixels to improve self-supervised learning of dense semantically rich
visual concept embeddings. Decomposing images into a small set of visually
coherent regions reduces the computational complexity by $\mathcal{O}(1000)$
while preserving detail. We experimentally show that contrasting over regions
improves the effectiveness of contrastive learning methods, extends their
applicability to high-resolution images, improves overclustering performance,
superpixels are better than grids, and regional masking improves performance.
The expressiveness of our dense embeddings is demonstrated by improving the
SOTA unsupervised semantic segmentation benchmark on Cityscapes, and for
convolutional models on COCO.","['Robin Karlsson', 'Tomoki Hayashi', 'Keisuke Fujii', 'Alexander Carballo', 'Kento Ohtani', 'Kazuya Takeda']","['cs.CV', 'cs.LG', 'stat.ML', 'I.2.10; I.2.9']",2021-11-24 12:27:30+00:00
http://arxiv.org/abs/2111.12429v2,tsflex: flexible time series processing & feature extraction,"Time series processing and feature extraction are crucial and time-intensive
steps in conventional machine learning pipelines. Existing packages are limited
in their applicability, as they cannot cope with irregularly-sampled or
asynchronous data and make strong assumptions about the data format. Moreover,
these packages do not focus on execution speed and memory efficiency, resulting
in considerable overhead. We present $\texttt{tsflex}$, a Python toolkit for
time series processing and feature extraction, that focuses on performance and
flexibility, enabling broad applicability. This toolkit leverages window-stride
arguments of the same data type as the sequence-index, and maintains the
sequence-index through all operations. $\texttt{tsflex}$ is flexible as it
supports (1) multivariate time series, (2) multiple window-stride
configurations, and (3) integrates with processing and feature functions from
other packages, while (4) making no assumptions about the data sampling
regularity, series alignment, and data type. Other functionalities include
multiprocessing, detailed execution logging, chunking sequences, and
serialization. Benchmarks show that $\texttt{tsflex}$ is faster and more
memory-efficient compared to similar packages, while being more permissive and
flexible in its utilization.","['Jonas Van Der Donckt', 'Jeroen Van Der Donckt', 'Emiel Deprost', 'Sofie Van Hoecke']","['cs.LG', 'eess.SP', 'stat.ML']",2021-11-24 11:18:03+00:00
http://arxiv.org/abs/2111.12399v2,Dictionary-based Low-Rank Approximations and the Mixed Sparse Coding problem,"Constrained tensor and matrix factorization models allow to extract
interpretable patterns from multiway data. Therefore identifiability properties
and efficient algorithms for constrained low-rank approximations are nowadays
important research topics. This work deals with columns of factor matrices of a
low-rank approximation being sparse in a known and possibly overcomplete basis,
a model coined as Dictionary-based Low-Rank Approximation (DLRA). While earlier
contributions focused on finding factor columns inside a dictionary of
candidate columns, i.e. one-sparse approximations, this work is the first to
tackle DLRA with sparsity larger than one. I propose to focus on the
sparse-coding subproblem coined Mixed Sparse-Coding (MSC) that emerges when
solving DLRA with an alternating optimization strategy. Several algorithms
based on sparse-coding heuristics (greedy methods, convex relaxations) are
provided to solve MSC. The performance of these heuristics is evaluated on
simulated data. Then, I show how to adapt an efficient MSC solver based on the
LASSO to compute Dictionary-based Matrix Factorization and Canonical Polyadic
Decomposition in the context of hyperspectral image processing and
chemometrics. These experiments suggest that DLRA extends the modeling
capabilities of low-rank approximations, helps reducing estimation variance and
enhances the identifiability and interpretability of estimated factors.",['Jeremy E. Cohen'],"['cs.LG', 'stat.ML']",2021-11-24 10:32:48+00:00
http://arxiv.org/abs/2111.12295v3,Animal behavior classification via deep learning on embedded systems,"We develop an end-to-end deep-neural-network-based algorithm for classifying
animal behavior using accelerometry data on the embedded system of an
artificial intelligence of things (AIoT) device installed in a wearable collar
tag. The proposed algorithm jointly performs feature extraction and
classification utilizing a set of infinite-impulse-response (IIR) and
finite-impulse-response (FIR) filters together with a multilayer perceptron.
The utilized IIR and FIR filters can be viewed as specific types of recurrent
and convolutional neural network layers, respectively. We evaluate the
performance of the proposed algorithm via two real-world datasets collected
from total eighteen grazing beef cattle using collar tags. The results show
that the proposed algorithm offers good intra- and inter-dataset classification
accuracy and outperforms its closest contenders including two state-of-the-art
convolutional-neural-network-based time-series classification algorithms, which
are significantly more complex. We implement the proposed algorithm on the
embedded system of the utilized collar tags' AIoT device to perform in-situ
classification of animal behavior. We achieve real-time in-situ behavior
inference from accelerometry data without imposing any strain on the available
computational, memory, or energy resources of the embedded system.","['Reza Arablouei', 'Liang Wang', 'Lachlan Currie', 'Jordan Yates', 'Flavio A. P. Alvarenga', 'Greg J. Bishop-Hurley']","['cs.LG', 'cs.AI', 'eess.SP', 'stat.ML']",2021-11-24 06:26:15+00:00
http://arxiv.org/abs/2111.12292v2,Improved Fine-Tuning by Better Leveraging Pre-Training Data,"As a dominant paradigm, fine-tuning a pre-trained model on the target data is
widely used in many deep learning applications, especially for small data sets.
However, recent studies have empirically shown that training from scratch has
the final performance that is no worse than this pre-training strategy once the
number of training samples is increased in some vision tasks. In this work, we
revisit this phenomenon from the perspective of generalization analysis by
using excess risk bound which is popular in learning theory. The result reveals
that the excess risk bound may have a weak dependency on the pre-trained model.
The observation inspires us to leverage pre-training data for fine-tuning,
since this data is also available for fine-tuning. The generalization result of
using pre-training data shows that the excess risk bound on a target task can
be improved when the appropriate pre-training data is included in fine-tuning.
With the theoretical motivation, we propose a novel selection strategy to
select a subset from pre-training data to help improve the generalization on
the target task. Extensive experimental results for image classification tasks
on 8 benchmark data sets verify the effectiveness of the proposed data
selection based fine-tuning pipeline.","['Ziquan Liu', 'Yi Xu', 'Yuanhong Xu', 'Qi Qian', 'Hao Li', 'Xiangyang Ji', 'Antoni Chan', 'Rong Jin']","['cs.CV', 'cs.LG', 'stat.ML']",2021-11-24 06:18:32+00:00
http://arxiv.org/abs/2111.14630v1,On computable learning of continuous features,"We introduce definitions of computable PAC learning for binary classification
over computable metric spaces. We provide sufficient conditions for learners
that are empirical risk minimizers (ERM) to be computable, and bound the strong
Weihrauch degree of an ERM learner under more general conditions. We also give
a presentation of a hypothesis class that does not admit any proper computable
PAC learner with computable sample function, despite the underlying class being
PAC learnable.","['Nathanael Ackerman', 'Julian Asilis', 'Jieqi Di', 'Cameron Freer', 'Jean-Baptiste Tristan']","['cs.LG', 'cs.LO', 'math.LO', 'stat.ML']",2021-11-24 02:28:21+00:00
http://arxiv.org/abs/2111.12193v2,Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation,"Most set prediction models in deep learning use set-equivariant operations,
but they actually operate on multisets. We show that set-equivariant functions
cannot represent certain functions on multisets, so we introduce the more
appropriate notion of multiset-equivariance. We identify that the existing Deep
Set Prediction Network (DSPN) can be multiset-equivariant without being
hindered by set-equivariance and improve it with approximate implicit
differentiation, allowing for better optimization while being faster and saving
memory. In a range of toy experiments, we show that the perspective of
multiset-equivariance is beneficial and that our changes to DSPN achieve better
results in most cases. On CLEVR object property prediction, we substantially
improve over the state-of-the-art Slot Attention from 8% to 77% in one of the
strictest evaluation metrics because of the benefits made possible by implicit
differentiation.","['Yan Zhang', 'David W. Zhang', 'Simon Lacoste-Julien', 'Gertjan J. Burghouts', 'Cees G. M. Snoek']","['cs.LG', 'stat.ML']",2021-11-23 23:10:30+00:00
http://arxiv.org/abs/2111.12187v1,Input Convex Gradient Networks,"The gradients of convex functions are expressive models of non-trivial vector
fields. For example, Brenier's theorem yields that the optimal transport map
between any two measures on Euclidean space under the squared distance is
realized as a convex gradient, which is a key insight used in recent generative
flow models. In this paper, we study how to model convex gradients by
integrating a Jacobian-vector product parameterized by a neural network, which
we call the Input Convex Gradient Network (ICGN). We theoretically study ICGNs
and compare them to taking the gradient of an Input-Convex Neural Network
(ICNN), empirically demonstrating that a single layer ICGN can fit a toy
example better than a single layer ICNN. Lastly, we explore extensions to
deeper networks and connections to constructions from Riemannian geometry.","['Jack Richter-Powell', 'Jonathan Lorraine', 'Brandon Amos']","['cs.LG', 'stat.ML']",2021-11-23 22:51:25+00:00
http://arxiv.org/abs/2111.12166v2,Towards Empirical Sandwich Bounds on the Rate-Distortion Function,"Rate-distortion (R-D) function, a key quantity in information theory,
characterizes the fundamental limit of how much a data source can be compressed
subject to a fidelity criterion, by any compression algorithm. As researchers
push for ever-improving compression performance, establishing the R-D function
of a given data source is not only of scientific interest, but also sheds light
on the possible room for improving compression algorithms. Previous work on
this problem relied on distributional assumptions on the data source (Gibson,
2017) or only applied to discrete data (Blahut, 1972; Arimoto, 1972). By
contrast, this paper makes the first attempt at an algorithm for sandwiching
the R-D function of a general (not necessarily discrete) source requiring only
i.i.d. data samples. We estimate R-D sandwich bounds for a variety of
artificial and real-world data sources, in settings far beyond the feasibility
of any known method, and shed light on the optimality of neural data
compression (Ball\'e et al., 2021; Yang et al., 2022). Our R-D upper bound on
natural images indicates theoretical room for improving state-of-the-art image
compression methods by at least one dB in PSNR at various bitrates. Our data
and code can be found at https://github.com/mandt-lab/empirical-RD-sandwich.","['Yibo Yang', 'Stephan Mandt']","['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2021-11-23 21:50:27+00:00
http://arxiv.org/abs/2111.12157v3,A Bayesian Model for Online Activity Sample Sizes,"In many contexts it is useful to predict the number of individuals in some
population who will initiate a particular activity during a given period. For
example, the number of users who will install a software update, the number of
customers who will use a new feature on a website or who will participate in an
A/B test. In practical settings, there is heterogeneity amongst individuals
with regard to the distribution of time until they will initiate. For these
reasons it is inappropriate to assume that the number of new individuals
observed on successive days will be identically distributed. Given observations
on the number of unique users participating in an initial period, we present a
simple but novel Bayesian method for predicting the number of additional
individuals who will participate during a subsequent period. We illustrate the
performance of the method in predicting sample size in online experimentation.","['Thomas Richardson', 'Yu Liu', 'James McQueen', 'Doug Hains']","['stat.ML', 'cs.LG']",2021-11-23 21:16:17+00:00
http://arxiv.org/abs/2111.12151v1,Best Arm Identification with Safety Constraints,"The best arm identification problem in the multi-armed bandit setting is an
excellent model of many real-world decision-making problems, yet it fails to
capture the fact that in the real-world, safety constraints often must be met
while learning. In this work we study the question of best-arm identification
in safety-critical settings, where the goal of the agent is to find the best
safe option out of many, while exploring in a way that guarantees certain,
initially unknown safety constraints are met. We first analyze this problem in
the setting where the reward and safety constraint takes a linear structure,
and show nearly matching upper and lower bounds. We then analyze a much more
general version of the problem where we only assume the reward and safety
constraint can be modeled by monotonic functions, and propose an algorithm in
this setting which is guaranteed to learn safely. We conclude with experimental
results demonstrating the effectiveness of our approaches in scenarios such as
safely identifying the best drug out of many in order to treat an illness.","['Zhenlin Wang', 'Andrew Wagenmaker', 'Kevin Jamieson']","['cs.LG', 'stat.ML']",2021-11-23 20:53:12+00:00
http://arxiv.org/abs/2111.12148v1,Machine Learning Based Forward Solver: An Automatic Framework in gprMax,"General full-wave electromagnetic solvers, such as those utilizing the
finite-difference time-domain (FDTD) method, are computationally demanding for
simulating practical GPR problems. We explore the performance of a
near-real-time, forward modeling approach for GPR that is based on a machine
learning (ML) architecture. To ease the process, we have developed a framework
that is capable of generating these ML-based forward solvers automatically. The
framework uses an innovative training method that combines a predictive
dimensionality reduction technique and a large data set of modeled GPR
responses from our FDTD simulation software, gprMax. The forward solver is
parameterized for a specific GPR application, but the framework can be extended
in a straightforward manner to different electromagnetic problems.","['Utsav Akhaury', 'Iraklis Giannakis', 'Craig Warren', 'Antonios Giannopoulos']","['eess.SP', 'physics.geo-ph', 'stat.ML']",2021-11-23 20:46:21+00:00
http://arxiv.org/abs/2111.12143v4,Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians: General Theory and Applications,"Deep neural networks are notorious for defying theoretical treatment.
However, when the number of parameters in each layer tends to infinity, the
network function is a Gaussian process (GP) and quantitatively predictive
description is possible. Gaussian approximation allows one to formulate
criteria for selecting hyperparameters, such as variances of weights and
biases, as well as the learning rate. These criteria rely on the notion of
criticality defined for deep neural networks. In this work we describe a new
practical way to diagnose criticality. We introduce \emph{partial Jacobians} of
a network, defined as derivatives of preactivations in layer $l$ with respect
to preactivations in layer $l_0\leq l$. We derive recurrence relations for the
norms of partial Jacobians and utilize these relations to analyze criticality
of deep fully connected neural networks with LayerNorm and/or residual
connections. We derive and implement a simple and cheap numerical test that
allows one to select optimal initialization for a broad class of deep neural
networks; containing fully connected, convolutional and normalization layers.
Using these tools we show quantitatively that proper stacking of the LayerNorm
(applied to preactivations) and residual connections leads to an architecture
that is critical for any initialization. Finally, we apply our methods to
analyze ResNet and MLP-Mixer architectures; demonstrating the
everywhere-critical regime.","['Darshil Doshi', 'Tianyu He', 'Andrey Gromov']","['cs.LG', 'cond-mat.dis-nn', 'hep-th', 'stat.ML']",2021-11-23 20:31:42+00:00
http://arxiv.org/abs/2111.12140v1,Filter Methods for Feature Selection in Supervised Machine Learning Applications -- Review and Benchmark,"The amount of data for machine learning (ML) applications is constantly
growing. Not only the number of observations, especially the number of measured
variables (features) increases with ongoing digitization. Selecting the most
appropriate features for predictive modeling is an important lever for the
success of ML applications in business and research. Feature selection methods
(FSM) that are independent of a certain ML algorithm - so-called filter methods
- have been numerously suggested, but little guidance for researchers and
quantitative modelers exists to choose appropriate approaches for typical ML
problems. This review synthesizes the substantial literature on feature
selection benchmarking and evaluates the performance of 58 methods in the
widely used R environment. For concrete guidance, we consider four typical
dataset scenarios that are challenging for ML models (noisy, redundant,
imbalanced data and cases with more features than observations). Drawing on the
experience of earlier benchmarks, which have considered much fewer FSMs, we
compare the performance of the methods according to four criteria (predictive
performance, number of relevant features selected, stability of the feature
sets and runtime). We found methods relying on the random forest approach, the
double input symmetrical relevance filter (DISR) and the joint impurity filter
(JIM) were well-performing candidate methods for the given dataset scenarios.","['Konstantin Hopf', 'Sascha Reifenrath']","['cs.LG', 'cs.DB', 'stat.ML']",2021-11-23 20:20:24+00:00
http://arxiv.org/abs/2111.12139v1,ChebLieNet: Invariant Spectral Graph NNs Turned Equivariant by Riemannian Geometry on Lie Groups,"We introduce ChebLieNet, a group-equivariant method on (anisotropic)
manifolds. Surfing on the success of graph- and group-based neural networks, we
take advantage of the recent developments in the geometric deep learning field
to derive a new approach to exploit any anisotropies in data. Via discrete
approximations of Lie groups, we develop a graph neural network made of
anisotropic convolutional layers (Chebyshev convolutions), spatial pooling and
unpooling layers, and global pooling layers. Group equivariance is achieved via
equivariant and invariant operators on graphs with anisotropic left-invariant
Riemannian distance-based affinities encoded on the edges. Thanks to its simple
form, the Riemannian metric can model any anisotropies, both in the spatial and
orientation domains. This control on anisotropies of the Riemannian metrics
allows to balance equivariance (anisotropic metric) against invariance
(isotropic metric) of the graph convolution layers. Hence we open the doors to
a better understanding of anisotropic properties. Furthermore, we empirically
prove the existence of (data-dependent) sweet spots for anisotropic parameters
on CIFAR10. This crucial result is evidence of the benefice we could get by
exploiting anisotropic properties in data. We also evaluate the scalability of
this approach on STL10 (image data) and ClimateNet (spherical data), showing
its remarkable adaptability to diverse tasks.","['Hugo Aguettaz', 'Erik J. Bekkers', 'Michaël Defferrard']","['cs.LG', 'stat.ML']",2021-11-23 20:19:36+00:00
http://arxiv.org/abs/2111.12064v2,Semantic-Aware Collaborative Deep Reinforcement Learning Over Wireless Cellular Networks,"Collaborative deep reinforcement learning (CDRL) algorithms in which multiple
agents can coordinate over a wireless network is a promising approach to enable
future intelligent and autonomous systems that rely on real-time
decision-making in complex dynamic environments. Nonetheless, in practical
scenarios, CDRL faces many challenges due to the heterogeneity of agents and
their learning tasks, different environments, time constraints of the learning,
and resource limitations of wireless networks. To address these challenges, in
this paper, a novel semantic-aware CDRL method is proposed to enable a group of
heterogeneous untrained agents with semantically-linked DRL tasks to
collaborate efficiently across a resource-constrained wireless cellular
network. To this end, a new heterogeneous federated DRL (HFDRL) algorithm is
proposed to select the best subset of semantically relevant DRL agents for
collaboration. The proposed approach then jointly optimizes the training loss
and wireless bandwidth allocation for the cooperating selected agents in order
to train each agent within the time limit of its real-time task. Simulation
results show the superior performance of the proposed algorithm compared to
state-of-the-art baselines.","['Fatemeh Lotfi', 'Omid Semiari', 'Walid Saad']","['cs.IT', 'cs.LG', 'cs.NI', 'eess.SP', 'math.IT', 'stat.ML']",2021-11-23 18:24:47+00:00
http://arxiv.org/abs/2111.11971v5,Tree density estimation,"We study the problem of estimating the density $f(\boldsymbol x)$ of a random
vector ${\boldsymbol X}$ in $\mathbb R^d$. For a spanning tree $T$ defined on
the vertex set $\{1,\dots ,d\}$, the tree density $f_{T}$ is a product of
bivariate conditional densities. An optimal spanning tree minimizes the
Kullback-Leibler divergence between $f$ and $f_{T}$. From i.i.d. data we
identify an optimal tree $T^*$ and efficiently construct a tree density
estimate $f_n$ such that, without any regularity conditions on the density $f$,
one has $\lim_{n\to \infty} \int |f_n(\boldsymbol x)-f_{T^*}(\boldsymbol
x)|d\boldsymbol x=0$ a.s. For Lipschitz $f$ with bounded support, $\mathbb E
\left\{ \int |f_n(\boldsymbol x)-f_{T^*}(\boldsymbol x)|d\boldsymbol
x\right\}=O\big(n^{-1/4}\big)$, a dimension-free rate.","['László Györfi', 'Aryeh Kontorovich', 'Roi Weiss']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2021-11-23 16:05:59+00:00
http://arxiv.org/abs/2111.11954v1,Depth induces scale-averaging in overparameterized linear Bayesian neural networks,"Inference in deep Bayesian neural networks is only fully understood in the
infinite-width limit, where the posterior flexibility afforded by increased
depth washes out and the posterior predictive collapses to a shallow Gaussian
process. Here, we interpret finite deep linear Bayesian neural networks as
data-dependent scale mixtures of Gaussian process predictors across output
channels. We leverage this observation to study representation learning in
these networks, allowing us to connect limiting results obtained in previous
studies within a unified framework. In total, these results advance our
analytical understanding of how depth affects inference in a simple class of
Bayesian neural networks.","['Jacob A. Zavatone-Veth', 'Cengiz Pehlevan']","['cs.LG', 'stat.ML']",2021-11-23 15:48:47+00:00
http://arxiv.org/abs/2111.11946v1,Is Shapley Explanation for a model unique?,"Shapley value has recently become a popular way to explain the predictions of
complex and simple machine learning models. This paper is discusses the factors
that influence Shapley value. In particular, we explore the relationship
between the distribution of a feature and its Shapley value. We extend our
analysis by discussing the difference that arises in Shapley explanation for
different predicted outcomes from the same model. Our assessment is that
Shapley value for particular feature not only depends on its expected mean but
on other moments as well such as variance and there are disagreements for
baseline prediction, disagreements for signs and most important feature for
different outcomes such as probability, log odds, and binary decision generated
using same linear probability model (logit/probit). These disagreements not
only stay for local explainability but also affect the global feature
importance. We conclude that there is no unique Shapley explanation for a given
model. It varies with model outcome (Probability/Log-odds/binary decision such
as accept vs reject) and hence model application.","['Harsh Kumar', 'Jithu Chandran']","['stat.ML', 'cs.LG', 'stat.ME']",2021-11-23 15:31:46+00:00
http://arxiv.org/abs/2111.11763v1,Uncertainty estimation under model misspecification in neural network regression,"Although neural networks are powerful function approximators, the underlying
modelling assumptions ultimately define the likelihood and thus the hypothesis
class they are parameterizing. In classification, these assumptions are minimal
as the commonly employed softmax is capable of representing any categorical
distribution. In regression, however, restrictive assumptions on the type of
continuous distribution to be realized are typically placed, like the dominant
choice of training via mean-squared error and its underlying Gaussianity
assumption. Recently, modelling advances allow to be agnostic to the type of
continuous distribution to be modelled, granting regression the flexibility of
classification models. While past studies stress the benefit of such flexible
regression models in terms of performance, here we study the effect of the
model choice on uncertainty estimation. We highlight that under model
misspecification, aleatoric uncertainty is not properly captured, and that a
Bayesian treatment of a misspecified model leads to unreliable epistemic
uncertainty estimates. Overall, our study provides an overview on how modelling
choices in regression may influence uncertainty estimation and thus any
downstream decision making process.","['Maria R. Cervera', 'Rafael Dätwyler', ""Francesco D'Angelo"", 'Hamza Keurti', 'Benjamin F. Grewe', 'Christian Henning']","['cs.LG', 'stat.ML']",2021-11-23 10:18:41+00:00
http://arxiv.org/abs/2111.11703v1,A Contextual Latent Space Model: Subsequence Modulation in Melodic Sequence,"Some generative models for sequences such as music and text allow us to edit
only subsequences, given surrounding context sequences, which plays an
important part in steering generation interactively. However, editing
subsequences mainly involves randomly resampling subsequences from a possible
generation space. We propose a contextual latent space model (CLSM) in order
for users to be able to explore subsequence generation with a sense of
direction in the generation space, e.g., interpolation, as well as exploring
variations -- semantically similar possible subsequences. A context-informed
prior and decoder constitute the generative model of CLSM, and a context
position-informed encoder is the inference model. In experiments, we use a
monophonic symbolic music dataset, demonstrating that our contextual latent
space is smoother in interpolation than baselines, and the quality of generated
samples is superior to baseline models. The generation examples are available
online.",['Taketo Akama'],"['cs.LG', 'cs.AI', 'cs.SD', 'eess.AS', 'stat.ML']",2021-11-23 07:51:39+00:00
http://arxiv.org/abs/2111.11694v5,MARS via LASSO,"Multivariate adaptive regression splines (MARS) is a popular method for
nonparametric regression introduced by Friedman in 1991. MARS fits simple
nonlinear and non-additive functions to regression data. We propose and study a
natural lasso variant of the MARS method. Our method is based on least squares
estimation over a convex class of functions obtained by considering
infinite-dimensional linear combinations of functions in the MARS basis and
imposing a variation based complexity constraint. Our estimator can be computed
via finite-dimensional convex optimization, although it is defined as a
solution to an infinite-dimensional optimization problem. Under a few standard
design assumptions, we prove that our estimator achieves a rate of convergence
that depends only logarithmically on dimension and thus avoids the usual curse
of dimensionality to some extent. We also show that our method is naturally
connected to nonparametric estimation techniques based on smoothness
constraints. We implement our method with a cross-validation scheme for the
selection of the involved tuning parameter and compare it to the usual MARS
method in various simulation and real data settings.","['Dohyeong Ki', 'Billy Fang', 'Adityanand Guntuboyina']","['math.ST', 'stat.ML', 'stat.TH', '62G08']",2021-11-23 07:30:33+00:00
http://arxiv.org/abs/2111.11676v1,RIO: Rotation-equivariance supervised learning of robust inertial odometry,"This paper introduces rotation-equivariance as a self-supervisor to train
inertial odometry models. We demonstrate that the self-supervised scheme
provides a powerful supervisory signal at training phase as well as at
inference stage. It reduces the reliance on massive amounts of labeled data for
training a robust model and makes it possible to update the model using various
unlabeled data. Further, we propose adaptive Test-Time Training (TTT) based on
uncertainty estimations in order to enhance the generalizability of the
inertial odometry to various unseen data. We show in experiments that the
Rotation-equivariance-supervised Inertial Odometry (RIO) trained with 30% data
achieves on par performance with a model trained with the whole database.
Adaptive TTT improves models performance in all cases and makes more than 25%
improvements under several scenarios.","['Caifa Zhou', 'Xiya Cao', 'Dandan Zeng', 'Yongliang Wang']","['stat.ML', 'cs.LG']",2021-11-23 06:49:40+00:00
http://arxiv.org/abs/2111.11655v2,Multi-task manifold learning for small sample size datasets,"In this study, we develop a method for multi-task manifold learning. The
method aims to improve the performance of manifold learning for multiple tasks,
particularly when each task has a small number of samples. Furthermore, the
method also aims to generate new samples for new tasks, in addition to new
samples for existing tasks. In the proposed method, we use two different types
of information transfer: instance transfer and model transfer. For instance
transfer, datasets are merged among similar tasks, whereas for model transfer,
the manifold models are averaged among similar tasks. For this purpose, the
proposed method consists of a set of generative manifold models corresponding
to the tasks, which are integrated into a general model of a fiber bundle. We
applied the proposed method to artificial datasets and face image sets, and the
results showed that the method was able to estimate the manifolds, even for a
tiny number of samples.","['Hideaki Ishibashi', 'Kazushi Higa', 'Tetsuo Furukawa']","['cs.LG', 'stat.ML']",2021-11-23 05:03:37+00:00
http://arxiv.org/abs/2111.11639v1,Isolation forests: looking beyond tree depth,"The isolation forest algorithm for outlier detection exploits a simple yet
effective observation: if taking some multivariate data and making uniformly
random cuts across the feature space recursively, it will take fewer such
random cuts for an outlier to be left alone in a given subspace as compared to
regular observations. The original idea proposed an outlier score based on the
tree depth (number of random cuts) required for isolation, but experiments here
show that using information about the size of the feature space taken and the
number of points assigned to it can result in improved results in many
situations without any modification to the tree structure, especially in the
presence of categorical features.",['David Cortes'],"['stat.ML', 'cs.LG']",2021-11-23 04:04:31+00:00
http://arxiv.org/abs/2111.11630v1,"Aggregation of Models, Choices, Beliefs, and Preferences","A natural notion of rationality/consistency for aggregating models is that,
for all (possibly aggregated) models $A$ and $B$, if the output of model $A$ is
$f(A)$ and if the output model $B$ is $f(B)$, then the output of the model
obtained by aggregating $A$ and $B$ must be a weighted average of $f(A)$ and
$f(B)$. Similarly, a natural notion of rationality for aggregating preferences
of ensembles of experts is that, for all (possibly aggregated) experts $A$ and
$B$, and all possible choices $x$ and $y$, if both $A$ and $B$ prefer $x$ over
$y$, then the expert obtained by aggregating $A$ and $B$ must also prefer $x$
over $y$. Rational aggregation is an important element of uncertainty
quantification, and it lies behind many seemingly different results in economic
theory: spanning social choice, belief formation, and individual decision
making. Three examples of rational aggregation rules are as follows. (1) Give
each individual model (expert) a weight (a score) and use weighted averaging to
aggregate individual or finite ensembles of models (experts). (2) Order/rank
individual model (expert) and let the aggregation of a finite ensemble of
individual models (experts) be the highest-ranked individual model (expert) in
that ensemble. (3) Give each individual model (expert) a weight, introduce a
weak order/ranking over the set of models/experts, aggregate $A$ and $B$ as the
weighted average of the highest-ranked models (experts) in $A$ or $B$. Note
that (1) and (2) are particular cases of (3). In this paper, we show that all
rational aggregation rules are of the form (3). This result unifies aggregation
procedures across different economic environments. Following the main
representation, we show applications and extensions of our representation in
various separated economics topics such as belief formation, choice theory, and
social welfare economics.","['Hamed Hamze Bajgiran', 'Houman Owhadi']","['econ.TH', 'math.PR', 'stat.ML']",2021-11-23 03:26:42+00:00
http://arxiv.org/abs/2111.11556v2,FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning,"Federated Learning (FL) is an increasingly popular machine learning paradigm
in which multiple nodes try to collaboratively learn under privacy,
communication and multiple heterogeneity constraints. A persistent problem in
federated learning is that it is not clear what the optimization objective
should be: the standard average risk minimization of supervised learning is
inadequate in handling several major constraints specific to federated
learning, such as communication adaptivity and personalization control. We
identify several key desiderata in frameworks for federated learning and
introduce a new framework, FLIX, that takes into account the unique challenges
brought by federated learning. FLIX has a standard finite-sum form, which
enables practitioners to tap into the immense wealth of existing (potentially
non-local) methods for distributed optimization. Through a smart initialization
that does not require any communication, FLIX does not require the use of local
steps but is still provably capable of performing dissimilarity regularization
on par with local methods. We give several algorithms for solving the FLIX
formulation efficiently under communication constraints. Finally, we
corroborate our theoretical results with extensive experimentation.","['Elnur Gasanov', 'Ahmed Khaled', 'Samuel Horváth', 'Peter Richtárik']","['cs.LG', 'math.OC', 'stat.ML']",2021-11-22 22:06:58+00:00
http://arxiv.org/abs/2111.11550v1,Dynamic Regret for Strongly Adaptive Methods and Optimality of Online KRR,"We consider the framework of non-stationary Online Convex Optimization where
a learner seeks to control its dynamic regret against an arbitrary sequence of
comparators. When the loss functions are strongly convex or exp-concave, we
demonstrate that Strongly Adaptive (SA) algorithms can be viewed as a
principled way of controlling dynamic regret in terms of path variation $V_T$
of the comparator sequence. Specifically, we show that SA algorithms enjoy
$\tilde O(\sqrt{TV_T} \vee \log T)$ and $\tilde O(\sqrt{dTV_T} \vee d\log T)$
dynamic regret for strongly convex and exp-concave losses respectively without
apriori knowledge of $V_T$. The versatility of the principled approach is
further demonstrated by the novel results in the setting of learning against
bounded linear predictors and online regression with Gaussian kernels.
  Under a related setting, the second component of the paper addresses an open
question posed by Zhdanov and Kalnishkan (2010) that concerns online kernel
regression with squared error losses. We derive a new lower bound on a certain
penalized regret which establishes the near minimax optimality of online Kernel
Ridge Regression (KRR). Our lower bound can be viewed as an RKHS extension to
the lower bound derived in Vovk (2001) for online linear regression in finite
dimensions.","['Dheeraj Baby', 'Hilaf Hasson', 'Yuyang Wang']","['cs.LG', 'stat.ML']",2021-11-22 21:52:47+00:00
http://arxiv.org/abs/2111.11542v1,Depth Without the Magic: Inductive Bias of Natural Gradient Descent,"In gradient descent, changing how we parametrize the model can lead to
drastically different optimization trajectories, giving rise to a surprising
range of meaningful inductive biases: identifying sparse classifiers or
reconstructing low-rank matrices without explicit regularization. This implicit
regularization has been hypothesised to be a contributing factor to good
generalization in deep learning. However, natural gradient descent is
approximately invariant to reparameterization, it always follows the same
trajectory and finds the same optimum. The question naturally arises: What
happens if we eliminate the role of parameterization, which solution will be
found, what new properties occur? We characterize the behaviour of natural
gradient flow in deep linear networks for separable classification under
logistic loss and deep matrix factorization. Some of our findings extend to
nonlinear neural networks with sufficient but finite over-parametrization. We
demonstrate that there exist learning problems where natural gradient descent
fails to generalize, while gradient descent with the right architecture
performs well.","['Anna Kerekes', 'Anna Mészáros', 'Ferenc Huszár']","['stat.ML', 'cs.LG']",2021-11-22 21:20:10+00:00
http://arxiv.org/abs/2111.11510v4,Bootstrap Your Flow,"Normalizing flows are flexible, parameterized distributions that can be used
to approximate expectations from intractable distributions via importance
sampling. However, current flow-based approaches are limited on challenging
targets where they either suffer from mode seeking behaviour or high variance
in the training loss, or rely on samples from the target distribution, which
may not be available. To address these challenges, we combine flows with
annealed importance sampling (AIS), while using the $\alpha$-divergence as our
objective, in a novel training procedure, FAB (Flow AIS Bootstrap). Thereby,
the flow and AIS improve each other in a bootstrapping manner. We demonstrate
that FAB can be used to produce accurate approximations to complex target
distributions, including Boltzmann distributions, in problems where previous
flow-based methods fail.","['Laurence Illing Midgley', 'Vincent Stimper', 'Gregor N. C. Simm', 'José Miguel Hernández-Lobato']","['cs.LG', 'cs.AI', 'stat.ML']",2021-11-22 20:11:47+00:00
http://arxiv.org/abs/2111.11507v4,Approximate Bayesian Computation via Classification,"Approximate Bayesian Computation (ABC) enables statistical inference in
simulator-based models whose likelihoods are difficult to calculate but easy to
simulate from. ABC constructs a kernel-type approximation to the posterior
distribution through an accept/reject mechanism which compares summary
statistics of real and simulated data. To obviate the need for summary
statistics, we directly compare empirical distributions with a Kullback-Leibler
(KL) divergence estimator obtained via contrastive learning. In particular, we
blend flexible machine learning classifiers within ABC to automate fake/real
data comparisons. We consider the traditional accept/reject kernel as well as
an exponential weighting scheme which does not require the ABC acceptance
threshold. Our theoretical results show that the rate at which our ABC
posterior distributions concentrate around the true parameter depends on the
estimation error of the classifier. We derive limiting posterior shape results
and find that, with a properly scaled exponential kernel, asymptotic normality
holds. We demonstrate the usefulness of our approach on simulated examples as
well as real data in the context of stock volatility estimation.","['Yuexi Wang', 'Tetsuya Kaji', 'Veronika Ročková']","['stat.ME', 'stat.ML']",2021-11-22 20:07:55+00:00
http://arxiv.org/abs/2111.11485v2,A Free Lunch from the Noise: Provable and Practical Exploration for Representation Learning,"Representation learning lies at the heart of the empirical success of deep
learning for dealing with the curse of dimensionality. However, the power of
representation learning has not been fully exploited yet in reinforcement
learning (RL), due to i), the trade-off between expressiveness and
tractability; and ii), the coupling between exploration and representation
learning. In this paper, we first reveal the fact that under some noise
assumption in the stochastic control model, we can obtain the linear spectral
feature of its corresponding Markov transition operator in closed-form for
free. Based on this observation, we propose Spectral Dynamics Embedding
(SPEDE), which breaks the trade-off and completes optimistic exploration for
representation learning by exploiting the structure of the noise. We provide
rigorous theoretical analysis of SPEDE, and demonstrate the practical superior
performance over the existing state-of-the-art empirical algorithms on several
benchmarks.","['Tongzheng Ren', 'Tianjun Zhang', 'Csaba Szepesvári', 'Bo Dai']","['stat.ML', 'cs.AI', 'cs.LG']",2021-11-22 19:24:57+00:00
http://arxiv.org/abs/2111.11482v1,Graph Neural Networks with Parallel Neighborhood Aggregations for Graph Classification,"We focus on graph classification using a graph neural network (GNN) model
that precomputes the node features using a bank of neighborhood aggregation
graph operators arranged in parallel. These GNN models have a natural advantage
of reduced training and inference time due to the precomputations but are also
fundamentally different from popular GNN variants that update node features
through a sequential neighborhood aggregation procedure during training. We
provide theoretical conditions under which a generic GNN model with parallel
neighborhood aggregations (PA-GNNs, in short) are provably as powerful as the
well-known Weisfeiler-Lehman (WL) graph isomorphism test in discriminating
non-isomorphic graphs. Although PA-GNN models do not have an apparent
relationship with the WL test, we show that the graph embeddings obtained from
these two methods are injectively related. We then propose a specialized PA-GNN
model, called SPIN, which obeys the developed conditions. We demonstrate via
numerical experiments that the developed model achieves state-of-the-art
performance on many diverse real-world datasets while maintaining the
discriminative power of the WL test and the computational advantage of
preprocessing graphs before the training process.","['Siddhant Doshi', 'Sundeep Prabhakar Chepuri']","['cs.LG', 'eess.SP', 'stat.ML']",2021-11-22 19:19:40+00:00
http://arxiv.org/abs/2111.11344v3,Modeling Irregular Time Series with Continuous Recurrent Units,"Recurrent neural networks (RNNs) are a popular choice for modeling sequential
data. Modern RNN architectures assume constant time-intervals between
observations. However, in many datasets (e.g. medical records) observation
times are irregular and can carry important information. To address this
challenge, we propose continuous recurrent units (CRUs) -- a neural
architecture that can naturally handle irregular intervals between
observations. The CRU assumes a hidden state, which evolves according to a
linear stochastic differential equation and is integrated into an
encoder-decoder framework. The recursive computations of the CRU can be derived
using the continuous-discrete Kalman filter and are in closed form. The
resulting recurrent architecture has temporal continuity between hidden states
and a gating mechanism that can optimally integrate noisy observations. We
derive an efficient parameterization scheme for the CRU that leads to a fast
implementation f-CRU. We empirically study the CRU on a number of challenging
datasets and find that it can interpolate irregular time series better than
methods based on neural ordinary differential equations.","['Mona Schirmer', 'Mazin Eltayeb', 'Stefan Lessmann', 'Maja Rudolph']","['cs.LG', 'stat.ML']",2021-11-22 16:49:15+00:00
http://arxiv.org/abs/2111.11328v1,Cycle Consistent Probability Divergences Across Different Spaces,"Discrepancy measures between probability distributions are at the core of
statistical inference and machine learning. In many applications, distributions
of interest are supported on different spaces, and yet a meaningful
correspondence between data points is desired. Motivated to explicitly encode
consistent bidirectional maps into the discrepancy measure, this work proposes
a novel unbalanced Monge optimal transport formulation for matching, up to
isometries, distributions on different spaces. Our formulation arises as a
principled relaxation of the Gromov-Haussdroff distance between metric spaces,
and employs two cycle-consistent maps that push forward each distribution onto
the other. We study structural properties of the proposed discrepancy and, in
particular, show that it captures the popular cycle-consistent generative
adversarial network (GAN) framework as a special case, thereby providing the
theory to explain it. Motivated by computational efficiency, we then kernelize
the discrepancy and restrict the mappings to parametric function classes. The
resulting kernelized version is coined the generalized maximum mean discrepancy
(GMMD). Convergence rates for empirical estimation of GMMD are studied and
experiments to support our theory are provided.","['Zhengxin Zhang', 'Youssef Mroueh', 'Ziv Goldfeld', 'Bharath K. Sriperumbudur']","['cs.LG', 'stat.ML']",2021-11-22 16:35:58+00:00
http://arxiv.org/abs/2111.11320v3,Private and polynomial time algorithms for learning Gaussians and beyond,"We present a fairly general framework for reducing $(\varepsilon, \delta)$
differentially private (DP) statistical estimation to its non-private
counterpart. As the main application of this framework, we give a polynomial
time and $(\varepsilon,\delta)$-DP algorithm for learning (unrestricted)
Gaussian distributions in $\mathbb{R}^d$. The sample complexity of our approach
for learning the Gaussian up to total variation distance $\alpha$ is
$\widetilde{O}(d^2/\alpha^2 + d^2\sqrt{\ln(1/\delta)}/\alpha \varepsilon +
d\ln(1/\delta) / \alpha \varepsilon)$ matching (up to logarithmic factors) the
best known information-theoretic (non-efficient) sample complexity upper bound
due to Aden-Ali, Ashtiani, and Kamath (ALT'21). In an independent work, Kamath,
Mouzakis, Singhal, Steinke, and Ullman (arXiv:2111.04609) proved a similar
result using a different approach and with $O(d^{5/2})$ sample complexity
dependence on $d$. As another application of our framework, we provide the
first polynomial time $(\varepsilon, \delta)$-DP algorithm for robust learning
of (unrestricted) Gaussians with sample complexity $\widetilde{O}(d^{3.5})$. In
another independent work, Kothari, Manurangsi, and Velingker (arXiv:2112.03548)
also provided a polynomial time $(\varepsilon, \delta)$-DP algorithm for robust
learning of Gaussians with sample complexity $\widetilde{O}(d^8)$.","['Hassan Ashtiani', 'Christopher Liaw']","['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']",2021-11-22 16:25:51+00:00
http://arxiv.org/abs/2111.11306v2,Learning PSD-valued functions using kernel sums-of-squares,"Shape constraints such as positive semi-definiteness (PSD) for matrices or
convexity for functions play a central role in many applications in machine
learning and sciences, including metric learning, optimal transport, and
economics. Yet, very few function models exist that enforce PSD-ness or
convexity with good empirical performance and theoretical guarantees. In this
paper, we introduce a kernel sum-of-squares model for functions that take
values in the PSD cone, which extends kernel sums-of-squares models that were
recently proposed to encode non-negative scalar functions. We provide a
representer theorem for this class of PSD functions, show that it constitutes a
universal approximator of PSD functions, and derive eigenvalue bounds in the
case of subsampled equality constraints. We then apply our results to modeling
convex functions, by enforcing a kernel sum-of-squares representation of their
Hessian, and show that any smooth and strongly convex function may be thus
represented. Finally, we illustrate our methods on a PSD matrix-valued
regression task, and on scalar-valued convex regression.","['Boris Muzellec', 'Francis Bach', 'Alessandro Rudi']","['stat.ML', 'cs.LG']",2021-11-22 16:07:50+00:00
http://arxiv.org/abs/2111.11223v2,Transfer Learning with Gaussian Processes for Bayesian Optimization,"Bayesian optimization is a powerful paradigm to optimize black-box functions
based on scarce and noisy data. Its data efficiency can be further improved by
transfer learning from related tasks. While recent transfer models meta-learn a
prior based on large amount of data, in the low-data regime methods that
exploit the closed-form posterior of Gaussian processes (GPs) have an
advantage. In this setting, several analytically tractable transfer-model
posteriors have been proposed, but the relative advantages of these methods are
not well understood. In this paper, we provide a unified view on hierarchical
GP models for transfer learning, which allows us to analyze the relationship
between methods. As part of the analysis, we develop a novel closed-form
boosted GP transfer model that fits between existing approaches in terms of
complexity. We evaluate the performance of the different approaches in
large-scale experiments and highlight strengths and weaknesses of the different
transfer-learning methods.","['Petru Tighineanu', 'Kathrin Skubch', 'Paul Baireuther', 'Attila Reiss', 'Felix Berkenkamp', 'Julia Vinogradska']","['stat.ML', 'cs.AI', 'cs.LG']",2021-11-22 14:09:45+00:00
http://arxiv.org/abs/2111.11153v2,Plant 'n' Seek: Can You Find the Winning Ticket?,"The lottery ticket hypothesis has sparked the rapid development of pruning
algorithms that aim to reduce the computational costs associated with deep
learning during training and model deployment. Currently, such algorithms are
primarily evaluated on imaging data, for which we lack ground truth information
and thus the understanding of how sparse lottery tickets could be. To fill this
gap, we develop a framework that allows us to plant and hide winning tickets
with desirable properties in randomly initialized neural networks. To analyze
the ability of state-of-the-art pruning to identify tickets of extreme
sparsity, we design and hide such tickets solving four challenging tasks. In
extensive experiments, we observe similar trends as in imaging studies,
indicating that our framework can provide transferable insights into realistic
problems. Additionally, we can now see beyond such relative trends and
highlight limitations of current pruning methods. Based on our results, we
conclude that the current limitations in ticket sparsity are likely of
algorithmic rather than fundamental nature. We anticipate that comparisons to
planted tickets will facilitate future developments of efficient pruning
algorithms.","['Jonas Fischer', 'Rebekka Burkholz']","['cs.LG', 'cs.AI', 'stat.ML']",2021-11-22 12:32:25+00:00
http://arxiv.org/abs/2111.11146v2,On the Existence of Universal Lottery Tickets,"The lottery ticket hypothesis conjectures the existence of sparse subnetworks
of large randomly initialized deep neural networks that can be successfully
trained in isolation. Recent work has experimentally observed that some of
these tickets can be practically reused across a variety of tasks, hinting at
some form of universality. We formalize this concept and theoretically prove
that not only do such universal tickets exist but they also do not require
further training. Our proofs introduce a couple of technical innovations
related to pruning for strong lottery tickets, including extensions of subset
sum results and a strategy to leverage higher amounts of depth. Our explicit
sparse constructions of universal function families might be of independent
interest, as they highlight representational benefits induced by univariate
convolutional architectures.","['Rebekka Burkholz', 'Nilanjana Laha', 'Rajarshi Mukherjee', 'Alkis Gotovos']","['cs.LG', 'cs.AI', 'stat.ML']",2021-11-22 12:12:00+00:00
http://arxiv.org/abs/2111.11052v1,IAD: Indirect Anomalous VMMs Detection in the Cloud-based Environment,"Server virtualization in the form of virtual machines (VMs) with the use of a
hypervisor or a Virtual Machine Monitor (VMM) is an essential part of cloud
computing technology to provide infrastructure-as-a-service (IaaS). A fault or
an anomaly in the VMM can propagate to the VMs hosted on it and ultimately
affect the availability and reliability of the applications running on those
VMs. Therefore, identifying and eventually resolving it quickly is highly
important. However, anomalous VMM detection is a challenge in the cloud
environment since the user does not have access to the VMM.
  This paper addresses this challenge of anomalous VMM detection in the
cloud-based environment without having any knowledge or data from VMM by
introducing a novel machine learning-based algorithm called IAD: Indirect
Anomalous VMMs Detection. This algorithm solely uses the VM's resources
utilization data hosted on those VMMs for the anomalous VMMs detection. The
developed algorithm's accuracy was tested on four datasets comprising the
synthetic and real and compared against four other popular algorithms, which
can also be used to the described problem. It was found that the proposed IAD
algorithm has an average F1-score of 83.7% averaged across four datasets, and
also outperforms other algorithms by an average F1-score of 11\%.","['Anshul Jindal', 'Ilya Shakhat', 'Jorge Cardoso', 'Michael Gerndt', 'Vladimir Podolskiy']","['cs.DC', 'stat.ML']",2021-11-22 08:48:47+00:00
http://arxiv.org/abs/2111.11010v2,Density Ratio Estimation via Infinitesimal Classification,"Density ratio estimation (DRE) is a fundamental machine learning technique
for comparing two probability distributions. However, existing methods struggle
in high-dimensional settings, as it is difficult to accurately compare
probability distributions based on finite samples. In this work we propose
DRE-\infty, a divide-and-conquer approach to reduce DRE to a series of easier
subproblems. Inspired by Monte Carlo methods, we smoothly interpolate between
the two distributions via an infinite continuum of intermediate bridge
distributions. We then estimate the instantaneous rate of change of the bridge
distributions indexed by time (the ""time score"") -- a quantity defined
analogously to data (Stein) scores -- with a novel time score matching
objective. Crucially, the learned time scores can then be integrated to compute
the desired density ratio. In addition, we show that traditional (Stein) scores
can be used to obtain integration paths that connect regions of high density in
both distributions, improving performance in practice. Empirically, we
demonstrate that our approach performs well on downstream tasks such as mutual
information estimation and energy-based modeling on complex, high-dimensional
datasets.","['Kristy Choi', 'Chenlin Meng', 'Yang Song', 'Stefano Ermon']","['cs.LG', 'stat.ML']",2021-11-22 06:26:29+00:00
http://arxiv.org/abs/2111.10940v1,How do kernel-based sensor fusion algorithms behave under high dimensional noise?,"We study the behavior of two kernel based sensor fusion algorithms,
nonparametric canonical correlation analysis (NCCA) and alternating diffusion
(AD), under the nonnull setting that the clean datasets collected from two
sensors are modeled by a common low dimensional manifold embedded in a high
dimensional Euclidean space and the datasets are corrupted by high dimensional
noise. We establish the asymptotic limits and convergence rates for the
eigenvalues of the associated kernel matrices assuming that the sample
dimension and sample size are comparably large, where NCCA and AD are conducted
using the Gaussian kernel. It turns out that both the asymptotic limits and
convergence rates depend on the signal-to-noise ratio (SNR) of each sensor and
selected bandwidths. On one hand, we show that if NCCA and AD are directly
applied to the noisy point clouds without any sanity check, it may generate
artificial information that misleads scientists' interpretation. On the other
hand, we prove that if the bandwidths are selected adequately, both NCCA and AD
can be made robust to high dimensional noise when the SNRs are relatively
large.","['Xiucai Ding', 'Hau-Tieng Wu']","['stat.ML', 'cs.LG']",2021-11-22 01:42:28+00:00
http://arxiv.org/abs/2111.10919v2,Offline Reinforcement Learning: Fundamental Barriers for Value Function Approximation,"We consider the offline reinforcement learning problem, where the aim is to
learn a decision making policy from logged data. Offline RL -- particularly
when coupled with (value) function approximation to allow for generalization in
large or continuous state spaces -- is becoming increasingly relevant in
practice, because it avoids costly and time-consuming online data collection
and is well suited to safety-critical domains. Existing sample complexity
guarantees for offline value function approximation methods typically require
both (1) distributional assumptions (i.e., good coverage) and (2)
representational assumptions (i.e., ability to represent some or all $Q$-value
functions) stronger than what is required for supervised learning. However, the
necessity of these conditions and the fundamental limits of offline RL are not
well understood in spite of decades of research. This led Chen and Jiang (2019)
to conjecture that concentrability (the most standard notion of coverage) and
realizability (the weakest representation condition) alone are not sufficient
for sample-efficient offline RL. We resolve this conjecture in the positive by
proving that in general, even if both concentrability and realizability are
satisfied, any algorithm requires sample complexity polynomial in the size of
the state space to learn a non-trivial policy.
  Our results show that sample-efficient offline reinforcement learning
requires either restrictive coverage conditions or representation conditions
that go beyond supervised learning, and highlight a phenomenon called
over-coverage which serves as a fundamental barrier for offline value function
approximation methods. A consequence of our results for reinforcement learning
with linear function approximation is that the separation between online and
offline RL can be arbitrarily large, even in constant dimension.","['Dylan J. Foster', 'Akshay Krishnamurthy', 'David Simchi-Levi', 'Yunzong Xu']","['cs.LG', 'stat.ML']",2021-11-21 23:22:37+00:00
http://arxiv.org/abs/2111.10853v1,Decorrelated Variable Importance,"Because of the widespread use of black box prediction methods such as random
forests and neural nets, there is renewed interest in developing methods for
quantifying variable importance as part of the broader goal of interpretable
prediction. A popular approach is to define a variable importance parameter -
known as LOCO (Leave Out COvariates) - based on dropping covariates from a
regression model. This is essentially a nonparametric version of R-squared.
This parameter is very general and can be estimated nonparametrically, but it
can be hard to interpret because it is affected by correlation between
covariates. We propose a method for mitigating the effect of correlation by
defining a modified version of LOCO. This new parameter is difficult to
estimate nonparametrically, but we show how to estimate it using semiparametric
models.","['Isabella Verdinelli', 'Larry Wasserman']","['stat.ME', 'stat.ML', '62G08']",2021-11-21 16:31:36+00:00
http://arxiv.org/abs/2111.10846v1,Jointly Dynamic Topic Model for Recognition of Lead-lag Relationship in Two Text Corpora,"Topic evolution modeling has received significant attentions in recent
decades. Although various topic evolution models have been proposed, most
studies focus on the single document corpus. However in practice, we can easily
access data from multiple sources and also observe relationships between them.
Then it is of great interest to recognize the relationship between multiple
text corpora and further utilize this relationship to improve topic modeling.
In this work, we focus on a special type of relationship between two text
corpora, which we define as the ""lead-lag relationship"". This relationship
characterizes the phenomenon that one text corpus would influence the topics to
be discussed in the other text corpus in the future. To discover the lead-lag
relationship, we propose a jointly dynamic topic model and also develop an
embedding extension to address the modeling problem of large-scale text corpus.
With the recognized lead-lag relationship, the similarities of the two text
corpora can be figured out and the quality of topic learning in both corpora
can be improved. We numerically investigate the performance of the jointly
dynamic topic modeling approach using synthetic data. Finally, we apply the
proposed model on two text corpora consisting of statistical papers and the
graduation theses. Results show the proposed model can well recognize the
lead-lag relationship between the two corpora, and the specific and shared
topic patterns in the two corpora are also discovered.","['Yandi Zhu', 'Xiaoling Lu', 'Jingya Hong', 'Feifei Wang']","['cs.CL', 'stat.ME', 'stat.ML']",2021-11-21 15:53:15+00:00
http://arxiv.org/abs/2111.10806v1,A Data-Driven Line Search Rule for Support Recovery in High-dimensional Data Analysis,"In this work, we consider the algorithm to the (nonlinear) regression
problems with $\ell_0$ penalty. The existing algorithms for $\ell_0$ based
optimization problem are often carried out with a fixed step size, and the
selection of an appropriate step size depends on the restricted strong
convexity and smoothness for the loss function, hence it is difficult to
compute in practical calculation. In sprite of the ideas of support detection
and root finding \cite{HJK2020}, we proposes a novel and efficient data-driven
line search rule to adaptively determine the appropriate step size. We prove
the $\ell_2$ error bound to the proposed algorithm without much restrictions
for the cost functional. A large number of numerical comparisons with
state-of-the-art algorithms in linear and logistic regression problems show the
stability, effectiveness and superiority of the proposed algorithms.","['Peili Li', 'Yuling Jiao', 'Xiliang Lu', 'Lican Kang']","['stat.ML', 'cs.LG', 'stat.CO']",2021-11-21 12:18:18+00:00
http://arxiv.org/abs/2111.10734v4,Deep Probability Estimation,"Reliable probability estimation is of crucial importance in many real-world
applications where there is inherent (aleatoric) uncertainty.
Probability-estimation models are trained on observed outcomes (e.g. whether it
has rained or not, or whether a patient has died or not), because the
ground-truth probabilities of the events of interest are typically unknown. The
problem is therefore analogous to binary classification, with the difference
that the objective is to estimate probabilities rather than predicting the
specific outcome. This work investigates probability estimation from
high-dimensional data using deep neural networks. There exist several methods
to improve the probabilities generated by these models but they mostly focus on
model (epistemic) uncertainty. For problems with inherent uncertainty, it is
challenging to evaluate performance without access to ground-truth
probabilities. To address this, we build a synthetic dataset to study and
compare different computable metrics. We evaluate existing methods on the
synthetic data as well as on three real-world probability estimation tasks, all
of which involve inherent uncertainty: precipitation forecasting from radar
images, predicting cancer patient survival from histopathology images, and
predicting car crashes from dashcam videos. We also give a theoretical analysis
of a model for high-dimensional probability estimation which reproduces several
of the phenomena evinced in our experiments. Finally, we propose a new method
for probability estimation using neural networks, which modifies the training
process to promote output probabilities that are consistent with empirical
probabilities computed from the data. The method outperforms existing
approaches on most metrics on the simulated as well as real-world data.","['Sheng Liu', 'Aakash Kaku', 'Weicheng Zhu', 'Matan Leibovich', 'Sreyas Mohan', 'Boyang Yu', 'Haoxiang Huang', 'Laure Zanna', 'Narges Razavian', 'Jonathan Niles-Weed', 'Carlos Fernandez-Granda']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2021-11-21 03:55:50+00:00
http://arxiv.org/abs/2111.10722v2,A Deterministic Sampling Method via Maximum Mean Discrepancy Flow with Adaptive Kernel,"We propose a novel deterministic sampling method to approximate a target
distribution $\rho^*$ by minimizing the kernel discrepancy, also known as the
Maximum Mean Discrepancy (MMD). By employing the general \emph{energetic
variational inference} framework (Wang et al., 2021), we convert the problem of
minimizing MMD to solving a dynamic ODE system of the particles. We adopt the
implicit Euler numerical scheme to solve the ODE systems. This leads to a
proximal minimization problem in each iteration of updating the particles,
which can be solved by optimization algorithms such as L-BFGS. The proposed
method is named EVI-MMD. To overcome the long-existing issue of bandwidth
selection of the Gaussian kernel, we propose a novel way to specify the
bandwidth dynamically. Through comprehensive numerical studies, we have shown
the proposed adaptive bandwidth significantly improves the EVI-MMD. We use the
EVI-MMD algorithm to solve two types of sampling problems. In the first type,
the target distribution is given by a fully specified density function. The
second type is a ""two-sample problem"", where only training data are available.
The EVI-MMD method is used as a generative learning model to generate new
samples that follow the same distribution as the training data. With the
recommended settings of the tuning parameters, we show that the proposed
EVI-MMD method outperforms some existing methods for both types of problems.","['Yindong Chen', 'Yiwei Wang', 'Lulu Kang', 'Chun Liu']","['stat.ML', 'cs.LG', 'stat.CO']",2021-11-21 03:09:07+00:00
http://arxiv.org/abs/2111.10708v1,PAC-Learning Uniform Ergodic Communicative Networks,"This work addressed the problem of learning a network with communication
between vertices. The communication between vertices is presented in the form
of perturbation on the measure. We studied the scenario where samples are drawn
from a uniform ergodic Random Graph Process (RGPs for short), which provides a
natural mathematical context for the problem of interest. For the binary
classification problem, the result we obtained gives uniform learn-ability as
the worst-case theoretical limits. We introduced the structural Rademacher
complexity, which naturally fused into the VC theory to upperbound the first
moment. With the martingale method and Marton's coupling, we establish the tail
bound for uniform convergence and give consistency guarantee for empirical risk
minimizer. The technique used in this work to obtain high probability bounds is
of independent interest to other mixing processes with and without network
structure.",['Yihan He'],"['stat.ML', 'cs.LG']",2021-11-21 01:08:43+00:00
http://arxiv.org/abs/2111.10510v9,Bayesian Learning via Neural Schrödinger-Föllmer Flows,"In this work we explore a new framework for approximate Bayesian inference in
large datasets based on stochastic control (i.e. Schr\""odinger bridges). We
advocate stochastic control as a finite time and low variance alternative to
popular steady-state methods such as stochastic gradient Langevin dynamics
(SGLD). Furthermore, we discuss and adapt the existing theoretical guarantees
of this framework and establish connections to already existing VI routines in
SDE-based models.","['Francisco Vargas', 'Andrius Ovsianas', 'David Fernandes', 'Mark Girolami', 'Neil D. Lawrence', 'Nikolas Nüsken']","['stat.ML', 'cs.LG']",2021-11-20 03:51:18+00:00
http://arxiv.org/abs/2111.10476v2,Towards Return Parity in Markov Decision Processes,"Algorithmic decisions made by machine learning models in high-stakes domains
may have lasting impacts over time. However, naive applications of standard
fairness criterion in static settings over temporal domains may lead to delayed
and adverse effects. To understand the dynamics of performance disparity, we
study a fairness problem in Markov decision processes (MDPs). Specifically, we
propose return parity, a fairness notion that requires MDPs from different
demographic groups that share the same state and action spaces to achieve
approximately the same expected time-discounted rewards. We first provide a
decomposition theorem for return disparity, which decomposes the return
disparity of any two MDPs sharing the same state and action spaces into the
distance between group-wise reward functions, the discrepancy of group
policies, and the discrepancy between state visitation distributions induced by
the group policies. Motivated by our decomposition theorem, we propose
algorithms to mitigate return disparity via learning a shared group policy with
state visitation distributional alignment using integral probability metrics.
We conduct experiments to corroborate our results, showing that the proposed
algorithm can successfully close the disparity gap while maintaining the
performance of policies on two real-world recommender system benchmark
datasets.","['Jianfeng Chi', 'Jian Shen', 'Xinyi Dai', 'Weinan Zhang', 'Yuan Tian', 'Han Zhao']","['cs.LG', 'cs.AI', 'cs.CY', 'stat.ML']",2021-11-19 23:25:38+00:00
http://arxiv.org/abs/2111.10461v1,Gaussian Process Inference Using Mini-batch Stochastic Gradient Descent: Convergence Guarantees and Empirical Benefits,"Stochastic gradient descent (SGD) and its variants have established
themselves as the go-to algorithms for large-scale machine learning problems
with independent samples due to their generalization performance and intrinsic
computational advantage. However, the fact that the stochastic gradient is a
biased estimator of the full gradient with correlated samples has led to the
lack of theoretical understanding of how SGD behaves under correlated settings
and hindered its use in such cases. In this paper, we focus on hyperparameter
estimation for the Gaussian process (GP) and take a step forward towards
breaking the barrier by proving minibatch SGD converges to a critical point of
the full log-likelihood loss function, and recovers model hyperparameters with
rate $O(\frac{1}{K})$ for $K$ iterations, up to a statistical error term
depending on the minibatch size. Our theoretical guarantees hold provided that
the kernel functions exhibit exponential or polynomial eigendecay which is
satisfied by a wide range of kernels commonly used in GPs. Numerical studies on
both simulated and real datasets demonstrate that minibatch SGD has better
generalization over state-of-the-art GP methods while reducing the
computational burden and opening a new, previously unexplored, data size regime
for GPs.","['Hao Chen', 'Lili Zheng', 'Raed Al Kontar', 'Garvesh Raskutti']","['stat.ML', 'cs.LG']",2021-11-19 22:28:47+00:00
http://arxiv.org/abs/2111.10364v3,Generalized Decision Transformer for Offline Hindsight Information Matching,"How to extract as much learning signal from each trajectory data has been a
key problem in reinforcement learning (RL), where sample inefficiency has posed
serious challenges for practical applications. Recent works have shown that
using expressive policy function approximators and conditioning on future
trajectory information -- such as future states in hindsight experience replay
or returns-to-go in Decision Transformer (DT) -- enables efficient learning of
multi-task policies, where at times online RL is fully replaced by offline
behavioral cloning, e.g. sequence modeling. We demonstrate that all these
approaches are doing hindsight information matching (HIM) -- training policies
that can output the rest of trajectory that matches some statistics of future
state information. We present Generalized Decision Transformer (GDT) for
solving any HIM problem, and show how different choices for the feature
function and the anti-causal aggregator not only recover DT as a special case,
but also lead to novel Categorical DT (CDT) and Bi-directional DT (BDT) for
matching different statistics of the future. For evaluating CDT and BDT, we
define offline multi-task state-marginal matching (SMM) and imitation learning
(IL) as two generic HIM problems, propose a Wasserstein distance loss as a
metric for both, and empirically study them on MuJoCo continuous control
benchmarks. CDT, which simply replaces anti-causal summation with anti-causal
binning in DT, enables the first effective offline multi-task SMM algorithm
that generalizes well to unseen and even synthetic multi-modal state-feature
distributions. BDT, which uses an anti-causal second transformer as the
aggregator, can learn to model any statistics of the future and outperforms DT
variants in offline multi-task IL. Our generalized formulations from HIM and
GDT greatly expand the role of powerful sequence modeling architectures in
modern RL.","['Hiroki Furuta', 'Yutaka Matsuo', 'Shixiang Shane Gu']","['cs.LG', 'cs.AI', 'stat.ML']",2021-11-19 18:56:13+00:00
http://arxiv.org/abs/2111.10329v1,Physics-enhanced Neural Networks in the Small Data Regime,"Identifying the dynamics of physical systems requires a machine learning
model that can assimilate observational data, but also incorporate the laws of
physics. Neural Networks based on physical principles such as the Hamiltonian
or Lagrangian NNs have recently shown promising results in generating
extrapolative predictions and accurately representing the system's dynamics. We
show that by additionally considering the actual energy level as a
regularization term during training and thus using physical information as
inductive bias, the results can be further improved. Especially in the case
where only small amounts of data are available, these improvements can
significantly enhance the predictive capability. We apply the proposed
regularization term to a Hamiltonian Neural Network (HNN) and Constrained
Hamiltonian Neural Network (CHHN) for a single and double pendulum, generate
predictions under unseen initial conditions and report significant gains in
predictive accuracy.","['Jonas Eichelsdörfer', 'Sebastian Kaltenbach', 'Phaedon-Stelios Koutsourelakis']","['stat.ML', 'cs.LG', 'physics.comp-ph']",2021-11-19 17:21:14+00:00
http://arxiv.org/abs/2111.10275v4,Composite Goodness-of-fit Tests with Kernels,"Model misspecification can create significant challenges for the
implementation of probabilistic models, and this has led to development of a
range of robust methods which directly account for this issue. However, whether
these more involved methods are required will depend on whether the model is
really misspecified, and there is a lack of generally applicable methods to
answer this question. In this paper, we propose one such method. More
precisely, we propose kernel-based hypothesis tests for the challenging
composite testing problem, where we are interested in whether the data comes
from any distribution in some parametric family. Our tests make use of minimum
distance estimators based on the maximum mean discrepancy and the kernel Stein
discrepancy. They are widely applicable, including whenever the density of the
parametric model is known up to normalisation constant, or if the model takes
the form of a simulator. As our main result, we show that we are able to
estimate the parameter and conduct our test on the same data (without data
splitting), while maintaining a correct test level. Our approach is illustrated
on a range of problems, including testing for goodness-of-fit of an
unnormalised non-parametric density model, and an intractable generative model
of a biological cellular network.","['Oscar Key', 'Arthur Gretton', 'François-Xavier Briol', 'Tamara Fernandez']","['stat.ML', 'cs.LG', 'stat.ME']",2021-11-19 15:25:06+00:00
http://arxiv.org/abs/2111.10248v1,Non asymptotic bounds in asynchronous sum-weight gossip protocols,"This paper focuses on non-asymptotic diffusion time in asynchronous gossip
protocols. Asynchronous gossip protocols are designed to perform distributed
computation in a network of nodes by randomly exchanging messages on the
associated graph. To achieve consensus among nodes, a minimal number of
messages has to be exchanged. We provides a probabilistic bound to such number
for the general case. We provide a explicit formula for fully connected graphs
depending only on the number of nodes and an approximation for any graph
depending on the spectrum of the graph.","['David Picard', 'Jérôme Fellus', 'Stéphane Garnier']","['stat.ML', 'cs.LG', 'math.OC']",2021-11-19 14:39:45+00:00
http://arxiv.org/abs/2111.10192v1,An Expectation-Maximization Perspective on Federated Learning,"Federated learning describes the distributed training of models across
multiple clients while keeping the data private on-device. In this work, we
view the server-orchestrated federated learning process as a hierarchical
latent variable model where the server provides the parameters of a prior
distribution over the client-specific model parameters. We show that with
simple Gaussian priors and a hard version of the well known
Expectation-Maximization (EM) algorithm, learning in such a model corresponds
to FedAvg, the most popular algorithm for the federated learning setting. This
perspective on FedAvg unifies several recent works in the field and opens up
the possibility for extensions through different choices for the hierarchical
model. Based on this view, we further propose a variant of the hierarchical
model that employs prior distributions to promote sparsity. By similarly using
the hard-EM algorithm for learning, we obtain FedSparse, a procedure that can
learn sparse neural networks in the federated learning setting. FedSparse
reduces communication costs from client to server and vice-versa, as well as
the computational costs for inference with the sparsified network - both of
which are of great practical importance in federated learning.","['Christos Louizos', 'Matthias Reisser', 'Joseph Soriaga', 'Max Welling']","['cs.LG', 'stat.ML']",2021-11-19 12:58:59+00:00
http://arxiv.org/abs/2111.10189v3,Analysis of autocorrelation times in Neural Markov Chain Monte Carlo simulations,"We provide a deepened study of autocorrelations in Neural Markov Chain Monte
Carlo (NMCMC) simulations, a version of the traditional Metropolis algorithm
which employs neural networks to provide independent proposals. We illustrate
our ideas using the two-dimensional Ising model. We discuss several estimates
of autocorrelation times in the context of NMCMC, some inspired by analytical
results derived for the Metropolized Independent Sampler (MIS). We check their
reliability by estimating them on a small system where analytical results can
also be obtained. Based on the analytical results for MIS we propose a new loss
function and study its impact on the autocorelation times. Although, this
function's performance is a bit inferior to the traditional Kullback-Leibler
divergence, it offers two training algorithms which in some situations may be
beneficial. By studying a small, $4 \times 4$, system we gain access to the
dynamics of the training process which we visualize using several observables.
Furthermore, we quantitatively investigate the impact of imposing global
discrete symmetries of the system in the neural network training process on the
autocorrelation times. Eventually, we propose a scheme which incorporates
partial heat-bath updates which considerably improves the quality of the
training. The impact of the above enhancements is discussed for a $16 \times
16$ spin system. The summary of our findings may serve as a guidance to the
implementation of Neural Markov Chain Monte Carlo simulations for more
complicated models.","['Piotr Białas', 'Piotr Korcyl', 'Tomasz Stebel']","['cond-mat.stat-mech', 'cs.LG', 'hep-lat', 'stat.ML']",2021-11-19 12:56:56+00:00
http://arxiv.org/abs/2111.10187v1,Population based change-point detection for the identification of homozygosity islands,"In this paper, we propose a new method for offline change-point detection on
some parameters of the distribution of a random vector. We introduce a
penalized maximum likelihood approach that can be efficiently computed by a
dynamic programming algorithm or approximated by a fast greedy binary splitting
algorithm. We prove both algorithms converge almost surely to the set of
change-points under very general assumptions on the distribution and
independent sampling of the random vector. In particular, we show the
assumptions leading to the consistency of the algorithms are satisfied by
categorical and Gaussian random variables. This new approach is motivated by
the problem of identifying homozygosity islands on the genome of individuals in
a population. Our method directly tackles the issue of identification of the
homozygosity islands at the population level, without the need of analyzing
single individuals and then combining the results, as is made nowadays in
state-of-the-art approaches.","['Lucas Prates', 'Renan B Lemes', 'Tábita Hünemeier', 'Florencia Leonardi']","['stat.ME', 'stat.AP', 'stat.ML']",2021-11-19 12:53:41+00:00
http://arxiv.org/abs/2111.10178v1,Understanding Training-Data Leakage from Gradients in Neural Networks for Image Classification,"Federated learning of deep learning models for supervised tasks, e.g. image
classification and segmentation, has found many applications: for example in
human-in-the-loop tasks such as film post-production where it enables sharing
of domain expertise of human artists in an efficient and effective fashion. In
many such applications, we need to protect the training data from being leaked
when gradients are shared in the training process due to IP or privacy
concerns. Recent works have demonstrated that it is possible to reconstruct the
training data from gradients for an image-classification model when its
architecture is known. However, there is still an incomplete theoretical
understanding of the efficacy and failure of such attacks. In this paper, we
analyse the source of training-data leakage from gradients. We formulate the
problem of training data reconstruction as solving an optimisation problem
iteratively for each layer. The layer-wise objective function is primarily
defined by weights and gradients from the current layer as well as the output
from the reconstruction of the subsequent layer, but it might also involve a
'pull-back' constraint from the preceding layer. Training data can be
reconstructed when we solve the problem backward from the output of the network
through each layer. Based on this formulation, we are able to attribute the
potential leakage of the training data in a deep network to its architecture.
We also propose a metric to measure the level of security of a deep learning
model against gradient-based attacks on the training data.","['Cangxiong Chen', 'Neill D. F. Campbell']","['stat.ML', 'cs.LG']",2021-11-19 12:14:43+00:00
http://arxiv.org/abs/2111.10130v1,Fooling Adversarial Training with Inducing Noise,"Adversarial training is widely believed to be a reliable approach to improve
model robustness against adversarial attack. However, in this paper, we show
that when trained on one type of poisoned data, adversarial training can also
be fooled to have catastrophic behavior, e.g., $<1\%$ robust test accuracy with
$>90\%$ robust training accuracy on CIFAR-10 dataset. Previously, there are
other types of noise poisoned in the training data that have successfully
fooled standard training ($15.8\%$ standard test accuracy with $99.9\%$
standard training accuracy on CIFAR-10 dataset), but their poisonings can be
easily removed when adopting adversarial training. Therefore, we aim to design
a new type of inducing noise, named ADVIN, which is an irremovable poisoning of
training data. ADVIN can not only degrade the robustness of adversarial
training by a large margin, for example, from $51.7\%$ to $0.57\%$ on CIFAR-10
dataset, but also be effective for fooling standard training ($13.1\%$ standard
test accuracy with $100\%$ standard training accuracy). Additionally, ADVIN can
be applied to preventing personal data (like selfies) from being exploited
without authorization under whether standard or adversarial training.","['Zhirui Wang', 'Yifei Wang', 'Yisen Wang']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2021-11-19 09:59:28+00:00
http://arxiv.org/abs/2111.10106v1,A Large Scale Benchmark for Individual Treatment Effect Prediction and Uplift Modeling,"Individual Treatment Effect (ITE) prediction is an important area of research
in machine learning which aims at explaining and estimating the causal impact
of an action at the granular level. It represents a problem of growing interest
in multiple sectors of application such as healthcare, online advertising or
socioeconomics. To foster research on this topic we release a publicly
available collection of 13.9 million samples collected from several randomized
control trials, scaling up previously available datasets by a healthy 210x
factor. We provide details on the data collection and perform sanity checks to
validate the use of this data for causal inference tasks. First, we formalize
the task of uplift modeling (UM) that can be performed with this data, along
with the relevant evaluation metrics. Then, we propose synthetic response
surfaces and heterogeneous treatment assignment providing a general set-up for
ITE prediction. Finally, we report experiments to validate key characteristics
of the dataset leveraging its size to evaluate and compare - with high
statistical significance - a selection of baseline UM and ITE prediction
methods.","['Eustache Diemert', 'Artem Betlei', 'Christophe Renaudin', 'Massih-Reza Amini', 'Théophane Gregoir', 'Thibaud Rahier']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.AP']",2021-11-19 09:07:14+00:00
http://arxiv.org/abs/2111.10103v1,Uncertainty-aware Low-Rank Q-Matrix Estimation for Deep Reinforcement Learning,"Value estimation is one key problem in Reinforcement Learning. Albeit many
successes have been achieved by Deep Reinforcement Learning (DRL) in different
fields, the underlying structure and learning dynamics of value function,
especially with complex function approximation, are not fully understood. In
this paper, we report that decreasing rank of $Q$-matrix widely exists during
learning process across a series of continuous control tasks for different
popular algorithms. We hypothesize that the low-rank phenomenon indicates the
common learning dynamics of $Q$-matrix from stochastic high dimensional space
to smooth low dimensional space. Moreover, we reveal a positive correlation
between value matrix rank and value estimation uncertainty. Inspired by above
evidence, we propose a novel Uncertainty-Aware Low-rank Q-matrix Estimation
(UA-LQE) algorithm as a general framework to facilitate the learning of value
function. Through quantifying the uncertainty of state-action value estimation,
we selectively erase the entries of highly uncertain values in state-action
value matrix and conduct low-rank matrix reconstruction for them to recover
their values. Such a reconstruction exploits the underlying structure of value
matrix to improve the value approximation, thus leading to a more efficient
learning process of value function. In the experiments, we evaluate the
efficacy of UA-LQE in several representative OpenAI MuJoCo continuous control
tasks.","['Tong Sang', 'Hongyao Tang', 'Jianye Hao', 'Yan Zheng', 'Zhaopeng Meng']","['cs.LG', 'cs.AI', 'stat.ML']",2021-11-19 09:00:38+00:00
http://arxiv.org/abs/2111.10053v2,Unsupervised Learning Architecture for Classifying the Transient Noise of Interferometric Gravitational-wave Detectors,"In the data obtained by laser interferometric gravitational wave detectors,
transient noise with non-stationary and non-Gaussian features occurs at a high
rate. This often results in problems such as detector instability and the
hiding and/or imitation of gravitational-wave signals. This transient noise has
various characteristics in the time--frequency representation, which is
considered to be associated with environmental and instrumental origins.
Classification of transient noise can offer clues for exploring its origin and
improving the performance of the detector. One approach for accomplishing this
is supervised learning. However, in general, supervised learning requires
annotation of the training data, and there are issues with ensuring objectivity
in the classification and its corresponding new classes. By contrast,
unsupervised learning can reduce the annotation work for the training data and
ensure objectivity in the classification and its corresponding new classes. In
this study, we propose an unsupervised learning architecture for the
classification of transient noise that combines a variational autoencoder and
invariant information clustering. To evaluate the effectiveness of the proposed
architecture, we used the dataset (time--frequency two-dimensional spectrogram
images and labels) of the Laser Interferometer Gravitational-wave Observatory
(LIGO) first observation run prepared by the Gravity Spy project. The classes
provided by our proposed unsupervised learning architecture were consistent
with the labels annotated by the Gravity Spy project, which manifests the
potential for the existence of unrevealed classes.","['Yusuke Sakai', 'Yousuke Itoh', 'Piljong Jung', 'Keiko Kokeyama', 'Chihiro Kozakai', 'Katsuko T. Nakahira', 'Shoichi Oshino', 'Yutaka Shikano', 'Hirotaka Takahashi', 'Takashi Uchiyama', 'Gen Ueshima', 'Tatsuki Washimi', 'Takahiro Yamamoto', 'Takaaki Yokozawa']","['gr-qc', 'physics.data-an', 'stat.ML']",2021-11-19 05:37:06+00:00
http://arxiv.org/abs/2111.12550v3,A Worker-Task Specialization Model for Crowdsourcing: Efficient Inference and Fundamental Limits,"Crowdsourcing system has emerged as an effective platform for labeling data
with relatively low cost by using non-expert workers. Inferring correct labels
from multiple noisy answers on data, however, has been a challenging problem,
since the quality of the answers varies widely across tasks and workers. Many
existing works have assumed that there is a fixed ordering of workers in terms
of their skill levels, and focused on estimating worker skills to aggregate the
answers from workers with different weights. In practice, however, the worker
skill changes widely across tasks, especially when the tasks are heterogeneous.
In this paper, we consider a new model, called $d$-type specialization model,
in which each task and worker has its own (unknown) type and the reliability of
each worker can vary in the type of a given task and that of a worker. We allow
that the number $d$ of types can scale in the number of tasks. In this model,
we characterize the optimal sample complexity to correctly infer the labels
within any given accuracy, and propose label inference algorithms achieving the
order-wise optimal limit even when the types of tasks or those of workers are
unknown. We conduct experiments both on synthetic and real datasets, and show
that our algorithm outperforms the existing algorithms developed based on more
strict model assumptions.","['Doyeon Kim', 'Jeonghwan Lee', 'Hye Won Chung']","['cs.HC', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2021-11-19 05:32:59+00:00
http://arxiv.org/abs/2111.10021v1,Achievability and Impossibility of Exact Pairwise Ranking,"We consider the problem of recovering the rank of a set of $n$ items based on
noisy pairwise comparisons. We assume the SST class as the family of generative
models. Our analysis gave sharp information theoretic upper and lower bound for
the exact requirement, which matches exactly in the parametric limit. Our tight
analysis on the algorithm induced by the moment method gave better constant in
Minimax optimal rate than ~\citet{shah2017simple} and contribute to their open
problem. The strategy we used in this work to obtain information theoretic
bounds is based on combinatorial arguments and is of independent interest.",['Yihan He'],"['stat.ML', 'cs.LG']",2021-11-19 03:16:29+00:00
http://arxiv.org/abs/2111.10010v2,UN-AVOIDS: Unsupervised and Nonparametric Approach for Visualizing Outliers and Invariant Detection Scoring,"The visualization and detection of anomalies (outliers) are of crucial
importance to many fields, particularly cybersecurity. Several approaches have
been proposed in these fields, yet to the best of our knowledge, none of them
has fulfilled both objectives, simultaneously or cooperatively, in one coherent
framework. The visualization methods of these approaches were introduced for
explaining the output of a detection algorithm, not for data exploration that
facilitates a standalone visual detection. This is our point of departure:
UN-AVOIDS, an unsupervised and nonparametric approach for both visualization (a
human process) and detection (an algorithmic process) of outliers, that assigns
invariant anomalous scores (normalized to $[0,1]$), rather than hard
binary-decision. The main aspect of novelty of UN-AVOIDS is that it transforms
data into a new space, which is introduced in this paper as neighborhood
cumulative density function (NCDF), in which both visualization and detection
are carried out. In this space, outliers are remarkably visually
distinguishable, and therefore the anomaly scores assigned by the detection
algorithm achieved a high area under the ROC curve (AUC). We assessed UN-AVOIDS
on both simulated and two recently published cybersecurity datasets, and
compared it to three of the most successful anomaly detection methods: LOF, IF,
and FABOD. In terms of AUC, UN-AVOIDS was almost an overall winner. The article
concludes by providing a preview of new theoretical and practical avenues for
UN-AVOIDS. Among them is designing a visualization aided anomaly detection
(VAAD), a type of software that aids analysts by providing UN-AVOIDS' detection
algorithm (running in a back engine), NCDF visualization space (rendered to
plots), along with other conventional methods of visualization in the original
feature space, all of which are linked in one interactive environment.","['Waleed A. Yousef', 'Issa Traore', 'William Briguglio']","['cs.LG', 'stat.AP', 'stat.ML']",2021-11-19 02:31:06+00:00
http://arxiv.org/abs/2111.09990v1,Gaussian Determinantal Processes: a new model for directionality in data,"Determinantal point processes (a.k.a. DPPs) have recently become popular
tools for modeling the phenomenon of negative dependence, or repulsion, in
data. However, our understanding of an analogue of a classical parametric
statistical theory is rather limited for this class of models. In this work, we
investigate a parametric family of Gaussian DPPs with a clearly interpretable
effect of parametric modulation on the observed points. We show that parameter
modulation impacts the observed points by introducing directionality in their
repulsion structure, and the principal directions correspond to the directions
of maximal (i.e. the most long ranged) dependency.
  This model readily yields a novel and viable alternative to Principal
Component Analysis (PCA) as a dimension reduction tool that favors directions
along which the data is most spread out. This methodological contribution is
complemented by a statistical analysis of a spiked model similar to that
employed for covariance matrices as a framework to study PCA. These theoretical
investigations unveil intriguing questions for further examination in random
matrix theory, stochastic geometry and related topics.","['Subhro Ghosh', 'Philippe Rigollet']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG', 'math.PR', 'math.ST', 'stat.TH']",2021-11-19 00:57:33+00:00
http://arxiv.org/abs/2111.09964v2,Deep IDA: A Deep Learning Method for Integrative Discriminant Analysis of Multi-View Data with Feature Ranking -- An Application to COVID-19 severity,"COVID-19 severity is due to complications from SARS-Cov-2 but the clinical
course of the infection varies for individuals, emphasizing the need to better
understand the disease at the molecular level. We use clinical and multiple
molecular data (or views) obtained from patients with and without COVID-19 who
were (or not) admitted to the intensive care unit to shed light on COVID-19
severity. Methods for jointly associating the views and separating the COVID-19
groups (i.e., one-step methods) have focused on linear relationships. The
relationships between the views and COVID-19 patient groups, however, are too
complex to be understood solely by linear methods. Existing nonlinear one-step
methods cannot be used to identify signatures to aid in our understanding of
the complexity of the disease. We propose Deep IDA (Integrative Discriminant
Analysis) to address analytical challenges in our problem of interest. Deep IDA
learns nonlinear projections of two or more views that maximally associate the
views and separate the classes in each view, and permits feature ranking for
interpretable findings. Our applications demonstrate that Deep IDA has
competitive classification rates compared to other state-of-the-art methods and
is able to identify molecular signatures that facilitate an understanding of
COVID-19 severity.","['Jiuzhou Wang', 'Sandra E. Safo']","['stat.ML', 'cs.LG', 'stat.ME']",2021-11-18 22:44:38+00:00
http://arxiv.org/abs/2111.09933v2,Loss Functions for Discrete Contextual Pricing with Observational Data,"We study a pricing setting where each customer is offered a contextualized
price based on customer and/or product features. Often only historical sales
data are available, so we observe whether a customer purchased a product at the
price prescribed rather than the customer's true valuation. Such observational
data are influenced by historical pricing policies, which introduce
difficulties in evaluating the effectiveness of future policies. The goal of
this paper is to formulate loss functions that can be used for evaluating
pricing policies directly from observational data, rather than going through an
intermediate demand estimation stage, which may suffer from bias. To achieve
this, we adapt ideas from machine learning with corrupted labels, where we
consider each observed purchase decision as a known probabilistic
transformation of the customer's valuation. From this transformation, we derive
a class of unbiased loss functions. Within this class, we identify minimum
variance estimators and estimators robust to poor demand estimation.
Furthermore, we show that for contextual pricing, estimators popular in the
off-policy evaluation literature fall within this class of loss functions. We
offer managerial insights into scenarios under which these estimators are
effective.","['Max Biggs', 'Ruijiang Gao', 'Wei Sun']","['cs.LG', 'stat.ML']",2021-11-18 20:12:57+00:00
http://arxiv.org/abs/2111.09885v3,Rate-optimal Bayesian Simple Regret in Best Arm Identification,"We consider best arm identification in the multi-armed bandit problem.
Assuming certain continuity conditions of the prior, we characterize the rate
of the Bayesian simple regret. Differing from Bayesian regret minimization
(Lai, 1987), the leading term in the Bayesian simple regret derives from the
region where the gap between optimal and suboptimal arms is smaller than
$\sqrt{\frac{\log T}{T}}$. We propose a simple and easy-to-compute algorithm
with its leading term matching with the lower bound up to a constant factor;
simulation results support our theoretical findings.","['Junpei Komiyama', 'Kaito Ariu', 'Masahiro Kato', 'Chao Qin']","['cs.LG', 'stat.ML', 'Primary: 62L05, secondary: 62C10, 68W27']",2021-11-18 18:59:35+00:00
http://arxiv.org/abs/2111.09831v2,Causal Forecasting:Generalization Bounds for Autoregressive Models,"Despite the increasing relevance of forecasting methods, causal implications
of these algorithms remain largely unexplored. This is concerning considering
that, even under simplifying assumptions such as causal sufficiency, the
statistical risk of a model can differ significantly from its \textit{causal
risk}. Here, we study the problem of \textit{causal generalization} --
generalizing from the observational to interventional distributions -- in
forecasting. Our goal is to find answers to the question: How does the efficacy
of an autoregressive (VAR) model in predicting statistical associations compare
with its ability to predict under interventions?
  To this end, we introduce the framework of \textit{causal learning theory}
for forecasting. Using this framework, we obtain a characterization of the
difference between statistical and causal risks, which helps identify sources
of divergence between them. Under causal sufficiency, the problem of causal
generalization amounts to learning under covariate shifts, albeit with
additional structure (restriction to interventional distributions under the VAR
model). This structure allows us to obtain uniform convergence bounds on causal
generalizability for the class of VAR models. To the best of our knowledge,
this is the first work that provides theoretical guarantees for causal
generalization in the time-series setting.","['Leena Chennuru Vankadara', 'Philipp Michael Faller', 'Michaela Hardt', 'Lenon Minorics', 'Debarghya Ghoshdastidar', 'Dominik Janzing']","['stat.ML', 'cs.LG']",2021-11-18 17:56:20+00:00
http://arxiv.org/abs/2111.09790v2,MCCE: Monte Carlo sampling of realistic counterfactual explanations,"We introduce MCCE: Monte Carlo sampling of valid and realistic Counterfactual
Explanations for tabular data, a novel counterfactual explanation method that
generates on-manifold, actionable and valid counterfactuals by modeling the
joint distribution of the mutable features given the immutable features and the
decision. Unlike other on-manifold methods that tend to rely on variational
autoencoders and have strict prediction model and data requirements, MCCE
handles any type of prediction model and categorical features with more than
two levels. MCCE first models the joint distribution of the features and the
decision with an autoregressive generative model where the conditionals are
estimated using decision trees. Then, it samples a large set of observations
from this model, and finally, it removes the samples that do not obey certain
criteria. We compare MCCE with a range of state-of-the-art on-manifold
counterfactual methods using four well-known data sets and show that MCCE
outperforms these methods on all common performance metrics and speed. In
particular, including the decision in the modeling process improves the
efficiency of the method substantially.","['Annabelle Redelmeier', 'Martin Jullum', 'Kjersti Aas', 'Anders Løland']","['stat.ML', 'cs.LG']",2021-11-18 16:40:44+00:00
http://arxiv.org/abs/2111.09787v2,Near-Optimal Quantum Algorithms for Multivariate Mean Estimation,"We propose the first near-optimal quantum algorithm for estimating in
Euclidean norm the mean of a vector-valued random variable with finite mean and
covariance. Our result aims at extending the theory of multivariate
sub-Gaussian estimators to the quantum setting. Unlike classically, where any
univariate estimator can be turned into a multivariate estimator with at most a
logarithmic overhead in the dimension, no similar result can be proved in the
quantum setting. Indeed, Heinrich ruled out the existence of a quantum
advantage for the mean estimation problem when the sample complexity is smaller
than the dimension. Our main result is to show that, outside this low-precision
regime, there is a quantum estimator that outperforms any classical estimator.
Our approach is substantially more involved than in the univariate setting,
where most quantum estimators rely only on phase estimation. We exploit a
variety of additional algorithmic techniques such as amplitude amplification,
the Bernstein-Vazirani algorithm, and quantum singular value transformation.
Our analysis also uses concentration inequalities for multivariate truncated
statistics.
  We develop our quantum estimators in two different input models that showed
up in the literature before. The first one provides coherent access to the
binary representation of the random variable and it encompasses the classical
setting. In the second model, the random variable is directly encoded into the
phases of quantum registers. This model arises naturally in many quantum
algorithms but it is often incomparable to having classical samples. We adapt
our techniques to these two settings and we show that the second model is
strictly weaker for solving the mean estimation problem. Finally, we describe
several applications of our algorithms, notably in measuring the expectation
values of commuting observables and in the field of machine learning.","['Arjan Cornelissen', 'Yassine Hamoudi', 'Sofiene Jerbi']","['quant-ph', 'cs.CC', 'cs.DS', 'math.ST', 'stat.ML', 'stat.TH']",2021-11-18 16:35:32+00:00
