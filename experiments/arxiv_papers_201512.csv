id,title,abstract,authors,categories,date
http://arxiv.org/abs/1601.06911v2,Functional archetype and archetypoid analysis,"Archetype and archetypoid analysis can be extended to functional data. Each
function is represented as a mixture of actual observations (functional
archetypoids) or functional archetypes, which are a mixture of observations in
the data set. Well-known Canadian temperature data are used to illustrate the
analysis developed. Computational methods are proposed for performing these
analyses, based on the coefficients of a basis. Unlike a previous attempt to
compute functional archetypes, which was only valid for an orthogonal basis,
the proposed methodology can be used for any basis. It is computationally less
demanding than the simple approach of discretizing the functions. Multivariate
functional archetype and archetypoid analysis are also introduced and applied
in an interesting problem about the study of human development around the world
over the last 50 years. These tools can contribute to the understanding of a
functional data set, as in the multivariate case.",['Irene Epifanio'],"['stat.ME', 'stat.AP', 'stat.ML']",2016-01-26 07:34:41+00:00
http://arxiv.org/abs/1601.06750v2,A Robust UCB Scheme for Active Learning in Regression from Strategic Crowds,"We study the problem of training an accurate linear regression model by
procuring labels from multiple noisy crowd annotators, under a budget
constraint. We propose a Bayesian model for linear regression in crowdsourcing
and use variational inference for parameter estimation. To minimize the number
of labels crowdsourced from the annotators, we adopt an active learning
approach. In this specific context, we prove the equivalence of well-studied
criteria of active learning like entropy minimization and expected error
reduction. Interestingly, we observe that we can decouple the problems of
identifying an optimal unlabeled instance and identifying an annotator to label
it. We observe a useful connection between the multi-armed bandit framework and
the annotator selection in active learning. Due to the nature of the
distribution of the rewards on the arms, we use the Robust Upper Confidence
Bound (UCB) scheme with truncated empirical mean estimator to solve the
annotator selection problem. This yields provable guarantees on the regret. We
further apply our model to the scenario where annotators are strategic and
design suitable incentives to induce them to put in their best efforts.","['Divya Padmanabhan', 'Satyanath Bhat', 'Dinesh Garg', 'Shirish Shevade', 'Y. Narahari']","['cs.LG', 'stat.ML']",2016-01-25 20:14:22+00:00
http://arxiv.org/abs/1601.06680v1,Conditional distribution variability measures for causality detection,"In this paper we derive variability measures for the conditional probability
distributions of a pair of random variables, and we study its application in
the inference of causal-effect relationships. We also study the combination of
the proposed measures with standard statistical measures in the the framework
of the ChaLearn cause-effect pair challenge. The developed model obtains an AUC
score of 0.82 on the final test database and ranked second in the challenge.",['José A. R. Fonollosa'],"['stat.ML', 'cs.LG']",2016-01-25 17:14:31+00:00
http://arxiv.org/abs/1601.06651v1,Testing for Causality in Continuous Time Bayesian Network Models of High-Frequency Data,"Continuous time Bayesian networks are investigated with a special focus on
their ability to express causality. A framework is presented for doing
inference in these networks. The central contributions are a representation of
the intensity matrices for the networks and the introduction of a causality
measure. A new model for high-frequency financial data is presented. It is
calibrated to market data and by the new causality measure it performs better
than older models.","['Jonas Hallgren', 'Timo Koski']","['stat.ML', 'q-fin.TR', '60J28 (Primary), 65S99 (Secondary)']",2016-01-25 16:07:54+00:00
http://arxiv.org/abs/1601.06650v1,Time-Varying Gaussian Process Bandit Optimization,"We consider the sequential Bayesian optimization problem with bandit
feedback, adopting a formulation that allows for the reward function to vary
with time. We model the reward function using a Gaussian process whose
evolution obeys a simple Markov model. We introduce two natural extensions of
the classical Gaussian process upper confidence bound (GP-UCB) algorithm. The
first, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB,
instead forgets about old data in a smooth fashion. Our main contribution
comprises of novel regret bounds for these algorithms, providing an explicit
characterization of the trade-off between the time horizon and the rate at
which the function varies. We illustrate the performance of the algorithms on
both synthetic and real data, and we find the gradual forgetting of TV-GP-UCB
to perform favorably compared to the sharp resetting of R-GP-UCB. Moreover,
both algorithms significantly outperform classical GP-UCB, since it treats
stale and fresh data equally.","['Ilija Bogunovic', 'Jonathan Scarlett', 'Volkan Cevher']","['stat.ML', 'cs.LG']",2016-01-25 16:02:50+00:00
http://arxiv.org/abs/1601.06630v1,Bayesian Estimation of Bipartite Matchings for Record Linkage,"The bipartite record linkage task consists of merging two disparate datafiles
containing information on two overlapping sets of entities. This is non-trivial
in the absence of unique identifiers and it is important for a wide variety of
applications given that it needs to be solved whenever we have to combine
information from different sources. Most statistical techniques currently used
for record linkage are derived from a seminal paper by Fellegi and Sunter
(1969). These techniques usually assume independence in the matching statuses
of record pairs to derive estimation procedures and optimal point estimators.
We argue that this independence assumption is unreasonable and instead target a
bipartite matching between the two datafiles as our parameter of interest.
Bayesian implementations allow us to quantify uncertainty on the matching
decisions and derive a variety of point estimators using different loss
functions. We propose partial Bayes estimates that allow uncertain parts of the
bipartite matching to be left unresolved. We evaluate our approach to record
linkage using a variety of challenging scenarios and show that it outperforms
the traditional methodology. We illustrate the advantages of our methods
merging two datafiles on casualties from the civil war of El Salvador.",['Mauricio Sadinle'],"['stat.ME', 'stat.AP', 'stat.ML']",2016-01-25 14:58:41+00:00
http://arxiv.org/abs/1601.06403v5,Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An Information Theoretic Approach,"In latent Gaussian trees the pairwise correlation signs between the variables
are intrinsically unrecoverable. Such information is vital since it completely
determines the direction in which two variables are associated. In this work,
we resort to information theoretical approaches to achieve two fundamental
goals: First, we quantify the amount of information loss due to unrecoverable
sign information. Second, we show the importance of such information in
determining the maximum achievable rate region, in which the observed output
vector can be synthesized, given its probability density function. In
particular, we model the graphical model as a communication channel and propose
a new layered encoding framework to synthesize observed data using upper layer
Gaussian inputs and independent Bernoulli correlation sign inputs from each
layer. We find the achievable rate region for the rate tuples of multi-layer
latent Gaussian messages to synthesize the desired observables.","['Ali Moharrer', 'Shuangqing Wei', 'George T. Amariucai', 'Jing Deng']","['cs.IT', 'cs.CV', 'math.IT', 'stat.ML']",2016-01-24 15:59:44+00:00
http://arxiv.org/abs/1601.06259v1,Minimax Lower Bounds for Linear Independence Testing,"Linear independence testing is a fundamental information-theoretic and
statistical problem that can be posed as follows: given $n$ points
$\{(X_i,Y_i)\}^n_{i=1}$ from a $p+q$ dimensional multivariate distribution
where $X_i \in \mathbb{R}^p$ and $Y_i \in\mathbb{R}^q$, determine whether $a^T
X$ and $b^T Y$ are uncorrelated for every $a \in \mathbb{R}^p, b\in
\mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n
\to \infty$, $(p+q)/n \leq \kappa < \infty$, without sparsity assumptions). In
summary, our results imply that $n$ must be at least as large as $\sqrt
{pq}/\|\Sigma_{XY}\|_F^2$ for any procedure (test) to have non-trivial power,
where $\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also provide
some evidence that the lower bound is tight, by connections to two-sample
testing and regression in specific settings.","['Aaditya Ramdas', 'David Isenberg', 'Aarti Singh', 'Larry Wasserman']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",2016-01-23 10:20:58+00:00
http://arxiv.org/abs/1601.06207v6,Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least Squares Problem,"In this paper, we develop a Bayesian evidence maximization framework to solve
the sparse non-negative least squares (S-NNLS) problem. We introduce a family
of probability densities referred to as the Rectified Gaussian Scale Mixture
(R- GSM) to model the sparsity enforcing prior distribution for the solution.
The R-GSM prior encompasses a variety of heavy-tailed densities such as the
rectified Laplacian and rectified Student- t distributions with a proper choice
of the mixing density. We utilize the hierarchical representation induced by
the R-GSM prior and develop an evidence maximization framework based on the
Expectation-Maximization (EM) algorithm. Using the EM based method, we estimate
the hyper-parameters and obtain a point estimate for the solution. We refer to
the proposed method as rectified sparse Bayesian learning (R-SBL). We provide
four R- SBL variants that offer a range of options for computational complexity
and the quality of the E-step computation. These methods include the Markov
chain Monte Carlo EM, linear minimum mean-square-error estimation, approximate
message passing and a diagonal approximation. Using numerical experiments, we
show that the proposed R-SBL method outperforms existing S-NNLS solvers in
terms of both signal and support recovery performance, and is also very robust
against the structure of the design matrix.","['Alican Nalci', 'Igor Fedorov', 'Maher Al-Shoukairi', 'Thomas T. Liu', 'Bhaskar D. Rao']","['cs.LG', 'stat.ML']",2016-01-22 23:47:36+00:00
http://arxiv.org/abs/1601.06201v2,Universal Collaboration Strategies for Signal Detection: A Sparse Learning Approach,"This paper considers the problem of high dimensional signal detection in a
large distributed network whose nodes can collaborate with their one-hop
neighboring nodes (spatial collaboration). We assume that only a small subset
of nodes communicate with the Fusion Center (FC). We design optimal
collaboration strategies which are universal for a class of deterministic
signals. By establishing the equivalence between the collaboration strategy
design problem and sparse PCA, we solve the problem efficiently and evaluate
the impact of collaboration on detection performance.","['Prashant Khanduri', 'Bhavya Kailkhura', 'Jayaraman J. Thiagarajan', 'Pramod K. Varshney']","['cs.LG', 'stat.ML']",2016-01-22 23:15:42+00:00
http://arxiv.org/abs/1601.06116v3,A Mathematical Formalization of Hierarchical Temporal Memory's Spatial Pooler,"Hierarchical temporal memory (HTM) is an emerging machine learning algorithm,
with the potential to provide a means to perform predictions on spatiotemporal
data. The algorithm, inspired by the neocortex, currently does not have a
comprehensive mathematical framework. This work brings together all aspects of
the spatial pooler (SP), a critical learning component in HTM, under a single
unifying framework. The primary learning mechanism is explored, where a maximum
likelihood estimator for determining the degree of permanence update is
proposed. The boosting mechanisms are studied and found to be only relevant
during the initial few iterations of the network. Observations are made
relating HTM to well-known algorithms such as competitive learning and
attribute bagging. Methods are provided for using the SP for classification as
well as dimensionality reduction. Empirical evidence verifies that given the
proper parameterizations, the SP may be used for feature learning.","['James Mnatzaganian', 'Ernest Fokoué', 'Dhireesha Kudithipudi']","['stat.ML', 'cs.LG', 'q-bio.NC']",2016-01-22 19:26:16+00:00
http://arxiv.org/abs/1601.06105v1,Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs,"We propose a non-parametric anomaly detection algorithm for high dimensional
data. We first rank scores derived from nearest neighbor graphs on $n$-point
nominal training data. We then train limited complexity models to imitate these
scores based on the max-margin learning-to-rank framework. A test-point is
declared as an anomaly at $\alpha$-false alarm level if the predicted score is
in the $\alpha$-percentile. The resulting anomaly detector is shown to be
asymptotically optimal in that for any false alarm rate $\alpha$, its decision
region converges to the $\alpha$-percentile minimum volume level set of the
unknown underlying density. In addition, we test both the statistical
performance and computational efficiency of our algorithm on a number of
synthetic and real-data experiments. Our results demonstrate the superiority of
our algorithm over existing $K$-NN based anomaly detection algorithms, with
significant computational savings.","['Jonathan Root', 'Venkatesh Saligrama', 'Jing Qian']","['stat.ML', 'cs.LG']",2016-01-22 19:10:31+00:00
http://arxiv.org/abs/1601.06035v1,Recommender systems inspired by the structure of quantum theory,"Physicists use quantum models to describe the behavior of physical systems.
Quantum models owe their success to their interpretability, to their relation
to probabilistic models (quantization of classical models) and to their high
predictive power. Beyond physics, these properties are valuable in general data
science. This motivates the use of quantum models to analyze general
nonphysical datasets. Here we provide both empirical and theoretical insights
into the application of quantum models in data science. In the theoretical part
of this paper, we firstly show that quantum models can be exponentially more
efficient than probabilistic models because there exist datasets that admit
low-dimensional quantum models and only exponentially high-dimensional
probabilistic models. Secondly, we explain in what sense quantum models realize
a useful relaxation of compressed probabilistic models. Thirdly, we show that
sparse datasets admit low-dimensional quantum models and finally, we introduce
a method to compute hierarchical orderings of properties of users (e.g.,
personality traits) and items (e.g., genres of movies). In the empirical part
of the paper, we evaluate quantum models in item recommendation and observe
that the predictive power of quantum-inspired recommender systems can compete
with state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore,
we make use of the interpretability of quantum models by computing hierarchical
orderings of properties of users and items. This work establishes a connection
between data science (item recommendation), information theory (communication
complexity), mathematical programming (positive semidefinite factorizations)
and physics (quantum models).",['Cyril Stark'],"['cs.LG', 'cs.IT', 'math.IT', 'math.OC', 'quant-ph', 'stat.ML']",2016-01-22 15:09:18+00:00
http://arxiv.org/abs/1601.05936v1,Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic Modeling in Speech Recognition,"We propose to model the acoustic space of deep neural network (DNN)
class-conditional posterior probabilities as a union of low-dimensional
subspaces. To that end, the training posteriors are used for dictionary
learning and sparse coding. Sparse representation of the test posteriors using
this dictionary enables projection to the space of training data. Relying on
the fact that the intrinsic dimensions of the posterior subspaces are indeed
very small and the matrix of all posteriors belonging to a class has a very low
rank, we demonstrate how low-dimensional structures enable further enhancement
of the posteriors and rectify the spurious errors due to mismatch conditions.
The enhanced acoustic modeling method leads to improvements in continuous
speech recognition task using hybrid DNN-HMM (hidden Markov model) framework in
both clean and noisy conditions, where upto 15.4% relative reduction in word
error rate (WER) is achieved.","['Pranay Dighe', 'Gil Luyet', 'Afsaneh Asaei', 'Herve Bourlard']","['cs.CL', 'cs.LG', 'stat.ML']",2016-01-22 10:02:47+00:00
http://arxiv.org/abs/1601.05834v2,Active Sensing of Social Networks,"This paper develops an active sensing method to estimate the relative weight
(or trust) agents place on their neighbors' information in a social network.
The model used for the regression is based on the steady state equation in the
linear DeGroot model under the influence of stubborn agents, i.e., agents whose
opinions are not influenced by their neighbors. This method can be viewed as a
\emph{social RADAR}, where the stubborn agents excite the system and the latter
can be estimated through the reverberation observed from the analysis of the
agents' opinions. The social network sensing problem can be interpreted as a
blind compressed sensing problem with a sparse measurement matrix. We prove
that the network structure will be revealed when a sufficient number of
stubborn agents independently influence a number of ordinary (non-stubborn)
agents. We investigate the scenario with a deterministic or randomized DeGroot
model and propose a consistent estimator of the steady states for the latter
scenario. Simulation results on synthetic and real world networks support our
findings.","['Hoi-To Wai', 'Anna Scaglione', 'Amir Leshem']","['cs.SI', 'stat.ML']",2016-01-21 22:41:31+00:00
http://arxiv.org/abs/1601.05775v2,Local Network Community Detection with Continuous Optimization of Conductance and Weighted Kernel K-Means,"Local network community detection is the task of finding a single community
of nodes concentrated around few given seed nodes in a localized way.
Conductance is a popular objective function used in many algorithms for local
community detection. This paper studies a continuous relaxation of conductance.
We show that continuous optimization of this objective still leads to discrete
communities. We investigate the relation of conductance with weighted kernel
k-means for a single community, which leads to the introduction of a new
objective function, $\sigma$-conductance. Conductance is obtained by setting
$\sigma$ to $0$. Two algorithms, EMc and PGDc, are proposed to locally optimize
$\sigma$-conductance and automatically tune the parameter $\sigma$. They are
based on expectation maximization and projected gradient descent, respectively.
We prove locality and give performance guarantees for EMc and PGDc for a class
of dense and well separated communities centered around the seeds. Experiments
are conducted on networks with ground-truth communities, comparing to
state-of-the-art graph diffusion algorithms for conductance optimization. On
large graphs, results indicate that EMc and PGDc stay localized and produce
communities most similar to the ground, while graph diffusion algorithms
generate large communities of lower quality.","['Twan van Laarhoven', 'Elena Marchiori']","['cs.SI', 'cs.LG', 'stat.ML']",2016-01-21 20:36:59+00:00
http://arxiv.org/abs/1601.05675v1,Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning,"While the harmonic function solution performs well in many semi-supervised
learning (SSL) tasks, it is known to scale poorly with the number of samples.
Recent successful and scalable methods, such as the eigenfunction method focus
on efficiently approximating the whole spectrum of the graph Laplacian
constructed from the data. This is in contrast to various subsampling and
quantization methods proposed in the past, which may fail in preserving the
graph spectra. However, the impact of the approximation of the spectrum on the
final generalization error is either unknown, or requires strong assumptions on
the data. In this paper, we introduce Sparse-HFS, an efficient
edge-sparsification algorithm for SSL. By constructing an edge-sparse and
spectrally similar graph, we are able to leverage the approximation guarantees
of spectral sparsification methods to bound the generalization error of
Sparse-HFS. As a result, we obtain a theoretically-grounded approximation
scheme for graph-based SSL that also empirically matches the performance of
known large-scale methods.","['Daniele Calandriello', 'Alessandro Lazaric', 'Michal Valko', 'Ioannis Koutis']","['stat.ML', 'cs.LG']",2016-01-21 15:31:35+00:00
http://arxiv.org/abs/1601.05495v2,Data-driven Rank Breaking for Efficient Rank Aggregation,"Rank aggregation systems collect ordinal preferences from individuals to
produce a global ranking that represents the social preference. Rank-breaking
is a common practice to reduce the computational complexity of learning the
global ranking. The individual preferences are broken into pairwise comparisons
and applied to efficient algorithms tailored for independent paired
comparisons. However, due to the ignored dependencies in the data, naive
rank-breaking approaches can result in inconsistent estimates. The key idea to
produce accurate and consistent estimates is to treat the pairwise comparisons
unequally, depending on the topology of the collected data. In this paper, we
provide the optimal rank-breaking estimator, which not only achieves
consistency but also achieves the best error bound. This allows us to
characterize the fundamental tradeoff between accuracy and complexity. Further,
the analysis identifies how the accuracy depends on the spectral gap of a
corresponding comparison graph.","['Ashish Khetan', 'Sewoong Oh']","['cs.LG', 'stat.ML']",2016-01-21 02:39:39+00:00
http://arxiv.org/abs/1601.05285v4,Nonlinear variable selection with continuous outcome: a nonparametric incremental forward stagewise approach,"We present a method of variable selection for the sparse generalized additive
model. The method doesn't assume any specific functional form, and can select
from a large number of candidates. It takes the form of incremental forward
stagewise regression. Given no functional form is assumed, we devised an
approach termed roughening to adjust the residuals in the iterations. In
simulations, we show the new method is competitive against popular machine
learning approaches. We also demonstrate its performance using some real
datasets. The method is available as a part of the nlnet package on CRAN
https://cran.r-project.org/package=nlnet.",['Tianwei Yu'],['stat.ML'],2016-01-20 14:44:54+00:00
http://arxiv.org/abs/1601.05011v6,Non-smooth Variable Projection,"Variable projection solves structured optimization problems by completely
minimizing over a subset of the variables while iterating over the remaining
variables. Over the last 30 years, the technique has been widely used, with
empirical and theoretical results demonstrating both greater efficacy and
greater stability compared to competing approaches. Classic examples have
exploited closed-form projections and smoothness of the objective function. We
extend the approach to problems that include non-smooth terms, and where the
projection subproblems can only be solved inexactly by iterative methods. We
propose an inexact adaptive algonrithm for solving such problems and analyze
its computational complexity. Finally, we show how the theory can be used to
design methods for selected problems occurring frequently in machine-learning
and inverse problems.","['Tristan van Leeuwen', 'Aleksandr Aravkin']","['math.OC', 'stat.CO', 'stat.ML', '65K05, 65K10, 86-08']",2016-01-19 17:38:29+00:00
http://arxiv.org/abs/1601.04920v1,Understanding Deep Convolutional Networks,"Deep convolutional networks provide state of the art classifications and
regressions results over many high-dimensional problems. We review their
architecture, which scatters data with a cascade of linear filter weights and
non-linearities. A mathematical framework is introduced to analyze their
properties. Computations of invariants involve multiscale contractions, the
linearization of hierarchical symmetries, and sparse separations. Applications
are discussed.",['Stéphane Mallat'],"['stat.ML', 'cs.CV', 'cs.LG']",2016-01-19 13:40:47+00:00
http://arxiv.org/abs/1601.04800v1,Top-N Recommender System via Matrix Completion,"Top-N recommender systems have been investigated widely both in industry and
academia. However, the recommendation quality is far from satisfactory. In this
paper, we propose a simple yet promising algorithm. We fill the user-item
matrix based on a low-rank assumption and simultaneously keep the original
information. To do that, a nonconvex rank relaxation rather than the nuclear
norm is adopted to provide a better rank approximation and an efficient
optimization strategy is designed. A comprehensive set of experiments on real
datasets demonstrates that our method pushes the accuracy of Top-N
recommendation to a new level.","['Zhao Kang', 'Chong Peng', 'Qiang Cheng']","['cs.IR', 'cs.AI', 'cs.LG', 'stat.ML']",2016-01-19 04:48:42+00:00
http://arxiv.org/abs/1601.04738v3,Sub-Sampled Newton Methods II: Local Convergence Rates,"Many data-fitting applications require the solution of an optimization
problem involving a sum of large number of functions of high dimensional
parameter. Here, we consider the problem of minimizing a sum of $n$ functions
over a convex constraint set $\mathcal{X} \subseteq \mathbb{R}^{p}$ where both
$n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$
can offer great amount of computational efficiency.
  Within the context of second order methods, we first give quantitative local
convergence results for variants of Newton's method where the Hessian is
uniformly sub-sampled. Using random matrix concentration inequalities, one can
sub-sample in a way that the curvature information is preserved. Using such
sub-sampling strategy, we establish locally Q-linear and Q-superlinear
convergence rates. We also give additional convergence results for when the
sub-sampled Hessian is regularized by modifying its spectrum or Levenberg-type
regularization.
  Finally, in addition to Hessian sub-sampling, we consider sub-sampling the
gradient as way to further reduce the computational complexity per iteration.
We use approximate matrix multiplication results from randomized numerical
linear algebra (RandNLA) to obtain the proper sampling strategy and we
establish locally R-linear convergence rates. In such a setting, we also show
that a very aggressive sample size increase results in a R-superlinearly
convergent algorithm.
  While the sample size depends on the condition number of the problem, our
convergence rates are problem-independent, i.e., they do not depend on the
quantities related to the problem. Hence, our analysis here can be used to
complement the results of our basic framework from the companion paper, [38],
by exploring algorithmic trade-offs that are important in practice.","['Farbod Roosta-Khorasani', 'Michael W. Mahoney']","['math.OC', 'cs.LG', 'stat.ML']",2016-01-18 22:04:32+00:00
http://arxiv.org/abs/1601.04737v3,Sub-Sampled Newton Methods I: Globally Convergent Algorithms,"Large scale optimization problems are ubiquitous in machine learning and data
analysis and there is a plethora of algorithms for solving such problems. Many
of these algorithms employ sub-sampling, as a way to either speed up the
computations and/or to implicitly implement a form of statistical
regularization. In this paper, we consider second-order iterative optimization
algorithms and we provide bounds on the convergence of the variants of Newton's
method that incorporate uniform sub-sampling as a means to estimate the
gradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Our
algorithms are global and are guaranteed to converge from any initial iterate.
  Using random matrix concentration inequalities, one can sub-sample the
Hessian to preserve the curvature information. Our first algorithm incorporates
Hessian sub-sampling while using the full gradient. We also give additional
convergence results for when the sub-sampled Hessian is regularized by
modifying its spectrum or ridge-type regularization. Next, in addition to
Hessian sub-sampling, we also consider sub-sampling the gradient as a way to
further reduce the computational complexity per iteration. We use approximate
matrix multiplication results from randomized numerical linear algebra to
obtain the proper sampling strategy. In all these algorithms, computing the
update boils down to solving a large scale linear system, which can be
computationally expensive. As a remedy, for all of our algorithms, we also give
global convergence results for the case of inexact updates where such linear
system is solved only approximately.
  This paper has a more advanced companion paper, [42], in which we demonstrate
that, by doing a finer-grained analysis, we can get problem-independent bounds
for local convergence of these algorithms and explore trade-offs to improve
upon the basic results of the present paper.","['Farbod Roosta-Khorasani', 'Michael W. Mahoney']","['math.OC', 'cs.LG', 'stat.ML']",2016-01-18 21:59:21+00:00
http://arxiv.org/abs/1601.04674v2,A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure,"For many complex diseases, there is a wide variety of ways in which an
individual can manifest the disease. The challenge of personalized medicine is
to develop tools that can accurately predict the trajectory of an individual's
disease, which can in turn enable clinicians to optimize treatments. We
represent an individual's disease trajectory as a continuous-valued
continuous-time function describing the severity of the disease over time. We
propose a hierarchical latent variable model that individualizes predictions of
disease trajectories. This model shares statistical strength across
observations at different resolutions--the population, subpopulation and the
individual level. We describe an algorithm for learning population and
subpopulation parameters offline, and an online procedure for dynamically
learning individual-specific parameters. Finally, we validate our model on the
task of predicting the course of interstitial lung disease, a leading cause of
death among patients with the autoimmune disease scleroderma. We compare our
approach against state-of-the-art and demonstrate significant improvements in
predictive accuracy.","['Peter Schulam', 'Suchi Saria']",['stat.ML'],2016-01-18 20:01:16+00:00
http://arxiv.org/abs/1601.04650v2,Statistical Mechanics of High-Dimensional Inference,"To model modern large-scale datasets, we need efficient algorithms to infer a
set of $P$ unknown model parameters from $N$ noisy measurements. What are
fundamental limits on the accuracy of parameter inference, given finite
signal-to-noise ratios, limited measurements, prior information, and
computational tractability requirements? How can we combine prior information
with measurements to achieve these limits? Classical statistics gives incisive
answers to these questions as the measurement density $\alpha =
\frac{N}{P}\rightarrow \infty$. However, these classical results are not
relevant to modern high-dimensional inference problems, which instead occur at
finite $\alpha$. We formulate and analyze high-dimensional inference as a
problem in the statistical physics of quenched disorder. Our analysis uncovers
fundamental limits on the accuracy of inference in high dimensions, and reveals
that widely cherished inference algorithms like maximum likelihood (ML) and
maximum-a posteriori (MAP) inference cannot achieve these limits. We further
find optimal, computationally tractable algorithms that can achieve these
limits. Intriguingly, in high dimensions, these optimal algorithms become
computationally simpler than MAP and ML, while still outperforming them. For
example, such optimal algorithms can lead to as much as a 20% reduction in the
amount of data to achieve the same performance relative to MAP. Moreover, our
analysis reveals simple relations between optimal high dimensional inference
and low dimensional scalar Bayesian inference, insights into the nature of
generalization and predictive power in high dimensions, information theoretic
limits on compressed sensing, phase transitions in quadratic inference, and
connections to central mathematical objects in convex optimization theory and
random matrix theory.","['Madhu Advani', 'Surya Ganguli']","['stat.ML', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'math.ST', 'q-bio.QM', 'stat.TH']",2016-01-18 18:38:35+00:00
http://arxiv.org/abs/1601.04621v2,Probabilistic Inference of Twitter Users' Age based on What They Follow,"Twitter provides an open and rich source of data for studying human behaviour
at scale and is widely used in social and network sciences. However, a major
criticism of Twitter data is that demographic information is largely absent.
Enhancing Twitter data with user ages would advance our ability to study social
network structures, information flows and the spread of contagions. Approaches
toward age detection of Twitter users typically focus on specific properties of
tweets, e.g., linguistic features, which are language dependent. In this paper,
we devise a language-independent methodology for determining the age of Twitter
users from data that is native to the Twitter ecosystem. The key idea is to use
a Bayesian framework to generalise ground-truth age information from a few
Twitter users to the entire network based on what/whom they follow. Our
approach scales to inferring the age of 700 million Twitter accounts with high
accuracy.","['Benjamin Paul Chamberlain', 'Clive Humby', 'Marc Peter Deisenroth']","['cs.SI', 'stat.ML']",2016-01-18 17:40:56+00:00
http://arxiv.org/abs/1601.04586v4,Sparse Convex Clustering,"Convex clustering, a convex relaxation of k-means clustering and hierarchical
clustering, has drawn recent attentions since it nicely addresses the
instability issue of traditional nonconvex clustering methods. Although its
computational and statistical properties have been recently studied, the
performance of convex clustering has not yet been investigated in the
high-dimensional clustering scenario, where the data contains a large number of
features and many of them carry no information about the clustering structure.
In this paper, we demonstrate that the performance of convex clustering could
be distorted when the uninformative features are included in the clustering. To
overcome it, we introduce a new clustering method, referred to as Sparse Convex
Clustering, to simultaneously cluster observations and conduct feature
selection. The key idea is to formulate convex clustering in a form of
regularization, with an adaptive group-lasso penalty term on cluster centers.
In order to optimally balance the tradeoff between the cluster fitting and
sparsity, a tuning criterion based on clustering stability is developed. In
theory, we provide an unbiased estimator for the degrees of freedom of the
proposed sparse convex clustering method. Finally, the effectiveness of the
sparse convex clustering is examined through a variety of numerical experiments
and a real data application.","['Binhuan Wang', 'Yilong Zhang', 'Will Wei Sun', 'Yixin Fang']","['stat.ME', 'cs.LG', 'stat.ML']",2016-01-18 16:03:35+00:00
http://arxiv.org/abs/1601.04549v1,Incremental Semiparametric Inverse Dynamics Learning,"This paper presents a novel approach for incremental semiparametric inverse
dynamics learning. In particular, we consider the mixture of two approaches:
Parametric modeling based on rigid body dynamics equations and nonparametric
modeling based on incremental kernel methods, with no prior information on the
mechanical properties of the system. This yields to an incremental
semiparametric approach, leveraging the advantages of both the parametric and
nonparametric models. We validate the proposed technique learning the dynamics
of one arm of the iCub humanoid robot.","['Raffaello Camoriano', 'Silvio Traversaro', 'Lorenzo Rosasco', 'Giorgio Metta', 'Francesco Nori']","['stat.ML', 'cs.LG', 'cs.RO']",2016-01-18 14:54:37+00:00
http://arxiv.org/abs/1601.04530v2,Domain based classification,"The majority of traditional classification ru les minimizing the expected
probability of error (0-1 loss) are inappropriate if the class probability
distributions are ill-defined or impossible to estimate. We argue that in such
cases class domains should be used instead of class distributions or densities
to construct a reliable decision function. Proposals are presented for some
evaluation criteria and classifier learning schemes, illustrated by an example.","['Robert P. W. Duin', 'Elzbieta Pekalska']","['stat.ML', 'cs.LG']",2016-01-18 14:31:12+00:00
http://arxiv.org/abs/1601.04451v1,Zero-error dissimilarity based classifiers,"We consider general non-Euclidean distance measures between real world
objects that need to be classified. It is assumed that objects are represented
by distances to other objects only. Conditions for zero-error dissimilarity
based classifiers are derived. Additional conditions are given under which the
zero-error decision boundary is a continues function of the distances to a
finite set of training samples. These conditions affect the objects as well as
the distance measure used. It is argued that they can be met in practice.","['Robert P. W. Duin', 'Elzbieta Pekalska']","['stat.ML', 'cs.LG']",2016-01-18 10:12:15+00:00
http://arxiv.org/abs/1601.04366v2,Learning the kernel matrix via predictive low-rank approximations,"Efficient and accurate low-rank approximations of multiple data sources are
essential in the era of big data. The scaling of kernel-based learning
algorithms to large datasets is limited by the O(n^2) computation and storage
complexity of the full kernel matrix, which is required by most of the recent
kernel learning algorithms.
  We present the Mklaren algorithm to approximate multiple kernel matrices
learn a regression model, which is entirely based on geometrical concepts. The
algorithm does not require access to full kernel matrices yet it accounts for
the correlations between all kernels. It uses Incomplete Cholesky
decomposition, where pivot selection is based on least-angle regression in the
combined, low-dimensional feature space. The algorithm has linear complexity in
the number of data points and kernels. When explicit feature space induced by
the kernel can be constructed, a mapping from the dual to the primal Ridge
regression weights is used for model interpretation.
  The Mklaren algorithm was tested on eight standard regression datasets. It
outperforms contemporary kernel matrix approximation approaches when learning
with multiple kernels. It identifies relevant kernels, achieving highest
explained variance than other multiple kernel learning methods for the same
number of iterations. Test accuracy, equivalent to the one using full kernel
matrices, was achieved with at significantly lower approximation ranks. A
difference in run times of two orders of magnitude was observed when either the
number of samples or kernels exceeds 3000.","['Martin Stražar', 'Tomaž Curk']","['cs.LG', 'stat.ML']",2016-01-17 23:31:37+00:00
http://arxiv.org/abs/1601.04251v1,On-line Bayesian System Identification,"We consider an on-line system identification setting, in which new data
become available at given time steps. In order to meet real-time estimation
requirements, we propose a tailored Bayesian system identification procedure,
in which the hyper-parameters are still updated through Marginal Likelihood
maximization, but after only one iteration of a suitable iterative optimization
algorithm. Both gradient methods and the EM algorithm are considered for the
Marginal Likelihood optimization. We compare this ""1-step"" procedure with the
standard one, in which the optimization method is run until convergence to a
local minimum. The experiments we perform confirm the effectiveness of the
approach we propose.","['Diego Romeres', 'Giulia Prando', 'Gianluigi Pillonetto', 'Alessandro Chiuso']","['cs.SY', 'cs.LG', 'stat.AP', 'stat.ML']",2016-01-17 05:20:19+00:00
http://arxiv.org/abs/1601.04126v1,Engineering Safety in Machine Learning,"Machine learning algorithms are increasingly influencing our decisions and
interacting with us in all parts of our daily lives. Therefore, just like for
power plants, highways, and myriad other engineered sociotechnical systems, we
must consider the safety of systems involving machine learning. In this paper,
we first discuss the definition of safety in terms of risk, epistemic
uncertainty, and the harm incurred by unwanted outcomes. Then we examine
dimensions, such as the choice of cost function and the appropriateness of
minimizing the empirical average training cost, along which certain real-world
applications may not be completely amenable to the foundational principle of
modern statistical machine learning: empirical risk minimization. In
particular, we note an emerging dichotomy of applications: ones in which safety
is important and risk minimization is not the complete story (we name these
Type A applications), and ones in which safety is not so critical and risk
minimization is sufficient (we name these Type B applications). Finally, we
discuss how four different strategies for achieving safety in engineering
(inherently safe design, safety reserves, safe fail, and procedural safeguards)
can be mapped to the machine learning context through interpretability and
causality of predictive models, objectives beyond expected prediction accuracy,
human involvement for labeling difficult or rare examples, and user experience
design of software.",['Kush R. Varshney'],"['stat.ML', 'cs.AI', 'cs.CY', 'cs.LG']",2016-01-16 05:46:57+00:00
http://arxiv.org/abs/1601.04033v1,Faster Asynchronous SGD,"Asynchronous distributed stochastic gradient descent methods have trouble
converging because of stale gradients. A gradient update sent to a parameter
server by a client is stale if the parameters used to calculate that gradient
have since been updated on the server. Approaches have been proposed to
circumvent this problem that quantify staleness in terms of the number of
elapsed updates. In this work, we propose a novel method that quantifies
staleness in terms of moving averages of gradient statistics. We show that this
method outperforms previous methods with respect to convergence speed and
scalability to many clients. We also discuss how an extension to this method
can be used to dramatically reduce bandwidth costs in a distributed training
context. In particular, our method allows reduction of total bandwidth usage by
a factor of 5 with little impact on cost convergence. We also describe (and
link to) a software library that we have used to simulate these algorithms
deterministically on a single machine.",['Augustus Odena'],"['stat.ML', 'cs.LG']",2016-01-15 19:03:47+00:00
http://arxiv.org/abs/1601.03958v2,Real-Time Community Detection in Large Social Networks on a Laptop,"For a broad range of research, governmental and commercial applications it is
important to understand the allegiances, communities and structure of key
players in society. One promising direction towards extracting this information
is to exploit the rich relational data in digital social networks (the social
graph). As social media data sets are very large, most approaches make use of
distributed computing systems for this purpose. Distributing graph processing
requires solving many difficult engineering problems, which has lead some
researchers to look at single-machine solutions that are faster and easier to
maintain. In this article, we present a single-machine real-time system for
large-scale graph processing that allows analysts to interactively explore
graph structures. The key idea is that the aggregate actions of large numbers
of users can be compressed into a data structure that encapsulates user
similarities while being robust to noise and queryable in real-time. We achieve
single machine real-time performance by compressing the neighbourhood of each
vertex using minhash signatures and facilitate rapid queries through Locality
Sensitive Hashing. These techniques reduce query times from hours using
industrial desktop machines operating on the full graph to milliseconds on
standard laptops. Our method allows exploration of strongly associated regions
(i.e. communities) of large graphs in real-time on a laptop. It has been
deployed in software that is actively used by social network analysts and
offers another channel for media owners to monetise their data, helping them to
continue to provide free services that are valued by billions of people
globally.","['Benjamin Paul Chamberlain', 'Josh Levy-Kramer', 'Clive Humby', 'Marc Peter Deisenroth']","['cs.SI', 'stat.ML']",2016-01-15 15:16:00+00:00
http://arxiv.org/abs/1601.03945v1,Improved graph-based SFA: Information preservation complements the slowness principle,"Slow feature analysis (SFA) is an unsupervised-learning algorithm that
extracts slowly varying features from a multi-dimensional time series. A
supervised extension to SFA for classification and regression is graph-based
SFA (GSFA). GSFA is based on the preservation of similarities, which are
specified by a graph structure derived from the labels. It has been shown that
hierarchical GSFA (HGSFA) allows learning from images and other
high-dimensional data. The feature space spanned by HGSFA is complex due to the
composition of the nonlinearities of the nodes in the network. However, we show
that the network discards useful information prematurely before it reaches
higher nodes, resulting in suboptimal global slowness and an under-exploited
feature space.
  To counteract these problems, we propose an extension called hierarchical
information-preserving GSFA (HiGSFA), where information preservation
complements the slowness-maximization goal. We build a 10-layer HiGSFA network
to estimate human age from facial photographs of the MORPH-II database,
achieving a mean absolute error of 3.50 years, improving the state-of-the-art
performance. HiGSFA and HGSFA support multiple-labels and offer a rich feature
space, feed-forward training, and linear complexity in the number of samples
and dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature
slowness, estimation accuracy and input reconstruction, giving rise to a
promising hierarchical supervised-learning approach.","['Alberto N. Escalante-B.', 'Laurenz Wiskott']","['cs.CV', 'cs.LG', 'stat.ML']",2016-01-15 15:00:20+00:00
http://arxiv.org/abs/1601.03822v2,On the consistency of inversion-free parameter estimation for Gaussian random fields,"Gaussian random fields are a powerful tool for modeling environmental
processes. For high dimensional samples, classical approaches for estimating
the covariance parameters require highly challenging and massive computations,
such as the evaluation of the Cholesky factorization or solving linear systems.
Recently, Anitescu, Chen and Stein \cite{M.Anitescu} proposed a fast and
scalable algorithm which does not need such burdensome computations. The main
focus of this article is to study the asymptotic behavior of the algorithm of
Anitescu et al. (ACS) for regular and irregular grids in the increasing domain
setting. Consistency, minimax optimality and asymptotic normality of this
algorithm are proved under mild differentiability conditions on the covariance
function. Despite the fact that ACS's method entails a non-concave
maximization, our results hold for any stationary point of the objective
function. A numerical study is presented to evaluate the efficiency of this
algorithm for large data sets.","['Hossein Keshavarz', 'Clayton Scott', 'XuanLong Nguyen']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2016-01-15 05:47:29+00:00
http://arxiv.org/abs/1601.03764v6,"Linear Algebraic Structure of Word Senses, with Applications to Polysemy","Word embeddings are ubiquitous in NLP and information retrieval, but it is
unclear what they represent when the word is polysemous. Here it is shown that
multiple word senses reside in linear superposition within the word embedding
and simple sparse coding can recover vectors that approximately capture the
senses. The success of our approach, which applies to several embedding
methods, is mathematically explained using a variant of the random walk on
discourses model (Arora et al., 2016). A novel aspect of our technique is that
each extracted word sense is accompanied by one of about 2000 ""discourse atoms""
that gives a succinct description of which other words co-occur with that word
sense. Discourse atoms can be of independent interest, and make the method
potentially more useful. Empirical tests are used to verify and support the
theory.","['Sanjeev Arora', 'Yuanzhi Li', 'Yingyu Liang', 'Tengyu Ma', 'Andrej Risteski']","['cs.CL', 'cs.LG', 'stat.ML']",2016-01-14 22:02:18+00:00
http://arxiv.org/abs/1601.03124v1,Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization,"Dyadic Data Prediction (DDP) is an important problem in many research areas.
This paper develops a novel fully Bayesian nonparametric framework which
integrates two popular and complementary approaches, discrete mixed membership
modeling and continuous latent factor modeling into a unified Heterogeneous
Matrix Factorization~(HeMF) model, which can predict the unobserved dyadics
accurately. The HeMF can determine the number of communities automatically and
exploit the latent linear structure for each bicluster efficiently. We propose
a Variational Bayesian method to estimate the parameters and missing data. We
further develop a novel online learning approach for Variational inference and
use it for the online learning of HeMF, which can efficiently cope with the
important large-scale DDP problem. We evaluate the performance of our method on
the EachMoive, MovieLens and Netflix Prize collaborative filtering datasets.
The experiment shows that, our model outperforms state-of-the-art methods on
all benchmarks. Compared with Stochastic Gradient Method (SGD), our online
learning approach achieves significant improvement on the estimation accuracy
and robustness.","['Guangyong Chen', 'Fengyuan Zhu', 'Pheng Ann Heng']","['cs.LG', 'stat.ML']",2016-01-13 04:20:09+00:00
http://arxiv.org/abs/1601.03117v1,Blind Image Denoising via Dependent Dirichlet Process Tree,"Most existing image denoising approaches assumed the noise to be homogeneous
white Gaussian distributed with known intensity. However, in real noisy images,
the noise models are usually unknown beforehand and can be much more complex.
This paper addresses this problem and proposes a novel blind image denoising
algorithm to recover the clean image from noisy one with the unknown noise
model. To model the empirical noise of an image, our method introduces the
mixture of Gaussian distribution, which is flexible enough to approximate
different continuous distributions. The problem of blind image denoising is
reformulated as a learning problem. The procedure is to first build a two-layer
structural model for noisy patches and consider the clean ones as latent
variable. To control the complexity of the noisy patch model, this work
proposes a novel Bayesian nonparametric prior called ""Dependent Dirichlet
Process Tree"" to build the model. Then, this study derives a variational
inference algorithm to estimate model parameters and recover clean patches. We
apply our method on synthesis and real noisy images with different noise
models. Comparing with previous approaches, ours achieves better performance.
The experimental results indicate the efficiency of the proposed algorithm to
cope with practical image denoising tasks.","['Fengyuan Zhu', 'Guangyong Chen', 'Jianye Hao', 'Pheng-Ann Heng']","['cs.CV', 'stat.ML']",2016-01-13 02:44:36+00:00
http://arxiv.org/abs/1601.03073v1,Infomax strategies for an optimal balance between exploration and exploitation,"Proper balance between exploitation and exploration is what makes good
decisions, which achieve high rewards like payoff or evolutionary fitness. The
Infomax principle postulates that maximization of information directs the
function of diverse systems, from living systems to artificial neural networks.
While specific applications are successful, the validity of information as a
proxy for reward remains unclear. Here, we consider the multi-armed bandit
decision problem, which features arms (slot-machines) of unknown probabilities
of success and a player trying to maximize cumulative payoff by choosing the
sequence of arms to play. We show that an Infomax strategy (Info-p) which
optimally gathers information on the highest mean reward among the arms
saturates known optimal bounds and compares favorably to existing policies. The
highest mean reward considered by Info-p is not the quantity actually needed
for the choice of the arm to play, yet it allows for optimal tradeoffs between
exploration and exploitation.","['Gautam Reddy', 'Antonio Celani', 'Massimo Vergassola']","['cs.LG', 'cs.IT', 'math.IT', 'physics.data-an', 'q-bio.PE', 'stat.ML']",2016-01-12 21:50:03+00:00
http://arxiv.org/abs/1601.02789v1,Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking,"Re-speaking is a mechanism for obtaining high quality subtitles for use in
live broadcast and other public events. Because it relies on humans performing
the actual re-speaking, the task of estimating the quality of the results is
non-trivial. Most organisations rely on humans to perform the actual quality
assessment, but purely automatic methods have been developed for other similar
problems, like Machine Translation. This paper will try to compare several of
these methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will
then be matched to the human-derived NER metric, commonly used in re-speaking.","['Krzysztof Wołk', 'Danijel Koržinek']","['cs.CL', 'stat.AP', 'stat.ML']",2016-01-12 10:06:52+00:00
http://arxiv.org/abs/1601.02748v1,Robust Lineage Reconstruction from High-Dimensional Single-Cell Data,"Single-cell gene expression data provide invaluable resources for systematic
characterization of cellular hierarchy in multi-cellular organisms. However,
cell lineage reconstruction is still often associated with significant
uncertainty due to technological constraints. Such uncertainties have not been
taken into account in current methods. We present ECLAIR, a novel computational
method for the statistical inference of cell lineage relationships from
single-cell gene expression data. ECLAIR uses an ensemble approach to improve
the robustness of lineage predictions, and provides a quantitative estimate of
the uncertainty of lineage branchings. We show that the application of ECLAIR
to published datasets successfully reconstructs known lineage relationships and
significantly improves the robustness of predictions. In conclusion, ECLAIR is
a powerful bioinformatics tool for single-cell data analysis. It can be used
for robust lineage reconstruction with quantitative estimate of prediction
accuracy.","['Gregory Giecold', 'Eugenio Marco', 'Lorenzo Trippa', 'Guo-Cheng Yuan']","['q-bio.QM', 'stat.AP', 'stat.CO', 'stat.ML']",2016-01-12 07:01:55+00:00
http://arxiv.org/abs/1601.02733v1,Deep Learning of Part-based Representation of Data Using Sparse Autoencoders with Nonnegativity Constraints,"We demonstrate a new deep learning autoencoder network, trained by a
nonnegativity constraint algorithm (NCAE), that learns features which show
part-based representation of data. The learning algorithm is based on
constraining negative weights. The performance of the algorithm is assessed
based on decomposing data into parts and its prediction performance is tested
on three standard image data sets and one text dataset. The results indicate
that the nonnegativity constraint forces the autoencoder to learn features that
amount to a part-based representation of data, while improving sparsity and
reconstruction quality in comparison with the traditional sparse autoencoder
and Nonnegative Matrix Factorization. It is also shown that this newly acquired
representation improves the prediction performance of a deep neural network.","['Ehsan Hosseini-Asl', 'Jacek M. Zurada', 'Olfa Nasraoui']","['cs.LG', 'stat.ML']",2016-01-12 05:33:03+00:00
http://arxiv.org/abs/1601.02712v1,IRLS and Slime Mold: Equivalence and Convergence,"In this paper we present a connection between two dynamical systems arising
in entirely different contexts: one in signal processing and the other in
biology. The first is the famous Iteratively Reweighted Least Squares (IRLS)
algorithm used in compressed sensing and sparse recovery while the second is
the dynamics of a slime mold (Physarum polycephalum). Both of these dynamics
are geared towards finding a minimum l1-norm solution in an affine subspace.
Despite its simplicity the convergence of the IRLS method has been shown only
for a certain regularization of it and remains an important open problem. Our
first result shows that the two dynamics are projections of the same dynamical
system in higher dimensions. As a consequence, and building on the recent work
on Physarum dynamics, we are able to prove convergence and obtain complexity
bounds for a damped version of the IRLS algorithm.","['Damian Straszak', 'Nisheeth K. Vishnoi']","['cs.DS', 'cs.ET', 'cs.IT', 'math.IT', 'math.NA', 'math.OC', 'stat.ML']",2016-01-12 02:24:18+00:00
http://arxiv.org/abs/1601.02522v5,Stationary signal processing on graphs,"Graphs are a central tool in machine learning and information processing as
they allow to conveniently capture the structure of complex datasets. In this
context, it is of high importance to develop flexible models of signals defined
over graphs or networks. In this paper, we generalize the traditional concept
of wide sense stationarity to signals defined over the vertices of arbitrary
weighted undirected graphs. We show that stationarity is expressed through the
graph localization operator reminiscent of translation. We prove that
stationary graph signals are characterized by a well-defined Power Spectral
Density that can be efficiently estimated even for large graphs. We leverage
this new concept to derive Wiener-type estimation procedures of noisy and
partially observed signals and illustrate the performance of this new model for
denoising and regression.","['Nathanaël Perraudin', 'Pierre Vandergheynst']","['cs.DS', 'stat.AP', 'stat.ML']",2016-01-11 16:58:45+00:00
http://arxiv.org/abs/1601.02513v1,How to learn a graph from smooth signals,"We propose a framework that learns the graph structure underlying a set of
smooth signals. Given $X\in\mathbb{R}^{m\times n}$ whose rows reside on the
vertices of an unknown graph, we learn the edge weights
$w\in\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that
$\text{tr}{X^\top LX}$ is small. We show that the problem is a weighted
$\ell$-1 minimization that leads to naturally sparse solutions. We point out
how known graph learning or construction techniques fall within our framework
and propose a new model that performs better than the state of the art in many
settings. We present efficient, scalable primal-dual based algorithms for both
our model and the previous state of the art, and evaluate their performance on
artificial and real data.",['Vassilis Kalofolias'],"['stat.ML', 'cs.LG', 'physics.data-an']",2016-01-11 16:23:30+00:00
http://arxiv.org/abs/1601.02300v1,Temporal Multinomial Mixture for Instance-Oriented Evolutionary Clustering,"Evolutionary clustering aims at capturing the temporal evolution of clusters.
This issue is particularly important in the context of social media data that
are naturally temporally driven. In this paper, we propose a new probabilistic
model-based evolutionary clustering technique. The Temporal Multinomial Mixture
(TMM) is an extension of classical mixture model that optimizes feature
co-occurrences in the trade-off with temporal smoothness. Our model is
evaluated for two recent case studies on opinion aggregation over time. We
compare four different probabilistic clustering models and we show the
superiority of our proposal in the task of instance-oriented clustering.","['Young-Min Kim', 'Julien Velcin', 'Stéphane Bonnevay', 'Marian-Andrei Rizoiu']","['cs.IR', 'cs.LG', 'stat.ML']",2016-01-11 02:06:36+00:00
http://arxiv.org/abs/1601.02257v1,A Sufficient Statistics Construction of Bayesian Nonparametric Exponential Family Conjugate Models,"Conjugate pairs of distributions over infinite dimensional spaces are
prominent in statistical learning theory, particularly due to the widespread
adoption of Bayesian nonparametric methodologies for a host of models and
applications. Much of the existing literature in the learning community focuses
on processes possessing some form of computationally tractable conjugacy as is
the case for the beta and gamma processes (and, via normalization, the
Dirichlet process). For these processes, proofs of conjugacy and requisite
derivation of explicit computational formulae for posterior density parameters
are idiosyncratic to the stochastic process in question. As such, Bayesian
Nonparametric models are currently available for a limited number of conjugate
pairs, e.g. the Dirichlet-multinomial and beta-Bernoulli process pairs. In each
of these above cases the likelihood process belongs to the class of discrete
exponential family distributions. The exclusion of continuous likelihood
distributions from the known cases of Bayesian Nonparametric Conjugate models
stands as a disparity in the researcher's toolbox.
  In this paper we first address the problem of obtaining a general
construction of prior distributions over infinite dimensional spaces possessing
distributional properties amenable to conjugacy. Second, we bridge the divide
between the discrete and continuous likelihoods by illustrating a canonical
construction for stochastic processes whose Levy measure densities are from
positive exponential families, and then demonstrate that these processes in
fact form the prior, likelihood, and posterior in a conjugate family. Our
canonical construction subsumes known computational formulae for posterior
density parameters in the cases where the likelihood is from a discrete
distribution belonging to an exponential family.","['Robert Finn', 'Brian Kulis']","['cs.LG', 'stat.ML']",2016-01-10 19:23:27+00:00
http://arxiv.org/abs/1601.02213v1,On Clustering Time Series Using Euclidean Distance and Pearson Correlation,"For time series comparisons, it has often been observed that z-score
normalized Euclidean distances far outperform the unnormalized variant. In this
paper we show that a z-score normalized, squared Euclidean Distance is, in
fact, equal to a distance based on Pearson Correlation. This has profound
impact on many distance-based classification or clustering methods. In addition
to this theoretically sound result we also show that the often used k-Means
algorithm formally needs a mod ification to keep the interpretation as Pearson
correlation strictly valid. Experimental results demonstrate that in many cases
the standard k-Means algorithm generally produces the same results.","['Michael R. Berthold', 'Frank Höppner']","['cs.LG', 'cs.AI', 'stat.ML']",2016-01-10 13:17:46+00:00
http://arxiv.org/abs/1601.02068v6,On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models,"We derive computationally tractable methods to select a small subset of
experiment settings from a large pool of given design points. The primary focus
is on linear regression models, while the technique extends to generalized
linear models and Delta's method (estimating functions of linear regression
models) as well. The algorithms are based on a continuous relaxation of an
otherwise intractable combinatorial optimization problem, with sampling or
greedy procedures as post-processing steps. Formal approximation guarantees are
established for both algorithms, and numerical results on both synthetic and
real-world data confirm the effectiveness of the proposed methods.","['Yining Wang', 'Adams Wei Yu', 'Aarti Singh']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2016-01-09 03:05:31+00:00
http://arxiv.org/abs/1601.01972v2,Cox process representation and inference for stochastic reaction-diffusion processes,"Complex behaviour in many systems arises from the stochastic interactions of
spatially distributed particles or agents. Stochastic reaction-diffusion
processes are widely used to model such behaviour in disciplines ranging from
biology to the social sciences, yet they are notoriously difficult to simulate
and calibrate to observational data. Here we use ideas from statistical physics
and machine learning to provide a solution to the inverse problem of learning a
stochastic reaction-diffusion process from data. Our solution relies on a
non-trivial connection between stochastic reaction-diffusion processes and
spatio-temporal Cox processes, a well-studied class of models from
computational statistics. This connection leads to an efficient and flexible
algorithm for parameter inference and model selection. Our approach shows
excellent accuracy on numeric and real data examples from systems biology and
epidemiology. Our work provides both insights into spatio-temporal stochastic
systems, and a practical solution to a long-standing problem in computational
modelling.","['David Schnoerr', 'Ramon Grima', 'Guido Sanguinetti']","['cond-mat.stat-mech', 'math.ST', 'physics.data-an', 'q-bio.QM', 'stat.ML', 'stat.TH']",2016-01-08 18:44:03+00:00
http://arxiv.org/abs/1601.01966v1,Numerical Coding of Nominal Data,"In this paper, a novel approach for coding nominal data is proposed. For the
given nominal data, a rank in a form of complex number is assigned. The
proposed method does not lose any information about the attribute and brings
other properties previously unknown. The approach based on these knew
properties can been used for classification. The analyzed example shows that
classification with the use of coded nominal data or both numerical as well as
coded nominal data is more effective than the classification, which uses only
numerical data.","['Zenon Gniazdowski', 'Michal Grabowski']",['stat.ML'],2016-01-08 18:24:52+00:00
http://arxiv.org/abs/1601.01944v1,Nonparametric semi-supervised learning of class proportions,"The problem of developing binary classifiers from positive and unlabeled data
is often encountered in machine learning. A common requirement in this setting
is to approximate posterior probabilities of positive and negative classes for
a previously unseen data point. This problem can be decomposed into two steps:
(i) the development of accurate predictors that discriminate between positive
and unlabeled data, and (ii) the accurate estimation of the prior probabilities
of positive and negative examples. In this work we primarily focus on the
latter subproblem. We study nonparametric class prior estimation and formulate
this problem as an estimation of mixing proportions in two-component mixture
models, given a sample from one of the components and another sample from the
mixture itself. We show that estimation of mixing proportions is generally
ill-defined and propose a canonical form to obtain identifiability while
maintaining the flexibility to model any distribution. We use insights from
this theory to elucidate the optimization surface of the class priors and
propose an algorithm for estimating them. To address the problems of
high-dimensional density estimation, we provide practical transformations to
low-dimensional spaces that preserve class priors. Finally, we demonstrate the
efficacy of our method on univariate and multivariate data.","['Shantanu Jain', 'Martha White', 'Michael W. Trosset', 'Predrag Radivojac']","['stat.ML', 'cs.LG']",2016-01-08 16:56:55+00:00
http://arxiv.org/abs/1601.01892v2,Song Recommendation with Non-Negative Matrix Factorization and Graph Total Variation,"This work formulates a novel song recommender system as a matrix completion
problem that benefits from collaborative filtering through Non-negative Matrix
Factorization (NMF) and content-based filtering via total variation (TV) on
graphs. The graphs encode both playlist proximity information and song
similarity, using a rich combination of audio, meta-data and social features.
As we demonstrate, our hybrid recommendation system is very versatile and
incorporates several well-known methods while outperforming them. Particularly,
we show on real-world data that our model overcomes w.r.t. two evaluation
metrics the recommendation of models solely based on low-rank information,
graph-based information or a combination of both.","['Kirell Benzi', 'Vassilis Kalofolias', 'Xavier Bresson', 'Pierre Vandergheynst']","['stat.ML', 'cs.IR', 'cs.LG', 'physics.data-an']",2016-01-08 14:59:34+00:00
http://arxiv.org/abs/1601.01653v1,Large Collection of Diverse Gene Set Search Queries Recapitulate Known Protein-Protein Interactions and Gene-Gene Functional Associations,"Popular online enrichment analysis tools from the field of molecular systems
biology provide users with the ability to submit their experimental results as
gene sets for individual analysis. Such queries are kept private, and have
never before been considered as a resource for integrative analysis. By
harnessing gene set query submissions from thousands of users, we aim to
discover biological knowledge beyond the scope of an individual study. In this
work, we investigated a large collection of gene sets submitted to the tool
Enrichr by thousands of users. Based on co-occurrence, we constructed a global
gene-gene association network. We interpret this inferred network as providing
a summary of the structure present in this crowdsourced gene set library, and
show that this network recapitulates known protein-protein interactions and
functional associations between genes. This finding implies that this network
also offers predictive value. Furthermore, we visualize this gene-gene
association network using a new edge-pruning algorithm that retains both the
local and global structures of large-scale networks. Our ability to make
predictions for currently unknown gene associations, that may not be captured
by individual researchers and data sources, is a demonstration of the potential
of harnessing collective knowledge from users of popular tools in the field of
molecular systems biology.","[""Avi Ma'ayan"", 'Neil R. Clark']","['q-bio.MN', 'cs.AI', 'cs.SI', 'q-bio.GN', 'stat.ML']",2016-01-07 20:06:18+00:00
http://arxiv.org/abs/1601.01544v1,State Space representation of non-stationary Gaussian Processes,"The state space (SS) representation of Gaussian processes (GP) has recently
gained a lot of interest. The main reason is that it allows to compute GPs
based inferences in O(n), where $n$ is the number of observations. This
implementation makes GPs suitable for Big Data. For this reason, it is
important to provide a SS representation of the most important kernels used in
machine learning. The aim of this paper is to show how to exploit the transient
behaviour of SS models to map non-stationary kernels to SS models.","['Alessio Benavoli', 'Marco Zaffalon']","['cs.LG', 'stat.ML']",2016-01-07 14:25:07+00:00
http://arxiv.org/abs/1601.01507v3,Fast Kronecker product kernel methods via generalized vec trick,"Kronecker product kernel provides the standard approach in the kernel methods
literature for learning from graph data, where edges are labeled and both start
and end vertices have their own feature representations. The methods allow
generalization to such new edges, whose start and end vertices do not appear in
the training data, a setting known as zero-shot or zero-data learning. Such a
setting occurs in numerous applications, including drug-target interaction
prediction, collaborative filtering and information retrieval. Efficient
training algorithms based on the so-called vec trick, that makes use of the
special structure of the Kronecker product, are known for the case where the
training data is a complete bipartite graph. In this work we generalize these
results to non-complete training graphs. This allows us to derive a general
framework for training Kronecker product kernel methods, as specific examples
we implement Kronecker ridge regression and support vector machine algorithms.
Experimental results demonstrate that the proposed approach leads to accurate
models, while allowing order of magnitude improvements in training and
prediction time.","['Antti Airola', 'Tapio Pahikkala']","['stat.ML', 'cs.LG']",2016-01-07 12:25:53+00:00
http://arxiv.org/abs/1602.07960v1,Measuring and Discovering Correlations in Large Data Sets,"In this paper, a class of statistics named ART (the alternant recursive
topology statistics) is proposed to measure the properties of correlation
between two variables. A wide range of bi-variable correlations both linear and
nonlinear can be evaluated by ART efficiently and equitably even if nothing is
known about the specific types of those relationships. ART compensates the
disadvantages of Reshef's model in which no polynomial time precise algorithm
exists and the ""local random"" phenomenon can not be identified. As a class of
nonparametric exploration statistics, ART is applied for analyzing a dataset of
10 American classical indexes, as a result, lots of bi-variable correlations
are discovered.","['Lijue Liu', 'Ming Li', 'Sha Wen']","['stat.ME', 'stat.ML']",2016-01-07 09:58:16+00:00
http://arxiv.org/abs/1601.01411v1,Learning Kernels for Structured Prediction using Polynomial Kernel Transformations,"Learning the kernel functions used in kernel methods has been a vastly
explored area in machine learning. It is now widely accepted that to obtain
'good' performance, learning a kernel function is the key challenge. In this
work we focus on learning kernel representations for structured regression. We
propose use of polynomials expansion of kernels, referred to as Schoenberg
transforms and Gegenbaur transforms, which arise from the seminal result of
Schoenberg (1938). These kernels can be thought of as polynomial combination of
input features in a high dimensional reproducing kernel Hilbert space (RKHS).
We learn kernels over input and output for structured data, such that,
dependency between kernel features is maximized. We use Hilbert-Schmidt
Independence Criterion (HSIC) to measure this. We also give an efficient,
matrix decomposition-based algorithm to learn these kernel transformations, and
demonstrate state-of-the-art results on several real-world datasets.","['Chetan Tonde', 'Ahmed Elgammal']","['cs.LG', 'stat.ML']",2016-01-07 06:37:48+00:00
http://arxiv.org/abs/1601.01345v4,An Oracle Inequality for Quasi-Bayesian Non-Negative Matrix Factorization,"The aim of this paper is to provide some theoretical understanding of
quasi-Bayesian aggregation methods non-negative matrix factorization. We derive
an oracle inequality for an aggregated estimator. This result holds for a very
general class of prior distributions and shows how the prior affects the rate
of convergence.","['Pierre Alquier', 'Benjamin Guedj']","['stat.ML', 'math.ST', 'stat.TH']",2016-01-06 22:28:50+00:00
http://arxiv.org/abs/1601.01190v3,On Bayesian index policies for sequential resource allocation,"This paper is about index policies for minimizing (frequentist) regret in a
stochastic multi-armed bandit model, inspired by a Bayesian view on the
problem. Our main contribution is to prove that the Bayes-UCB algorithm, which
relies on quantiles of posterior distributions, is asymptotically optimal when
the reward distributions belong to a one-dimensional exponential family, for a
large class of prior distributions. We also show that the Bayesian literature
gives new insight on what kind of exploration rates could be used in
frequentist, UCB-type algorithms. Indeed, approximations of the Bayesian
optimal solution or the Finite Horizon Gittins indices provide a justification
for the kl-UCB+ and kl-UCB-H+ algorithms, whose asymptotic optimality is also
established.",['Emilie Kaufmann'],['stat.ML'],2016-01-06 14:24:59+00:00
http://arxiv.org/abs/1601.01142v1,Streaming Gibbs Sampling for LDA Model,"Streaming variational Bayes (SVB) is successful in learning LDA models in an
online manner. However previous attempts toward developing online Monte-Carlo
methods for LDA have little success, often by having much worse perplexity than
their batch counterparts. We present a streaming Gibbs sampling (SGS) method,
an online extension of the collapsed Gibbs sampling (CGS). Our empirical study
shows that SGS can reach similar perplexity as CGS, much better than SVB. Our
distributed version of SGS, DSGS, is much more scalable than SVB mainly because
the updates' communication complexity is small.","['Yang Gao', 'Jianfei Chen', 'Jun Zhu']","['cs.LG', 'stat.ML']",2016-01-06 11:15:45+00:00
http://arxiv.org/abs/1601.01073v1,"Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism","We propose multi-way, multilingual neural machine translation. The proposed
approach enables a single neural translation model to translate between
multiple languages, with a number of parameters that grows only linearly with
the number of languages. This is made possible by having a single attention
mechanism that is shared across all language pairs. We train the proposed
multi-way, multilingual model on ten language pairs from WMT'15 simultaneously
and observe clear performance improvements over models trained on only one
language pair. In particular, we observe that the proposed model significantly
improves the translation quality of low-resource language pairs.","['Orhan Firat', 'Kyunghyun Cho', 'Yoshua Bengio']","['cs.CL', 'stat.ML']",2016-01-06 04:00:50+00:00
http://arxiv.org/abs/1601.00955v1,Optimally Pruning Decision Tree Ensembles With Feature Cost,"We consider the problem of learning decision rules for prediction with
feature budget constraint. In particular, we are interested in pruning an
ensemble of decision trees to reduce expected feature cost while maintaining
high prediction accuracy for any test example. We propose a novel 0-1 integer
program formulation for ensemble pruning. Our pruning formulation is general -
it takes any ensemble of decision trees as input. By explicitly accounting for
feature-sharing across trees together with accuracy/cost trade-off, our method
is able to significantly reduce feature cost by pruning subtrees that introduce
more loss in terms of feature cost than benefit in terms of prediction accuracy
gain. Theoretically, we prove that a linear programming relaxation produces the
exact solution of the original integer program. This allows us to use efficient
convex optimization tools to obtain an optimally pruned ensemble for any given
budget. Empirically, we see that our pruning algorithm significantly improves
the performance of the state of the art ensemble method BudgetRF.","['Feng Nan', 'Joseph Wang', 'Venkatesh Saligrama']","['stat.ML', 'cs.LG']",2016-01-05 20:38:35+00:00
http://arxiv.org/abs/1601.00909v1,The high-conductance state enables neural sampling in networks of LIF neurons,"The apparent stochasticity of in-vivo neural circuits has long been
hypothesized to represent a signature of ongoing stochastic inference in the
brain. More recently, a theoretical framework for neural sampling has been
proposed, which explains how sample-based inference can be performed by
networks of spiking neurons. One particular requirement of this approach is
that the neural response function closely follows a logistic curve.
  Analytical approaches to calculating neural response functions have been the
subject of many theoretical studies. In order to make the problem tractable,
particular assumptions regarding the neural or synaptic parameters are usually
made. However, biologically significant activity regimes exist which are not
covered by these approaches: Under strong synaptic bombardment, as is often the
case in cortex, the neuron is shifted into a high-conductance state (HCS)
characterized by a small membrane time constant. In this regime, synaptic time
constants and refractory periods dominate membrane dynamics.
  The core idea of our approach is to separately consider two different ""modes""
of spiking dynamics: burst spiking and transient quiescence, in which the
neuron does not spike for longer periods. We treat the former by propagating
the PDF of the effective membrane potential from spike to spike within a burst,
while using a diffusion approximation for the latter. We find that our
prediction of the neural response function closely matches simulation data.
Moreover, in the HCS scenario, we show that the neural response function
becomes symmetric and can be well approximated by a logistic function, thereby
providing the correct dynamics in order to perform neural sampling. We hereby
provide not only a normative framework for Bayesian inference in cortex, but
also powerful applications of low-power, accelerated neuromorphic systems to
relevant machine learning tasks.","['Mihai A. Petrovici', 'Ilja Bytschok', 'Johannes Bill', 'Johannes Schemmel', 'Karlheinz Meier']","['q-bio.NC', 'cond-mat.dis-nn', 'cs.NE', 'physics.bio-ph', 'stat.ML']",2016-01-05 17:15:37+00:00
http://arxiv.org/abs/1601.00863v3,"Coordinate Friendly Structures, Algorithms and Applications","This paper focuses on coordinate update methods, which are useful for solving
problems involving large or high-dimensional datasets. They decompose a problem
into simple subproblems, where each updates one, or a small block of, variables
while fixing others. These methods can deal with linear and nonlinear mappings,
smooth and nonsmooth functions, as well as convex and nonconvex problems. In
addition, they are easy to parallelize.
  The great performance of coordinate update methods depends on solving simple
sub-problems. To derive simple subproblems for several new classes of
applications, this paper systematically studies coordinate-friendly operators
that perform low-cost coordinate updates.
  Based on the discovered coordinate friendly operators, as well as operator
splitting techniques, we obtain new coordinate update algorithms for a variety
of problems in machine learning, image processing, as well as sub-areas of
optimization. Several problems are treated with coordinate update for the first
time in history. The obtained algorithms are scalable to large instances
through parallel and even asynchronous computing. We present numerical examples
to illustrate how effective these algorithms are.","['Zhimin Peng', 'Tianyu Wu', 'Yangyang Xu', 'Ming Yan', 'Wotao Yin']","['math.OC', 'cs.CE', 'cs.DC', 'math.NA', 'stat.ML']",2016-01-05 15:33:05+00:00
http://arxiv.org/abs/1601.00670v9,Variational Inference: A Review for Statisticians,"One of the core problems of modern statistics is to approximate
difficult-to-compute probability densities. This problem is especially
important in Bayesian statistics, which frames all inference about unknown
quantities as a calculation involving the posterior density. In this paper, we
review variational inference (VI), a method from machine learning that
approximates probability densities through optimization. VI has been used in
many applications and tends to be faster than classical methods, such as Markov
chain Monte Carlo sampling. The idea behind VI is to first posit a family of
densities and then to find the member of that family which is close to the
target. Closeness is measured by Kullback-Leibler divergence. We review the
ideas behind mean-field variational inference, discuss the special case of VI
applied to exponential family models, present a full example with a Bayesian
mixture of Gaussians, and derive a variant that uses stochastic optimization to
scale up to massive data. We discuss modern research in VI and highlight
important open problems. VI is powerful, but it is not yet well understood. Our
hope in writing this paper is to catalyze statistical research on this class of
algorithms.","['David M. Blei', 'Alp Kucukelbir', 'Jon D. McAuliffe']","['stat.CO', 'cs.LG', 'stat.ML']",2016-01-04 21:28:04+00:00
http://arxiv.org/abs/1601.00595v2,Robust Non-linear Regression: A Greedy Approach Employing Kernels with Application to Image Denoising,"We consider the task of robust non-linear regression in the presence of both
inlier noise and outliers. Assuming that the unknown non-linear function
belongs to a Reproducing Kernel Hilbert Space (RKHS), our goal is to estimate
the set of the associated unknown parameters. Due to the presence of outliers,
common techniques such as the Kernel Ridge Regression (KRR) or the Support
Vector Regression (SVR) turn out to be inadequate. Instead, we employ sparse
modeling arguments to explicitly model and estimate the outliers, adopting a
greedy approach. The proposed robust scheme, i.e., Kernel Greedy Algorithm for
Robust Denoising (KGARD), is inspired by the classical Orthogonal Matching
Pursuit (OMP) algorithm. Specifically, the proposed method alternates between a
KRR task and an OMP-like selection step. Theoretical results concerning the
identification of the outliers are provided. Moreover, KGARD is compared
against other cutting edge methods, where its performance is evaluated via a
set of experiments with various types of noise. Finally, the proposed robust
estimation framework is applied to the task of image denoising, and its
enhanced performance in the presence of outliers is demonstrated.","['George Papageorgiou', 'Pantelis Bouboulis', 'Sergios Theodoridis']","['cs.LG', 'stat.ML']",2016-01-04 18:11:45+00:00
http://arxiv.org/abs/1601.00504v1,Learning relationships between data obtained independently,"The aim of this paper is to provide a new method for learning the
relationships between data that have been obtained independently. Unlike
existing methods like matching, the proposed technique does not require any
contextual information, provided that the dependency between the variables of
interest is monotone. It can therefore be easily combined with matching in
order to exploit the advantages of both methods. This technique can be
described as a mix between quantile matching, and deconvolution. We provide for
it a theoretical and an empirical validation.","['Alexandra Carpentier', 'Teresa Schlueter']",['stat.ML'],2016-01-04 13:52:49+00:00
http://arxiv.org/abs/1601.00496v2,Nonparametric Modeling of Dynamic Functional Connectivity in fMRI Data,"Dynamic functional connectivity (FC) has in recent years become a topic of
interest in the neuroimaging community. Several models and methods exist for
both functional magnetic resonance imaging (fMRI) and electroencephalography
(EEG), and the results point towards the conclusion that FC exhibits dynamic
changes. The existing approaches modeling dynamic connectivity have primarily
been based on time-windowing the data and k-means clustering. We propose a
non-parametric generative model for dynamic FC in fMRI that does not rely on
specifying window lengths and number of dynamic states. Rooted in Bayesian
statistical modeling we use the predictive likelihood to investigate if the
model can discriminate between a motor task and rest both within and across
subjects. We further investigate what drives dynamic states using the model on
the entire data collated across subjects and task/rest. We find that the number
of states extracted are driven by subject variability and preprocessing
differences while the individual states are almost purely defined by either
task or rest. This questions how we in general interpret dynamic FC and points
to the need for more research on what drives dynamic FC.","['Søren F. V. Nielsen', 'Kristoffer H. Madsen', 'Rasmus Røge', 'Mikkel N. Schmidt', 'Morten Mørup']","['stat.AP', 'q-bio.NC', 'stat.ML']",2016-01-04 13:24:45+00:00
http://arxiv.org/abs/1601.00449v1,Fitting Spectral Decay with the $k$-Support Norm,"The spectral $k$-support norm enjoys good estimation properties in low rank
matrix learning problems, empirically outperforming the trace norm. Its unit
ball is the convex hull of rank $k$ matrices with unit Frobenius norm. In this
paper we generalize the norm to the spectral $(k,p)$-support norm, whose
additional parameter $p$ can be used to tailor the norm to the decay of the
spectrum of the underlying model. We characterize the unit ball and we
explicitly compute the norm. We further provide a conditional gradient method
to solve regularization problems with the norm, and we derive an efficient
algorithm to compute the Euclidean projection on the unit ball in the case
$p=\infty$. In numerical experiments, we show that allowing $p$ to vary
significantly improves performance over the spectral $k$-support norm on
various matrix completion benchmarks, and better captures the spectral decay of
the underlying model.","['Andrew M. McDonald', 'Massimiliano Pontil', 'Dimitris Stamos']","['cs.LG', 'stat.ML']",2016-01-04 10:48:29+00:00
http://arxiv.org/abs/1601.00393v1,On the Reducibility of Submodular Functions,"The scalability of submodular optimization methods is critical for their
usability in practice. In this paper, we study the reducibility of submodular
functions, a property that enables us to reduce the solution space of
submodular optimization problems without performance loss. We introduce the
concept of reducibility using marginal gains. Then we show that by adding
perturbation, we can endow irreducible functions with reducibility, based on
which we propose the perturbation-reduction optimization framework. Our
theoretical analysis proves that given the perturbation scales, the
reducibility gain could be computed, and the performance loss has additive
upper bounds. We further conduct empirical studies and the results demonstrate
that our proposed framework significantly accelerates existing optimization
methods for irreducible submodular functions with a cost of only small
performance losses.","['Jincheng Mei', 'Hao Zhang', 'Bao-Liang Lu']","['cs.LG', 'stat.ML']",2016-01-04 07:16:35+00:00
http://arxiv.org/abs/1601.00350v1,Sparse Diffusion Steepest-Descent for One Bit Compressed Sensing in Wireless Sensor Networks,"This letter proposes a sparse diffusion steepest-descent algorithm for one
bit compressed sensing in wireless sensor networks. The approach exploits the
diffusion strategy from distributed learning in the one bit compressed sensing
framework. To estimate a common sparse vector cooperatively from only the sign
of measurements, steepest-descent is used to minimize the suitable global and
local convex cost functions. A diffusion strategy is suggested for distributive
learning of the sparse vector. Simulation results show the effectiveness of the
proposed distributed algorithm compared to the state-of-the-art non
distributive algorithms in the one bit compressed sensing framework.","['Hadi Zayyani', 'Mehdi Korki', 'Farrokh Marvasti']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2016-01-03 23:03:09+00:00
http://arxiv.org/abs/1601.00238v2,Dimensionality-Dependent Generalization Bounds for $k$-Dimensional Coding Schemes,"The $k$-dimensional coding schemes refer to a collection of methods that
attempt to represent data using a set of representative $k$-dimensional
vectors, and include non-negative matrix factorization, dictionary learning,
sparse coding, $k$-means clustering and vector quantization as special cases.
Previous generalization bounds for the reconstruction error of the
$k$-dimensional coding schemes are mainly dimensionality independent. A major
advantage of these bounds is that they can be used to analyze the
generalization error when data is mapped into an infinite- or high-dimensional
feature space. However, many applications use finite-dimensional data features.
Can we obtain dimensionality-dependent generalization bounds for
$k$-dimensional coding schemes that are tighter than dimensionality-independent
bounds when data is in a finite-dimensional feature space? The answer is
positive. In this paper, we address this problem and derive a
dimensionality-dependent generalization bound for $k$-dimensional coding
schemes by bounding the covering number of the loss function class induced by
the reconstruction error. The bound is of order
$\mathcal{O}\left(\left(mk\ln(mkn)/n\right)^{\lambda_n}\right)$, where $m$ is
the dimension of features, $k$ is the number of the columns in the linear
implementation of coding schemes, $n$ is the size of sample, $\lambda_n>0.5$
when $n$ is finite and $\lambda_n=0.5$ when $n$ is infinite. We show that our
bound can be tighter than previous results, because it avoids inducing the
worst-case upper bound on $k$ of the loss function and converges faster. The
proposed generalization bound is also applied to some specific coding schemes
to demonstrate that the dimensionality-dependent bound is an indispensable
complement to these dimensionality-independent generalization bounds.","['Tongliang Liu', 'Dacheng Tao', 'Dong Xu']","['stat.ML', 'cs.LG']",2016-01-03 01:17:04+00:00
http://arxiv.org/abs/1601.00236v1,Supervised Dimensionality Reduction via Distance Correlation Maximization,"In our work, we propose a novel formulation for supervised dimensionality
reduction based on a nonlinear dependency criterion called Statistical Distance
Correlation, Szekely et. al. (2007). We propose an objective which is free of
distributional assumptions on regression variables and regression model
assumptions. Our proposed formulation is based on learning a low-dimensional
feature representation $\mathbf{z}$, which maximizes the squared sum of
Distance Correlations between low dimensional features $\mathbf{z}$ and
response $y$, and also between features $\mathbf{z}$ and covariates
$\mathbf{x}$. We propose a novel algorithm to optimize our proposed objective
using the Generalized Minimization Maximizaiton method of \Parizi et. al.
(2015). We show superior empirical results on multiple datasets proving the
effectiveness of our proposed approach over several relevant state-of-the-art
supervised dimensionality reduction methods.","['Praneeth Vepakomma', 'Chetan Tonde', 'Ahmed Elgammal']","['cs.LG', 'stat.ML']",2016-01-03 00:14:23+00:00
http://arxiv.org/abs/1601.00142v1,Joint Estimation of Precision Matrices in Heterogeneous Populations,"We introduce a general framework for estimation of inverse covariance, or
precision, matrices from heterogeneous populations. The proposed framework uses
a Laplacian shrinkage penalty to encourage similarity among estimates from
disparate, but related, subpopulations, while allowing for differences among
matrices. We propose an efficient alternating direction method of multipliers
(ADMM) algorithm for parameter estimation, as well as its extension for faster
computation in high dimensions by thresholding the empirical covariance matrix
to identify the joint block diagonal structure in the estimated precision
matrices. We establish both variable selection and norm consistency of the
proposed estimator for distributions with exponential or polynomial tails.
Further, to extend the applicability of the method to the settings with unknown
populations structure, we propose a Laplacian penalty based on hierarchical
clustering, and discuss conditions under which this data-driven choice results
in consistent estimation of precision matrices in heterogenous populations.
Extensive numerical studies and applications to gene expression data from
subtypes of cancer with distinct clinical outcomes indicate the potential
advantages of the proposed method over existing approaches.","['Takumi Saegusa', 'Ali Shojaie']",['stat.ML'],2016-01-02 06:11:06+00:00
http://arxiv.org/abs/1601.00062v2,Practical Algorithms for Learning Near-Isometric Linear Embeddings,"We propose two practical non-convex approaches for learning near-isometric,
linear embeddings of finite sets of data points. Given a set of training points
$\mathcal{X}$, we consider the secant set $S(\mathcal{X})$ that consists of all
pairwise difference vectors of $\mathcal{X}$, normalized to lie on the unit
sphere. The problem can be formulated as finding a symmetric and positive
semi-definite matrix $\boldsymbol{\Psi}$ that preserves the norms of all the
vectors in $S(\mathcal{X})$ up to a distortion parameter $\delta$. Motivated by
non-negative matrix factorization, we reformulate our problem into a Frobenius
norm minimization problem, which is solved by the Alternating Direction Method
of Multipliers (ADMM) and develop an algorithm, FroMax. Another method solves
for a projection matrix $\boldsymbol{\Psi}$ by minimizing the restricted
isometry property (RIP) directly over the set of symmetric, postive
semi-definite matrices. Applying ADMM and a Moreau decomposition on a proximal
mapping, we develop another algorithm, NILE-Pro, for dimensionality reduction.
FroMax is shown to converge faster for smaller $\delta$ while NILE-Pro
converges faster for larger $\delta$. Both non-convex approaches are then
empirically demonstrated to be more computationally efficient than prior convex
approaches for a number of applications in machine learning and signal
processing.","['Jerry Luo', 'Kayla Shapiro', 'Hao-Jun Michael Shi', 'Qi Yang', 'Kan Zhu']","['stat.ML', 'cs.LG', 'math.OC', '90C90']",2016-01-01 09:06:11+00:00
http://arxiv.org/abs/1601.00034v4,Stochastic Neural Networks with Monotonic Activation Functions,"We propose a Laplace approximation that creates a stochastic unit from any
smooth monotonic activation function, using only Gaussian noise. This paper
investigates the application of this stochastic approximation in training a
family of Restricted Boltzmann Machines (RBM) that are closely linked to
Bregman divergences. This family, that we call exponential family RBM
(Exp-RBM), is a subset of the exponential family Harmoniums that expresses
family members through a choice of smooth monotonic non-linearity for each
neuron. Using contrastive divergence along with our Gaussian approximation, we
show that Exp-RBM can learn useful representations using novel stochastic
units.","['Siamak Ravanbakhsh', 'Barnabas Poczos', 'Jeff Schneider', 'Dale Schuurmans', 'Russell Greiner']","['stat.ML', 'cs.LG', 'cs.NE']",2016-01-01 00:47:29+00:00
http://arxiv.org/abs/1601.00024v1,Selecting Near-Optimal Learners via Incremental Data Allocation,"We study a novel machine learning (ML) problem setting of sequentially
allocating small subsets of training data amongst a large set of classifiers.
The goal is to select a classifier that will give near-optimal accuracy when
trained on all data, while also minimizing the cost of misallocated samples.
This is motivated by large modern datasets and ML toolkits with many
combinations of learning algorithms and hyper-parameters. Inspired by the
principle of ""optimism under uncertainty,"" we propose an innovative strategy,
Data Allocation using Upper Bounds (DAUB), which robustly achieves these
objectives across a variety of real-world datasets.
  We further develop substantial theoretical support for DAUB in an idealized
setting where the expected accuracy of a classifier trained on $n$ samples can
be known exactly. Under these conditions we establish a rigorous sub-linear
bound on the regret of the approach (in terms of misallocated data), as well as
a rigorous bound on suboptimality of the selected classifier. Our accuracy
estimates using real-world datasets only entail mild violations of the
theoretical scenario, suggesting that the practical behavior of DAUB is likely
to approach the idealized behavior.","['Ashish Sabharwal', 'Horst Samulowitz', 'Gerald Tesauro']","['cs.LG', 'stat.ML']",2015-12-31 22:19:09+00:00
http://arxiv.org/abs/1512.09327v4,Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server,"This paper makes two contributions to Bayesian machine learning algorithms.
Firstly, we propose stochastic natural gradient expectation propagation (SNEP),
a novel alternative to expectation propagation (EP), a popular variational
inference algorithm. SNEP is a black box variational algorithm, in that it does
not require any simplifying assumptions on the distribution of interest, beyond
the existence of some Monte Carlo sampler for estimating the moments of the EP
tilted distributions. Further, as opposed to EP which has no guarantee of
convergence, SNEP can be shown to be convergent, even when using Monte Carlo
moment estimates. Secondly, we propose a novel architecture for distributed
Bayesian learning which we call the posterior server. The posterior server
allows scalable and robust Bayesian learning in cases where a data set is
stored in a distributed manner across a cluster, with each compute node
containing a disjoint subset of data. An independent Monte Carlo sampler is run
on each compute node, with direct access only to the local data subset, but
which targets an approximation to the global posterior distribution given all
data across the whole cluster. This is achieved by using a distributed
asynchronous implementation of SNEP to pass messages across the cluster. We
demonstrate SNEP and the posterior server on distributed Bayesian learning of
logistic regression and neural networks.
  Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian
Learn- ing, Variational Inference, Expectation Propagation, Stochastic
Approximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server,
Posterior Server.","['Leonard Hasenclever', 'Stefan Webb', 'Thibaut Lienart', 'Sebastian Vollmer', 'Balaji Lakshminarayanan', 'Charles Blundell', 'Yee Whye Teh']","['cs.LG', 'stat.ML']",2015-12-31 17:30:45+00:00
http://arxiv.org/abs/1512.09302v2,Linear Convergence of Proximal Gradient Algorithm with Extrapolation for a Class of Nonconvex Nonsmooth Minimization Problems,"In this paper, we study the proximal gradient algorithm with extrapolation
for minimizing the sum of a Lipschitz differentiable function and a proper
closed convex function. Under the error bound condition used in [19] for
analyzing the convergence of the proximal gradient algorithm, we show that
there exists a threshold such that if the extrapolation coefficients are chosen
below this threshold, then the sequence generated converges $R$-linearly to a
stationary point of the problem. Moreover, the corresponding sequence of
objective values is also $R$-linearly convergent. In addition, the threshold
reduces to $1$ for convex problems and, as a consequence, we obtain the
$R$-linear convergence of the sequence generated by FISTA with fixed restart.
Finally, we present some numerical experiments to illustrate our results.","['Bo Wen', 'Xiaojun Chen', 'Ting Kei Pong']","['math.OC', 'stat.ML']",2015-12-31 14:57:03+00:00
http://arxiv.org/abs/1512.09300v2,Autoencoding beyond pixels using a learned similarity metric,"We present an autoencoder that leverages learned representations to better
measure similarities in data space. By combining a variational autoencoder with
a generative adversarial network we can use learned feature representations in
the GAN discriminator as basis for the VAE reconstruction objective. Thereby,
we replace element-wise errors with feature-wise errors to better capture the
data distribution while offering invariance towards e.g. translation. We apply
our method to images of faces and show that it outperforms VAEs with
element-wise similarity measures in terms of visual fidelity. Moreover, we show
that the method learns an embedding in which high-level abstract visual
features (e.g. wearing glasses) can be modified using simple arithmetic.","['Anders Boesen Lindbo Larsen', 'Søren Kaae Sønderby', 'Hugo Larochelle', 'Ole Winther']","['cs.LG', 'cs.CV', 'stat.ML']",2015-12-31 14:53:39+00:00
http://arxiv.org/abs/1512.09295v1,Strategies and Principles of Distributed Machine Learning on Big Data,"The rise of Big Data has led to new demands for Machine Learning (ML) systems
to learn complex models with millions to billions of parameters, that promise
adequate capacity to digest massive datasets and offer powerful predictive
analytics thereupon. In order to run ML algorithms at such scales, on a
distributed cluster with 10s to 1000s of machines, it is often the case that
significant engineering efforts are required --- and one might fairly ask if
such engineering truly falls within the domain of ML research or not. Taking
the view that Big ML systems can benefit greatly from ML-rooted statistical and
algorithmic insights --- and that ML researchers should therefore not shy away
from such systems design --- we discuss a series of principles and strategies
distilled from our recent efforts on industrial-scale ML solutions. These
principles and strategies span a continuum from application, to engineering,
and to theoretical research and development of Big ML systems and
architectures, with the goal of understanding how to make them efficient,
generally-applicable, and supported with convergence and scaling guarantees.
They concern four key questions which traditionally receive little attention in
ML research: How to distribute an ML program over a cluster? How to bridge ML
computation with inter-machine communication? How to perform such
communication? What should be communicated between machines? By exposing
underlying statistical and algorithmic characteristics unique to ML programs
but not typically seen in traditional computer programs, and by dissecting
successful cases to reveal how we have harnessed these principles to design and
develop both high-performance distributed ML software as well as
general-purpose ML frameworks, we present opportunities for ML researchers and
practitioners to further shape and grow the area that lies between ML and
systems.","['Eric P. Xing', 'Qirong Ho', 'Pengtao Xie', 'Wei Dai']","['stat.ML', 'cs.DC', 'cs.LG']",2015-12-31 14:33:53+00:00
http://arxiv.org/abs/1512.09251v1,Solving the G-problems in less than 500 iterations: Improved efficient constrained optimization by surrogate modeling and adaptive parameter control,"Constrained optimization of high-dimensional numerical problems plays an
important role in many scientific and industrial applications. Function
evaluations in many industrial applications are severely limited and no
analytical information about objective function and constraint functions is
available. For such expensive black-box optimization tasks, the constraint
optimization algorithm COBRA was proposed, making use of RBF surrogate modeling
for both the objective and the constraint functions. COBRA has shown remarkable
success in solving reliably complex benchmark problems in less than 500
function evaluations. Unfortunately, COBRA requires careful adjustment of
parameters in order to do so.
  In this work we present a new self-adjusting algorithm SACOBRA, which is
based on COBRA and capable to achieve high-quality results with very few
function evaluations and no parameter tuning. It is shown with the help of
performance profiles on a set of benchmark problems (G-problems, MOPTA08) that
SACOBRA consistently outperforms any COBRA algorithm with fixed parameter
setting. We analyze the importance of the several new elements in SACOBRA and
find that each element of SACOBRA plays a role to boost up the overall
optimization performance. We discuss the reasons behind and get in this way a
better understanding of high-quality RBF surrogate modeling.","['Samineh Bagheri', 'Wolfgang Konen', 'Michael Emmerich', 'Thomas Bäck']","['math.OC', 'cs.NE', 'stat.ML']",2015-12-31 10:30:21+00:00
http://arxiv.org/abs/1512.09206v1,Nonparametric mixture of Gaussian graphical models,"Graphical model has been widely used to investigate the complex dependence
structure of high-dimensional data, and it is common to assume that observed
data follow a homogeneous graphical model. However, observations usually come
from different resources and have heterogeneous hidden commonality in
real-world applications. Thus, it is of great importance to estimate
heterogeneous dependencies and discover subpopulation with certain commonality
across the whole population. In this work, we introduce a novel regularized
estimation scheme for learning nonparametric mixture of Gaussian graphical
models, which extends the methodology and applicability of Gaussian graphical
models and mixture models. We propose a unified penalized likelihood approach
to effectively estimate nonparametric functional parameters and heterogeneous
graphical parameters. We further design an efficient generalized effective EM
algorithm to address three significant challenges: high-dimensionality,
non-convexity, and label switching. Theoretically, we study both the
algorithmic convergence of our proposed algorithm and the asymptotic properties
of our proposed estimators. Numerically, we demonstrate the performance of our
method in simulation studies and a real application to estimate human brain
functional connectivity from ADHD imaging data, where two heterogeneous
conditional dependencies are explained through profiling demographic variables
and supported by existing scientific findings.","['Kevin Lee', 'Lingzhou Xue']","['stat.ME', 'stat.ML']",2015-12-31 03:41:38+00:00
http://arxiv.org/abs/1512.09204v1,Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index Policies,"We consider effort allocation in crowdsourcing, where we wish to assign
labeling tasks to imperfect homogeneous crowd workers to maximize overall
accuracy in a continuous-time Bayesian setting, subject to budget and time
constraints. The Bayes-optimal policy for this problem is the solution to a
partially observable Markov decision process, but the curse of dimensionality
renders the computation infeasible. Based on the Lagrangian Relaxation
technique in Adelman & Mersereau (2008), we provide a computationally tractable
instance-specific upper bound on the value of this Bayes-optimal policy, which
can in turn be used to bound the optimality gap of any other sub-optimal
policy. In an approach similar in spirit to the Whittle index for restless
multiarmed bandits, we provide an index policy for effort allocation in
crowdsourcing and demonstrate numerically that it outperforms other stateof-
arts and performs close to optimal solution.","['Weici Hu', 'Peter I. Frazier']","['cs.LG', 'cs.AI', 'stat.ML']",2015-12-31 03:09:33+00:00
http://arxiv.org/abs/1512.09103v3,Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling,"Accelerated coordinate descent is widely used in optimization due to its
cheap per-iteration cost and scalability to large-scale problems. Up to a
primal-dual transformation, it is also the same as accelerated stochastic
gradient descent that is one of the central methods used in machine learning.
  In this paper, we improve the best known running time of accelerated
coordinate descent by a factor up to $\sqrt{n}$. Our improvement is based on a
clean, novel non-uniform sampling that selects each coordinate with a
probability proportional to the square root of its smoothness parameter. Our
proof technique also deviates from the classical estimation sequence technique
used in prior work. Our speed-up applies to important problems such as
empirical risk minimization and solving linear systems, both in theory and in
practice.","['Zeyuan Allen-Zhu', 'Zheng Qu', 'Peter Richtárik', 'Yang Yuan']","['math.OC', 'cs.DS', 'math.NA', 'stat.ML']",2015-12-30 20:30:07+00:00
http://arxiv.org/abs/1512.08996v1,Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices,"A gamma process dynamic Poisson factor analysis model is proposed to
factorize a dynamic count matrix, whose columns are sequentially observed count
vectors. The model builds a novel Markov chain that sends the latent gamma
random variables at time $(t-1)$ as the shape parameters of those at time $t$,
which are linked to observed or latent counts under the Poisson likelihood. The
significant challenge of inferring the gamma shape parameters is fully
addressed, using unique data augmentation and marginalization techniques for
the negative binomial distribution. The same nonparametric Bayesian model also
applies to the factorization of a dynamic binary matrix, via a
Bernoulli-Poisson link that connects a binary observation to a latent count,
with closed-form conditional posteriors for the latent counts and efficient
computation for sparse observations. We apply the model to text and music
analysis, with state-of-the-art results.","['Ayan Acharya', 'Joydeep Ghosh', 'Mingyuan Zhou']","['stat.ML', 'stat.AP', 'stat.ME']",2015-12-30 16:28:55+00:00
http://arxiv.org/abs/1512.08949v2,"Simple, Robust and Optimal Ranking from Pairwise Comparisons","We consider data in the form of pairwise comparisons of n items, with the
goal of precisely identifying the top k items for some value of k < n, or
alternatively, recovering a ranking of all the items. We analyze the Copeland
counting algorithm that ranks the items in order of the number of pairwise
comparisons won, and show it has three attractive features: (a) its
computational efficiency leads to speed-ups of several orders of magnitude in
computation time as compared to prior work; (b) it is robust in that
theoretical guarantees impose no conditions on the underlying matrix of
pairwise-comparison probabilities, in contrast to some prior work that applies
only to the BTL parametric model; and (c) it is an optimal method up to
constant factors, meaning that it achieves the information-theoretic limits for
recovering the top k-subset. We extend our results to obtain sharp guarantees
for approximate recovery under the Hamming distortion metric, and more
generally, to any arbitrary error requirement that satisfies a simple and
natural monotonicity condition.","['Nihar B. Shah', 'Martin J. Wainwright']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",2015-12-30 14:25:23+00:00
http://arxiv.org/abs/1512.08887v3,Estimation of the sample covariance matrix from compressive measurements,"This paper focuses on the estimation of the sample covariance matrix from
low-dimensional random projections of data known as compressive measurements.
In particular, we present an unbiased estimator to extract the covariance
structure from compressive measurements obtained by a general class of random
projection matrices consisting of i.i.d. zero-mean entries and finite first
four moments. In contrast to previous works, we make no structural assumptions
about the underlying covariance matrix such as being low-rank. In fact, our
analysis is based on a non-Bayesian data setting which requires no
distributional assumptions on the set of data samples. Furthermore, inspired by
the generality of the projection matrices, we propose an approach to covariance
estimation that utilizes sparse Rademacher matrices. Therefore, our algorithm
can be used to estimate the covariance matrix in applications with limited
memory and computation power at the acquisition devices. Experimental results
demonstrate that our approach allows for accurate estimation of the sample
covariance matrix on several real-world data sets, including video data.",['Farhad Pourkamali-Anaraki'],"['stat.ML', 'cs.LG']",2015-12-30 09:18:18+00:00
http://arxiv.org/abs/1512.08861v1,Sharp Computational-Statistical Phase Transitions via Oracle Computational Model,"We study the fundamental tradeoffs between computational tractability and
statistical accuracy for a general family of hypothesis testing problems with
combinatorial structures. Based upon an oracle model of computation, which
captures the interactions between algorithms and data, we establish a general
lower bound that explicitly connects the minimum testing risk under
computational budget constraints with the intrinsic probabilistic and
combinatorial structures of statistical problems. This lower bound mirrors the
classical statistical lower bound by Le Cam (1986) and allows us to quantify
the optimal statistical performance achievable given limited computational
budgets in a systematic fashion. Under this unified framework, we sharply
characterize the statistical-computational phase transition for two testing
problems, namely, normal mean detection and sparse principal component
detection. For normal mean detection, we consider two combinatorial structures,
namely, sparse set and perfect matching. For these problems we identify
significant gaps between the optimal statistical accuracy that is achievable
under computational tractability constraints and the classical statistical
lower bounds. Compared with existing works on computational lower bounds for
statistical problems, which consider general polynomial-time algorithms on
Turing machines, and rely on computational hardness hypotheses on problems like
planted clique detection, we focus on the oracle computational model, which
covers a broad range of popular algorithms, and do not rely on unproven
hypotheses. Moreover, our result provides an intuitive and concrete
interpretation for the intrinsic computational intractability of
high-dimensional statistical problems. One byproduct of our result is a lower
bound for a strict generalization of the matrix permanent problem, which is of
independent interest.","['Zhaoran Wang', 'Quanquan Gu', 'Han Liu']",['stat.ML'],2015-12-30 06:16:46+00:00
http://arxiv.org/abs/1512.08819v1,Joint limiting laws for high-dimensional independence tests,"Testing independence is of significant interest in many important areas of
large-scale inference. Using extreme-value form statistics to test against
sparse alternatives and using quadratic form statistics to test against dense
alternatives are two important testing procedures for high-dimensional
independence. However, quadratic form statistics suffer from low power against
sparse alternatives, and extreme-value form statistics suffer from low power
against dense alternatives with small disturbances and may have size
distortions due to its slow convergence. For real-world applications, it is
important to derive powerful testing procedures against more general
alternatives. Based on intermediate limiting distributions, we derive
(model-free) joint limiting laws of extreme-value form and quadratic form
statistics, and surprisingly, we prove that they are asymptotically
independent. Given such asymptotic independencies, we propose (model-free)
testing procedures to boost the power against general alternatives and also
retain the correct asymptotic size. Under the high-dimensional setting, we
derive the closed-form limiting null distributions, and obtain their explicit
rates of uniform convergence. We prove their consistent statistical powers
against general alternatives. We demonstrate the performance of our proposed
test statistics in simulation studies. Our work provides very helpful insights
to high-dimensional independence tests, and fills an important gap.","['Danning Li', 'Lingzhou Xue']","['math.ST', 'stat.ME', 'stat.ML', 'stat.OT', 'stat.TH', '62H12, 60F05']",2015-12-30 00:30:19+00:00
http://arxiv.org/abs/1512.08808v2,Sparse group factor analysis for biclustering of multiple data sources,"Motivation: Modelling methods that find structure in data are necessary with
the current large volumes of genomic data, and there have been various efforts
to find subsets of genes exhibiting consistent patterns over subsets of
treatments. These biclustering techniques have focused on one data source,
often gene expression data. We present a Bayesian approach for joint
biclustering of multiple data sources, extending a recent method Group Factor
Analysis (GFA) to have a biclustering interpretation with additional sparsity
assumptions. The resulting method enables data-driven detection of linear
structure present in parts of the data sources. Results: Our simulation studies
show that the proposed method reliably infers bi-clusters from heterogeneous
data sources. We tested the method on data from the NCI-DREAM drug sensitivity
prediction challenge, resulting in an excellent prediction accuracy. Moreover,
the predictions are based on several biclusters which provide insight into the
data sources, in this case on gene expression, DNA methylation, protein
abundance, exome sequence, functional connectivity fingerprints and drug
sensitivity.","['Kerstin Bunte', 'Eemeli Leppäaho', 'Inka Saarinen', 'Samuel Kaski']","['cs.LG', 'cs.IR', 'stat.ML']",2015-12-29 22:07:35+00:00
http://arxiv.org/abs/1512.08806v3,Common Variable Learning and Invariant Representation Learning using Siamese Neural Networks,"We consider the statistical problem of learning common source of variability
in data which are synchronously captured by multiple sensors, and demonstrate
that Siamese neural networks can be naturally applied to this problem. This
approach is useful in particular in exploratory, data-driven applications,
where neither a model nor label information is available. In recent years, many
researchers have successfully applied Siamese neural networks to obtain an
embedding of data which corresponds to a ""semantic similarity"". We present an
interpretation of this ""semantic similarity"" as learning of equivalence
classes. We discuss properties of the embedding obtained by Siamese networks
and provide empirical results that demonstrate the ability of Siamese networks
to learn common variability.","['Uri Shaham', 'Roy Lederman']","['stat.ML', 'cs.LG', 'cs.NE']",2015-12-29 22:06:00+00:00
http://arxiv.org/abs/1512.08787v1,Matrix Completion Under Monotonic Single Index Models,"Most recent results in matrix completion assume that the matrix under
consideration is low-rank or that the columns are in a union of low-rank
subspaces. In real-world settings, however, the linear structure underlying
these models is distorted by a (typically unknown) nonlinear transformation.
This paper addresses the challenge of matrix completion in the face of such
nonlinearities. Given a few observations of a matrix that are obtained by
applying a Lipschitz, monotonic function to a low rank matrix, our task is to
estimate the remaining unobserved entries. We propose a novel matrix completion
method that alternates between low-rank matrix estimation and monotonic
function estimation to estimate the missing matrix elements. Mean squared error
bounds provide insight into how well the matrix can be estimated based on the
size, rank of the matrix and properties of the nonlinear transformation.
Empirical results on synthetic and real-world datasets demonstrate the
competitiveness of the proposed approach.","['Ravi Ganti', 'Laura Balzano', 'Rebecca Willett']","['stat.ML', 'cs.LG']",2015-12-29 20:52:41+00:00
http://arxiv.org/abs/1512.08673v1,Error Bounds for Compressed Sensing Algorithms With Group Sparsity: A Unified Approach,"In compressed sensing, in order to recover a sparse or nearly sparse vector
from possibly noisy measurements, the most popular approach is $\ell_1$-norm
minimization. Upper bounds for the $\ell_2$- norm of the error between the true
and estimated vectors are given in [1] and reviewed in [2], while bounds for
the $\ell_1$-norm are given in [3]. When the unknown vector is not
conventionally sparse but is ""group sparse"" instead, a variety of alternatives
to the $\ell_1$-norm have been proposed in the literature, including the group
LASSO, sparse group LASSO, and group LASSO with tree structured overlapping
groups. However, no error bounds are available for any of these modified
objective functions. In the present paper, a unified approach is presented for
deriving upper bounds on the error between the true vector and its
approximation, based on the notion of decomposable and $\gamma$-decomposable
norms. The bounds presented cover all of the norms mentioned above, and also
provide a guideline for choosing norms in future to accommodate alternate forms
of sparsity.","['M. Eren Ahsen', 'M. Vidyasagar']","['stat.ML', '62J99']",2015-12-29 13:10:25+00:00
http://arxiv.org/abs/1512.08643v2,Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity,"Functional brain networks are well described and estimated from data with
Gaussian Graphical Models (GGMs), e.g. using sparse inverse covariance
estimators. Comparing functional connectivity of subjects in two populations
calls for comparing these estimated GGMs. Our goal is to identify differences
in GGMs known to have similar structure. We characterize the uncertainty of
differences with confidence intervals obtained using a parametric distribution
on parameters of a sparse estimator. Sparse penalties enable statistical
guarantees and interpretable models even in high-dimensional and low-sample
settings. Characterizing the distributions of sparse models is inherently
challenging as the penalties produce a biased estimator. Recent work invokes
the sparsity assumptions to effectively remove the bias from a sparse estimator
such as the lasso. These distributions can be used to give confidence intervals
on edges in GGMs, and by extension their differences. However, in the case of
comparing GGMs, these estimators do not make use of any assumed joint structure
among the GGMs. Inspired by priors from brain functional connectivity we derive
the distribution of parameter differences under a joint penalty when parameters
are known to be sparse in the difference. This leads us to introduce the
debiased multi-task fused lasso, whose distribution can be characterized in an
efficient manner. We then show how the debiased lasso and multi-task fused
lasso can be used to obtain confidence intervals on edge differences in GGMs.
We validate the techniques proposed on a set of synthetic examples as well as
neuro-imaging dataset created for the study of autism.","['Eugene Belilovsky', 'Gaël Varoquaux', 'Matthew B. Blaschko']",['stat.ML'],2015-12-29 10:07:20+00:00
http://arxiv.org/abs/1512.08571v1,Structured Pruning of Deep Convolutional Neural Networks,"Real time application of deep learning algorithms is often hindered by high
computational complexity and frequent memory accesses. Network pruning is a
promising technique to solve this problem. However, pruning usually results in
irregular network connections that not only demand extra representation efforts
but also do not fit well on parallel computation. We introduce structured
sparsity at various scales for convolutional neural networks, which are channel
wise, kernel wise and intra kernel strided sparsity. This structured sparsity
is very advantageous for direct computational resource savings on embedded
computers, parallel computing environments and hardware based systems. To
decide the importance of network connections and paths, the proposed method
uses a particle filtering approach. The importance weight of each particle is
assigned by computing the misclassification rate with corresponding
connectivity pattern. The pruned network is re-trained to compensate for the
losses due to pruning. While implementing convolutions as matrix products, we
particularly show that intra kernel strided sparsity with a simple constraint
can significantly reduce the size of kernel and feature map matrices. The
pruned network is finally fixed point optimized with reduced word length
precision. This results in significant reduction in the total storage size
providing advantages for on-chip memory based implementations of deep neural
networks.","['Sajid Anwar', 'Kyuyeon Hwang', 'Wonyong Sung']","['cs.NE', 'cs.LG', 'stat.ML']",2015-12-29 01:21:08+00:00
