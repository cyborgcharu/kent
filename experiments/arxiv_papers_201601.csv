id,title,abstract,authors,categories,date
http://arxiv.org/abs/1602.08151v2,Learning to Abstain from Binary Prediction,"A binary classifier capable of abstaining from making a label prediction has
two goals in tension: minimizing errors, and avoiding abstaining unnecessarily
often. In this work, we exactly characterize the best achievable tradeoff
between these two goals in a general semi-supervised setting, given an ensemble
of predictors of varying competence as well as unlabeled data on which we wish
to predict or abstain. We give an algorithm for learning a classifier in this
setting which trades off its errors with abstentions in a minimax optimal
manner, is as efficient as linear learning and prediction, and is demonstrably
practical. Our analysis extends to a large class of loss functions and other
scenarios, including ensembles comprised of specialists that can themselves
abstain.",['Akshay Balsubramani'],"['cs.LG', 'stat.ML']",2016-02-25 23:46:57+00:00
http://arxiv.org/abs/1602.08017v1,Meta-learning within Projective Simulation,"Learning models of artificial intelligence can nowadays perform very well on
a large variety of tasks. However, in practice different task environments are
best handled by different learning models, rather than a single, universal,
approach. Most non-trivial models thus require the adjustment of several to
many learning parameters, which is often done on a case-by-case basis by an
external party. Meta-learning refers to the ability of an agent to autonomously
and dynamically adjust its own learning parameters, or meta-parameters. In this
work we show how projective simulation, a recently developed model of
artificial intelligence, can naturally be extended to account for meta-learning
in reinforcement learning settings. The projective simulation approach is based
on a random walk process over a network of clips. The suggested meta-learning
scheme builds upon the same design and employs clip networks to monitor the
agent's performance and to adjust its meta-parameters ""on the fly"". We
distinguish between ""reflexive adaptation"" and ""adaptation through learning"",
and show the utility of both approaches. In addition, a trade-off between
flexibility and learning-time is addressed. The extended model is examined on
three different kinds of reinforcement learning tasks, in which the agent has
different optimal values of the meta-parameters, and is shown to perform well,
reaching near-optimal to optimal success rates in all of them, without ever
needing to manually adjust any meta-parameter.","['Adi Makmal', 'Alexey A. Melnikov', 'Vedran Dunjko', 'Hans J. Briegel']","['cs.AI', 'cs.LG', 'stat.ML']",2016-02-25 18:07:53+00:00
http://arxiv.org/abs/1602.08007v1,Practical Riemannian Neural Networks,"We provide the first experimental results on non-synthetic datasets for the
quasi-diagonal Riemannian gradient descents for neural networks introduced in
[Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as a
previously unpublished electroencephalogram dataset. The quasi-diagonal
Riemannian algorithms consistently beat simple stochastic gradient gradient
descents by a varying margin. The computational overhead with respect to simple
backpropagation is around a factor $2$. Perhaps more interestingly, these
methods also reach their final performance quickly, thus requiring fewer
training epochs and a smaller total computation time.
  We also present an implementation guide to these Riemannian gradient descents
for neural networks, showing how the quasi-diagonal versions can be implemented
with minimal effort on top of existing routines which compute gradients.","['Gaétan Marceau-Caron', 'Yann Ollivier']","['cs.NE', 'cs.LG', 'stat.ML']",2016-02-25 17:37:28+00:00
http://arxiv.org/abs/1602.07905v2,Thompson Sampling is Asymptotically Optimal in General Environments,"We discuss a variant of Thompson sampling for nonparametric reinforcement
learning in a countable classes of general stochastic environments. These
environments can be non-Markov, non-ergodic, and partially observable. We show
that Thompson sampling learns the environment class in the sense that (1)
asymptotically its value converges to the optimal value in mean and (2) given a
recoverability assumption regret is sublinear.","['Jan Leike', 'Tor Lattimore', 'Laurent Orseau', 'Marcus Hutter']","['cs.LG', 'cs.AI', 'stat.ML']",2016-02-25 12:37:21+00:00
http://arxiv.org/abs/1602.07865v1,Projected Estimators for Robust Semi-supervised Classification,"For semi-supervised techniques to be applied safely in practice we at least
want methods to outperform their supervised counterparts. We study this
question for classification using the well-known quadratic surrogate loss
function. Using a projection of the supervised estimate onto a set of
constraints imposed by the unlabeled data, we find we can safely improve over
the supervised solution in terms of this quadratic loss. Unlike other
approaches to semi-supervised learning, the procedure does not rely on
assumptions that are not intrinsic to the classifier at hand. It is
theoretically demonstrated that, measured on the labeled and unlabeled training
data, this semi-supervised procedure never gives a lower quadratic loss than
the supervised alternative. To our knowledge this is the first approach that
offers such strong, albeit conservative, guarantees for improvement over the
supervised solution. The characteristics of our approach are explicated using
benchmark datasets to further understand the similarities and differences
between the quadratic loss criterion used in the theoretical results and the
classification accuracy often considered in practice.","['Jesse H. Krijthe', 'Marco Loog']","['stat.ML', 'cs.LG']",2016-02-25 09:57:42+00:00
http://arxiv.org/abs/1602.07863v1,Learning Gaussian Graphical Models With Fractional Marginal Pseudo-likelihood,"We propose a Bayesian approximate inference method for learning the
dependence structure of a Gaussian graphical model. Using pseudo-likelihood, we
derive an analytical expression to approximate the marginal likelihood for an
arbitrary graph structure without invoking any assumptions about
decomposability. The majority of the existing methods for learning Gaussian
graphical models are either restricted to decomposable graphs or require
specification of a tuning parameter that may have a substantial impact on
learned structures. By combining a simple sparsity inducing prior for the graph
structures with a default reference prior for the model parameters, we obtain a
fast and easily applicable scoring function that works well for even
high-dimensional data. We demonstrate the favourable performance of our
approach by large-scale comparisons against the leading methods for learning
non-decomposable Gaussian graphical models. A theoretical justification for our
method is provided by showing that it yields a consistent estimator of the
graph structure.","['Janne Leppä-aho', 'Johan Pensar', 'Teemu Roos', 'Jukka Corander']","['stat.ML', 'cs.LG']",2016-02-25 09:42:46+00:00
http://arxiv.org/abs/1602.07860v2,Probably Approximately Correct Greedy Maximization with Efficient Bounds on Information Gain for Sensor Selection,"Submodular function maximization finds application in a variety of real-world
decision-making problems. However, most existing methods, based on greedy
maximization, assume it is computationally feasible to evaluate F, the function
being maximized. Unfortunately, in many realistic settings F is too expensive
to evaluate exactly even once. We present probably approximately correct greedy
maximization, which requires access only to cheap anytime confidence bounds on
F and uses them to prune elements. We show that, with high probability, our
method returns an approximately optimal set. We propose novel, cheap confidence
bounds for conditional entropy, which appears in many common choices of F and
for which it is difficult to find unbiased or bounded estimates. Finally,
results on a real-world dataset from a multi-camera tracking system in a
shopping mall demonstrate that our approach performs comparably to existing
methods, but at a fraction of the computational cost.","['Yash Satsangi', 'Shimon Whiteson', 'Frans A. Oliehoek']","['cs.AI', 'cs.LG', 'stat.ML']",2016-02-25 09:34:38+00:00
http://arxiv.org/abs/1602.07844v1,Fast Nonsmooth Regularized Risk Minimization with Continuation,"In regularized risk minimization, the associated optimization problem becomes
particularly difficult when both the loss and regularizer are nonsmooth.
Existing approaches either have slow or unclear convergence properties, are
restricted to limited problem subclasses, or require careful setting of a
smoothing parameter. In this paper, we propose a continuation algorithm that is
applicable to a large class of nonsmooth regularized risk minimization
problems, can be flexibly used with a number of existing solvers for the
underlying smoothed subproblem, and with convergence results on the whole
algorithm rather than just one of its subproblems. In particular, when
accelerated solvers are used, the proposed algorithm achieves the fastest known
rates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex
problems. Experiments on nonsmooth classification and regression tasks
demonstrate that the proposed algorithm outperforms the state-of-the-art.","['Shuai Zheng', 'Ruiliang Zhang', 'James T. Kwok']","['cs.LG', 'math.OC', 'stat.ML']",2016-02-25 08:34:59+00:00
http://arxiv.org/abs/1602.07836v2,A Bayesian baseline for belief in uncommon events,"The plausibility of uncommon events and miracles based on testimony of such
an event has been much discussed. When analyzing the probabilities involved, it
has mostly been assumed that the common events can be taken as data in the
calculations. However, we usually have only testimonies for the common events.
While this difference does not have a significant effect on the inductive part
of the inference, it has a large influence on how one should view the
reliability of testimonies. In this work, a full Bayesian solution is given for
the more realistic case, where one has a large number of testimonies for a
common event and one testimony for an uncommon event. It is seen that, in order
for there to be a large amount of testimonies for a common event, the
testimonies will probably be quite reliable. For this reason, because the
testimonies are quite reliable based on the testimonies for the common events,
the probability for the uncommon event, given a testimony for it, is also
higher. Hence, one should be more open-minded when considering the plausibility
of uncommon events.",['V. Palonen'],"['stat.AP', 'stat.ML', 'stat.OT']",2016-02-25 07:57:07+00:00
http://arxiv.org/abs/1602.07807v2,Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly Detection,"Many important forms of data are stored digitally in XML format. Errors can
occur in the textual content of the data in the fields of the XML. Fixing these
errors manually is time-consuming and expensive, especially for large amounts
of data. There is increasing interest in the research, development, and use of
automated techniques for assisting with data cleaning. Electronic dictionaries
are an important form of data frequently stored in XML format that frequently
have errors introduced through a mixture of manual typographical entry errors
and optical character recognition errors. In this paper we describe methods for
flagging statistical anomalies as likely errors in electronic dictionaries
stored in XML format. We describe six systems based on different sources of
information. The systems detect errors using various signals in the data
including uncommon characters, text length, character-based language models,
word-based language models, tied-field length ratios, and tied-field
transliteration models. Four of the systems detect errors based on expectations
automatically inferred from content within elements of a single field type. We
call these single-field systems. Two of the systems detect errors based on
correspondence expectations automatically inferred from content within elements
of multiple related field types. We call these tied-field systems. For each
system, we provide an intuitive analysis of the type of error that it is
successful at detecting. Finally, we describe two larger-scale evaluations
using crowdsourcing with Amazon's Mechanical Turk platform and using the
annotations of a domain expert. The evaluations consistently show that the
systems are useful for improving the efficiency with which errors in XML
electronic dictionaries can be detected.","['Michael Bloodgood', 'Benjamin Strauss']","['cs.DB', 'cs.CL', 'stat.ML', 'I.5.1; I.5.4; G.3; I.2.7; I.2.6']",2016-02-25 05:49:36+00:00
http://arxiv.org/abs/1602.07800v5,Towards Unifying Hamiltonian Monte Carlo and Slice Sampling,"We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling,
demonstrating their connection via the Hamiltonian-Jacobi equation from
Hamiltonian mechanics. This insight enables extension of HMC and slice sampling
to a broader family of samplers, called Monomial Gamma Samplers (MGS). We
provide a theoretical analysis of the mixing performance of such samplers,
proving that in the limit of a single parameter, the MGS draws decorrelated
samples from the desired target distribution. We further show that as this
parameter tends toward this limit, performance gains are achieved at a cost of
increasing numerical difficulty and some practical convergence issues. Our
theoretical results are validated with synthetic data and real-world
applications.","['Yizhe Zhang', 'Xiangyu Wang', 'Changyou Chen', 'Ricardo Henao', 'Kai Fan', 'Lawrence Carin']","['stat.ML', 'stat.ME']",2016-02-25 05:14:45+00:00
http://arxiv.org/abs/1602.07795v2,Expectation Consistent Approximate Inference: Generalizations and Convergence,"Approximations of loopy belief propagation, including expectation propagation
and approximate message passing, have attracted considerable attention for
probabilistic inference problems. This paper proposes and analyzes a
generalization of Opper and Winther's expectation consistent (EC) approximate
inference method. The proposed method, called Generalized Expectation
Consistency (GEC), can be applied to both maximum a posteriori (MAP) and
minimum mean squared error (MMSE) estimation. Here we characterize its fixed
points, convergence, and performance relative to the replica prediction of
optimality.","['Alyson K. Fletcher', 'Mojtaba Sahraee-Ardakan', 'Sundeep Rangan', 'Philip Schniter']","['cs.IT', 'math.IT', 'stat.ML']",2016-02-25 05:06:47+00:00
http://arxiv.org/abs/1602.07783v2,Top-N Recommendation with Novel Rank Approximation,"The importance of accurate recommender systems has been widely recognized by
academia and industry. However, the recommendation quality is still rather low.
Recently, a linear sparse and low-rank representation of the user-item matrix
has been applied to produce Top-N recommendations. This approach uses the
nuclear norm as a convex relaxation for the rank function and has achieved
better recommendation accuracy than the state-of-the-art methods. In the past
several years, solving rank minimization problems by leveraging nonconvex
relaxations has received increasing attention. Some empirical results
demonstrate that it can provide a better approximation to original problems
than convex relaxation. In this paper, we propose a novel rank approximation to
enhance the performance of Top-N recommendation systems, where the
approximation error is controllable. Experimental results on real data show
that the proposed rank approximation improves the Top-$N$ recommendation
accuracy substantially.","['Zhao Kang', 'Qiang Cheng']","['cs.IR', 'cs.AI', 'stat.ML']",2016-02-25 03:33:44+00:00
http://arxiv.org/abs/1602.07764v2,Reinforcement Learning of POMDPs using Spectral Methods,"We propose a new reinforcement learning algorithm for partially observable
Markov decision processes (POMDP) based on spectral decomposition methods.
While spectral methods have been previously employed for consistent learning of
(passive) latent variable models such as hidden Markov models, POMDPs are more
challenging since the learner interacts with the environment and possibly
changes the future observations in the process. We devise a learning algorithm
running through episodes, in each episode we employ spectral techniques to
learn the POMDP parameters from a trajectory generated by a fixed policy. At
the end of the episode, an optimization oracle returns the optimal memoryless
planning policy which maximizes the expected reward based on the estimated
POMDP model. We prove an order-optimal regret bound with respect to the optimal
memoryless policy and efficient scaling with respect to the dimensionality of
observation and action spaces.","['Kamyar Azizzadenesheli', 'Alessandro Lazaric', 'Animashree Anandkumar']","['cs.AI', 'cs.LG', 'cs.NA', 'math.OC', 'stat.ML']",2016-02-25 01:25:36+00:00
http://arxiv.org/abs/1602.07754v2,A Compressed Sensing Based Decomposition of Electrodermal Activity Signals,"The measurement and analysis of Electrodermal Activity (EDA) offers
applications in diverse areas ranging from market research, to seizure
detection, to human stress analysis. Unfortunately, the analysis of EDA signals
is made difficult by the superposition of numerous components which can obscure
the signal information related to a user's response to a stimulus. We show how
simple pre-processing followed by a novel compressed sensing based
decomposition can mitigate the effects of the undesired noise components and
help reveal the underlying physiological signal. The proposed framework allows
for decomposition of EDA signals with provable bounds on the recovery of user
responses. We test our procedure on both synthetic and real-world EDA signals
from wearable sensors and demonstrate that our approach allows for more
accurate recovery of user responses as compared to the existing techniques.","['Swayambhoo Jain', 'Urvashi Oswal', 'Kevin S. Xu', 'Brian Eriksson', 'Jarvis Haupt']","['stat.ML', 'cs.LG', 'stat.AP']",2016-02-24 23:52:07+00:00
http://arxiv.org/abs/1602.07714v2,Learning values across many orders of magnitude,"Most learning algorithms are not invariant to the scale of the function that
is being approximated. We propose to adaptively normalize the targets used in
learning. This is useful in value-based reinforcement learning, where the
magnitude of appropriate value approximations can change over time when we
update the policy of behavior. Our main motivation is prior work on learning to
play Atari games, where the rewards were all clipped to a predetermined range.
This clipping facilitates learning across many different games with a single
learning algorithm, but a clipped reward function can result in qualitatively
different behavior. Using the adaptive normalization we can remove this
domain-specific heuristic without diminishing overall performance.","['Hado van Hasselt', 'Arthur Guez', 'Matteo Hessel', 'Volodymyr Mnih', 'David Silver']","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",2016-02-24 21:14:52+00:00
http://arxiv.org/abs/1602.07630v1,Online Dual Coordinate Ascent Learning,"The stochastic dual coordinate-ascent (S-DCA) technique is a useful
alternative to the traditional stochastic gradient-descent algorithm for
solving large-scale optimization problems due to its scalability to large data
sets and strong theoretical guarantees. However, the available S-DCA
formulation is limited to finite sample sizes and relies on performing multiple
passes over the same data. This formulation is not well-suited for online
implementations where data keep streaming in. In this work, we develop an {\em
online} dual coordinate-ascent (O-DCA) algorithm that is able to respond to
streaming data and does not need to revisit the past data. This feature embeds
the resulting construction with continuous adaptation, learning, and tracking
abilities, which are particularly attractive for online learning scenarios.","['Bicheng Ying', 'Kun Yuan', 'Ali H. Sayed']","['math.OC', 'cs.LG', 'stat.ML']",2016-02-24 18:26:35+00:00
http://arxiv.org/abs/1602.07576v3,Group Equivariant Convolutional Networks,"We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a
natural generalization of convolutional neural networks that reduces sample
complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of
layer that enjoys a substantially higher degree of weight sharing than regular
convolution layers. G-convolutions increase the expressive capacity of the
network without increasing the number of parameters. Group convolution layers
are easy to use and can be implemented with negligible computational overhead
for discrete groups generated by translations, reflections and rotations.
G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.","['Taco S. Cohen', 'Max Welling']","['cs.LG', 'stat.ML']",2016-02-24 16:17:15+00:00
http://arxiv.org/abs/1602.07466v1,Asymptotic consistency and order specification for logistic classifier chains in multi-label learning,"Classifier chains are popular and effective method to tackle a multi-label
classification problem. The aim of this paper is to study the asymptotic
properties of the chain model in which the conditional probabilities are of the
logistic form. In particular we find conditions on the number of labels and the
distribution of feature vector under which the estimated mode of the joint
distribution of labels converges to the true mode. Best of our knowledge, this
important issue has not yet been studied in the context of multi-label
learning. We also investigate how the order of model building in a chain
influences the estimation of the joint distribution of labels. We establish the
link between the problem of incorrect ordering in the chain and incorrect model
specification. We propose a procedure of determining the optimal ordering of
labels in the chain, which is based on using measures of correct specification
and allows to find the ordering such that the consecutive logistic models are
best possibly specified. The other important question raised in this paper is
how accurately can we estimate the joint posterior probability when the
ordering of labels is wrong or the logistic models in the chain are incorrectly
specified. The numerical experiments illustrate the theoretical results.",['Paweł Teisseyre'],"['cs.LG', 'stat.ML']",2016-02-24 11:15:03+00:00
http://arxiv.org/abs/1602.07464v1,Feature ranking for multi-label classification using Markov Networks,"We propose a simple and efficient method for ranking features in multi-label
classification. The method produces a ranking of features showing their
relevance in predicting labels, which in turn allows to choose a final subset
of features. The procedure is based on Markov Networks and allows to model the
dependencies between labels and features in a direct way. In the first step we
build a simple network using only labels and then we test how much adding a
single feature affects the initial network. More specifically, in the first
step we use the Ising model whereas the second step is based on the score
statistic, which allows to test a significance of added features very quickly.
The proposed approach does not require transformation of label space, gives
interpretable results and allows for attractive visualization of dependency
structure. We give a theoretical justification of the procedure by discussing
some theoretical properties of the Ising model and the score statistic. We also
discuss feature ranking procedure based on fitting Ising model using $l_1$
regularized logistic regressions. Numerical experiments show that the proposed
methods outperform the conventional approaches on the considered artificial and
real datasets.",['Paweł Teisseyre'],"['cs.LG', 'stat.ML']",2016-02-24 11:11:10+00:00
http://arxiv.org/abs/1602.07428v1,Max-Margin Nonparametric Latent Feature Models for Link Prediction,"Link prediction is a fundamental task in statistical network analysis. Recent
advances have been made on learning flexible nonparametric Bayesian latent
feature models for link prediction. In this paper, we present a max-margin
learning method for such nonparametric latent feature relational models. Our
approach attempts to unite the ideas of max-margin learning and Bayesian
nonparametrics to discover discriminative latent features for link prediction.
It inherits the advances of nonparametric Bayesian methods to infer the unknown
latent social dimension, while for discriminative link prediction, it adopts
the max-margin learning principle by minimizing a hinge-loss using the linear
expectation operator, without dealing with a highly nonlinear link likelihood
function. For posterior inference, we develop an efficient stochastic
variational inference algorithm under a truncated mean-field assumption. Our
methods can scale up to large-scale real networks with millions of entities and
tens of millions of positive links. We also provide a full Bayesian
formulation, which can avoid tuning regularization hyper-parameters.
Experimental results on a diverse range of real datasets demonstrate the
benefits inherited from max-margin learning and Bayesian nonparametric
inference.","['Jun Zhu', 'Jiaming Song', 'Bei Chen']","['cs.LG', 'cs.SI', 'stat.ME', 'stat.ML']",2016-02-24 08:08:05+00:00
http://arxiv.org/abs/1602.07387v3,Discrete Distribution Estimation under Local Privacy,"The collection and analysis of user data drives improvements in the app and
web ecosystems, but comes with risks to privacy. This paper examines discrete
distribution estimation under local privacy, a setting wherein service
providers can learn the distribution of a categorical statistic of interest
without collecting the underlying data. We present new mechanisms, including
hashed K-ary Randomized Response (KRR), that empirically meet or exceed the
utility of existing mechanisms at all privacy levels. New theoretical results
demonstrate the order-optimality of KRR and the existing RAPPOR mechanism at
different privacy regimes.","['Peter Kairouz', 'Keith Bonawitz', 'Daniel Ramage']","['stat.ML', 'cs.LG']",2016-02-24 03:48:19+00:00
http://arxiv.org/abs/1602.07349v3,Parsimonious modeling with Information Filtering Networks,"We introduce a methodology to construct parsimonious probabilistic models.
This method makes use of Information Filtering Networks to produce a robust
estimate of the global sparse inverse covariance from a simple sum of local
inverse covariances computed on small sub-parts of the network. Being based on
local and low-dimensional inversions, this method is computationally very
efficient and statistically robust even for the estimation of inverse
covariance of high-dimensional, noisy and short time-series. Applied to
financial data our method results computationally more efficient than
state-of-the-art methodologies such as Glasso producing, in a fraction of the
computation time, models that can have equivalent or better performances but
with a sparser inference structure. We also discuss performances with sparse
factor models where we notice that relative performances decrease with the
number of factors. The local nature of this approach allows us to perform
computations in parallel and provides a tool for dynamical adaptation by
partial updating when the properties of some variables change without the need
of recomputing the whole model. This makes this approach particularly suitable
to handle big datasets with large numbers of variables. Examples of practical
application for forecasting, stress testing and risk allocation in financial
systems are also provided.","['Wolfram Barfuss', 'Guido Previde Massara', 'T. Di Matteo', 'Tomaso Aste']","['cs.IT', 'math.IT', 'stat.ML']",2016-02-23 23:03:56+00:00
http://arxiv.org/abs/1602.07291v1,The IBM 2016 Speaker Recognition System,"In this paper we describe the recent advancements made in the IBM i-vector
speaker recognition system for conversational speech. In particular, we
identify key techniques that contribute to significant improvements in
performance of our system, and quantify their contributions. The techniques
include: 1) a nearest-neighbor discriminant analysis (NDA) approach that is
formulated to alleviate some of the limitations associated with the
conventional linear discriminant analysis (LDA) that assumes Gaussian
class-conditional distributions, 2) the application of speaker- and
channel-adapted features, which are derived from an automatic speech
recognition (ASR) system, for speaker recognition, and 3) the use of a deep
neural network (DNN) acoustic model with a large number of output units (~10k
senones) to compute the frame-level soft alignments required in the i-vector
estimation process. We evaluate these techniques on the NIST 2010 speaker
recognition evaluation (SRE) extended core conditions involving telephone and
microphone trials. Experimental results indicate that: 1) the NDA is more
effective (up to 35% relative improvement in terms of EER) than the traditional
parametric LDA for speaker recognition, 2) when compared to raw acoustic
features (e.g., MFCCs), the ASR speaker-adapted features provide gains in
speaker recognition performance, and 3) increasing the number of output units
in the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)
provides consistent improvements in performance (for example from 37% to 57%
relative EER gains over our baseline GMM i-vector system). To our knowledge,
results reported in this paper represent the best performances published to
date on the NIST SRE 2010 extended core tasks.","['Seyed Omid Sadjadi', 'Sriram Ganapathy', 'Jason W. Pelecanos']","['cs.SD', 'cs.CL', 'stat.ML']",2016-02-23 20:39:40+00:00
http://arxiv.org/abs/1602.07277v2,A Simple Approach to Sparse Clustering,"Consider the problem of sparse clustering, where it is assumed that only a
subset of the features are useful for clustering purposes. In the framework of
the COSA method of Friedman and Meulman, subsequently improved in the form of
the Sparse K-means method of Witten and Tibshirani, a natural and simpler
hill-climbing approach is introduced. The new method is shown to be competitive
with these two methods and others.","['Ery Arias-Castro', 'Xiao Pu']",['stat.ML'],2016-02-23 19:49:16+00:00
http://arxiv.org/abs/1602.07265v2,Search Improves Label for Active Learning,"We investigate active learning with access to two distinct oracles: Label
(which is standard) and Search (which is not). The Search oracle models the
situation where a human searches a database to seed or counterexample an
existing solution. Search is stronger than Label while being natural to
implement in many situations. We show that an algorithm using both oracles can
provide exponentially large problem-dependent improvements over Label alone.","['Alina Beygelzimer', 'Daniel Hsu', 'John Langford', 'Chicheng Zhang']","['cs.LG', 'stat.ML']",2016-02-23 19:05:09+00:00
http://arxiv.org/abs/1602.07194v2,Lens depth function and k-relative neighborhood graph: versatile tools for ordinal data analysis,"In recent years it has become popular to study machine learning problems in a
setting of ordinal distance information rather than numerical distance
measurements. By ordinal distance information we refer to binary answers to
distance comparisons such as $d(A,B)<d(C,D)$. For many problems in machine
learning and statistics it is unclear how to solve them in such a scenario. Up
to now, the main approach is to explicitly construct an ordinal embedding of
the data points in the Euclidean space, an approach that has a number of
drawbacks. In this paper, we propose algorithms for the problems of medoid
estimation, outlier identification, classification, and clustering when given
only ordinal data. They are based on estimating the lens depth function and the
$k$-relative neighborhood graph on a data set. Our algorithms are simple, are
much faster than an ordinal embedding approach and avoid some of its drawbacks,
and can easily be parallelized.","['Matthäus Kleindessner', 'Ulrike von Luxburg']","['stat.ML', 'cs.DS', 'cs.LG']",2016-02-23 15:30:46+00:00
http://arxiv.org/abs/1602.07120v3,Submodular Learning and Covering with Response-Dependent Costs,"We consider interactive learning and covering problems, in a setting where
actions may incur different costs, depending on the response to the action. We
propose a natural greedy algorithm for response-dependent costs. We bound the
approximation factor of this greedy algorithm in active learning settings as
well as in the general setting. We show that a different property of the cost
function controls the approximation factor in each of these scenarios. We
further show that in both settings, the approximation factor of this greedy
algorithm is near-optimal among all greedy algorithms. Experiments demonstrate
the advantages of the proposed algorithm in the response-dependent cost
setting.",['Sivan Sabato'],"['cs.LG', 'stat.ML']",2016-02-23 11:20:37+00:00
http://arxiv.org/abs/1602.07109v5,Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series,"Approximate variational inference has shown to be a powerful tool for
modeling unknown complex probability distributions. Recent advances in the
field allow us to learn probabilistic models of sequences that actively exploit
spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN)
to learn robot time series data. Our evaluation demonstrates that we can
robustly detect anomalies both off- and on-line.","['Maximilian Soelch', 'Justin Bayer', 'Marvin Ludersdorfer', 'Patrick van der Smagt']","['stat.ML', 'cs.LG']",2016-02-23 10:31:51+00:00
http://arxiv.org/abs/1602.07107v1,A Streaming Algorithm for Crowdsourced Data Classification,"We propose a streaming algorithm for the binary classification of data based
on crowdsourcing. The algorithm learns the competence of each labeller by
comparing her labels to those of other labellers on the same tasks and uses
this information to minimize the prediction error rate on each task. We provide
performance guarantees of our algorithm for a fixed population of independent
labellers. In particular, we show that our algorithm is optimal in the sense
that the cumulative regret compared to the optimal decision with known labeller
error probabilities is finite, independently of the number of tasks to label.
The complexity of the algorithm is linear in the number of labellers and the
number of tasks, up to some logarithmic factors. Numerical experiments
illustrate the performance of our algorithm compared to existing algorithms,
including simple majority voting and expectation-maximization algorithms, on
both synthetic and real datasets.","['Thomas Bonald', 'Richard Combes']","['stat.ML', 'cs.LG']",2016-02-23 10:21:58+00:00
http://arxiv.org/abs/1602.07046v1,An Improved Gap-Dependency Analysis of the Noisy Power Method,"We consider the noisy power method algorithm, which has wide applications in
machine learning and statistics, especially those related to principal
component analysis (PCA) under resource (communication, memory or privacy)
constraints. Existing analysis of the noisy power method shows an
unsatisfactory dependency over the ""consecutive"" spectral gap
$(\sigma_k-\sigma_{k+1})$ of an input data matrix, which could be very small
and hence limits the algorithm's applicability. In this paper, we present a new
analysis of the noisy power method that achieves improved gap dependency for
both sample complexity and noise tolerance bounds. More specifically, we
improve the dependency over $(\sigma_k-\sigma_{k+1})$ to dependency over
$(\sigma_k-\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter and
could be much larger than the target rank $k$. Our proofs are built upon a
novel characterization of proximity between two subspaces that differ from
canonical angle characterizations analyzed in previous works. Finally, we apply
our improved bounds to distributed private PCA and memory-efficient streaming
PCA and obtain bounds that are superior to existing results in the literature.","['Maria Florina Balcan', 'Simon S. Du', 'Yining Wang', 'Adams Wei Yu']","['stat.ML', 'cs.LG', 'math.NA']",2016-02-23 05:15:08+00:00
http://arxiv.org/abs/1602.07043v2,Auditing Black-box Models for Indirect Influence,"Data-trained predictive models see widespread use, but for the most part they
are used as black boxes which output a prediction or score. It is therefore
hard to acquire a deeper understanding of model behavior, and in particular how
different features influence the model prediction. This is important when
interpreting the behavior of complex models, or asserting that certain
problematic attributes (like race or gender) are not unduly influencing
decisions.
  In this paper, we present a technique for auditing black-box models, which
lets us study the extent to which existing models take advantage of particular
features in the dataset, without knowing how the models work. Our work focuses
on the problem of indirect influence: how some features might indirectly
influence outcomes via other, related features. As a result, we can find
attribute influences even in cases where, upon further direct examination of
the model, the attribute is not referred to by the model at all.
  Our approach does not require the black-box model to be retrained. This is
important if (for example) the model is only accessible via an API, and
contrasts our work with other methods that investigate feature influence like
feature selection. We present experimental evidence for the effectiveness of
our procedure using a variety of publicly available datasets and models. We
also validate our procedure using techniques from interpretable learning and
feature selection, as well as against other black-box auditing procedures.","['Philip Adler', 'Casey Falk', 'Sorelle A. Friedler', 'Gabriel Rybeck', 'Carlos Scheidegger', 'Brandon Smith', 'Suresh Venkatasubramanian']","['stat.ML', 'cs.LG']",2016-02-23 04:52:28+00:00
http://arxiv.org/abs/1602.06989v1,Recovering the number of clusters in data sets with noise features using feature rescaling factors,"In this paper we introduce three methods for re-scaling data sets aiming at
improving the likelihood of clustering validity indexes to return the true
number of spherical Gaussian clusters with additional noise features. Our
method obtains feature re-scaling factors taking into account the structure of
a given data set and the intuitive idea that different features may have
different degrees of relevance at different clusters.
  We experiment with the Silhouette (using squared Euclidean, Manhattan, and
the p$^{th}$ power of the Minkowski distance), Dunn's, Calinski-Harabasz and
Hartigan indexes on data sets with spherical Gaussian clusters with and without
noise features. We conclude that our methods indeed increase the chances of
estimating the true number of clusters in a data set.","['Renato Cordeiro de Amorim', 'Christian Hennig']","['stat.ML', 'cs.LG']",2016-02-22 22:40:00+00:00
http://arxiv.org/abs/1602.06929v2,Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm,"This work provides improved guarantees for streaming principle component
analysis (PCA). Given $A_1, \ldots, A_n\in \mathbb{R}^{d\times d}$ sampled
independently from distributions satisfying $\mathbb{E}[A_i] = \Sigma$ for
$\Sigma \succeq \mathbf{0}$, this work provides an $O(d)$-space linear-time
single-pass streaming algorithm for estimating the top eigenvector of $\Sigma$.
The algorithm nearly matches (and in certain cases improves upon) the accuracy
obtained by the standard batch method that computes top eigenvector of the
empirical covariance $\frac{1}{n} \sum_{i \in [n]} A_i$ as analyzed by the
matrix Bernstein inequality. Moreover, to achieve constant accuracy, our
algorithm improves upon the best previous known sample complexities of
streaming algorithms by either a multiplicative factor of $O(d)$ or
$1/\mathrm{gap}$ where $\mathrm{gap}$ is the relative distance between the top
two eigenvalues of $\Sigma$.
  These results are achieved through a novel analysis of the classic Oja's
algorithm, one of the oldest and most popular algorithms for streaming PCA. In
particular, this work shows that simply picking a random initial point $w_0$
and applying the update rule $w_{i + 1} = w_i + \eta_i A_i w_i$ suffices to
accurately estimate the top eigenvector, with a suitable choice of $\eta_i$. We
believe our result sheds light on how to efficiently perform streaming PCA both
in theory and in practice and we hope that our analysis may serve as the basis
for analyzing many variants and extensions of streaming PCA.","['Prateek Jain', 'Chi Jin', 'Sham M. Kakade', 'Praneeth Netrapalli', 'Aaron Sidford']","['cs.LG', 'cs.DS', 'cs.NE', 'stat.ML']",2016-02-22 20:30:37+00:00
http://arxiv.org/abs/1602.06916v2,Sparse Linear Regression via Generalized Orthogonal Least-Squares,"Sparse linear regression, which entails finding a sparse solution to an
underdetermined system of linear equations, can formally be expressed as an
$l_0$-constrained least-squares problem. The Orthogonal Least-Squares (OLS)
algorithm sequentially selects the features (i.e., columns of the coefficient
matrix) to greedily find an approximate sparse solution. In this paper, a
generalization of Orthogonal Least-Squares which relies on a recursive relation
between the components of the optimal solution to select L features at each
step and solve the resulting overdetermined system of equations is proposed.
Simulation results demonstrate that the generalized OLS algorithm is
computationally efficient and achieves performance superior to that of existing
greedy algorithms broadly used in the literature.","['Abolfazl Hashemi', 'Haris Vikalo']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2016-02-22 19:55:01+00:00
http://arxiv.org/abs/1602.06886v2,Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation,"A good clustering can help a data analyst to explore and understand a data
set, but what constitutes a good clustering may depend on domain-specific and
application-specific criteria. These criteria can be difficult to formalize,
even when it is easy for an analyst to know a good clustering when she sees
one. We present a new approach to interactive clustering for data exploration,
called \ciif, based on a particularly simple feedback mechanism, in which an
analyst can choose to reject individual clusters and request new ones. The new
clusters should be different from previously rejected clusters while still
fitting the data well. We formalize this interaction in a novel Bayesian prior
elicitation framework. In each iteration, the prior is adapted to account for
all the previous feedback, and a new clustering is then produced from the
posterior distribution. To achieve the computational efficiency necessary for
an interactive setting, we propose an incremental optimization method over data
minibatches using Lagrangian relaxation. Experiments demonstrate that \ciif can
produce accurate and diverse clusterings.","['Akash Srivastava', 'James Zou', 'Charles Sutton']","['stat.ML', 'cs.LG']",2016-02-22 18:38:27+00:00
http://arxiv.org/abs/1602.06872v2,Principal Component Projection Without Principal Component Analysis,"We show how to efficiently project a vector onto the top principal components
of a matrix, without explicitly computing these components. Specifically, we
introduce an iterative algorithm that provably computes the projection using
few calls to any black-box routine for ridge regression.
  By avoiding explicit principal component analysis (PCA), our algorithm is the
first with no runtime dependence on the number of top principal components. We
show that it can be used to give a fast iterative method for the popular
principal component regression problem, giving the first major runtime
improvement over the naive method of combining PCA with regression.
  To achieve our results, we first observe that ridge regression can be used to
obtain a ""smooth projection"" onto the top principal components. We then sharpen
this approximation to true projection using a low-degree polynomial
approximation to the matrix step function. Step function approximation is a
topic of long-term interest in scientific computing. We extend prior theory by
constructing polynomials with simple iterative structure and rigorously
analyzing their behavior under limited precision.","['Roy Frostig', 'Cameron Musco', 'Christopher Musco', 'Aaron Sidford']","['cs.DS', 'cs.LG', 'stat.ML']",2016-02-22 17:52:02+00:00
http://arxiv.org/abs/1602.06746v1,Convexification of Learning from Constraints,"Regularized empirical risk minimization with constrained labels (in contrast
to fixed labels) is a remarkably general abstraction of learning. For common
loss and regularization functions, this optimization problem assumes the form
of a mixed integer program (MIP) whose objective function is non-convex. In
this form, the problem is resistant to standard optimization techniques. We
construct MIPs with the same solutions whose objective functions are convex.
Specifically, we characterize the tightest convex extension of the objective
function, given by the Legendre-Fenchel biconjugate. Computing values of this
tightest convex extension is NP-hard. However, by applying our characterization
to every function in an additive decomposition of the objective function, we
obtain a class of looser convex extensions that can be computed efficiently.
For some decompositions, common loss and regularization functions, we derive a
closed form.","['Iaroslav Shcherbatyi', 'Bjoern Andres']","['cs.LG', 'math.OC', 'stat.ML']",2016-02-22 12:20:50+00:00
http://arxiv.org/abs/1602.06725v2,Variational inference for Monte Carlo objectives,"Recent progress in deep latent variable models has largely been driven by the
development of flexible and scalable variational inference methods. Variational
training of this type involves maximizing a lower bound on the log-likelihood,
using samples from the variational posterior to compute the required gradients.
Recently, Burda et al. (2016) have derived a tighter lower bound using a
multi-sample importance sampling estimate of the likelihood and showed that
optimizing it yields models that use more of their capacity and achieve higher
likelihoods. This development showed the importance of such multi-sample
objectives and explained the success of several related approaches.
  We extend the multi-sample approach to discrete latent variables and analyze
the difficulty encountered when estimating the gradients involved. We then
develop the first unbiased gradient estimator designed for importance-sampled
objectives and evaluate it at training generative and structured output
prediction models. The resulting estimator, which is based on low-variance
per-sample learning signals, is both simpler and more effective than the NVIL
estimator proposed for the single-sample variational objective, and is
competitive with the currently used biased estimators.","['Andriy Mnih', 'Danilo J. Rezende']","['cs.LG', 'stat.ML']",2016-02-22 11:06:06+00:00
http://arxiv.org/abs/1602.06701v2,Inference Networks for Sequential Monte Carlo in Graphical Models,"We introduce a new approach for amortizing inference in directed graphical
models by learning heuristic approximations to stochastic inverses, designed
specifically for use as proposal distributions in sequential Monte Carlo
methods. We describe a procedure for constructing and learning a structured
neural network which represents an inverse factorization of the graphical
model, resulting in a conditional density estimator that takes as input
particular values of the observed random variables, and returns an
approximation to the distribution of the latent variables. This recognition
model can be learned offline, independent from any particular dataset, prior to
performing inference. The output of these networks can be used as
automatically-learned high-quality proposal distributions to accelerate
sequential Monte Carlo across a diverse range of problem settings.","['Brooks Paige', 'Frank Wood']",['stat.ML'],2016-02-22 09:39:09+00:00
http://arxiv.org/abs/1602.06693v2,Preconditioning Kernel Matrices,"The computational and storage complexity of kernel machines presents the
primary barrier to their scaling to large, modern, datasets. A common way to
tackle the scalability issue is to use the conjugate gradient algorithm, which
relieves the constraints on both storage (the kernel matrix need not be stored)
and computation (both stochastic gradients and parallelization can be used).
Even so, conjugate gradient is not without its own issues: the conditioning of
kernel matrices is often such that conjugate gradients will have poor
convergence in practice. Preconditioning is a common approach to alleviating
this issue. Here we propose preconditioned conjugate gradients for kernel
machines, and develop a broad range of preconditioners particularly useful for
kernel matrices. We describe a scalable approach to both solving kernel
machines and learning their hyperparameters. We show this approach is exact in
the limit of iterations and outperforms state-of-the-art approximations for a
given computational budget.","['Kurt Cutajar', 'Michael A. Osborne', 'John P. Cunningham', 'Maurizio Filippone']","['stat.ML', 'stat.CO', 'stat.ME']",2016-02-22 09:21:48+00:00
http://arxiv.org/abs/1602.06687v1,An Effective and Efficient Approach for Clusterability Evaluation,"Clustering is an essential data mining tool that aims to discover inherent
cluster structure in data. As such, the study of clusterability, which
evaluates whether data possesses such structure, is an integral part of cluster
analysis. Yet, despite their central role in the theory and application of
clustering, current notions of clusterability fall short in two crucial aspects
that render them impractical; most are computationally infeasible and others
fail to classify the structure of real datasets.
  In this paper, we propose a novel approach to clusterability evaluation that
is both computationally efficient and successfully captures the structure of
real data. Our method applies multimodality tests to the (one-dimensional) set
of pairwise distances based on the original, potentially high-dimensional data.
We present extensive analyses of our approach for both the Dip and Silverman
multimodality tests on real data as well as 17,000 simulations, demonstrating
the success of our approach as the first practical notion of clusterability.","['Margareta Ackerman', 'Andreas Adolfsson', 'Naomi Brownstein']","['cs.LG', 'stat.ML']",2016-02-22 09:01:10+00:00
http://arxiv.org/abs/1602.06664v3,A Geometric Analysis of Phase Retrieval,"Can we recover a complex signal from its Fourier magnitudes? More generally,
given a set of $m$ measurements, $y_k = |\mathbf a_k^* \mathbf x|$ for $k = 1,
\dots, m$, is it possible to recover $\mathbf x \in \mathbb{C}^n$ (i.e.,
length-$n$ complex vector)? This **generalized phase retrieval** (GPR) problem
is a fundamental task in various disciplines, and has been the subject of much
recent investigation. Natural nonconvex heuristics often work remarkably well
for GPR in practice, but lack clear theoretical explanations. In this paper, we
take a step towards bridging this gap. We prove that when the measurement
vectors $\mathbf a_k$'s are generic (i.i.d. complex Gaussian) and the number of
measurements is large enough ($m \ge C n \log^3 n$), with high probability, a
natural least-squares formulation for GPR has the following benign geometric
structure: (1) there are no spurious local minimizers, and all global
minimizers are equal to the target signal $\mathbf x$, up to a global phase;
and (2) the objective function has a negative curvature around each saddle
point. This structure allows a number of iterative optimization methods to
efficiently find a global minimizer, without special initialization. To
corroborate the claim, we describe and analyze a second-order trust-region
algorithm.","['Ju Sun', 'Qing Qu', 'John Wright']","['cs.IT', 'math.IT', 'math.OC', 'stat.ML']",2016-02-22 06:54:07+00:00
http://arxiv.org/abs/1602.06662v2,Recurrent Orthogonal Networks and Long-Memory Tasks,"Although RNNs have been shown to be powerful tools for processing sequential
data, finding architectures or optimization strategies that allow them to model
very long term dependencies is still an active area of research. In this work,
we carefully analyze two synthetic datasets originally outlined in (Hochreiter
and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store
information over many time steps. We explicitly construct RNN solutions to
these problems, and using these constructions, illuminate both the problems
themselves and the way in which RNNs store different types of information in
their hidden states. These constructions furthermore explain the success of
recent methods that specify unitary initializations or constraints on the
transition matrices.","['Mikael Henaff', 'Arthur Szlam', 'Yann LeCun']","['cs.NE', 'cs.AI', 'cs.LG', 'stat.ML']",2016-02-22 06:51:25+00:00
http://arxiv.org/abs/1602.06632v3,Denoising and Covariance Estimation of Single Particle Cryo-EM Images,"The problem of image restoration in cryo-EM entails correcting for the
effects of the Contrast Transfer Function (CTF) and noise. Popular methods for
image restoration include `phase flipping', which corrects only for the Fourier
phases but not amplitudes, and Wiener filtering, which requires the spectral
signal to noise ratio. We propose a new image restoration method which we call
`Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of the
projection images is used within the classical Wiener filtering framework for
solving the image restoration deconvolution problem. Our estimation procedure
for the covariance matrix is new and successfully corrects for the CTF. We
demonstrate the efficacy of CWF by applying it to restore both simulated and
experimental cryo-EM images. Results with experimental datasets demonstrate
that CWF provides a good way to evaluate the particle images and to see what
the dataset contains even without 2D classification and averaging.","['Tejal Bhamre', 'Teng Zhang', 'Amit Singer']","['cs.CV', 'q-bio.BM', 'stat.ML']",2016-02-22 03:04:44+00:00
http://arxiv.org/abs/1602.06612v2,Clustering subgaussian mixtures by semidefinite programming,"We introduce a model-free relax-and-round algorithm for k-means clustering
based on a semidefinite relaxation due to Peng and Wei. The algorithm
interprets the SDP output as a denoised version of the original data and then
rounds this output to a hard clustering. We provide a generic method for
proving performance guarantees for this algorithm, and we analyze the algorithm
in the context of subgaussian mixture models. We also study the fundamental
limits of estimating Gaussian centers by k-means clustering in order to compare
our approximation guarantee to the theoretically optimal k-means clustering
solution.","['Dustin G. Mixon', 'Soledad Villar', 'Rachel Ward']","['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",2016-02-22 00:19:20+00:00
http://arxiv.org/abs/1602.06606v2,Estimating Structured Vector Autoregressive Model,"While considerable advances have been made in estimating high-dimensional
structured models from independent data using Lasso-type models, limited
progress has been made for settings when the samples are dependent. We consider
estimating structured VAR (vector auto-regressive models), where the structure
can be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted
Lasso, sparse group Lasso, etc. In VAR setting with correlated noise, although
there is strong dependence over time and covariates, we establish bounds on the
non-asymptotic estimation error of structured VAR parameters. Surprisingly, the
estimation error is of the same order as that of the corresponding Lasso-type
estimator with independent samples, and the analysis holds for any norm. Our
analysis relies on results in generic chaining, sub-exponential martingales,
and spectral representation of VAR models. Experimental results on synthetic
data with a variety of structures as well as real aviation data are presented,
validating theoretical results.","['Igor Melnyk', 'Arindam Banerjee']","['math.ST', 'stat.ML', 'stat.TH']",2016-02-21 23:47:36+00:00
http://arxiv.org/abs/1602.06577v1,"2-Bit Random Projections, NonLinear Estimators, and Approximate Near Neighbor Search","The method of random projections has become a standard tool for machine
learning, data mining, and search with massive data at Web scale. The effective
use of random projections requires efficient coding schemes for quantizing
(real-valued) projected data into integers. In this paper, we focus on a simple
2-bit coding scheme. In particular, we develop accurate nonlinear estimators of
data similarity based on the 2-bit strategy. This work will have important
practical applications. For example, in the task of near neighbor search, a
crucial step (often called re-ranking) is to compute or estimate data
similarities once a set of candidate data points have been identified by hash
table techniques. This re-ranking step can take advantage of the proposed
coding scheme and estimator.
  As a related task, in this paper, we also study a simple uniform quantization
scheme for the purpose of building hash tables with projected data. Our
analysis shows that typically only a small number of bits are needed. For
example, when the target similarity level is high, 2 or 3 bits might be
sufficient. When the target similarity level is not so high, it is preferable
to use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good
choice for the task of sublinear time approximate near neighbor search via hash
tables.
  Combining these results, we conclude that 2-bit random projections should be
recommended for approximate near neighbor search and similarity estimation.
Extensive experimental results are provided.","['Ping Li', 'Michael Mitzenmacher', 'Anshumali Shrivastava']","['stat.ML', 'cs.DS', 'cs.LG']",2016-02-21 20:46:13+00:00
http://arxiv.org/abs/1602.06566v1,Interactive Storytelling over Document Collections,"Storytelling algorithms aim to 'connect the dots' between disparate documents
by linking starting and ending documents through a series of intermediate
documents. Existing storytelling algorithms are based on notions of coherence
and connectivity, and thus the primary way by which users can steer the story
construction is via design of suitable similarity functions. We present an
alternative approach to storytelling wherein the user can interactively and
iteratively provide 'must use' constraints to preferentially support the
construction of some stories over others. The three innovations in our approach
are distance measures based on (inferred) topic distributions, the use of
constraints to define sets of linear inequalities over paths, and the
introduction of slack and surplus variables to condition the topic distribution
to preferentially emphasize desired terms over others. We describe experimental
results to illustrate the effectiveness of our interactive storytelling
approach over multiple text datasets.","['Dipayan Maiti', 'Mohammad Raihanul Islam', 'Scotland Leman', 'Naren Ramakrishnan']","['cs.AI', 'cs.LG', 'stat.ML']",2016-02-21 18:46:35+00:00
http://arxiv.org/abs/1602.06550v2,Semi-Markov Switching Vector Autoregressive Model-based Anomaly Detection in Aviation Systems,"In this work we consider the problem of anomaly detection in heterogeneous,
multivariate, variable-length time series datasets. Our focus is on the
aviation safety domain, where data objects are flights and time series are
sensor readings and pilot switches. In this context the goal is to detect
anomalous flight segments, due to mechanical, environmental, or human factors
in order to identifying operationally significant events and provide insights
into the flight operations and highlight otherwise unavailable potential safety
risks and precursors to accidents. For this purpose, we propose a framework
which represents each flight using a semi-Markov switching vector
autoregressive (SMS-VAR) model. Detection of anomalies is then based on
measuring dissimilarities between the model's prediction and data observation.
The framework is scalable, due to the inherent parallel nature of most
computations, and can be used to perform online anomaly detection. Extensive
experimental results on simulated and real datasets illustrate that the
framework can detect various types of anomalies along with the key parameters
involved.","['Igor Melnyk', 'Arindam Banerjee', 'Bryan Matthews', 'Nikunj Oza']","['cs.LG', 'stat.AP', 'stat.ML']",2016-02-21 16:55:36+00:00
http://arxiv.org/abs/1602.06531v2,Multi-task and Lifelong Learning of Kernels,"We consider a problem of learning kernels for use in SVM classification in
the multi-task and lifelong scenarios and provide generalization bounds on the
error of a large margin classifier. Our results show that, under mild
conditions on the family of kernels used for learning, solving several related
tasks simultaneously is beneficial over single task learning. In particular, as
the number of observed tasks grows, assuming that in the considered family of
kernels there exists one that yields low approximation error on all tasks, the
overhead associated with learning such a kernel vanishes and the complexity
converges to that of learning when this good kernel is given to the learner.","['Anastasia Pentina', 'Shai Ben-David']","['stat.ML', 'cs.LG']",2016-02-21 14:05:48+00:00
http://arxiv.org/abs/1602.06518v4,Multi-Task Learning with Labeled and Unlabeled Tasks,"In multi-task learning, a learner is given a collection of prediction tasks
and needs to solve all of them. In contrast to previous work, which required
that annotated training data is available for all tasks, we consider a new
setting, in which for some tasks, potentially most of them, only unlabeled
training data is provided. Consequently, to solve all tasks, information must
be transferred between tasks with labels and tasks without labels. Focusing on
an instance-based transfer method we analyze two variants of this setting: when
the set of labeled tasks is fixed, and when it can be actively selected by the
learner. We state and prove a generalization bound that covers both scenarios
and derive from it an algorithm for making the choice of labeled tasks (in the
active case) and for transferring information between the tasks in a principled
way. We also illustrate the effectiveness of the algorithm by experiments on
synthetic and real data.","['Anastasia Pentina', 'Christoph H. Lampert']","['stat.ML', 'cs.LG']",2016-02-21 11:18:10+00:00
http://arxiv.org/abs/1602.06516v4,Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques,"In a series of recent works, we have generalised the consistency results in
the stochastic block model literature to the case of uniform and non-uniform
hypergraphs. The present paper continues the same line of study, where we focus
on partitioning weighted uniform hypergraphs---a problem often encountered in
computer vision. This work is motivated by two issues that arise when a
hypergraph partitioning approach is used to tackle computer vision problems:
(i) The uniform hypergraphs constructed for higher-order learning contain all
edges, but most have negligible weights. Thus, the adjacency tensor is nearly
sparse, and yet, not binary. (ii) A more serious concern is that standard
partitioning algorithms need to compute all edge weights, which is
computationally expensive for hypergraphs. This is usually resolved in practice
by merging the clustering algorithm with a tensor sampling strategy---an
approach that is yet to be analysed rigorously. We build on our earlier work on
partitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati,
ICML, 2015), and address the aforementioned issues by proposing provable and
efficient partitioning algorithms. Our analysis justifies the empirical success
of practical sampling techniques. We also complement our theoretical findings
by elaborate empirical comparison of various hypergraph partitioning schemes.","['Debarghya Ghoshdastidar', 'Ambedkar Dukkipati']","['cs.LG', 'stat.ML']",2016-02-21 10:52:42+00:00
http://arxiv.org/abs/1602.06431v1,Burstiness Scale: a highly parsimonious model for characterizing random series of events,"The problem to accurately and parsimoniously characterize random series of
events (RSEs) present in the Web, such as e-mail conversations or Twitter
hashtags, is not trivial. Reports found in the literature reveal two apparent
conflicting visions of how RSEs should be modeled. From one side, the
Poissonian processes, of which consecutive events follow each other at a
relatively regular time and should not be correlated. On the other side, the
self-exciting processes, which are able to generate bursts of correlated events
and periods of inactivities. The existence of many and sometimes conflicting
approaches to model RSEs is a consequence of the unpredictability of the
aggregated dynamics of our individual and routine activities, which sometimes
show simple patterns, but sometimes results in irregular rising and falling
trends. In this paper we propose a highly parsimonious way to characterize
general RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSE
as a mix of two independent process: a Poissonian and a self-exciting one. Here
we describe a fast method to extract the two parameters of BuSca that,
together, gives the burstyness scale, which represents how much of the RSE is
due to bursty and viral effects. We validated our method in eight diverse and
large datasets containing real random series of events seen in Twitter, Yelp,
e-mail conversations, Digg, and online forums. Results showed that, even using
only two parameters, BuSca is able to accurately describe RSEs seen in these
diverse systems, what can leverage many applications.","['Rodrigo A S Alves', 'Renato Assunção', 'Pedro O S Vaz de Melo']","['stat.ML', 'cs.SI', 'H.2.8; G.3']",2016-02-20 16:47:10+00:00
http://arxiv.org/abs/1602.06429v3,Generalized Statistical Tests for mRNA and Protein Subcellular Spatial Patterning against Complete Spatial Randomness,"We derive generalized estimators for a number of spatial statistics that have
been used in the analysis of spatially resolved omics data, such as Ripley's K,
H and L functions, clustering index, and degree of clustering, which allow
these statistics to be calculated on data modelled by arbitrary random measures
(RMs). Our estimators generalize those typically used to calculate these
statistics on point process data, allowing them to be calculated on RMs which
assign continuous values to spatial regions, for instance to model protein
intensity. The clustering index (H*) compares Ripley's H function calculated
empirically to its distribution under complete spatial randomness (CSR),
leading us to consider CSR null hypotheses for RMs which are not
point-processes when generalizing this statistic. We thus consider restricted
classes of completely random measures which can be simulated directly (Gamma
processes and Marked Poisson Processes), as well as the general class of all
CSR RMs, for which we derive an exact permutation-based H* estimator. We
establish several properties of the estimators, including bounds on the
accuracy of our general Ripley K estimator, its relationship to a previous
estimator for the cross-correlation measure, and the relationship of our
generalized H* estimator to previous statistics. To test the ability of our
approach to identify spatial patterning, we use Fluorescent In Situ
Hybridization (FISH) and Immunofluorescence (IF) data to probe for mRNA and
protein subcellular localization patterns respectively in polarizing mouse
fibroblasts on micropattened cells. We observe correlated patterns of
clustering over time for corresponding mRNAs and proteins, suggesting a
deterministic effect of mRNA localization on protein localization for several
pairs tested, including one case in which spatial patterning at the mRNA level
has not been previously demonstrated.","['Jonathan H. Warrell', 'Anca F. Savulescu', 'Robyn Brackin', 'Musa M. Mhlanga']","['stat.ML', 'q-bio.QM', 'stat.AP']",2016-02-20 16:31:18+00:00
http://arxiv.org/abs/1602.06410v2,Semidefinite Programs for Exact Recovery of a Hidden Community,"We study a semidefinite programming (SDP) relaxation of the maximum
likelihood estimation for exactly recovering a hidden community of cardinality
$K$ from an $n \times n$ symmetric data matrix $A$, where for distinct indices
$i,j$, $A_{ij} \sim P$ if $i, j$ are both in the community and $A_{ij} \sim Q$
otherwise, for two known probability distributions $P$ and $Q$. We identify a
sufficient condition and a necessary condition for the success of SDP for the
general model. For both the Bernoulli case ($P={{\rm Bern}}(p)$ and $Q={{\rm
Bern}}(q)$ with $p>q$) and the Gaussian case ($P=\mathcal{N}(\mu,1)$ and
$Q=\mathcal{N}(0,1)$ with $\mu>0$), which correspond to the problem of planted
dense subgraph recovery and submatrix localization respectively, the general
results lead to the following findings: (1) If $K=\omega( n /\log n)$, SDP
attains the information-theoretic recovery limits with sharp constants; (2) If
$K=\Theta(n/\log n)$, SDP is order-wise optimal, but strictly suboptimal by a
constant factor; (3) If $K=o(n/\log n)$ and $K \to \infty$, SDP is order-wise
suboptimal. The same critical scaling for $K$ is found to hold, up to constant
factors, for the performance of SDP on the stochastic block model of $n$
vertices partitioned into multiple communities of equal size $K$. A key
ingredient in the proof of the necessary condition is a construction of a
primal feasible solution based on random perturbation of the true cluster
matrix.","['Bruce Hajek', 'Yihong Wu', 'Jiaming Xu']","['stat.ML', 'cs.IT', 'cs.SI', 'math.IT', 'math.ST', 'stat.TH']",2016-02-20 14:15:59+00:00
http://arxiv.org/abs/1602.06349v1,"The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM","We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markov
model (iHMM) that supports a simple, efficient inference scheme. The siHMM is
well suited to segmentation problems, where the goal is to identify points at
which a time series transitions from one relatively stable regime to a new
regime. Conventional iHMMs often struggle with such problems, since they have
no mechanism for distinguishing between high- and low-level dynamics.
Hierarchical HMMs (HHMMs) can do better, but they require much more complex and
expensive inference algorithms. The siHMM retains the simplicity and efficiency
of the iHMM, but outperforms it on a variety of segmentation problems,
achieving performance that matches or exceeds that of a more complicated HHMM.","['Ardavan Saeedi', 'Matthew Hoffman', 'Matthew Johnson', 'Ryan Adams']",['stat.ML'],2016-02-20 00:30:03+00:00
http://arxiv.org/abs/1602.06346v2,Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models,"In this paper we study a model-based approach to calculating approximately
optimal policies in Markovian Decision Processes. In particular, we derive
novel bounds on the loss of using a policy derived from a factored linear
model, a class of models which generalize numerous previous models out of those
that come with strong computational guarantees. For the first time in the
literature, we derive performance bounds for model-based techniques where the
model inaccuracy is measured in weighted norms. Moreover, our bounds show a
decreased sensitivity to the discount factor and, unlike similar bounds derived
for other approaches, they are insensitive to measure mismatch. Similarly to
previous works, our proofs are also based on contraction arguments, but with
the main differences that we use carefully constructed norms building on Banach
lattices, and the contraction property is only assumed for operators acting on
""compressed"" spaces, thus weakening previous assumptions, while strengthening
previous results.","['Bernardo Ávila Pires', 'Csaba Szepesvári']","['stat.ML', 'cs.LG']",2016-02-19 23:46:11+00:00
http://arxiv.org/abs/1602.06276v1,Semi-parametric Order-based Generalized Multivariate Regression,"In this paper, we consider a generalized multivariate regression problem
where the responses are monotonic functions of linear transformations of
predictors. We propose a semi-parametric algorithm based on the ordering of the
responses which is invariant to the functional form of the transformation
function. We prove that our algorithm, which maximizes the rank correlation of
responses and linear transformations of predictors, is a consistent estimator
of the true coefficient matrix. We also identify the rate of convergence and
show that the squared estimation error decays with a rate of $o(1/\sqrt{n})$.
We then propose a greedy algorithm to maximize the highly non-smooth objective
function of our model and examine its performance through extensive
simulations. Finally, we compare our algorithm with traditional multivariate
regression algorithms over synthetic and real data.","['Milad Kharratzadeh', 'Mark Coates']","['stat.ML', 'math.ST', 'stat.TH']",2016-02-19 20:20:56+00:00
http://arxiv.org/abs/1602.06235v1,A Mutual Contamination Analysis of Mixed Membership and Partial Label Models,"Many machine learning problems can be characterized by mutual contamination
models. In these problems, one observes several random samples from different
convex combinations of a set of unknown base distributions. It is of interest
to decontaminate mutual contamination models, i.e., to recover the base
distributions either exactly or up to a permutation. This paper considers the
general setting where the base distributions are defined on arbitrary
probability spaces. We examine the decontamination problem in two mutual
contamination models that describe popular machine learning tasks: recovering
the base distributions up to a permutation in a mixed membership model, and
recovering the base distributions exactly in a partial label model for
classification. We give necessary and sufficient conditions for identifiability
of both mutual contamination models, algorithms for both problems in the
infinite and finite sample cases, and introduce novel proof techniques based on
affine geometry.","['Julian Katz-Samuels', 'Clayton Scott']",['stat.ML'],2016-02-19 17:40:58+00:00
http://arxiv.org/abs/1602.06225v1,GAP Safe Screening Rules for Sparse-Group-Lasso,"In high dimensional settings, sparse structures are crucial for efficiency,
either in term of memory, computation or performance. In some contexts, it is
natural to handle more refined structures than pure sparsity, such as for
instance group sparsity. Sparse-Group Lasso has recently been introduced in the
context of linear regression to enforce sparsity both at the feature level and
at the group level. We adapt to the case of Sparse-Group Lasso recent safe
screening rules that discard early in the solver irrelevant features/groups.
Such rules have led to important speed-ups for a wide range of iterative
methods. Thanks to dual gap computations, we provide new safe screening rules
for Sparse-Group Lasso and show significant gains in term of computing time for
a coordinate descent implementation.","['Eugene Ndiaye', 'Olivier Fercoq', 'Alexandre Gramfort', 'Joseph Salmon']","['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",2016-02-19 17:08:34+00:00
http://arxiv.org/abs/1602.06053v1,First-order Methods for Geodesically Convex Optimization,"Geodesic convexity generalizes the notion of (vector space) convexity to
nonlinear metric spaces. But unlike convex optimization, geodesically convex
(g-convex) optimization is much less developed. In this paper we contribute to
the understanding of g-convex optimization by developing iteration complexity
analysis for several first-order algorithms on Hadamard manifolds.
Specifically, we prove upper bounds for the global complexity of deterministic
and stochastic (sub)gradient methods for optimizing smooth and nonsmooth
g-convex functions, both with and without strong g-convexity. Our analysis also
reveals how the manifold geometry, especially \emph{sectional curvature},
impacts convergence rates. To the best of our knowledge, our work is the first
to provide global complexity analysis for first-order algorithms for general
g-convex optimization.","['Hongyi Zhang', 'Suvrit Sra']","['math.OC', 'cs.LG', 'stat.ML']",2016-02-19 06:56:50+00:00
http://arxiv.org/abs/1602.06049v1,Scaling up Dynamic Topic Models,"Dynamic topic models (DTMs) are very effective in discovering topics and
capturing their evolution trends in time series data. To do posterior inference
of DTMs, existing methods are all batch algorithms that scan the full dataset
before each update of the model and make inexact variational approximations
with mean-field assumptions. Due to a lack of a more scalable inference
algorithm, despite the usefulness, DTMs have not captured large topic dynamics.
  This paper fills this research void, and presents a fast and parallelizable
inference algorithm using Gibbs Sampling with Stochastic Gradient Langevin
Dynamics that does not make any unwarranted assumptions. We also present a
Metropolis-Hastings based $O(1)$ sampler for topic assignments for each word
token. In a distributed environment, our algorithm requires very little
communication between workers during sampling (almost embarrassingly parallel)
and scales up to large-scale applications. We are able to learn the largest
Dynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topics
from 2.6 million documents in less than half an hour, and our empirical results
show that our algorithm is not only orders of magnitude faster than the
baselines but also achieves lower perplexity.","['Arnab Bhadury', 'Jianfei Chen', 'Jun Zhu', 'Shixia Liu']","['stat.ML', 'H.4; G.3']",2016-02-19 05:55:08+00:00
http://arxiv.org/abs/1602.06042v2,Structured Sparse Regression via Greedy Hard-Thresholding,"Several learning applications require solving high-dimensional regression
problems where the relevant features belong to a small number of (overlapping)
groups. For very large datasets and under standard sparsity constraints, hard
thresholding methods have proven to be extremely efficient, but such methods
require NP hard projections when dealing with overlapping groups. In this
paper, we show that such NP-hard projections can not only be avoided by
appealing to submodular optimization, but such methods come with strong
theoretical guarantees even in the presence of poorly conditioned data (i.e.
say when two features have correlation $\geq 0.99$), which existing analyses
cannot handle. These methods exhibit an interesting computation-accuracy
trade-off and can be extended to significantly harder problems such as sparse
overlapping groups. Experiments on both real and synthetic data validate our
claims and demonstrate that the proposed methods are orders of magnitude faster
than other greedy and convex relaxation techniques for learning with
group-structured sparsity.","['Prateek Jain', 'Nikhil Rao', 'Inderjit Dhillon']","['stat.ML', 'cs.LG']",2016-02-19 04:28:50+00:00
http://arxiv.org/abs/1602.06025v1,Spectral Learning for Supervised Topic Models,"Supervised topic models simultaneously model the latent topic structure of
large collections of documents and a response variable associated with each
document. Existing inference methods are based on variational approximation or
Monte Carlo sampling, which often suffers from the local minimum defect.
Spectral methods have been applied to learn unsupervised topic models, such as
latent Dirichlet allocation (LDA), with provable guarantees. This paper
investigates the possibility of applying spectral methods to recover the
parameters of supervised LDA (sLDA). We first present a two-stage spectral
method, which recovers the parameters of LDA followed by a power update method
to recover the regression model parameters. Then, we further present a
single-phase spectral algorithm to jointly recover the topic distribution
matrix as well as the regression weights. Our spectral algorithms are provably
correct and computationally efficient. We prove a sample complexity bound for
each algorithm and subsequently derive a sufficient condition for the
identifiability of sLDA. Thorough experiments on synthetic and real-world
datasets verify the theory and demonstrate the practical effectiveness of the
spectral algorithms. In fact, our results on a large-scale review rating
dataset demonstrate that our single-phase spectral algorithm alone gets
comparable or even better performance than state-of-the-art methods, while
previous work on spectral methods has rarely reported such promising
performance.","['Yong Ren', 'Yining Wang', 'Jun Zhu']","['cs.LG', 'cs.CL', 'cs.IR', 'stat.ML']",2016-02-19 02:07:20+00:00
http://arxiv.org/abs/1602.05908v1,Efficient approaches for escaping higher order saddle points in non-convex optimization,"Local search heuristics for non-convex optimizations are popular in applied
machine learning. However, in general it is hard to guarantee that such
algorithms even converge to a local minimum, due to the existence of
complicated saddle point structures in high dimensions. Many functions have
degenerate saddle points such that the first and second order derivatives
cannot distinguish them with local optima. In this paper we use higher order
derivatives to escape these saddle points: we design the first efficient
algorithm guaranteed to converge to a third order local optimum (while existing
techniques are at most second order). We also show that it is NP-hard to extend
this further to finding fourth order local optima.","['Anima Anandkumar', 'Rong Ge']","['cs.LG', 'stat.ML']",2016-02-18 18:52:15+00:00
http://arxiv.org/abs/1602.05897v2,Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity,"We develop a general duality between neural networks and compositional
kernels, striving towards a better understanding of deep learning. We show that
initial representations generated by common random initializations are
sufficiently rich to express all functions in the dual kernel space. Hence,
though the training objective is hard to optimize in the worst case, the
initial weights form a good starting point for optimization. Our dual view also
reveals a pragmatic and aesthetic perspective of neural networks and
underscores their expressive power.","['Amit Daniely', 'Roy Frostig', 'Yoram Singer']","['cs.LG', 'cs.AI', 'cs.CC', 'cs.DS', 'stat.ML']",2016-02-18 18:14:19+00:00
http://arxiv.org/abs/1602.05875v3,Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data,"Traditional convolutional layers extract features from patches of data by
applying a non-linearity on an affine function of the input. We propose a model
that enhances this feature extraction process for the case of sequential data,
by feeding patches of the data into a recurrent neural network and using the
outputs or hidden states of the recurrent units to compute the extracted
features. By doing so, we exploit the fact that a window containing a few
frames of the sequential data is a sequence itself and this additional
structure might encapsulate valuable information. In addition, we allow for
more steps of computation in the feature extraction process, which is
potentially beneficial as an affine function followed by a non-linearity can
result in too simple features. Using our convolutional recurrent layers we
obtain an improvement in performance in two audio classification tasks,
compared to traditional convolutional layers. Tensorflow code for the
convolutional recurrent layers is publicly available in
https://github.com/cruvadom/Convolutional-RNN.","['Gil Keren', 'Björn Schuller']","['stat.ML', 'cs.CL']",2016-02-18 16:55:30+00:00
http://arxiv.org/abs/1602.05822v1,What is the distribution of the number of unique original items in a bootstrap sample?,"Sampling with replacement occurs in many settings in machine learning,
notably in the bagging ensemble technique and the .632+ validation scheme. The
number of unique original items in a bootstrap sample can have an important
role in the behaviour of prediction models learned on it. Indeed, there are
uncontrived examples where duplicate items have no effect. The purpose of this
report is to present the distribution of the number of unique original items in
a bootstrap sample clearly and concisely, with a view to enabling other machine
learning researchers to understand and control this quantity in existing and
future resampling techniques. We describe the key characteristics of this
distribution along with the generalisation for the case where items come from
distinct categories, as in classification. In both cases we discuss the normal
limit, and conduct an empirical investigation to derive a heuristic for when a
normal approximation is permissible.","['Alex F. Mendelson', 'Maria A. Zuluaga', 'Brian F. Hutton', 'Sébastien Ourselin']","['stat.ML', '62G09']",2016-02-18 14:56:47+00:00
http://arxiv.org/abs/1602.05702v4,EEG-informed attended speaker extraction from recorded speech mixtures with application in neuro-steered hearing prostheses,"OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy,
two-speaker acoustic scenario, relying on microphone array recordings from a
binaural hearing aid, which are complemented with electroencephalography (EEG)
recordings to infer the speaker of interest. METHODS: In this study, we propose
a modular processing flow that first extracts the two speech envelopes from the
microphone recordings, then selects the attended speech envelope based on the
EEG, and finally uses this envelope to inform a multi-channel speech separation
and denoising algorithm. RESULTS: Strong suppression of interfering
(unattended) speech and background noise is achieved, while the attended speech
is preserved. Furthermore, EEG-based auditory attention detection (AAD) is
shown to be robust to the use of noisy speech signals. CONCLUSIONS: Our results
show that AAD-based speaker extraction from microphone array recordings is
feasible and robust, even in noisy acoustic environments, and without access to
the clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Current
research on AAD always assumes the availability of the clean speech signals,
which limits the applicability in real settings. We have extended this research
to detect the attended speaker even when only microphone recordings with noisy
speech mixtures are available. This is an enabling ingredient for new
brain-computer interfaces and effective filtering schemes in neuro-steered
hearing prostheses. Here, we provide a first proof of concept for EEG-informed
attended speaker extraction and denoising.","['Simon Van Eyndhoven', 'Tom Francart', 'Alexander Bertrand']","['cs.SD', 'cs.SY', 'stat.ML']",2016-02-18 07:32:00+00:00
http://arxiv.org/abs/1602.05563v1,Robust Kernel (Cross-) Covariance Operators in Reproducing Kernel Hilbert Space toward Kernel Methods,"To the best of our knowledge, there are no general well-founded robust
methods for statistical unsupervised learning. Most of the unsupervised methods
explicitly or implicitly depend on the kernel covariance operator (kernel CO)
or kernel cross-covariance operator (kernel CCO). They are sensitive to
contaminated data, even when using bounded positive definite kernels. First, we
propose robust kernel covariance operator (robust kernel CO) and robust kernel
crosscovariance operator (robust kernel CCO) based on a generalized loss
function instead of the quadratic loss function. Second, we propose influence
function of classical kernel canonical correlation analysis (classical kernel
CCA). Third, using this influence function, we propose a visualization method
to detect influential observations from two sets of data. Finally, we propose a
method based on robust kernel CO and robust kernel CCO, called robust kernel
CCA, which is designed for contaminated data and less sensitive to noise than
classical kernel CCA. The principles we describe also apply to many kernel
methods which must deal with the issue of kernel CO or kernel CCO. Experiments
on synthesized and imaging genetics analysis demonstrate that the proposed
visualization and robust kernel CCA can be applied effectively to both ideal
data and contaminated data. The robust methods show the superior performance
over the state-of-the-art methods.","['Md. Ashad Alam', 'Kenji Fukumizu', 'Yu-Ping Wang']",['stat.ML'],2016-02-17 20:37:40+00:00
http://arxiv.org/abs/1602.05473v4,Auxiliary Deep Generative Models,"Deep generative models parameterized by neural networks have recently
achieved state-of-the-art performance in unsupervised and semi-supervised
learning. We extend deep generative models with auxiliary variables which
improves the variational approximation. The auxiliary variables leave the
generative model unchanged but make the variational distribution more
expressive. Inspired by the structure of the auxiliary variable we also propose
a model with two stochastic layers and skip connections. Our findings suggest
that more expressive and properly specified deep generative models converge
faster with better results. We show state-of-the-art performance within
semi-supervised learning on MNIST, SVHN and NORB datasets.","['Lars Maaløe', 'Casper Kaae Sønderby', 'Søren Kaae Sønderby', 'Ole Winther']","['stat.ML', 'cs.AI', 'cs.LG']",2016-02-17 16:24:50+00:00
http://arxiv.org/abs/1602.05450v2,Inverse Reinforcement Learning in Swarm Systems,"Inverse reinforcement learning (IRL) has become a useful tool for learning
behavioral models from demonstration data. However, IRL remains mostly
unexplored for multi-agent systems. In this paper, we show how the principle of
IRL can be extended to homogeneous large-scale problems, inspired by the
collective swarming behavior of natural systems. In particular, we make the
following contributions to the field: 1) We introduce the swarMDP framework, a
sub-class of decentralized partially observable Markov decision processes
endowed with a swarm characterization. 2) Exploiting the inherent homogeneity
of this framework, we reduce the resulting multi-agent IRL problem to a
single-agent one by proving that the agent-specific value functions in this
model coincide. 3) To solve the corresponding control problem, we propose a
novel heterogeneous learning scheme that is particularly tailored to the swarm
setting. Results on two example systems demonstrate that our framework is able
to produce meaningful local reward models from which we can replicate the
observed global system dynamics.","['Adrian Šošić', 'Wasiur R. KhudaBukhsh', 'Abdelhak M. Zoubir', 'Heinz Koeppl']","['stat.ML', 'cs.AI', 'cs.MA', 'cs.SY']",2016-02-17 15:19:56+00:00
http://arxiv.org/abs/1602.05436v1,Low-Rank Factorization of Determinantal Point Processes for Recommendation,"Determinantal point processes (DPPs) have garnered attention as an elegant
probabilistic model of set diversity. They are useful for a number of subset
selection tasks, including product recommendation. DPPs are parametrized by a
positive semi-definite kernel matrix. In this work we present a new method for
learning the DPP kernel from observed data using a low-rank factorization of
this kernel. We show that this low-rank factorization enables a learning
algorithm that is nearly an order of magnitude faster than previous approaches,
while also providing for a method for computing product recommendation
predictions that is far faster (up to 20x faster or more for large item
catalogs) than previous techniques that involve a full-rank DPP kernel.
Furthermore, we show that our method provides equivalent or sometimes better
predictive performance than prior full-rank DPP approaches, and better
performance than several other competing recommendation methods in many cases.
We conduct an extensive experimental evaluation using several real-world
datasets in the domain of product recommendation to demonstrate the utility of
our method, along with its limitations.","['Mike Gartrell', 'Ulrich Paquet', 'Noam Koenigstein']","['stat.ML', 'cs.LG']",2016-02-17 14:40:52+00:00
http://arxiv.org/abs/1602.05419v2,"Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression","We consider the optimization of a quadratic objective function whose
gradients are only accessible through a stochastic oracle that returns the
gradient at any given point plus a zero-mean finite variance random error. We
present the first algorithm that achieves jointly the optimal prediction error
rates for least-squares regression, both in terms of forgetting of initial
conditions in O(1/n 2), and in terms of dependence on the noise and dimension d
of the problem, as O(d/n). Our new algorithm is based on averaged accelerated
regularized gradient descent, and may also be analyzed through finer
assumptions on initial conditions and the Hessian matrix, leading to
dimension-free quantities that may still be small while the "" optimal "" terms
above are large. In order to characterize the tightness of these new bounds, we
consider an application to non-parametric regression and use the known lower
bounds on the statistical performance (without computational limits), which
happen to match our bounds obtained from a single pass on the data and thus
show optimality of our algorithm in a wide variety of particular trade-offs
between bias and variance.","['Aymeric Dieuleveut', 'Nicolas Flammarion', 'Francis Bach']","['math.OC', 'cs.LG', 'stat.ML']",2016-02-17 14:06:34+00:00
http://arxiv.org/abs/1602.05394v2,Online optimization and regret guarantees for non-additive long-term constraints,"We consider online optimization in the 1-lookahead setting, where the
objective does not decompose additively over the rounds of the online game. The
resulting formulation enables us to deal with non-stationary and/or long-term
constraints , which arise, for example, in online display advertising problems.
We propose an on-line primal-dual algorithm for which we obtain dynamic
cumulative regret guarantees. They depend on the convexity and the smoothness
of the non-additive penalty, as well as terms capturing the smoothness with
which the residuals of the non-stationary and long-term constraints vary over
the rounds. We conduct experiments on synthetic data to illustrate the benefits
of the non-additive penalty and show vanishing regret convergence on live
traffic data collected by a display advertising platform in production.","['Rodolphe Jenatton', 'Jim Huang', 'Dominik Csiba', 'Cedric Archambeau']","['stat.ML', 'cs.LG', 'math.OC', 'math.ST', 'stat.TH']",2016-02-17 12:57:08+00:00
http://arxiv.org/abs/1602.05310v1,Large Scale Kernel Learning using Block Coordinate Descent,"We demonstrate that distributed block coordinate descent can quickly solve
kernel regression and classification problems with millions of data points.
Armed with this capability, we conduct a thorough comparison between the full
kernel, the Nystr\""om method, and random features on three large classification
tasks from various domains. Our results suggest that the Nystr\""om method
generally achieves better statistical accuracy than random features, but can
require significantly more iterations of optimization. Lastly, we derive new
rates for block coordinate descent which support our experimental findings when
specialized to kernel methods.","['Stephen Tu', 'Rebecca Roelofs', 'Shivaram Venkataraman', 'Benjamin Recht']","['cs.LG', 'math.OC', 'stat.ML']",2016-02-17 05:41:07+00:00
http://arxiv.org/abs/1602.05285v1,Choice by Elimination via Deep Neural Networks,"We introduce Neural Choice by Elimination, a new framework that integrates
deep neural networks into probabilistic sequential choice models for learning
to rank. Given a set of items to chose from, the elimination strategy starts
with the whole item set and iteratively eliminates the least worthy item in the
remaining subset. We prove that the choice by elimination is equivalent to
marginalizing out the random Gompertz latent utilities. Coupled with the choice
model is the recently introduced Neural Highway Networks for approximating
arbitrarily complex rank functions. We evaluate the proposed framework on a
large-scale public dataset with over 425K items, drawn from the Yahoo! learning
to rank challenge. It is demonstrated that the proposed method is competitive
against state-of-the-art learning to rank methods.","['Truyen Tran', 'Dinh Phung', 'Svetha Venkatesh']","['stat.ML', 'cs.IR', 'cs.LG']",2016-02-17 03:17:10+00:00
http://arxiv.org/abs/1602.05264v1,Anomaly Detection in Clutter using Spectrally Enhanced Ladar,"Discrete return (DR) Laser Detection and Ranging (Ladar) systems provide a
series of echoes that reflect from objects in a scene. These can be first, last
or multi-echo returns. In contrast, Full-Waveform (FW)-Ladar systems measure
the intensity of light reflected from objects continuously over a period of
time. In a camouflaged scenario, e.g., objects hidden behind dense foliage, a
FW-Ladar penetrates such foliage and returns a sequence of echoes including
buried faint echoes. The aim of this paper is to learn local-patterns of
co-occurring echoes characterised by their measured spectra. A deviation from
such patterns defines an abnormal event in a forest/tree depth profile. As far
as the authors know, neither DR or FW-Ladar, along with several spectral
measurements, has not been applied to anomaly detection. This work presents an
algorithm that allows detection of spectral and temporal anomalies in FW-Multi
Spectral Ladar (FW-MSL) data samples. An anomaly is defined as a full waveform
temporal and spectral signature that does not conform to a prior expectation,
represented using a learnt subspace (dictionary) and set of coefficients that
capture co-occurring local-patterns using an overlapping temporal window. A
modified optimization scheme is proposed for subspace learning based on
stochastic approximations. The objective function is augmented with a
discriminative term that represents the subspace's separability properties and
supports anomaly characterisation. The algorithm detects several man-made
objects and anomalous spectra hidden in a dense clutter of vegetation and also
allows tree species classification.","['Puneet S Chhabra', 'Andrew M Wallace', 'James R Hopgood']","['physics.optics', 'cs.LG', 'physics.ins-det', 'stat.AP', 'stat.ML']",2016-02-17 01:39:29+00:00
http://arxiv.org/abs/1602.05257v3,Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector Data Description,"Support Vector Data Description (SVDD) is a machine-learning technique used
for single class classification and outlier detection. SVDD formulation with
kernel function provides a flexible boundary around data. The value of kernel
function parameters affects the nature of the data boundary. For example, it is
observed that with a Gaussian kernel, as the value of kernel bandwidth is
lowered, the data boundary changes from spherical to wiggly. The spherical data
boundary leads to underfitting, and an extremely wiggly data boundary leads to
overfitting. In this paper, we propose empirical criterion to obtain good
values of the Gaussian kernel bandwidth parameter. This criterion provides a
smooth boundary that captures the essential geometric features of the data.","['Deovrat Kakde', 'Arin Chaudhuri', 'Seunghyun Kong', 'Maria Jahja', 'Hansi Jiang', 'Jorge Silva']","['cs.LG', 'stat.AP', 'stat.ML']",2016-02-17 00:51:18+00:00
http://arxiv.org/abs/1602.05236v1,A Sparse PCA Approach to Clustering,"We discuss a clustering method for Gaussian mixture model based on the sparse
principal component analysis (SPCA) method and compare it with the IF-PCA
method. We also discuss the dependent case where the covariance matrix $\Sigma$
is not necessarily diagonal.","['T. Tony Cai', 'Linjun Zhang']","['stat.ME', 'stat.ML']",2016-02-16 22:49:41+00:00
http://arxiv.org/abs/1602.05221v2,Patterns of Scalable Bayesian Inference,"Datasets are growing not just in size but in complexity, creating a demand
for rich models and quantification of uncertainty. Bayesian methods are an
excellent fit for this demand, but scaling Bayesian inference is a challenge.
In response to this challenge, there has been considerable recent work based on
varying assumptions about model structure, underlying computational resources,
and the importance of asymptotic correctness. As a result, there is a zoo of
ideas with few clear overarching principles.
  In this paper, we seek to identify unifying principles, patterns, and
intuitions for scaling Bayesian inference. We review existing work on utilizing
modern computing resources with both MCMC and variational approximation
techniques. From this taxonomy of ideas, we characterize the general principles
that have proven successful for designing scalable inference procedures and
comment on the path forward.","['Elaine Angelino', 'Matthew James Johnson', 'Ryan P. Adams']",['stat.ML'],2016-02-16 22:13:00+00:00
http://arxiv.org/abs/1602.05149v4,Parallel Bayesian Global Optimization of Expensive Functions,"We consider parallel global optimization of derivative-free
expensive-to-evaluate functions, and propose an efficient method based on
stochastic approximation for implementing a conceptual Bayesian optimization
algorithm proposed by Ginsbourger et al. (2007). At the heart of this algorithm
is maximizing the information criterion called the ""multi-points expected
improvement'', or the q-EI. To accomplish this, we use infinitessimal
perturbation analysis (IPA) to construct a stochastic gradient estimator and
show that this estimator is unbiased. We also show that the stochastic gradient
ascent algorithm using the constructed gradient estimator converges to a
stationary point of the q-EI surface, and therefore, as the number of multiple
starts of the gradient ascent algorithm and the number of steps for each start
grow large, the one-step Bayes optimal set of points is recovered. We show in
numerical experiments that our method for maximizing the q-EI is faster than
methods based on closed-form evaluation using high-dimensional integration,
when considering many parallel function evaluations, and is comparable in speed
when considering few. We also show that the resulting one-step Bayes optimal
algorithm for parallel global optimization finds high-quality solutions with
fewer evaluations than a heuristic based on approximately maximizing the q-EI.
A high-quality open source implementation of this algorithm is available in the
open source Metrics Optimization Engine (MOE).","['Jialei Wang', 'Scott C. Clark', 'Eric Liu', 'Peter I. Frazier']","['stat.ML', 'math.OC']",2016-02-16 19:40:15+00:00
http://arxiv.org/abs/1602.05128v3,Interacting Particle Markov Chain Monte Carlo,"We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMC
method based on an interacting pool of standard and conditional sequential
Monte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte
Carlo sampler on an extended space. We present empirical results that show
significant improvements in mixing rates relative to both non-interacting PMCMC
samplers, and a single PMCMC sampler with an equivalent memory and
computational budget. An additional advantage of the iPMCMC method is that it
is suitable for distributed and multi-core architectures.","['Tom Rainforth', 'Christian A. Naesseth', 'Fredrik Lindsten', 'Brooks Paige', 'Jan-Willem van de Meent', 'Arnaud Doucet', 'Frank Wood']","['stat.CO', 'stat.ML']",2016-02-16 18:36:19+00:00
http://arxiv.org/abs/1602.05012v2,A Subsequence Interleaving Model for Sequential Pattern Mining,"Recent sequential pattern mining methods have used the minimum description
length (MDL) principle to define an encoding scheme which describes an
algorithm for mining the most compressing patterns in a database. We present a
novel subsequence interleaving model based on a probabilistic model of the
sequence database, which allows us to search for the most compressing set of
patterns without designing a specific encoding scheme. Our proposed algorithm
is able to efficiently mine the most relevant sequential patterns and rank them
using an associated measure of interestingness. The efficient inference in our
model is a direct result of our use of a structural expectation-maximization
framework, in which the expectation-step takes the form of a submodular
optimization problem subject to a coverage constraint. We show on both
synthetic and real world datasets that our model mines a set of sequential
patterns with low spuriousness and redundancy, high interpretability and
usefulness in real-world applications. Furthermore, we demonstrate that the
quality of the patterns from our approach is comparable to, if not better than,
existing state of the art sequential pattern mining algorithms.","['Jaroslav Fowkes', 'Charles Sutton']","['stat.ML', 'cs.AI', 'cs.LG']",2016-02-16 13:30:10+00:00
http://arxiv.org/abs/1602.05003v6,The Multivariate Generalised von Mises distribution: Inference and applications,"Circular variables arise in a multitude of data-modelling contexts ranging
from robotics to the social sciences, but they have been largely overlooked by
the machine learning community. This paper partially redresses this imbalance
by extending some standard probabilistic modelling tools to the circular
domain. First we introduce a new multivariate distribution over circular
variables, called the multivariate Generalised von Mises (mGvM) distribution.
This distribution can be constructed by restricting and renormalising a general
multivariate Gaussian distribution to the unit hyper-torus. Previously proposed
multivariate circular distributions are shown to be special cases of this
construction. Second, we introduce a new probabilistic model for circular
regression, that is inspired by Gaussian Processes, and a method for
probabilistic principal component analysis with circular hidden variables.
These models can leverage standard modelling tools (e.g. covariance functions
and methods for automatic relevance determination). Third, we show that the
posterior distribution in these models is a mGvM distribution which enables
development of an efficient variational free-energy scheme for performing
approximate inference and approximate maximum-likelihood learning.","['Alexandre K. W. Navarro', 'Jes Frellsen', 'Richard E. Turner']","['stat.ML', 'G.3; I.2']",2016-02-16 12:50:16+00:00
http://arxiv.org/abs/1602.04976v1,Stochastic Process Bandits: Upper Confidence Bounds Algorithms via Generic Chaining,"The paper considers the problem of global optimization in the setup of
stochastic process bandits. We introduce an UCB algorithm which builds a
cascade of discretization trees based on generic chaining in order to render
possible his operability over a continuous domain. The theoretical framework
applies to functions under weak probabilistic smoothness assumptions and also
extends significantly the spectrum of application of UCB strategies. Moreover
generic regret bounds are derived which are then specialized to Gaussian
processes indexed on infinite-dimensional spaces as well as to quadratic forms
of Gaussian processes. Lower bounds are also proved in the case of Gaussian
processes to assess the optimality of the proposed algorithm.","['Emile Contal', 'Nicolas Vayatis']","['stat.ML', 'cs.LG']",2016-02-16 10:48:28+00:00
http://arxiv.org/abs/1602.04951v2,Q($λ$) with Off-Policy Corrections,"We propose and analyze an alternate approach to off-policy multi-step
temporal difference learning, in which off-policy returns are corrected with
the current Q-function in terms of rewards, rather than with the target policy
in terms of transition probabilities. We prove that such approximate
corrections are sufficient for off-policy convergence both in policy evaluation
and control, provided certain conditions. These conditions relate the distance
between the target and behavior policies, the eligibility trace parameter and
the discount factor, and formalize an underlying tradeoff in off-policy
TD($\lambda$). We illustrate this theoretical relationship empirically on a
continuous-state control task.","['Anna Harutyunyan', 'Marc G. Bellemare', 'Tom Stepleton', 'Remi Munos']","['cs.AI', 'cs.LG', 'stat.ML']",2016-02-16 09:09:56+00:00
http://arxiv.org/abs/1602.04938v3,"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.","['Marco Tulio Ribeiro', 'Sameer Singh', 'Carlos Guestrin']","['cs.LG', 'cs.AI', 'stat.ML']",2016-02-16 08:20:14+00:00
http://arxiv.org/abs/1602.04915v2,Gradient Descent Converges to Minimizers,"We show that gradient descent converges to a local minimizer, almost surely
with random initialization. This is proved by applying the Stable Manifold
Theorem from dynamical systems theory.","['Jason D. Lee', 'Max Simchowitz', 'Michael I. Jordan', 'Benjamin Recht']","['stat.ML', 'cs.LG', 'math.OC']",2016-02-16 05:43:31+00:00
http://arxiv.org/abs/1602.04912v4,Uniform {\varepsilon}-Stability of Distributed Nonlinear Filtering over DNAs: Gaussian-Finite HMMs,"In this work, we study stability of distributed filtering of Markov chains
with finite state space, partially observed in conditionally Gaussian noise. We
consider a nonlinear filtering scheme over a Distributed Network of Agents
(DNA), which relies on the distributed evaluation of the likelihood part of the
centralized nonlinear filter and is based on a particular specialization of the
Alternating Direction Method of Multipliers (ADMM) for fast average consensus.
Assuming the same number of consensus steps between any two consecutive noisy
measurements for each sensor in the network, we fully characterize a minimal
number of such steps, such that the distributed filter remains uniformly stable
with a prescribed accuracy level, {\varepsilon} \in (0,1], within a finite
operational horizon, T, and across all sensors. Stability is in the sense of
the \ell_1-norm between the centralized and distributed versions of the
posterior at each sensor, and at each time within T. Roughly speaking, our main
result shows that uniform {\varepsilon}-stability of the distributed filtering
process depends only loglinearly on T and (roughly) the size of the network,
and only logarithmically on 1/{\varepsilon}. If this total loglinear bound is
fulfilled, any additional consensus iterations will incur a fully quantified
further exponential decay in the consensus error. Our bounds are universal, in
the sense that they are independent of the particular structure of the Gaussian
Hidden Markov Model (HMM) under consideration.","['Dionysios S. Kalogerias', 'Athina P. Petropulu']","['math.ST', 'math.OC', 'stat.ML', 'stat.TH']",2016-02-16 05:23:13+00:00
http://arxiv.org/abs/1602.04910v1,Bayesian generalized fused lasso modeling via NEG distribution,"The fused lasso penalizes a loss function by the $L_1$ norm for both the
regression coefficients and their successive differences to encourage sparsity
of both. In this paper, we propose a Bayesian generalized fused lasso modeling
based on a normal-exponential-gamma (NEG) prior distribution. The NEG prior is
assumed into the difference of successive regression coefficients. The proposed
method enables us to construct a more versatile sparse model than the ordinary
fused lasso by using a flexible regularization term. We also propose a sparse
fused algorithm to produce exact sparse solutions. Simulation studies and real
data analyses show that the proposed method has superior performance to the
ordinary fused lasso.","['Kaito Shimamura', 'Masao Ueki', 'Shuichi Kawano', 'Sadanori Konishi']","['stat.ME', 'stat.ML', 'Primary 62F15, 62J07, Secondary 62J05']",2016-02-16 05:20:14+00:00
http://arxiv.org/abs/1602.04805v1,DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression,"Performing exact posterior inference in complex generative models is often
difficult or impossible due to an expensive to evaluate or intractable
likelihood function. Approximate Bayesian computation (ABC) is an inference
framework that constructs an approximation to the true likelihood based on the
similarity between the observed and simulated data as measured by a predefined
set of summary statistics. Although the choice of appropriate problem-specific
summary statistics crucially influences the quality of the likelihood
approximation and hence also the quality of the posterior sample in ABC, there
are only few principled general-purpose approaches to the selection or
construction of such summary statistics. In this paper, we develop a novel
framework for this task using kernel-based distribution regression. We model
the functional relationship between data distributions and the optimal choice
(with respect to a loss function) of summary statistics using kernel-based
distribution regression. We show that our approach can be implemented in a
computationally and statistically efficient way using the random Fourier
features framework for large-scale kernel learning. In addition to that, our
framework shows superior performance when compared to related methods on toy
and real-world problems.","['Jovana Mitrovic', 'Dino Sejdinovic', 'Yee Whye Teh']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2016-02-15 20:55:57+00:00
http://arxiv.org/abs/1602.04799v1,Quantum Perceptron Models,"We demonstrate how quantum computation can provide non-trivial improvements
in the computational and statistical complexity of the perceptron model. We
develop two quantum algorithms for perceptron learning. The first algorithm
exploits quantum information processing to determine a separating hyperplane
using a number of steps sublinear in the number of data points $N$, namely
$O(\sqrt{N})$. The second algorithm illustrates how the classical mistake bound
of $O(\frac{1}{\gamma^2})$ can be further improved to
$O(\frac{1}{\sqrt{\gamma}})$ through quantum means, where $\gamma$ denotes the
margin. Such improvements are achieved through the application of quantum
amplitude amplification to the version space interpretation of the perceptron
model.","['Nathan Wiebe', 'Ashish Kapoor', 'Krysta M Svore']","['quant-ph', 'cs.LG', 'stat.ML']",2016-02-15 20:45:35+00:00
http://arxiv.org/abs/1602.04723v1,Efficient Representation of Low-Dimensional Manifolds using Deep Networks,"We consider the ability of deep neural networks to represent data that lies
near a low-dimensional manifold in a high-dimensional space. We show that deep
networks can efficiently extract the intrinsic, low-dimensional coordinates of
such data. We first show that the first two layers of a deep network can
exactly embed points lying on a monotonic chain, a special type of piecewise
linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,
the network can do this using an almost optimal number of parameters. We also
show that this network projects nearby points onto the manifold and then embeds
them with little error. We then extend these results to more general manifolds.","['Ronen Basri', 'David Jacobs']","['cs.NE', 'cs.LG', 'stat.ML']",2016-02-15 16:16:56+00:00
http://arxiv.org/abs/1602.04676v1,Maximin Action Identification: A New Bandit Framework for Games,"We study an original problem of pure exploration in a strategic bandit model
motivated by Monte Carlo Tree Search. It consists in identifying the best
action in a game, when the player may sample random outcomes of sequentially
chosen pairs of actions. We propose two strategies for the fixed-confidence
setting: Maximin-LUCB, based on lower-and upper-confidence bounds; and
Maximin-Racing, which operates by successively eliminating the sub-optimal
actions. We discuss the sample complexity of both methods and compare their
performance empirically. We sketch a lower bound analysis, and possible
connections to an optimal algorithm.","['Aurélien Garivier', 'Emilie Kaufmann', 'Wouter Koolen']","['math.ST', 'cs.GT', 'stat.ML', 'stat.TH']",2016-02-15 13:55:45+00:00
http://arxiv.org/abs/1602.04621v3,Deep Exploration via Bootstrapped DQN,"Efficient exploration in complex environments remains a major challenge for
reinforcement learning. We propose bootstrapped DQN, a simple algorithm that
explores in a computationally and statistically efficient manner through use of
randomized value functions. Unlike dithering strategies such as epsilon-greedy
exploration, bootstrapped DQN carries out temporally-extended (or deep)
exploration; this can lead to exponentially faster learning. We demonstrate
these benefits in complex stochastic MDPs and in the large-scale Arcade
Learning Environment. Bootstrapped DQN substantially improves learning times
and performance across most Atari games.","['Ian Osband', 'Charles Blundell', 'Alexander Pritzel', 'Benjamin Van Roy']","['cs.LG', 'cs.AI', 'cs.SY', 'stat.ML']",2016-02-15 10:54:20+00:00
http://arxiv.org/abs/1602.04601v2,Selective Inference Approach for Statistically Sound Predictive Pattern Mining,"Discovering statistically significant patterns from databases is an important
challenging problem. The main obstacle of this problem is in the difficulty of
taking into account the selection bias, i.e., the bias arising from the fact
that patterns are selected from extremely large number of candidates in
databases. In this paper, we introduce a new approach for predictive pattern
mining problems that can address the selection bias issue. Our approach is
built on a recently popularized statistical inference framework called
selective inference. In selective inference, statistical inferences (such as
statistical hypothesis testing) are conducted based on sampling distributions
conditional on a selection event. If the selection event is characterized in a
tractable way, statistical inferences can be made without minding selection
bias issue. However, in pattern mining problems, it is difficult to
characterize the entire selection process of mining algorithms. Our main
contribution in this paper is to solve this challenging problem for a class of
predictive pattern mining problems by introducing a novel algorithmic
framework. We demonstrate that our approach is useful for finding statistically
significant patterns from databases.","['Shinya Suzumura', 'Kazuya Nakagawa', 'Mahito Sugiyama', 'Koji Tsuda', 'Ichiro Takeuchi']",['stat.ML'],2016-02-15 09:52:00+00:00
http://arxiv.org/abs/1602.04589v2,Optimal Best Arm Identification with Fixed Confidence,"We give a complete characterization of the complexity of best-arm
identification in one-parameter bandit problems. We prove a new, tight lower
bound on the sample complexity. We propose the `Track-and-Stop' strategy, which
we prove to be asymptotically optimal. It consists in a new sampling rule
(which tracks the optimal proportions of arm draws highlighted by the lower
bound) and in a stopping rule named after Chernoff, for which we give a new
analysis.","['Aurélien Garivier', 'Emilie Kaufmann']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2016-02-15 08:25:02+00:00
http://arxiv.org/abs/1602.04579v1,Secure Approximation Guarantee for Cryptographically Private Empirical Risk Minimization,"Privacy concern has been increasingly important in many machine learning (ML)
problems. We study empirical risk minimization (ERM) problems under secure
multi-party computation (MPC) frameworks. Main technical tools for MPC have
been developed based on cryptography. One of limitations in current
cryptographically private ML is that it is computationally intractable to
evaluate non-linear functions such as logarithmic functions or exponential
functions. Therefore, for a class of ERM problems such as logistic regression
in which non-linear function evaluations are required, one can only obtain
approximate solutions. In this paper, we introduce a novel cryptographically
private tool called secure approximation guarantee (SAG) method. The key
property of SAG method is that, given an arbitrary approximate solution, it can
provide a non-probabilistic assumption-free bound on the approximation quality
under cryptographically secure computation framework. We demonstrate the
benefit of the SAG method by applying it to several problems including a
practical privacy-preserving data analysis task on genomic and clinical
information.","['Toshiyuki Takada', 'Hiroyuki Hanada', 'Yoshiji Yamada', 'Jun Sakuma', 'Ichiro Takeuchi']","['stat.ML', 'cs.CR', 'cs.LG']",2016-02-15 07:22:42+00:00
