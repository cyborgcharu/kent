id,title,abstract,authors,categories,date
http://arxiv.org/abs/2004.02046v1,Inferring Network Structure From Data,"Networks are complex models for underlying data in many application domains.
In most instances, raw data is not natively in the form of a network, but
derived from sensors, logs, images, or other data. Yet, the impact of the
various choices in translating this data to a network have been largely
unexamined. In this work, we propose a network model selection methodology that
focuses on evaluating a network's utility for varying tasks, together with an
efficiency measure which selects the most parsimonious model. We demonstrate
that this network definition matters in several ways for modeling the behavior
of the underlying system.","['Ivan Brugere', 'Tanya Y. Berger-Wolf']","['cs.SI', 'cs.LG', 'stat.ML']",2020-04-04 23:30:54+00:00
http://arxiv.org/abs/2004.02043v1,LU-Net: a multi-task network to improve the robustness of segmentation of left ventriclular structures by deep learning in 2D echocardiography,"Segmentation of cardiac structures is one of the fundamental steps to
estimate volumetric indices of the heart. This step is still performed
semi-automatically in clinical routine, and is thus prone to inter- and
intra-observer variability. Recent studies have shown that deep learning has
the potential to perform fully automatic segmentation. However, the current
best solutions still suffer from a lack of robustness. In this work, we
introduce an end-to-end multi-task network designed to improve the overall
accuracy of cardiac segmentation while enhancing the estimation of clinical
indices and reducing the number of outliers. Results obtained on a large open
access dataset show that our method outperforms the current best performing
deep learning solution and achieved an overall segmentation accuracy lower than
the intra-observer variability for the epicardial border (i.e. on average a
mean absolute error of 1.5mm and a Hausdorff distance of 5.1mm) with 11% of
outliers. Moreover, we demonstrate that our method can closely reproduce the
expert analysis for the end-diastolic and end-systolic left ventricular
volumes, with a mean correlation of 0.96 and a mean absolute error of 7.6ml.
Concerning the ejection fraction of the left ventricle, results are more
contrasted with a mean correlation coefficient of 0.83 and an absolute mean
error of 5.0%, producing scores that are slightly below the intra-observer
margin. Based on this observation, areas for improvement are suggested.","['Sarah Leclerc', 'Erik Smistad', 'Andreas Østvik', 'Frederic Cervenansky', 'Florian Espinosa', 'Torvald Espeland', 'Erik Andreas Rye Berg', 'Thomas Grenier', 'Carole Lartizien', 'Pierre-Marc Jodoin', 'Lasse Lovstakken', 'Olivier Bernard']","['eess.IV', 'cs.LG', 'stat.ML']",2020-04-04 23:07:53+00:00
http://arxiv.org/abs/2004.04072v2,CNN-MoE based framework for classification of respiratory anomalies and lung disease detection,"This paper presents and explores a robust deep learning framework for
auscultation analysis. This aims to classify anomalies in respiratory cycles
and detect disease, from respiratory sound recordings. The framework begins
with front-end feature extraction that transforms input sound into a
spectrogram representation. Then, a back-end deep learning network is used to
classify the spectrogram features into categories of respiratory anomaly cycles
or diseases. Experiments, conducted over the ICBHI benchmark dataset of
respiratory sounds, confirm three main contributions towards respiratory-sound
analysis. Firstly, we carry out an extensive exploration of the effect of
spectrogram type, spectral-time resolution, overlapped/non-overlapped windows,
and data augmentation on final prediction accuracy. This leads us to propose a
novel deep learning system, built on the proposed framework, which outperforms
current state-of-the-art methods. Finally, we apply a Teacher-Student scheme to
achieve a trade-off between model performance and model complexity which
additionally helps to increase the potential of the proposed framework for
building real-time applications.","['Lam Pham', 'Huy Phan', 'Ramaswamy Palaniappan', 'Alfred Mertins', 'Ian McLoughlin']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2020-04-04 21:45:06+00:00
http://arxiv.org/abs/2004.01942v1,Tracking Performance of Online Stochastic Learners,"The utilization of online stochastic algorithms is popular in large-scale
learning settings due to their ability to compute updates on the fly, without
the need to store and process data in large batches. When a constant step-size
is used, these algorithms also have the ability to adapt to drifts in problem
parameters, such as data or model properties, and track the optimal solution
with reasonable accuracy. Building on analogies with the study of adaptive
filters, we establish a link between steady-state performance derived under
stationarity assumptions and the tracking performance of online learners under
random walk models. The link allows us to infer the tracking performance from
steady-state expressions directly and almost by inspection.","['Stefan Vlaski', 'Elsa Rizk', 'Ali H. Sayed']","['math.OC', 'cs.LG', 'cs.MA', 'eess.SP', 'stat.ML']",2020-04-04 14:16:27+00:00
http://arxiv.org/abs/2004.01902v2,Rational neural networks,"We consider neural networks with rational activation functions. The choice of
the nonlinear activation function in deep learning architectures is crucial and
heavily impacts the performance of a neural network. We establish optimal
bounds in terms of network complexity and prove that rational neural networks
approximate smooth functions more efficiently than ReLU networks with
exponentially smaller depth. The flexibility and smoothness of rational
activation functions make them an attractive alternative to ReLU, as we
demonstrate with numerical experiments.","['Nicolas Boullé', 'Yuji Nakatsukasa', 'Alex Townsend']","['cs.NE', 'cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2020-04-04 10:36:11+00:00
http://arxiv.org/abs/2004.01899v3,A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS,"This work proposes a novel Graph-based neural ArchiTecture Encoding Scheme,
a.k.a. GATES, to improve the predictor-based neural architecture search.
Specifically, different from existing graph-based schemes, GATES models the
operations as the transformation of the propagating information, which mimics
the actual data processing of neural architecture. GATES is a more reasonable
modeling of the neural architectures, and can encode architectures from both
the ""operation on node"" and ""operation on edge"" cell search spaces
consistently. Experimental results on various search spaces confirm GATES's
effectiveness in improving the performance predictor. Furthermore, equipped
with the improved performance predictor, the sample efficiency of the
predictor-based neural architecture search (NAS) flow is boosted. Codes are
available at https://github.com/walkerning/aw_nas.","['Xuefei Ning', 'Yin Zheng', 'Tianchen Zhao', 'Yu Wang', 'Huazhong Yang']","['cs.LG', 'cs.NE', 'stat.ML']",2020-04-04 09:54:49+00:00
http://arxiv.org/abs/2004.01875v1,A Bayesian approach for initialization of weights in backpropagation neural net with application to character recognition,"Convergence rate of training algorithms for neural networks is heavily
affected by initialization of weights. In this paper, an original algorithm for
initialization of weights in backpropagation neural net is presented with
application to character recognition. The initialization method is mainly based
on a customization of the Kalman filter, translating it into Bayesian
statistics terms. A metrological approach is used in this context considering
weights as measurements modeled by mutually dependent normal random variables.
The algorithm performance is demonstrated by reporting and discussing results
of simulation trials. Results are compared with random weights initialization
and other methods. The proposed method shows an improved convergence rate for
the backpropagation training algorithm.","['Nadir Murru', 'Rosaria Rossini']","['cs.LG', 'stat.ML']",2020-04-04 06:42:07+00:00
http://arxiv.org/abs/2004.01864v1,Theoretical Insights into the Use of Structural Similarity Index In Generative Models and Inferential Autoencoders,"Generative models and inferential autoencoders mostly make use of $\ell_2$
norm in their optimization objectives. In order to generate perceptually better
images, this short paper theoretically discusses how to use Structural
Similarity Index (SSIM) in generative models and inferential autoencoders. We
first review SSIM, SSIM distance metrics, and SSIM kernel. We show that the
SSIM kernel is a universal kernel and thus can be used in unconditional and
conditional generated moment matching networks. Then, we explain how to use
SSIM distance in variational and adversarial autoencoders and unconditional and
conditional Generative Adversarial Networks (GANs). Finally, we propose to use
SSIM distance rather than $\ell_2$ norm in least squares GAN.","['Benyamin Ghojogh', 'Fakhri Karray', 'Mark Crowley']","['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']",2020-04-04 05:39:15+00:00
http://arxiv.org/abs/2004.01857v1,Weighted Fisher Discriminant Analysis in the Input and Feature Spaces,"Fisher Discriminant Analysis (FDA) is a subspace learning method which
minimizes and maximizes the intra- and inter-class scatters of data,
respectively. Although, in FDA, all the pairs of classes are treated the same
way, some classes are closer than the others. Weighted FDA assigns weights to
the pairs of classes to address this shortcoming of FDA. In this paper, we
propose a cosine-weighted FDA as well as an automatically weighted FDA in which
weights are found automatically. We also propose a weighted FDA in the feature
space to establish a weighted kernel FDA for both existing and newly proposed
weights. Our experiments on the ORL face recognition dataset show the
effectiveness of the proposed weighting schemes.","['Benyamin Ghojogh', 'Milad Sikaroudi', 'H. R. Tizhoosh', 'Fakhri Karray', 'Mark Crowley']","['stat.ML', 'cs.CV', 'cs.LG']",2020-04-04 05:17:53+00:00
http://arxiv.org/abs/2004.01840v1,"Abstracting Fairness: Oracles, Metrics, and Interpretability","It is well understood that classification algorithms, for example, for
deciding on loan applications, cannot be evaluated for fairness without taking
context into account. We examine what can be learned from a fairness oracle
equipped with an underlying understanding of ``true'' fairness. The oracle
takes as input a (context, classifier) pair satisfying an arbitrary fairness
definition, and accepts or rejects the pair according to whether the classifier
satisfies the underlying fairness truth. Our principal conceptual result is an
extraction procedure that learns the underlying truth; moreover, the procedure
can learn an approximation to this truth given access to a weak form of the
oracle. Since every ``truly fair'' classifier induces a coarse metric, in which
those receiving the same decision are at distance zero from one another and
those receiving different decisions are at distance one, this extraction
process provides the basis for ensuring a rough form of metric fairness, also
known as individual fairness. Our principal technical result is a higher
fidelity extractor under a mild technical constraint on the weak oracle's
conception of fairness. Our framework permits the scenario in which many
classifiers, with differing outcomes, may all be considered fair. Our results
have implications for interpretablity -- a highly desired but poorly defined
property of classification systems that endeavors to permit a human arbiter to
reject classifiers deemed to be ``unfair'' or illegitimately derived.","['Cynthia Dwork', 'Christina Ilvento', 'Guy N. Rothblum', 'Pragya Sur']","['cs.LG', 'stat.ML']",2020-04-04 03:14:53+00:00
http://arxiv.org/abs/2004.01832v2,SOAR: Second-Order Adversarial Regularization,"Adversarial training is a common approach to improving the robustness of deep
neural networks against adversarial examples. In this work, we propose a novel
regularization approach as an alternative. To derive the regularizer, we
formulate the adversarial robustness problem under the robust optimization
framework and approximate the loss function using a second-order Taylor series
expansion. Our proposed second-order adversarial regularizer (SOAR) is an upper
bound based on the Taylor approximation of the inner-max in the robust
optimization objective. We empirically show that the proposed method
significantly improves the robustness of networks against the $\ell_\infty$ and
$\ell_2$ bounded perturbations generated using cross-entropy-based PGD on
CIFAR-10 and SVHN.","['Avery Ma', 'Fartash Faghri', 'Nicolas Papernot', 'Amir-massoud Farahmand']","['cs.LG', 'stat.ML']",2020-04-04 01:35:07+00:00
http://arxiv.org/abs/2004.01822v1,The equivalence between Stein variational gradient descent and black-box variational inference,"We formalize an equivalence between two popular methods for Bayesian
inference: Stein variational gradient descent (SVGD) and black-box variational
inference (BBVI). In particular, we show that BBVI corresponds precisely to
SVGD when the kernel is the neural tangent kernel. Furthermore, we interpret
SVGD and BBVI as kernel gradient flows; we do this by leveraging the recent
perspective that views SVGD as a gradient flow in the space of probability
distributions and showing that BBVI naturally motivates a Riemannian structure
on that space. We observe that kernel gradient flow also describes dynamics
found in the training of generative adversarial networks (GANs). This work
thereby unifies several existing techniques in variational inference and
generative modeling and identifies the kernel as a fundamental object governing
the behavior of these algorithms, motivating deeper analysis of its properties.","['Casey Chu', 'Kentaro Minami', 'Kenji Fukumizu']","['cs.LG', 'stat.ML']",2020-04-04 00:39:12+00:00
http://arxiv.org/abs/2004.01819v1,DNA Methylation Data to Predict Suicidal and Non-Suicidal Deaths: A Machine Learning Approach,"The objective of this study is to predict suicidal and non-suicidal deaths
from DNA methylation data using a modern machine learning algorithm. We used
support vector machines to classify existing secondary data consisting of
normalized values of methylated DNA probe intensities from tissues of two
cortical brain regions to distinguish suicide cases from control cases. Before
classification, we employed Principal component analysis (PCA) and
t-distributed Stochastic Neighbor Embedding (t-SNE) to reduce the dimension of
the data. In comparison to PCA, the modern data visualization method t-SNE
performs better in dimensionality reduction. t-SNE accounts for the possible
non-linear patterns in low-dimensional data. We applied four-fold
cross-validation in which the resulting output from t-SNE was used as training
data for the Support Vector Machine (SVM). Despite the use of cross-validation,
the nominally perfect prediction of suicidal deaths for BA11 data suggests
possible over-fitting of the model. The study also may have suffered from
'spectrum bias' since the individuals were only studied from two extreme
scenarios. This research constitutes a baseline study for classifying suicidal
and non-suicidal deaths from DNA methylation data. Future studies with larger
sample size, while possibly incorporating methylation data from living
individuals, may reduce the bias and improve the accuracy of the results.","['Rifat Zahan', 'Ian McQuillan', 'Nathaniel D. Osgood']","['q-bio.GN', 'cs.LG', 'stat.ML']",2020-04-04 00:34:22+00:00
http://arxiv.org/abs/2004.01764v1,Stacked Generalizations in Imbalanced Fraud Data Sets using Resampling Methods,"This study uses stacked generalization, which is a two-step process of
combining machine learning methods, called meta or super learners, for
improving the performance of algorithms in step one (by minimizing the error
rate of each individual algorithm to reduce its bias in the learning set) and
then in step two inputting the results into the meta learner with its stacked
blended output (demonstrating improved performance with the weakest algorithms
learning better). The method is essentially an enhanced cross-validation
strategy. Although the process uses great computational resources, the
resulting performance metrics on resampled fraud data show that increased
system cost can be justified. A fundamental key to fraud data is that it is
inherently not systematic and, as of yet, the optimal resampling methodology
has not been identified. Building a test harness that accounts for all
permutations of algorithm sample set pairs demonstrates that the complex,
intrinsic data structures are all thoroughly tested. Using a comparative
analysis on fraud data that applies stacked generalizations provides useful
insight needed to find the optimal mathematical formula to be used for
imbalanced fraud data sets.","['Kathleen Kerwin', 'Nathaniel D. Bastian']","['cs.LG', 'stat.AP', 'stat.ML']",2020-04-03 20:38:22+00:00
http://arxiv.org/abs/2004.03385v1,Differential 3D Facial Recognition: Adding 3D to Your State-of-the-Art 2D Method,"Active illumination is a prominent complement to enhance 2D face recognition
and make it more robust, e.g., to spoofing attacks and low-light conditions. In
the present work we show that it is possible to adopt active illumination to
enhance state-of-the-art 2D face recognition approaches with 3D features, while
bypassing the complicated task of 3D reconstruction. The key idea is to project
over the test face a high spatial frequency pattern, which allows us to
simultaneously recover real 3D information plus a standard 2D facial image.
Therefore, state-of-the-art 2D face recognition solution can be transparently
applied, while from the high frequency component of the input image,
complementary 3D facial features are extracted. Experimental results on ND-2006
dataset show that the proposed ideas can significantly boost face recognition
performance and dramatically improve the robustness to spoofing attacks.","['J. Matias Di Martino', 'Fernando Suzacq', 'Mauricio Delbracio', 'Qiang Qiu', 'Guillermo Sapiro']","['cs.CV', 'cs.LG', 'stat.ML']",2020-04-03 20:17:14+00:00
http://arxiv.org/abs/2004.01743v1,TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications,"As machine learning (ML) has seen increasing adoption in safety-critical
domains (e.g., autonomous vehicles), the reliability of ML systems has also
grown in importance. While prior studies have proposed techniques to enable
efficient error-resilience techniques (e.g., selective instruction
duplication), a fundamental requirement for realizing these techniques is a
detailed understanding of the application's resilience.
  In this work, we present TensorFI, a high-level fault injection (FI)
framework for TensorFlow-based applications. TensorFI is able to inject both
hardware and software faults in general TensorFlow programs. TensorFI is a
configurable FI tool that is flexible, easy to use, and portable. It can be
integrated into existing TensorFlow programs to assess their resilience for
different fault types (e.g., faults in particular operators). We use TensorFI
to evaluate the resilience of 12 ML programs, including DNNs used in the
autonomous vehicle domain. Our tool is publicly available at
https://github.com/DependableSystemsLab/TensorFI.","['Zitao Chen', 'Niranjhana Narayanan', 'Bo Fang', 'Guanpeng Li', 'Karthik Pattabiraman', 'Nathan DeBardeleben']","['cs.DC', 'cs.CV', 'cs.LG', 'stat.ML']",2020-04-03 19:26:23+00:00
http://arxiv.org/abs/2004.01739v2,Lazy Online Gradient Descent is Universal on Polytopes,"We prove the familiar Lazy Online Gradient Descent algorithm is universal on
polytope domains. That means it gets $O(1)$ pseudo-regret against i.i.d
opponents, while simultaneously achieving the well-known $O(\sqrt N)$
worst-case regret bound. For comparison the bulk of the literature focuses on
variants of the Hedge (exponential weights) algorithm on the simplex. These can
in principle be lifted to general polytopes; however the process is
computationally unfeasible for many important classes where the number of
vertices grows quickly with the dimension. The lifting procedure also ignores
any Euclidean bounds on the cost vectors, and can create extra factors of
dimension in the pseudo-regret bound. Gradient Descent is simpler than the
handful of purpose-built algorithms for polytopes in the literature, and works
in a broader setting. In particular existing algorithms assume the optimiser is
unique, while our bound allows for several optimal vertices.","['Daron Anderson', 'Douglas Leith']","['cs.LG', 'stat.ML', '68W27', 'F.2.2; G.1.6; I.2.6']",2020-04-03 19:00:42+00:00
http://arxiv.org/abs/2004.01735v2,Unsupervised Domain Adaptation with Progressive Domain Augmentation,"Domain adaptation aims to exploit a label-rich source domain for learning
classifiers in a different label-scarce target domain. It is particularly
challenging when there are significant divergences between the two domains. In
the paper, we propose a novel unsupervised domain adaptation method based on
progressive domain augmentation. The proposed method generates virtual
intermediate domains via domain interpolation, progressively augments the
source domain and bridges the source-target domain divergence by conducting
multiple subspace alignment on the Grassmann manifold. We conduct experiments
on multiple domain adaptation tasks and the results shows the proposed method
achieves the state-of-the-art performance.","['Kevin Hua', 'Yuhong Guo']","['cs.LG', 'cs.CV', 'stat.ML']",2020-04-03 18:45:39+00:00
http://arxiv.org/abs/2004.01732v1,Leveraging Multi-Source Weak Social Supervision for Early Detection of Fake News,"Social media has greatly enabled people to participate in online activities
at an unprecedented rate. However, this unrestricted access also exacerbates
the spread of misinformation and fake news online which might cause confusion
and chaos unless being detected early for its mitigation. Given the rapidly
evolving nature of news events and the limited amount of annotated data,
state-of-the-art systems on fake news detection face challenges due to the lack
of large numbers of annotated training instances that are hard to come by for
early detection. In this work, we exploit multiple weak signals from different
sources given by user and content engagements (referred to as weak social
supervision), and their complementary utilities to detect fake news. We jointly
leverage the limited amount of clean data along with weak signals from social
engagements to train deep neural networks in a meta-learning framework to
estimate the quality of different weak instances. Experiments on realworld
datasets demonstrate that the proposed framework outperforms state-of-the-art
baselines for early detection of fake news without using any user engagements
at prediction time.","['Kai Shu', 'Guoqing Zheng', 'Yichuan Li', 'Subhabrata Mukherjee', 'Ahmed Hassan Awadallah', 'Scott Ruston', 'Huan Liu']","['cs.LG', 'cs.SI', 'stat.ML']",2020-04-03 18:26:33+00:00
http://arxiv.org/abs/2004.02625v1,Dynamic Modeling and Adaptive Controlling in GPS-Intelligent Buoy (GIB) Systems Based on Neural-Fuzzy Networks,"Recently, various relations and criteria have been presented to establish a
proper relationship between control systems and control the Global Positioning
System (GPS)-intelligent buoy system. Given the importance of controlling the
position of buoys and the construction of intelligent systems, in this paper,
dynamic system modeling is applied to position marine buoys through the
improved neural network with a backstepping technique. This study aims at
developing a novel controller based on an adaptive fuzzy neural network to
optimally track the dynamically positioned vehicle on the water with
unavailable velocities and unidentified control parameters. In order to model
the network with the proposed technique, uncertainties and the unwanted
disturbances are studied in the neural network. The presented study aims at
developing a neural controlling which applies the vectorial back-stepping
technique to the surface ships, which have been dynamically positioned with
undetermined disturbances and ambivalences. Moreover, the objective function is
to minimize the output error for the neural network (NN) based on the
closed-loop system. The most important feature of the proposed model for the
positioning buoys is its independence from comparative knowledge or information
on the dynamics and the unwanted disturbances of ships. The numerical and
obtained consequences demonstrate that the control system can adjust the routes
and the position of the buoys to the desired objective with relatively few
position errors.","['Dangquan Zhang', 'Muhammad Aqeel Ashraf', 'Zhenling Liu', 'Wan-Xi Peng', 'Mohammad Javad Golkar', 'Amir Mosavi']","['eess.SY', 'cs.LG', 'cs.SY', 'stat.ML', '68T01']",2020-04-03 17:28:53+00:00
http://arxiv.org/abs/2004.01655v1,Aligned Cross Entropy for Non-Autoregressive Machine Translation,"Non-autoregressive machine translation models significantly speed up decoding
by allowing for parallel prediction of the entire target sequence. However,
modeling word order is more challenging due to the lack of autoregressive
factors in the model. This difficultly is compounded during training with cross
entropy loss, which can highly penalize small shifts in word order. In this
paper, we propose aligned cross entropy (AXE) as an alternative loss function
for training of non-autoregressive models. AXE uses a differentiable dynamic
program to assign loss based on the best possible monotonic alignment between
target tokens and model predictions. AXE-based training of conditional masked
language models (CMLMs) substantially improves performance on major WMT
benchmarks, while setting a new state of the art for non-autoregressive models.","['Marjan Ghazvininejad', 'Vladimir Karpukhin', 'Luke Zettlemoyer', 'Omer Levy']","['cs.CL', 'cs.LG', 'stat.ML']",2020-04-03 16:24:47+00:00
http://arxiv.org/abs/2004.01653v6,Orthogonal Inductive Matrix Completion,"We propose orthogonal inductive matrix completion (OMIC), an interpretable
approach to matrix completion based on a sum of multiple orthonormal side
information terms, together with nuclear-norm regularization.
  The approach allows us to inject prior knowledge about the singular vectors
of the ground truth matrix.
  We optimize the approach by a provably converging algorithm, which optimizes
all components of the model simultaneously. We study the generalization
capabilities of our method in both the distribution-free setting and in the
case where the sampling distribution admits uniform marginals, yielding
learning guarantees that improve with the quality of the injected knowledge in
both cases. As particular cases of our framework, we present models which can
incorporate user and item biases or community information in a joint and
additive fashion.
  We analyse the performance of OMIC on several synthetic and real datasets.
  On synthetic datasets with a sliding scale of user bias relevance, we show
that OMIC better adapts to different regimes than other methods. On real-life
datasets containing user/items recommendations and relevant side information,
we find that OMIC surpasses the state-of-the-art, with the added benefit of
greater interpretability.","['Antoine Ledent', 'Rodrigo Alves', 'Marius Kloft']","['cs.LG', 'stat.ML']",2020-04-03 16:21:23+00:00
http://arxiv.org/abs/2004.01646v4,"M2: Mixed Models with Preferences, Popularities and Transitions for Next-Basket Recommendation","Next-basket recommendation considers the problem of recommending a set of
items into the next basket that users will purchase as a whole. In this paper,
we develop a novel mixed model with preferences, popularities and transitions
(M2) for the next-basket recommendation. This method models three important
factors in next-basket generation process: 1) users' general preferences, 2)
items' global popularities and 3) transition patterns among items. Unlike
existing recurrent neural network-based approaches, M2 does not use the
complicated networks to model the transitions among items, or generate
embeddings for users. Instead, it has a simple encoder-decoder based approach
(ed-Trans) to better model the transition patterns among items. We compared M2
with different combinations of the factors with 5 state-of-the-art next-basket
recommendation methods on 4 public benchmark datasets in recommending the
first, second and third next basket. Our experimental results demonstrate that
M2 significantly outperforms the state-of-the-art methods on all the datasets
in all the tasks, with an improvement of up to 22.1%. In addition, our ablation
study demonstrates that the ed-Trans is more effective than recurrent neural
networks in terms of the recommendation performance. We also have a thorough
discussion on various experimental protocols and evaluation metrics for
next-basket recommendation evaluation.","['Bo Peng', 'Zhiyun Ren', 'Srinivasan Parthasarathy', 'Xia Ning']","['cs.LG', 'cs.IR', 'stat.ML']",2020-04-03 16:11:26+00:00
http://arxiv.org/abs/2004.03375v1,Robust Self-Supervised Convolutional Neural Network for Subspace Clustering and Classification,"Insufficient capability of existing subspace clustering methods to handle
data coming from nonlinear manifolds, data corruptions, and out-of-sample data
hinders their applicability to address real-world clustering and classification
problems. This paper proposes the robust formulation of the self-supervised
convolutional subspace clustering network ($S^2$ConvSCN) that incorporates the
fully connected (FC) layer and, thus, it is capable for handling out-of-sample
data by classifying them using a softmax classifier. $S^2$ConvSCN clusters data
coming from nonlinear manifolds by learning the linear self-representation
model in the feature space. Robustness to data corruptions is achieved by using
the correntropy induced metric (CIM) of the error. Furthermore, the
block-diagonal (BD) structure of the representation matrix is enforced
explicitly through BD regularization. In a truly unsupervised training
environment, Robust $S^2$ConvSCN outperforms its baseline version by a
significant amount for both seen and unseen data on four well-known datasets.
Arguably, such an ablation study has not been reported before.","['Dario Sitnik', 'Ivica Kopriva']","['cs.CV', 'cs.LG', 'stat.ML']",2020-04-03 16:07:58+00:00
http://arxiv.org/abs/2004.01628v1,Weighted Random Search for Hyperparameter Optimization,"We introduce an improved version of Random Search (RS), used here for
hyperparameter optimization of machine learning algorithms. Unlike the standard
RS, which generates for each trial new values for all hyperparameters, we
generate new values for each hyperparameter with a probability of change. The
intuition behind our approach is that a value that already triggered a good
result is a good candidate for the next step, and should be tested in new
combinations of hyperparameter values. Within the same computational budget,
our method yields better results than the standard RS. Our theoretical results
prove this statement. We test our method on a variation of one of the most
commonly used objective function for this class of problems (the Grievank
function) and for the hyperparameter optimization of a deep learning CNN
architecture. Our results can be generalized to any optimization problem
defined on a discrete domain.","['Adrian-Catalin Florea', 'Razvan Andonie']","['cs.LG', 'stat.ML']",2020-04-03 15:41:22+00:00
http://arxiv.org/abs/2004.01623v2,Estimation and Uniform Inference in Sparse High-Dimensional Additive Models,"We develop a novel method to construct uniformly valid confidence bands for a
nonparametric component $f_1$ in the sparse additive model $Y=f_1(X_1)+\ldots +
f_p(X_p) + \varepsilon$ in a high-dimensional setting. Our method integrates
sieve estimation into a high-dimensional Z-estimation framework, facilitating
the construction of uniformly valid confidence bands for the target component
$f_1$. To form these confidence bands, we employ a multiplier bootstrap
procedure. Additionally, we provide rates for the uniform lasso estimation in
high dimensions, which may be of independent interest. Through simulation
studies, we demonstrate that our proposed method delivers reliable results in
terms of estimation and coverage, even in small samples.","['Philipp Bach', 'Sven Klaassen', 'Jannis Kueck', 'Martin Spindler']","['stat.ME', 'econ.EM', 'stat.ML', '62G08, 62-07']",2020-04-03 15:30:29+00:00
http://arxiv.org/abs/2004.01608v3,Learning 2-opt Heuristics for the Traveling Salesman Problem via Deep Reinforcement Learning,"Recent works using deep learning to solve the Traveling Salesman Problem
(TSP) have focused on learning construction heuristics. Such approaches find
TSP solutions of good quality but require additional procedures such as beam
search and sampling to improve solutions and achieve state-of-the-art
performance. However, few studies have focused on improvement heuristics, where
a given solution is improved until reaching a near-optimal one. In this work,
we propose to learn a local search heuristic based on 2-opt operators via deep
reinforcement learning. We propose a policy gradient algorithm to learn a
stochastic policy that selects 2-opt operations given a current solution.
Moreover, we introduce a policy neural network that leverages a pointing
attention mechanism, which unlike previous works, can be easily extended to
more general k-opt moves. Our results show that the learned policies can
improve even over random initial solutions and approach near-optimal solutions
at a faster rate than previous state-of-the-art deep learning methods.","['Paulo R. de O. da Costa', 'Jason Rhuggenaath', 'Yingqian Zhang', 'Alp Akcay']","['cs.LG', 'cs.AI', 'stat.ML']",2020-04-03 14:51:54+00:00
http://arxiv.org/abs/2004.01603v1,On-Device Transfer Learning for Personalising Psychological Stress Modelling using a Convolutional Neural Network,"Stress is a growing concern in modern society adversely impacting the wider
population more than ever before. The accurate inference of stress may result
in the possibility for personalised interventions. However, individual
differences between people limits the generalisability of machine learning
models to infer emotions as people's physiology when experiencing the same
emotions widely varies. In addition, it is time consuming and extremely
challenging to collect large datasets of individuals' emotions as it relies on
users labelling sensor data in real-time for extended periods. We propose the
development of a personalised, cross-domain 1D CNN by utilising transfer
learning from an initial base model trained using data from 20 participants
completing a controlled stressor experiment. By utilising physiological sensors
(HR, HRV EDA) embedded within edge computing interfaces that additionally
contain a labelling technique, it is possible to collect a small real-world
personal dataset that can be used for on-device transfer learning to improve
model personalisation and cross-domain performance.","['Kieran Woodward', 'Eiman Kanjo', 'David J. Brown', 'T. M. McGinnity']","['cs.LG', 'stat.ML']",2020-04-03 14:48:36+00:00
http://arxiv.org/abs/2004.01602v1,Predicting rice blast disease: machine learning versus process based models,"Rice is the second most important cereal crop worldwide, and the first in
terms of number of people who depend on it as a major staple food. Rice blast
disease is the most important biotic constraint of rice cultivation causing
each year millions of dollars of losses. Despite the efforts for breeding new
resistant varieties, agricultural practices and chemical control are still the
most important methods for disease management. Thus, rice blast forecasting is
a primary tool to support rice growers in controlling the disease. In this
study, we compared four models for predicting rice blast disease, two
operational process-based models (Yoshino and WARM) and two approaches based on
machine learning algorithms (M5Rules and RNN), the former inducing a rule-based
model and the latter building a neural network. In situ telemetry is important
to obtain quality in-field data for predictive models and this was a key aspect
of the RICE-GUARD project on which this study is based. According to the
authors, this is the first time process-based and machine learning modelling
approaches for supporting plant disease management are compared.","['David F. Nettleton', 'Dimitrios Katsantonis', 'Argyris Kalaitzidis', 'Natasa Sarafijanovic-Djukic', 'Pau Puigdollers', 'Roberto Confalonieri']","['q-bio.QM', 'cs.LG', 'stat.ML']",2020-04-03 14:48:14+00:00
http://arxiv.org/abs/2004.01584v5,How Good are Low-Rank Approximations in Gaussian Process Regression?,"We provide guarantees for approximate Gaussian Process (GP) regression
resulting from two common low-rank kernel approximations: based on random
Fourier features, and based on truncating the kernel's Mercer expansion. In
particular, we bound the Kullback-Leibler divergence between an exact GP and
one resulting from one of the afore-described low-rank approximations to its
kernel, as well as between their corresponding predictive densities, and we
also bound the error between predictive mean vectors and between predictive
covariance matrices computed using the exact versus using the approximate GP.
We provide experiments on both simulated data and standard benchmarks to
evaluate the effectiveness of our theoretical bounds.","['Constantinos Daskalakis', 'Petros Dellaportas', 'Aristeidis Panos']","['stat.ML', 'cs.LG']",2020-04-03 14:15:10+00:00
http://arxiv.org/abs/2004.01580v1,Hawkes Process Multi-armed Bandits for Disaster Search and Rescue,"We propose a novel framework for integrating Hawkes processes with
multi-armed bandit algorithms to solve spatio-temporal event forecasting and
detection problems when data may be undersampled or spatially biased. In
particular, we introduce an upper confidence bound algorithm using Bayesian
spatial Hawkes process estimation for balancing the tradeoff between exploiting
geographic regions where data has been collected and exploring geographic
regions where data is unobserved. We first validate our model using simulated
data and then apply it to the problem of disaster search and rescue using calls
for service data from hurricane Harvey in 2017. Our model outperforms the state
of the art baseline spatial MAB algorithms in terms of cumulative reward and
several other ranking evaluation metrics.","['Wen-Hao Chiang', 'George Mohler']","['cs.LG', 'stat.ML']",2020-04-03 14:05:09+00:00
http://arxiv.org/abs/2004.01574v1,Analysis of the COVID-19 pandemic by SIR model and machine learning technics for forecasting,"This work is a trial in which we propose SIR model and machine learning tools
to analyze the coronavirus pandemic in the real world. Based on the public data
from \cite{datahub}, we estimate main key pandemic parameters and make
predictions on the inflection point and possible ending time for the real world
and specifically for Senegal. The coronavirus disease 2019, by World Health
Organization, rapidly spread out in the whole China and then in the whole
world. Under optimistic estimation, the pandemic in some countries will end
soon, while for most part of countries in the world (US, Italy, etc.), the hit
of anti-pandemic will be no later than the end of April.","['Babacar Mbaye Ndiaye', 'Lena Tendeng', 'Diaraf Seck']","['q-bio.PE', 'math.OC', 'stat.ML']",2020-04-03 13:56:54+00:00
http://arxiv.org/abs/2004.01571v3,Tree-AMP: Compositional Inference with Tree Approximate Message Passing,"We introduce Tree-AMP, standing for Tree Approximate Message Passing, a
python package for compositional inference in high-dimensional tree-structured
models. The package provides a unifying framework to study several approximate
message passing algorithms previously derived for a variety of machine learning
tasks such as generalized linear models, inference in multi-layer networks,
matrix factorization, and reconstruction using non-separable penalties. For
some models, the asymptotic performance of the algorithm can be theoretically
predicted by the state evolution, and the measurements entropy estimated by the
free entropy formalism. The implementation is modular by design: each module,
which implements a factor, can be composed at will with other modules to solve
complex inference tasks. The user only needs to declare the factor graph of the
model: the inference algorithm, state evolution and entropy estimation are
fully automated.","['Antoine Baker', 'Benjamin Aubin', 'Florent Krzakala', 'Lenka Zdeborová']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG', 'eess.SP', 'math.ST', 'stat.CO', 'stat.TH']",2020-04-03 13:51:10+00:00
http://arxiv.org/abs/2004.01570v5,A New Method to Compare the Interpretability of Rule-based Algorithms,"Interpretability is becoming increasingly important for predictive model
analysis. Unfortunately, as remarked by many authors, there is still no
consensus regarding this notion. The goal of this paper is to propose the
definition of a score that allows to quickly compare interpretable algorithms.
This definition consists of three terms, each one being quantitatively measured
with a simple formula: predictivity, stability and simplicity. While
predictivity has been extensively studied to measure the accuracy of predictive
algorithms, stability is based on the Dice-Sorensen index for comparing two
rule sets generated by an algorithm using two independent samples. The
simplicity is based on the sum of the lengths of the rules derived from the
predictive model. The proposed score is a weighted sum of the three terms
mentioned above. We use this score to compare the interpretability of a set of
rule-based algorithms and tree-based algorithms for the regression case and for
the classification case.","['Vincent Margot', 'George Luta']","['stat.ML', 'cs.LG']",2020-04-03 13:50:27+00:00
http://arxiv.org/abs/2004.03376v2,Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle,"The computation and memory needed for Convolutional Neural Network (CNN)
inference can be reduced by pruning weights from the trained network. Pruning
is guided by a pruning saliency, which heuristically approximates the change in
the loss function associated with the removal of specific weights. Many pruning
signals have been proposed, but the performance of each heuristic depends on
the particular trained network. This leaves the data scientist with a difficult
choice. When using any one saliency metric for the entire pruning process, we
run the risk of the metric assumptions being invalidated, leading to poor
decisions being made by the metric. Ideally we could combine the best aspects
of different saliency metrics. However, despite an extensive literature review,
we are unable to find any prior work on composing different saliency metrics.
The chief difficulty lies in combining the numerical output of different
saliency metrics, which are not directly comparable.
  We propose a method to compose several primitive pruning saliencies, to
exploit the cases where each saliency measure does well. Our experiments show
that the composition of saliencies avoids many poor pruning choices identified
by individual saliencies. In most cases our method finds better selections than
even the best individual pruning saliency.","['Kaveena Persand', 'Andrew Anderson', 'David Gregg']","['cs.CV', 'cs.LG', 'stat.ML']",2020-04-03 11:29:41+00:00
http://arxiv.org/abs/2004.01468v3,Epitomic Variational Graph Autoencoder,"Variational autoencoder (VAE) is a widely used generative model for learning
latent representations. Burda et al. in their seminal paper showed that
learning capacity of VAE is limited by over-pruning. It is a phenomenon where a
significant number of latent variables fail to capture any information about
the input data and the corresponding hidden units become inactive. This
adversely affects learning diverse and interpretable latent representations. As
variational graph autoencoder (VGAE) extends VAE for graph-structured data, it
inherits the over-pruning problem. In this paper, we adopt a model based
approach and propose epitomic VGAE (EVGAE),a generative variational framework
for graph datasets which successfully mitigates the over-pruning problem and
also boosts the generative ability of VGAE. We consider EVGAE to consist of
multiple sparse VGAE models, called epitomes, that are groups of latent
variables sharing the latent space. This approach aids in increasing active
units as epitomes compete to learn better representation of the graph data. We
verify our claims via experiments on three benchmark datasets. Our experiments
show that EVGAE has a better generative ability than VGAE. Moreover, EVGAE
outperforms VGAE on link prediction task in citation networks.","['Rayyan Ahmad Khan', 'Muhammad Umer Anwaar', 'Martin Kleinsteuber']","['cs.LG', 'stat.ML']",2020-04-03 11:05:17+00:00
http://arxiv.org/abs/2004.02635v4,"Dualize, Split, Randomize: Toward Fast Nonsmooth Optimization Algorithms","We consider minimizing the sum of three convex functions, where the first one
F is smooth, the second one is nonsmooth and proximable and the third one is
the composition of a nonsmooth proximable function with a linear operator L.
This template problem has many applications, for instance, in image processing
and machine learning. First, we propose a new primal-dual algorithm, which we
call PDDY, for this problem. It is constructed by applying Davis-Yin splitting
to a monotone inclusion in a primal-dual product space, where the operators are
monotone under a specific metric depending on L. We show that three existing
algorithms (the two forms of the Condat-Vu algorithm and the PD3O algorithm)
have the same structure, so that PDDY is the fourth missing link in this
self-consistent class of primal-dual algorithms. This representation eases the
convergence analysis: it allows us to derive sublinear convergence rates in
general, and linear convergence results in presence of strong convexity.
Moreover, within our broad and flexible analysis framework, we propose new
stochastic generalizations of the algorithms, in which a variance-reduced
random estimate of the gradient of F is used, instead of the true gradient.
Furthermore, we obtain, as a special case of PDDY, a linearly converging
algorithm for the minimization of a strongly convex function F under a linear
constraint; we discuss its important application to decentralized optimization.","['Adil Salim', 'Laurent Condat', 'Konstantin Mishchenko', 'Peter Richtárik']","['math.OC', 'cs.LG', 'stat.ML']",2020-04-03 10:48:01+00:00
http://arxiv.org/abs/2004.01454v1,Infomax Neural Joint Source-Channel Coding via Adversarial Bit Flip,"Although Shannon theory states that it is asymptotically optimal to separate
the source and channel coding as two independent processes, in many practical
communication scenarios this decomposition is limited by the finite bit-length
and computational power for decoding. Recently, neural joint source-channel
coding (NECST) is proposed to sidestep this problem. While it leverages the
advancements of amortized inference and deep learning to improve the encoding
and decoding process, it still cannot always achieve compelling results in
terms of compression and error correction performance due to the limited
robustness of its learned coding networks. In this paper, motivated by the
inherent connections between neural joint source-channel coding and discrete
representation learning, we propose a novel regularization method called
Infomax Adversarial-Bit-Flip (IABF) to improve the stability and robustness of
the neural joint source-channel coding scheme. More specifically, on the
encoder side, we propose to explicitly maximize the mutual information between
the codeword and data; while on the decoder side, the amortized reconstruction
is regularized within an adversarial framework. Extensive experiments conducted
on various real-world datasets evidence that our IABF can achieve
state-of-the-art performances on both compression and error correction
benchmarks and outperform the baselines by a significant margin.","['Yuxuan Song', 'Minkai Xu', 'Lantao Yu', 'Hao Zhou', 'Shuo Shao', 'Yong Yu']","['cs.LG', 'stat.ML']",2020-04-03 10:00:02+00:00
http://arxiv.org/abs/2004.01442v2,From Local SGD to Local Fixed-Point Methods for Federated Learning,"Most algorithms for solving optimization problems or finding saddle points of
convex-concave functions are fixed-point algorithms. In this work we consider
the generic problem of finding a fixed point of an average of operators, or an
approximation thereof, in a distributed setting. Our work is motivated by the
needs of federated learning. In this context, each local operator models the
computations done locally on a mobile device. We investigate two strategies to
achieve such a consensus: one based on a fixed number of local steps, and the
other based on randomized computations. In both cases, the goal is to limit
communication of the locally-computed variables, which is often the bottleneck
in distributed frameworks. We perform convergence analysis of both methods and
conduct a number of experiments highlighting the benefits of our approach.","['Grigory Malinovsky', 'Dmitry Kovalev', 'Elnur Gasanov', 'Laurent Condat', 'Peter Richtárik']","['cs.LG', 'math.OC', 'stat.ML']",2020-04-03 09:24:10+00:00
http://arxiv.org/abs/2004.01395v3,Neural Architecture Generator Optimization,"Neural Architecture Search (NAS) was first proposed to achieve
state-of-the-art performance through the discovery of new architecture
patterns, without human intervention. An over-reliance on expert knowledge in
the search space design has however led to increased performance (local optima)
without significant architectural breakthroughs, thus preventing truly novel
solutions from being reached. In this work we 1) are the first to investigate
casting NAS as a problem of finding the optimal network generator and 2) we
propose a new, hierarchical and graph-based search space capable of
representing an extremely large variety of network types, yet only requiring
few continuous hyper-parameters. This greatly reduces the dimensionality of the
problem, enabling the effective use of Bayesian Optimisation as a search
strategy. At the same time, we expand the range of valid architectures,
motivating a multi-objective learning approach. We demonstrate the
effectiveness of this strategy on six benchmark datasets and show that our
search space generates extremely lightweight yet highly competitive models.","['Binxin Ru', 'Pedro Esperanca', 'Fabio Carlucci']","['cs.LG', 'cs.NE', 'stat.ML']",2020-04-03 06:38:07+00:00
http://arxiv.org/abs/2004.01377v1,Sequential Learning for Domain Generalization,"In this paper we propose a sequential learning framework for Domain
Generalization (DG), the problem of training a model that is robust to domain
shift by design. Various DG approaches have been proposed with different
motivating intuitions, but they typically optimize for a single step of domain
generalization -- training on one set of domains and generalizing to one other.
Our sequential learning is inspired by the idea lifelong learning, where
accumulated experience means that learning the $n^{th}$ thing becomes easier
than the $1^{st}$ thing. In DG this means encountering a sequence of domains
and at each step training to maximise performance on the next domain. The
performance at domain $n$ then depends on the previous $n-1$ learning problems.
Thus backpropagating through the sequence means optimizing performance not just
for the next domain, but all following domains. Training on all such sequences
of domains provides dramatically more `practice' for a base DG learner compared
to existing approaches, thus improving performance on a true testing domain.
This strategy can be instantiated for different base DG algorithms, but we
focus on its application to the recently proposed Meta-Learning Domain
generalization (MLDG). We show that for MLDG it leads to a simple to implement
and fast algorithm that provides consistent performance improvement on a
variety of DG benchmarks.","['Da Li', 'Yongxin Yang', 'Yi-Zhe Song', 'Timothy Hospedales']","['cs.CV', 'cs.LG', 'stat.ML']",2020-04-03 05:10:33+00:00
http://arxiv.org/abs/2004.01376v1,Neural Conditional Event Time Models,"Event time models predict occurrence times of an event of interest based on
known features. Recent work has demonstrated that neural networks achieve
state-of-the-art event time predictions in a variety of settings. However,
standard event time models suppose that the event occurs, eventually, in all
cases. Consequently, no distinction is made between a) the probability of event
occurrence, and b) the predicted time of occurrence. This distinction is
critical when predicting medical diagnoses, equipment defects, social media
posts, and other events that or may not occur, and for which the features
affecting a) may be different from those affecting b). In this work, we develop
a conditional event time model that distinguishes between these components,
implement it as a neural network with a binary stochastic layer representing
finite event occurrence, and show how it may be learned from right-censored
event times via maximum likelihood estimation. Results demonstrate superior
event occurrence and event time predictions on synthetic data, medical events
(MIMIC-III), and social media posts (Reddit), comprising 21 total prediction
tasks.","['Matthew Engelhard', 'Samuel Berchuck', ""Joshua D'Arcy"", 'Ricardo Henao']","['stat.ML', 'cs.LG']",2020-04-03 05:08:13+00:00
http://arxiv.org/abs/2004.01375v1,Attribute2vec: Deep Network Embedding Through Multi-Filtering GCN,"We present a multi-filtering Graph Convolution Neural Network (GCN) framework
for network embedding task. It uses multiple local GCN filters to do feature
extraction in every propagation layer. We show this approach could capture
different important aspects of node features against the existing attribute
embedding based method. We also show that with multi-filtering GCN approach, we
can achieve significant improvement against baseline methods when training data
is limited. We also perform many empirical experiments and demonstrate the
benefit of using multiple filters against single filter as well as most current
existing network embedding methods for both the link prediction and node
classification tasks.","['Tingyi Wanyan', 'Chenwei Zhang', 'Ariful Azad', 'Xiaomin Liang', 'Daifeng Li', 'Ying Ding']","['cs.LG', 'stat.ML', '68T99']",2020-04-03 05:06:16+00:00
http://arxiv.org/abs/2004.01358v1,Unpack Local Model Interpretation for GBDT,"A gradient boosting decision tree (GBDT), which aggregates a collection of
single weak learners (i.e. decision trees), is widely used for data mining
tasks. Because GBDT inherits the good performance from its ensemble essence,
much attention has been drawn to the optimization of this model. With its
popularization, an increasing need for model interpretation arises. Besides the
commonly used feature importance as a global interpretation, feature
contribution is a local measure that reveals the relationship between a
specific instance and the related output. This work focuses on the local
interpretation and proposes an unified computation mechanism to get the
instance-level feature contributions for GBDT in any version. Practicality of
this mechanism is validated by the listed experiments as well as applications
in real industry scenarios.","['Wenjing Fang', 'Jun Zhou', 'Xiaolong Li', 'Kenny Q. Zhu']","['cs.LG', 'stat.ML']",2020-04-03 03:25:07+00:00
http://arxiv.org/abs/2004.01339v2,Multi-agent Reinforcement Learning for Networked System Control,"This paper considers multi-agent reinforcement learning (MARL) in networked
system control. Specifically, each agent learns a decentralized control policy
based on local observations and messages from connected neighbors. We formulate
such a networked MARL (NMARL) problem as a spatiotemporal Markov decision
process and introduce a spatial discount factor to stabilize the training of
each local agent. Further, we propose a new differentiable communication
protocol, called NeurComm, to reduce information loss and non-stationarity in
NMARL. Based on experiments in realistic NMARL scenarios of adaptive traffic
signal control and cooperative adaptive cruise control, an appropriate spatial
discount factor effectively enhances the learning curves of non-communicative
MARL algorithms, while NeurComm outperforms existing communication protocols in
both learning efficiency and control performance.","['Tianshu Chu', 'Sandeep Chinchali', 'Sachin Katti']","['cs.LG', 'stat.ML']",2020-04-03 02:21:07+00:00
http://arxiv.org/abs/2004.01305v1,Distributed Primal-Dual Optimization for Online Multi-Task Learning,"Conventional online multi-task learning algorithms suffer from two critical
limitations: 1) Heavy communication caused by delivering high velocity of
sequential data to a central machine; 2) Expensive runtime complexity for
building task relatedness. To address these issues, in this paper we consider a
setting where multiple tasks are geographically located in different places,
where one task can synchronize data with others to leverage knowledge of
related tasks. Specifically, we propose an adaptive primal-dual algorithm,
which not only captures task-specific noise in adversarial learning but also
carries out a projection-free update with runtime efficiency. Moreover, our
model is well-suited to decentralized periodic-connected tasks as it allows the
energy-starved or bandwidth-constraint tasks to postpone the update.
Theoretical results demonstrate the convergence guarantee of our distributed
algorithm with an optimal regret. Empirical results confirm that the proposed
model is highly effective on various real-world datasets.","['Peng Yang', 'Ping Li']","['stat.ML', 'cs.LG']",2020-04-02 23:36:07+00:00
http://arxiv.org/abs/2004.01299v1,IVFS: Simple and Efficient Feature Selection for High Dimensional Topology Preservation,"Feature selection is an important tool to deal with high dimensional data. In
unsupervised case, many popular algorithms aim at maintaining the structure of
the original data. In this paper, we propose a simple and effective feature
selection algorithm to enhance sample similarity preservation through a new
perspective, topology preservation, which is represented by persistent diagrams
from the context of computational topology. This method is designed upon a
unified feature selection framework called IVFS, which is inspired by random
subset method. The scheme is flexible and can handle cases where the problem is
analytically intractable. The proposed algorithm is able to well preserve the
pairwise distances, as well as topological patterns, of the full data. We
demonstrate that our algorithm can provide satisfactory performance under a
sharp sub-sampling rate, which supports efficient implementation of our
proposed method to large scale datasets. Extensive experiments validate the
effectiveness of the proposed feature selection scheme.","['Xiaoyun Li', 'Chengxi Wu', 'Ping Li']","['stat.ML', 'cs.LG']",2020-04-02 23:05:00+00:00
http://arxiv.org/abs/2004.01293v2,Motif-Based Spectral Clustering of Weighted Directed Networks,"Clustering is an essential technique for network analysis, with applications
in a diverse range of fields. Although spectral clustering is a popular and
effective method, it fails to consider higher-order structure and can perform
poorly on directed networks. One approach is to capture and cluster
higher-order structures using motif adjacency matrices. However, current
formulations fail to take edge weights into account, and thus are somewhat
limited when weight is a key component of the network under study.
  We address these shortcomings by exploring motif-based weighted spectral
clustering methods. We present new and computationally useful matrix formulae
for motif adjacency matrices on weighted networks, which can be used to
construct efficient algorithms for any anchored or non-anchored motif on three
nodes. In a very sparse regime, our proposed method can handle graphs with a
million nodes and tens of millions of edges. We further use our framework to
construct a motif-based approach for clustering bipartite networks.
  We provide comprehensive experimental results, demonstrating (i) the
scalability of our approach, (ii) advantages of higher-order clustering on
synthetic examples, and (iii) the effectiveness of our techniques on a variety
of real world data sets; and compare against several techniques from the
literature. We conclude that motif-based spectral clustering is a valuable tool
for analysis of directed and bipartite weighted networks, which is also
scalable and easy to implement.","['William George Underwood', 'Andrew Elliott', 'Mihai Cucuringu']","['cs.SI', 'cs.LG', 'physics.soc-ph', 'stat.ML']",2020-04-02 22:45:28+00:00
http://arxiv.org/abs/2004.01275v6,AI4COVID-19: AI Enabled Preliminary Diagnosis for COVID-19 from Cough Samples via an App,"Background: The inability to test at scale has become humanity's Achille's
heel in the ongoing war against the COVID-19 pandemic. A scalable screening
tool would be a game changer. Building on the prior work on cough-based
diagnosis of respiratory diseases, we propose, develop and test an Artificial
Intelligence (AI)-powered screening solution for COVID-19 infection that is
deployable via a smartphone app. The app, named AI4COVID-19 records and sends
three 3-second cough sounds to an AI engine running in the cloud, and returns a
result within two minutes. Methods: Cough is a symptom of over thirty
non-COVID-19 related medical conditions. This makes the diagnosis of a COVID-19
infection by cough alone an extremely challenging multidisciplinary problem. We
address this problem by investigating the distinctness of pathomorphological
alterations in the respiratory system induced by COVID-19 infection when
compared to other respiratory infections. To overcome the COVID-19 cough
training data shortage we exploit transfer learning. To reduce the misdiagnosis
risk stemming from the complex dimensionality of the problem, we leverage a
multi-pronged mediator centered risk-averse AI architecture. Results: Results
show AI4COVID-19 can distinguish among COVID-19 coughs and several types of
non-COVID-19 coughs. The accuracy is promising enough to encourage a
large-scale collection of labeled cough data to gauge the generalization
capability of AI4COVID-19. AI4COVID-19 is not a clinical grade testing tool.
Instead, it offers a screening tool deployable anytime, anywhere, by anyone. It
can also be a clinical decision assistance tool used to channel
clinical-testing and treatment to those who need it the most, thereby saving
more lives.","['Ali Imran', 'Iryna Posokhova', 'Haneya N. Qureshi', 'Usama Masood', 'Muhammad Sajid Riaz', 'Kamran Ali', 'Charles N. John', 'MD Iftikhar Hussain', 'Muhammad Nabeel']","['eess.AS', 'cs.LG', 'cs.SD', 'q-bio.QM', 'stat.ML']",2020-04-02 21:39:34+00:00
http://arxiv.org/abs/2004.01221v1,Towards Relevance and Sequence Modeling in Language Recognition,"The task of automatic language identification (LID) involving multiple
dialects of the same language family in the presence of noise is a challenging
problem. In these scenarios, the identity of the language/dialect may be
reliably present only in parts of the temporal sequence of the speech signal.
The conventional approaches to LID (and for speaker recognition) ignore the
sequence information by extracting long-term statistical summary of the
recording assuming an independence of the feature frames. In this paper, we
propose a neural network framework utilizing short-sequence information in
language recognition. In particular, a new model is proposed for incorporating
relevance in language recognition, where parts of speech data are weighted more
based on their relevance for the language recognition task. This relevance
weighting is achieved using the bidirectional long short-term memory (BLSTM)
network with attention modeling. We explore two approaches, the first approach
uses segment level i-vector/x-vector representations that are aggregated in the
neural model and the second approach where the acoustic features are directly
modeled in an end-to-end neural model. Experiments are performed using the
language recognition task in NIST LRE 2017 Challenge using clean, noisy and
multi-speaker speech data as well as in the RATS language recognition corpus.
In these experiments on noisy LRE tasks as well as the RATS dataset, the
proposed approach yields significant improvements over the conventional
i-vector/x-vector based language recognition approaches as well as with other
previous models incorporating sequence information.","['Bharat Padi', 'Anand Mohan', 'Sriram Ganapathy']","['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD', 'stat.ML']",2020-04-02 18:31:18+00:00
http://arxiv.org/abs/2004.01215v2,CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models,"The novel nature of SARS-CoV-2 calls for the development of efficient de novo
drug design approaches. In this study, we propose an end-to-end framework,
named CogMol (Controlled Generation of Molecules), for designing new drug-like
small molecules targeting novel viral proteins with high affinity and
off-target selectivity. CogMol combines adaptive pre-training of a molecular
SMILES Variational Autoencoder (VAE) and an efficient multi-attribute
controlled sampling scheme that uses guidance from attribute predictors trained
on latent features. To generate novel and optimal drug-like molecules for
unseen viral targets, CogMol leverages a protein-molecule binding affinity
predictor that is trained using SMILES VAE embeddings and protein sequence
embeddings learned unsupervised from a large corpus. CogMol framework is
applied to three SARS-CoV-2 target proteins: main protease, receptor-binding
domain of the spike protein, and non-structural protein 9 replicase. The
generated candidates are novel at both molecular and chemical scaffold levels
when compared to the training data. CogMol also includes insilico screening for
assessing toxicity of parent molecules and their metabolites with a multi-task
toxicity classifier, synthetic feasibility with a chemical retrosynthesis
predictor, and target structure binding with docking simulations. Docking
reveals favorable binding of generated molecules to the target protein
structure, where 87-95 % of high affinity molecules showed docking free energy
< -6 kcal/mol. When compared to approved drugs, the majority of designed
compounds show low parent molecule and metabolite toxicity and high synthetic
feasibility. In summary, CogMol handles multi-constraint design of
synthesizable, low-toxic, drug-like molecules with high target specificity and
selectivity, and does not need target-dependent fine-tuning of the framework or
target structure information.","['Vijil Chenthamarakshan', 'Payel Das', 'Samuel C. Hoffman', 'Hendrik Strobelt', 'Inkit Padhi', 'Kar Wai Lim', 'Benjamin Hoover', 'Matteo Manica', 'Jannis Born', 'Teodoro Laino', 'Aleksandra Mojsilovic']","['cs.LG', 'q-bio.QM', 'stat.ML']",2020-04-02 18:17:20+00:00
http://arxiv.org/abs/2004.01190v3,Predicting the outputs of finite deep neural networks trained with noisy gradients,"A recent line of works studied wide deep neural networks (DNNs) by
approximating them as Gaussian Processes (GPs). A DNN trained with gradient
flow was shown to map to a GP governed by the Neural Tangent Kernel (NTK),
whereas earlier works showed that a DNN with an i.i.d. prior over its weights
maps to the so-called Neural Network Gaussian Process (NNGP). Here we consider
a DNN training protocol, involving noise, weight decay and finite width, whose
outcome corresponds to a certain non-Gaussian stochastic process. An analytical
framework is then introduced to analyze this non-Gaussian process, whose
deviation from a GP is controlled by the finite width. Our contribution is
three-fold: (i) In the infinite width limit, we establish a correspondence
between DNNs trained with noisy gradients and the NNGP, not the NTK. (ii) We
provide a general analytical form for the finite width correction (FWC) for
DNNs with arbitrary activation functions and depth and use it to predict the
outputs of empirical finite networks with high accuracy. Analyzing the FWC
behavior as a function of $n$, the training set size, we find that it is
negligible for both the very small $n$ regime, and, surprisingly, for the large
$n$ regime (where the GP error scales as $O(1/n)$). (iii) We flesh out
algebraically how these FWCs can improve the performance of finite
convolutional neural networks (CNNs) relative to their GP counterparts on image
classification tasks.","['Gadi Naveh', 'Oded Ben-David', 'Haim Sompolinsky', 'Zohar Ringel']","['stat.ML', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.LG', 'cs.NE']",2020-04-02 18:00:01+00:00
http://arxiv.org/abs/2004.01157v2,Identification Methods With Arbitrary Interventional Distributions as Inputs,"Causal inference quantifies cause-effect relationships by estimating
counterfactual parameters from data. This entails using \emph{identification
theory} to establish a link between counterfactual parameters of interest and
distributions from which data is available. A line of work characterized
non-parametric identification for a wide variety of causal parameters in terms
of the \emph{observed data distribution}. More recently, identification results
have been extended to settings where experimental data from interventional
distributions is also available. In this paper, we use Single World
Intervention Graphs and a nested factorization of models associated with mixed
graphs to give a very simple view of existing identification theory for
experimental data. We use this view to yield general identification algorithms
for settings where the input distributions consist of an arbitrary set of
observational and experimental distributions, including marginal and
conditional distributions. We show that for problems where inputs are
interventional marginal distributions of a certain type (ancestral marginals),
our algorithm is complete.","['Jaron J. R. Lee', 'Ilya Shpitser']","['stat.ML', 'cs.LG']",2020-04-02 17:27:18+00:00
http://arxiv.org/abs/2004.01144v1,Predicting Injectable Medication Adherence via a Smart Sharps Bin and Machine Learning,"Medication non-adherence is a widespread problem affecting over 50% of people
who have chronic illness and need chronic treatment. Non-adherence exacerbates
health risks and drives significant increases in treatment costs. In order to
address these challenges, the importance of predicting patients' adherence has
been recognised. In other words, it is important to improve the efficiency of
interventions of the current healthcare system by prioritizing resources to the
patients who are most likely to be non-adherent. Our objective in this work is
to make predictions regarding individual patients' behaviour in terms of taking
their medication on time during their next scheduled medication opportunity. We
do this by leveraging a number of machine learning models. In particular, we
demonstrate the use of a connected IoT device; a ""Smart Sharps Bin"", invented
by HealthBeacon Ltd.; to monitor and track injection disposal of patients in
their home environment. Using extensive data collected from these devices, five
machine learning models, namely Extra Trees Classifier, Random Forest, XGBoost,
Gradient Boosting and Multilayer Perception were trained and evaluated on a
large dataset comprising 165,223 historic injection disposal records collected
from 5,915 HealthBeacon units over the course of 3 years. The testing work was
conducted on real-time data generated by the smart device over a time period
after the model training was complete, i.e. true future data. The proposed
machine learning approach demonstrated very good predictive performance
exhibiting an Area Under the Receiver Operating Characteristic Curve (ROC AUC)
of 0.86.","['Yingqi Gu', 'Akshay Zalkikar', 'Lara Kelly', 'Kieran Daly', 'Tomas E. Ward']","['cs.LG', 'stat.ML']",2020-04-02 17:16:51+00:00
http://arxiv.org/abs/2004.01143v1,Randomized Kernel Multi-view Discriminant Analysis,"In many artificial intelligence and computer vision systems, the same object
can be observed at distinct viewpoints or by diverse sensors, which raises the
challenges for recognizing objects from different, even heterogeneous views.
Multi-view discriminant analysis (MvDA) is an effective multi-view subspace
learning method, which finds a discriminant common subspace by jointly learning
multiple view-specific linear projections for object recognition from multiple
views, in a non-pairwise way. In this paper, we propose the kernel version of
multi-view discriminant analysis, called kernel multi-view discriminant
analysis (KMvDA). To overcome the well-known computational bottleneck of kernel
methods, we also study the performance of using random Fourier features (RFF)
to approximate Gaussian kernels in KMvDA, for large scale learning. Theoretical
analysis on stability of this approximation is developed. We also conduct
experiments on several popular multi-view datasets to illustrate the
effectiveness of our proposed strategy.","['Xiaoyun Li', 'Jie Gui', 'Ping Li']","['stat.ML', 'cs.LG']",2020-04-02 17:15:32+00:00
http://arxiv.org/abs/2004.01141v1,Predictive Bandits,"We introduce and study a new class of stochastic bandit problems, referred to
as predictive bandits. In each round, the decision maker first decides whether
to gather information about the rewards of particular arms (so that their
rewards in this round can be predicted). These measurements are costly, and may
be corrupted by noise. The decision maker then selects an arm to be actually
played in the round. Predictive bandits find applications in many areas; e.g.
they can be applied to channel selection problems in radio communication
systems. In this paper, we provide the first theoretical results about
predictive bandits, and focus on scenarios where the decision maker is allowed
to measure at most one arm per round. We derive asymptotic instance-specific
regret lower bounds for these problems, and develop algorithms whose regret
match these fundamental limits. We illustrate the performance of our algorithms
through numerical experiments. In particular, we highlight the gains that can
be achieved by using reward predictions, and investigate the impact of the
noise in the corresponding measurements.","['Simon Lindståhl', 'Alexandre Proutiere', 'Andreas Johnsson']","['cs.LG', 'stat.ML']",2020-04-02 17:12:33+00:00
http://arxiv.org/abs/2004.03456v1,Binary and Multiclass Classifiers based on Multitaper Spectral Features for Epilepsy Detection,"Epilepsy is one of the most common neurological disorders that can be
diagnosed through electroencephalogram (EEG), in which the following epileptic
events can be observed: pre-ictal, ictal, post-ictal, and interictal. In this
paper, we present a novel method for epilepsy detection into two
differentiation contexts: binary and multiclass classification. For feature
extraction, a total of 105 measures were extracted from power spectrum,
spectrogram, and bispectrogram. For classifier building, eight different
machine learning algorithms were used. Our method was applied in a widely used
EEG database. As a result, random forest and backpropagation based on
multilayer perceptron algorithms reached the highest accuracy for binary
(98.75%) and multiclass (96.25%) classification problems, respectively.
Subsequently, the statistical tests did not find a model that would achieve a
better performance than the other classifiers. In the evaluation based on
confusion matrices, it was also not possible to identify a classifier that
stands out in relation to other models for EEG classification. Even so, our
results are promising and competitive with the findings in the literature.","['Jefferson Tales Oliva', 'João Luís Garcia Rosa']","['cs.LG', 'eess.SP', 'stat.ML']",2020-04-02 17:12:33+00:00
http://arxiv.org/abs/2004.01136v2,Hierarchical Adaptive Contextual Bandits for Resource Constraint based Recommendation,"Contextual multi-armed bandit (MAB) achieves cutting-edge performance on a
variety of problems. When it comes to real-world scenarios such as
recommendation system and online advertising, however, it is essential to
consider the resource consumption of exploration. In practice, there is
typically non-zero cost associated with executing a recommendation (arm) in the
environment, and hence, the policy should be learned with a fixed exploration
cost constraint. It is challenging to learn a global optimal policy directly,
since it is a NP-hard problem and significantly complicates the exploration and
exploitation trade-off of bandit algorithms. Existing approaches focus on
solving the problems by adopting the greedy policy which estimates the expected
rewards and costs and uses a greedy selection based on each arm's expected
reward/cost ratio using historical observation until the exploration resource
is exhausted. However, existing methods are hard to extend to infinite time
horizon, since the learning process will be terminated when there is no more
resource. In this paper, we propose a hierarchical adaptive contextual bandit
method (HATCH) to conduct the policy learning of contextual bandits with a
budget constraint. HATCH adopts an adaptive method to allocate the exploration
resource based on the remaining resource/time and the estimation of reward
distribution among different user contexts. In addition, we utilize full of
contextual feature information to find the best personalized recommendation.
Finally, in order to prove the theoretical guarantee, we present a regret bound
analysis and prove that HATCH achieves a regret bound as low as $O(\sqrt{T})$.
The experimental results demonstrate the effectiveness and efficiency of the
proposed method on both synthetic data sets and the real-world applications.","['Mengyue Yang', 'Qingyang Li', 'Zhiwei Qin', 'Jieping Ye']","['cs.LG', 'stat.ML']",2020-04-02 17:04:52+00:00
http://arxiv.org/abs/2004.01123v2,Surrogate-assisted performance prediction for data-driven knowledge discovery algorithms: application to evolutionary modeling of clinical pathways,"The paper proposes and investigates an approach for surrogate-assisted
performance prediction of data-driven knowledge discovery algorithms. The
approach is based on the identification of surrogate models for prediction of
the target algorithm's quality and performance. The proposed approach was
implemented and investigated as applied to an evolutionary algorithm for
discovering clusters of interpretable clinical pathways in electronic health
records of patients with acute coronary syndrome. Several clustering metrics
and execution time were used as the target quality and performance metrics
respectively. An analytical software prototype based on the proposed approach
for the prediction of algorithm characteristics and feature analysis was
developed to provide a more interpretable prediction of the target algorithm's
performance and quality that can be further used for parameter tuning.","['Anastasia A. Funkner', 'Aleksey N. Yakovlev', 'Sergey V. Kovalchuk']","['cs.LG', 'stat.ML']",2020-04-02 16:49:43+00:00
http://arxiv.org/abs/2004.01097v2,Learning to cooperate: Emergent communication in multi-agent navigation,"Emergent communication in artificial agents has been studied to understand
language evolution, as well as to develop artificial systems that learn to
communicate with humans. We show that agents performing a cooperative
navigation task in various gridworld environments learn an interpretable
communication protocol that enables them to efficiently, and in many cases,
optimally, solve the task. An analysis of the agents' policies reveals that
emergent signals spatially cluster the state space, with signals referring to
specific locations and spatial directions such as ""left"", ""up"", or ""upper left
room"". Using populations of agents, we show that the emergent protocol has
basic compositional structure, thus exhibiting a core property of natural
language.","['Ivana Kajić', 'Eser Aygün', 'Doina Precup']","['cs.LG', 'cs.CL', 'cs.MA', 'stat.ML']",2020-04-02 16:03:17+00:00
http://arxiv.org/abs/2004.01077v2,Learning Sparse & Ternary Neural Networks with Entropy-Constrained Trained Ternarization (EC2T),"Deep neural networks (DNN) have shown remarkable success in a variety of
machine learning applications. The capacity of these models (i.e., number of
parameters), endows them with expressive power and allows them to reach the
desired performance. In recent years, there is an increasing interest in
deploying DNNs to resource-constrained devices (i.e., mobile devices) with
limited energy, memory, and computational budget. To address this problem, we
propose Entropy-Constrained Trained Ternarization (EC2T), a general framework
to create sparse and ternary neural networks which are efficient in terms of
storage (e.g., at most two binary-masks and two full-precision values are
required to save a weight matrix) and computation (e.g., MAC operations are
reduced to a few accumulations plus two multiplications). This approach
consists of two steps. First, a super-network is created by scaling the
dimensions of a pre-trained model (i.e., its width and depth). Subsequently,
this super-network is simultaneously pruned (using an entropy constraint) and
quantized (that is, ternary values are assigned layer-wise) in a training
process, resulting in a sparse and ternary network representation. We validate
the proposed approach in CIFAR-10, CIFAR-100, and ImageNet datasets, showing
its effectiveness in image classification tasks.","['Arturo Marban', 'Daniel Becking', 'Simon Wiedemann', 'Wojciech Samek']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2020-04-02 15:38:00+00:00
http://arxiv.org/abs/2004.01025v3,Mirrorless Mirror Descent: A Natural Derivation of Mirror Descent,"We present a primal only derivation of Mirror Descent as a ""partial""
discretization of gradient flow on a Riemannian manifold where the metric
tensor is the Hessian of the Mirror Descent potential. We contrast this
discretization to Natural Gradient Descent, which is obtained by a ""full""
forward Euler discretization. This view helps shed light on the relationship
between the methods and allows generalizing Mirror Descent to general
Riemannian geometries, even when the metric tensor is {\em not} a Hessian, and
thus there is no ""dual.""","['Suriya Gunasekar', 'Blake Woodworth', 'Nathan Srebro']","['cs.LG', 'math.OC', 'stat.ML']",2020-04-02 14:31:04+00:00
http://arxiv.org/abs/2004.03459v2,Hierarchical Image Classification using Entailment Cone Embeddings,"Image classification has been studied extensively, but there has been limited
work in using unconventional, external guidance other than traditional
image-label pairs for training. We present a set of methods for leveraging
information about the semantic hierarchy embedded in class labels. We first
inject label-hierarchy knowledge into an arbitrary CNN-based classifier and
empirically show that availability of such external semantic information in
conjunction with the visual semantics from images boosts overall performance.
Taking a step further in this direction, we model more explicitly the
label-label and label-image interactions using order-preserving embeddings
governed by both Euclidean and hyperbolic geometries, prevalent in natural
language, and tailor them to hierarchical image classification and
representation learning. We empirically validate all the models on the
hierarchical ETHEC dataset.","['Ankit Dhall', 'Anastasia Makarova', 'Octavian Ganea', 'Dario Pavllo', 'Michael Greeff', 'Andreas Krause']","['cs.CV', 'cs.LG', 'stat.ML']",2020-04-02 10:22:02+00:00
http://arxiv.org/abs/2004.00909v2,Learning Representations For Images With Hierarchical Labels,"Image classification has been studied extensively but there has been limited
work in the direction of using non-conventional, external guidance other than
traditional image-label pairs to train such models. In this thesis we present a
set of methods to leverage information about the semantic hierarchy induced by
class labels. In the first part of the thesis, we inject label-hierarchy
knowledge to an arbitrary classifier and empirically show that availability of
such external semantic information in conjunction with the visual semantics
from images boosts overall performance. Taking a step further in this
direction, we model more explicitly the label-label and label-image
interactions by using order-preserving embedding-based models, prevalent in
natural language, and tailor them to the domain of computer vision to perform
image classification. Although, contrasting in nature, both the CNN-classifiers
injected with hierarchical information, and the embedding-based models
outperform a hierarchy-agnostic model on the newly presented, real-world ETH
Entomological Collection image dataset
https://www.research-collection.ethz.ch/handle/20.500.11850/365379.",['Ankit Dhall'],"['cs.LG', 'cs.CV', 'stat.ML']",2020-04-02 09:56:03+00:00
http://arxiv.org/abs/2004.00891v2,Kernel Autocovariance Operators of Stationary Processes: Estimation and Convergence,"We consider autocovariance operators of a stationary stochastic process on a
Polish space that is embedded into a reproducing kernel Hilbert space. We
investigate how empirical estimates of these operators converge along
realizations of the process under various conditions. In particular, we examine
ergodic and strongly mixing processes and obtain several asymptotic results as
well as finite sample error bounds. We provide applications of our theory in
terms of consistency results for kernel PCA with dependent data and the
conditional mean embedding of transition probabilities. Finally, we use our
approach to examine the nonparametric estimation of Markov transition operators
and highlight how our theory can give a consistency analysis for a large family
of spectral analysis methods including kernel-based dynamic mode decomposition.","['Mattes Mollenhauer', 'Stefan Klus', 'Christof Schütte', 'Péter Koltai']","['math.PR', 'cs.LG', 'math.FA', 'stat.ML']",2020-04-02 09:17:32+00:00
http://arxiv.org/abs/2004.07067v1,Gestalt: a Stacking Ensemble for SQuAD2.0,"We propose a deep-learning system -- for the SQuAD2.0 task -- that finds, or
indicates the lack of, a correct answer to a question in a context paragraph.
Our goal is to learn an ensemble of heterogeneous SQuAD2.0 models that, when
blended properly, outperforms the best model in the ensemble per se. We created
a stacking ensemble that combines top-N predictions from two models, based on
ALBERT and RoBERTa, into a multiclass classification task to pick the best
answer out of their predictions. We explored various ensemble configurations,
input representations, and model architectures. For evaluation, we examined
test-set EM and F1 scores; our best-performing ensemble incorporated a
CNN-based meta-model and scored 87.117 and 90.306, respectively -- a relative
improvement of 0.55% for EM and 0.61% for F1 scores, compared to the baseline
performance of the best model in the ensemble, an ALBERT-based model, at 86.644
for EM and 89.760 for F1.",['Mohamed El-Geish'],"['cs.CL', 'cs.LG', 'stat.ML']",2020-04-02 08:09:22+00:00
http://arxiv.org/abs/2004.02607v1,Semantic Image Search for Robotic Applications,"Generalization in robotics is one of the most important problems. New
generalization approaches use internet databases in order to solve new tasks.
Modern search engines can return a large amount of information according to a
query within milliseconds. However, not all of the returned information is task
relevant, partly due to the problem of polysemes. Here we specifically address
the problem of object generalization by using image search. We suggest a
bi-modal solution, combining visual and textual information, based on the
observation that humans use additional linguistic cues to demarcate intended
word meaning. We evaluate the quality of our approach by comparing it to human
labelled data and find that, on average, our approach leads to improved results
in comparison to Google searches, and that it can treat the problem of
polysemes.","['Tomas Kulvicius', 'Irene Markelic', 'Minija Tamosiunaite', 'Florentin Wörgötter']","['cs.IR', 'cs.LG', 'stat.ML']",2020-04-02 08:09:06+00:00
http://arxiv.org/abs/2004.00857v1,Average Reward Adjusted Discounted Reinforcement Learning: Near-Blackwell-Optimal Policies for Real-World Applications,"Although in recent years reinforcement learning has become very popular the
number of successful applications to different kinds of operations research
problems is rather scarce. Reinforcement learning is based on the well-studied
dynamic programming technique and thus also aims at finding the best stationary
policy for a given Markov Decision Process, but in contrast does not require
any model knowledge. The policy is assessed solely on consecutive states (or
state-action pairs), which are observed while an agent explores the solution
space. The contributions of this paper are manifold. First we provide deep
theoretical insights to the widely applied standard discounted reinforcement
learning framework, which give rise to the understanding of why these
algorithms are inappropriate when permanently provided with non-zero rewards,
such as costs or profit. Second, we establish a novel near-Blackwell-optimal
reinforcement learning algorithm. In contrary to former method it assesses the
average reward per step separately and thus prevents the incautious combination
of different types of state values. Thereby, the Laurent Series expansion of
the discounted state values forms the foundation for this development and also
provides the connection between the two approaches. Finally, we prove the
viability of our algorithm on a challenging problem set, which includes a
well-studied M/M/1 admission control queuing system. In contrast to standard
discounted reinforcement learning our algorithm infers the optimal policy on
all tested problems. The insights are that in the operations research domain
machine learning techniques have to be adapted and advanced to successfully
apply these methods in our settings.",['Manuel Schneckenreither'],"['cs.LG', 'stat.ML']",2020-04-02 08:05:18+00:00
http://arxiv.org/abs/2004.01546v1,Temporarily-Aware Context Modelling using Generative Adversarial Networks for Speech Activity Detection,"This paper presents a novel framework for Speech Activity Detection (SAD).
Inspired by the recent success of multi-task learning approaches in the speech
processing domain, we propose a novel joint learning framework for SAD. We
utilise generative adversarial networks to automatically learn a loss function
for joint prediction of the frame-wise speech/ non-speech classifications
together with the next audio segment. In order to exploit the temporal
relationships within the input signal, we propose a temporal discriminator
which aims to ensure that the predicted signal is temporally consistent. We
evaluate the proposed framework on multiple public benchmarks, including NIST
OpenSAT' 17, AMI Meeting and HAVIC, where we demonstrate its capability to
outperform state-of-the-art SAD approaches. Furthermore, our cross-database
evaluations demonstrate the robustness of the proposed approach across
different languages, accents, and acoustic environments.","['Tharindu Fernando', 'Sridha Sridharan', 'Mitchell McLaren', 'Darshana Priyasad', 'Simon Denman', 'Clinton Fookes']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2020-04-02 02:33:13+00:00
http://arxiv.org/abs/2004.02965v2,TSception: A Deep Learning Framework for Emotion Detection Using EEG,"In this paper, we propose a deep learning framework, TSception, for emotion
detection from electroencephalogram (EEG). TSception consists of temporal and
spatial convolutional layers, which learn discriminative representations in the
time and channel domains simultaneously. The temporal learner consists of
multi-scale 1D convolutional kernels whose lengths are related to the sampling
rate of the EEG signal, which learns multiple temporal and frequency
representations. The spatial learner takes advantage of the asymmetry property
of emotion responses at the frontal brain area to learn the discriminative
representations from the left and right hemispheres of the brain. In our study,
a system is designed to study the emotional arousal in an immersive virtual
reality (VR) environment. EEG data were collected from 18 healthy subjects
using this system to evaluate the performance of the proposed deep learning
network for the classification of low and high emotional arousal states. The
proposed method is compared with SVM, EEGNet, and LSTM. TSception achieves a
high classification accuracy of 86.03%, which outperforms the prior methods
significantly (p<0.05). The code is available at
https://github.com/deepBrains/TSception","['Yi Ding', 'Neethu Robinson', 'Qiuhao Zeng', 'Duo Chen', 'Aung Aung Phyo Wai', 'Tih-Shih Lee', 'Cuntai Guan']","['eess.SP', 'cs.LG', 'stat.ML']",2020-04-02 02:10:07+00:00
http://arxiv.org/abs/2004.03712v1,Heart Sound Segmentation using Bidirectional LSTMs with Attention,"This paper proposes a novel framework for the segmentation of phonocardiogram
(PCG) signals into heart states, exploiting the temporal evolution of the PCG
as well as considering the salient information that it provides for the
detection of the heart state. We propose the use of recurrent neural networks
and exploit recent advancements in attention based learning to segment the PCG
signal. This allows the network to identify the most salient aspects of the
signal and disregard uninformative information. The proposed method attains
state-of-the-art performance on multiple benchmarks including both human and
animal heart recordings. Furthermore, we empirically analyse different feature
combinations including envelop features, wavelet and Mel Frequency Cepstral
Coefficients (MFCC), and provide quantitative measurements that explore the
importance of different features in the proposed approach. We demonstrate that
a recurrent neural network coupled with attention mechanisms can effectively
learn from irregular and noisy PCG recordings. Our analysis of different
feature combinations shows that MFCC features and their derivatives offer the
best performance compared to classical wavelet and envelop features. Heart
sound segmentation is a crucial pre-processing step for many diagnostic
applications. The proposed method provides a cost effective alternative to
labour extensive manual segmentation, and provides a more accurate segmentation
than existing methods. As such, it can improve the performance of further
analysis including the detection of murmurs and ejection clicks. The proposed
method is also applicable for detection and segmentation of other one
dimensional biomedical signals.","['Tharindu Fernando', 'Houman Ghaemmaghami', 'Simon Denman', 'Sridha Sridharan', 'Nayyar Hussain', 'Clinton Fookes']","['eess.SP', 'cs.LG', 'cs.SD', 'eess.AS', 'q-bio.QM', 'stat.ML']",2020-04-02 02:09:11+00:00
http://arxiv.org/abs/2004.00762v1,In Automation We Trust: Investigating the Role of Uncertainty in Active Learning Systems,"We investigate how different active learning (AL) query policies coupled with
classification uncertainty visualizations affect analyst trust in automated
classification systems. A current standard policy for AL is to query the oracle
(e.g., the analyst) to refine labels for datapoints where the classifier has
the highest uncertainty. This is an optimal policy for the automation system as
it yields maximal information gain. However, model-centric policies neglect the
effects of this uncertainty on the human component of the system and the
consequent manner in which the human will interact with the system
post-training. In this paper, we present an empirical study evaluating how AL
query policies and visualizations lending transparency to classification
influence trust in automated classification of image data. We found that query
policy significantly influences an analyst's trust in an image classification
system, and we use these results to propose a set of oracle query policies and
visualizations for use during AL training phases that can influence analyst
trust in classification.","['Michael L. Iuzzolino', 'Tetsumichi Umada', 'Nisar R. Ahmed', 'Danielle A. Szafir']","['cs.LG', 'cs.HC', 'stat.ML']",2020-04-02 00:52:49+00:00
http://arxiv.org/abs/2004.00715v2,An approximate KLD based experimental design for models with intractable likelihoods,"Data collection is a critical step in statistical inference and data science,
and the goal of statistical experimental design (ED) is to find the data
collection setup that can provide most information for the inference. In this
work we consider a special type of ED problems where the likelihoods are not
available in a closed form. In this case, the popular information-theoretic
Kullback-Leibler divergence (KLD) based design criterion can not be used
directly, as it requires to evaluate the likelihood function. To address the
issue, we derive a new utility function, which is a lower bound of the original
KLD utility. This lower bound is expressed in terms of the summation of two or
more entropies in the data space, and thus can be evaluated efficiently via
entropy estimation methods. We provide several numerical examples to
demonstrate the performance of the proposed method.","['Ziqiao Ao', 'Jinglai Li']","['stat.CO', 'stat.ML']",2020-04-01 21:18:28+00:00
http://arxiv.org/abs/2004.01495v3,Can Machine Learning Be Used to Recognize and Diagnose Coughs?,"Emerging wireless technologies, such as 5G and beyond, are bringing new use
cases to the forefront, one of the most prominent being machine learning
empowered health care. One of the notable modern medical concerns that impose
an immense worldwide health burden are respiratory infections. Since cough is
an essential symptom of many respiratory infections, an automated system to
screen for respiratory diseases based on raw cough data would have a multitude
of beneficial research and medical applications. In literature, machine
learning has already been successfully used to detect cough events in
controlled environments. In this paper, we present a low complexity, automated
recognition and diagnostic tool for screening respiratory infections that
utilizes Convolutional Neural Networks (CNNs) to detect cough within
environment audio and diagnose three potential illnesses (i.e., bronchitis,
bronchiolitis and pertussis) based on their unique cough audio features. Both
proposed detection and diagnosis models achieve an accuracy of over 89%, while
also remaining computationally efficient. Results show that the proposed system
is successfully able to detect and separate cough events from background noise.
Moreover, the proposed single diagnosis model is capable of distinguishing
between different illnesses without the need of separate models.","['Charles Bales', 'Muhammad Nabeel', 'Charles N. John', 'Usama Masood', 'Haneya N. Qureshi', 'Hasan Farooq', 'Iryna Posokhova', 'Ali Imran']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2020-04-01 20:14:36+00:00
http://arxiv.org/abs/2004.00668v2,Understanding Global Feature Contributions With Additive Importance Measures,"Understanding the inner workings of complex machine learning models is a
long-standing problem and most recent research has focused on local
interpretability. To assess the role of individual input features in a global
sense, we explore the perspective of defining feature importance through the
predictive power associated with each feature. We introduce two notions of
predictive power (model-based and universal) and formalize this approach with a
framework of additive importance measures, which unifies numerous methods in
the literature. We then propose SAGE, a model-agnostic method that quantifies
predictive power while accounting for feature interactions. Our experiments
show that SAGE can be calculated efficiently and that it assigns more accurate
importance values than other methods.","['Ian Covert', 'Scott Lundberg', 'Su-In Lee']","['cs.LG', 'stat.ML']",2020-04-01 19:17:58+00:00
http://arxiv.org/abs/2004.00667v2,Projection Pursuit Gaussian Process Regression,"A primary goal of computer experiments is to reconstruct the function given
by the computer code via scattered evaluations. Traditional isotropic Gaussian
process models suffer from the curse of dimensionality, when the input
dimension is relatively high given limited data points. Gaussian process models
with additive correlation functions are scalable to dimensionality, but they
are more restrictive as they only work for additive functions. In this work, we
consider a projection pursuit model, in which the nonparametric part is driven
by an additive Gaussian process regression. We choose the dimension of the
additive function higher than the original input dimension, and call this
strategy ""dimension expansion"". We show that dimension expansion can help
approximate more complex functions. A gradient descent algorithm is proposed
for model training based on the maximum likelihood estimation. Simulation
studies show that the proposed method outperforms the traditional Gaussian
process models. The Supplementary Materials are available online.","['Gecheng Chen', 'Rui Tuo']","['stat.ML', 'cs.LG']",2020-04-01 19:12:01+00:00
http://arxiv.org/abs/2004.03455v1,Deep Learning Based Multi-Label Text Classification of UNGA Resolutions,"The main goal of this research is to produce a useful software for United
Nations (UN), that could help to speed up the process of qualifying the UN
documents following the Sustainable Development Goals (SDGs) in order to
monitor the progresses at the world level to fight poverty, discrimination,
climate changes. In fact human labeling of UN documents would be a daunting
task given the size of the impacted corpus. Thus, automatic labeling must be
adopted at least as a first step of a multi-phase process to reduce the overall
effort of cataloguing and classifying. Deep Learning (DL) is nowadays one of
the most powerful tools for state-of-the-art (SOTA) AI for this task, but very
often it comes with the cost of an expensive and error-prone preparation of a
training-set. In the case of multi-label text classification of domain-specific
text it seems that we cannot effectively adopt DL without a big-enough
domain-specific training-set. In this paper, we show that this is not always
true. In fact we propose a novel method that is able, through statistics like
TF-IDF, to exploit pre-trained SOTA DL models (such as the Universal Sentence
Encoder) without any need for traditional transfer learning or any other
expensive training procedure. We show the effectiveness of our method in a
legal context, by classifying UN Resolutions according to their most related
SDGs.","['Francesco Sovrano', 'Monica Palmirani', 'Fabio Vitali']","['cs.IR', 'cs.LG', 'stat.ML']",2020-04-01 18:54:38+00:00
http://arxiv.org/abs/2004.00663v1,Synchronizing Probability Measures on Rotations via Optimal Transport,"We introduce a new paradigm, $\textit{measure synchronization}$, for
synchronizing graphs with measure-valued edges. We formulate this problem as
maximization of the cycle-consistency in the space of probability measures over
relative rotations. In particular, we aim at estimating marginal distributions
of absolute orientations by synchronizing the $\textit{conditional}$ ones,
which are defined on the Riemannian manifold of quaternions. Such graph
optimization on distributions-on-manifolds enables a natural treatment of
multimodal hypotheses, ambiguities and uncertainties arising in many computer
vision applications such as SLAM, SfM, and object pose estimation. We first
formally define the problem as a generalization of the classical rotation graph
synchronization, where in our case the vertices denote probability measures
over rotations. We then measure the quality of the synchronization by using
Sinkhorn divergences, which reduces to other popular metrics such as
Wasserstein distance or the maximum mean discrepancy as limit cases. We propose
a nonparametric Riemannian particle optimization approach to solve the problem.
Even though the problem is non-convex, by drawing a connection to the recently
proposed sparse optimization methods, we show that the proposed algorithm
converges to the global optimum in a special case of the problem under certain
conditions. Our qualitative and quantitative experiments show the validity of
our approach and we bring in new perspectives to the study of synchronization.","['Tolga Birdal', 'Michael Arbel', 'Umut Şimşekli', 'Leonidas Guibas']","['cs.CV', 'cs.GR', 'cs.LG', 'cs.RO', 'stat.ML']",2020-04-01 18:44:18+00:00
http://arxiv.org/abs/2004.00658v2,Sequential Feature Classification in the Context of Redundancies,"The problem of all-relevant feature selection is concerned with finding a
relevant feature set with preserved redundancies. There exist several
approximations to solve this problem but only one could give a distinction
between strong and weak relevance. This approach was limited to the case of
linear problems. In this work, we present a new solution for this distinction
in the non-linear case through the use of random forest models and statistical
methods.","['Lukas Pfannschmidt', 'Barbara Hammer']","['cs.LG', 'stat.ML']",2020-04-01 18:20:51+00:00
http://arxiv.org/abs/2004.00642v1,"Object-Centric Image Generation with Factored Depths, Locations, and Appearances","We present a generative model of images that explicitly reasons over the set
of objects they show. Our model learns a structured latent representation that
separates objects from each other and from the background; unlike prior works,
it explicitly represents the 2D position and depth of each object, as well as
an embedding of its segmentation mask and appearance. The model can be trained
from images alone in a purely unsupervised fashion without the need for object
masks or depth information. Moreover, it always generates complete objects,
even though a significant fraction of training images contain occlusions.
Finally, we show that our model can infer decompositions of novel images into
their constituent objects, including accurate prediction of depth ordering and
segmentation of occluded parts.","['Titas Anciukevicius', 'Christoph H. Lampert', 'Paul Henderson']","['cs.LG', 'cs.CV', 'stat.ML']",2020-04-01 18:00:11+00:00
http://arxiv.org/abs/2004.00601v2,Parallel Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints,"Real-world problems often involve the optimization of several objectives
under multiple constraints. An example is the hyper-parameter tuning problem of
machine learning algorithms. In particular, the minimization of the estimation
of the generalization error of a deep neural network and at the same time the
minimization of its prediction time. We may also consider as a constraint that
the deep neural network must be implemented in a chip with an area below some
size. Here, both the objectives and the constraint are black boxes, i.e.,
functions whose analytical expressions are unknown and are expensive to
evaluate. Bayesian optimization (BO) methodologies have given state-of-the-art
results for the optimization of black-boxes. Nevertheless, most BO methods are
sequential and evaluate the objectives and the constraints at just one input
location, iteratively. Sometimes, however, we may have resources to evaluate
several configurations in parallel. Notwithstanding, no parallel BO method has
been proposed to deal with the optimization of multiple objectives under
several constraints. If the expensive evaluations can be carried out in
parallel (as when a cluster of computers is available), sequential evaluations
result in a waste of resources. This article introduces PPESMOC, Parallel
Predictive Entropy Search for Multi-objective Bayesian Optimization with
Constraints, an information-based batch method for the simultaneous
optimization of multiple expensive-to-evaluate black-box functions under the
presence of several constraints. Iteratively, PPESMOC selects a batch of input
locations at which to evaluate the black-boxes so as to maximally reduce the
entropy of the Pareto set of the optimization problem. We present empirical
evidence in the form of synthetic, benchmark and real-world experiments that
illustrate the effectiveness of PPESMOC.","['Eduardo C. Garrido-Merchán', 'Daniel Hernández-Lobato']","['stat.ML', 'cs.LG', 'math.OC']",2020-04-01 17:37:58+00:00
http://arxiv.org/abs/2004.01022v1,Provable Sample Complexity Guarantees for Learning of Continuous-Action Graphical Games with Nonparametric Utilities,"In this paper, we study the problem of learning the exact structure of
continuous-action games with non-parametric utility functions. We propose an
$\ell_1$ regularized method which encourages sparsity of the coefficients of
the Fourier transform of the recovered utilities. Our method works by accessing
very few Nash equilibria and their noisy utilities. Under certain technical
conditions, our method also recovers the exact structure of these utility
functions, and thus, the exact structure of the game. Furthermore, our method
only needs a logarithmic number of samples in terms of the number of players
and runs in polynomial time. We follow the primal-dual witness framework to
provide provable theoretical guarantees.","['Adarsh Barik', 'Jean Honorio']","['cs.GT', 'cs.LG', 'stat.ML']",2020-04-01 17:32:27+00:00
http://arxiv.org/abs/2004.01024v1,Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN,"Network embedding aims to learn low-dimensional representations of nodes
while capturing structure information of networks. It has achieved great
success on many tasks of network analysis such as link prediction and node
classification. Most of existing network embedding algorithms focus on how to
learn static homogeneous networks effectively. However, networks in the real
world are more complex, e.g., networks may consist of several types of nodes
and edges (called heterogeneous information) and may vary over time in terms of
dynamic nodes and edges (called evolutionary patterns). Limited work has been
done for network embedding of dynamic heterogeneous networks as it is
challenging to learn both evolutionary and heterogeneous information
simultaneously. In this paper, we propose a novel dynamic heterogeneous network
embedding method, termed as DyHATR, which uses hierarchical attention to learn
heterogeneous information and incorporates recurrent neural networks with
temporal attention to capture evolutionary patterns. We benchmark our method on
four real-world datasets for the task of link prediction. Experimental results
show that DyHATR significantly outperforms several state-of-the-art baselines.","['Hansheng Xue', 'Luwei Yang', 'Wen Jiang', 'Yi Wei', 'Yi Hu', 'Yu Lin']","['cs.SI', 'cs.LG', 'stat.ML']",2020-04-01 17:16:47+00:00
http://arxiv.org/abs/2004.00574v1,From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction,"We propose spectral methods for long-term forecasting of temporal signals
stemming from linear and nonlinear quasi-periodic dynamical systems. For linear
signals, we introduce an algorithm with similarities to the Fourier transform
but which does not rely on periodicity assumptions, allowing for forecasting
given potentially arbitrary sampling intervals. We then extend this algorithm
to handle nonlinearities by leveraging Koopman theory. The resulting algorithm
performs a spectral decomposition in a nonlinear, data-dependent basis. The
optimization objective for both algorithms is highly non-convex. However,
expressing the objective in the frequency domain allows us to compute global
optima of the error surface in a scalable and efficient manner, partially by
exploiting the computational properties of the Fast Fourier Transform. Because
of their close relation to Bayesian Spectral Analysis, uncertainty
quantification metrics are a natural byproduct of the spectral forecasting
methods. We extensively benchmark these algorithms against other leading
forecasting methods on a range of synthetic experiments as well as in the
context of real-world power systems and fluid flows.","['Henning Lange', 'Steven L. Brunton', 'Nathan Kutz']","['cs.LG', 'eess.SP', 'stat.ML']",2020-04-01 17:04:02+00:00
http://arxiv.org/abs/2004.00570v2,Tightened Convex Relaxations for Neural Network Robustness Certification,"In this paper, we consider the problem of certifying the robustness of neural
networks to perturbed and adversarial input data. Such certification is
imperative for the application of neural networks in safety-critical
decision-making and control systems. Certification techniques using convex
optimization have been proposed, but they often suffer from relaxation errors
that void the certificate. Our work exploits the structure of ReLU networks to
improve relaxation errors through a novel partition-based certification
procedure. The proposed method is proven to tighten existing linear programming
relaxations, and asymptotically achieves zero relaxation error as the partition
is made finer. We develop a finite partition that attains zero relaxation error
and use the result to derive a tractable partitioning scheme that minimizes the
worst-case relaxation error. Experiments using real data show that the
partitioning procedure is able to issue robustness certificates in cases where
prior methods fail. Consequently, partition-based certification procedures are
found to provide an intuitive, effective, and theoretically justified method
for tightening existing convex relaxation techniques.","['Brendon G. Anderson', 'Ziye Ma', 'Jingqi Li', 'Somayeh Sojoudi']","['cs.LG', 'math.OC', 'stat.ML']",2020-04-01 16:59:21+00:00
http://arxiv.org/abs/2004.00568v1,One-shot path planning for multi-agent systems using fully convolutional neural network,"Path planning plays a crucial role in robot action execution, since a path or
a motion trajectory for a particular action has to be defined first before the
action can be executed. Most of the current approaches are iterative methods
where the trajectory is generated iteratively by predicting the next state
based on the current state. Moreover, in case of multi-agent systems, paths are
planned for each agent separately. In contrast to that, we propose a novel
method by utilising fully convolutional neural network, which allows generation
of complete paths, even for more than one agent, in one-shot, i.e., with a
single prediction step. We demonstrate that our method is able to successfully
generate optimal or close to optimal paths in more than 98\% of the cases for
single path predictions. Moreover, we show that although the network has never
been trained on multi-path planning it is also able to generate optimal or
close to optimal paths in 85.7\% and 65.4\% of the cases when generating two
and three paths, respectively.","['Tomas Kulvicius', 'Sebastian Herzog', 'Timo Lüddecke', 'Minija Tamosiunaite', 'Florentin Wörgötter']","['cs.LG', 'stat.ML']",2020-04-01 16:56:39+00:00
http://arxiv.org/abs/2004.00567v2,Obstacle Tower Without Human Demonstrations: How Far a Deep Feed-Forward Network Goes with Reinforcement Learning,"The Obstacle Tower Challenge is the task to master a procedurally generated
chain of levels that subsequently get harder to complete. Whereas the most top
performing entries of last year's competition used human demonstrations or
reward shaping to learn how to cope with the challenge, we present an approach
that performed competitively (placed 7th) but starts completely from scratch by
means of Deep Reinforcement Learning with a relatively simple feed-forward deep
network structure. We especially look at the generalization performance of the
taken approach concerning different seeds and various visual themes that have
become available after the competition, and investigate where the agent fails
and why. Note that our approach does not possess a short-term memory like
employing recurrent hidden states. With this work, we hope to contribute to a
better understanding of what is possible with a relatively simple, flexible
solution that can be applied to learning in environments featuring complex 3D
visual input where the abstract task structure itself is still fairly simple.","['Marco Pleines', 'Jenia Jitsev', 'Mike Preuss', 'Frank Zimmer']","['cs.LG', 'cs.AI', 'stat.ML']",2020-04-01 16:55:51+00:00
http://arxiv.org/abs/2004.00566v5,Assisted Learning: A Framework for Multi-Organization Learning,"In an increasing number of AI scenarios, collaborations among different
organizations or agents (e.g., human and robots, mobile units) are often
essential to accomplish an organization-specific mission. However, to avoid
leaking useful and possibly proprietary information, organizations typically
enforce stringent security constraints on sharing modeling algorithms and data,
which significantly limits collaborations. In this work, we introduce the
Assisted Learning framework for organizations to assist each other in
supervised learning tasks without revealing any organization's algorithm, data,
or even task. An organization seeks assistance by broadcasting task-specific
but nonsensitive statistics and incorporating others' feedback in one or more
iterations to eventually improve its predictive performance. Theoretical and
experimental studies, including real-world medical benchmarks, show that
Assisted Learning can often achieve near-oracle learning performance as if data
and training processes were centralized.","['Xun Xian', 'Xinran Wang', 'Jie Ding', 'Reza Ghanadan']","['cs.LG', 'cs.CR', 'stat.ML']",2020-04-01 16:54:49+00:00
http://arxiv.org/abs/2004.00558v1,Multi-label learning for dynamic model type recommendation,"Dynamic selection techniques aim at selecting the local experts around each
test sample in particular for performing its classification. While generating
the classifier on a local scope may make it easier for singling out the locally
competent ones, as in the online local pool (OLP) technique, using the same
base-classifier model in uneven distributions may restrict the local level of
competence, since each region may have a data distribution that favors one
model over the others. Thus, we propose in this work a problem-independent
dynamic base-classifier model recommendation for the OLP technique, which uses
information regarding the behavior of a portfolio of models over the samples of
different problems to recommend one (or several) of them on a per-instance
manner. Our proposed framework builds a multi-label meta-classifier responsible
for recommending a set of relevant model types based on the local data
complexity of the region surrounding each test sample. The OLP technique then
produces a local pool with the model that yields the highest probability score
of the meta-classifier. Experimental results show that different data
distributions favored different model types on a local scope. Moreover, based
on the performance of an ideal model type selector, it was observed that there
is a clear advantage in choosing a relevant model type for each test instance.
Overall, the proposed model type recommender system yielded a statistically
similar performance to the original OLP with fixed base-classifier model. Given
the novelty of the approach and the gap in performance between the proposed
framework and the ideal selector, we regard this as a promising research
direction. Code available at
github.com/marianaasouza/dynamic-model-recommender.","['Mariana A. Souza', 'Robert Sabourin', 'George D. C. Cavalcanti', 'Rafael M. O. Cruz']","['cs.LG', 'stat.ML']",2020-04-01 16:42:12+00:00
http://arxiv.org/abs/2004.00557v2,Statistical Queries and Statistical Algorithms: Foundations and Applications,"We give a survey of the foundations of statistical queries and their many
applications to other areas. We introduce the model, give the main definitions,
and we explore the fundamental theory statistical queries and how how it
connects to various notions of learnability. We also give a detailed summary of
some of the applications of statistical queries to other areas, including to
optimization, to evolvability, and to differential privacy.",['Lev Reyzin'],"['cs.LG', 'cs.CC', 'stat.ML']",2020-04-01 16:37:10+00:00
http://arxiv.org/abs/2004.01028v1,DeepSIBA: Chemical Structure-based Inference of Biological Alterations,"Predicting whether a chemical structure shares a desired biological effect
can have a significant impact for in-silico compound screening in early drug
discovery. In this study, we developed a deep learning model where compound
structures are represented as graphs and then linked to their biological
footprint. To make this complex problem computationally tractable, compound
differences were mapped to biological effect alterations using Siamese Graph
Convolutional Neural Networks. The proposed model was able to learn new
representations from chemical structures and identify structurally dissimilar
compounds that affect similar biological processes with high precision.
Additionally, by utilizing deep ensembles to estimate uncertainty, we were able
to provide reliable and accurate predictions for chemical structures that are
very different from the ones used during training. Finally, we present a novel
inference approach, where the trained models are used to estimate the signaling
pathways affected by a compound perturbation in a specific cell line, using
only its chemical structure as input. As a use case, this approach was used to
infer signaling pathways affected by FDA-approved anticancer drugs.","['C. Fotis', 'N. Meimetis', 'A. Sardis', 'L. G. Alexopoulos']","['q-bio.QM', 'cs.LG', 'stat.ML']",2020-04-01 16:29:45+00:00
http://arxiv.org/abs/2004.00540v1,Generation of Paths in a Maze using a Deep Network without Learning,"Trajectory- or path-planning is a fundamental issue in a wide variety of
applications. Here we show that it is possible to solve path planning for
multiple start- and end-points highly efficiently with a network that consists
only of max pooling layers, for which no network training is needed. Different
from competing approaches, very large mazes containing more than half a billion
nodes with dense obstacle configuration and several thousand path end-points
can this way be solved in very short time on parallel hardware.","['Tomas Kulvicius', 'Sebastian Herzog', 'Minija Tamosiunaite', 'Florentin Wörgötter']","['cs.LG', 'stat.ML']",2020-04-01 16:08:45+00:00
http://arxiv.org/abs/2004.00530v1,Learning Sparse Rewarded Tasks from Sub-Optimal Demonstrations,"Model-free deep reinforcement learning (RL) has demonstrated its superiority
on many complex sequential decision-making problems. However, heavy dependence
on dense rewards and high sample-complexity impedes the wide adoption of these
methods in real-world scenarios. On the other hand, imitation learning (IL)
learns effectively in sparse-rewarded tasks by leveraging the existing expert
demonstrations. In practice, collecting a sufficient amount of expert
demonstrations can be prohibitively expensive, and the quality of
demonstrations typically limits the performance of the learning policy. In this
work, we propose Self-Adaptive Imitation Learning (SAIL) that can achieve
(near) optimal performance given only a limited number of sub-optimal
demonstrations for highly challenging sparse reward tasks. SAIL bridges the
advantages of IL and RL to reduce the sample complexity substantially, by
effectively exploiting sup-optimal demonstrations and efficiently exploring the
environment to surpass the demonstrated performance. Extensive empirical
results show that not only does SAIL significantly improve the
sample-efficiency but also leads to much better final performance across
different continuous control tasks, comparing to the state-of-the-art.","['Zhuangdi Zhu', 'Kaixiang Lin', 'Bo Dai', 'Jiayu Zhou']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2020-04-01 15:57:15+00:00
http://arxiv.org/abs/2004.00478v1,Distance and Equivalence between Finite State Machines and Recurrent Neural Networks: Computational results,"The need of interpreting Deep Learning (DL) models has led, during the past
years, to a proliferation of works concerned by this issue. Among strategies
which aim at shedding some light on how information is represented internally
in DL models, one consists in extracting symbolic rule-based machines from
connectionist models that are supposed to approximate well their behaviour. In
order to better understand how reasonable these approximation strategies are,
we need to know the computational complexity of measuring the quality of
approximation. In this article, we will prove some computational results
related to the problem of extracting Finite State Machine (FSM) based models
from trained RNN Language models. More precisely, we'll show the following: (a)
For general weighted RNN-LMs with a single hidden layer and a ReLu activation:
- The equivalence problem of a PDFA/PFA/WFA and a weighted first-order RNN-LM
is undecidable; - As a corollary, the distance problem between languages
generated by PDFA/PFA/WFA and that of a weighted RNN-LM is not recursive; -The
intersection between a DFA and the cut language of a weighted RNN-LM is
undecidable; - The equivalence of a PDFA/PFA/WFA and weighted RNN-LM in a
finite support is EXP-Hard; (b) For consistent weight RNN-LMs with any
computable activation function: - The Tcheybechev distance approximation is
decidable; - The Tcheybechev distance approximation in a finite support is
NP-Hard. Moreover, our reduction technique from 3-SAT makes this latter fact
easily generalizable to other RNN architectures (e.g. LSTMs/RNNs), and RNNs
with finite precision.","['Reda Marzouk', 'Colin de la Higuera']","['cs.LG', 'stat.ML']",2020-04-01 14:48:59+00:00
http://arxiv.org/abs/2004.00464v1,Deep transformation models: Tackling complex regression problems with neural network based transformation models,"We present a deep transformation model for probabilistic regression. Deep
learning is known for outstandingly accurate predictions on complex data but in
regression tasks, it is predominantly used to just predict a single number.
This ignores the non-deterministic character of most tasks. Especially if
crucial decisions are based on the predictions, like in medical applications,
it is essential to quantify the prediction uncertainty. The presented deep
learning transformation model estimates the whole conditional probability
distribution, which is the most thorough way to capture uncertainty about the
outcome. We combine ideas from a statistical transformation model (most likely
transformation) with recent transformation models from deep learning
(normalizing flows) to predict complex outcome distributions. The core of the
method is a parameterized transformation function which can be trained with the
usual maximum likelihood framework using gradient descent. The method can be
combined with existing deep learning architectures. For small machine learning
benchmark datasets, we report state of the art performance for most dataset and
partly even outperform it. Our method works for complex input data, which we
demonstrate by employing a CNN architecture on image data.","['Beate Sick', 'Torsten Hothorn', 'Oliver Dürr']","['stat.ML', 'cs.LG']",2020-04-01 14:23:12+00:00
http://arxiv.org/abs/2004.00438v1,Handling Concept Drifts in Regression Problems -- the Error Intersection Approach,"Machine learning models are omnipresent for predictions on big data. One
challenge of deployed models is the change of the data over time, a phenomenon
called concept drift. If not handled correctly, a concept drift can lead to
significant mispredictions. We explore a novel approach for concept drift
handling, which depicts a strategy to switch between the application of simple
and complex machine learning models for regression tasks. We assume that the
approach plays out the individual strengths of each model, switching to the
simpler model if a drift occurs and switching back to the complex model for
typical situations. We instantiate the approach on a real-world data set of
taxi demand in New York City, which is prone to multiple drifts, e.g. the
weather phenomena of blizzards, resulting in a sudden decrease of taxi demand.
We are able to show that our suggested approach outperforms all regarded
baselines significantly.","['Lucas Baier', 'Marcel Hofmann', 'Niklas Kühl', 'Marisa Mohr', 'Gerhard Satzger']","['cs.LG', 'stat.ML']",2020-04-01 13:30:05+00:00
http://arxiv.org/abs/2004.00433v1,Anomaly Detection in Univariate Time-series: A Survey on the State-of-the-Art,"Anomaly detection for time-series data has been an important research field
for a long time. Seminal work on anomaly detection methods has been focussing
on statistical approaches. In recent years an increasing number of machine
learning algorithms have been developed to detect anomalies on time-series.
Subsequently, researchers tried to improve these techniques using (deep) neural
networks. In the light of the increasing number of anomaly detection methods,
the body of research lacks a broad comparative evaluation of statistical,
machine learning and deep learning methods. This paper studies 20 univariate
anomaly detection methods from the all three categories. The evaluation is
conducted on publicly available datasets, which serve as benchmarks for
time-series anomaly detection. By analyzing the accuracy of each method as well
as the computation time of the algorithms, we provide a thorough insight about
the performance of these anomaly detection approaches, alongside some general
notion of which method is suited for a certain type of data.","['Mohammad Braei', 'Sebastian Wagner']","['cs.LG', 'stat.ML']",2020-04-01 13:22:34+00:00
http://arxiv.org/abs/2004.00431v2,M2m: Imbalanced Classification via Major-to-minor Translation,"In most real-world scenarios, labeled training datasets are highly
class-imbalanced, where deep neural networks suffer from generalizing to a
balanced testing criterion. In this paper, we explore a novel yet simple way to
alleviate this issue by augmenting less-frequent classes via translating
samples (e.g., images) from more-frequent classes. This simple approach enables
a classifier to learn more generalizable features of minority classes, by
transferring and leveraging the diversity of the majority information. Our
experimental results on a variety of class-imbalanced datasets show that the
proposed method improves the generalization on minority classes significantly
compared to other existing re-sampling or re-weighting methods. The performance
of our method even surpasses those of previous state-of-the-art methods for the
imbalanced classification.","['Jaehyung Kim', 'Jongheon Jeong', 'Jinwoo Shin']","['cs.CV', 'cs.LG', 'stat.ML']",2020-04-01 13:21:17+00:00
http://arxiv.org/abs/2004.00412v2,Total Variation Regularization for Compartmental Epidemic Models with Time-Varying Dynamics,"Compartmental epidemic models are among the most popular ones in
epidemiology. For the parameters (e.g., the transmission rate) characterizing
these models, the majority of researchers simplify them as constants, while
some others manage to detect their continuous variations. In this paper, we aim
at capturing, on the other hand, discontinuous variations, which better
describe the impact of many noteworthy events, such as city lockdowns, the
opening of field hospitals, and the mutation of the virus, whose effect should
be instant. To achieve this, we balance the model's likelihood by total
variation, which regulates the temporal variations of the model parameters. To
infer these parameters, instead of using Monte Carlo methods, we design a novel
yet straightforward optimization algorithm, dubbed Iterated Nelder--Mead, which
repeatedly applies the Nelder--Mead algorithm. Experiments conducted on the
simulated data demonstrate that our approach can reproduce these
discontinuities and precisely depict the epidemics.",['Wenjie Zheng'],"['stat.ML', 'cs.LG', 'q-bio.QM']",2020-04-01 13:06:10+00:00
http://arxiv.org/abs/2004.00407v1,Drug-disease Graph: Predicting Adverse Drug Reaction Signals via Graph Neural Network with Clinical Data,"Adverse Drug Reaction (ADR) is a significant public health concern
world-wide. Numerous graph-based methods have been applied to biomedical graphs
for predicting ADRs in pre-marketing phases. ADR detection in post-market
surveillance is no less important than pre-marketing assessment, and ADR
detection with large-scale clinical data have attracted much attention in
recent years. However, there are not many studies considering graph structures
from clinical data for detecting an ADR signal, which is a pair of a
prescription and a diagnosis that might be a potential ADR. In this study, we
develop a novel graph-based framework for ADR signal detection using healthcare
claims data. We construct a Drug-disease graph with nodes representing the
medical codes. The edges are given as the relationships between two codes,
computed using the data. We apply Graph Neural Network to predict ADR signals,
using labels from the Side Effect Resource database. The model shows improved
AUROC and AUPRC performance of 0.795 and 0.775, compared to other algorithms,
showing that it successfully learns node representations expressive of those
relationships. Furthermore, our model predicts ADR pairs that do not exist in
the established ADR database, showing its capability to supplement the ADR
database.","['Heeyoung Kwak', 'Minwoo Lee', 'Seunghyun Yoon', 'Jooyoung Chang', 'Sangmin Park', 'Kyomin Jung']","['cs.LG', 'stat.ML']",2020-04-01 13:01:02+00:00
