id,title,abstract,authors,categories,date
http://arxiv.org/abs/2008.00565v1,Geometrically Enriched Latent Spaces,"A common assumption in generative models is that the generator immerses the
latent space into a Euclidean ambient space. Instead, we consider the ambient
space to be a Riemannian manifold, which allows for encoding domain knowledge
through the associated Riemannian metric. Shortest paths can then be defined
accordingly in the latent space to both follow the learned manifold and respect
the ambient geometry. Through careful design of the ambient metric we can
ensure that shortest paths are well-behaved even for deterministic generators
that otherwise would exhibit a misleading bias. Experimentally we show that our
approach improves interpretability of learned representations both using
stochastic and deterministic generators.","['Georgios Arvanitidis', 'Søren Hauberg', 'Bernhard Schölkopf']","['stat.ML', 'cs.LG']",2020-08-02 20:57:58+00:00
http://arxiv.org/abs/2008.00546v1,A Foliated View of Transfer Learning,"Transfer learning considers a learning process where a new task is solved by
transferring relevant knowledge from known solutions to related tasks. While
this has been studied experimentally, there lacks a foundational description of
the transfer learning problem that exposes what related tasks are, and how they
can be exploited. In this work, we present a definition for relatedness between
tasks and identify foliations as a mathematical framework to represent such
relationships.","['Janith Petangoda', 'Nick A. M. Monk', 'Marc Peter Deisenroth']","['cs.LG', 'stat.ML']",2020-08-02 19:30:59+00:00
http://arxiv.org/abs/2008.00511v2,Curriculum Learning with a Progression Function,"Curriculum Learning for Reinforcement Learning is an increasingly popular
technique that involves training an agent on a sequence of intermediate tasks,
called a Curriculum, to increase the agent's performance and learning speed.
This paper introduces a novel paradigm for curriculum generation based on
progression and mapping functions. While progression functions specify the
complexity of the environment at any given time, mapping functions generate
environments of a specific complexity. Different progression functions are
introduced, including an autonomous online task progression based on the
agent's performance. Our approach's benefits and wide applicability are shown
by empirically comparing its performance to two state-of-the-art Curriculum
Learning algorithms on six domains.","['Andrea Bassich', 'Francesco Foglino', 'Matteo Leonetti', 'Daniel Kudenko']","['cs.LG', 'stat.ML']",2020-08-02 16:18:41+00:00
http://arxiv.org/abs/2008.00504v1,Variational Filtering with Copula Models for SLAM,"The ability to infer map variables and estimate pose is crucial to the
operation of autonomous mobile robots. In most cases the shared dependency
between these variables is modeled through a multivariate Gaussian
distribution, but there are many situations where that assumption is
unrealistic. Our paper shows how it is possible to relax this assumption and
perform simultaneous localization and mapping (SLAM) with a larger class of
distributions, whose multivariate dependency is represented with a copula
model. We integrate the distribution model with copulas into a Sequential Monte
Carlo estimator and show how unknown model parameters can be learned through
gradient-based optimization. We demonstrate our approach is effective in
settings where Gaussian assumptions are clearly violated, such as environments
with uncertain data association and nonlinear transition models.","['John D. Martin', 'Kevin Doherty', 'Caralyn Cyr', 'Brendan Englot', 'John Leonard']","['cs.RO', 'stat.ML']",2020-08-02 15:38:23+00:00
http://arxiv.org/abs/2008.00500v3,Structural Estimation of Partially Observable Markov Decision Processes,"In many practical settings control decisions must be made under
partial/imperfect information about the evolution of a relevant state variable.
Partially Observable Markov Decision Processes (POMDPs) is a relatively
well-developed framework for modeling and analyzing such problems. In this
paper we consider the structural estimation of the primitives of a POMDP model
based upon the observable history of the process. We analyze the structural
properties of POMDP model with random rewards and specify conditions under
which the model is identifiable without knowledge of the state dynamics. We
consider a soft policy gradient algorithm to compute a maximum likelihood
estimator and provide a finite-time characterization of convergence to a
stationary point. We illustrate the estimation methodology with an application
to optimal equipment replacement. In this context, replacement decisions must
be made under partial/imperfect information on the true state (i.e. condition
of the equipment). We use synthetic and real data to highlight the robustness
of the proposed methodology and characterize the potential for misspecification
when partial state observability is ignored.","['Yanling Chang', 'Alfredo Garcia', 'Zhide Wang', 'Lu Sun']","['cs.LG', 'stat.ML']",2020-08-02 15:04:27+00:00
http://arxiv.org/abs/2008.00483v2,Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy,"We study the global convergence and global optimality of actor-critic, one of
the most popular families of reinforcement learning algorithms. While most
existing works on actor-critic employ bi-level or two-timescale updates, we
focus on the more practical single-timescale setting, where the actor and
critic are updated simultaneously. Specifically, in each iteration, the critic
update is obtained by applying the Bellman evaluation operator only once while
the actor is updated in the policy gradient direction computed using the
critic. Moreover, we consider two function approximation settings where both
the actor and critic are represented by linear or deep neural networks. For
both cases, we prove that the actor sequence converges to a globally optimal
policy at a sublinear $O(K^{-1/2})$ rate, where $K$ is the number of
iterations. To the best of our knowledge, we establish the rate of convergence
and global optimality of single-timescale actor-critic with linear function
approximation for the first time. Moreover, under the broader scope of policy
optimization with nonlinear function approximation, we prove that actor-critic
with deep neural network finds the globally optimal policy at a sublinear rate
for the first time.","['Zuyue Fu', 'Zhuoran Yang', 'Zhaoran Wang']","['cs.LG', 'math.OC', 'stat.ML']",2020-08-02 14:01:49+00:00
http://arxiv.org/abs/2008.00444v3,Principles and Algorithms for Forecasting Groups of Time Series: Locality and Globality,"Forecasting groups of time series is of increasing practical importance, e.g.
forecasting the demand for multiple products offered by a retailer or server
loads within a data center. The local approach to this problem considers each
time series separately and fits a function or model to each series. The global
approach fits a single function to all series. For groups of similar time
series, global methods outperform the more established local methods. However,
recent results show good performance of global models even in heterogeneous
datasets. This suggests a more general applicability of global methods,
potentially leading to more accurate tools and new scenarios to study.
  Formalizing the setting of forecasting a set of time series with local and
global methods, we provide the following contributions:
  1) Global methods are not more restrictive than local methods, both can
produce the same forecasts without any assumptions about similarity of the
series. Global models can succeed in a wider range of problems than previously
thought.
  2) Basic generalization bounds for local and global algorithms. The
complexity of local methods grows with the size of the set while it remains
constant for global methods. In large datasets, a global algorithm can afford
to be quite complex and still benefit from better generalization. These bounds
serve to clarify and support recent experimental results in the field, and
guide the design of new algorithms. For the class of autoregressive models,
this implies that global models can have much larger memory than local methods.
  3) In an extensive empirical study, purposely naive algorithms derived from
these principles, such as global linear models or deep networks result in
superior accuracy.
  In particular, global linear models can provide competitive accuracy with two
orders of magnitude fewer parameters than local methods.","['Pablo Montero-Manso', 'Rob J Hyndman']","['cs.LG', 'stat.ML']",2020-08-02 10:22:05+00:00
http://arxiv.org/abs/2008.00422v2,Rule-based Bayesian regression,"We introduce a novel rule-based approach for handling regression problems.
The new methodology carries elements from two frameworks: (i) it provides
information about the uncertainty of the parameters of interest using Bayesian
inference, and (ii) it allows the incorporation of expert knowledge through
rule-based systems. The blending of those two different frameworks can be
particularly beneficial for various domains (e.g. engineering), where, even
though the significance of uncertainty quantification motivates a Bayesian
approach, there is no simple way to incorporate researcher intuition into the
model. We validate our models by applying them to synthetic applications: a
simple linear regression problem and two more complex structures based on
partial differential equations. Finally, we review the advantages of our
methodology, which include the simplicity of the implementation, the
uncertainty reduction due to the added information and, in some occasions, the
derivation of better point predictions, and we address limitations, mainly from
the computational complexity perspective, such as the difficulty in choosing an
appropriate algorithm and the added computational burden.","['Themistoklis Botsas', 'Lachlan R. Mason', 'Indranil Pan']","['stat.ML', 'cs.LG', 'stat.ME']",2020-08-02 07:20:45+00:00
http://arxiv.org/abs/2008.00410v1,Interpretable Rule Discovery Through Bilevel Optimization of Split-Rules of Nonlinear Decision Trees for Classification Problems,"For supervised classification problems involving design, control, other
practical purposes, users are not only interested in finding a highly accurate
classifier, but they also demand that the obtained classifier be easily
interpretable. While the definition of interpretability of a classifier can
vary from case to case, here, by a humanly interpretable classifier we restrict
it to be expressed in simplistic mathematical terms. As a novel approach, we
represent a classifier as an assembly of simple mathematical rules using a
non-linear decision tree (NLDT). Each conditional (non-terminal) node of the
tree represents a non-linear mathematical rule (split-rule) involving features
in order to partition the dataset in the given conditional node into two
non-overlapping subsets. This partitioning is intended to minimize the impurity
of the resulting child nodes. By restricting the structure of split-rule at
each conditional node and depth of the decision tree, the interpretability of
the classifier is assured. The non-linear split-rule at a given conditional
node is obtained using an evolutionary bilevel optimization algorithm, in which
while the upper-level focuses on arriving at an interpretable structure of the
split-rule, the lower-level achieves the most appropriate weights
(coefficients) of individual constituents of the rule to minimize the net
impurity of two resulting child nodes. The performance of the proposed
algorithm is demonstrated on a number of controlled test problems, existing
benchmark problems, and industrial problems. Results on two to 500-feature
problems are encouraging and open up further scopes of applying the proposed
approach to more challenging and complex classification tasks.","['Yashesh Dhebar', 'Kalyanmoy Deb']","['cs.LG', 'cs.NE', 'stat.ML']",2020-08-02 06:35:32+00:00
http://arxiv.org/abs/2008.00404v6,Detecting Beneficial Feature Interactions for Recommender Systems,"Feature interactions are essential for achieving high accuracy in recommender
systems. Many studies take into account the interaction between every pair of
features. However, this is suboptimal because some feature interactions may not
be that relevant to the recommendation result, and taking them into account may
introduce noise and decrease recommendation accuracy. To make the best out of
feature interactions, we propose a graph neural network approach to effectively
model them, together with a novel technique to automatically detect those
feature interactions that are beneficial in terms of recommendation accuracy.
The automatic feature interaction detection is achieved via edge prediction
with an L0 activation regularization. Our proposed model is proved to be
effective through the information bottleneck principle and statistical
interaction theory. Experimental results show that our model (i) outperforms
existing baselines in terms of accuracy, and (ii) automatically identifies
beneficial feature interactions.","['Yixin Su', 'Rui Zhang', 'Sarah Erfani', 'Zhenghua Xu']","['cs.LG', 'cs.IR', 'stat.ML']",2020-08-02 06:08:23+00:00
http://arxiv.org/abs/2008.00386v1,Bayesian Optimization for Selecting Efficient Machine Learning Models,"The performance of many machine learning models depends on their
hyper-parameter settings. Bayesian Optimization has become a successful tool
for hyper-parameter optimization of machine learning algorithms, which aims to
identify optimal hyper-parameters during an iterative sequential process.
However, most of the Bayesian Optimization algorithms are designed to select
models for effectiveness only and ignore the important issue of model training
efficiency. Given that both model effectiveness and training time are important
for real-world applications, models selected for effectiveness may not meet the
strict training time requirements necessary to deploy in a production
environment. In this work, we present a unified Bayesian Optimization framework
for jointly optimizing models for both prediction effectiveness and training
efficiency. We propose an objective that captures the tradeoff between these
two metrics and demonstrate how we can jointly optimize them in a principled
Bayesian Optimization framework. Experiments on model selection for
recommendation tasks indicate models selected this way significantly improves
model training efficiency while maintaining strong effectiveness as compared to
state-of-the-art Bayesian Optimization algorithms.","['Lidan Wang', 'Franck Dernoncourt', 'Trung Bui']","['cs.LG', 'cs.CL', 'cs.NE', 'stat.ML']",2020-08-02 02:56:30+00:00
http://arxiv.org/abs/2008.00357v1,A Causal Lens for Peeking into Black Box Predictive Models: Predictive Model Interpretation via Causal Attribution,"With the increasing adoption of predictive models trained using machine
learning across a wide range of high-stakes applications, e.g., health care,
security, criminal justice, finance, and education, there is a growing need for
effective techniques for explaining such models and their predictions. We aim
to address this problem in settings where the predictive model is a black box;
That is, we can only observe the response of the model to various inputs, but
have no knowledge about the internal structure of the predictive model, its
parameters, the objective function, and the algorithm used to optimize the
model. We reduce the problem of interpreting a black box predictive model to
that of estimating the causal effects of each of the model inputs on the model
output, from observations of the model inputs and the corresponding outputs. We
estimate the causal effects of model inputs on model output using variants of
the Rubin Neyman potential outcomes framework for estimating causal effects
from observational data. We show how the resulting causal attribution of
responsibility for model output to the different model inputs can be used to
interpret the predictive model and to explain its predictions. We present
results of experiments that demonstrate the effectiveness of our approach to
the interpretation of black box predictive models via causal attribution in the
case of deep neural network models trained on one synthetic data set (where the
input variables that impact the output variable are known by design) and two
real-world data sets: Handwritten digit classification, and Parkinson's disease
severity prediction. Because our approach does not require knowledge about the
predictive model algorithm and is free of assumptions regarding the black box
predictive model except that its input-output responses be observable, it can
be applied, in principle, to any black box predictive model.","['Aria Khademi', 'Vasant Honavar']","['cs.LG', 'cs.AI', 'stat.ML']",2020-08-01 23:20:57+00:00
http://arxiv.org/abs/2008.01558v2,Federated Learning with Sparsification-Amplified Privacy and Adaptive Optimization,"Federated learning (FL) enables distributed agents to collaboratively learn a
centralized model without sharing their raw data with each other. However, data
locality does not provide sufficient privacy protection, and it is desirable to
facilitate FL with rigorous differential privacy (DP) guarantee. Existing DP
mechanisms would introduce random noise with magnitude proportional to the
model size, which can be quite large in deep neural networks. In this paper, we
propose a new FL framework with sparsification-amplified privacy. Our approach
integrates random sparsification with gradient perturbation on each agent to
amplify privacy guarantee. Since sparsification would increase the number of
communication rounds required to achieve a certain target accuracy, which is
unfavorable for DP guarantee, we further introduce acceleration techniques to
help reduce the privacy cost. We rigorously analyze the convergence of our
approach and utilize Renyi DP to tightly account the end-to-end DP guarantee.
Extensive experiments on benchmark datasets validate that our approach
outperforms previous differentially-private FL approaches in both privacy
guarantee and communication efficiency.","['Rui Hu', 'Yanmin Gong', 'Yuanxiong Guo']","['cs.LG', 'cs.CR', 'stat.ML']",2020-08-01 20:22:57+00:00
http://arxiv.org/abs/2008.00331v1,Learning from Mixtures of Private and Public Populations,"We initiate the study of a new model of supervised learning under privacy
constraints. Imagine a medical study where a dataset is sampled from a
population of both healthy and unhealthy individuals. Suppose healthy
individuals have no privacy concerns (in such case, we call their data
""public"") while the unhealthy individuals desire stringent privacy protection
for their data. In this example, the population (data distribution) is a
mixture of private (unhealthy) and public (healthy) sub-populations that could
be very different.
  Inspired by the above example, we consider a model in which the population
$\mathcal{D}$ is a mixture of two sub-populations: a private sub-population
$\mathcal{D}_{\sf priv}$ of private and sensitive data, and a public
sub-population $\mathcal{D}_{\sf pub}$ of data with no privacy concerns. Each
example drawn from $\mathcal{D}$ is assumed to contain a privacy-status bit
that indicates whether the example is private or public. The goal is to design
a learning algorithm that satisfies differential privacy only with respect to
the private examples.
  Prior works in this context assumed a homogeneous population where private
and public data arise from the same distribution, and in particular designed
solutions which exploit this assumption. We demonstrate how to circumvent this
assumption by considering, as a case study, the problem of learning linear
classifiers in $\mathbb{R}^d$. We show that in the case where the privacy
status is correlated with the target label (as in the above example), linear
classifiers in $\mathbb{R}^d$ can be learned, in the agnostic as well as the
realizable setting, with sample complexity which is comparable to that of the
classical (non-private) PAC-learning. It is known that this task is impossible
if all the data is considered private.","['Raef Bassily', 'Shay Moran', 'Anupama Nandi']","['cs.LG', 'stat.ML']",2020-08-01 20:11:50+00:00
http://arxiv.org/abs/2008.00325v3,Bringing UMAP Closer to the Speed of Light with GPU Acceleration,"The Uniform Manifold Approximation and Projection (UMAP) algorithm has become
widely popular for its ease of use, quality of results, and support for
exploratory, unsupervised, supervised, and semi-supervised learning. While many
algorithms can be ported to a GPU in a simple and direct fashion, such efforts
have resulted in inefficient and inaccurate versions of UMAP. We show a number
of techniques that can be used to make a faster and more faithful GPU version
of UMAP, and obtain speedups of up to 100x in practice. Many of these design
choices/lessons are general purpose and may inform the conversion of other
graph and manifold learning algorithms to use GPUs. Our implementation has been
made publicly available as part of the open source RAPIDS cuML library
(https://github.com/rapidsai/cuml).","['Corey J. Nolet', 'Victor Lafargue', 'Edward Raff', 'Thejaswi Nanditale', 'Tim Oates', 'John Zedlewski', 'Joshua Patterson']","['cs.LG', 'cs.DS', 'stat.ML']",2020-08-01 19:35:56+00:00
http://arxiv.org/abs/2008.00323v1,Convergence of Sparse Variational Inference in Gaussian Processes Regression,"Gaussian processes are distributions over functions that are versatile and
mathematically convenient priors in Bayesian modelling. However, their use is
often impeded for data with large numbers of observations, $N$, due to the
cubic (in $N$) cost of matrix operations used in exact inference. Many
solutions have been proposed that rely on $M \ll N$ inducing variables to form
an approximation at a cost of $\mathcal{O}(NM^2)$. While the computational cost
appears linear in $N$, the true complexity depends on how $M$ must scale with
$N$ to ensure a certain quality of the approximation. In this work, we
investigate upper and lower bounds on how $M$ needs to grow with $N$ to ensure
high quality approximations. We show that we can make the KL-divergence between
the approximate model and the exact posterior arbitrarily small for a
Gaussian-noise regression model with $M\ll N$. Specifically, for the popular
squared exponential kernel and $D$-dimensional Gaussian distributed covariates,
$M=\mathcal{O}((\log N)^D)$ suffice and a method with an overall computational
cost of $\mathcal{O}(N(\log N)^{2D}(\log\log N)^2)$ can be used to perform
inference.","['David R. Burt', 'Carl Edward Rasmussen', 'Mark van der Wilk']","['stat.ML', 'cs.LG']",2020-08-01 19:23:34+00:00
http://arxiv.org/abs/2008.00311v3,Learning with Safety Constraints: Sample Complexity of Reinforcement Learning for Constrained MDPs,"Many physical systems have underlying safety considerations that require that
the policy employed ensures the satisfaction of a set of constraints. The
analytical formulation usually takes the form of a Constrained Markov Decision
Process (CMDP). We focus on the case where the CMDP is unknown, and RL
algorithms obtain samples to discover the model and compute an optimal
constrained policy. Our goal is to characterize the relationship between safety
constraints and the number of samples needed to ensure a desired level of
accuracy -- both objective maximization and constraint satisfaction -- in a PAC
sense. We explore two classes of RL algorithms, namely, (i) a generative model
based approach, wherein samples are taken initially to estimate a model, and
(ii) an online approach, wherein the model is updated as samples are obtained.
Our main finding is that compared to the best known bounds of the unconstrained
regime, the sample complexity of constrained RL algorithms are increased by a
factor that is logarithmic in the number of constraints, which suggests that
the approach may be easily utilized in real systems.","['Aria HasanzadeZonuzy', 'Archana Bura', 'Dileep Kalathil', 'Srinivas Shakkottai']","['cs.LG', 'stat.ML']",2020-08-01 18:17:08+00:00
http://arxiv.org/abs/2008.00235v1,Two-step penalised logistic regression for multi-omic data with an application to cardiometabolic syndrome,"Building classification models that predict a binary class label on the basis
of high dimensional multi-omics datasets poses several challenges, due to the
typically widely differing characteristics of the data layers in terms of
number of predictors, type of data, and levels of noise. Previous research has
shown that applying classical logistic regression with elastic-net penalty to
these datasets can lead to poor results (Liu et al., 2018). We implement a
two-step approach to multi-omic logistic regression in which variable selection
is performed on each layer separately and a predictive model is then built
using the variables selected in the first step. Here, our approach is compared
to other methods that have been developed for the same purpose, and we adapt
existing software for multi-omic linear regression (Zhao and Zucknick, 2020) to
the logistic regression setting. Extensive simulation studies show that our
approach should be preferred if the goal is to select as many relevant
predictors as possible, as well as achieving prediction performances comparable
to those of the best competitors. Our motivating example is a cardiometabolic
syndrome dataset comprising eight 'omic data types for 2 extreme phenotype
groups (10 obese and 10 lipodystrophy individuals) and 185 blood donors. Our
proposed approach allows us to identify features that characterise
cardiometabolic syndrome at the molecular level. R code is available at
https://github.com/acabassi/logistic-regression-for-multi-omic-data.","['Alessandra Cabassi', 'Denis Seyres', 'Mattia Frontini', 'Paul D. W. Kirk']","['stat.ME', 'stat.AP', 'stat.ML']",2020-08-01 10:36:27+00:00
http://arxiv.org/abs/2008.00234v1,Ergodic Annealing,"Simulated Annealing is the crowning glory of Markov Chain Monte Carlo Methods
for the solution of NP-hard optimization problems in which the cost function is
known. Here, by replacing the Metropolis engine of Simulated Annealing with a
reinforcement learning variation -- that we call Macau Algorithm -- we show
that the Simulated Annealing heuristic can be very effective also when the cost
function is unknown and has to be learned by an artificial agent.","['Carlo Baldassi', 'Fabio Maccheroni', 'Massimo Marinacci', 'Marco Pirazzini']","['cs.AI', 'econ.TH', 'math.PR', 'stat.ML']",2020-08-01 10:17:11+00:00
http://arxiv.org/abs/2008.00177v1,Multi-node Bert-pretraining: Cost-efficient Approach,"Recently, large scale Transformer-based language models such as BERT, GPT-2,
and XLNet have brought about exciting leaps in state-of-the-art results for
many Natural Language Processing (NLP) tasks. One of the common trends in these
recent models is a significant increase in model complexity, which introduces
both more weights and computation. Moreover, with the advent of large-scale
unsupervised datasets, training time is further extended due to the increased
amount of data samples within a single training epoch. As a result, to train
these models within a reasonable time, machine learning (ML) programmers often
require advanced hardware setups such as the premium GPU-enabled NVIDIA DGX
workstations or specialized accelerators such as Google's TPU Pods. Our work
addresses this limitation and demonstrates that the BERT pre-trained model can
be trained within 2 weeks on an academic-size cluster of widely available GPUs
through careful algorithmic and software optimizations. In this paper, we
present these optimizations on how to improve single device training
throughput, distribute the training workload over multiple nodes and GPUs, and
overcome the communication bottleneck introduced by the large data exchanges
over the network. We show that we are able to perform pre-training on BERT
within a reasonable time budget (12 days) in an academic setting, but with a
much less expensive and less aggressive hardware resource requirement than in
previously demonstrated industrial settings based on NVIDIA DGX machines or
Google's TPU Pods.","['Jiahuang Lin', 'Xin Li', 'Gennady Pekhimenko']","['cs.LG', 'cs.CL', 'stat.ML']",2020-08-01 05:49:20+00:00
http://arxiv.org/abs/2008.00163v3,The Importance of Being Correlated: Implications of Dependence in Joint Spectral Inference across Multiple Networks,"Spectral inference on multiple networks is a rapidly-developing subfield of
graph statistics. Recent work has demonstrated that joint, or simultaneous,
spectral embedding of multiple independent networks can deliver more accurate
estimation than individual spectral decompositions of those same networks. Such
inference procedures typically rely heavily on independence assumptions across
the multiple network realizations, and even in this case, little attention has
been paid to the induced network correlation in such joint embeddings. Here, we
present a generalized omnibus embedding methodology and provide a detailed
analysis of this embedding across both independent and correlated networks, the
latter of which significantly extends the reach of such procedures. We describe
how this omnibus embedding can itself induce correlation, leading us to
distinguish between inherent correlation -- the correlation that arises
naturally in multisample network data -- and induced correlation, which is an
artifice of the joint embedding methodology. We show that the generalized
omnibus embedding procedure is flexible and robust, and prove both consistency
and a central limit theorem for the embedded points. We examine how induced and
inherent correlation can impact inference for network time series data, and we
provide network analogues of classical questions such as the effective sample
size for more generally correlated data. Further, we show how an appropriately
calibrated generalized omnibus embedding can detect changes in real biological
networks that previous embedding procedures could not discern, confirming that
the effect of inherent and induced correlation can be subtle and
transformative, with import in theory and practice.","['Konstantinos Pantazis', 'Avanti Athreya', 'Jesús Arroyo', 'William N. Frost', 'Evan S. Hill', 'Vince Lyzinski']","['stat.ME', 'stat.ML', '62H12, 62E20, 05C80']",2020-08-01 03:43:52+00:00
http://arxiv.org/abs/2008.00138v1,Vulnerability Under Adversarial Machine Learning: Bias or Variance?,"Prior studies have unveiled the vulnerability of the deep neural networks in
the context of adversarial machine learning, leading to great recent attention
into this area. One interesting question that has yet to be fully explored is
the bias-variance relationship of adversarial machine learning, which can
potentially provide deeper insights into this behaviour. The notion of bias and
variance is one of the main approaches to analyze and evaluate the
generalization and reliability of a machine learning model. Although it has
been extensively used in other machine learning models, it is not well explored
in the field of deep learning and it is even less explored in the area of
adversarial machine learning.
  In this study, we investigate the effect of adversarial machine learning on
the bias and variance of a trained deep neural network and analyze how
adversarial perturbations can affect the generalization of a network. We derive
the bias-variance trade-off for both classification and regression applications
based on two main loss functions: (i) mean squared error (MSE), and (ii)
cross-entropy. Furthermore, we perform quantitative analysis with both
simulated and real data to empirically evaluate consistency with the derived
bias-variance tradeoffs. Our analysis sheds light on why the deep neural
networks have poor performance under adversarial perturbation from a
bias-variance point of view and how this type of perturbation would change the
performance of a network. Moreover, given these new theoretical findings, we
introduce a new adversarial machine learning algorithm with lower computational
complexity than well-known adversarial machine learning strategies (e.g., PGD)
while providing a high success rate in fooling deep neural networks in lower
perturbation magnitudes.","['Hossein Aboutalebi', 'Mohammad Javad Shafiee', 'Michelle Karg', 'Christian Scharfenberger', 'Alexander Wong']","['cs.LG', 'cs.NE', 'stat.ML']",2020-08-01 00:58:54+00:00
http://arxiv.org/abs/2008.00123v2,Noise-Response Analysis of Deep Neural Networks Quantifies Robustness and Fingerprints Structural Malware,"The ubiquity of deep neural networks (DNNs), cloud-based training, and
transfer learning is giving rise to a new cybersecurity frontier in which
unsecure DNNs have `structural malware' (i.e., compromised weights and
activation pathways). In particular, DNNs can be designed to have backdoors
that allow an adversary to easily and reliably fool an image classifier by
adding a pattern of pixels called a trigger. It is generally difficult to
detect backdoors, and existing detection methods are computationally expensive
and require extensive resources (e.g., access to the training data). Here, we
propose a rapid feature-generation technique that quantifies the robustness of
a DNN, `fingerprints' its nonlinearity, and allows us to detect backdoors (if
present). Our approach involves studying how a DNN responds to noise-infused
images with varying noise intensity, which we summarize with titration curves.
We find that DNNs with backdoors are more sensitive to input noise and respond
in a characteristic way that reveals the backdoor and where it leads (its
`target'). Our empirical results demonstrate that we can accurately detect
backdoors with high confidence orders-of-magnitude faster than existing
approaches (seconds versus hours).","['N. Benjamin Erichson', 'Dane Taylor', 'Qixuan Wu', 'Michael W. Mahoney']","['cs.LG', 'stat.ML']",2020-07-31 23:52:58+00:00
http://arxiv.org/abs/2008.00104v2,Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach,"Most recommender systems (RS) research assumes that a user's utility can be
maximized independently of the utility of the other agents (e.g., other users,
content providers). In realistic settings, this is often not true---the
dynamics of an RS ecosystem couple the long-term utility of all agents. In this
work, we explore settings in which content providers cannot remain viable
unless they receive a certain level of user engagement. We formulate the
recommendation problem in this setting as one of equilibrium selection in the
induced dynamical system, and show that it can be solved as an optimal
constrained matching problem. Our model ensures the system reaches an
equilibrium with maximal social welfare supported by a sufficiently diverse set
of viable providers. We demonstrate that even in a simple, stylized dynamical
RS model, the standard myopic approach to recommendation---always matching a
user to the best provider---performs poorly. We develop several scalable
techniques to solve the matching problem, and also draw connections to various
notions of user regret and fairness, arguing that these outcomes are fairer in
a utilitarian sense.","['Martin Mladenov', 'Elliot Creager', 'Omer Ben-Porat', 'Kevin Swersky', 'Richard Zemel', 'Craig Boutilier']","['cs.LG', 'cs.AI', 'cs.IR', 'stat.ML']",2020-07-31 22:40:47+00:00
http://arxiv.org/abs/2008.00103v3,F*: An Interpretable Transformation of the F-measure,"The F-measure, also known as the F1-score, is widely used to assess the
performance of classification algorithms. However, some researchers find it
lacking in intuitive interpretation, questioning the appropriateness of
combining two aspects of performance as conceptually distinct as precision and
recall, and also questioning whether the harmonic mean is the best way to
combine them. To ease this concern, we describe a simple transformation of the
F-measure, which we call F* (F-star), which has an immediate practical
interpretation.","['David J. Hand', 'Peter Christen', 'Nishadi Kirielle']","['cs.LG', 'cs.AI', 'cs.CV', 'cs.IR', 'stat.ML']",2020-07-31 22:37:08+00:00
http://arxiv.org/abs/2008.00051v2,On the Convergence of SGD with Biased Gradients,"We analyze the complexity of biased stochastic gradient methods (SGD), where
individual updates are corrupted by deterministic, i.e. biased error terms. We
derive convergence results for smooth (non-convex) functions and give improved
rates under the Polyak-Lojasiewicz condition. We quantify how the magnitude of
the bias impacts the attainable accuracy and the convergence rates (sometimes
leading to divergence).
  Our framework covers many applications where either only biased gradient
updates are available, or preferred, over unbiased ones for performance
reasons. For instance, in the domain of distributed learning, biased gradient
compression techniques such as top-k compression have been proposed as a tool
to alleviate the communication bottleneck and in derivative-free optimization,
only biased gradient estimators can be queried. We discuss a few guiding
examples that show the broad applicability of our analysis.","['Ahmad Ajalloeian', 'Sebastian U. Stich']","['cs.LG', 'math.OC', 'stat.ML']",2020-07-31 19:37:59+00:00
http://arxiv.org/abs/2008.00047v2,Towards Class-Oriented Poisoning Attacks Against Neural Networks,"Poisoning attacks on machine learning systems compromise the model
performance by deliberately injecting malicious samples in the training dataset
to influence the training process. Prior works focus on either availability
attacks (i.e., lowering the overall model accuracy) or integrity attacks (i.e.,
enabling specific instance-based backdoor). In this paper, we advance the
adversarial objectives of the availability attacks to a per-class basis, which
we refer to as class-oriented poisoning attacks. We demonstrate that the
proposed attack is capable of forcing the corrupted model to predict in two
specific ways: (i) classify unseen new images to a targeted ""supplanter"" class,
and (ii) misclassify images from a ""victim"" class while maintaining the
classification accuracy on other non-victim classes. To maximize the
adversarial effect as well as reduce the computational complexity of poisoned
data generation, we propose a gradient-based framework that crafts poisoning
images with carefully manipulated feature information for each scenario. Using
newly defined metrics at the class level, we demonstrate the effectiveness of
the proposed class-oriented poisoning attacks on various models (e.g., LeNet-5,
Vgg-9, and ResNet-50) over a wide range of datasets (e.g., MNIST, CIFAR-10, and
ImageNet-ILSVRC2012) in an end-to-end training setting.","['Bingyin Zhao', 'Yingjie Lao']","['cs.LG', 'cs.CR', 'stat.ML']",2020-07-31 19:27:37+00:00
http://arxiv.org/abs/2008.01571v2,IntelligentPooling: Practical Thompson Sampling for mHealth,"In mobile health (mHealth) smart devices deliver behavioral treatments
repeatedly over time to a user with the goal of helping the user adopt and
maintain healthy behaviors. Reinforcement learning appears ideal for learning
how to optimally make these sequential treatment decisions. However,
significant challenges must be overcome before reinforcement learning can be
effectively deployed in a mobile healthcare setting. In this work we are
concerned with the following challenges: 1) individuals who are in the same
context can exhibit differential response to treatments 2) only a limited
amount of data is available for learning on any one individual, and 3)
non-stationary responses to treatment. To address these challenges we
generalize Thompson-Sampling bandit algorithms to develop IntelligentPooling.
IntelligentPooling learns personalized treatment policies thus addressing
challenge one. To address the second challenge, IntelligentPooling updates each
user's degree of personalization while making use of available data on other
users to speed up learning. Lastly, IntelligentPooling allows responsivity to
vary as a function of a user's time since beginning treatment, thus addressing
challenge three. We show that IntelligentPooling achieves an average of 26%
lower regret than state-of-the-art. We demonstrate the promise of this approach
and its ability to learn from even a small group of users in a live clinical
trial.","['Sabina Tomkins', 'Peng Liao', 'Predrag Klasnja', 'Susan Murphy']","['cs.LG', 'cs.CY', 'stat.ML']",2020-07-31 19:03:09+00:00
http://arxiv.org/abs/2008.00029v1,Cold Posteriors and Aleatoric Uncertainty,"Recent work has observed that one can outperform exact inference in Bayesian
neural networks by tuning the ""temperature"" of the posterior on a validation
set (the ""cold posterior"" effect). To help interpret this phenomenon, we argue
that commonly used priors in Bayesian neural networks can significantly
overestimate the aleatoric uncertainty in the labels on many classification
datasets. This problem is particularly pronounced in academic benchmarks like
MNIST or CIFAR, for which the quality of the labels is high. For the special
case of Gaussian process regression, any positive temperature corresponds to a
valid posterior under a modified prior, and tuning this temperature is directly
analogous to empirical Bayes. On classification tasks, there is no direct
equivalence between modifying the prior and tuning the temperature, however
reducing the temperature can lead to models which better reflect our belief
that one gains little information by relabeling existing examples in the
training set. Therefore although cold posteriors do not always correspond to an
exact inference procedure, we believe they may often better reflect our true
prior beliefs.","['Ben Adlam', 'Jasper Snoek', 'Samuel L. Smith']","['stat.ML', 'cs.LG']",2020-07-31 18:37:31+00:00
http://arxiv.org/abs/2008.00025v3,Rethinking Default Values: a Low Cost and Efficient Strategy to Define Hyperparameters,"Machine Learning (ML) algorithms have been increasingly applied to problems
from several different areas. Despite their growing popularity, their
predictive performance is usually affected by the values assigned to their
hyperparameters (HPs). As consequence, researchers and practitioners face the
challenge of how to set these values. Many users have limited knowledge about
ML algorithms and the effect of their HP values and, therefore, do not take
advantage of suitable settings. They usually define the HP values by trial and
error, which is very subjective, not guaranteed to find good values and
dependent on the user experience. Tuning techniques search for HP values able
to maximize the predictive performance of induced models for a given dataset,
but have the drawback of a high computational cost. Thus, practitioners use
default values suggested by the algorithm developer or by tools implementing
the algorithm. Although default values usually result in models with acceptable
predictive performance, different implementations of the same algorithm can
suggest distinct default values. To maintain a balance between tuning and using
default values, we propose a strategy to generate new optimized default values.
Our approach is grounded on a small set of optimized values able to obtain
predictive performance values better than default settings provided by popular
tools. After performing a large experiment and a careful analysis of the
results, we concluded that our approach delivers better default values.
Besides, it leads to competitive solutions when compared to tuned values,
making it easier to use and having a lower cost. We also extracted simple rules
to guide practitioners in deciding whether to use our new methodology or a HP
tuning approach.","['Rafael Gomes Mantovani', 'André Luis Debiaso Rossi', 'Edesio Alcobaça', 'Jadson Castro Gertrudes', 'Sylvio Barbon Junior', 'André Carlos Ponce de Leon Ferreira de Carvalho']","['cs.LG', 'stat.ML']",2020-07-31 18:23:35+00:00
http://arxiv.org/abs/2007.16204v1,Adversarial Attacks with Multiple Antennas Against Deep Learning-Based Modulation Classifiers,"We consider a wireless communication system, where a transmitter sends
signals to a receiver with different modulation types while the receiver
classifies the modulation types of the received signals using its deep
learning-based classifier. Concurrently, an adversary transmits adversarial
perturbations using its multiple antennas to fool the classifier into
misclassifying the received signals. From the adversarial machine learning
perspective, we show how to utilize multiple antennas at the adversary to
improve the adversarial (evasion) attack performance. Two main points are
considered while exploiting the multiple antennas at the adversary, namely the
power allocation among antennas and the utilization of channel diversity.
First, we show that multiple independent adversaries, each with a single
antenna cannot improve the attack performance compared to a single adversary
with multiple antennas using the same total power. Then, we consider various
ways to allocate power among multiple antennas at a single adversary such as
allocating power to only one antenna, and proportional or inversely
proportional to the channel gain. By utilizing channel diversity, we introduce
an attack to transmit the adversarial perturbation through the channel with the
largest channel gain at the symbol level. We show that this attack reduces the
classifier accuracy significantly compared to other attacks under different
channel conditions in terms of channel variance and channel correlation across
antennas. Also, we show that the attack success improves significantly as the
number of antennas increases at the adversary that can better utilize channel
diversity to craft adversarial attacks.","['Brian Kim', 'Yalin E. Sagduyu', 'Tugba Erpek', 'Kemal Davaslioglu', 'Sennur Ulukus']","['eess.SP', 'cs.LG', 'cs.NI', 'stat.ML']",2020-07-31 17:56:50+00:00
http://arxiv.org/abs/2007.16187v1,Ultra-light deep MIR by trimming lottery tickets,"Current state-of-the-art results in Music Information Retrieval are largely
dominated by deep learning approaches. These provide unprecedented accuracy
across all tasks. However, the consistently overlooked downside of these models
is their stunningly massive complexity, which seems concomitantly crucial to
their success. In this paper, we address this issue by proposing a model
pruning method based on the lottery ticket hypothesis. We modify the original
approach to allow for explicitly removing parameters, through structured
trimming of entire units, instead of simply masking individual weights. This
leads to models which are effectively lighter in terms of size, memory and
number of operations. We show that our proposal can remove up to 90% of the
model parameters without loss of accuracy, leading to ultra-light deep MIR
models. We confirm the surprising result that, at smaller compression ratios
(removing up to 85% of a network), lighter models consistently outperform their
heavier counterparts. We exhibit these results on a large array of MIR tasks
including audio classification, pitch recognition, chord extraction, drum
transcription and onset estimation. The resulting ultra-light deep learning
models for MIR can run on CPU, and can even fit on embedded devices with
minimal degradation of accuracy.","['Philippe Esling', 'Theis Bazin', 'Adrien Bitton', 'Tristan Carsault', 'Ninon Devis']","['cs.LG', 'cs.IR', 'cs.MM', 'cs.SD', 'eess.AS', 'stat.ML']",2020-07-31 17:30:28+00:00
http://arxiv.org/abs/2007.16173v1,Embedding Ranking-Oriented Recommender System Graphs,"Graph-based recommender systems (GRSs) analyze the structural information in
the graphical representation of data to make better recommendations, especially
when the direct user-item relation data is sparse. Ranking-oriented GRSs that
form a major class of recommendation systems, mostly use the graphical
representation of preference (or rank) data for measuring node similarities,
from which they can infer a recommendation list using a neighborhood-based
mechanism. In this paper, we propose PGRec, a novel graph-based
ranking-oriented recommendation framework. PGRec models the preferences of the
users over items, by a novel graph structure called PrefGraph. This graph is
then exploited by an improved embedding approach, taking advantage of both
factorization and deep learning methods, to extract vectors representing users,
items, and preferences. The resulting embedding are then used for predicting
users' unknown pairwise preferences from which the final recommendation lists
are inferred. We have evaluated the performance of the proposed method against
the state of the art model-based and neighborhood-based recommendation methods,
and our experiments show that PGRec outperforms the baseline algorithms up to
3.2% in terms of NDCG@10 in different MovieLens datasets.","['Taher Hekmatfar', 'Saman Haratizadeh', 'Sama Goliaei']","['cs.IR', 'cs.LG', 'stat.ML']",2020-07-31 16:56:54+00:00
http://arxiv.org/abs/2007.16170v1,Diet deep generative audio models with structured lottery,"Deep learning models have provided extremely successful solutions in most
audio application fields. However, the high accuracy of these models comes at
the expense of a tremendous computation cost. This aspect is almost always
overlooked in evaluating the quality of proposed models. However, models should
not be evaluated without taking into account their complexity. This aspect is
especially critical in audio applications, which heavily relies on specialized
embedded hardware with real-time constraints. In this paper, we build on recent
observations that deep models are highly overparameterized, by studying the
lottery ticket hypothesis on deep generative audio models. This hypothesis
states that extremely efficient small sub-networks exist in deep models and
would provide higher accuracy than larger models if trained in isolation.
However, lottery tickets are found by relying on unstructured masking, which
means that resulting models do not provide any gain in either disk size or
inference time. Instead, we develop here a method aimed at performing
structured trimming. We show that this requires to rely on global selection and
introduce a specific criterion based on mutual information. First, we confirm
the surprising result that smaller models provide higher accuracy than their
large counterparts. We further show that we can remove up to 95% of the model
weights without significant degradation in accuracy. Hence, we can obtain very
light models for generative audio across popular methods such as Wavenet, SING
or DDSP, that are up to 100 times smaller with commensurate accuracy. We study
the theoretical bounds for embedding these models on Raspberry Pi and Arduino,
and show that we can obtain generative models on CPU with equivalent quality as
large GPU models. Finally, we discuss the possibility of implementing deep
generative audio models on embedded platforms.","['Philippe Esling', 'Ninon Devis', 'Adrien Bitton', 'Antoine Caillon', 'Axel Chemla--Romeu-Santos', 'Constance Douwes']","['cs.LG', 'cs.MM', 'cs.SD', 'eess.AS', 'stat.ML']",2020-07-31 16:43:10+00:00
http://arxiv.org/abs/2007.16149v1,HMCNAS: Neural Architecture Search using Hidden Markov Chains and Bayesian Optimization,"Neural Architecture Search has achieved state-of-the-art performance in a
variety of tasks, out-performing human-designed networks. However, many
assumptions, that require human definition, related with the problems being
solved or the models generated are still needed: final model architectures,
number of layers to be sampled, forced operations, small search spaces, which
ultimately contributes to having models with higher performances at the cost of
inducing bias into the system. In this paper, we propose HMCNAS, which is
composed of two novel components: i) a method that leverages information about
human-designed models to autonomously generate a complex search space, and ii)
an Evolutionary Algorithm with Bayesian Optimization that is capable of
generating competitive CNNs from scratch, without relying on human-defined
parameters or small search spaces. The experimental results show that the
proposed approach results in competitive architectures obtained in a very short
time. HMCNAS provides a step towards generalizing NAS, by providing a way to
create competitive models, without requiring any human knowledge about the
specific task.","['Vasco Lopes', 'Luís A. Alexandre']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2020-07-31 16:04:08+00:00
http://arxiv.org/abs/2007.16112v2,Neural Architecture Search as Sparse Supernet,"This paper aims at enlarging the problem of Neural Architecture Search (NAS)
from Single-Path and Multi-Path Search to automated Mixed-Path Search. In
particular, we model the NAS problem as a sparse supernet using a new
continuous architecture representation with a mixture of sparsity constraints.
The sparse supernet enables us to automatically achieve sparsely-mixed paths
upon a compact set of nodes. To optimize the proposed sparse supernet, we
exploit a hierarchical accelerated proximal gradient algorithm within a
bi-level optimization framework. Extensive experiments on Convolutional Neural
Network and Recurrent Neural Network search demonstrate that the proposed
method is capable of searching for compact, general and powerful neural
architectures.","['Yan Wu', 'Aoming Liu', 'Zhiwu Huang', 'Siwei Zhang', 'Luc Van Gool']","['cs.CV', 'stat.ML']",2020-07-31 14:51:52+00:00
http://arxiv.org/abs/2007.16109v1,Sequential Drift Detection in Deep Learning Classifiers,"We utilize neural network embeddings to detect data drift by formulating the
drift detection within an appropriate sequential decision framework. This
enables control of the false alarm rate although the statistical tests are
repeatedly applied. Since change detection algorithms naturally face a tradeoff
between avoiding false alarms and quick correct detection, we introduce a loss
function which evaluates an algorithm's ability to balance these two concerns,
and we use it in a series of experiments.","['Samuel Ackerman', 'Parijat Dube', 'Eitan Farchi']","['stat.AP', 'cs.LG', 'stat.ML']",2020-07-31 14:46:21+00:00
http://arxiv.org/abs/2007.16104v1,Uncovering the structure of clinical EEG signals with self-supervised learning,"Objective. Supervised learning paradigms are often limited by the amount of
labeled data that is available. This phenomenon is particularly problematic in
clinically-relevant data, such as electroencephalography (EEG), where labeling
can be costly in terms of specialized expertise and human processing time.
Consequently, deep learning architectures designed to learn on EEG data have
yielded relatively shallow models and performances at best similar to those of
traditional feature-based approaches. However, in most situations, unlabeled
data is available in abundance. By extracting information from this unlabeled
data, it might be possible to reach competitive performance with deep neural
networks despite limited access to labels. Approach. We investigated
self-supervised learning (SSL), a promising technique for discovering structure
in unlabeled data, to learn representations of EEG signals. Specifically, we
explored two tasks based on temporal context prediction as well as contrastive
predictive coding on two clinically-relevant problems: EEG-based sleep staging
and pathology detection. We conducted experiments on two large public datasets
with thousands of recordings and performed baseline comparisons with purely
supervised and hand-engineered approaches. Main results. Linear classifiers
trained on SSL-learned features consistently outperformed purely supervised
deep neural networks in low-labeled data regimes while reaching competitive
performance when all labels were available. Additionally, the embeddings
learned with each method revealed clear latent structures related to
physiological and clinical phenomena, such as age effects. Significance. We
demonstrate the benefit of self-supervised learning approaches on EEG data. Our
results suggest that SSL may pave the way to a wider use of deep learning
models on EEG data.","['Hubert Banville', 'Omar Chehab', 'Aapo Hyvärinen', 'Denis-Alexander Engemann', 'Alexandre Gramfort']","['stat.ML', 'cs.LG', 'eess.SP', 'q-bio.NC', 'q-bio.QM']",2020-07-31 14:34:47+00:00
http://arxiv.org/abs/2007.16103v1,Learning-based Computer-aided Prescription Model for Parkinson's Disease: A Data-driven Perspective,"In this paper, we study a novel problem: ""automatic prescription
recommendation for PD patients."" To realize this goal, we first build a dataset
by collecting 1) symptoms of PD patients, and 2) their prescription drug
provided by neurologists. Then, we build a novel computer-aided prescription
model by learning the relation between observed symptoms and prescription drug.
Finally, for the new coming patients, we could recommend (predict) suitable
prescription drug on their observed symptoms by our prescription model. From
the methodology part, our proposed model, namely Prescription viA Learning
lAtent Symptoms (PALAS), could recommend prescription using the multi-modality
representation of the data. In PALAS, a latent symptom space is learned to
better model the relationship between symptoms and prescription drug, as there
is a large semantic gap between them. Moreover, we present an efficient
alternating optimization method for PALAS. We evaluated our method using the
data collected from 136 PD patients at Nanjing Brain Hospital, which can be
regarded as a large dataset in PD research community. The experimental results
demonstrate the effectiveness and clinical potential of our method in this
recommendation task, if compared with other competing methods.","['Yinghuan Shi', 'Wanqi Yang', 'Kim-Han Thung', 'Hao Wang', 'Yang Gao', 'Yang Pan', 'Li Zhang', 'Dinggang Shen']","['cs.LG', 'cs.CV', 'stat.ML']",2020-07-31 14:34:35+00:00
http://arxiv.org/abs/2007.16061v1,Graph signal processing for machine learning: A review and new perspectives,"The effective representation, processing, analysis, and visualization of
large-scale structured data, especially those related to complex domains such
as networks and graphs, are one of the key questions in modern machine
learning. Graph signal processing (GSP), a vibrant branch of signal processing
models and algorithms that aims at handling data supported on graphs, opens new
paths of research to address this challenge. In this article, we review a few
important contributions made by GSP concepts and tools, such as graph filters
and transforms, to the development of novel machine learning algorithms. In
particular, our discussion focuses on the following three aspects: exploiting
data structure and relational priors, improving data and computational
efficiency, and enhancing model interpretability. Furthermore, we provide new
perspectives on future development of GSP techniques that may serve as a bridge
between applied mathematics and signal processing on one side, and machine
learning and network science on the other. Cross-fertilization across these
different disciplines may help unlock the numerous challenges of complex data
analysis in the modern age.","['Xiaowen Dong', 'Dorina Thanou', 'Laura Toni', 'Michael Bronstein', 'Pascal Frossard']","['cs.LG', 'cs.SI', 'eess.SP', 'stat.ML']",2020-07-31 13:21:33+00:00
http://arxiv.org/abs/2007.16056v2,node2coords: Graph Representation Learning with Wasserstein Barycenters,"In order to perform network analysis tasks, representations that capture the
most relevant information in the graph structure are needed. However, existing
methods do not learn representations that can be interpreted in a
straightforward way and that are robust to perturbations to the graph
structure. In this work, we address these two limitations by proposing
node2coords, a representation learning algorithm for graphs, which learns
simultaneously a low-dimensional space and coordinates for the nodes in that
space. The patterns that span the low dimensional space reveal the graph's most
important structural information. The coordinates of the nodes reveal the
proximity of their local structure to the graph structural patterns. In order
to measure this proximity by taking into account the underlying graph, we
propose to use Wasserstein distances. We introduce an autoencoder that employs
a linear layer in the encoder and a novel Wasserstein barycentric layer at the
decoder. Node connectivity descriptors, that capture the local structure of the
nodes, are passed through the encoder to learn the small set of graph
structural patterns. In the decoder, the node connectivity descriptors are
reconstructed as Wasserstein barycenters of the graph structural patterns. The
optimal weights for the barycenter representation of a node's connectivity
descriptor correspond to the coordinates of that node in the low-dimensional
space. Experimental results demonstrate that the representations learned with
node2coords are interpretable, lead to node embeddings that are stable to
perturbations of the graph structure and achieve competitive or superior
results compared to state-of-the-art methods in node classification.","['Effrosyni Simou', 'Dorina Thanou', 'Pascal Frossard']","['cs.LG', 'stat.ML']",2020-07-31 13:14:25+00:00
http://arxiv.org/abs/2007.16054v2,Learning to Learn to Compress,"In this paper we present an end-to-end meta-learned system for image
compression. Traditional machine learning based approaches to image compression
train one or more neural network for generalization performance. However, at
inference time, the encoder or the latent tensor output by the encoder can be
optimized for each test image. This optimization can be regarded as a form of
adaptation or benevolent overfitting to the input content. In order to reduce
the gap between training and inference conditions, we propose a new training
paradigm for learned image compression, which is based on meta-learning. In a
first phase, the neural networks are trained normally. In a second phase, the
Model-Agnostic Meta-learning approach is adapted to the specific case of image
compression, where the inner-loop performs latent tensor overfitting, and the
outer loop updates both encoder and decoder neural networks based on the
overfitting performance. Furthermore, after meta-learning, we propose to
overfit and cluster the bias terms of the decoder on training image patches, so
that at inference time the optimal content-specific bias terms can be selected
at encoder-side. Finally, we propose a new probability model for lossless
compression, which combines concepts from both multi-scale and super-resolution
probability model approaches. We show the benefits of all our proposed ideas
via carefully designed experiments.","['Nannan Zou', 'Honglei Zhang', 'Francesco Cricri', 'Hamed R. Tavakoli', 'Jani Lainema', 'Miska Hannuksela', 'Emre Aksu', 'Esa Rahtu']","['eess.IV', 'cs.CV', 'cs.LG', 'cs.MM', 'stat.ML']",2020-07-31 13:13:53+00:00
http://arxiv.org/abs/2008.01175v1,Identifying meaningful clusters in malware data,"Finding meaningful clusters in drive-by-download malware data is a
particularly difficult task. Malware data tends to contain overlapping clusters
with wide variations of cardinality. This happens because there can be
considerable similarity between malware samples (some are even said to belong
to the same family), and these tend to appear in bursts. Clustering algorithms
are usually applied to normalised data sets. However, the process of
normalisation aims at setting features with different range values to have a
similar contribution to the clustering. It does not favour more meaningful
features over those that are less meaningful, an effect one should perhaps
expect of the data pre-processing stage.
  In this paper we introduce a method to deal precisely with the problem above.
This is an iterative data pre-processing method capable of aiding to increase
the separation between clusters. It does so by calculating the within-cluster
degree of relevance of each feature, and then it uses these as a data rescaling
factor. By repeating this until convergence our malware data was separated in
clear clusters, leading to a higher average silhouette width.","['Renato Cordeiro de Amorim', 'Carlos David Lopez Ruiz']","['cs.CR', 'cs.LG', 'stat.ML']",2020-07-31 12:36:08+00:00
http://arxiv.org/abs/2007.15973v1,Predicting heave and surge motions of a semi-submersible with neural networks,"Real-time motion prediction of a vessel or a floating platform can help to
improve the performance of motion compensation systems. It can also provide
useful early-warning information for offshore operations that are critical with
regard to motion. In this study, a long short-term memory (LSTM) -based machine
learning model was developed to predict heave and surge motions of a
semi-submersible. The training and test data came from a model test carried out
in the deep-water ocean basin, at Shanghai Jiao Tong University, China. The
motion and measured waves were fed into LSTM cells and then went through serval
fully connected (FC) layers to obtain the prediction. With the help of measured
waves, the prediction extended 46.5 s into future with an average accuracy
close to 90%. Using a noise-extended dataset, the trained model effectively
worked with a noise level up to 0.8. As a further step, the model could predict
motions only based on the motion itself. Based on sensitive studies on the
architectures of the model, guidelines for the construction of the machine
learning model are proposed. The proposed LSTM model shows a strong ability to
predict vessel wave-excited motions.","['Xiaoxian Guo', 'Xiantao Zhang', 'Xinliang Tian', 'Xin Li', 'Wenyue Lu']","['stat.ML', 'cs.LG']",2020-07-31 11:24:46+00:00
http://arxiv.org/abs/2007.15951v4,An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks,"In recent times, deep artificial neural networks have achieved many successes
in pattern recognition. Part of this success can be attributed to the reliance
on big data to increase generalization. However, in the field of time series
recognition, many datasets are often very small. One method of addressing this
problem is through the use of data augmentation. In this paper, we survey data
augmentation techniques for time series and their application to time series
classification with neural networks. We propose a taxonomy and outline the four
families in time series data augmentation, including transformation-based
methods, pattern mixing, generative models, and decomposition methods.
Furthermore, we empirically evaluate 12 time series data augmentation methods
on 128 time series classification datasets with six different types of neural
networks. Through the results, we are able to analyze the characteristics,
advantages and disadvantages, and recommendations of each data augmentation
method. This survey aims to help in the selection of time series data
augmentation for neural network applications.","['Brian Kenji Iwana', 'Seiichi Uchida']","['cs.LG', 'stat.ML']",2020-07-31 10:33:54+00:00
http://arxiv.org/abs/2008.01171v1,Deep Reinforcement Learning using Cyclical Learning Rates,"Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning
of hyperparameters to successfully resolve problems. One of the most
influential parameters in optimization procedures based on stochastic gradient
descent (SGD) is the learning rate. We investigate cyclical learning and
propose a method for defining a general cyclical learning rate for various DRL
problems. In this paper we present a method for cyclical learning applied to
complex DRL problems. Our experiments show that, utilizing cyclical learning
achieves similar or even better results than highly tuned fixed learning rates.
This paper presents the first application of cyclical learning rates in DRL
settings and is a step towards overcoming manual hyperparameter tuning.","['Ralf Gulde', 'Marc Tuscher', 'Akos Csiszar', 'Oliver Riedel', 'Alexander Verl']","['cs.LG', 'stat.ML']",2020-07-31 10:06:02+00:00
http://arxiv.org/abs/2007.15930v1,Variational approximations of empirical Bayes posteriors in high-dimensional linear models,"In high-dimensions, the prior tails can have a significant effect on both
posterior computation and asymptotic concentration rates. To achieve optimal
rates while keeping the posterior computations relatively simple, an empirical
Bayes approach has recently been proposed, featuring thin-tailed conjugate
priors with data-driven centers. While conjugate priors ease some of the
computational burden, Markov chain Monte Carlo methods are still needed, which
can be expensive when dimension is high. In this paper, we develop a
variational approximation to the empirical Bayes posterior that is fast to
compute and retains the optimal concentration rate properties of the original.
In simulations, our method is shown to have superior performance compared to
existing variational approximations in the literature across a wide range of
high-dimensional settings.","['Yue Yang', 'Ryan Martin']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2020-07-31 09:50:01+00:00
http://arxiv.org/abs/2007.15911v2,"The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies","Artificial intelligence (AI) has huge potential to improve the health and
well-being of people, but adoption in clinical practice is still limited. Lack
of transparency is identified as one of the main barriers to implementation, as
clinicians should be confident the AI system can be trusted. Explainable AI has
the potential to overcome this issue and can be a step towards trustworthy AI.
In this paper we review the recent literature to provide guidance to
researchers and practitioners on the design of explainable AI systems for the
health-care domain and contribute to formalization of the field of explainable
AI. We argue the reason to demand explainability determines what should be
explained as this determines the relative importance of the properties of
explainability (i.e. interpretability and fidelity). Based on this, we propose
a framework to guide the choice between classes of explainable AI methods
(explainable modelling versus post-hoc explanation; model-based,
attribution-based, or example-based explanations; global and local
explanations). Furthermore, we find that quantitative evaluation metrics, which
are important for objective standardized evaluation, are still lacking for some
properties (e.g. clarity) and types of explanations (e.g. example-based
methods). We conclude that explainable modelling can contribute to trustworthy
AI, but the benefits of explainability still need to be proven in practice and
complementary measures might be needed to create trustworthy AI in health care
(e.g. reporting data quality, performing extensive (external) validation, and
regulation).","['Aniek F. Markus', 'Jan A. Kors', 'Peter R. Rijnbeek']","['cs.AI', 'cs.LG', 'stat.ML']",2020-07-31 09:08:27+00:00
http://arxiv.org/abs/2007.15890v1,Towards Deep Robot Learning with Optimizer applicable to Non-stationary Problems,"This paper proposes a new optimizer for deep learning, named d-AmsGrad. In
the real-world data, noise and outliers cannot be excluded from dataset to be
used for learning robot skills. This problem is especially striking for robots
that learn by collecting data in real time, which cannot be sorted manually.
Several noise-robust optimizers have therefore been developed to resolve this
problem, and one of them, named AmsGrad, which is a variant of Adam optimizer,
has a proof of its convergence. However, in practice, it does not improve
learning performance in robotics scenarios. This reason is hypothesized that
most of robot learning problems are non-stationary, but AmsGrad assumes the
maximum second momentum during learning to be stationarily given. In order to
adapt to the non-stationary problems, an improved version, which slowly decays
the maximum second momentum, is proposed. The proposed optimizer has the same
capability of reaching the global optimum as baselines, and its performance
outperformed that of the baselines in robotics problems.",['Taisuke Kobayashi'],"['cs.LG', 'cs.RO', 'stat.ML']",2020-07-31 07:55:09+00:00
http://arxiv.org/abs/2007.15884v2,The Kolmogorov-Arnold representation theorem revisited,"There is a longstanding debate whether the Kolmogorov-Arnold representation
theorem can explain the use of more than one hidden layer in neural networks.
The Kolmogorov-Arnold representation decomposes a multivariate function into an
interior and an outer function and therefore has indeed a similar structure as
a neural network with two hidden layers. But there are distinctive differences.
One of the main obstacles is that the outer function depends on the represented
function and can be wildly varying even if the represented function is smooth.
We derive modifications of the Kolmogorov-Arnold representation that transfer
smoothness properties of the represented function to the outer function and can
be well approximated by ReLU networks. It appears that instead of two hidden
layers, a more natural interpretation of the Kolmogorov-Arnold representation
is that of a deep neural network where most of the layers are required to
approximate the interior function.",['Johannes Schmidt-Hieber'],"['cs.LG', 'cs.NE', 'stat.ML', '41A30']",2020-07-31 07:41:09+00:00
http://arxiv.org/abs/2007.15859v1,Learning Forward Reuse Distance,"Caching techniques are widely used in the era of cloud computing from
applications, such as Web caches to infrastructures, Memcached and memory
caches in computer architectures. Prediction of cached data can greatly help
improve cache management and performance. The recent advancement of deep
learning techniques enables the design of novel intelligent cache replacement
policies. In this work, we propose a learning-aided approach to predict future
data accesses. We find that a powerful LSTM-based recurrent neural network
model can provide high prediction accuracy based on only a cache trace as
input. The high accuracy results from a carefully crafted locality-driven
feature design. Inspired by the high prediction accuracy, we propose a pseudo
OPT policy and evaluate it upon 13 real-world storage workloads from Microsoft
Research. Results demonstrate that the new cache policy improves state-of-art
practical policies by up to 19.2% and incurs only 2.3% higher miss ratio than
OPT on average.","['Pengcheng Li', 'Yongbin Gu']","['cs.LG', 'cs.DC', 'stat.ML']",2020-07-31 05:57:50+00:00
http://arxiv.org/abs/2007.15857v2,Real-Time Uncertainty Estimation in Computer Vision via Uncertainty-Aware Distribution Distillation,"Calibrated estimates of uncertainty are critical for many real-world computer
vision applications of deep learning. While there are several widely-used
uncertainty estimation methods, dropout inference stands out for its simplicity
and efficacy. This technique, however, requires multiple forward passes through
the network during inference and therefore can be too resource-intensive to be
deployed in real-time applications. We propose a simple, easy-to-optimize
distillation method for learning the conditional predictive distribution of a
pre-trained dropout model for fast, sample-free uncertainty estimation in
computer vision tasks. We empirically test the effectiveness of the proposed
method on both semantic segmentation and depth estimation tasks and demonstrate
our method can significantly reduce the inference time, enabling real-time
uncertainty quantification, while achieving improved quality of both the
uncertainty estimates and predictive performance over the regular dropout
model.","['Yichen Shen', 'Zhilu Zhang', 'Mert R. Sabuncu', 'Lin Sun']","['cs.CV', 'cs.LG', 'stat.ML']",2020-07-31 05:40:39+00:00
http://arxiv.org/abs/2007.15847v2,A Functional Model for Structure Learning and Parameter Estimation in Continuous Time Bayesian Network: An Application in Identifying Patterns of Multiple Chronic Conditions,"Bayesian networks are powerful statistical models to study the probabilistic
relationships among set random variables with major applications in disease
modeling and prediction. Here, we propose a continuous time Bayesian network
with conditional dependencies, represented as Poisson regression, to model the
impact of exogenous variables on the conditional dependencies of the network.
We also propose an adaptive regularization method with an intuitive early
stopping feature based on density based clustering for efficient learning of
the structure and parameters of the proposed network. Using a dataset of
patients with multiple chronic conditions extracted from electronic health
records of the Department of Veterans Affairs we compare the performance of the
proposed approach with some of the existing methods in the literature for both
short-term (one-year ahead) and long-term (multi-year ahead) predictions. The
proposed approach provides a sparse intuitive representation of the complex
functional relationships between multiple chronic conditions. It also provides
the capability of analyzing multiple disease trajectories over time given any
combination of prior conditions.","['Syed Hasib Akhter Faruqui', 'Adel Alaeddini', 'Jing Wang', 'Carlos A. Jaramillo']","['cs.LG', 'cs.AI', 'stat.ML']",2020-07-31 05:02:34+00:00
http://arxiv.org/abs/2007.15840v3,A Survey on Concept Factorization: From Shallow to Deep Representation Learning,"The quality of learned features by representation learning determines the
performance of learning algorithms and the related application tasks (such as
high-dimensional data clustering). As a relatively new paradigm for
representation learning, Concept Factorization (CF) has attracted a great deal
of interests in the areas of machine learning and data mining for over a
decade. Lots of effective CF based methods have been proposed based on
different perspectives and properties, but note that it still remains not easy
to grasp the essential connections and figure out the underlying explanatory
factors from exiting studies. In this paper, we therefore survey the recent
advances on CF methodologies and the potential benchmarks by categorizing and
summarizing the current methods. Specifically, we first re-view the root CF
method, and then explore the advancement of CF-based representation learning
ranging from shallow to deep/multilayer cases. We also introduce the potential
application areas of CF-based methods. Finally, we point out some future
directions for studying the CF-based representation learning. Overall, this
survey provides an insightful overview of both theoretical basis and current
developments in the field of CF, which can also help the interested researchers
to understand the current trends of CF and find the most appropriate CF
techniques to deal with particular applications.","['Zhao Zhang', 'Yan Zhang', 'Mingliang Xu', 'Li Zhang', 'Yi Yang', 'Shuicheng Yan']","['cs.LG', 'cs.CV', 'stat.ML']",2020-07-31 04:19:14+00:00
http://arxiv.org/abs/2007.15839v2,"Robust and Heavy-Tailed Mean Estimation Made Simple, via Regret Minimization","We study the problem of estimating the mean of a distribution in high
dimensions when either the samples are adversarially corrupted or the
distribution is heavy-tailed. Recent developments in robust statistics have
established efficient and (near) optimal procedures for both settings. However,
the algorithms developed on each side tend to be sophisticated and do not
directly transfer to the other, with many of them having ad-hoc or complicated
analyses.
  In this paper, we provide a meta-problem and a duality theorem that lead to a
new unified view on robust and heavy-tailed mean estimation in high dimensions.
We show that the meta-problem can be solved either by a variant of the Filter
algorithm from the recent literature on robust estimation or by the quantum
entropy scoring scheme (QUE), due to Dong, Hopkins and Li (NeurIPS '19). By
leveraging our duality theorem, these results translate into simple and
efficient algorithms for both robust and heavy-tailed settings. Furthermore,
the QUE-based procedure has run-time that matches the fastest known algorithms
on both fronts.
  Our analysis of Filter is through the classic regret bound of the
multiplicative weights update method. This connection allows us to avoid the
technical complications in previous works and improve upon the run-time
analysis of a gradient-descent-based algorithm for robust mean estimation by
Cheng, Diakonikolas, Ge and Soltanolkotabi (ICML '20).","['Samuel B. Hopkins', 'Jerry Li', 'Fred Zhang']","['cs.DS', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2020-07-31 04:18:32+00:00
http://arxiv.org/abs/2007.15836v2,TEAM: We Need More Powerful Adversarial Examples for DNNs,"Although deep neural networks (DNNs) have achieved success in many
application fields, it is still vulnerable to imperceptible adversarial
examples that can lead to misclassification of DNNs easily. To overcome this
challenge, many defensive methods are proposed. Indeed, a powerful adversarial
example is a key benchmark to measure these defensive mechanisms. In this
paper, we propose a novel method (TEAM, Taylor Expansion-Based Adversarial
Methods) to generate more powerful adversarial examples than previous methods.
The main idea is to craft adversarial examples by minimizing the confidence of
the ground-truth class under untargeted attacks or maximizing the confidence of
the target class under targeted attacks. Specifically, we define the new
objective functions that approximate DNNs by using the second-order Taylor
expansion within a tiny neighborhood of the input. Then the Lagrangian
multiplier method is used to obtain the optimize perturbations for these
objective functions. To decrease the amount of computation, we further
introduce the Gauss-Newton (GN) method to speed it up. Finally, the
experimental result shows that our method can reliably produce adversarial
examples with 100% attack success rate (ASR) while only by smaller
perturbations. In addition, the adversarial example generated with our method
can defeat defensive distillation based on gradient masking.","['Yaguan Qian', 'Ximin Zhang', 'Bin Wang', 'Wei Li', 'Zhaoquan Gu', 'Haijiang Wang', 'Wassim Swaileh']","['cs.LG', 'stat.ML']",2020-07-31 04:11:02+00:00
http://arxiv.org/abs/2007.15835v1,Deep Direct Likelihood Knockoffs,"Predictive modeling often uses black box machine learning methods, such as
deep neural networks, to achieve state-of-the-art performance. In scientific
domains, the scientist often wishes to discover which features are actually
important for making the predictions. These discoveries may lead to costly
follow-up experiments and as such it is important that the error rate on
discoveries is not too high. Model-X knockoffs enable important features to be
discovered with control of the FDR. However, knockoffs require rich generative
models capable of accurately modeling the knockoff features while ensuring they
obey the so-called ""swap"" property. We develop Deep Direct Likelihood Knockoffs
(DDLK), which directly minimizes the KL divergence implied by the knockoff swap
property. DDLK consists of two stages: it first maximizes the explicit
likelihood of the features, then minimizes the KL divergence between the joint
distribution of features and knockoffs and any swap between them. To ensure
that the generated knockoffs are valid under any possible swap, DDLK uses the
Gumbel-Softmax trick to optimize the knockoff generator under the worst-case
swap. We find DDLK has higher power than baselines while controlling the false
discovery rate on a variety of synthetic and real benchmarks including a task
involving a large dataset from one of the epicenters of COVID-19.","['Mukund Sudarshan', 'Wesley Tansey', 'Rajesh Ranganath']","['stat.ML', 'cs.LG', 'stat.ME']",2020-07-31 04:09:46+00:00
http://arxiv.org/abs/2007.15821v2,Geometric All-Way Boolean Tensor Decomposition,"Boolean tensor has been broadly utilized in representing high dimensional
logical data collected on spatial, temporal and/or other relational domains.
Boolean Tensor Decomposition (BTD) factorizes a binary tensor into the Boolean
sum of multiple rank-1 tensors, which is an NP-hard problem. Existing BTD
methods have been limited by their high computational cost, in applications to
large scale or higher order tensors. In this work, we presented a
computationally efficient BTD algorithm, namely \textit{Geometric Expansion for
all-order Tensor Factorization} (GETF), that sequentially identifies the rank-1
basis components for a tensor from a geometric perspective. We conducted
rigorous theoretical analysis on the validity as well as algorithemic
efficiency of GETF in decomposing all-order tensor. Experiments on both
synthetic and real-world data demonstrated that GETF has significantly improved
performance in reconstruction accuracy, extraction of latent structures and it
is an order of magnitude faster than other state-of-the-art methods.","['Changlin Wan', 'Wennan Chang', 'Tong Zhao', 'Sha Cao', 'Chi Zhang']","['cs.LG', 'cs.CG', 'stat.ML']",2020-07-31 03:29:44+00:00
http://arxiv.org/abs/2007.15816v2,Denoising individual bias for a fairer binary submatrix detection,"Low rank representation of binary matrix is powerful in disentangling sparse
individual-attribute associations, and has received wide applications. Existing
binary matrix factorization (BMF) or co-clustering (CC) methods often assume
i.i.d background noise. However, this assumption could be easily violated in
real data, where heterogeneous row- or column-wise probability of binary
entries results in disparate element-wise background distribution, and
paralyzes the rationality of existing methods. We propose a binary data
denoising framework, namely BIND, which optimizes the detection of true
patterns by estimating the row- or column-wise mixture distribution of patterns
and disparate background, and eliminating the binary attributes that are more
likely from the background. BIND is supported by thoroughly derived
mathematical property of the row- and column-wise mixture distributions. Our
experiment on synthetic and real-world data demonstrated BIND effectively
removes background noise and drastically increases the fairness and accuracy of
state-of-the arts BMF and CC methods.","['Changlin Wan', 'Wennan Chang', 'Tong Zhao', 'Sha Cao', 'Chi Zhang']","['cs.LG', 'stat.ML']",2020-07-31 02:52:25+00:00
http://arxiv.org/abs/2007.15802v1,Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases,"When the training data are maliciously tampered, the predictions of the
acquired deep neural network (DNN) can be manipulated by an adversary known as
the Trojan attack (or poisoning backdoor attack). The lack of robustness of
DNNs against Trojan attacks could significantly harm real-life machine learning
(ML) systems in downstream applications, therefore posing widespread concern to
their trustworthiness. In this paper, we study the problem of the Trojan
network (TrojanNet) detection in the data-scarce regime, where only the weights
of a trained DNN are accessed by the detector. We first propose a data-limited
TrojanNet detector (TND), when only a few data samples are available for
TrojanNet detection. We show that an effective data-limited TND can be
established by exploring connections between Trojan attack and
prediction-evasion adversarial attacks including per-sample attack as well as
all-sample universal attack. In addition, we propose a data-free TND, which can
detect a TrojanNet without accessing any data samples. We show that such a TND
can be built by leveraging the internal response of hidden neurons, which
exhibits the Trojan behavior even at random noise inputs. The effectiveness of
our proposals is evaluated by extensive experiments under different model
architectures and datasets including CIFAR-10, GTSRB, and ImageNet.","['Ren Wang', 'Gaoyuan Zhang', 'Sijia Liu', 'Pin-Yu Chen', 'Jinjun Xiong', 'Meng Wang']","['cs.LG', 'cs.CR', 'stat.ML']",2020-07-31 02:00:38+00:00
http://arxiv.org/abs/2007.15801v2,Finite Versus Infinite Neural Networks: an Empirical Study,"We perform a careful, thorough, and large scale empirical study of the
correspondence between wide neural networks and kernel methods. By doing so, we
resolve a variety of open questions related to the study of infinitely wide
neural networks. Our experimental results include: kernel methods outperform
fully-connected finite-width networks, but underperform convolutional finite
width networks; neural network Gaussian process (NNGP) kernels frequently
outperform neural tangent (NT) kernels; centered and ensembled finite networks
have reduced posterior variance and behave more similarly to infinite networks;
weight decay and the use of a large learning rate break the correspondence
between finite and infinite networks; the NTK parameterization outperforms the
standard parameterization for finite width networks; diagonal regularization of
kernels acts similarly to early stopping; floating point precision limits
kernel performance beyond a critical dataset size; regularized ZCA whitening
improves accuracy; finite network performance depends non-monotonically on
width in ways not captured by double descent phenomena; equivariance of CNNs is
only beneficial for narrow networks far from the kernel regime. Our experiments
additionally motivate an improved layer-wise scaling for weight decay which
improves generalization in finite-width networks. Finally, we develop improved
best practices for using NNGP and NT kernels for prediction, including a novel
ensembling technique. Using these best practices we achieve state-of-the-art
results on CIFAR-10 classification for kernels corresponding to each
architecture class we consider.","['Jaehoon Lee', 'Samuel S. Schoenholz', 'Jeffrey Pennington', 'Ben Adlam', 'Lechao Xiao', 'Roman Novak', 'Jascha Sohl-Dickstein']","['cs.LG', 'stat.ML']",2020-07-31 01:57:47+00:00
http://arxiv.org/abs/2007.15788v3,Stochastic Low-rank Tensor Bandits for Multi-dimensional Online Decision Making,"Multi-dimensional online decision making plays a crucial role in many real
applications such as online recommendation and digital marketing. In these
problems, a decision at each time is a combination of choices from different
types of entities. To solve it, we introduce stochastic low-rank tensor
bandits, a class of bandits whose mean rewards can be represented as a low-rank
tensor. We consider two settings, tensor bandits without context and tensor
bandits with context. In the first setting, the platform aims to find the
optimal decision with the highest expected reward, a.k.a, the largest entry of
true reward tensor. In the second setting, some modes of the tensor are
contexts and the rest modes are decisions, and the goal is to find the optimal
decision given the contextual information. We propose two learning algorithms
tensor elimination and tensor epoch-greedy for tensor bandits without context,
and derive finite-time regret bounds for them. Comparing with existing
competitive methods, tensor elimination has the best overall regret bound and
tensor epoch-greedy has a sharper dependency on dimensions of the reward
tensor. Furthermore, we develop a practically effective Bayesian algorithm
called tensor ensemble sampling for tensor bandits with context. Extensive
simulations and real analysis in online advertising data back up our
theoretical findings and show that our algorithms outperform various
state-of-the-art approaches that ignore the tensor low-rank structure.","['Jie Zhou', 'Botao Hao', 'Zheng Wen', 'Jingfei Zhang', 'Will Wei Sun']","['stat.ML', 'cs.LG']",2020-07-31 01:05:53+00:00
http://arxiv.org/abs/2007.15776v4,Random Vector Functional Link Networks for Function Approximation on Manifolds,"The learning speed of feed-forward neural networks is notoriously slow and
has presented a bottleneck in deep learning applications for several decades.
For instance, gradient-based learning algorithms, which are used extensively to
train neural networks, tend to work slowly when all of the network parameters
must be iteratively tuned. To counter this, both researchers and practitioners
have tried introducing randomness to reduce the learning requirement. Based on
the original construction of Igelnik and Pao, single layer neural-networks with
random input-to-hidden layer weights and biases have seen success in practice,
but the necessary theoretical justification is lacking. In this paper, we begin
to fill this theoretical gap. We provide a (corrected) rigorous proof that the
Igelnik and Pao construction is a universal approximator for continuous
functions on compact domains, with approximation error decaying asymptotically
like $O(1/\sqrt{n})$ for the number $n$ of network nodes. We then extend this
result to the non-asymptotic setting, proving that one can achieve any desired
approximation error with high probability provided $n$ is sufficiently large.
We further adapt this randomized neural network architecture to approximate
functions on smooth, compact submanifolds of Euclidean space, providing
theoretical guarantees in both the asymptotic and non-asymptotic forms.
Finally, we illustrate our results on manifolds with numerical experiments.","['Deanna Needell', 'Aaron A. Nelson', 'Rayan Saab', 'Palina Salanevich', 'Olov Schavemaker']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math.PR', '62M45']",2020-07-30 23:50:44+00:00
http://arxiv.org/abs/2007.15769v2,Instrument variable detection with graph learning : an application to high dimensional GIS-census data for house pricing,"Endogeneity bias and instrument variable validation have always been
important topics in statistics and econometrics. In the era of big data, such
issues typically combine with dimensionality issues and, hence, require even
more attention. In this paper, we merge two well-known tools from machine
learning and biostatistics---variable selection algorithms and probablistic
graphs---to estimate house prices and the corresponding causal structure using
2010 data on Sydney. The estimation uses a 200-gigabyte ultrahigh dimensional
database consisting of local school data, GIS information, census data, house
characteristics and other socio-economic records. Using ""big data"", we show
that it is possible to perform a data-driven instrument selection efficiently
and purge out the invalid instruments. Our approach improves the sparsity of
variable selection, stability and robustness in the presence of high
dimensionality, complicated causal structures and the consequent
multicollinearity, and recovers a sparse and intuitive causal structure. The
approach also reveals an efficiency and effectiveness in endogeneity detection,
instrument validation, weak instrument pruning and the selection of valid
instruments. From the perspective of machine learning, the estimation results
both align with and confirms the facts of Sydney house market, the classical
economic theories and the previous findings of simultaneous equations modeling.
Moreover, the estimation results are consistent with and supported by classical
econometric tools such as two-stage least square regression and different
instrument tests. All the code may be found at
\url{https://github.com/isaac2math/solar_graph_learning}.","['Ning Xu', 'Timothy C. G. Fisher', 'Jian Hong']","['stat.ML', 'cs.LG', 'stat.AP']",2020-07-30 23:11:54+00:00
http://arxiv.org/abs/2007.15766v4,Additive interaction modelling using I-priors,"Additive regression models with interactions are widely studied in the
literature, using methods such as splines or Gaussian process regression.
However, these methods can pose challenges for estimation and model selection,
due to the presence of many smoothing parameters and the lack of suitable
criteria. We propose to address these challenges by extending the I-prior
methodology (Bergsma, 2020) to multiple covariates, which may be
multidimensional. The I-prior methodology has some advantages over other
methods, such as Gaussian process regression and Tikhonov regularization, both
theoretically and practically. In particular, the I-prior is a proper prior, is
based on minimal assumptions, yields an admissible posterior mean, and
estimation of the scale (or smoothing) parameters can be done using an EM
algorithm with simple E and M steps. Moreover, we introduce a parsimonious
specification of models with interactions, which has two benefits: (i) it
reduces the number of scale parameters and thus facilitates the estimation of
models with interactions, and (ii) it enables straightforward model selection
(among models with different interactions) based on the marginal likelihood.","['Wicher Bergsma', 'Haziq Jamil']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2020-07-30 22:52:22+00:00
http://arxiv.org/abs/2007.15745v3,On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice,"Machine learning algorithms have been used widely in various applications and
areas. To fit a machine learning model into different problems, its
hyper-parameters must be tuned. Selecting the best hyper-parameter
configuration for machine learning models has a direct impact on the model's
performance. It often requires deep knowledge of machine learning algorithms
and appropriate hyper-parameter optimization techniques. Although several
automatic optimization techniques exist, they have different strengths and
drawbacks when applied to different types of problems. In this paper,
optimizing the hyper-parameters of common machine learning models is studied.
We introduce several state-of-the-art optimization techniques and discuss how
to apply them to machine learning algorithms. Many available libraries and
frameworks developed for hyper-parameter optimization problems are provided,
and some open challenges of hyper-parameter optimization research are also
discussed in this paper. Moreover, experiments are conducted on benchmark
datasets to compare the performance of different optimization methods and
provide practical examples of hyper-parameter optimization. This survey paper
will help industrial users, data analysts, and researchers to better develop
machine learning models by identifying the proper hyper-parameter
configurations effectively.","['Li Yang', 'Abdallah Shami']","['cs.LG', 'stat.ML', '68T01, 90C31', 'I.2.0; I.2.2; C.2.0']",2020-07-30 21:11:01+00:00
http://arxiv.org/abs/2007.15710v4,Privacy Enhancing Machine Learning via Removal of Unwanted Dependencies,"The rapid rise of IoT and Big Data has facilitated copious data driven
applications to enhance our quality of life. However, the omnipresent and
all-encompassing nature of the data collection can generate privacy concerns.
Hence, there is a strong need to develop techniques that ensure the data serve
only the intended purposes, giving users control over the information they
share. To this end, this paper studies new variants of supervised and
adversarial learning methods, which remove the sensitive information in the
data before they are sent out for a particular application. The explored
methods optimize privacy preserving feature mappings and predictive models
simultaneously in an end-to-end fashion. Additionally, the models are built
with an emphasis on placing little computational burden on the user side so
that the data can be desensitized on device in a cheap manner. Experimental
results on mobile sensing and face datasets demonstrate that our models can
successfully maintain the utility performances of predictive models while
causing sensitive predictions to perform poorly.","['Mert Al', 'Semih Yagli', 'Sun-Yuan Kung']","['cs.LG', 'stat.ML']",2020-07-30 19:55:10+00:00
http://arxiv.org/abs/2007.15707v3,Solar: $L_0$ solution path averaging for fast and accurate variable selection in high-dimensional data,"We propose a new variable selection algorithm, subsample-ordered least-angle
regression (solar), and its coordinate descent generalization, solar-cd. Solar
re-constructs lasso paths using the $L_0$ norm and averages the resulting
solution paths across subsamples. Path averaging retains the ranking
information of the informative variables while averaging out sensitivity to
high dimensionality, improving variable selection stability, efficiency, and
accuracy. We prove that: (i) with a high probability, path averaging perfectly
separates informative variables from redundant variables on the average $L_0$
path; (ii) solar variable selection is consistent and accurate; and (iii) the
probability that solar omits weak signals is controllable for finite sample
size. We also demonstrate that: (i) solar yields, with less than $1/3$ of the
lasso computation load, substantial improvements over lasso in terms of the
sparsity (64-84\% reduction in redundant variable selection) and accuracy of
variable selection; (ii) compared with the lasso safe/strong rule and variable
screening, solar largely avoids selection of redundant variables and rejection
of informative variables in the presence of complicated dependence structures;
(iii) the sparsity and stability of solar conserves residual degrees of freedom
for data-splitting hypothesis testing, improving the accuracy of post-selection
inference on weak signals with limited $n$; (iv) replacing lasso with solar in
bootstrap selection (e.g., bolasso or stability selection) produces a
multi-layer variable ranking scheme that improves selection sparsity and
ranking accuracy with the computation load of only one lasso realization; and
(v) given the computation resources, solar bootstrap selection is substantially
faster (98\% lower computation time) than the theoretical maximum speedup for
parallelized bootstrap lasso (confirmed by Amdahl's law).","['Ning Xu', 'Timothy C. G. Fisher']","['stat.ML', 'cs.LG']",2020-07-30 19:45:59+00:00
http://arxiv.org/abs/2007.15623v1,"On the Banach spaces associated with multi-layer ReLU networks: Function representation, approximation theory and gradient descent dynamics","We develop Banach spaces for ReLU neural networks of finite depth $L$ and
infinite width. The spaces contain all finite fully connected $L$-layer
networks and their $L^2$-limiting objects under bounds on the natural
path-norm. Under this norm, the unit ball in the space for $L$-layer networks
has low Rademacher complexity and thus favorable generalization properties.
Functions in these spaces can be approximated by multi-layer neural networks
with dimension-independent convergence rates.
  The key to this work is a new way of representing functions in some form of
expectations, motivated by multi-layer neural networks. This representation
allows us to define a new class of continuous models for machine learning. We
show that the gradient flow defined this way is the natural continuous analog
of the gradient descent dynamics for the associated multi-layer neural
networks. We show that the path-norm increases at most polynomially under this
continuous gradient flow dynamics.","['Weinan E', 'Stephan Wojtowytsch']","['stat.ML', 'cs.LG', 'math.FA', '68T07, 46E15, 26B35, 26B40']",2020-07-30 17:47:05+00:00
http://arxiv.org/abs/2007.15618v2,Outlier Robust Mean Estimation with Subgaussian Rates via Stability,"We study the problem of outlier robust high-dimensional mean estimation under
a finite covariance assumption, and more broadly under finite low-degree moment
assumptions. We consider a standard stability condition from the recent robust
statistics literature and prove that, except with exponentially small failure
probability, there exists a large fraction of the inliers satisfying this
condition. As a corollary, it follows that a number of recently developed
algorithms for robust mean estimation, including iterative filtering and
non-convex gradient descent, give optimal error estimators with
(near-)subgaussian rates. Previous analyses of these algorithms gave
significantly suboptimal rates. As a corollary of our approach, we obtain the
first computationally efficient algorithm with subgaussian rate for
outlier-robust mean estimation in the strong contamination model under a finite
covariance assumption.","['Ilias Diakonikolas', 'Daniel M. Kane', 'Ankit Pensia']","['math.ST', 'cs.DS', 'cs.LG', 'stat.ML', 'stat.TH']",2020-07-30 17:33:03+00:00
http://arxiv.org/abs/2007.15614v2,Accuracy and stability of solar variable selection comparison under complicated dependence structures,"In this paper we focus on the empirical variable-selection peformance of
subsample-ordered least angle regression (Solar) -- a novel ultrahigh
dimensional redesign of lasso -- on the empirical data with complicated
dependence structures and, hence, severe multicollinearity and grouping effect
issues. Previous researches show that Solar largely alleviates several known
high-dimensional issues with least-angle regression and $\mathcal{L}_1$
shrinkage. Also, With the same computation load, solar yields substantiali
mprovements over two lasso solvers (least-angle regression for lasso and
coordinate-descent) in terms of the sparsity (37-64\% reduction in the average
number of selected variables), stability and accuracy of variable selection.
Simulations also demonstrate that solar enhances the robustness of variable
selection to different settings of the irrepresentable condition and to
variations in the dependence structures assumed in regression analysis. To
confirm that the improvements are also available for empirical researches, we
choose the prostate cancer data and the Sydney house price data and apply two
lasso solvers, elastic net and Solar on them for comparison. The results shows
that (i) lasso is affected by the grouping effect and randomly drop variables
with high correlations, resulting unreliable and uninterpretable results; (ii)
elastic net is more robust to grouping effect; however, it completely lose
variable-selection sparsity when the dependence structure of the data is
complicated; (iii) solar demonstrates its superior robustness to complicated
dependence structures and grouping effect, returning variable-selection results
with better stability and sparsity. The code can be found at
https://github.com/isaac2math/solar_application","['Ning Xu', 'Timothy C. G. Fisher', 'Jian Hong']","['stat.ML', 'cs.LG']",2020-07-30 17:29:00+00:00
http://arxiv.org/abs/2007.15598v1,Rademacher upper bounds for cross-validation errors with an application to the lasso,"We establish a general upper bound for $K$-fold cross-validation ($K$-CV)
errors that can be adapted to many $K$-CV-based estimators and learning
algorithms. Based on Rademacher complexity of the model and the
Orlicz-$\Psi_{\nu}$ norm of the error process, the CV error upper bound applies
to both light-tail and heavy-tail error distributions. We also extend the CV
error upper bound to $\beta$-mixing data using the technique of independent
blocking. We provide a Python package (\texttt{CVbound},
\url{https://github.com/isaac2math}) for computing the CV error upper bound in
$K$-CV-based algorithms. Using the lasso as an example, we demonstrate in
simulations that the upper bounds are tight and stable across different
parameter settings and random seeds. As well as accurately bounding the CV
errors for the lasso, the minimizer of the new upper bounds can be used as a
criterion for variable selection. Compared with the CV-error minimizer,
simulations show that tuning the lasso penalty parameter according to the
minimizer of the upper bound yields a more sparse and more stable model that
retains all of the relevant variables.","['Ning Xu', 'Timothy C. G. Fisher', 'Jian Hong']","['stat.ML', 'cs.LG']",2020-07-30 17:13:03+00:00
http://arxiv.org/abs/2007.15588v2,Data-efficient Hindsight Off-policy Option Learning,"We introduce Hindsight Off-policy Options (HO2), a data-efficient option
learning algorithm. Given any trajectory, HO2 infers likely option choices and
backpropagates through the dynamic programming inference procedure to robustly
train all policy components off-policy and end-to-end. The approach outperforms
existing option learning methods on common benchmarks. To better understand the
option framework and disentangle benefits from both temporal and action
abstraction, we evaluate ablations with flat policies and mixture policies with
comparable optimization. The results highlight the importance of both types of
abstraction as well as off-policy training and trust-region constraints,
particularly in challenging, simulated 3D robot manipulation tasks from raw
pixel inputs. Finally, we intuitively adapt the inference step to investigate
the effect of increased temporal abstraction on training with pre-trained
options and from scratch.","['Markus Wulfmeier', 'Dushyant Rao', 'Roland Hafner', 'Thomas Lampe', 'Abbas Abdolmaleki', 'Tim Hertweck', 'Michael Neunert', 'Dhruva Tirumala', 'Noah Siegel', 'Nicolas Heess', 'Martin Riedmiller']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2020-07-30 16:52:33+00:00
http://arxiv.org/abs/2007.15568v2,Stopping Criterion Design for Recursive Bayesian Classification: Analysis and Decision Geometry,"Systems that are based on recursive Bayesian updates for classification limit
the cost of evidence collection through certain stopping/termination criteria
and accordingly enforce decision making. Conventionally, two termination
criteria based on pre-defined thresholds over (i) the maximum of the state
posterior distribution; and (ii) the state posterior uncertainty are commonly
used. In this paper, we propose a geometric interpretation over the state
posterior progression and accordingly we provide a point-by-point analysis over
the disadvantages of using such conventional termination criteria. For example,
through the proposed geometric interpretation we show that confidence
thresholds defined over maximum of the state posteriors suffer from stiffness
that results in unnecessary evidence collection whereas uncertainty based
thresholding methods are fragile to number of categories and terminate
prematurely if some state candidates are already discovered to be unfavorable.
Moreover, both types of termination methods neglect the evolution of posterior
updates. We then propose a new stopping/termination criterion with a
geometrical insight to overcome the limitations of these conventional methods
and provide a comparison in terms of decision accuracy and speed. We validate
our claims using simulations and using real experimental data obtained through
a brain computer interfaced typing system.","['Aziz Kocanaogullari', 'Murat Akcakaya', 'Deniz Erdogmus']","['cs.LG', 'eess.SP', 'stat.ML']",2020-07-30 16:21:10+00:00
http://arxiv.org/abs/2007.15567v1,Beyond $\mathcal{H}$-Divergence: Domain Adaptation Theory With Jensen-Shannon Divergence,"We reveal the incoherence between the widely-adopted empirical domain
adversarial training and its generally-assumed theoretical counterpart based on
$\mathcal{H}$-divergence. Concretely, we find that $\mathcal{H}$-divergence is
not equivalent to Jensen-Shannon divergence, the optimization objective in
domain adversarial training. To this end, we establish a new theoretical
framework by directly proving the upper and lower target risk bounds based on
joint distributional Jensen-Shannon divergence. We further derive
bi-directional upper bounds for marginal and conditional shifts. Our framework
exhibits inherent flexibilities for different transfer learning problems, which
is usable for various scenarios where $\mathcal{H}$-divergence-based theory
fails to adapt. From an algorithmic perspective, our theory enables a generic
guideline unifying principles of semantic conditional matching, feature
marginal matching, and label marginal shift correction. We employ algorithms
for each principle and empirically validate the benefits of our framework on
real datasets.","['Changjian Shui', 'Qi Chen', 'Jun Wen', 'Fan Zhou', 'Christian Gagné', 'Boyu Wang']","['cs.LG', 'stat.ML']",2020-07-30 16:19:59+00:00
http://arxiv.org/abs/2007.15553v1,Bilevel Continual Learning,"Continual learning aims to learn continuously from a stream of tasks and data
in an online-learning fashion, being capable of exploiting what was learned
previously to improve current and future tasks while still being able to
perform well on the previous tasks. One common limitation of many existing
continual learning methods is that they often train a model directly on all
available training data without validation due to the nature of continual
learning, thus suffering poor generalization at test time. In this work, we
present a novel framework of continual learning named ""Bilevel Continual
Learning"" (BCL) by unifying a {\it bilevel optimization} objective and a {\it
dual memory management} strategy comprising both episodic memory and
generalization memory to achieve effective knowledge transfer to future tasks
and alleviate catastrophic forgetting on old tasks simultaneously. Our
extensive experiments on continual learning benchmarks demonstrate the efficacy
of the proposed BCL compared to many state-of-the-art methods. Our
implementation is available at
https://github.com/phquang/bilevel-continual-learning.","['Quang Pham', 'Doyen Sahoo', 'Chenghao Liu', 'Steven C. H Hoi']","['cs.LG', 'stat.ML']",2020-07-30 16:00:23+00:00
http://arxiv.org/abs/2007.15543v2,PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping Pixels to Rewards,"Reinforcement learning (RL), particularly in sparse reward settings, often
requires prohibitively large numbers of interactions with the environment,
thereby limiting its applicability to complex problems. To address this,
several prior approaches have used natural language to guide the agent's
exploration. However, these approaches typically operate on structured
representations of the environment, and/or assume some structure in the natural
language commands. In this work, we propose a model that directly maps pixels
to rewards, given a free-form natural language description of the task, which
can then be used for policy learning. Our experiments on the Meta-World robot
manipulation domain show that language-based rewards significantly improves the
sample efficiency of policy learning, both in sparse and dense reward settings.","['Prasoon Goyal', 'Scott Niekum', 'Raymond J. Mooney']","['cs.LG', 'cs.AI', 'stat.ML']",2020-07-30 15:50:38+00:00
http://arxiv.org/abs/2007.15541v1,Anomaly Detection at Scale: The Case for Deep Distributional Time Series Models,"This paper introduces a new methodology for detecting anomalies in time
series data, with a primary application to monitoring the health of (micro-)
services and cloud resources. The main novelty in our approach is that instead
of modeling time series consisting of real values or vectors of real values, we
model time series of probability distributions over real values (or vectors).
This extension to time series of probability distributions allows the technique
to be applied to the common scenario where the data is generated by requests
coming in to a service, which is then aggregated at a fixed temporal frequency.
Our method is amenable to streaming anomaly detection and scales to monitoring
for anomalies on millions of time series. We show the superior accuracy of our
method on synthetic and public real-world data. On the Yahoo Webscope data set,
we outperform the state of the art in 3 out of 4 data sets and we show that we
outperform popular open-source anomaly detection tools by up to 17% average
improvement for a real-world data set.","['Fadhel Ayed', 'Lorenzo Stella', 'Tim Januschowski', 'Jan Gasthaus']","['cs.LG', 'stat.ML']",2020-07-30 15:48:55+00:00
http://arxiv.org/abs/2007.15535v2,Structural Inference in Sparse High-Dimensional Vector Autoregressions,"We consider statistical inference for impulse responses in sparse, structural
high-dimensional vector autoregressive (SVAR) systems. We introduce consistent
estimators of impulse responses in the high-dimensional setting and suggest
valid inference procedures for the same parameters. Statistical inference in
our setting is much more involved since standard procedures, like the
delta-method, do not apply. By using local projection equations, we first
construct a de-sparsified version of regularized estimators of the moving
average parameters associated with the VAR system. We then obtain estimators of
the structural impulse responses by combining the aforementioned de-sparsified
estimators with a non-regularized estimator of the contemporaneous impact
matrix, also taking into account the high-dimensionality of the system. We show
that the distribution of the derived estimators of structural impulse responses
has a Gaussian limit. We also present a valid bootstrap procedure to estimate
this distribution. Applications of the inference procedure in the construction
of confidence intervals for impulse responses as well as in tests for forecast
error variance decomposition are presented. Our procedure is illustrated by
means of simulations.","['Jonas Krampe', 'Efstathios Paparoditis', 'Carsten Trenkler']","['stat.ME', 'stat.ML']",2020-07-30 15:43:15+00:00
http://arxiv.org/abs/2007.15531v2,FC-GAGA: Fully Connected Gated Graph Architecture for Spatio-Temporal Traffic Forecasting,"Forecasting of multivariate time-series is an important problem that has
applications in traffic management, cellular network configuration, and
quantitative finance. A special case of the problem arises when there is a
graph available that captures the relationships between the time-series. In
this paper we propose a novel learning architecture that achieves performance
competitive with or better than the best existing algorithms, without requiring
knowledge of the graph. The key element of our proposed architecture is the
learnable fully connected hard graph gating mechanism that enables the use of
the state-of-the-art and highly computationally efficient fully connected
time-series forecasting architecture in traffic forecasting applications.
Experimental results for two public traffic network datasets illustrate the
value of our approach, and ablation studies confirm the importance of each
element of the architecture. The code is available here:
https://github.com/boreshkinai/fc-gaga.","['Boris N. Oreshkin', 'Arezou Amini', 'Lucy Coyle', 'Mark J. Coates']","['cs.LG', 'stat.ML']",2020-07-30 15:35:15+00:00
http://arxiv.org/abs/2007.15528v3,Membership Leakage in Label-Only Exposures,"Machine learning (ML) has been widely adopted in various privacy-critical
applications, e.g., face recognition and medical image analysis. However,
recent research has shown that ML models are vulnerable to attacks against
their training data. Membership inference is one major attack in this domain:
Given a data sample and model, an adversary aims to determine whether the
sample is part of the model's training set. Existing membership inference
attacks leverage the confidence scores returned by the model as their inputs
(score-based attacks). However, these attacks can be easily mitigated if the
model only exposes the predicted label, i.e., the final model decision.
  In this paper, we propose decision-based membership inference attacks and
demonstrate that label-only exposures are also vulnerable to membership
leakage. In particular, we develop two types of decision-based attacks, namely
transfer attack, and boundary attack. Empirical evaluation shows that our
decision-based attacks can achieve remarkable performance, and even outperform
the previous score-based attacks in some cases. We further present new insights
on the success of membership inference based on quantitative and qualitative
analysis, i.e., member samples of a model are more distant to the model's
decision boundary than non-member samples. Finally, we evaluate multiple
defense mechanisms against our decision-based attacks and show that our two
types of attacks can bypass most of these defenses.","['Zheng Li', 'Yang Zhang']","['cs.LG', 'cs.CR', 'stat.ML']",2020-07-30 15:27:55+00:00
http://arxiv.org/abs/2007.15421v2,Random Forests for dependent data,"Random forest (RF) is one of the most popular methods for estimating
regression functions. The local nature of the RF algorithm, based on intra-node
means and variances, is ideal when errors are i.i.d. For dependent error
processes like time series and spatial settings where data in all the nodes
will be correlated, operating locally ignores this dependence. Also, RF will
involve resampling of correlated data, violating the principles of bootstrap.
Theoretically, consistency of RF has been established for i.i.d. errors, but
little is known about the case of dependent errors.
  We propose RF-GLS, a novel extension of RF for dependent error processes in
the same way Generalized Least Squares (GLS) fundamentally extends Ordinary
Least Squares (OLS) for linear models under dependence. The key to this
extension is the equivalent representation of the local decision-making in a
regression tree as a global OLS optimization which is then replaced with a GLS
loss to create a GLS-style regression tree. This also synergistically addresses
the resampling issue, as the use of GLS loss amounts to resampling uncorrelated
contrasts (pre-whitened data) instead of the correlated data. For spatial
settings, RF-GLS can be used in conjunction with Gaussian Process correlated
errors to generate kriging predictions at new locations. RF becomes a special
case of RF-GLS with an identity working covariance matrix.
  We establish consistency of RF-GLS under beta- (absolutely regular) mixing
error processes and show that this general result subsumes important cases like
autoregressive time series and spatial Matern Gaussian Processes. As a
byproduct, we also establish consistency of RF for beta-mixing processes, which
to our knowledge, is the first such result for RF under dependence.
  We empirically demonstrate the improvement achieved by RF-GLS over RF for
both estimation and prediction under dependence.","['Arkajyoti Saha', 'Sumanta Basu', 'Abhirup Datta']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2020-07-30 12:36:09+00:00
http://arxiv.org/abs/2007.15418v1,Momentum Q-learning with Finite-Sample Convergence Guarantee,"Existing studies indicate that momentum ideas in conventional optimization
can be used to improve the performance of Q-learning algorithms. However, the
finite-sample analysis for momentum-based Q-learning algorithms is only
available for the tabular case without function approximations. This paper
analyzes a class of momentum-based Q-learning algorithms with finite-sample
guarantee. Specifically, we propose the MomentumQ algorithm, which integrates
the Nesterov's and Polyak's momentum schemes, and generalizes the existing
momentum-based Q-learning algorithms. For the infinite state-action space case,
we establish the convergence guarantee for MomentumQ with linear function
approximations and Markovian sampling. In particular, we characterize the
finite-sample convergence rate which is provably faster than the vanilla
Q-learning. This is the first finite-sample analysis for momentum-based
Q-learning algorithms with function approximations. For the tabular case under
synchronous sampling, we also obtain a finite-sample convergence rate that is
slightly better than the SpeedyQ \citep{azar2011speedy} when choosing a special
family of step sizes. Finally, we demonstrate through various experiments that
the proposed MomentumQ outperforms other momentum-based Q-learning algorithms.","['Bowen Weng', 'Huaqing Xiong', 'Lin Zhao', 'Yingbin Liang', 'Wei Zhang']","['cs.LG', 'math.OC', 'stat.ML']",2020-07-30 12:27:03+00:00
http://arxiv.org/abs/2007.15409v1,Evolving Context-Aware Recommender Systems With Users in Mind,"A context-aware recommender system (CARS) applies sensing and analysis of
user context to provide personalized services. The contextual information can
be driven from sensors in order to improve the accuracy of the recommendations.
Yet, generating accurate recommendations is not enough to constitute a useful
system from the users' perspective, since certain contextual information may
cause different issues, such as draining the user's battery, privacy issues,
and more. Adding high-dimensional contextual information may increase both the
dimensionality and sparsity of the model. Previous studies suggest reducing the
amount of contextual information by selecting the most suitable contextual
information using a domain knowledge. Another solution is compressing it into a
denser latent space, thus disrupting the ability to explain the recommendation
item to the user, and damaging users' trust. In this paper we present an
approach for selecting low-dimensional subsets of the contextual information
and incorporating them explicitly within CARS. Specifically, we present a novel
feature-selection algorithm, based on genetic algorithms (GA), that outperforms
SOTA dimensional-reduction CARS algorithms, improves the accuracy and the
explainability of the recommendations, and allows for controlling user aspects,
such as privacy and battery consumption. Furthermore, we exploit the top
subsets that are generated along the evolutionary process, by learning multiple
deep context-aware models and applying a stacking technique on them, thus
improving the accuracy while remaining at the explicit space. We evaluated our
approach on two high-dimensional context-aware datasets driven from
smartphones. An empirical analysis of our results validates that our proposed
approach outperforms SOTA CARS models while improving transparency and
explainability to the user.","['Amit Livne', 'Eliad Shem Tov', 'Adir Solomon', 'Achiya Elyasaf', 'Bracha Shapira', 'Lior Rokach']","['cs.LG', 'cs.IR', 'cs.SI', 'stat.ML']",2020-07-30 12:03:22+00:00
http://arxiv.org/abs/2007.15404v1,Regional Rainfall Prediction Using Support Vector Machine Classification of Large-Scale Precipitation Maps,"Rainfall prediction helps planners anticipate potential social and economic
impacts produced by too much or too little rain. This research investigates a
class-based approach to rainfall prediction from 1-30 days in advance. The
study made regional predictions based on sequences of daily rainfall maps of
the continental US, with rainfall quantized at 3 levels: light or no rain;
moderate; and heavy rain. Three regions were selected, corresponding to three
squares from a $5\times5$ grid covering the map area. Rainfall predictions up
to 30 days ahead for these three regions were based on a support vector machine
(SVM) applied to consecutive sequences of prior daily rainfall map images. The
results show that predictions for corner squares in the grid were less accurate
than predictions obtained by a simple untrained classifier. However, SVM
predictions for a central region outperformed the other two regions, as well as
the untrained classifier. We conclude that there is some evidence that SVMs
applied to large-scale precipitation maps can under some conditions give useful
information for predicting regional rainfall, but care must be taken to avoid
pitfall","['Eslam A. Hussein', 'Mehrdad Ghaziasgar', 'Christopher Thron']","['cs.LG', 'stat.ML']",2020-07-30 11:56:19+00:00
http://arxiv.org/abs/2007.15397v2,Improving Sample Efficiency with Normalized RBF Kernels,"In deep learning models, learning more with less data is becoming more
important. This paper explores how neural networks with normalized Radial Basis
Function (RBF) kernels can be trained to achieve better sample efficiency.
Moreover, we show how this kind of output layer can find embedding spaces where
the classes are compact and well-separated. In order to achieve this, we
propose a two-phase method to train those type of neural networks on
classification tasks. Experiments on CIFAR-10 and CIFAR-100 show that networks
with normalized kernels as output layer can achieve higher sample efficiency,
high compactness and well-separability through the presented method in
comparison to networks with SoftMax output layer.","['Sebastian Pineda-Arango', 'David Obando-Paniagua', 'Alperen Dedeoglu', 'Philip Kurzendörfer', 'Friedemann Schestag', 'Randolf Scholz']","['cs.LG', 'stat.ML']",2020-07-30 11:40:29+00:00
http://arxiv.org/abs/2007.15386v2,ResNet After All? Neural ODEs and Their Numerical Solution,"A key appeal of the recently proposed Neural Ordinary Differential Equation
(ODE) framework is that it seems to provide a continuous-time extension of
discrete residual neural networks. As we show herein, though, trained Neural
ODE models actually depend on the specific numerical method used during
training. If the trained model is supposed to be a flow generated from an ODE,
it should be possible to choose another numerical solver with equal or smaller
numerical error without loss of performance. We observe that if training relies
on a solver with overly coarse discretization, then testing with another solver
of equal or smaller numerical error results in a sharp drop in accuracy. In
such cases, the combination of vector field and numerical method cannot be
interpreted as a flow generated from an ODE, which arguably poses a fatal
breakdown of the Neural ODE concept. We observe, however, that there exists a
critical step size beyond which the training yields a valid ODE vector field.
We propose a method that monitors the behavior of the ODE solver during
training to adapt its step size, aiming to ensure a valid ODE without
unnecessarily increasing computational cost. We verify this adaptation
algorithm on a common bench mark dataset as well as a synthetic dataset.","['Katharina Ott', 'Prateek Katiyar', 'Philipp Hennig', 'Michael Tiemann']","['cs.LG', 'stat.ML']",2020-07-30 11:24:05+00:00
http://arxiv.org/abs/2007.15378v1,Generalization Comparison of Deep Neural Networks via Output Sensitivity,"Although recent works have brought some insights into the performance
improvement of techniques used in state-of-the-art deep-learning models, more
work is needed to understand their generalization properties. We shed light on
this matter by linking the loss function to the output's sensitivity to its
input. We find a rather strong empirical relation between the output
sensitivity and the variance in the bias-variance decomposition of the loss
function, which hints on using sensitivity as a metric for comparing the
generalization performance of networks, without requiring labeled data. We find
that sensitivity is decreased by applying popular methods which improve the
generalization performance of the model, such as (1) using a deep network
rather than a wide one, (2) adding convolutional layers to baseline classifiers
instead of adding fully-connected layers, (3) using batch normalization,
dropout and max-pooling, and (4) applying parameter initialization techniques.","['Mahsa Forouzesh', 'Farnood Salehi', 'Patrick Thiran']","['cs.LG', 'stat.ML']",2020-07-30 11:08:42+00:00
http://arxiv.org/abs/2007.15359v1,Trade-offs in Top-k Classification Accuracies on Losses for Deep Learning,"This paper presents an experimental analysis about trade-offs in top-k
classification accuracies on losses for deep leaning and proposal of a novel
top-k loss. Commonly-used cross entropy (CE) is not guaranteed to optimize
top-k prediction without infinite training data and model complexities. The
objective is to clarify when CE sacrifices top-k accuracies to optimize top-1
prediction, and to design loss that improve top-k accuracy under such
conditions. Our novel loss is basically CE modified by grouping temporal top-k
classes as a single class. To obtain a robust decision boundary, we introduce
an adaptive transition from normal CE to our loss, and thus call it top-k
transition loss. It is demonstrated that CE is not always the best choice to
learn top-k prediction in our experiments. First, we explore trade-offs between
top-1 and top-k (=2) accuracies on synthetic datasets, and find a failure of CE
in optimizing top-k prediction when we have complex data distribution for a
given model to represent optimal top-1 prediction. Second, we compare top-k
accuracies on CIFAR-100 dataset targeting top-5 prediction in deep learning.
While CE performs the best in top-1 accuracy, in top-5 accuracy our loss
performs better than CE except using one experimental setup. Moreover, our loss
has been found to provide better top-k accuracies compared to CE at k larger
than 10. As a result, a ResNet18 model trained with our loss reaches 99 %
accuracy with k=25 candidates, which is a smaller candidate number than that of
CE by 8.","['Azusa Sawada', 'Eiji Kaneko', 'Kazutoshi Sagi']","['cs.LG', 'stat.ML']",2020-07-30 10:18:57+00:00
http://arxiv.org/abs/2007.15353v2,Growing Efficient Deep Networks by Structured Continuous Sparsification,"We develop an approach to growing deep network architectures over the course
of training, driven by a principled combination of accuracy and sparsity
objectives. Unlike existing pruning or architecture search techniques that
operate on full-sized models or supernet architectures, our method can start
from a small, simple seed architecture and dynamically grow and prune both
layers and filters. By combining a continuous relaxation of discrete network
structure optimization with a scheme for sampling sparse subnetworks, we
produce compact, pruned networks, while also drastically reducing the
computational expense of training. For example, we achieve $49.7\%$ inference
FLOPs and $47.4\%$ training FLOPs savings compared to a baseline ResNet-50 on
ImageNet, while maintaining $75.2\%$ top-1 accuracy -- all without any
dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC,
and Penn Treebank, with convolutional networks for image classification and
semantic segmentation, and recurrent networks for language modeling,
demonstrate that we both train faster and produce more efficient networks than
competing architecture pruning or search methods.","['Xin Yuan', 'Pedro Savarese', 'Michael Maire']","['cs.LG', 'stat.ML']",2020-07-30 10:03:47+00:00
http://arxiv.org/abs/2007.15331v2,A PAC algorithm in relative precision for bandit problem with costly sampling,"This paper considers the problem of maximizing an expectation function over a
finite set, or finite-arm bandit problem. We first propose a naive stochastic
bandit algorithm for obtaining a probably approximately correct (PAC) solution
to this discrete optimization problem in relative precision, that is a solution
which solves the optimization problem up to a relative error smaller than a
prescribed tolerance, with high probability. We also propose an adaptive
stochastic bandit algorithm which provides a PAC-solution with the same
guarantees. The adaptive algorithm outperforms the mean complexity of the naive
algorithm in terms of number of generated samples and is particularly well
suited for applications with high sampling cost.","['Marie Billaud-Friess', 'Arthur Macherey', 'Anthony Nouy', 'Clémentine Prieur']","['math.OC', 'cs.LG', 'stat.ML']",2020-07-30 09:22:25+00:00
http://arxiv.org/abs/2007.15326v1,A Recommendation and Risk Classification System for Connecting Rough Sleepers to Essential Outreach Services,"Rough sleeping is a chronic problem faced by some of the most disadvantaged
people in modern society. This paper describes work carried out in partnership
with Homeless Link, a UK-based charity, in developing a data-driven approach to
assess the quality of incoming alerts from members of the public aimed at
connecting people sleeping rough on the streets with outreach service
providers. Alerts are prioritised based on the predicted likelihood of
successfully connecting with the rough sleeper, helping to address capacity
limitations and to quickly, effectively, and equitably process all of the
alerts that they receive. Initial evaluation concludes that our approach
increases the rate at which rough sleepers are found following a referral by at
least 15\% based on labelled data, implying a greater overall increase when the
alerts with unknown outcomes are considered, and suggesting the benefit in a
trial taking place over a longer period to assess the models in practice. The
discussion and modelling process is done with careful considerations of ethics,
transparency and explainability due to the sensitive nature of the data in this
context and the vulnerability of the people that are affected.","['Harrison Wilde', 'Lucia Lushi Chen', 'Austin Nguyen', 'Zoe Kimpel', 'Joshua Sidgwick', 'Adolfo De Unanue', 'Davide Veronese', 'Bilal Mateen', 'Rayid Ghani', 'Sebastian Vollmer']","['stat.AP', 'stat.ML']",2020-07-30 09:14:46+00:00
http://arxiv.org/abs/2007.15310v1,Black-box Adversarial Sample Generation Based on Differential Evolution,"Deep Neural Networks (DNNs) are being used in various daily tasks such as
object detection, speech processing, and machine translation. However, it is
known that DNNs suffer from robustness problems -- perturbed inputs called
adversarial samples leading to misbehaviors of DNNs. In this paper, we propose
a black-box technique called Black-box Momentum Iterative Fast Gradient Sign
Method (BMI-FGSM) to test the robustness of DNN models. The technique does not
require any knowledge of the structure or weights of the target DNN. Compared
to existing white-box testing techniques that require accessing model internal
information such as gradients, our technique approximates gradients through
Differential Evolution and uses approximated gradients to construct adversarial
samples. Experimental results show that our technique can achieve 100% success
in generating adversarial samples to trigger misclassification, and over 95%
success in generating samples to trigger misclassification to a specific target
output label. It also demonstrates better perturbation distance and better
transferability. Compared to the state-of-the-art black-box technique, our
technique is more efficient. Furthermore, we conduct testing on the commercial
Aliyun API and successfully trigger its misbehavior within a limited number of
queries, demonstrating the feasibility of real-world black-box attack.","['Junyu Lin', 'Lei Xu', 'Yingqi Liu', 'Xiangyu Zhang']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2020-07-30 08:43:45+00:00
http://arxiv.org/abs/2007.15270v2,Fairness-Aware Online Personalization,"Decision making in crucial applications such as lending, hiring, and college
admissions has witnessed increasing use of algorithmic models and techniques as
a result of a confluence of factors such as ubiquitous connectivity, ability to
collect, aggregate, and process large amounts of fine-grained data using cloud
computing, and ease of access to applying sophisticated machine learning
models. Quite often, such applications are powered by search and recommendation
systems, which in turn make use of personalized ranking algorithms. At the same
time, there is increasing awareness about the ethical and legal challenges
posed by the use of such data-driven systems. Researchers and practitioners
from different disciplines have recently highlighted the potential for such
systems to discriminate against certain population groups, due to biases in the
datasets utilized for learning their underlying recommendation models. We
present a study of fairness in online personalization settings involving the
ranking of individuals. Starting from a fair warm-start machine-learned model,
we first demonstrate that online personalization can cause the model to learn
to act in an unfair manner if the user is biased in his/her responses. For this
purpose, we construct a stylized model for generating training data with
potentially biased features as well as potentially biased labels and quantify
the extent of bias that is learned by the model when the user responds in a
biased manner as in many real-world scenarios. We then formulate the problem of
learning personalized models under fairness constraints and present a
regularization based approach for mitigating biases in machine learning. We
demonstrate the efficacy of our approach through extensive simulations with
different parameter settings. Code:
https://github.com/groshanlal/Fairness-Aware-Online-Personalization","['G Roshan Lal', 'Sahin Cem Geyik', 'Krishnaram Kenthapadi']","['cs.AI', 'cs.LG', 'stat.ML']",2020-07-30 07:16:17+00:00
http://arxiv.org/abs/2007.15255v2,Instance Selection for GANs,"Recent advances in Generative Adversarial Networks (GANs) have led to their
widespread adoption for the purposes of generating high quality synthetic
imagery. While capable of generating photo-realistic images, these models often
produce unrealistic samples which fall outside of the data manifold. Several
recently proposed techniques attempt to avoid spurious samples, either by
rejecting them after generation, or by truncating the model's latent space.
While effective, these methods are inefficient, as a large fraction of training
time and model capacity are dedicated towards samples that will ultimately go
unused. In this work we propose a novel approach to improve sample quality:
altering the training dataset via instance selection before model training has
taken place. By refining the empirical data distribution before training, we
redirect model capacity towards high-density regions, which ultimately improves
sample fidelity, lowers model capacity requirements, and significantly reduces
training time. Code is available at
https://github.com/uoguelph-mlrg/instance_selection_for_gans.","['Terrance DeVries', 'Michal Drozdzal', 'Graham W. Taylor']","['cs.CV', 'cs.LG', 'stat.ML']",2020-07-30 06:33:51+00:00
http://arxiv.org/abs/2007.15248v1,DeepPeep: Exploiting Design Ramifications to Decipher the Architecture of Compact DNNs,"The remarkable predictive performance of deep neural networks (DNNs) has led
to their adoption in service domains of unprecedented scale and scope. However,
the widespread adoption and growing commercialization of DNNs have underscored
the importance of intellectual property (IP) protection. Devising techniques to
ensure IP protection has become necessary due to the increasing trend of
outsourcing the DNN computations on the untrusted accelerators in cloud-based
services. The design methodologies and hyper-parameters of DNNs are crucial
information, and leaking them may cause massive economic loss to the
organization. Furthermore, the knowledge of DNN's architecture can increase the
success probability of an adversarial attack where an adversary perturbs the
inputs and alter the prediction.
  In this work, we devise a two-stage attack methodology ""DeepPeep"" which
exploits the distinctive characteristics of design methodologies to
reverse-engineer the architecture of building blocks in compact DNNs. We show
the efficacy of ""DeepPeep"" on P100 and P4000 GPUs. Additionally, we propose
intelligent design maneuvering strategies for thwarting IP theft through the
DeepPeep attack and proposed ""Secure MobileNet-V1"". Interestingly, compared to
vanilla MobileNet-V1, secure MobileNet-V1 provides a significant reduction in
inference latency ($\approx$60%) and improvement in predictive performance
($\approx$2%) with very-low memory and computation overheads.","['Nandan Kumar Jha', 'Sparsh Mittal', 'Binod Kumar', 'Govardhan Mattela']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML', 'K.4.1; K.4.4']",2020-07-30 06:01:41+00:00
http://arxiv.org/abs/2007.15241v4,Out-of-distribution Generalization via Partial Feature Decorrelation,"Most deep-learning-based image classification methods assume that all samples
are generated under an independent and identically distributed (IID) setting.
However, out-of-distribution (OOD) generalization is more common in practice,
which means an agnostic context distribution shift between training and testing
environments. To address this problem, we present a novel Partial Feature
Decorrelation Learning (PFDL) algorithm, which jointly optimizes a feature
decomposition network and the target image classification model. The feature
decomposition network decomposes feature embeddings into the independent and
the correlated parts such that the correlations between features will be
highlighted. Then, the correlated features help learn a stable feature
representation by decorrelating the highlighted correlations while optimizing
the image classification model. We verify the correlation modeling ability of
the feature decomposition network on a synthetic dataset. The experiments on
real-world datasets demonstrate that our method can improve the backbone
model's accuracy on OOD image classification datasets.","['Xin Guo', 'Zhengxu Yu', 'Chao Xiang', 'Zhongming Jin', 'Jianqiang Huang', 'Deng Cai', 'Xiaofei He', 'Xian-Sheng Hua']","['cs.LG', 'stat.ML']",2020-07-30 05:48:48+00:00
http://arxiv.org/abs/2007.16208v1,G-CREWE: Graph CompREssion With Embedding for Network Alignment,"Network alignment is useful for multiple applications that require
increasingly large graphs to be processed. Existing research approaches this as
an optimization problem or computes the similarity based on node
representations. However, the process of aligning every pair of nodes between
relatively large networks is time-consuming and resource-intensive. In this
paper, we propose a framework, called G-CREWE (Graph CompREssion With
Embedding) to solve the network alignment problem. G-CREWE uses node embeddings
to align the networks on two levels of resolution, a fine resolution given by
the original network and a coarse resolution given by a compressed version, to
achieve an efficient and effective network alignment. The framework first
extracts node features and learns the node embedding via a Graph Convolutional
Network (GCN). Then, node embedding helps to guide the process of graph
compression and finally improve the alignment performance. As part of G-CREWE,
we also propose a new compression mechanism called MERGE (Minimum dEgRee
neiGhbors comprEssion) to reduce the size of the input networks while
preserving the consistency in their topological structure. Experiments on all
real networks show that our method is more than twice as fast as the most
competitive existing methods while maintaining high accuracy.","['Kyle K. Qin', 'Flora D. Salim', 'Yongli Ren', 'Wei Shao', 'Mark Heimann', 'Danai Koutra']","['cs.SI', 'cs.DB', 'cs.IR', 'cs.LG', 'stat.ML']",2020-07-30 05:30:21+00:00
http://arxiv.org/abs/2007.15222v2,SynergicLearning: Neural Network-Based Feature Extraction for Highly-Accurate Hyperdimensional Learning,"Machine learning models differ in terms of accuracy, computational/memory
complexity, training time, and adaptability among other characteristics. For
example, neural networks (NNs) are well-known for their high accuracy due to
the quality of their automatic feature extraction while brain-inspired
hyperdimensional (HD) learning models are famous for their quick training,
computational efficiency, and adaptability. This work presents a hybrid,
synergic machine learning model that excels at all the said characteristics and
is suitable for incremental, on-line learning on a chip. The proposed model
comprises an NN and a classifier. The NN acts as a feature extractor and is
specifically trained to work well with the classifier that employs the HD
computing framework. This work also presents a parameterized hardware
implementation of the said feature extraction and classification components
while introducing a compiler that maps any arbitrary NN and/or classifier to
the aforementioned hardware. The proposed hybrid machine learning model has the
same level of accuracy (i.e. $\pm$1%) as NNs while achieving at least 10%
improvement in accuracy compared to HD learning models. Additionally, the
end-to-end hardware realization of the hybrid model improves power efficiency
by 1.60x compared to state-of-the-art, high-performance HD learning
implementations while improving latency by 2.13x. These results have profound
implications for the application of such synergic models in challenging
cognitive tasks.","['Mahdi Nazemi', 'Amirhossein Esmaili', 'Arash Fayyazi', 'Massoud Pedram']","['cs.LG', 'stat.ML']",2020-07-30 04:35:20+00:00
http://arxiv.org/abs/2007.15220v1,The Complexity of Adversarially Robust Proper Learning of Halfspaces with Agnostic Noise,"We study the computational complexity of adversarially robust proper learning
of halfspaces in the distribution-independent agnostic PAC model, with a focus
on $L_p$ perturbations. We give a computationally efficient learning algorithm
and a nearly matching computational hardness result for this problem. An
interesting implication of our findings is that the $L_{\infty}$ perturbations
case is provably computationally harder than the case $2 \leq p < \infty$.","['Ilias Diakonikolas', 'Daniel M. Kane', 'Pasin Manurangsi']","['cs.LG', 'cs.CC', 'cs.DS', 'stat.ML']",2020-07-30 04:18:51+00:00
