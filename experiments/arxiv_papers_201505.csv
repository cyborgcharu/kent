id,title,abstract,authors,categories,date
http://arxiv.org/abs/1507.00066v1,Fast Cross-Validation for Incremental Learning,"Cross-validation (CV) is one of the main tools for performance estimation and
parameter tuning in machine learning. The general recipe for computing CV
estimate is to run a learning algorithm separately for each CV fold, a
computationally expensive process. In this paper, we propose a new approach to
reduce the computational burden of CV-based performance estimation. As opposed
to all previous attempts, which are specific to a particular learning model or
problem domain, we propose a general method applicable to a large class of
incremental learning algorithms, which are uniquely fitted to big data
problems. In particular, our method applies to a wide range of supervised and
unsupervised learning tasks with different performance criteria, as long as the
base learning algorithm is incremental. We show that the running time of the
algorithm scales logarithmically, rather than linearly, in the number of CV
folds. Furthermore, the algorithm has favorable properties for parallel and
distributed implementation. Experiments with state-of-the-art incremental
learning algorithms confirm the practicality of the proposed method.","['Pooria Joulani', 'András György', 'Csaba Szepesvári']","['stat.ML', 'cs.AI', 'cs.LG']",2015-06-30 23:30:28+00:00
http://arxiv.org/abs/1507.00052v2,Gaussian Process for Noisy Inputs with Ordering Constraints,"We study the Gaussian Process regression model in the context of training
data with noise in both input and output. The presence of two sources of noise
makes the task of learning accurate predictive models extremely challenging.
However, in some instances additional constraints may be available that can
reduce the uncertainty in the resulting predictive models. In particular, we
consider the case of monotonically ordered latent input, which occurs in many
application domains that deal with temporal data. We present a novel inference
and learning approach based on non-parametric Gaussian variational
approximation to learn the GP model while taking into account the new
constraints. The resulting strategy allows one to gain access to posterior
estimates of both the input and the output and results in improved predictive
performance. We compare our proposed models to state-of-the-art Noisy Input
Gaussian Process (NIGP) and other competing approaches on synthetic and real
sea-level rise data. Experimental results suggest that the proposed approach
consistently outperforms selected methods while, at the same time, reducing the
computational costs of learning and inference.","['Cuong Tran', 'Vladimir Pavlovic', 'Robert Kopp']",['stat.ML'],2015-06-30 22:28:06+00:00
http://arxiv.org/abs/1507.00043v2,Top-N recommendations in the presence of sparsity: An NCD-based approach,"Making recommendations in the presence of sparsity is known to present one of
the most challenging problems faced by collaborative filtering methods. In this
work we tackle this problem by exploiting the innately hierarchical structure
of the item space following an approach inspired by the theory of
Decomposability. We view the itemspace as a Nearly Decomposable system and we
define blocks of closely related elements and corresponding indirect proximity
components. We study the theoretical properties of the decomposition and we
derive sufficient conditions that guarantee full item space coverage even in
cold-start recommendation scenarios. A comprehensive set of experiments on the
MovieLens and the Yahoo!R2Music datasets, using several widely applied
performance metrics, support our model's theoretically predicted properties and
verify that NCDREC outperforms several state-of-the-art algorithms, in terms of
recommendation accuracy, diversity and sparseness insensitivity.","['Athanasios N. Nikolakopoulos', 'John D. Garofalakis']","['cs.IR', 'cs.AI', 'stat.ML']",2015-06-30 21:34:53+00:00
http://arxiv.org/abs/1507.00039v1,Selective Inference and Learning Mixed Graphical Models,"This thesis studies two problems in modern statistics. First, we study
selective inference, or inference for hypothesis that are chosen after looking
at the data. The motiving application is inference for regression coefficients
selected by the lasso. We present the Condition-on-Selection method that allows
for valid selective inference, and study its application to the lasso, and
several other selection algorithms.
  In the second part, we consider the problem of learning the structure of a
pairwise graphical model over continuous and discrete variables. We present a
new pairwise model for graphical models with both continuous and discrete
variables that is amenable to structure learning. In previous work, authors
have considered structure learning of Gaussian graphical models and structure
learning of discrete models. Our approach is a natural generalization of these
two lines of work to the mixed case. The penalization scheme involves a novel
symmetric use of the group-lasso norm and follows naturally from a particular
parametrization of the model. We provide conditions under which our estimator
is model selection consistent in the high-dimensional regime.",['Jason D. Lee'],"['stat.ML', 'cs.LG']",2015-06-30 21:10:18+00:00
http://arxiv.org/abs/1506.09153v1,Framework for Multi-task Multiple Kernel Learning and Applications in Genome Analysis,"We present a general regularization-based framework for Multi-task learning
(MTL), in which the similarity between tasks can be learned or refined using
$\ell_p$-norm Multiple Kernel learning (MKL). Based on this very general
formulation (including a general loss function), we derive the corresponding
dual formulation using Fenchel duality applied to Hermitian matrices. We show
that numerous established MTL methods can be derived as special cases from
both, the primal and dual of our formulation. Furthermore, we derive a modern
dual-coordinate descend optimization strategy for the hinge-loss variant of our
formulation and provide convergence bounds for our algorithm. As a special
case, we implement in C++ a fast LibLinear-style solver for $\ell_p$-norm MKL.
In the experimental section, we analyze various aspects of our algorithm such
as predictive performance and ability to reconstruct task relationships on
biologically inspired synthetic data, where we have full control over the
underlying ground truth. We also experiment on a new dataset from the domain of
computational biology that we collected for the purpose of this paper. It
concerns the prediction of transcription start sites (TSS) over nine organisms,
which is a crucial task in gene finding. Our solvers including all discussed
special cases are made available as open-source software as part of the SHOGUN
machine learning toolbox (available at \url{http://shogun.ml}).","['Christian Widmer', 'Marius Kloft', 'Vipin T Sreedharan', 'Gunnar Rätsch']","['stat.ML', 'cs.CE', 'cs.LG']",2015-06-30 16:52:27+00:00
http://arxiv.org/abs/1506.09068v2,On the Equivalence of Factorized Information Criterion Regularization and the Chinese Restaurant Process Prior,"Factorized Information Criterion (FIC) is a recently developed information
criterion, based on which a novel model selection methodology, namely
Factorized Asymptotic Bayesian (FAB) Inference, has been developed and
successfully applied to various hierarchical Bayesian models. The Dirichlet
Process (DP) prior, and one of its well known representations, the Chinese
Restaurant Process (CRP), derive another line of model selection methods. FIC
can be viewed as a prior distribution over the latent variable configurations.
Under this view, we prove that when the parameter dimensionality $D_{c}=2$, FIC
is equivalent to CRP. We argue that when $D_{c}>2$, FIC avoids an inherent
problem of DP/CRP, i.e. the data likelihood will dominate the impact of the
prior, and thus the model selection capability will weaken as $D_{c}$
increases. However, FIC overestimates the data likelihood. As a result, FIC may
be overly biased towards models with less components. We propose a natural
generalization of FIC, which finds a middle ground between CRP and FIC, and may
yield more accurate model selection results than FIC.",['Shaohua Li'],['stat.ML'],2015-06-30 13:02:15+00:00
http://arxiv.org/abs/1506.09039v3,Scalable Discrete Sampling as a Multi-Armed Bandit Problem,"Drawing a sample from a discrete distribution is one of the building
components for Monte Carlo methods. Like other sampling algorithms, discrete
sampling suffers from the high computational burden in large-scale inference
problems. We study the problem of sampling a discrete random variable with a
high degree of dependency that is typical in large-scale Bayesian inference and
graphical models, and propose an efficient approximate solution with a
subsampling approach. We make a novel connection between the discrete sampling
and Multi-Armed Bandits problems with a finite reward population and provide
three algorithms with theoretical guarantees. Empirical evaluations show the
robustness and efficiency of the approximate algorithms in both synthetic and
real-world large-scale problems.","['Yutian Chen', 'Zoubin Ghahramani']","['stat.ML', 'cs.LG']",2015-06-30 11:20:45+00:00
http://arxiv.org/abs/1506.09016v2,Online Learning to Sample,"Stochastic Gradient Descent (SGD) is one of the most widely used techniques
for online optimization in machine learning. In this work, we accelerate SGD by
adaptively learning how to sample the most useful training examples at each
time step. First, we show that SGD can be used to learn the best possible
sampling distribution of an importance sampling estimator. Second, we show that
the sampling distribution of an SGD algorithm can be estimated online by
incrementally minimizing the variance of the gradient. The resulting algorithm
- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to
optimize, as well as a set of parameters to sample learning examples. We show
that AWSGD yields faster convergence in three different applications: (i) image
classification with deep features, where the sampling of images depends on
their labels, (ii) matrix factorization, where rows and columns are not sampled
uniformly, and (iii) reinforcement learning, where the optimized and
exploration policies are estimated at the same time, where our approach
corresponds to an off-policy gradient algorithm.","['Guillaume Bouchard', 'Théo Trouillon', 'Julien Perez', 'Adrien Gaidon']","['cs.LG', 'cs.CV', 'cs.NA', 'math.OC', 'stat.ML']",2015-06-30 10:08:35+00:00
http://arxiv.org/abs/1506.08910v1,Learning Single Index Models in High Dimensions,"Single Index Models (SIMs) are simple yet flexible semi-parametric models for
classification and regression. Response variables are modeled as a nonlinear,
monotonic function of a linear combination of features. Estimation in this
context requires learning both the feature weights, and the nonlinear function.
While methods have been described to learn SIMs in the low dimensional regime,
a method that can efficiently learn SIMs in high dimensions has not been
forthcoming. We propose three variants of a computationally and statistically
efficient algorithm for SIM inference in high dimensions. We establish excess
risk bounds for the proposed algorithms and experimentally validate the
advantages that our SIM learning methods provide relative to Generalized Linear
Model (GLM) and low dimensional SIM based learning methods.","['Ravi Ganti', 'Nikhil Rao', 'Rebecca M. Willett', 'Robert Nowak']","['stat.ML', 'cs.LG', 'stat.ME']",2015-06-30 00:45:25+00:00
http://arxiv.org/abs/1506.08858v1,Machine learning for many-body physics: efficient solution of dynamical mean-field theory,"Machine learning methods for solving the equations of dynamical mean-field
theory are developed. The method is demonstrated on the three dimensional
Hubbard model. The key technical issues are defining a mapping of an input
function to an output function, and distinguishing metallic from insulating
solutions. Both metallic and Mott insulator solutions can be predicted. The
validity of the machine learning scheme is assessed by comparing predictions of
full correlation functions, of quasi-particle weight and particle density to
values directly computed. The results indicate that with modest further
development, machine learning approach may be an attractive computational
efficient option for real materials predictions for strongly correlated
systems.","['Louis-François Arsenault', 'O. Anatole von Lilienfeld', 'Andrew J. Millis']","['cond-mat.str-el', 'stat.ML']",2015-06-29 20:36:53+00:00
http://arxiv.org/abs/1506.08826v2,Statistical Inference using the Morse-Smale Complex,"The Morse-Smale complex of a function $f$ decomposes the sample space into
cells where $f$ is increasing or decreasing. When applied to nonparametric
density estimation and regression, it provides a way to represent, visualize,
and compare multivariate functions. In this paper, we present some statistical
results on estimating Morse-Smale complexes. This allows us to derive new
results for two existing methods: mode clustering and Morse-Smale regression.
We also develop two new methods based on the Morse-Smale complex: a
visualization technique for multivariate functions and a two-sample,
multivariate hypothesis test.","['Yen-Chi Chen', 'Christopher R. Genovese', 'Larry Wasserman']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH', '62G20 (Primary), 62G05, 62G08 (Secondary)']",2015-06-29 20:00:40+00:00
http://arxiv.org/abs/1506.08776v2,Bayesian Nonparametric Kernel-Learning,"Kernel methods are ubiquitous tools in machine learning. However, there is
often little reason for the common practice of selecting a kernel a priori.
Even if a universal approximating kernel is selected, the quality of the finite
sample estimator may be greatly affected by the choice of kernel. Furthermore,
when directly applying kernel methods, one typically needs to compute a $N
\times N$ Gram matrix of pairwise kernel evaluations to work with a dataset of
$N$ instances. The computation of this Gram matrix precludes the direct
application of kernel methods on large datasets, and makes kernel learning
especially difficult. In this paper we introduce Bayesian nonparmetric
kernel-learning (BaNK), a generic, data-driven framework for scalable learning
of kernels. BaNK places a nonparametric prior on the spectral distribution of
random frequencies allowing it to both learn kernels and scale to large
datasets. We show that this framework can be used for large scale regression
and classification tasks. Furthermore, we show that BaNK outperforms several
other scalable approaches for kernel learning on a variety of real world
datasets.","['Junier Oliva', 'Avinava Dubey', 'Andrew G. Wilson', 'Barnabas Poczos', 'Jeff Schneider', 'Eric P. Xing']",['stat.ML'],2015-06-29 18:48:47+00:00
http://arxiv.org/abs/1506.08760v1,S2: An Efficient Graph Based Active Learning Algorithm with Application to Nonparametric Classification,"This paper investigates the problem of active learning for binary label
prediction on a graph. We introduce a simple and label-efficient algorithm
called S2 for this task. At each step, S2 selects the vertex to be labeled
based on the structure of the graph and all previously gathered labels.
Specifically, S2 queries for the label of the vertex that bisects the *shortest
shortest* path between any pair of oppositely labeled vertices. We present a
theoretical estimate of the number of queries S2 needs in terms of a novel
parametrization of the complexity of binary functions on graphs. We also
present experimental results demonstrating the performance of S2 on both real
and synthetic data. While other graph-based active learning algorithms have
shown promise in practice, our algorithm is the first with both good
performance and theoretical guarantees. Finally, we demonstrate the
implications of the S2 algorithm to the theory of nonparametric active
learning. In particular, we show that S2 achieves near minimax optimal excess
risk for an important class of nonparametric classification problems.","['Gautam Dasarathy', 'Robert Nowak', 'Xiaojin Zhu']","['cs.LG', 'stat.ML']",2015-06-29 18:03:25+00:00
http://arxiv.org/abs/1506.08700v4,Dropout as data augmentation,"Dropout is typically interpreted as bagging a large number of models sharing
parameters. We show that using dropout in a network can also be interpreted as
a kind of data augmentation in the input space without domain knowledge. We
present an approach to projecting the dropout noise within a network back into
the input space, thereby generating augmented versions of the training data,
and we show that training a deterministic network on the augmented samples
yields similar results. Finally, we propose a new dropout noise scheme based on
our observations and show that it improves dropout results without adding
significant computational cost.","['Xavier Bouthillier', 'Kishore Konda', 'Pascal Vincent', 'Roland Memisevic']","['stat.ML', 'cs.LG']",2015-06-29 15:55:45+00:00
http://arxiv.org/abs/1506.08690v1,Portfolio optimization using local linear regression ensembles in RapidMiner,"In this paper we implement a Local Linear Regression Ensemble Committee
(LOLREC) to predict 1-day-ahead returns of 453 assets form the S&P500. The
estimates and the historical returns of the committees are used to compute the
weights of the portfolio from the 453 stock. The proposed method outperforms
benchmark portfolio selection strategies that optimize the growth rate of the
capital. We investigate the effect of algorithm parameter m: the number of
selected stocks on achieved average annual yields. Results suggest the
algorithm's practical usefulness in everyday trading.","['Gabor Nagy', 'Gergo Barta', 'Tamas Henk']","['q-fin.PM', 'cs.LG', 'stat.ML']",2015-06-29 15:42:39+00:00
http://arxiv.org/abs/1506.08669v3,Efficient and Parsimonious Agnostic Active Learning,"We develop a new active learning algorithm for the streaming setting
satisfying three important properties: 1) It provably works for any classifier
representation and classification problem including those with severe noise. 2)
It is efficiently implementable with an ERM oracle. 3) It is more aggressive
than all previous approaches satisfying 1 and 2. To do this we create an
algorithm based on a newly defined optimization problem and analyze it. We also
conduct the first experimental analysis of all efficient agnostic active
learning algorithms, evaluating their strengths and weaknesses in different
settings.","['Tzu-Kuo Huang', 'Alekh Agarwal', 'Daniel J. Hsu', 'John Langford', 'Robert E. Schapire']","['cs.LG', 'stat.ML']",2015-06-29 15:02:55+00:00
http://arxiv.org/abs/1506.08621v3,A spectral method for community detection in moderately-sparse degree-corrected stochastic block models,"We consider community detection in Degree-Corrected Stochastic Block Models
(DC-SBM). We propose a spectral clustering algorithm based on a suitably
normalized adjacency matrix. We show that this algorithm consistently recovers
the block-membership of all but a vanishing fraction of nodes, in the regime
where the lowest degree is of order log$(n)$ or higher. Recovery succeeds even
for very heterogeneous degree-distributions. The used algorithm does not rely
on parameters as input. In particular, it does not need to know the number of
communities.","['Lennart Gulikers', 'Marc Lelarge', 'Laurent Massoulié']","['math.PR', 'cs.LG', 'cs.SI', 'stat.ML']",2015-06-29 13:44:54+00:00
http://arxiv.org/abs/1506.08544v2,Exact and approximate inference in graphical models: variable elimination and beyond,"Probabilistic graphical models offer a powerful framework to account for the
dependence structure between variables, which is represented as a graph.
However, the dependence between variables may render inference tasks
intractable. In this paper we review techniques exploiting the graph structure
for exact inference, borrowed from optimisation and computer science. They are
built on the principle of variable elimination whose complexity is dictated in
an intricate way by the order in which variables are eliminated. The so-called
treewidth of the graph characterises this algorithmic complexity: low-treewidth
graphs can be processed efficiently. The first message that we illustrate is
therefore the idea that for inference in graphical model, the number of
variables is not the limiting factor, and it is worth checking for the
treewidth before turning to approximate methods. We show how algorithms
providing an upper bound of the treewidth can be exploited to derive a 'good'
elimination order enabling to perform exact inference. The second message is
that when the treewidth is too large, algorithms for approximate inference
linked to the principle of variable elimination, such as loopy belief
propagation and variational approaches, can lead to accurate results while
being much less time consuming than Monte-Carlo approaches. We illustrate the
techniques reviewed in this article on benchmarks of inference problems in
genetic linkage analysis and computer vision, as well as on hidden variables
restoration in coupled Hidden Markov Models.","['Nathalie Peyrard', 'Marie-Josée Cros', 'Simon de Givry', 'Alain Franc', 'Stéphane Robin', 'Régis Sabbadin', 'Thomas Schiex', 'Matthieu Vignes']","['stat.ML', 'cs.AI', 'cs.LG']",2015-06-29 08:45:11+00:00
http://arxiv.org/abs/1506.08536v3,A simple yet efficient algorithm for multiple kernel learning under elastic-net constraints,"This papers introduces an algorithm for the solution of multiple kernel
learning (MKL) problems with elastic-net constraints on the kernel weights. The
algorithm compares very favourably in terms of time and space complexity to
existing approaches and can be implemented with simple code that does not rely
on external libraries (except a conventional SVM solver).",['Luca Citi'],"['stat.ML', 'cs.LG']",2015-06-29 08:11:52+00:00
http://arxiv.org/abs/1506.08511v1,Integrative analysis of gene expression and phenotype data,"The linking genotype to phenotype is the fundamental aim of modern genetics.
We focus on study of links between gene expression data and phenotype data
through integrative analysis. We propose three approaches.
  1) The inherent complexity of phenotypes makes high-throughput phenotype
profiling a very difficult and laborious process. We propose a method of
automated multi-dimensional profiling which uses gene expression similarity.
Large-scale analysis show that our method can provide robust profiling that
reveals different phenotypic aspects of samples. This profiling technique is
also capable of interpolation and extrapolation beyond the phenotype
information given in training data. It can be used in many applications,
including facilitating experimental design and detecting confounding factors.
  2) Phenotype association analysis problems are complicated by small sample
size and high dimensionality. Consequently, phenotype-associated gene subsets
obtained from training data are very sensitive to selection of training
samples, and the constructed sample phenotype classifiers tend to have poor
generalization properties. To eliminate these obstacles, we propose a novel
approach that generates sequences of increasingly discriminative gene cluster
combinations. Our experiments on both simulated and real datasets show robust
and accurate classification performance.
  3) Many complex phenotypes, such as cancer, are the product of not only gene
expression, but also gene interaction. We propose an integrative approach to
find gene network modules that activate under different phenotype conditions.
Using our method, we discovered cancer subtype-specific network modules, as
well as the ways in which these modules coordinate. In particular, we detected
a breast-cancer specific tumor suppressor network module with a hub gene,
PDGFRL, which may play an important role in this module.",['Min Xu'],"['q-bio.QM', 'q-bio.GN', 'q-bio.MN', 'stat.ML']",2015-06-29 05:26:29+00:00
http://arxiv.org/abs/1506.08499v1,Compressed Sensing of Multi-Channel EEG Signals: The Simultaneous Cosparsity and Low Rank Optimization,"Goal: This paper deals with the problems that some EEG signals have no good
sparse representation and single channel processing is not computationally
efficient in compressed sensing of multi-channel EEG signals. Methods: An
optimization model with L0 norm and Schatten-0 norm is proposed to enforce
cosparsity and low rank structures in the reconstructed multi-channel EEG
signals. Both convex relaxation and global consensus optimization with
alternating direction method of multipliers are used to compute the
optimization model. Results: The performance of multi-channel EEG signal
reconstruction is improved in term of both accuracy and computational
complexity. Conclusion: The proposed method is a better candidate than previous
sparse signal recovery methods for compressed sensing of EEG signals.
Significance: The proposed method enables successful compressed sensing of EEG
signals even when the signals have no good sparse representation. Using
compressed sensing would much reduce the power consumption of wireless EEG
system.","['Yipeng Liu', 'Maarten De Vos', 'Sabine Van Huffel']","['cs.IT', 'math.IT', 'stat.ML']",2015-06-29 03:51:14+00:00
http://arxiv.org/abs/1506.08473v3,Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods,"Training neural networks is a challenging non-convex optimization problem,
and backpropagation or gradient descent can get stuck in spurious local optima.
We propose a novel algorithm based on tensor decomposition for guaranteed
training of two-layer neural networks. We provide risk bounds for our proposed
method, with a polynomial sample complexity in the relevant parameters, such as
input dimension and number of neurons. While learning arbitrary target
functions is NP-hard, we provide transparent conditions on the function and the
input for learnability. Our training method is based on tensor decomposition,
which provably converges to the global optimum, under a set of mild
non-degeneracy conditions. It consists of simple embarrassingly parallel linear
and multi-linear operations, and is competitive with standard stochastic
gradient descent (SGD), in terms of computational complexity. Thus, we propose
a computationally efficient method with guaranteed risk bounds for training
neural networks with one hidden layer.","['Majid Janzamin', 'Hanie Sedghi', 'Anima Anandkumar']","['cs.LG', 'cs.NE', 'stat.ML']",2015-06-28 23:19:49+00:00
http://arxiv.org/abs/1506.08448v4,Neural Simpletrons - Minimalistic Directed Generative Networks for Learning with Few Labels,"Classifiers for the semi-supervised setting often combine strong supervised
models with additional learning objectives to make use of unlabeled data. This
results in powerful though very complex models that are hard to train and that
demand additional labels for optimal parameter tuning, which are often not
given when labeled data is very sparse. We here study a minimalistic
multi-layer generative neural network for semi-supervised learning in a form
and setting as similar to standard discriminative networks as possible. Based
on normalized Poisson mixtures, we derive compact and local learning and neural
activation rules. Learning and inference in the network can be scaled using
standard deep learning tools for parallelized GPU implementation. With the
single objective of likelihood optimization, both labeled and unlabeled data
are naturally incorporated into learning. Empirical evaluations on standard
benchmarks show, that for datasets with few labels the derived minimalistic
network improves on all classical deep learning approaches and is competitive
with their recent variants without the need of additional labels for parameter
tuning. Furthermore, we find that the studied network is the best performing
monolithic (`non-hybrid') system for few labels, and that it can be applied in
the limit of very few labels, where no other system has been reported to
operate so far.","['Dennis Forster', 'Abdul-Saboor Sheikh', 'Jörg Lücke']","['stat.ML', 'cs.LG']",2015-06-28 20:25:15+00:00
http://arxiv.org/abs/1506.08387v2,Robustness Analysis of Preconditioned Successive Projection Algorithm for General Form of Separable NMF Problem,"The successive projection algorithm (SPA) has been known to work well for
separable nonnegative matrix factorization (NMF) problems arising in
applications, such as topic extraction from documents and endmember detection
in hyperspectral images. One of the reasons is in that the algorithm is robust
to noise. Gillis and Vavasis showed in [SIAM J. Optim., 25(1), pp. 677-698,
2015] that a preconditioner can further enhance its noise robustness. The proof
rested on the condition that the dimension $d$ and factorization rank $r$ in
the separable NMF problem coincide with each other. However, it may be
unrealistic to expect that the condition holds in separable NMF problems
appearing in actual applications; in such problems, $d$ is usually greater than
$r$. This paper shows, without the condition $d=r$, that the preconditioned SPA
is robust to noise.",['Tomohiko Mizutani'],"['stat.ML', 'math.OC']",2015-06-28 12:10:27+00:00
http://arxiv.org/abs/1506.08301v2,A Novel Approach for Stable Selection of Informative Redundant Features from High Dimensional fMRI Data,"Feature selection is among the most important components because it not only
helps enhance the classification accuracy, but also or even more important
provides potential biomarker discovery. However, traditional multivariate
methods is likely to obtain unstable and unreliable results in case of an
extremely high dimensional feature space and very limited training samples,
where the features are often correlated or redundant. In order to improve the
stability, generalization and interpretations of the discovered potential
biomarker and enhance the robustness of the resultant classifier, the redundant
but informative features need to be also selected. Therefore we introduced a
novel feature selection method which combines a recent implementation of the
stability selection approach and the elastic net approach. The advantage in
terms of better control of false discoveries and missed discoveries of our
approach, and the resulted better interpretability of the obtained potential
biomarker is verified in both synthetic and real fMRI experiments. In addition,
we are among the first to demonstrate the robustness of feature selection
benefiting from the incorporation of stability selection and also among the
first to demonstrate the possible unrobustness of the classical univariate
two-sample t-test method. Specifically, we show the robustness of our feature
selection results in existence of noisy (wrong) training labels, as well as the
robustness of the resulted classifier based on our feature selection results in
the existence of data variation, demonstrated by a multi-center
attention-deficit/hyperactivity disorder (ADHD) fMRI data.","['Yilun Wang', 'Zhiqiang Li', 'Yifeng Wang', 'Xiaona Wang', 'Junjie Zheng', 'Xujuan Duan', 'Huafu Chen']","['cs.CV', 'cs.LG', 'stat.ML', 'I.5.2']",2015-06-27 15:28:31+00:00
http://arxiv.org/abs/1506.08272v5,Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization,"Asynchronous parallel implementations of stochastic gradient (SG) have been
broadly used in solving deep neural network and received many successes in
practice recently. However, existing theories cannot explain their convergence
and speedup properties, mainly due to the nonconvexity of most deep learning
formulations and the asynchronous parallel mechanism. To fill the gaps in
theory and provide theoretical supports, this paper studies two asynchronous
parallel implementations of SG: one is on the computer network and the other is
on the shared memory system. We establish an ergodic convergence rate
$O(1/\sqrt{K})$ for both algorithms and prove that the linear speedup is
achievable if the number of workers is bounded by $\sqrt{K}$ ($K$ is the total
number of iterations). Our results generalize and improve existing analysis for
convex minimization.","['Xiangru Lian', 'Yijun Huang', 'Yuncheng Li', 'Ji Liu']","['math.OC', 'cs.NA', 'stat.ML']",2015-06-27 08:41:50+00:00
http://arxiv.org/abs/1506.08180v1,An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process,"Stochastic variational inference (SVI) is emerging as the most promising
candidate for scaling inference in Bayesian probabilistic models to large
datasets. However, the performance of these methods has been assessed primarily
in the context of Bayesian topic models, particularly latent Dirichlet
allocation (LDA). Deriving several new algorithms, and using synthetic, image
and genomic datasets, we investigate whether the understanding gleaned from LDA
applies in the setting of sparse latent factor models, specifically beta
process factor analysis (BPFA). We demonstrate that the big picture is
consistent: using Gibbs sampling within SVI to maintain certain posterior
dependencies is extremely effective. However, we find that different posterior
dependencies are important in BPFA relative to LDA. Particularly,
approximations able to model intra-local variable dependence perform best.","['Amar Shah', 'David A. Knowles', 'Zoubin Ghahramani']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.CO', 'stat.ME']",2015-06-26 18:55:11+00:00
http://arxiv.org/abs/1506.08170v1,Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis,"Canonical Correlation Analysis (CCA) is a widely used spectral technique for
finding correlation structures in multi-view datasets. In this paper, we tackle
the problem of large scale CCA, where classical algorithms, usually requiring
computing the product of two huge matrices and huge matrix decomposition, are
computationally and storage expensive. We recast CCA from a novel perspective
and propose a scalable and memory efficient Augmented Approximate Gradient
(AppGrad) scheme for finding top $k$ dimensional canonical subspace which only
involves large matrix multiplying a thin matrix of width $k$ and small matrix
decomposition of dimension $k\times k$. Further, AppGrad achieves optimal
storage complexity $O(k(p_1+p_2))$, compared with classical algorithms which
usually require $O(p_1^2+p_2^2)$ space to store two dense whitening matrices.
The proposed scheme naturally generalizes to stochastic optimization regime,
especially efficient for huge datasets where batch algorithms are prohibitive.
The online property of stochastic AppGrad is also well suited to the streaming
scenario, where data comes sequentially. To the best of our knowledge, it is
the first stochastic algorithm for CCA. Experiments on four real data sets are
provided to show the effectiveness of the proposed methods.","['Zhuang Ma', 'Yichao Lu', 'Dean Foster']","['stat.ML', 'stat.CO']",2015-06-26 17:51:57+00:00
http://arxiv.org/abs/1506.08126v1,Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest,"The New Yorker publishes a weekly captionless cartoon. More than 5,000
readers submit captions for it. The editors select three of them and ask the
readers to pick the funniest one. We describe an experiment that compares a
dozen automatic methods for selecting the funniest caption. We show that
negative sentiment, human-centeredness, and lexical centrality most strongly
match the funniest captions, followed by positive sentiment. These results are
useful for understanding humor and also in the design of more engaging
conversational agents in text and multimodal (vision+text) systems. As part of
this work, a large set of cartoons and captions is being made available to the
community.","['Dragomir Radev', 'Amanda Stent', 'Joel Tetreault', 'Aasish Pappu', 'Aikaterini Iliakopoulou', 'Agustin Chanfreau', 'Paloma de Juan', 'Jordi Vallmitjana', 'Alejandro Jaimes', 'Rahul Jha', 'Bob Mankoff']","['cs.CL', 'cs.AI', 'cs.MM', 'stat.ML']",2015-06-26 15:48:10+00:00
http://arxiv.org/abs/1506.08105v1,Modelling of directional data using Kent distributions,"The modelling of data on a spherical surface requires the consideration of
directional probability distributions. To model asymmetrically distributed data
on a three-dimensional sphere, Kent distributions are often used. The moment
estimates of the parameters are typically used in modelling tasks involving
Kent distributions. However, these lack a rigorous statistical treatment. The
focus of the paper is to introduce a Bayesian estimation of the parameters of
the Kent distribution which has not been carried out in the literature, partly
because of its complex mathematical form. We employ the Bayesian
information-theoretic paradigm of Minimum Message Length (MML) to bridge this
gap and derive reliable estimators. The inferred parameters are subsequently
used in mixture modelling of Kent distributions. The problem of inferring the
suitable number of mixture components is also addressed using the MML
criterion. We demonstrate the superior performance of the derived MML-based
parameter estimates against the traditional estimators. We apply the MML
principle to infer mixtures of Kent distributions to model empirical data
corresponding to protein conformations. We demonstrate the effectiveness of
Kent models to act as improved descriptors of protein structural data as
compared to commonly used von Mises-Fisher distributions.",['Parthan Kasarapu'],"['cs.LG', 'stat.ML']",2015-06-26 15:03:33+00:00
http://arxiv.org/abs/1506.08009v4,Skopus: Mining top-k sequential patterns under leverage,"This paper presents a framework for exact discovery of the top-k sequential
patterns under Leverage. It combines (1) a novel definition of the expected
support for a sequential pattern - a concept on which most interestingness
measures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for
the exact discovery of top-k sequential patterns under a given measure of
interest. Our interestingness measure employs the partition approach. A pattern
is interesting to the extent that it is more frequent than can be explained by
assuming independence between any of the pairs of patterns from which it can be
composed. The larger the support compared to the expectation under
independence, the more interesting is the pattern. We build on these two
elements to exactly extract the k sequential patterns with highest leverage,
consistent with our definition of expected support. We conduct experiments on
both synthetic data with known patterns and real-world datasets; both
experiments confirm the consistency and relevance of our approach with regard
to the state of the art. This article was published in Data Mining and
Knowledge Discovery and is accessible at
http://dx.doi.org/10.1007/s10618-016-0467-9.","['Francois Petitjean', 'Tao Li', 'Nikolaj Tatti', 'Geoffrey I. Webb']","['cs.AI', 'cs.LG', 'stat.ML']",2015-06-26 09:36:10+00:00
http://arxiv.org/abs/1506.08002v1,Safe Feature Pruning for Sparse High-Order Interaction Models,"Taking into account high-order interactions among covariates is valuable in
many practical regression problems. This is, however, computationally
challenging task because the number of high-order interaction features to be
considered would be extremely large unless the number of covariates is
sufficiently small. In this paper, we propose a novel efficient algorithm for
LASSO-based sparse learning of such high-order interaction models. Our basic
strategy for reducing the number of features is to employ the idea of recently
proposed safe feature screening (SFS) rule. An SFS rule has a property that, if
a feature satisfies the rule, then the feature is guaranteed to be non-active
in the LASSO solution, meaning that it can be safely screened-out prior to the
LASSO training process. If a large number of features can be screened-out
before training the LASSO, the computational cost and the memory requirment can
be dramatically reduced. However, applying such an SFS rule to each of the
extremely large number of high-order interaction features would be
computationally infeasible. Our key idea for solving this computational issue
is to exploit the underlying tree structure among high-order interaction
features. Specifically, we introduce a pruning condition called safe feature
pruning (SFP) rule which has a property that, if the rule is satisfied in a
certain node of the tree, then all the high-order interaction features
corresponding to its descendant nodes can be guaranteed to be non-active at the
optimal solution. Our algorithm is extremely efficient, making it possible to
work, e.g., with 3rd order interactions of 10,000 original covariates, where
the number of possible high-order interaction features is greater than 10^{12}.","['Kazuya Nakagawa', 'Shinya Suzumura', 'Masayuki Karasuyama', 'Koji Tsuda', 'Ichiro Takeuchi']",['stat.ML'],2015-06-26 09:08:26+00:00
http://arxiv.org/abs/1506.07997v1,An Efficient Post-Selection Inference on High-Order Interaction Models,"Finding statistically significant high-order interaction features in
predictive modeling is important but challenging task. The difficulty lies in
the fact that, for a recent applications with high-dimensional covariates, the
number of possible high-order interaction features would be extremely large.
Identifying statistically significant features from such a huge pool of
candidates would be highly challenging both in computational and statistical
senses. To work with this problem, we consider a two stage algorithm where we
first select a set of high-order interaction features by marginal screening,
and then make statistical inferences on the regression model fitted only with
the selected features. Such statistical inferences are called post-selection
inference (PSI), and receiving an increasing attention in the literature. One
of the seminal recent advancements in PSI literature is the works by Lee et al.
where the authors presented an algorithmic framework for computing exact
sampling distributions in PSI. A main challenge when applying their approach to
our high-order interaction models is to cope with the fact that PSI in general
depends not only on the selected features but also on the unselected features,
making it hard to apply to our extremely high-dimensional high-order
interaction models. The goal of this paper is to overcome this difficulty by
introducing a novel efficient method for PSI. Our key idea is to exploit the
underlying tree structure among high-order interaction features, and to develop
a pruning method of the tree which enables us to quickly identify a group of
unselected features that are guaranteed to have no influence on PSI. The
experimental results indicate that the proposed method allows us to reliably
identify statistically significant high-order interaction features with
reasonable computational cost.","['S. Suzumura', 'K. Nakagawa', 'K. Tsuda', 'I. Takeuchi']",['stat.ML'],2015-06-26 08:52:37+00:00
http://arxiv.org/abs/1506.07959v1,Factorized Asymptotic Bayesian Inference for Factorial Hidden Markov Models,"Factorial hidden Markov models (FHMMs) are powerful tools of modeling
sequential data. Learning FHMMs yields a challenging simultaneous model
selection issue, i.e., selecting the number of multiple Markov chains and the
dimensionality of each chain. Our main contribution is to address this model
selection issue by extending Factorized Asymptotic Bayesian (FAB) inference to
FHMMs. First, we offer a better approximation of marginal log-likelihood than
the previous FAB inference. Our key idea is to integrate out transition
probabilities, yet still apply the Laplace approximation to emission
probabilities. Second, we prove that if there are two very similar hidden
states in an FHMM, i.e. one is redundant, then FAB will almost surely shrink
and eliminate one of them, making the model parsimonious. Experimental results
show that FAB for FHMMs significantly outperforms state-of-the-art
nonparametric Bayesian iFHMM and Variational FHMM in model selection accuracy,
with competitive held-out perplexity.","['Shaohua Li', 'Ryohei Fujimaki', 'Chunyan Miao']",['stat.ML'],2015-06-26 05:24:30+00:00
http://arxiv.org/abs/1506.07947v1,Collaboratively Learning Preferences from Ordinal Data,"In applications such as recommendation systems and revenue management, it is
important to predict preferences on items that have not been seen by a user or
predict outcomes of comparisons among those that have never been compared. A
popular discrete choice model of multinomial logit model captures the structure
of the hidden preferences with a low-rank matrix. In order to predict the
preferences, we want to learn the underlying model from noisy observations of
the low-rank matrix, collected as revealed preferences in various forms of
ordinal data. A natural approach to learn such a model is to solve a convex
relaxation of nuclear norm minimization. We present the convex relaxation
approach in two contexts of interest: collaborative ranking and bundled choice
modeling. In both cases, we show that the convex relaxation is minimax optimal.
We prove an upper bound on the resulting error with finite samples, and provide
a matching information-theoretic lower bound.","['Sewoong Oh', 'Kiran K. Thekumparampil', 'Jiaming Xu']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2015-06-26 03:06:27+00:00
http://arxiv.org/abs/1506.07944v2,Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric,"Given a family of probability measures in P(X), the space of probability
measures on a Hilbert space X, our goal in this paper is to highlight one ore
more curves in P(X) that summarize efficiently that family. We propose to study
this problem under the optimal transport (Wasserstein) geometry, using curves
that are restricted to be geodesic segments under that metric. We show that
concepts that play a key role in Euclidean PCA, such as data centering or
orthogonality of principal directions, find a natural equivalent in the optimal
transport geometry, using Wasserstein means and differential geometry. The
implementation of these ideas is, however, computationally challenging. To
achieve scalable algorithms that can handle thousands of measures, we propose
to use a relaxed definition for geodesics and regularized optimal transport
distances. The interest of our approach is demonstrated on images seen either
as shapes or color histograms.","['Vivien Seguy', 'Marco Cuturi']",['stat.ML'],2015-06-26 02:37:26+00:00
http://arxiv.org/abs/1506.07930v1,Clustering categorical data via ensembling dissimilarity matrices,"We present a technique for clustering categorical data by generating many
dissimilarity matrices and averaging over them. We begin by demonstrating our
technique on low dimensional categorical data and comparing it to several other
techniques that have been proposed. Then we give conditions under which our
method should yield good results in general. Our method extends to high
dimensional categorical data of equal lengths by ensembling over many choices
of explanatory variables. In this context we compare our method with two other
methods. Finally, we extend our method to high dimensional categorical data
vectors of unequal length by using alignment techniques to equalize the
lengths. We give examples to show that our method continues to provide good
results, in particular, better in the context of genome sequences than
clusterings suggested by phylogenetic trees.","['Saeid Amiri', 'Bertrand Clarke', 'Jennifer Clarke']",['stat.ML'],2015-06-26 00:48:17+00:00
http://arxiv.org/abs/1506.07925v1,Analyzing statistical and computational tradeoffs of estimation procedures,"The recent explosion in the amount and dimensionality of data has exacerbated
the need of trading off computational and statistical efficiency carefully, so
that inference is both tractable and meaningful. We propose a framework that
provides an explicit opportunity for practitioners to specify how much
statistical risk they are willing to accept for a given computational cost, and
leads to a theoretical risk-computation frontier for any given inference
problem. We illustrate the tradeoff between risk and computation and illustrate
the frontier in three distinct settings. First, we derive analytic forms for
the risk of estimating parameters in the classical setting of estimating the
mean and variance for normally distributed data and for the more general
setting of parameters of an exponential family. The second example concentrates
on computationally constrained Hodges-Lehmann estimators. We conclude with an
evaluation of risk associated with early termination of iterative matrix
inversion algorithms in the context of linear regression.","['Daniel L. Sussman', 'Alexander Volfovsky', 'Edoardo M. Airoldi']","['stat.CO', 'stat.ML']",2015-06-25 23:57:34+00:00
http://arxiv.org/abs/1506.07902v2,Minimax Structured Normal Means Inference,"We provide a unified treatment of a broad class of noisy structure recovery
problems, known as structured normal means problems. In this setting, the goal
is to identify, from a finite collection of Gaussian distributions with
different means, the distribution that produced some observed data. Recent work
has studied several special cases including sparse vectors, biclusters, and
graph-based structures. We establish nearly matching upper and lower bounds on
the minimax probability of error for any structured normal means problem, and
we derive an optimality certificate for the maximum likelihood estimator, which
can be applied to many instantiations. We also consider an experimental design
setting, where we generalize our minimax bounds and derive an algorithm for
computing a design strategy with a certain optimality property. We show that
our results give tight minimax bounds for many structure recovery problems and
consider some consequences for interactive sampling.",['Akshay Krishnamurthy'],"['stat.ML', 'cs.IT', 'math.IT']",2015-06-25 21:35:21+00:00
http://arxiv.org/abs/1506.07868v5,The local convexity of solving systems of quadratic equations,"This paper considers the recovery of a rank $r$ positive semidefinite matrix
$X X^T\in\mathbb{R}^{n\times n}$ from $m$ scalar measurements of the form $y_i
:= a_i^T X X^T a_i$ (i.e., quadratic measurements of $X$). Such problems arise
in a variety of applications, including covariance sketching of
high-dimensional data streams, quadratic regression, quantum state tomography,
among others. A natural approach to this problem is to minimize the loss
function $f(U) = \sum_i (y_i - a_i^TUU^Ta_i)^2$ which has an entire manifold of
solutions given by $\{XO\}_{O\in\mathcal{O}_r}$ where $\mathcal{O}_r$ is the
orthogonal group of $r\times r$ orthogonal matrices; this is {\it non-convex}
in the $n\times r$ matrix $U$, but methods like gradient descent are simple and
easy to implement (as compared to semidefinite relaxation approaches).
  In this paper we show that once we have $m \geq C nr \log^2(n)$ samples from
isotropic gaussian $a_i$, with high probability {\em (a)} this function admits
a dimension-independent region of {\em local strong convexity} on lines
perpendicular to the solution manifold, and {\em (b)} with an additional
polynomial factor of $r$ samples, a simple spectral initialization will land
within the region of convexity with high probability. Together, this implies
that gradient descent with initialization (but no re-sampling) will converge
linearly to the correct $X$, up to an orthogonal transformation. We believe
that this general technique (local convexity reachable by spectral
initialization) should prove applicable to a broader class of nonconvex
optimization problems.","['Chris D. White', 'Sujay Sanghavi', 'Rachel Ward']","['math.NA', 'math.OC', 'stat.ML']",2015-06-25 19:44:51+00:00
http://arxiv.org/abs/1506.07840v1,Diffusion Nets,"Non-linear manifold learning enables high-dimensional data analysis, but
requires out-of-sample-extension methods to process new data points. In this
paper, we propose a manifold learning algorithm based on deep learning to
create an encoder, which maps a high-dimensional dataset and its
low-dimensional embedding, and a decoder, which takes the embedded data back to
the high-dimensional space. Stacking the encoder and decoder together
constructs an autoencoder, which we term a diffusion net, that performs
out-of-sample-extension as well as outlier detection. We introduce new neural
net constraints for the encoder, which preserves the local geometry of the
points, and we prove rates of convergence for the encoder. Also, our approach
is efficient in both computational complexity and memory requirements, as
opposed to previous methods that require storage of all training points in both
the high-dimensional and the low-dimensional spaces to calculate the
out-of-sample-extension and the pre-image.","['Gal Mishne', 'Uri Shaham', 'Alexander Cloninger', 'Israel Cohen']","['stat.ML', 'cs.LG', 'math.CA']",2015-06-25 18:13:49+00:00
http://arxiv.org/abs/1506.07721v1,Fairness-Aware Learning with Restriction of Universal Dependency using f-Divergences,"Fairness-aware learning is a novel framework for classification tasks. Like
regular empirical risk minimization (ERM), it aims to learn a classifier with a
low error rate, and at the same time, for the predictions of the classifier to
be independent of sensitive features, such as gender, religion, race, and
ethnicity. Existing methods can achieve low dependencies on given samples, but
this is not guaranteed on unseen samples. The existing fairness-aware learning
algorithms employ different dependency measures, and each algorithm is
specifically designed for a particular one. Such diversity makes it difficult
to theoretically analyze and compare them. In this paper, we propose a general
framework for fairness-aware learning that uses f-divergences and that covers
most of the dependency measures employed in the existing methods. We introduce
a way to estimate the f-divergences that allows us to give a unified analysis
for the upper bound of the estimation error; this bound is tighter than that of
the existing convergence rate analysis of the divergence estimation. With our
divergence estimate, we propose a fairness-aware learning algorithm, and
perform a theoretical analysis of its generalization error. Our analysis
reveals that, under mild assumptions and even with enforcement of fairness, the
generalization error of our method is $O(\sqrt{1/n})$, which is the same as
that of the regular ERM. In addition, and more importantly, we show that, for
any f-divergence, the upper bound of the estimation error of the divergence is
$O(\sqrt{1/n})$. This indicates that our fairness-aware learning algorithm
guarantees low dependencies on unseen samples for any dependency measure
represented by an f-divergence.","['Kazuto Fukuchi', 'Jun Sakuma']","['stat.ML', 'cs.LG']",2015-06-25 12:24:43+00:00
http://arxiv.org/abs/1506.07677v1,Manifold Optimization for Gaussian Mixture Models,"We take a new look at parameter estimation for Gaussian Mixture Models
(GMMs). In particular, we propose using \emph{Riemannian manifold optimization}
as a powerful counterpart to Expectation Maximization (EM). An out-of-the-box
invocation of manifold optimization, however, fails spectacularly: it converges
to the same solution but vastly slower. Driven by intuition from manifold
convexity, we then propose a reparamerization that has remarkable empirical
consequences. It makes manifold optimization not only match EM---a highly
encouraging result in itself given the poor record nonlinear programming
methods have had against EM so far---but also outperform EM in many practical
settings, while displaying much less variability in running times. We further
highlight the strengths of manifold optimization by developing a somewhat tuned
manifold LBFGS method that proves even more competitive and reliable than
existing manifold optimization tools. We hope that our results encourage a
wider consideration of manifold optimization for parameter estimation problems.","['Reshad Hosseini', 'Suvrit Sra']","['stat.ML', 'cs.LG', 'math.OC']",2015-06-25 09:40:51+00:00
http://arxiv.org/abs/1506.07615v2,Completing Low-Rank Matrices with Corrupted Samples from Few Coefficients in General Basis,"Subspace recovery from corrupted and missing data is crucial for various
applications in signal processing and information theory. To complete missing
values and detect column corruptions, existing robust Matrix Completion (MC)
methods mostly concentrate on recovering a low-rank matrix from few corrupted
coefficients w.r.t. standard basis, which, however, does not apply to more
general basis, e.g., Fourier basis. In this paper, we prove that the range
space of an $m\times n$ matrix with rank $r$ can be exactly recovered from few
coefficients w.r.t. general basis, though $r$ and the number of corrupted
samples are both as high as $O(\min\{m,n\}/\log^3 (m+n))$. Our model covers
previous ones as special cases, and robust MC can recover the intrinsic matrix
with a higher rank. Moreover, we suggest a universal choice of the
regularization parameter, which is $\lambda=1/\sqrt{\log n}$. By our
$\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we can
further reduce the computational cost of our model. As an application, we also
find that the solutions to extended robust Low-Rank Representation and to our
extended robust MC are mutually expressible, so both our theory and algorithm
can be applied to the subspace clustering problem with missing values under
certain conditions. Experiments verify our theories.","['Hongyang Zhang', 'Zhouchen Lin', 'Chao Zhang']","['cs.IT', 'cs.LG', 'cs.NA', 'math.IT', 'math.NA', 'stat.ML', '68T05', 'G.1.6; K.3.2']",2015-06-25 05:11:44+00:00
http://arxiv.org/abs/1506.07613v3,Generalized Majorization-Minimization,"Non-convex optimization is ubiquitous in machine learning.
Majorization-Minimization (MM) is a powerful iterative procedure for optimizing
non-convex functions that works by optimizing a sequence of bounds on the
function. In MM, the bound at each iteration is required to \emph{touch} the
objective function at the optimizer of the previous bound. We show that this
touching constraint is unnecessary and overly restrictive. We generalize MM by
relaxing this constraint, and propose a new optimization framework, named
Generalized Majorization-Minimization (G-MM), that is more flexible. For
instance, G-MM can incorporate application-specific biases into the
optimization procedure without changing the objective function. We derive G-MM
algorithms for several latent variable models and show empirically that they
consistently outperform their MM counterparts in optimizing non-convex
objectives. In particular, G-MM algorithms appear to be less sensitive to
initialization.","['Sobhan Naderi Parizi', 'Kun He', 'Reza Aghajani', 'Stan Sclaroff', 'Pedro Felzenszwalb']","['cs.CV', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML']",2015-06-25 04:56:50+00:00
http://arxiv.org/abs/1506.07611v1,Joint community and anomaly tracking in dynamic networks,"Most real-world networks exhibit community structure, a phenomenon
characterized by existence of node clusters whose intra-edge connectivity is
stronger than edge connectivities between nodes belonging to different
clusters. In addition to facilitating a better understanding of network
behavior, community detection finds many practical applications in diverse
settings. Communities in online social networks are indicative of shared
functional roles, or affiliation to a common socio-economic status, the
knowledge of which is vital for targeted advertisement. In buyer-seller
networks, community detection facilitates better product recommendations.
Unfortunately, reliability of community assignments is hindered by anomalous
user behavior often observed as unfair self-promotion, or ""fake""
highly-connected accounts created to promote fraud. The present paper advocates
a novel approach for jointly tracking communities while detecting such
anomalous nodes in time-varying networks. By postulating edge creation as the
result of mutual community participation by node pairs, a dynamic factor model
with anomalous memberships captured through a sparse outlier matrix is put
forth. Efficient tracking algorithms suitable for both online and decentralized
operation are developed. Experiments conducted on both synthetic and real
network time series successfully unveil underlying communities and anomalous
nodes.","['Brian Baingana', 'Georgios B. Giannakis']","['stat.ML', 'cs.SI', 'physics.soc-ph']",2015-06-25 04:40:14+00:00
http://arxiv.org/abs/1506.07609v1,CRAFT: ClusteR-specific Assorted Feature selecTion,"We present a framework for clustering with cluster-specific feature
selection. The framework, CRAFT, is derived from asymptotic log posterior
formulations of nonparametric MAP-based clustering models. CRAFT handles
assorted data, i.e., both numeric and categorical data, and the underlying
objective functions are intuitively appealing. The resulting algorithm is
simple to implement and scales nicely, requires minimal parameter tuning,
obviates the need to specify the number of clusters a priori, and compares
favorably with other methods on real datasets.","['Vikas K. Garg', 'Cynthia Rudin', 'Tommi Jaakkola']","['cs.LG', 'stat.ML']",2015-06-25 04:14:49+00:00
http://arxiv.org/abs/1506.07540v1,"Global Optimality in Tensor Factorization, Deep Learning, and Beyond","Techniques involving factorization are found in a wide range of applications
and have enjoyed significant empirical success in many fields. However, common
to a vast majority of these problems is the significant disadvantage that the
associated optimization problems are typically non-convex due to a multilinear
form or other convexity destroying transformation. Here we build on ideas from
convex relaxations of matrix factorizations and present a very general
framework which allows for the analysis of a wide range of non-convex
factorization problems - including matrix factorization, tensor factorization,
and deep neural network training formulations. We derive sufficient conditions
to guarantee that a local minimum of the non-convex optimization problem is a
global minimum and show that if the size of the factorized variables is large
enough then from any initialization it is possible to find a global minimizer
using a purely local descent algorithm. Our framework also provides a partial
theoretical justification for the increasingly common use of Rectified Linear
Units (ReLUs) in deep neural networks and offers guidance on deep network
architectures and regularization strategies to facilitate efficient
optimization.","['Benjamin D. Haeffele', 'Rene Vidal']","['cs.NA', 'cs.LG', 'stat.ML']",2015-06-24 20:08:47+00:00
http://arxiv.org/abs/1506.07512v1,Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization,"We develop a family of accelerated stochastic algorithms that minimize sums
of convex functions. Our algorithms improve upon the fastest running time for
empirical risk minimization (ERM), and in particular linear least-squares
regression, across a wide range of problem settings. To achieve this, we
establish a framework based on the classical proximal point algorithm. Namely,
we provide several algorithms that reduce the minimization of a strongly convex
function to approximate minimizations of regularizations of the function. Using
these results, we accelerate recent fast stochastic algorithms in a black-box
fashion. Empirically, we demonstrate that the resulting algorithms exhibit
notions of stability that are advantageous in practice. Both in theory and in
practice, the provided algorithms reap the computational benefits of adding a
large strongly convex regularization term, without incurring a corresponding
bias to the original problem.","['Roy Frostig', 'Rong Ge', 'Sham M. Kakade', 'Aaron Sidford']","['stat.ML', 'cs.DS', 'cs.LG']",2015-06-24 19:53:45+00:00
http://arxiv.org/abs/1506.07504v1,Objective Variables for Probabilistic Revenue Maximization in Second-Price Auctions with Reserve,"Many online companies sell advertisement space in second-price auctions with
reserve. In this paper, we develop a probabilistic method to learn a profitable
strategy to set the reserve price. We use historical auction data with features
to fit a predictor of the best reserve price. This problem is delicate - the
structure of the auction is such that a reserve price set too high is much
worse than a reserve price set too low. To address this we develop objective
variables, a new framework for combining probabilistic modeling with optimal
decision-making. Objective variables are ""hallucinated observations"" that
transform the revenue maximization task into a regularized maximum likelihood
estimation problem, which we solve with an EM algorithm. This framework enables
a variety of prediction mechanisms to set the reserve price. As examples, we
study objective variable methods with regression, kernelized regression, and
neural networks on simulated and real data. Our methods outperform previous
approaches both in terms of scalability and profit.","['Maja R. Rudolph', 'Joseph G. Ellis', 'David M. Blei']","['stat.ML', 'cs.AI', 'cs.GT', 'cs.LG', 'stat.AP']",2015-06-24 19:20:18+00:00
http://arxiv.org/abs/1506.07503v1,Attention-Based Models for Speech Recognition,"Recurrent sequence generators conditioned on input data through an attention
mechanism have recently shown very good performance on a range of tasks in-
cluding machine translation, handwriting synthesis and image caption gen-
eration. We extend the attention-mechanism with features needed for speech
recognition. We show that while an adaptation of the model used for machine
translation in reaches a competitive 18.7% phoneme error rate (PER) on the
TIMIT phoneme recognition task, it can only be applied to utterances which are
roughly as long as the ones it was trained on. We offer a qualitative
explanation of this failure and propose a novel and generic method of adding
location-awareness to the attention mechanism to alleviate this issue. The new
method yields a model that is robust to long inputs and achieves 18% PER in
single utterances and 20% in 10-times longer (repeated) utterances. Finally, we
propose a change to the at- tention mechanism that prevents it from
concentrating too much on single frames, which further reduces PER to 17.6%
level.","['Jan Chorowski', 'Dzmitry Bahdanau', 'Dmitriy Serdyuk', 'Kyunghyun Cho', 'Yoshua Bengio']","['cs.CL', 'cs.LG', 'cs.NE', 'stat.ML']",2015-06-24 19:10:33+00:00
http://arxiv.org/abs/1506.07477v1,Efficient Learning for Undirected Topic Models,"Replicated Softmax model, a well-known undirected topic model, is powerful in
extracting semantic representations of documents. Traditional learning
strategies such as Contrastive Divergence are very inefficient. This paper
provides a novel estimator to speed up the learning based on Noise Contrastive
Estimate, extended for documents of variant lengths and weighted inputs.
Experiments on two benchmarks show that the new estimator achieves great
learning efficiency and high accuracy on document retrieval and classification.","['Jiatao Gu', 'Victor O. K. Li']","['cs.LG', 'cs.CL', 'cs.IR', 'stat.ML']",2015-06-24 17:27:28+00:00
http://arxiv.org/abs/1506.07405v3,Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation,"It has been observed in a variety of contexts that gradient descent methods
have great success in solving low-rank matrix factorization problems, despite
the relevant problem formulation being non-convex. We tackle a particular
instance of this scenario, where we seek the $d$-dimensional subspace spanned
by a streaming data matrix. We apply the natural first order incremental
gradient descent method, constraining the gradient method to the Grassmannian.
In this paper, we propose an adaptive step size scheme that is greedy for the
noiseless case, that maximizes the improvement of our metric of convergence at
each data index $t$, and yields an expected improvement for the noisy case. We
show that, with noise-free data, this method converges from any random
initialization to the global minimum of the problem. For noisy data, we provide
the expected convergence rate of the proposed algorithm per iteration.","['Dejiao Zhang', 'Laura Balzano']","['cs.NA', 'math.NA', 'stat.ML', '90C52, 65Y20', 'G.1.6; F.2.1']",2015-06-24 14:59:27+00:00
http://arxiv.org/abs/1506.07365v3,Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images,"We introduce Embed to Control (E2C), a method for model learning and control
of non-linear dynamical systems from raw pixel images. E2C consists of a deep
generative model, belonging to the family of variational autoencoders, that
learns to generate image trajectories from a latent space in which the dynamics
is constrained to be locally linear. Our model is derived directly from an
optimal control formulation in latent space, supports long-term prediction of
image sequences and exhibits strong performance on a variety of complex control
problems.","['Manuel Watter', 'Jost Tobias Springenberg', 'Joschka Boedecker', 'Martin Riedmiller']","['cs.LG', 'cs.CV', 'stat.ML']",2015-06-24 13:48:51+00:00
http://arxiv.org/abs/1506.07251v1,Benchmark of structured machine learning methods for microbial identification from mass-spectrometry data,"Microbial identification is a central issue in microbiology, in particular in
the fields of infectious diseases diagnosis and industrial quality control. The
concept of species is tightly linked to the concept of biological and clinical
classification where the proximity between species is generally measured in
terms of evolutionary distances and/or clinical phenotypes. Surprisingly, the
information provided by this well-known hierarchical structure is rarely used
by machine learning-based automatic microbial identification systems.
Structured machine learning methods were recently proposed for taking into
account the structure embedded in a hierarchy and using it as additional a
priori information, and could therefore allow to improve microbial
identification systems. We test and compare several state-of-the-art machine
learning methods for microbial identification on a new Matrix-Assisted Laser
Desorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset.
We include in the benchmark standard and structured methods, that leverage the
knowledge of the underlying hierarchical structure in the learning process. Our
results show that although some methods perform better than others, structured
methods do not consistently perform better than their ""flat"" counterparts. We
postulate that this is partly due to the fact that standard methods already
reach a high level of accuracy in this context, and that they mainly confuse
species close to each other in the tree, a case where using the known hierarchy
is not helpful.","['Kévin Vervier', 'Pierre Mahé', 'Jean-Baptiste Veyrieras', 'Jean-Philippe Vert']","['stat.ML', 'cs.LG', 'q-bio.QM']",2015-06-24 06:13:15+00:00
http://arxiv.org/abs/1506.07216v3,Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality,"We study the tradeoff between the statistical error and communication cost of
distributed statistical estimation problems in high dimensions. In the
distributed sparse Gaussian mean estimation problem, each of the $m$ machines
receives $n$ data points from a $d$-dimensional Gaussian distribution with
unknown mean $\theta$ which is promised to be $k$-sparse. The machines
communicate by message passing and aim to estimate the mean $\theta$. We
provide a tight (up to logarithmic factors) tradeoff between the estimation
error and the number of bits communicated between the machines. This directly
leads to a lower bound for the distributed \textit{sparse linear regression}
problem: to achieve the statistical minimax error, the total communication is
at least $\Omega(\min\{n,d\}m)$, where $n$ is the number of observations that
each machine receives and $d$ is the ambient dimension. These lower results
improve upon [Sha14,SD'14] by allowing multi-round iterative communication
model. We also give the first optimal simultaneous protocol in the dense case
for mean estimation.
  As our main technique, we prove a \textit{distributed data processing
inequality}, as a generalization of usual data processing inequalities, which
might be of independent interest and useful for other problems.","['Mark Braverman', 'Ankit Garg', 'Tengyu Ma', 'Huy L. Nguyen', 'David P. Woodruff']","['cs.LG', 'cs.CC', 'cs.IT', 'math.IT', 'stat.ML']",2015-06-24 01:01:41+00:00
http://arxiv.org/abs/1506.06975v3,Bayesian optimisation for fast approximate inference in state-space models with intractable likelihoods,"We consider the problem of approximate Bayesian parameter inference in
non-linear state-space models with intractable likelihoods. Sequential Monte
Carlo with approximate Bayesian computations (SMC-ABC) is one approach to
approximate the likelihood in this type of models. However, such approximations
can be noisy and computationally costly which hinders efficient implementations
using standard methods based on optimisation and Monte Carlo methods. We
propose a computationally efficient novel method based on the combination of
Gaussian process optimisation and SMC-ABC to create a Laplace approximation of
the intractable posterior. We exemplify the proposed algorithm for inference in
stochastic volatility models with both synthetic and real-world data as well as
for estimating the Value-at-Risk for two portfolios using a copula model. We
document speed-ups of between one and two orders of magnitude compared to
state-of-the-art algorithms for posterior inference.","['Johan Dahlin', 'Mattias Villani', 'Thomas B. Schön']","['stat.CO', 'q-fin.RM', 'stat.ML']",2015-06-23 13:04:03+00:00
http://arxiv.org/abs/1506.06972v1,GEFCOM 2014 - Probabilistic Electricity Price Forecasting,"Energy price forecasting is a relevant yet hard task in the field of
multi-step time series forecasting. In this paper we compare a well-known and
established method, ARMA with exogenous variables with a relatively new
technique Gradient Boosting Regression. The method was tested on data from
Global Energy Forecasting Competition 2014 with a year long rolling window
forecast. The results from the experiment reveal that a multi-model approach is
significantly better performing in terms of error metrics. Gradient Boosting
can deal with seasonality and auto-correlation out-of-the box and achieve lower
rate of normalized mean absolute error on real-world data.","['Gergo Barta', 'Gyula Borbely', 'Gabor Nagy', 'Sandor Kazi', 'Tamas Henk']","['stat.ML', 'cs.CE', 'cs.LG', 'stat.AP']",2015-06-23 12:27:50+00:00
http://arxiv.org/abs/1506.06962v1,Graphs in machine learning: an introduction,"Graphs are commonly used to characterise interactions between objects of
interest. Because they are based on a straightforward formalism, they are used
in many scientific fields from computer science to historical sciences. In this
paper, we give an introduction to some methods relying on graphs for learning.
This includes both unsupervised and supervised methods. Unsupervised learning
algorithms usually aim at visualising graphs in latent spaces and/or clustering
the nodes. Both focus on extracting knowledge from graph topologies. While most
existing techniques are only applicable to static graphs, where edges do not
evolve through time, recent developments have shown that they could be extended
to deal with evolving networks. In a supervised context, one generally aims at
inferring labels or numerical values attached to nodes using both the graph
and, when they are available, node characteristics. Balancing the two sources
of information can be challenging, especially as they can disagree locally or
globally. In both contexts, supervised and un-supervised, data can be
relational (augmented with one or several global graphs) as described above, or
graph valued. In this latter case, each object of interest is given as a full
graph (possibly completed by other characteristics). In this context, natural
tasks include graph clustering (as in producing clusters of graphs rather than
clusters of nodes in a single graph), graph classification, etc. 1 Real
networks One of the first practical studies on graphs can be dated back to the
original work of Moreno [51] in the 30s. Since then, there has been a growing
interest in graph analysis associated with strong developments in the modelling
and the processing of these data. Graphs are now used in many scientific
fields. In Biology [54, 2, 7], for instance, metabolic networks can describe
pathways of biochemical reactions [41], while in social sciences networks are
used to represent relation ties between actors [66, 56, 36, 34]. Other examples
include powergrids [71] and the web [75]. Recently, networks have also been
considered in other areas such as geography [22] and history [59, 39]. In
machine learning, networks are seen as powerful tools to model problems in
order to extract information from data and for prediction purposes. This is the
object of this paper. For more complete surveys, we refer to [28, 62, 49, 45].
In this section, we introduce notations and highlight properties shared by most
real networks. In Section 2, we then consider methods aiming at extracting
information from a unique network. We will particularly focus on clustering
methods where the goal is to find clusters of vertices. Finally, in Section 3,
techniques that take a series of networks into account, where each network is","['Pierre Latouche', 'Fabrice Rossi']","['stat.ML', 'cs.LG', 'cs.SI', 'physics.soc-ph']",2015-06-23 12:12:45+00:00
http://arxiv.org/abs/1506.06840v2,On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants,"We study optimization algorithms based on variance reduction for stochastic
gradient descent (SGD). Remarkable recent progress has been made in this
direction through development of algorithms like SAG, SVRG, SAGA. These
algorithms have been shown to outperform SGD, both theoretically and
empirically. However, asynchronous versions of these algorithms---a crucial
requirement for modern large-scale applications---have not been studied. We
bridge this gap by presenting a unifying framework for many variance reduction
techniques. Subsequently, we propose an asynchronous algorithm grounded in our
framework, and prove its fast convergence. An important consequence of our
general approach is that it yields asynchronous versions of variance reduction
algorithms such as SVRG and SAGA as a byproduct. Our method achieves near
linear speedup in sparse settings common to machine learning. We demonstrate
the empirical performance of our method through a concrete realization of
asynchronous SVRG.","['Sashank J. Reddi', 'Ahmed Hefny', 'Suvrit Sra', 'Barnabás Póczos', 'Alex Smola']","['cs.LG', 'stat.ML']",2015-06-23 01:57:19+00:00
http://arxiv.org/abs/1506.06707v2,Non-Normal Mixtures of Experts,"Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in
data for regression, classification and clustering. For continuous data which
we consider here in the context of regression and cluster analysis, MoE usually
use normal experts, that is, expert components following the Gaussian
distribution. However, for a set of data containing a group or groups of
observations with asymmetric behavior, heavy tails or atypical observations,
the use of normal experts may be unsuitable and can unduly affect the fit of
the MoE model. In this paper, we introduce new non-normal mixture of experts
(NNMoE) which can deal with these issues regarding possibly skewed,
heavy-tailed data and with outliers. The proposed models are the skew-normal
MoE and the robust $t$ MoE and skew $t$ MoE, respectively named SNMoE, TMoE and
STMoE. We develop dedicated expectation-maximization (EM) and expectation
conditional maximization (ECM) algorithms to estimate the parameters of the
proposed models by monotonically maximizing the observed data log-likelihood.
We describe how the presented models can be used in prediction and in
model-based clustering of regression data. Numerical experiments carried out on
simulated data show the effectiveness and the robustness of the proposed models
in terms modeling non-linear regression functions as well as in model-based
clustering. Then, to show their usefulness for practical applications, the
proposed models are applied to the real-world data of tone perception for
musical data analysis, and the one of temperature anomalies for the analysis of
climate change data.",['Faicel Chamroukhi'],"['stat.ME', 'cs.LG', 'stat.ML', '62-XX, 62H30, 62H12, 62-07, 62Fxx']",2015-06-22 18:12:36+00:00
http://arxiv.org/abs/1506.06650v2,Blind Source Separation Algorithms Using Hyperbolic and Givens Rotations for High-Order QAM Constellations,"This paper addresses the problem of blind demixing of instantaneous mixtures
in a multiple-input multiple-output communication system. The main objective is
to present efficient blind source separation (BSS) algorithms dedicated to
moderate or high-order QAM constellations. Four new iterative batch BSS
algorithms are presented dealing with the multimodulus (MM) and alphabet
matched (AM) criteria. For the optimization of these cost functions, iterative
methods of Givens and hyperbolic rotations are used. A pre-whitening operation
is also utilized to reduce the complexity of design problem. It is noticed that
the designed algorithms using Givens rotations gives satisfactory performance
only for large number of samples. However, for small number of samples, the
algorithms designed by combining both Givens and hyperbolic rotations
compensate for the ill-whitening that occurs in this case and thus improves the
performance. Two algorithms dealing with the MM criterion are presented for
moderate order QAM signals such as 16-QAM. The other two dealing with the AM
criterion are presented for high-order QAM signals. These methods are finally
compared with the state of art batch BSS algorithms in terms of
signal-to-interference and noise ratio, symbol error rate and convergence rate.
Simulation results show that the proposed methods outperform the contemporary
batch BSS algorithms.","['Syed A. W. Shah', 'Karim Abed-Meraim', 'Tareq Y. Al-Naffouri']","['cs.IT', 'math.IT', 'stat.ML']",2015-06-22 15:26:49+00:00
http://arxiv.org/abs/1506.06646v2,Nonparametric Bayesian Double Articulation Analyzer for Direct Language Acquisition from Continuous Speech Signals,"Human infants can discover words directly from unsegmented speech signals
without any explicitly labeled data. In this paper, we develop a novel machine
learning method called nonparametric Bayesian double articulation analyzer
(NPB-DAA) that can directly acquire language and acoustic models from observed
continuous speech signals. For this purpose, we propose an integrative
generative model that combines a language model and an acoustic model into a
single generative model called the ""hierarchical Dirichlet process hidden
language model"" (HDP-HLM). The HDP-HLM is obtained by extending the
hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by
Johnson et al. An inference procedure for the HDP-HLM is derived using the
blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure
enables the simultaneous and direct inference of language and acoustic models
from continuous speech signals. Based on the HDP-HLM and its inference
procedure, we developed a novel double articulation analyzer. By assuming
HDP-HLM as a generative model of observed time series data, and by inferring
latent variables of the model, the method can analyze latent double
articulation structure, i.e., hierarchically organized latent words and
phonemes, of the data in an unsupervised manner. The novel unsupervised double
articulation analyzer is called NPB-DAA.
  The NPB-DAA can automatically estimate double articulation structure embedded
in speech signals. We also carried out two evaluation experiments using
synthetic data and actual human continuous speech signals representing Japanese
vowel sequences. In the word acquisition and phoneme categorization tasks, the
NPB-DAA outperformed a conventional double articulation analyzer (DAA) and
baseline automatic speech recognition system whose acoustic model was trained
in a supervised manner.","['Tadahiro Taniguchi', 'Ryo Nakashima', 'Shogo Nagasaka']","['cs.AI', 'cs.CL', 'cs.LG', 'stat.ML']",2015-06-22 15:21:57+00:00
http://arxiv.org/abs/1506.06573v1,PAC-Bayes Iterated Logarithm Bounds for Martingale Mixtures,"We give tight concentration bounds for mixtures of martingales that are
simultaneously uniform over (a) mixture distributions, in a PAC-Bayes sense;
and (b) all finite times. These bounds are proved in terms of the martingale
variance, extending classical Bernstein inequalities, and sharpening and
simplifying prior work.",['Akshay Balsubramani'],"['cs.LG', 'math.PR', 'stat.ML']",2015-06-22 12:47:07+00:00
http://arxiv.org/abs/1506.06472v2,"A Theory of Local Learning, the Learning Channel, and the Optimality of Backpropagation","In a physical neural system, where storage and processing are intimately
intertwined, the rules for adjusting the synaptic weights can only depend on
variables that are available locally, such as the activity of the pre- and
post-synaptic neurons, resulting in local learning rules. A systematic
framework for studying the space of local learning rules is obtained by first
specifying the nature of the local variables, and then the functional form that
ties them together into each learning rule. Such a framework enables also the
systematic discovery of new learning rules and exploration of relationships
between learning rules and group symmetries. We study polynomial local learning
rules stratified by their degree and analyze their behavior and capabilities in
both linear and non-linear units and networks. Stacking local learning rules in
deep feedforward networks leads to deep local learning. While deep local
learning can learn interesting representations, it cannot learn complex
input-output functions, even when targets are available for the top layer.
Learning complex input-output functions requires local deep learning where
target information is communicated to the deep layers through a backward
learning channel. The nature of the communicated information about the targets
and the structure of the learning channel partition the space of learning
algorithms. We estimate the learning channel capacity associated with several
algorithms and show that backpropagation outperforms them by simultaneously
maximizing the information rate and minimizing the computational cost, even in
recurrent networks. The theory clarifies the concept of Hebbian learning,
establishes the power and limitations of local learning rules, introduces the
learning channel which enables a formal analysis of the optimality of
backpropagation, and explains the sparsity of the space of learning rules
discovered so far.","['Pierre Baldi', 'Peter Sadowski']","['cs.LG', 'cs.NE', 'stat.ML']",2015-06-22 05:16:57+00:00
http://arxiv.org/abs/1506.06438v2,Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms,"Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of
machine learning problems. Researchers and industry have developed several
techniques to optimize SGD's runtime performance, including asynchronous
execution and reduced precision. Our main result is a martingale-based analysis
that enables us to capture the rich noise models that may arise from such
techniques. Specifically, we use our new analysis in three ways: (1) we derive
convergence rates for the convex case (Hogwild!) with relaxed assumptions on
the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for
non-convex matrix problems including matrix completion; and (3) we design and
analyze an asynchronous SGD algorithm, called Buckwild!, that uses
lower-precision arithmetic. We show experimentally that our algorithms run
efficiently for a variety of problems on modern hardware.","['Christopher De Sa', 'Ce Zhang', 'Kunle Olukotun', 'Christopher Ré']","['cs.LG', 'math.OC', 'stat.ML']",2015-06-22 01:48:39+00:00
http://arxiv.org/abs/1506.06422v2,Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical Clustering,"Hierarchical clustering is a popular method for analyzing data which
associates a tree to a dataset. Hartigan consistency has been used extensively
as a framework to analyze such clustering algorithms from a statistical point
of view. Still, as we show in the paper, a tree which is Hartigan consistent
with a given density can look very different than the correct limit tree.
Specifically, Hartigan consistency permits two types of undesirable
configurations which we term over-segmentation and improper nesting. Moreover,
Hartigan consistency is a limit property and does not directly quantify
difference between trees.
  In this paper we identify two limit properties, separation and minimality,
which address both over-segmentation and improper nesting and together imply
(but are not implied by) Hartigan consistency. We proceed to introduce a merge
distortion metric between hierarchical clusterings and show that convergence in
our distance implies both separation and minimality. We also prove that uniform
separation and minimality imply convergence in the merge distortion metric.
Furthermore, we show that our merge distortion metric is stable under
perturbations of the density.
  Finally, we demonstrate applicability of these concepts by proving
convergence results for two clustering algorithms. First, we show convergence
(and hence separation and minimality) of the recent robust single linkage
algorithm of Chaudhuri and Dasgupta (2010). Second, we provide convergence
results on manifolds for topological split tree clustering.","['Justin Eldridge', 'Mikhail Belkin', 'Yusu Wang']","['stat.ML', 'math.ST', 'stat.TH']",2015-06-21 23:19:37+00:00
http://arxiv.org/abs/1506.06318v2,Communication Efficient Distributed Agnostic Boosting,"We consider the problem of learning from distributed data in the agnostic
setting, i.e., in the presence of arbitrary forms of noise. Our main
contribution is a general distributed boosting-based procedure for learning an
arbitrary concept space, that is simultaneously noise tolerant, communication
efficient, and computationally efficient. This improves significantly over
prior works that were either communication efficient only in noise-free
scenarios or computationally prohibitive. Empirical results on large synthetic
and real-world datasets demonstrate the effectiveness and scalability of the
proposed approach.","['Shang-Tse Chen', 'Maria-Florina Balcan', 'Duen Horng Chau']","['cs.LG', 'stat.ML']",2015-06-21 04:35:42+00:00
http://arxiv.org/abs/1506.06272v1,Aligning where to see and what to tell: image caption with region-based attention and scene factorization,"Recent progress on automatic generation of image captions has shown that it
is possible to describe the most salient information conveyed by images with
accurate and meaningful sentences. In this paper, we propose an image caption
system that exploits the parallel structures between images and sentences. In
our model, the process of generating the next word, given the previously
generated ones, is aligned with the visual perception experience where the
attention shifting among the visual regions imposes a thread of visual
ordering. This alignment characterizes the flow of ""abstract meaning"", encoding
what is semantically shared by both the visual scene and the text description.
Our system also makes another novel modeling contribution by introducing
scene-specific contexts that capture higher-level semantic information encoded
in an image. The contexts adapt language models for word generation to specific
scene types. We benchmark our system and contrast to published results on
several popular datasets. We show that using either region-based attention or
scene-specific contexts improves systems without those components. Furthermore,
combining these two modeling ingredients attains the state-of-the-art
performance.","['Junqi Jin', 'Kun Fu', 'Runpeng Cui', 'Fei Sha', 'Changshui Zhang']","['cs.CV', 'cs.LG', 'stat.ML']",2015-06-20 17:25:38+00:00
http://arxiv.org/abs/1506.06179v1,Detectability thresholds and optimal algorithms for community structure in dynamic networks,"We study the fundamental limits on learning latent community structure in
dynamic networks. Specifically, we study dynamic stochastic block models where
nodes change their community membership over time, but where edges are
generated independently at each time step. In this setting (which is a special
case of several existing models), we are able to derive the detectability
threshold exactly, as a function of the rate of change and the strength of the
communities. Below this threshold, we claim that no algorithm can identify the
communities better than chance. We then give two algorithms that are optimal in
the sense that they succeed all the way down to this limit. The first uses
belief propagation (BP), which gives asymptotically optimal accuracy, and the
second is a fast spectral clustering algorithm, based on linearizing the BP
equations. We verify our analytic and algorithmic results via numerical
simulation, and close with a brief discussion of extensions and open questions.","['Amir Ghasemian', 'Pan Zhang', 'Aaron Clauset', 'Cristopher Moore', 'Leto Peel']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG', 'cs.SI', 'physics.data-an']",2015-06-19 23:07:51+00:00
http://arxiv.org/abs/1506.06100v1,Approximate Inference with the Variational Holder Bound,"We introduce the Variational Holder (VH) bound as an alternative to
Variational Bayes (VB) for approximate Bayesian inference. Unlike VB which
typically involves maximization of a non-convex lower bound with respect to the
variational parameters, the VH bound involves minimization of a convex upper
bound to the intractable integral with respect to the variational parameters.
Minimization of the VH bound is a convex optimization problem; hence the VH
method can be applied using off-the-shelf convex optimization algorithms and
the approximation error of the VH bound can also be analyzed using tools from
convex optimization literature. We present experiments on the task of
integrating a truncated multivariate Gaussian distribution and compare our
method to VB, EP and a state-of-the-art numerical integration method for this
problem.","['Guillaume Bouchard', 'Balaji Lakshminarayanan']","['stat.ML', 'cs.LG', 'math.FA']",2015-06-19 18:00:40+00:00
http://arxiv.org/abs/1506.06081v3,A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements,"We propose a simple, scalable, and fast gradient descent algorithm to
optimize a nonconvex objective for the rank minimization problem and a closely
related family of semidefinite programs. With $O(r^3 \kappa^2 n \log n)$ random
measurements of a positive semidefinite $n \times n$ matrix of rank $r$ and
condition number $\kappa$, our method is guaranteed to converge linearly to the
global optimum.","['Qinqing Zheng', 'John Lafferty']","['stat.ML', 'cs.LG']",2015-06-19 16:41:08+00:00
http://arxiv.org/abs/1506.06068v1,A general framework for the IT-based clustering methods,"Previously, we proposed a physically inspired rule to organize the data
points in a sparse yet effective structure, called the in-tree (IT) graph,
which is able to capture a wide class of underlying cluster structures in the
datasets, especially for the density-based datasets. Although there are some
redundant edges or lines between clusters requiring to be removed by computer,
this IT graph has a big advantage compared with the k-nearest-neighborhood
(k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges in
the IT graph are much more distinguishable and thus can be easily determined by
several methods previously proposed by us.
  In this paper, we propose a general framework to re-construct the IT graph,
based on an initial neighborhood graph, such as the k-NN or MST, etc, and the
corresponding graph distances. For this general framework, our previous way of
constructing the IT graph turns out to be a special case of it. This general
framework 1) can make the IT graph capture a wider class of underlying cluster
structures in the datasets, especially for the manifolds, and 2) should be more
effective to cluster the sparse or graph-based datasets.","['Teng Qiu', 'Yongjie Li']","['cs.CV', 'cs.LG', 'stat.ML']",2015-06-19 16:03:31+00:00
http://arxiv.org/abs/1506.06040v1,Tensor Analysis and Fusion of Multimodal Brain Images,"Current high-throughput data acquisition technologies probe dynamical systems
with different imaging modalities, generating massive data sets at different
spatial and temporal resolutions posing challenging problems in multimodal data
fusion. A case in point is the attempt to parse out the brain structures and
networks that underpin human cognitive processes by analysis of different
neuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that the
multimodal, multi-scale nature of neuroimaging data is well reflected by a
multi-way (tensor) structure where the underlying processes can be summarized
by a relatively small number of components or ""atoms"". We introduce
Markov-Penrose diagrams - an integration of Bayesian DAG and tensor network
notation in order to analyze these models. These diagrams not only clarify
matrix and tensor EEG and fMRI time/frequency analysis and inverse problems,
but also help understand multimodal fusion via Multiway Partial Least Squares
and Coupled Matrix-Tensor Factorization. We show here, for the first time, that
Granger causal analysis of brain networks is a tensor regression problem, thus
allowing the atomic decomposition of brain networks. Analysis of EEG and fMRI
recordings shows the potential of the methods and suggests their use in other
scientific domains.","['Esin Karahan', 'Pedro A. Rojas-Lopez', 'Maria L. Bringas-Vega', 'Pedro A. Valdes-Hernandez', 'Pedro A. Valdes-Sosa']","['stat.ME', 'cs.NA', 'stat.AP', 'stat.ML']",2015-06-19 15:03:22+00:00
http://arxiv.org/abs/1506.05985v1,Enhanced Lasso Recovery on Graph,"This work aims at recovering signals that are sparse on graphs. Compressed
sensing offers techniques for signal recovery from a few linear measurements
and graph Fourier analysis provides a signal representation on graph. In this
paper, we leverage these two frameworks to introduce a new Lasso recovery
algorithm on graphs. More precisely, we present a non-convex, non-smooth
algorithm that outperforms the standard convex Lasso technique. We carry out
numerical experiments on three benchmark graph datasets.","['Xavier Bresson', 'Thomas Laurent', 'James von Brecht']","['cs.LG', 'stat.ML']",2015-06-19 12:59:18+00:00
http://arxiv.org/abs/1506.05967v3,Doubly Decomposing Nonparametric Tensor Regression,"Nonparametric extension of tensor regression is proposed. Nonlinearity in a
high-dimensional tensor space is broken into simple local functions by
incorporating low-rank tensor decomposition. Compared to naive nonparametric
approaches, our formulation considerably improves the convergence rate of
estimation while maintaining consistency with the same function class under
specific conditions. To estimate local functions, we develop a Bayesian
estimator with the Gaussian process prior. Experimental results show its
theoretical properties and high performance in terms of predicting a summary
statistic of a real complex network.","['Masaaki Imaizumi', 'Kohei Hayashi']",['stat.ML'],2015-06-19 12:03:52+00:00
http://arxiv.org/abs/1506.05950v1,Spectral Analysis of Symmetric and Anti-Symmetric Pairwise Kernels,"We consider the problem of learning regression functions from pairwise data
when there exists prior knowledge that the relation to be learned is symmetric
or anti-symmetric. Such prior knowledge is commonly enforced by symmetrizing or
anti-symmetrizing pairwise kernel functions. Through spectral analysis, we show
that these transformations reduce the kernel's effective dimension. Further, we
provide an analysis of the approximation properties of the resulting kernels,
and bound the regularization bias of the kernels in terms of the corresponding
bias of the original kernel.","['Tapio Pahikkala', 'Markus Viljanen', 'Antti Airola', 'Willem Waegeman']","['cs.LG', 'stat.ML']",2015-06-19 10:24:01+00:00
http://arxiv.org/abs/1506.05936v1,Sampling constrained probability distributions using Spherical Augmentation,"Statistical models with constrained probability distributions are abundant in
machine learning. Some examples include regression models with norm constraints
(e.g., Lasso), probit, many copula models, and latent Dirichlet allocation
(LDA). Bayesian inference involving probability distributions confined to
constrained domains could be quite challenging for commonly used sampling
algorithms. In this paper, we propose a novel augmentation technique that
handles a wide range of constraints by mapping the constrained domain to a
sphere in the augmented space. By moving freely on the surface of this sphere,
sampling algorithms handle constraints implicitly and generate proposals that
remain within boundaries when mapped back to the original space. Our proposed
method, called {Spherical Augmentation}, provides a mathematically natural and
computationally efficient framework for sampling from constrained probability
distributions. We show the advantages of our method over state-of-the-art
sampling algorithms, such as exact Hamiltonian Monte Carlo, using several
examples including truncated Gaussian distributions, Bayesian Lasso, Bayesian
bridge regression, reconstruction of quantized stationary Gaussian process, and
LDA for topic modeling.","['Shiwei Lan', 'Babak Shahbaba']","['stat.CO', 'stat.ML']",2015-06-19 09:44:53+00:00
http://arxiv.org/abs/1506.05934v1,Expectation Particle Belief Propagation,"We propose an original particle-based implementation of the Loopy Belief
Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a
continuous state space. The algorithm constructs adaptively efficient proposal
distributions approximating the local beliefs at each note of the MRF. This is
achieved by considering proposal distributions in the exponential family whose
parameters are updated iterately in an Expectation Propagation (EP) framework.
The proposed particle scheme provides consistent estimation of the LBP
marginals as the number of particles increases. We demonstrate that it provides
more accurate results than the Particle Belief Propagation (PBP) algorithm of
Ihler and McAllester (2009) at a fraction of the computational cost and is
additionally more robust empirically. The computational complexity of our
algorithm at each iteration is quadratic in the number of particles. We also
propose an accelerated implementation with sub-quadratic computational
complexity which still provides consistent estimates of the loopy BP marginal
distributions and performs almost as well as the original procedure.","['Thibaut Lienart', 'Yee Whye Teh', 'Arnaud Doucet']","['stat.CO', 'cs.AI', 'stat.ML']",2015-06-19 09:34:21+00:00
http://arxiv.org/abs/1506.05900v1,Representation Learning for Clustering: A Statistical Framework,"We address the problem of communicating domain knowledge from a user to the
designer of a clustering algorithm. We propose a protocol in which the user
provides a clustering of a relatively small random sample of a data set. The
algorithm designer then uses that sample to come up with a data representation
under which $k$-means clustering results in a clustering (of the full data set)
that is aligned with the user's clustering. We provide a formal statistical
model for analyzing the sample complexity of learning a clustering
representation with this paradigm. We then introduce a notion of capacity of a
class of possible representations, in the spirit of the VC-dimension, showing
that classes of representations that have finite such dimension can be
successfully learned with sample size error bounds, and end our discussion with
an analysis of that dimension for classes of representations induced by linear
embeddings.","['Hassan Ashtiani', 'Shai Ben-David']","['stat.ML', 'cs.LG']",2015-06-19 08:18:59+00:00
http://arxiv.org/abs/1506.05860v3,Variational Gaussian Copula Inference,"We utilize copulas to constitute a unified framework for constructing and
optimizing variational proposals in hierarchical Bayesian models. For models
with continuous and non-Gaussian hidden variables, we propose a semiparametric
and automated variational Gaussian copula approach, in which the parametric
Gaussian copula family is able to preserve multivariate posterior dependence,
and the nonparametric transformations based on Bernstein polynomials provide
ample flexibility in characterizing the univariate marginal posteriors.","['Shaobo Han', 'Xuejun Liao', 'David B. Dunson', 'Lawrence Carin']","['stat.ML', 'cs.LG', 'stat.CO']",2015-06-19 01:49:46+00:00
http://arxiv.org/abs/1506.05855v5,Information-based inference for singular models and finite sample sizes: A frequentist information criterion,"In the information-based paradigm of inference, model selection is performed
by selecting the candidate model with the best estimated predictive
performance. The success of this approach depends on the accuracy of the
estimate of the predictive complexity. In the large-sample-size limit of a
regular model, the predictive performance is well estimated by the Akaike
Information Criterion (AIC). However, this approximation can either
significantly under or over-estimating the complexity in a wide range of
important applications where models are either non-regular or
finite-sample-size corrections are significant. We introduce an improved
approximation for the complexity that is used to define a new information
criterion: the Frequentist Information Criterion (QIC). QIC extends the
applicability of information-based inference to the finite-sample-size regime
of regular models and to singular models. We demonstrate the power and the
comparative advantage of QIC in a number of example analyses.","['Colin H. LaMont', 'Paul A. Wiggins']","['stat.ML', 'cs.LG', 'physics.data-an']",2015-06-19 00:53:40+00:00
http://arxiv.org/abs/1506.06129v1,A simple application of FIC to model selection,"We have recently proposed a new information-based approach to model
selection, the Frequentist Information Criterion (FIC), that reconciles
information-based and frequentist inference. The purpose of this current paper
is to provide a simple example of the application of this criterion and a
demonstration of the natural emergence of model complexities with both AIC-like
($N^0$) and BIC-like ($\log N$) scaling with observation number $N$. The
application developed is deliberately simplified to make the analysis
analytically tractable.",['Paul A. Wiggins'],"['physics.data-an', 'cs.LG', 'stat.ML']",2015-06-19 00:39:43+00:00
http://arxiv.org/abs/1506.05843v1,Dependent Multinomial Models Made Easy: Stick Breaking with the Pólya-Gamma Augmentation,"Many practical modeling problems involve discrete data that are best
represented as draws from multinomial or categorical distributions. For
example, nucleotides in a DNA sequence, children's names in a given state and
year, and text documents are all commonly modeled with multinomial
distributions. In all of these cases, we expect some form of dependency between
the draws: the nucleotide at one position in the DNA strand may depend on the
preceding nucleotides, children's names are highly correlated from year to
year, and topics in text may be correlated and dynamic. These dependencies are
not naturally captured by the typical Dirichlet-multinomial formulation. Here,
we leverage a logistic stick-breaking representation and recent innovations in
P\'olya-gamma augmentation to reformulate the multinomial distribution in terms
of latent variables with jointly Gaussian likelihoods, enabling us to take
advantage of a host of Bayesian inference techniques for Gaussian models with
minimal overhead.","['Scott W. Linderman', 'Matthew J. Johnson', 'Ryan P. Adams']",['stat.ML'],2015-06-18 22:45:16+00:00
http://arxiv.org/abs/1506.05822v1,Optimal model-free prediction from multivariate time series,"Forecasting a time series from multivariate predictors constitutes a
challenging problem, especially using model-free approaches. Most techniques,
such as nearest-neighbor prediction, quickly suffer from the curse of
dimensionality and overfitting for more than a few predictors which has limited
their application mostly to the univariate case. Therefore, selection
strategies are needed that harness the available information as efficiently as
possible. Since often the right combination of predictors matters, ideally all
subsets of possible predictors should be tested for their predictive power, but
the exponentially growing number of combinations makes such an approach
computationally prohibitive. Here a prediction scheme that overcomes this
strong limitation is introduced utilizing a causal pre-selection step which
drastically reduces the number of possible predictors to the most predictive
set of causal drivers making a globally optimal search scheme tractable. The
information-theoretic optimality is derived and practical selection criteria
are discussed. As demonstrated for multivariate nonlinear stochastic delay
processes, the optimal scheme can even be less computationally expensive than
commonly used sub-optimal schemes like forward selection. The method suggests a
general framework to apply the optimal model-free approach to select variables
and subsequently fit a model to further improve a prediction or learn
statistical dependencies. The performance of this framework is illustrated on a
climatological index of El Ni\~no Southern Oscillation.","['Jakob Runge', 'Reik V. Donner', 'Jürgen Kurths']","['stat.ML', 'stat.ME']",2015-06-18 21:17:11+00:00
http://arxiv.org/abs/1506.05776v1,A tree augmented naive Bayesian network experiment for breast cancer prediction,"In order to investigate the breast cancer prediction problem on the aging
population with the grades of DCIS, we conduct a tree augmented naive Bayesian
network experiment trained and tested on a large clinical dataset including
consecutive diagnostic mammography examinations, consequent biopsy outcomes and
related cancer registry records in the population of women across all ages. The
aggregated results of our ten-fold cross validation method recommend a biopsy
threshold higher than 2% for the aging population.",['Ping Ren'],"['stat.ML', 'q-bio.QM']",2015-06-18 19:22:42+00:00
http://arxiv.org/abs/1506.05692v1,A hybrid algorithm for Bayesian network structure learning with application to multi-label learning,"We present a novel hybrid algorithm for Bayesian network structure learning,
called H2PC. It first reconstructs the skeleton of a Bayesian network and then
performs a Bayesian-scoring greedy hill-climbing search to orient the edges.
The algorithm is based on divide-and-conquer constraint-based subroutines to
learn the local structure around a target variable. We conduct two series of
experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is
currently the most powerful state-of-the-art algorithm for Bayesian network
structure learning. First, we use eight well-known Bayesian network benchmarks
with various data sizes to assess the quality of the learned structure returned
by the algorithms. Our extensive experiments show that H2PC outperforms MMHC in
terms of goodness of fit to new data and quality of the network structure with
respect to the true dependence structure of the data. Second, we investigate
H2PC's ability to solve the multi-label learning problem. We provide
theoretical results to characterize and identify graphically the so-called
minimal label powersets that appear as irreducible factors in the joint
distribution under the faithfulness condition. The multi-label learning problem
is then decomposed into a series of multi-class classification problems, where
each multi-class variable encodes a label powerset. H2PC is shown to compare
favorably to MMHC in terms of global classification accuracy over ten
multi-label data sets covering different application domains. Overall, our
experiments support the conclusions that local structural learning with H2PC in
the form of local neighborhood induction is a theoretically well-motivated and
empirically effective learning framework that is well suited to multi-label
learning. The source code (in R) of H2PC as well as all data sets used for the
empirical tests are publicly available.","['Maxime Gasse', 'Alex Aussem', 'Haytham Elghazel']","['stat.ML', 'cs.AI', 'cs.LG']",2015-06-18 14:24:19+00:00
http://arxiv.org/abs/1506.05666v2,Simultaneous Estimation of Non-Gaussian Components and their Correlation Structure,"The statistical dependencies which independent component analysis (ICA)
cannot remove often provide rich information beyond the linear independent
components. It would thus be very useful to estimate the dependency structure
from data. While such models have been proposed, they usually concentrated on
higher-order correlations such as energy (square) correlations. Yet, linear
correlations are a most fundamental and informative form of dependency in many
real data sets. Linear correlations are usually completely removed by ICA and
related methods, so they can only be analyzed by developing new methods which
explicitly allow for linearly correlated components. In this paper, we propose
a probabilistic model of linear non-Gaussian components which are allowed to
have both linear and energy correlations. The precision matrix of the linear
components is assumed to be randomly generated by a higher-order process and
explicitly parametrized by a parameter matrix. The estimation of the parameter
matrix is shown to be particularly simple because using score matching, the
objective function is a quadratic form. Using simulations with artificial data,
we demonstrate that the proposed method improves identifiability of
non-Gaussian components by simultaneously learning their correlation structure.
Applications on simulated complex cells with natural image input, as well as
spectrograms of natural audio data show that the method finds new kinds of
dependencies between the components.","['Hiroaki Sasaki', 'Michael U. Gutmann', 'Hayaru Shouno', 'Aapo Hyvärinen']",['stat.ML'],2015-06-18 13:18:15+00:00
http://arxiv.org/abs/1506.05600v3,Causality on Cross-Sectional Data: Stable Specification Search in Constrained Structural Equation Modeling,"Causal modeling has long been an attractive topic for many researchers and in
recent decades there has seen a surge in theoretical development and discovery
algorithms. Generally discovery algorithms can be divided into two approaches:
constraint-based and score-based. The constraint-based approach is able to
detect common causes of the observed variables but the use of independence
tests makes it less reliable. The score-based approach produces a result that
is easier to interpret as it also measures the reliability of the inferred
causal relationships, but it is unable to detect common confounders of the
observed variables. A drawback of both score-based and constrained-based
approaches is the inherent instability in structure estimation. With finite
samples small changes in the data can lead to completely different optimal
structures. The present work introduces a new hypothesis-free score-based
causal discovery algorithm, called stable specification search, that is robust
for finite samples based on recent advances in stability selection using
subsampling and selection algorithms. Structure search is performed over
Structural Equation Models. Our approach uses exploratory search but allows
incorporation of prior background knowledge. We validated our approach on one
simulated data set, which we compare to the known ground truth, and two
real-world data sets for Chronic Fatigue Syndrome and Attention Deficit
Hyperactivity Disorder, which we compare to earlier medical studies. The
results on the simulated data set show significant improvement over alternative
approaches and the results on the real-word data sets show consistency with the
hypothesis driven models constructed by medical experts.","['Ridho Rahmadi', 'Perry Groot', 'Marianne Heins', 'Hans Knoop', 'Tom Heskes']","['stat.ML', 'cs.LG']",2015-06-18 09:33:10+00:00
http://arxiv.org/abs/1506.05555v5,Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with Random Bases,"For big data analysis, high computational cost for Bayesian methods often
limits their applications in practice. In recent years, there have been many
attempts to improve computational efficiency of Bayesian inference. Here we
propose an efficient and scalable computational technique for a
state-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian
Monte Carlo (HMC). The key idea is to explore and exploit the structure and
regularity in parameter space for the underlying probabilistic model to
construct an effective approximation of its geometric properties. To this end,
we build a surrogate function to approximate the target distribution using
properly chosen random bases and an efficient optimization process. The
resulting method provides a flexible, scalable, and efficient sampling
algorithm, which converges to the correct target distribution. We show that by
choosing the basis functions and optimization process differently, our method
can be related to other approaches for the construction of surrogate functions
such as generalized additive models or Gaussian process models. Experiments
based on simulated and real data show that our approach leads to substantially
more efficient sampling algorithms compared to existing state-of-the art
methods.","['Cheng Zhang', 'Babak Shahbaba', 'Hongkai Zhao']","['stat.CO', 'stat.ML']",2015-06-18 06:25:59+00:00
http://arxiv.org/abs/1506.05446v2,Communication-Efficient False Discovery Rate Control via Knockoff Aggregation,"The false discovery rate (FDR)---the expected fraction of spurious
discoveries among all the discoveries---provides a popular statistical
assessment of the reproducibility of scientific studies in various disciplines.
In this work, we introduce a new method for controlling the FDR in
meta-analysis of many decentralized linear models. Our method targets the
scenario where many research groups---possibly the number of which is
random---are independently testing a common set of hypotheses and then sending
summary statistics to a coordinating center in an online manner. Built on the
knockoffs framework introduced by Barber and Candes (2015), our procedure
starts by applying the knockoff filter to each linear model and then aggregates
the summary statistics via one-shot communication in a novel way. This method
gives exact FDR control non-asymptotically without any knowledge of the noise
variances or making any assumption about sparsity of the signal. In certain
settings, it has a communication complexity that is optimal up to a logarithmic
factor.","['Weijie Su', 'Junyang Qian', 'Linxi Liu']","['stat.ML', 'stat.ME']",2015-06-17 19:56:59+00:00
http://arxiv.org/abs/1506.05439v3,Learning with a Wasserstein Loss,"Learning to predict multi-label outputs is challenging, but in many problems
there is a natural metric on the outputs that can be used to improve
predictions. In this paper we develop a loss function for multi-label learning,
based on the Wasserstein distance. The Wasserstein distance provides a natural
notion of dissimilarity for probability measures. Although optimizing with
respect to the exact Wasserstein distance is costly, recent work has described
a regularized approximation that is efficiently computed. We describe an
efficient learning algorithm based on this regularization, as well as a novel
extension of the Wasserstein distance from probability measures to unnormalized
measures. We also describe a statistical learning bound for the loss. The
Wasserstein loss can encourage smoothness of the predictions with respect to a
chosen metric on the output space. We demonstrate this property on a real-data
tag prediction problem, using the Yahoo Flickr Creative Commons dataset,
outperforming a baseline that doesn't use the metric.","['Charlie Frogner', 'Chiyuan Zhang', 'Hossein Mobahi', 'Mauricio Araya-Polo', 'Tomaso Poggio']","['cs.LG', 'cs.CV', 'stat.ML']",2015-06-17 19:36:41+00:00
http://arxiv.org/abs/1506.05244v3,Detection of Epigenomic Network Community Oncomarkers,"In this paper we propose network methodology to infer prognostic cancer
biomarkers based on the epigenetic pattern DNA methylation. Epigenetic
processes such as DNA methylation reflect environmental risk factors, and are
increasingly recognised for their fundamental role in diseases such as cancer.
DNA methylation is a gene-regulatory pattern, and hence provides a means by
which to assess genomic regulatory interactions. Network models are a natural
way to represent and analyse groups of such interactions. The utility of
network models also increases as the quantity of data and number of variables
increase, making them increasingly relevant to large-scale genomic studies. We
propose methodology to infer prognostic genomic networks from a DNA
methylation-based measure of genomic interaction and association. We then show
how to identify prognostic biomarkers from such networks, which we term
`network community oncomarkers'. We illustrate the power of our proposed
methodology in the context of a large publicly available breast cancer dataset.","['Thomas E. Bartlett', 'Alexey Zaikin']","['stat.AP', 'q-bio.GN', 'q-bio.MN', 'stat.ML']",2015-06-17 08:48:27+00:00
http://arxiv.org/abs/1506.05215v1,Robust Estimation of Structured Covariance Matrix for Heavy-Tailed Elliptical Distributions,"This paper considers the problem of robustly estimating a structured
covariance matrix with an elliptical underlying distribution with known mean.
In applications where the covariance matrix naturally possesses a certain
structure, taking the prior structure information into account in the
estimation procedure is beneficial to improve the estimation accuracy. We
propose incorporating the prior structure information into Tyler's M-estimator
and formulate the problem as minimizing the cost function of Tyler's estimator
under the prior structural constraint. First, the estimation under a general
convex structural constraint is introduced with an efficient algorithm for
finding the estimator derived based on the majorization minimization (MM)
algorithm framework. Then, the algorithm is tailored to several special
structures that enjoy a wide range of applications in signal processing related
fields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitz
structure. In addition, two types of non-convex structures, i.e., the Kronecker
structure and the spiked covariance structure, are also discussed, where it is
shown that simple algorithms can be derived under the guidelines of MM.
Numerical results show that the proposed estimator achieves a smaller
estimation error than the benchmark estimators at a lower computational cost.","['Ying Sun', 'Prabhu Babu', 'Daniel P. Palomar']","['stat.AP', 'stat.ML']",2015-06-17 06:17:26+00:00
http://arxiv.org/abs/1506.05173v2,Feature Selection for Ridge Regression with Provable Guarantees,"We introduce single-set spectral sparsification as a deterministic sampling
based feature selection technique for regularized least squares classification,
which is the classification analogue to ridge regression. The method is
unsupervised and gives worst-case guarantees of the generalization power of the
classification function after feature selection with respect to the
classification function obtained using all features. We also introduce
leverage-score sampling as an unsupervised randomized feature selection method
for ridge regression. We provide risk bounds for both single-set spectral
sparsification and leverage-score sampling on ridge regression in the fixed
design setting and show that the risk in the sampled space is comparable to the
risk in the full-feature space. We perform experiments on synthetic and
real-world datasets, namely a subset of TechTC-300 datasets, to support our
theory. Experimental results indicate that the proposed methods perform better
than the existing feature selection methods.","['Saurabh Paul', 'Petros Drineas']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2015-06-17 00:05:04+00:00
http://arxiv.org/abs/1506.05011v4,Bayesian representation learning with oracle constraints,"Representation learning systems typically rely on massive amounts of labeled
data in order to be trained to high accuracy. Recently, high-dimensional
parametric models like neural networks have succeeded in building rich
representations using either compressive, reconstructive or supervised
criteria. However, the semantic structure inherent in observations is
oftentimes lost in the process. Human perception excels at understanding
semantics but cannot always be expressed in terms of labels. Thus,
\emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing,
are often employed to generate similarity constraints using an implicit
similarity function encoded in human perception. In this work we propose to
combine \emph{generative unsupervised feature learning} with a
\emph{probabilistic treatment of oracle information like triplets} in order to
transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian
latent factor models of the observations. We use a fast variational algorithm
to learn the joint model and demonstrate applicability to a well-known image
dataset. We show how implicit triplet information can provide rich information
to learn representations that outperform previous metric learning approaches as
well as generative models without this side-information in a variety of
predictive tasks. In addition, we illustrate that the proposed approach
compartmentalizes the latent spaces semantically which allows interpretation of
the latent variables.","['Theofanis Karaletsos', 'Serge Belongie', 'Gunnar Rätsch']","['stat.ML', 'cs.CV', 'cs.LG']",2015-06-16 15:54:59+00:00
http://arxiv.org/abs/1506.04855v2,PCA with Gaussian perturbations,"Most of machine learning deals with vector parameters. Ideally we would like
to take higher order information into account and make use of matrix or even
tensor parameters. However the resulting algorithms are usually inefficient.
Here we address on-line learning with matrix parameters. It is often easy to
obtain online algorithm with good generalization performance if you
eigendecompose the current parameter matrix in each trial (at a cost of
$O(n^3)$ per trial). Ideally we want to avoid the decompositions and spend
$O(n^2)$ per trial, i.e. linear time in the size of the matrix data. There is a
core trade-off between the running time and the generalization performance,
here measured by the regret of the on-line algorithm (total gain of the best
off-line predictor minus the total gain of the on-line algorithm). We focus on
the key matrix problem of rank $k$ Principal Component Analysis in
$\mathbb{R}^n$ where $k \ll n$. There are $O(n^3)$ algorithms that achieve the
optimum regret but require eigendecompositions. We develop a simple algorithm
that needs $O(kn^2)$ per trial whose regret is off by a small factor of
$O(n^{1/4})$. The algorithm is based on the Follow the Perturbed Leader
paradigm. It replaces full eigendecompositions at each trial by the problem
finding $k$ principal components of the current covariance matrix that is
perturbed by Gaussian noise.","['Wojciech Kotłowski', 'Manfred K. Warmuth']","['cs.LG', 'stat.ML']",2015-06-16 07:04:19+00:00
http://arxiv.org/abs/1506.04838v1,Spectral Sparsification and Regret Minimization Beyond Matrix Multiplicative Updates,"In this paper, we provide a novel construction of the linear-sized spectral
sparsifiers of Batson, Spielman and Srivastava [BSS14]. While previous
constructions required $\Omega(n^4)$ running time [BSS14, Zou12], our
sparsification routine can be implemented in almost-quadratic running time
$O(n^{2+\varepsilon})$.
  The fundamental conceptual novelty of our work is the leveraging of a strong
connection between sparsification and a regret minimization problem over
density matrices. This connection was known to provide an interpretation of the
randomized sparsifiers of Spielman and Srivastava [SS11] via the application of
matrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, we
explain how matrix MWU naturally arises as an instance of the
Follow-the-Regularized-Leader framework and generalize this approach to yield a
larger class of updates. This new class allows us to accelerate the
construction of linear-sized spectral sparsifiers, and give novel insights on
the motivation behind Batson, Spielman and Srivastava [BSS14].","['Zeyuan Allen-Zhu', 'Zhenyu Liao', 'Lorenzo Orecchia']","['cs.LG', 'cs.DS', 'math.OC', 'stat.ML']",2015-06-16 05:31:57+00:00
http://arxiv.org/abs/1506.04744v1,Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game,"Interpersonal relations are fickle, with close friendships often dissolving
into enmity. In this work, we explore linguistic cues that presage such
transitions by studying dyadic interactions in an online strategy game where
players form alliances and break those alliances through betrayal. We
characterize friendships that are unlikely to last and examine temporal
patterns that foretell betrayal.
  We reveal that subtle signs of imminent betrayal are encoded in the
conversational patterns of the dyad, even if the victim is not aware of the
relationship's fate. In particular, we find that lasting friendships exhibit a
form of balance that manifests itself through language. In contrast, sudden
changes in the balance of certain conversational attributes---such as positive
sentiment, politeness, or focus on future planning---signal impending betrayal.","['Vlad Niculae', 'Srijan Kumar', 'Jordan Boyd-Graber', 'Cristian Danescu-Niculescu-Mizil']","['cs.CL', 'cs.AI', 'cs.SI', 'physics.soc-ph', 'stat.ML']",2015-06-15 20:00:29+00:00
http://arxiv.org/abs/1506.04725v1,Fast Two-Sample Testing with Analytic Representations of Probability Measures,"We propose a class of nonparametric two-sample tests with a cost linear in
the sample size. Two tests are given, both based on an ensemble of distances
between analytic functions representing each of the distributions. The first
test uses smoothed empirical characteristic functions to represent the
distributions, the second uses distribution embeddings in a reproducing kernel
Hilbert space. Analyticity implies that differences in the distributions may be
detected almost surely at a finite number of randomly chosen
locations/frequencies. The new tests are consistent against a larger class of
alternatives than the previous linear-time tests based on the (non-smoothed)
empirical characteristic functions, while being much faster than the current
state-of-the-art quadratic-time kernel-based or energy distance-based tests.
Experiments on artificial benchmarks and on challenging real-world testing
problems demonstrate that our tests give a better power/time tradeoff than
competing approaches, and in some cases, better outright power than even the
most expensive quadratic-time tests. This performance advantage is retained
even in high dimensions, and in cases where the difference in distributions is
not observable with low order statistics.","['Kacper Chwialkowski', 'Aaditya Ramdas', 'Dino Sejdinovic', 'Arthur Gretton']","['stat.ML', '62G10', 'G.3']",2015-06-15 19:42:45+00:00
