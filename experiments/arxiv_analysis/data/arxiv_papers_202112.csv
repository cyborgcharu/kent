id,title,abstract,authors,categories,date
http://arxiv.org/abs/2201.09818v1,Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise,"We give tight statistical query (SQ) lower bounds for learnining halfspaces
in the presence of Massart noise. In particular, suppose that all labels are
corrupted with probability at most $\eta$. We show that for arbitrary $\eta \in
[0,1/2]$ every SQ algorithm achieving misclassification error better than
$\eta$ requires queries of superpolynomial accuracy or at least a
superpolynomial number of queries. Further, this continues to hold even if the
information-theoretically optimal error $\mathrm{OPT}$ is as small as
$\exp\left(-\log^c(d)\right)$, where $d$ is the dimension and $0 < c < 1$ is an
arbitrary absolute constant, and an overwhelming fraction of examples are
noiseless. Our lower bound matches known polynomial time algorithms, which are
also implementable in the SQ framework. Previously, such lower bounds only
ruled out algorithms achieving error $\mathrm{OPT} + \epsilon$ or error better
than $\Omega(\eta)$ or, if $\eta$ is close to $1/2$, error $\eta - o_\eta(1)$,
where the term $o_\eta(1)$ is constant in $d$ but going to 0 for $\eta$
approaching $1/2$.
  As a consequence, we also show that achieving misclassification error better
than $1/2$ in the $(A,\alpha)$-Tsybakov model is SQ-hard for $A$ constant and
$\alpha$ bounded away from 1.","['Rajai Nasser', 'Stefan Tiegel']","['cs.LG', 'cs.CC', 'math.ST', 'stat.ML', 'stat.TH']",2022-01-24 17:33:19+00:00
http://arxiv.org/abs/2201.09644v2,Multiscale Generative Models: Improving Performance of a Generative Model Using Feedback from Other Dependent Generative Models,"Realistic fine-grained multi-agent simulation of real-world complex systems
is crucial for many downstream tasks such as reinforcement learning. Recent
work has used generative models (GANs in particular) for providing
high-fidelity simulation of real-world systems. However, such generative models
are often monolithic and miss out on modeling the interaction in multi-agent
systems. In this work, we take a first step towards building multiple
interacting generative models (GANs) that reflects the interaction in real
world. We build and analyze a hierarchical set-up where a higher-level GAN is
conditioned on the output of multiple lower-level GANs. We present a technique
of using feedback from the higher-level GAN to improve performance of
lower-level GANs. We mathematically characterize the conditions under which our
technique is impactful, including understanding the transfer learning nature of
our set-up. We present three distinct experiments on synthetic data, time
series data, and image domain, revealing the wide applicability of our
technique.","['Changyu Chen', 'Avinandan Bose', 'Shih-Fen Cheng', 'Arunesh Sinha']","['cs.LG', 'stat.ML']",2022-01-24 13:05:56+00:00
http://arxiv.org/abs/2201.09485v1,Spherical Poisson Point Process Intensity Function Modeling and Estimation with Measure Transport,"Recent years have seen an increased interest in the application of methods
and techniques commonly associated with machine learning and artificial
intelligence to spatial statistics. Here, in a celebration of the ten-year
anniversary of the journal Spatial Statistics, we bring together normalizing
flows, commonly used for density function estimation in machine learning, and
spherical point processes, a topic of particular interest to the journal's
readership, to present a new approach for modeling non-homogeneous Poisson
process intensity functions on the sphere. The central idea of this framework
is to build, and estimate, a flexible bijective map that transforms the
underlying intensity function of interest on the sphere into a simpler,
reference, intensity function, also on the sphere. Map estimation can be done
efficiently using automatic differentiation and stochastic gradient descent,
and uncertainty quantification can be done straightforwardly via nonparametric
bootstrap. We investigate the viability of the proposed method in a simulation
study, and illustrate its use in a proof-of-concept study where we model the
intensity of cyclone events in the North Pacific Ocean. Our experiments reveal
that normalizing flows present a flexible and straightforward way to model
intensity functions on spheres, but that their potential to yield a good fit
depends on the architecture of the bijective map, which can be difficult to
establish in practice.","['Tin Lok James Ng', 'Andrew Zammit-Mangion']","['stat.ME', 'stat.ML']",2022-01-24 06:46:22+00:00
http://arxiv.org/abs/2201.09483v2,A Machine Learning Framework for Distributed Functional Compression over Wireless Channels in IoT,"IoT devices generating enormous data and state-of-the-art machine learning
techniques together will revolutionize cyber-physical systems. In many diverse
fields, from autonomous driving to augmented reality, distributed IoT devices
compute specific target functions without simple forms like obstacle detection,
object recognition, etc. Traditional cloud-based methods that focus on
transferring data to a central location either for training or inference place
enormous strain on network resources. To address this, we develop, to the best
of our knowledge, the first machine learning framework for distributed
functional compression over both the Gaussian Multiple Access Channel (GMAC)
and orthogonal AWGN channels. Due to the Kolmogorov-Arnold representation
theorem, our machine learning framework can, by design, compute any arbitrary
function for the desired functional compression task in IoT. Importantly the
raw sensory data are never transferred to a central node for training or
inference, thus reducing communication. For these algorithms, we provide
theoretical convergence guarantees and upper bounds on communication. Our
simulations show that the learned encoders and decoders for functional
compression perform significantly better than traditional approaches, are
robust to channel condition changes and sensor outages. Compared to the
cloud-based scenario, our algorithms reduce channel use by two orders of
magnitude.","['Yashas Malur Saidutta', 'Afshin Abdi', 'Faramarz Fekri']","['cs.LG', 'cs.DC', 'cs.IT', 'eess.SP', 'math.IT', 'stat.ML']",2022-01-24 06:38:39+00:00
http://arxiv.org/abs/2201.09460v1,Probability Distribution on Rooted Trees,"The hierarchical and recursive expressive capability of rooted trees is
applicable to represent statistical models in various areas, such as data
compression, image processing, and machine learning. On the other hand, such
hierarchical expressive capability causes a problem in tree selection to avoid
overfitting. One unified approach to solve this is a Bayesian approach, on
which the rooted tree is regarded as a random variable and a direct loss
function can be assumed on the selected model or the predicted value for a new
data point. However, all the previous studies on this approach are based on the
probability distribution on full trees, to the best of our knowledge. In this
paper, we propose a generalized probability distribution for any rooted trees
in which only the maximum number of child nodes and the maximum depth are
fixed. Furthermore, we derive recursive methods to evaluate the characteristics
of the probability distribution without any approximations.","['Yuta Nakahara', 'Shota Saito', 'Akira Kamatsuka', 'Toshiyasu Matsushima']","['cs.LG', 'stat.ML']",2022-01-24 05:13:58+00:00
http://arxiv.org/abs/2201.09433v2,Active Learning Polynomial Threshold Functions,"We initiate the study of active learning polynomial threshold functions
(PTFs). While traditional lower bounds imply that even univariate quadratics
cannot be non-trivially actively learned, we show that allowing the learner
basic access to the derivatives of the underlying classifier circumvents this
issue and leads to a computationally efficient algorithm for active learning
degree-$d$ univariate PTFs in $\tilde{O}(d^3\log(1/\varepsilon\delta))$
queries. We also provide near-optimal algorithms and analyses for active
learning PTFs in several average case settings. Finally, we prove that access
to derivatives is insufficient for active learning multivariate PTFs, even
those of just two variables.","['Omri Ben-Eliezer', 'Max Hopkins', 'Chutong Yang', 'Hantao Yu']","['cs.LG', 'cs.CC', 'stat.ML', '68Q32']",2022-01-24 03:23:41+00:00
http://arxiv.org/abs/2201.09418v3,Approximation bounds for norm constrained neural networks with applications to regression and GANs,"This paper studies the approximation capacity of ReLU neural networks with
norm constraint on the weights. We prove upper and lower bounds on the
approximation error of these networks for smooth function classes. The lower
bound is derived through the Rademacher complexity of neural networks, which
may be of independent interest. We apply these approximation bounds to analyze
the convergences of regression using norm constrained neural networks and
distribution estimation by GANs. In particular, we obtain convergence rates for
over-parameterized neural networks. It is also shown that GANs can achieve
optimal rate of learning probability distributions, when the discriminator is a
properly chosen norm constrained neural network.","['Yuling Jiao', 'Yang Wang', 'Yunfei Yang']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2022-01-24 02:19:05+00:00
http://arxiv.org/abs/2201.09345v2,Machine Learning Symmetry,"We review recent work in machine learning aspects of conformal field theory
and Lie algebra representation theory using neural networks.",['Shailesh Lal'],"['hep-th', 'math-ph', 'math.MP', 'stat.ML']",2022-01-23 19:09:22+00:00
http://arxiv.org/abs/2201.09320v1,Robust Wavelet-based Assessment of Scaling with Applications,"A number of approaches have dealt with statistical assessment of
self-similarity, and many of those are based on multiscale concepts. Most rely
on certain distributional assumptions which are usually violated by real data
traces, often characterized by large temporal or spatial mean level shifts,
missing values or extreme observations. A novel, robust approach based on
Theil-type weighted regression is proposed for estimating self-similarity in
two-dimensional data (images). The method is compared to two traditional
estimation techniques that use wavelet decompositions; ordinary least squares
(OLS) and Abry-Veitch bias correcting estimator (AV). As an application, the
suitability of the self-similarity estimate resulting from the the robust
approach is illustrated as a predictive feature in the classification of
digitized mammogram images as cancerous or non-cancerous. The diagnostic
employed here is based on the properties of image backgrounds, which is
typically an unused modality in breast cancer screening. Classification results
show nearly 68% accuracy, varying slightly with the choice of wavelet basis,
and the range of multiresolution levels used.","['Erin K. Hamilton', 'Seonghye Jeon', 'Pepa Ramirez Cobo', 'Kichun Sky Lee', 'Brani Vidakovic']","['stat.ME', 'stat.ML']",2022-01-23 17:10:26+00:00
http://arxiv.org/abs/2201.09286v2,How to scale hyperparameters for quickshift image segmentation,"Quickshift is a popular algorithm for image segmentation, used as a
preprocessing step in many applications. Unfortunately, it is quite challenging
to understand the hyperparameters' influence on the number and shape of
superpixels produced by the method. In this paper, we study theoretically a
slightly modified version of the quickshift algorithm, with a particular
emphasis on homogeneous image patches with i.i.d. pixel noise and sharp
boundaries between such patches. Leveraging this analysis, we derive a simple
heuristic to scale quickshift hyperparameters with respect to the image size,
which we check empirically.",['Damien Garreau'],"['cs.CV', 'stat.ML']",2022-01-23 15:05:54+00:00
http://arxiv.org/abs/2201.09267v1,"Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey","This is a tutorial and survey paper on metric learning. Algorithms are
divided into spectral, probabilistic, and deep metric learning. We first start
with the definition of distance metric, Mahalanobis distance, and generalized
Mahalanobis distance. In spectral methods, we start with methods using scatters
of data, including the first spectral metric learning, relevant methods to
Fisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant
Component Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric
learning, imbalanced metric learning, locally linear metric adaptation, and
adversarial metric learning are covered. We also explain several kernel
spectral methods for metric learning in the feature space. We also introduce
geometric metric learning methods on the Riemannian manifolds. In probabilistic
methods, we start with collapsing classes in both input and feature spaces and
then explain the neighborhood component analysis methods, Bayesian metric
learning, information theoretic methods, and empirical risk minimization in
metric learning. In deep learning methods, we first introduce reconstruction
autoencoders and supervised loss functions for metric learning. Then, Siamese
networks and its various loss functions, triplet mining, and triplet sampling
are explained. Deep discriminant analysis methods, based on Fisher discriminant
analysis, are also reviewed. Finally, we introduce multi-modal deep metric
learning, geometric metric learning by neural networks, and few-shot metric
learning.","['Benyamin Ghojogh', 'Ali Ghodsi', 'Fakhri Karray', 'Mark Crowley']","['stat.ML', 'cs.CV', 'cs.LG']",2022-01-23 13:53:23+00:00
http://arxiv.org/abs/2201.09209v2,Weight Expansion: A New Perspective on Dropout and Generalization,"While dropout is known to be a successful regularization technique, insights
into the mechanisms that lead to this success are still lacking. We introduce
the concept of \emph{weight expansion}, an increase in the signed volume of a
parallelotope spanned by the column or row vectors of the weight covariance
matrix, and show that weight expansion is an effective means of increasing the
generalization in a PAC-Bayesian setting. We provide a theoretical argument
that dropout leads to weight expansion and extensive empirical support for the
correlation between dropout and weight expansion. To support our hypothesis
that weight expansion can be regarded as an \emph{indicator} of the enhanced
generalization capability endowed by dropout, and not just as a mere
by-product, we have studied other methods that achieve weight expansion (resp.\
contraction), and found that they generally lead to an increased (resp.\
decreased) generalization ability. This suggests that dropout is an attractive
regularizer, because it is a computationally cheap method for obtaining weight
expansion. This insight justifies the role of dropout as a regularizer, while
paving the way for identifying regularizers that promise improved
generalization through weight expansion.","['Gaojie Jin', 'Xinping Yi', 'Pengfei Yang', 'Lijun Zhang', 'Sven Schewe', 'Xiaowei Huang']","['cs.LG', 'stat.ML']",2022-01-23 09:03:22+00:00
http://arxiv.org/abs/2201.09172v1,An Attention-based ConvLSTM Autoencoder with Dynamic Thresholding for Unsupervised Anomaly Detection in Multivariate Time Series,"As a substantial amount of multivariate time series data is being produced by
the complex systems in Smart Manufacturing, improved anomaly detection
frameworks are needed to reduce the operational risks and the monitoring burden
placed on the system operators. However, building such frameworks is
challenging, as a sufficiently large amount of defective training data is often
not available and frameworks are required to capture both the temporal and
contextual dependencies across different time steps while being robust to
noise. In this paper, we propose an unsupervised Attention-based Convolutional
Long Short-Term Memory (ConvLSTM) Autoencoder with Dynamic Thresholding
(ACLAE-DT) framework for anomaly detection and diagnosis in multivariate time
series. The framework starts by pre-processing and enriching the data, before
constructing feature images to characterize the system statuses across
different time steps by capturing the inter-correlations between pairs of time
series. Afterwards, the constructed feature images are fed into an
attention-based ConvLSTM autoencoder, which aims to encode the constructed
feature images and capture the temporal behavior, followed by decoding the
compressed knowledge representation to reconstruct the feature images input.
The reconstruction errors are then computed and subjected to a
statistical-based, dynamic thresholding mechanism to detect and diagnose the
anomalies. Evaluation results conducted on real-life manufacturing data
demonstrate the performance strengths of the proposed approach over
state-of-the-art methods under different experimental settings.","['Tareq Tayeh', 'Sulaiman Aburakhia', 'Ryan Myers', 'Abdallah Shami']","['cs.LG', 'cs.SE', 'stat.ML']",2022-01-23 04:01:43+00:00
http://arxiv.org/abs/2201.09119v1,A Causal Lens for Controllable Text Generation,"Controllable text generation concerns two fundamental tasks of wide
applications, namely generating text of given attributes (i.e.,
attribute-conditional generation), and minimally editing existing text to
possess desired attributes (i.e., text attribute transfer). Extensive prior
work has largely studied the two problems separately, and developed different
conditional models which, however, are prone to producing biased text (e.g.,
various gender stereotypes). This paper proposes to formulate controllable text
generation from a principled causal perspective which models the two tasks with
a unified framework. A direct advantage of the causal formulation is the use of
rich causality tools to mitigate generation biases and improve control. We
treat the two tasks as interventional and counterfactual causal inference based
on a structural causal model, respectively. We then apply the framework to the
challenging practical setting where confounding factors (that induce spurious
correlations) are observable only on a small fraction of data. Experiments show
significant superiority of the causal approach over previous conditional models
for improved control accuracy and reduced bias.","['Zhiting Hu', 'Li Erran Li']","['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']",2022-01-22 19:31:43+00:00
http://arxiv.org/abs/2201.09040v1,Optimal Estimation and Computational Limit of Low-rank Gaussian Mixtures,"Structural matrix-variate observations routinely arise in diverse fields such
as multi-layer network analysis and brain image clustering. While data of this
type have been extensively investigated with fruitful outcomes being delivered,
the fundamental questions like its statistical optimality and computational
limit are largely under-explored. In this paper, we propose a low-rank Gaussian
mixture model (LrMM) assuming each matrix-valued observation has a planted
low-rank structure. Minimax lower bounds for estimating the underlying low-rank
matrix are established allowing a whole range of sample sizes and signal
strength. Under a minimal condition on signal strength, referred to as the
information-theoretical limit or statistical limit, we prove the minimax
optimality of a maximum likelihood estimator which, in general, is
computationally infeasible. If the signal is stronger than a certain threshold,
called the computational limit, we design a computationally fast estimator
based on spectral aggregation and demonstrate its minimax optimality. Moreover,
when the signal strength is smaller than the computational limit, we provide
evidences based on the low-degree likelihood ratio framework to claim that no
polynomial-time algorithm can consistently recover the underlying low-rank
matrix. Our results reveal multiple phase transitions in the minimax error
rates and the statistical-to-computational gap. Numerical experiments confirm
our theoretical findings. We further showcase the merit of our spectral
aggregation method on the worldwide food trading dataset.","['Zhongyuan Lyu', 'Dong Xia']","['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']",2022-01-22 12:43:25+00:00
http://arxiv.org/abs/2201.08956v1,The Many Faces of Adversarial Risk,"Adversarial risk quantifies the performance of classifiers on adversarially
perturbed data. Numerous definitions of adversarial risk -- not all
mathematically rigorous and differing subtly in the details -- have appeared in
the literature. In this paper, we revisit these definitions, make them
rigorous, and critically examine their similarities and differences. Our
technical tools derive from optimal transport, robust statistics, functional
analysis, and game theory. Our contributions include the following:
generalizing Strassen's theorem to the unbalanced optimal transport setting
with applications to adversarial classification with unequal priors; showing an
equivalence between adversarial robustness and robust hypothesis testing with
$\infty$-Wasserstein uncertainty sets; proving the existence of a pure Nash
equilibrium in the two-player game between the adversary and the algorithm; and
characterizing adversarial risk by the minimum Bayes error between a pair of
distributions belonging to the $\infty$-Wasserstein uncertainty sets. Our
results generalize and deepen recently discovered connections between optimal
transport and adversarial robustness and reveal new connections to Choquet
capacities and game theory.","['Muni Sreenivas Pydi', 'Varun Jog']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2022-01-22 03:05:09+00:00
http://arxiv.org/abs/2201.08932v2,Overcoming Oversmoothness in Graph Convolutional Networks via Hybrid Scattering Networks,"Geometric deep learning has made great strides towards generalizing the
design of structure-aware neural networks from traditional domains to
non-Euclidean ones, giving rise to graph neural networks (GNN) that can be
applied to graph-structured data arising in, e.g., social networks,
biochemistry, and material science. Graph convolutional networks (GCNs) in
particular, inspired by their Euclidean counterparts, have been successful in
processing graph data by extracting structure-aware features. However, current
GNN models are often constrained by various phenomena that limit their
expressive power and ability to generalize to more complex graph datasets. Most
models essentially rely on low-pass filtering of graph signals via local
averaging operations, leading to oversmoothing. Moreover, to avoid severe
oversmoothing, most popular GCN-style networks tend to be shallow, with narrow
receptive fields, leading to underreaching. Here, we propose a hybrid GNN
framework that combines traditional GCN filters with band-pass filters defined
via geometric scattering. We further introduce an attention framework that
allows the model to locally attend over combined information from different
filters at the node level. Our theoretical results establish the complementary
benefits of the scattering filters to leverage structural information from the
graph, while our experiments show the benefits of our method on various
learning tasks.","['Frederik Wenkel', 'Yimeng Min', 'Matthew Hirn', 'Michael Perlmutter', 'Guy Wolf']","['stat.ML', 'cs.LG', '68T07']",2022-01-22 00:47:41+00:00
http://arxiv.org/abs/2201.08919v1,Recurrent Neural Networks with Mixed Hierarchical Structures and EM Algorithm for Natural Language Processing,"How to obtain hierarchical representations with an increasing level of
abstraction becomes one of the key issues of learning with deep neural
networks. A variety of RNN models have recently been proposed to incorporate
both explicit and implicit hierarchical information in modeling languages in
the literature. In this paper, we propose a novel approach called the latent
indicator layer to identify and learn implicit hierarchical information (e.g.,
phrases), and further develop an EM algorithm to handle the latent indicator
layer in training. The latent indicator layer further simplifies a text's
hierarchical structure, which allows us to seamlessly integrate different
levels of attention mechanisms into the structure. We called the resulting
architecture as the EM-HRNN model. Furthermore, we develop two bootstrap
strategies to effectively and efficiently train the EM-HRNN model on long text
documents. Simulation studies and real data applications demonstrate that the
EM-HRNN model with bootstrap training outperforms other RNN-based models in
document classification tasks. The performance of the EM-HRNN model is
comparable to a Transformer-based method called Bert-base, though the former is
much smaller model and does not require pre-training.","['Zhaoxin Luo', 'Michael Zhu']","['cs.CL', 'stat.ML']",2022-01-21 23:08:33+00:00
http://arxiv.org/abs/2201.08905v1,Optimal Dynamic Regret in Proper Online Learning with Strongly Convex Losses and Beyond,"We study the framework of universal dynamic regret minimization with strongly
convex losses. We answer an open problem in Baby and Wang 2021 by showing that
in a proper learning setup, Strongly Adaptive algorithms can achieve the near
optimal dynamic regret of $\tilde O(d^{1/3} n^{1/3}\text{TV}[u_{1:n}]^{2/3}
\vee d)$ against any comparator sequence $u_1,\ldots,u_n$ simultaneously, where
$n$ is the time horizon and $\text{TV}[u_{1:n}]$ is the Total Variation of
comparator. These results are facilitated by exploiting a number of new
structures imposed by the KKT conditions that were not considered in Baby and
Wang 2021 which also lead to other improvements over their results such as: (a)
handling non-smooth losses and (b) improving the dimension dependence on
regret. Further, we also derive near optimal dynamic regret rates for the
special case of proper online learning with exp-concave losses and an
$L_\infty$ constrained decision set.","['Dheeraj Baby', 'Yu-Xiang Wang']","['cs.LG', 'math.OC', 'stat.ML']",2022-01-21 22:08:07+00:00
http://arxiv.org/abs/2201.08903v1,Universal Online Learning with Unbounded Losses: Memory Is All You Need,"We resolve an open problem of Hanneke on the subject of universally
consistent online learning with non-i.i.d. processes and unbounded losses. The
notion of an optimistically universal learning rule was defined by Hanneke in
an effort to study learning theory under minimal assumptions. A given learning
rule is said to be optimistically universal if it achieves a low long-run
average loss whenever the data generating process makes this goal achievable by
some learning rule. Hanneke posed as an open problem whether, for every
unbounded loss, the family of processes admitting universal learning are
precisely those having a finite number of distinct values almost surely. In
this paper, we completely resolve this problem, showing that this is indeed the
case. As a consequence, this also offers a dramatically simpler formulation of
an optimistically universal learning rule for any unbounded loss: namely, the
simple memorization rule already suffices. Our proof relies on constructing
random measurable partitions of the instance space and could be of independent
interest for solving other open questions. We extend the results to the
non-realizable setting thereby providing an optimistically universal Bayes
consistent learning rule.","['Moise Blanchard', 'Romain Cosson', 'Steve Hanneke']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2022-01-21 22:03:18+00:00
http://arxiv.org/abs/2201.08862v3,Stochastic normalizing flows as non-equilibrium transformations,"Normalizing flows are a class of deep generative models that provide a
promising route to sample lattice field theories more efficiently than
conventional Monte Carlo simulations. In this work we show that the theoretical
framework of stochastic normalizing flows, in which neural-network layers are
combined with Monte Carlo updates, is the same that underlies
out-of-equilibrium simulations based on Jarzynski's equality, which have been
recently deployed to compute free-energy differences in lattice gauge theories.
We lay out a strategy to optimize the efficiency of this extended class of
generative models and present examples of applications.","['Michele Caselle', 'Elia Cellini', 'Alessandro Nada', 'Marco Panero']","['hep-lat', 'cond-mat.stat-mech', 'cs.LG', 'stat.ML']",2022-01-21 19:00:18+00:00
http://arxiv.org/abs/2201.08837v1,Marginal Effects for Non-Linear Prediction Functions,"Beta coefficients for linear regression models represent the ideal form of an
interpretable feature effect. However, for non-linear models and especially
generalized linear models, the estimated coefficients cannot be interpreted as
a direct feature effect on the predicted outcome. Hence, marginal effects are
typically used as approximations for feature effects, either in the shape of
derivatives of the prediction function or forward differences in prediction due
to a change in a feature value. While marginal effects are commonly used in
many scientific fields, they have not yet been adopted as a model-agnostic
interpretation method for machine learning models. This may stem from their
inflexibility as a univariate feature effect and their inability to deal with
the non-linearities found in black box models. We introduce a new class of
marginal effects termed forward marginal effects. We argue to abandon
derivatives in favor of better-interpretable forward differences. Furthermore,
we generalize marginal effects based on forward differences to multivariate
changes in feature values. To account for the non-linearity of prediction
functions, we introduce a non-linearity measure for marginal effects. We argue
against summarizing feature effects of a non-linear prediction function in a
single metric such as the average marginal effect. Instead, we propose to
partition the feature space to compute conditional average marginal effects on
feature subspaces, which serve as conditional feature effect estimates.","['Christian A. Scholbeck', 'Giuseppe Casalicchio', 'Christoph Molnar', 'Bernd Bischl', 'Christian Heumann']","['cs.LG', 'econ.EM', 'stat.AP', 'stat.ME', 'stat.ML']",2022-01-21 18:47:38+00:00
http://arxiv.org/abs/2201.08756v1,Tuned Regularized Estimators for Linear Regression via Covariance Fitting,"We consider the problem of finding tuned regularized parameter estimators for
linear models. We start by showing that three known optimal linear estimators
belong to a wider class of estimators that can be formulated as a solution to a
weighted and constrained minimization problem. The optimal weights, however,
are typically unknown in many applications. This begs the question, how should
we choose the weights using only the data? We propose using the covariance
fitting SPICE-methodology to obtain data-adaptive weights and show that the
resulting class of estimators yields tuned versions of known regularized
estimators - such as ridge regression, LASSO, and regularized least absolute
deviation. These theoretical results unify several important estimators under a
common umbrella. The resulting tuned estimators are also shown to be
practically relevant by means of a number of numerical examples.","['Per Mattsson', 'Dave Zachariah', 'Petre Stoica']","['math.ST', 'stat.ML', 'stat.TH']",2022-01-21 16:08:08+00:00
http://arxiv.org/abs/2201.08712v4,Improved Random Features for Dot Product Kernels,"Dot product kernels, such as polynomial and exponential (softmax) kernels,
are among the most widely used kernels in machine learning, as they enable
modeling the interactions between input features, which is crucial in
applications like computer vision, natural language processing, and recommender
systems. We make several novel contributions for improving the efficiency of
random feature approximations for dot product kernels, to make these kernels
more useful in large scale learning. First, we present a generalization of
existing random feature approximations for polynomial kernels, such as
Rademacher and Gaussian sketches and TensorSRHT, using complex-valued random
features. We show empirically that the use of complex features can
significantly reduce the variances of these approximations. Second, we provide
a theoretical analysis for understanding the factors affecting the efficiency
of various random feature approximations, by deriving closed-form expressions
for their variances. These variance formulas elucidate conditions under which
certain approximations (e.g., TensorSRHT) achieve lower variances than others
(e.g., Rademacher sketches), and conditions under which the use of complex
features leads to lower variances than real features. Third, by using these
variance formulas, which can be evaluated in practice, we develop a data-driven
optimization approach to improve random feature approximations for general dot
product kernels, which is also applicable to the Gaussian kernel. We describe
the improvements brought by these contributions with extensive experiments on a
variety of tasks and datasets.","['Jonas Wacker', 'Motonobu Kanagawa', 'Maurizio Filippone']","['stat.ML', 'cs.LG', 'stat.CO']",2022-01-21 14:16:56+00:00
http://arxiv.org/abs/2201.08676v1,Distance-Ratio-Based Formulation for Metric Learning,"In metric learning, the goal is to learn an embedding so that data points
with the same class are close to each other and data points with different
classes are far apart. We propose a distance-ratio-based (DR) formulation for
metric learning. Like softmax-based formulation for metric learning, it models
$p(y=c|x')$, which is a probability that a query point $x'$ belongs to a class
$c$. The DR formulation has two useful properties. First, the corresponding
loss is not affected by scale changes of an embedding. Second, it outputs the
optimal (maximum or minimum) classification confidence scores on representing
points for classes. To demonstrate the effectiveness of our formulation, we
conduct few-shot classification experiments using softmax-based and DR
formulations on CUB and mini-ImageNet datasets. The results show that DR
formulation generally enables faster and more stable metric learning than the
softmax-based formulation. As a result, using DR formulation achieves improved
or comparable generalization performances.","['Hyeongji Kim', 'Pekka Parviainen', 'Ketil Malde']","['cs.LG', 'cs.CV', 'stat.ML']",2022-01-21 12:45:23+00:00
http://arxiv.org/abs/2201.08652v1,A phase transition for finding needles in nonlinear haystacks with LASSO artificial neural networks,"To fit sparse linear associations, a LASSO sparsity inducing penalty with a
single hyperparameter provably allows to recover the important features
(needles) with high probability in certain regimes even if the sample size is
smaller than the dimension of the input vector (haystack). More recently
learners known as artificial neural networks (ANN) have shown great successes
in many machine learning tasks, in particular fitting nonlinear associations.
Small learning rate, stochastic gradient descent algorithm and large training
set help to cope with the explosion in the number of parameters present in deep
neural networks. Yet few ANN learners have been developed and studied to find
needles in nonlinear haystacks. Driven by a single hyperparameter, our ANN
learner, like for sparse linear associations, exhibits a phase transition in
the probability of retrieving the needles, which we do not observe with other
ANN learners. To select our penalty parameter, we generalize the universal
threshold of Donoho and Johnstone (1994) which is a better rule than the
conservative (too many false detections) and expensive cross-validation. In the
spirit of simulated annealing, we propose a warm-start sparsity inducing
algorithm to solve the high-dimensional, non-convex and non-differentiable
optimization problem. We perform precise Monte Carlo simulations to show the
effectiveness of our approach.","['Xiaoyu Ma', 'Sylvain Sardy', 'Nick Hengartner', 'Nikolai Bobenko', 'Yen Ting Lin']","['stat.ML', 'cs.LG']",2022-01-21 11:39:04+00:00
http://arxiv.org/abs/2201.08543v2,Deep Learning-Accelerated 3D Carbon Storage Reservoir Pressure Forecasting Based on Data Assimilation Using Surface Displacement from InSAR,"Fast forecasting of reservoir pressure distribution in geologic carbon
storage (GCS) by assimilating monitoring data is a challenging problem. Due to
high drilling cost, GCS projects usually have spatially sparse measurements
from wells, leading to high uncertainties in reservoir pressure prediction. To
address this challenge, we propose to use low-cost Interferometric
Synthetic-Aperture Radar (InSAR) data as monitoring data to infer reservoir
pressure build up. We develop a deep learning-accelerated workflow to
assimilate surface displacement maps interpreted from InSAR and to forecast
dynamic reservoir pressure. Employing an Ensemble Smoother Multiple Data
Assimilation (ES-MDA) framework, the workflow updates three-dimensional (3D)
geologic properties and predicts reservoir pressure with quantified
uncertainties. We use a synthetic commercial-scale GCS model with bimodally
distributed permeability and porosity to demonstrate the efficacy of the
workflow. A two-step CNN-PCA approach is employed to parameterize the bimodal
fields. The computational efficiency of the workflow is boosted by two residual
U-Net based surrogate models for surface displacement and reservoir pressure
predictions, respectively. The workflow can complete data assimilation and
reservoir pressure forecasting in half an hour on a personal computer.","['Hewei Tang', 'Pengcheng Fu', 'Honggeun Jo', 'Su Jiang', 'Christopher S. Sherman', 'François Hamon', 'Nicholas A. Azzolina', 'Joseph P. Morris']","['stat.ML', 'cs.LG', 'physics.geo-ph']",2022-01-21 05:17:08+00:00
http://arxiv.org/abs/2201.08536v1,Instance-Dependent Confidence and Early Stopping for Reinforcement Learning,"Various algorithms for reinforcement learning (RL) exhibit dramatic variation
in their convergence rates as a function of problem structure. Such
problem-dependent behavior is not captured by worst-case analyses and has
accordingly inspired a growing effort in obtaining instance-dependent
guarantees and deriving instance-optimal algorithms for RL problems. This
research has been carried out, however, primarily within the confines of
theory, providing guarantees that explain \textit{ex post} the performance
differences observed. A natural next step is to convert these theoretical
guarantees into guidelines that are useful in practice. We address the problem
of obtaining sharp instance-dependent confidence regions for the policy
evaluation problem and the optimal value estimation problem of an MDP, given
access to an instance-optimal algorithm. As a consequence, we propose a
data-dependent stopping rule for instance-optimal algorithms. The proposed
stopping rule adapts to the instance-specific difficulty of the problem and
allows for early termination for problems with favorable structure.","['Koulik Khamaru', 'Eric Xia', 'Martin J. Wainwright', 'Michael I. Jordan']","['stat.ML', 'cs.LG']",2022-01-21 04:25:35+00:00
http://arxiv.org/abs/2201.08530v1,Spatiotemporal Analysis Using Riemannian Composition of Diffusion Operators,"Multivariate time-series have become abundant in recent years, as many
data-acquisition systems record information through multiple sensors
simultaneously. In this paper, we assume the variables pertain to some geometry
and present an operator-based approach for spatiotemporal analysis. Our
approach combines three components that are often considered separately: (i)
manifold learning for building operators representing the geometry of the
variables, (ii) Riemannian geometry of symmetric positive-definite matrices for
multiscale composition of operators corresponding to different time samples,
and (iii) spectral analysis of the composite operators for extracting different
dynamic modes. We propose a method that is analogous to the classical wavelet
analysis, which we term Riemannian multi-resolution analysis (RMRA). We provide
some theoretical results on the spectral analysis of the composite operators,
and we demonstrate the proposed method on simulations and on real data.","['Tal Shnitzer', 'Hau-Tieng Wu', 'Ronen Talmon']","['stat.ML', 'cs.LG', 'eess.SP']",2022-01-21 03:52:33+00:00
http://arxiv.org/abs/2201.08518v2,Optimal variance-reduced stochastic approximation in Banach spaces,"We study the problem of estimating the fixed point of a contractive operator
defined on a separable Banach space. Focusing on a stochastic query model that
provides noisy evaluations of the operator, we analyze a variance-reduced
stochastic approximation scheme, and establish non-asymptotic bounds for both
the operator defect and the estimation error, measured in an arbitrary
semi-norm. In contrast to worst-case guarantees, our bounds are
instance-dependent, and achieve the local asymptotic minimax risk
non-asymptotically. For linear operators, contractivity can be relaxed to
multi-step contractivity, so that the theory can be applied to problems like
average reward policy evaluation problem in reinforcement learning. We
illustrate the theory via applications to stochastic shortest path problems,
two-player zero-sum Markov games, as well as policy evaluation and $Q$-learning
for tabular Markov decision processes.","['Wenlong Mou', 'Koulik Khamaru', 'Martin J. Wainwright', 'Peter L. Bartlett', 'Michael I. Jordan']","['math.ST', 'cs.LG', 'math.OC', 'stat.ML', 'stat.TH']",2022-01-21 02:46:57+00:00
http://arxiv.org/abs/2201.08504v4,Deep reinforcement learning under signal temporal logic constraints using Lagrangian relaxation,"Deep reinforcement learning (DRL) has attracted much attention as an approach
to solve optimal control problems without mathematical models of systems. On
the other hand, in general, constraints may be imposed on optimal control
problems. In this study, we consider the optimal control problems with
constraints to complete temporal control tasks. We describe the constraints
using signal temporal logic (STL), which is useful for time sensitive control
tasks since it can specify continuous signals within bounded time intervals. To
deal with the STL constraints, we introduce an extended constrained Markov
decision process (CMDP), which is called a $\tau$-CMDP. We formulate the
STL-constrained optimal control problem as the $\tau$-CMDP and propose a
two-phase constrained DRL algorithm using the Lagrangian relaxation method.
Through simulations, we also demonstrate the learning performance of the
proposed algorithm.","['Junya Ikemoto', 'Toshimitsu Ushio']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY']",2022-01-21 00:56:25+00:00
http://arxiv.org/abs/2201.08417v2,Scalable Sampling for Nonsymmetric Determinantal Point Processes,"A determinantal point process (DPP) on a collection of $M$ items is a model,
parameterized by a symmetric kernel matrix, that assigns a probability to every
subset of those items. Recent work shows that removing the kernel symmetry
constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant
predictive performance gains for machine learning applications. However,
existing work leaves open the question of scalable NDPP sampling. There is only
one known DPP sampling algorithm, based on Cholesky decomposition, that can
directly apply to NDPPs as well. Unfortunately, its runtime is cubic in $M$,
and thus does not scale to large item collections. In this work, we first note
that this algorithm can be transformed into a linear-time one for kernels with
low-rank structure. Furthermore, we develop a scalable sublinear-time rejection
sampling algorithm by constructing a novel proposal distribution. Additionally,
we show that imposing certain structural constraints on the NDPP kernel enables
us to bound the rejection rate in a way that depends only on the kernel rank.
In our experiments we compare the speed of all of these samplers for a variety
of real-world tasks.","['Insu Han', 'Mike Gartrell', 'Jennifer Gillenwater', 'Elvis Dohmatob', 'Amin Karbasi']","['cs.LG', 'stat.ML']",2022-01-20 19:21:59+00:00
http://arxiv.org/abs/2201.08349v1,Heavy-tailed Sampling via Transformed Unadjusted Langevin Algorithm,"We analyze the oracle complexity of sampling from polynomially decaying
heavy-tailed target densities based on running the Unadjusted Langevin
Algorithm on certain transformed versions of the target density. The specific
class of closed-form transformation maps that we construct are shown to be
diffeomorphisms, and are particularly suited for developing efficient
diffusion-based samplers. We characterize the precise class of heavy-tailed
densities for which polynomial-order oracle complexities (in dimension and
inverse target accuracy) could be obtained, and provide illustrative examples.
We highlight the relationship between our assumptions and functional
inequalities (super and weak Poincar\'e inequalities) based on non-local
Dirichlet forms defined via fractional Laplacian operators, used to
characterize the heavy-tailed equilibrium densities of certain stable-driven
stochastic differential equations.","['Ye He', 'Krishnakumar Balasubramanian', 'Murat A. Erdogdu']","['math.ST', 'stat.CO', 'stat.ML', 'stat.TH']",2022-01-20 18:32:41+00:00
http://arxiv.org/abs/2201.08343v2,Using Machine Learning to Test Causal Hypotheses in Conjoint Analysis,"Conjoint analysis is a popular experimental design used to measure
multidimensional preferences. Researchers examine how varying a factor of
interest, while controlling for other relevant factors, influences
decision-making. Currently, there exist two methodological approaches to
analyzing data from a conjoint experiment. The first focuses on estimating the
average marginal effects of each factor while averaging over the other factors.
Although this allows for straightforward design-based estimation, the results
critically depend on the distribution of other factors and how interaction
effects are aggregated. An alternative model-based approach can compute various
quantities of interest, but requires researchers to correctly specify the
model, a challenging task for conjoint analysis with many factors and possible
interactions. In addition, a commonly used logistic regression has poor
statistical properties even with a moderate number of factors when
incorporating interactions. We propose a new hypothesis testing approach based
on the conditional randomization test to answer the most fundamental question
of conjoint analysis: Does a factor of interest matter in any way given the
other factors? Our methodology is solely based on the randomization of factors,
and hence is free from assumptions. Yet, it allows researchers to use any test
statistic, including those based on complex machine learning algorithms. As a
result, we are able to combine the strengths of the existing design-based and
model-based approaches. We illustrate the proposed methodology through conjoint
analysis of immigration preferences and political candidate evaluation. We also
extend the proposed approach to test for regularity assumptions commonly used
in conjoint analysis. An open-source software package is available for
implementing the proposed methodology.","['Dae Woong Ham', 'Kosuke Imai', 'Lucas Janson']","['stat.ME', 'stat.ML']",2022-01-20 18:23:12+00:00
http://arxiv.org/abs/2201.08326v1,Learning with latent group sparsity via heat flow dynamics on networks,"Group or cluster structure on explanatory variables in machine learning
problems is a very general phenomenon, which has attracted broad interest from
practitioners and theoreticians alike. In this work we contribute an approach
to learning under such group structure, that does not require prior information
on the group identities. Our paradigm is motivated by the Laplacian geometry of
an underlying network with a related community structure, and proceeds by
directly incorporating this into a penalty that is effectively computed via a
heat flow-based local network dynamics. In fact, we demonstrate a procedure to
construct such a network based on the available data. Notably, we dispense with
computationally intensive pre-processing involving clustering of variables,
spectral or otherwise. Our technique is underpinned by rigorous theorems that
guarantee its effective performance and provide bounds on its sample
complexity. In particular, in a wide range of settings, it provably suffices to
run the heat flow dynamics for time that is only logarithmic in the problem
dimensions. We explore in detail the interfaces of our approach with key
statistical physics models in network science, such as the Gaussian Free Field
and the Stochastic Block Model. We validate our approach by successful
applications to real-world data from a wide array of application domains,
including computer science, genetics, climatology and economics. Our work
raises the possibility of applying similar diffusion-based techniques to
classical learning tasks, exploiting the interplay between geometric, dynamical
and stochastic structures underlying the data.","['Subhroshekhar Ghosh', 'Soumendu Sundar Mukherjee']","['stat.ME', 'cs.LG', 'econ.EM', 'math.ST', 'stat.CO', 'stat.ML', 'stat.TH']",2022-01-20 17:45:57+00:00
http://arxiv.org/abs/2201.08315v2,Predictive Inference with Weak Supervision,"The expense of acquiring labels in large-scale statistical machine learning
makes partially and weakly-labeled data attractive, though it is not always
apparent how to leverage such data for model fitting or validation. We present
a methodology to bridge the gap between partial supervision and validation,
developing a conformal prediction framework to provide valid predictive
confidence sets -- sets that cover a true label with a prescribed probability,
independent of the underlying distribution -- using weakly labeled data. To do
so, we introduce a (necessary) new notion of coverage and predictive validity,
then develop several application scenarios, providing efficient algorithms for
classification and several large-scale structured prediction problems. We
corroborate the hypothesis that the new coverage definition allows for tighter
and more informative (but valid) confidence sets through several experiments.","['Maxime Cauchois', 'Suyash Gupta', 'Alnur Ali', 'John Duchi']","['stat.ML', 'cs.LG']",2022-01-20 17:26:52+00:00
http://arxiv.org/abs/2201.08311v1,"Accelerated Gradient Flow: Risk, Stability, and Implicit Regularization","Acceleration and momentum are the de facto standard in modern applications of
machine learning and optimization, yet the bulk of the work on implicit
regularization focuses instead on unaccelerated methods. In this paper, we
study the statistical risk of the iterates generated by Nesterov's accelerated
gradient method and Polyak's heavy ball method, when applied to least squares
regression, drawing several connections to explicit penalization. We carry out
our analyses in continuous-time, allowing us to make sharper statements than in
prior work, and revealing complex interactions between early stopping,
stability, and the curvature of the loss function.","['Yue Sheng', 'Alnur Ali']","['stat.ML', 'cs.LG', 'math.OC']",2022-01-20 17:23:43+00:00
http://arxiv.org/abs/2201.08283v1,Lead-lag detection and network clustering for multivariate time series with an application to the US equity market,"In multivariate time series systems, it has been observed that certain groups
of variables partially lead the evolution of the system, while other variables
follow this evolution with a time delay; the result is a lead-lag structure
amongst the time series variables. In this paper, we propose a method for the
detection of lead-lag clusters of time series in multivariate systems. We
demonstrate that the web of pairwise lead-lag relationships between time series
can be helpfully construed as a directed network, for which there exist
suitable algorithms for the detection of pairs of lead-lag clusters with high
pairwise imbalance. Within our framework, we consider a number of choices for
the pairwise lead-lag metric and directed network clustering components. Our
framework is validated on both a synthetic generative model for multivariate
lead-lag time series systems and daily real-world US equity prices data. We
showcase that our method is able to detect statistically significant lead-lag
clusters in the US equity market. We study the nature of these clusters in the
context of the empirical finance literature on lead-lag relations and
demonstrate how these can be used for the construction of predictive financial
signals.","['Stefanos Bennett', 'Mihai Cucuringu', 'Gesine Reinert']","['stat.ML', 'cs.LG', 'q-fin.ST', 'stat.ME']",2022-01-20 16:39:57+00:00
http://arxiv.org/abs/2201.08262v1,Generalizing Off-Policy Evaluation From a Causal Perspective For Sequential Decision-Making,"Assessing the effects of a policy based on observational data from a
different policy is a common problem across several high-stake decision-making
domains, and several off-policy evaluation (OPE) techniques have been proposed.
However, these methods largely formulate OPE as a problem disassociated from
the process used to generate the data (i.e. structural assumptions in the form
of a causal graph). We argue that explicitly highlighting this association has
important implications on our understanding of the fundamental limits of OPE.
First, this implies that current formulation of OPE corresponds to a narrow set
of tasks, i.e. a specific causal estimand which is focused on prospective
evaluation of policies over populations or sub-populations. Second, we
demonstrate how this association motivates natural desiderata to consider a
general set of causal estimands, particularly extending the role of OPE for
counterfactual off-policy evaluation at the level of individuals of the
population. A precise description of the causal estimand highlights which OPE
estimands are identifiable from observational data under the stated generative
assumptions. For those OPE estimands that are not identifiable, the causal
perspective further highlights where more experimental data is necessary, and
highlights situations where human expertise can aid identification and
estimation. Furthermore, many formalisms of OPE overlook the role of
uncertainty entirely in the estimation process.We demonstrate how specifically
characterising the causal estimand highlights the different sources of
uncertainty and when human expertise can naturally manage this uncertainty. We
discuss each of these aspects as actionable desiderata for future OPE research
at scale and in-line with practical utility.","['Sonali Parbhoo', 'Shalmali Joshi', 'Finale Doshi-Velez']","['cs.LG', 'stat.ML']",2022-01-20 16:13:16+00:00
http://arxiv.org/abs/2201.08226v2,Sketch-and-Lift: Scalable Subsampled Semidefinite Program for $K$-means Clustering,"Semidefinite programming (SDP) is a powerful tool for tackling a wide range
of computationally hard problems such as clustering. Despite the high accuracy,
semidefinite programs are often too slow in practice with poor scalability on
large (or even moderate) datasets. In this paper, we introduce a linear time
complexity algorithm for approximating an SDP relaxed $K$-means clustering. The
proposed sketch-and-lift (SL) approach solves an SDP on a subsampled dataset
and then propagates the solution to all data points by a nearest-centroid
rounding procedure. It is shown that the SL approach enjoys a similar exact
recovery threshold as the $K$-means SDP on the full dataset, which is known to
be information-theoretically tight under the Gaussian mixture model. The SL
method can be made adaptive with enhanced theoretic properties when the cluster
sizes are unbalanced. Our simulation experiments demonstrate that the
statistical accuracy of the proposed method outperforms state-of-the-art fast
clustering algorithms without sacrificing too much computational efficiency,
and is comparable to the original $K$-means SDP with substantially reduced
runtime.","['Yubo Zhuang', 'Xiaohui Chen', 'Yun Yang']","['stat.ML', 'cs.LG']",2022-01-20 15:31:28+00:00
http://arxiv.org/abs/2201.08115v2,"Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning","The ability to discover behaviours from past experience and transfer them to
new tasks is a hallmark of intelligent agents acting sample-efficiently in the
real world. Equipping embodied reinforcement learners with the same ability may
be crucial for their successful deployment in robotics. While hierarchical and
KL-regularized reinforcement learning individually hold promise here, arguably
a hybrid approach could combine their respective benefits. Key to these fields
is the use of information asymmetry across architectural modules to bias which
skills are learnt. While asymmetry choice has a large influence on
transferability, existing methods base their choice primarily on intuition in a
domain-independent, potentially sub-optimal, manner. In this paper, we
theoretically and empirically show the crucial expressivity-transferability
trade-off of skills across sequential tasks, controlled by information
asymmetry. Given this insight, we introduce Attentive Priors for Expressive and
Transferable Skills (APES), a hierarchical KL-regularized method, heavily
benefiting from both priors and hierarchy. Unlike existing approaches, APES
automates the choice of asymmetry by learning it in a data-driven,
domain-dependent, way based on our expressivity-transferability theorems.
Experiments over complex transfer domains of varying levels of extrapolation
and sparsity, such as robot block stacking, demonstrate the criticality of the
correct asymmetric choice, with APES drastically outperforming previous
methods.","['Sasha Salter', 'Kristian Hartikainen', 'Walter Goodwin', 'Ingmar Posner']","['cs.AI', 'cs.LG', 'cs.RO', 'stat.ML']",2022-01-20 11:12:56+00:00
http://arxiv.org/abs/2201.08105v1,"Statistical Depth Functions for Ranking Distributions: Definitions, Statistical Learning and Applications","The concept of median/consensus has been widely investigated in order to
provide a statistical summary of ranking data, i.e. realizations of a random
permutation $\Sigma$ of a finite set, $\{1,\; \ldots,\; n\}$ with $n\geq 1$
say. As it sheds light onto only one aspect of $\Sigma$'s distribution $P$, it
may neglect other informative features. It is the purpose of this paper to
define analogs of quantiles, ranks and statistical procedures based on such
quantities for the analysis of ranking data by means of a metric-based notion
of depth function on the symmetric group. Overcoming the absence of vector
space structure on $\mathfrak{S}_n$, the latter defines a center-outward
ordering of the permutations in the support of $P$ and extends the classic
metric-based formulation of consensus ranking (medians corresponding then to
the deepest permutations). The axiomatic properties that ranking depths should
ideally possess are listed, while computational and generalization issues are
studied at length. Beyond the theoretical analysis carried out, the relevance
of the novel concepts and methods introduced for a wide variety of statistical
tasks are also supported by numerous numerical experiments.","['Morgane Goibert', 'Stéphan Clémençon', 'Ekhine Irurozki', 'Pavlo Mozharovskyi']","['cs.LG', 'stat.ML']",2022-01-20 10:30:56+00:00
http://arxiv.org/abs/2201.08082v1,Kernel Methods and Multi-layer Perceptrons Learn Linear Models in High Dimensions,"Empirical observation of high dimensional phenomena, such as the double
descent behaviour, has attracted a lot of interest in understanding classical
techniques such as kernel methods, and their implications to explain
generalization properties of neural networks. Many recent works analyze such
models in a certain high-dimensional regime where the covariates are
independent and the number of samples and the number of covariates grow at a
fixed ratio (i.e. proportional asymptotics). In this work we show that for a
large class of kernels, including the neural tangent kernel of fully connected
networks, kernel methods can only perform as well as linear models in this
regime. More surprisingly, when the data is generated by a kernel model where
the relationship between input and the response could be very nonlinear, we
show that linear models are in fact optimal, i.e. linear models achieve the
minimum risk among all models, linear or nonlinear. These results suggest that
more complex models for the data other than independent features are needed for
high-dimensional analysis.","['Mojtaba Sahraee-Ardakan', 'Melikasadat Emami', 'Parthe Pandit', 'Sundeep Rangan', 'Alyson K. Fletcher']","['stat.ML', 'cs.LG']",2022-01-20 09:35:46+00:00
http://arxiv.org/abs/2201.07998v3,Statistical Learning for Individualized Asset Allocation,"We establish a high-dimensional statistical learning framework for
individualized asset allocation. Our proposed methodology addresses
continuous-action decision-making with a large number of characteristics. We
develop a discretization approach to model the effect of continuous actions and
allow the discretization frequency to be large and diverge with the number of
observations. The value function of continuous-action is estimated using
penalized regression with our proposed generalized penalties that are imposed
on linear transformations of the model coefficients. We show that our proposed
Discretization and Regression with generalized fOlded concaVe penalty on Effect
discontinuity (DROVE) approach enjoys desirable theoretical properties and
allows for statistical inference of the optimal value associated with optimal
decision-making. Empirically, the proposed framework is exercised with the
Health and Retirement Study data in finding individualized optimal asset
allocation. The results show that our individualized optimal strategy improves
the population financial well-being.","['Yi Ding', 'Yingying Li', 'Rui Song']","['stat.ML', 'math.ST', 'stat.TH']",2022-01-20 04:40:03+00:00
http://arxiv.org/abs/2201.07912v2,Communication-Efficient Device Scheduling for Federated Learning Using Stochastic Optimization,"Federated learning (FL) is a useful tool in distributed machine learning that
utilizes users' local datasets in a privacy-preserving manner. When deploying
FL in a constrained wireless environment; however, training models in a
time-efficient manner can be a challenging task due to intermittent
connectivity of devices, heterogeneous connection quality, and non-i.i.d. data.
In this paper, we provide a novel convergence analysis of non-convex loss
functions using FL on both i.i.d. and non-i.i.d. datasets with arbitrary device
selection probabilities for each round. Then, using the derived convergence
bound, we use stochastic optimization to develop a new client selection and
power allocation algorithm that minimizes a function of the convergence bound
and the average communication time under a transmit power constraint. We find
an analytical solution to the minimization problem. One key feature of the
algorithm is that knowledge of the channel statistics is not required and only
the instantaneous channel state information needs to be known. Using the
FEMNIST and CIFAR-10 datasets, we show through simulations that the
communication time can be significantly decreased using our algorithm, compared
to uniformly random participation.","['Jake Perazzone', 'Shiqiang Wang', 'Mingyue Ji', 'Kevin Chan']","['cs.LG', 'cs.DC', 'cs.IT', 'math.IT', 'stat.ML']",2022-01-19 23:25:24+00:00
http://arxiv.org/abs/2201.07753v1,Deep Capsule Encoder-Decoder Network for Surrogate Modeling and Uncertainty Quantification,"We propose a novel \textit{capsule} based deep encoder-decoder model for
surrogate modeling and uncertainty quantification of systems in mechanics from
sparse data. The proposed framework is developed by adapting Capsule Network
(CapsNet) architecture into image-to-image regression encoder-decoder network.
Specifically, the aim is to exploit the benefits of CapsNet over convolution
neural network (CNN) $-$ retaining pose and position information related to an
entity to name a few. The performance of proposed approach is illustrated by
solving an elliptic stochastic partial differential equation (SPDE), which also
governs systems in mechanics such as steady heat conduction, ground water flow
or other diffusion processes, based uncertainty quantification problem with an
input dimensionality of $1024$. However, the problem definition does not the
restrict the random diffusion field to a particular covariance structure, and
the more strenuous task of response prediction for an arbitrary diffusion field
is solved. The obtained results from performance evaluation indicate that the
proposed approach is accurate, efficient, and robust.","['Akshay Thakur', 'Souvik Chakraborty']","['stat.ML', 'cs.LG']",2022-01-19 17:45:01+00:00
http://arxiv.org/abs/2201.07684v1,On the Complexity of a Practical Primal-Dual Coordinate Method,"We prove complexity bounds for the primal-dual algorithm with random
extrapolation and coordinate descent (PURE-CD), which has been shown to obtain
good practical performance for solving convex-concave min-max problems with
bilinear coupling. Our complexity bounds either match or improve the best-known
results in the literature for both dense and sparse
(strongly)-convex-(strongly)-concave problems.","['Ahmet Alacaoglu', 'Volkan Cevher', 'Stephen J. Wright']","['math.OC', 'cs.LG', 'stat.ML']",2022-01-19 16:14:27+00:00
http://arxiv.org/abs/2201.07683v1,Coupled Support Tensor Machine Classification for Multimodal Neuroimaging Data,"Multimodal data arise in various applications where information about the
same phenomenon is acquired from multiple sensors and across different imaging
modalities. Learning from multimodal data is of great interest in machine
learning and statistics research as this offers the possibility of capturing
complementary information among modalities. Multimodal modeling helps to
explain the interdependence between heterogeneous data sources, discovers new
insights that may not be available from a single modality, and improves
decision-making. Recently, coupled matrix-tensor factorization has been
introduced for multimodal data fusion to jointly estimate latent factors and
identify complex interdependence among the latent factors. However, most of the
prior work on coupled matrix-tensor factors focuses on unsupervised learning
and there is little work on supervised learning using the jointly estimated
latent factors. This paper considers the multimodal tensor data classification
problem. A Coupled Support Tensor Machine (C-STM) built upon the latent factors
jointly estimated from the Advanced Coupled Matrix Tensor Factorization (ACMTF)
is proposed. C-STM combines individual and shared latent factors with multiple
kernels and estimates a maximal-margin classifier for coupled matrix tensor
data. The classification risk of C-STM is shown to converge to the optimal
Bayes risk, making it a statistically consistent rule. C-STM is validated
through simulation studies as well as a simultaneous EEG-fMRI analysis. The
empirical evidence shows that C-STM can utilize information from multiple
sources and provide a better classification performance than traditional
single-mode classifiers.","['Li Peide', 'Seyyid Emre Sofuoglu', 'Tapabrata Maiti', 'Selin Aviyente']","['stat.ML', 'cs.LG', 'stat.ME']",2022-01-19 16:13:09+00:00
http://arxiv.org/abs/2201.07586v1,RAMANMETRIX: a delightful way to analyze Raman spectra,"Although Raman spectroscopy is widely used for the investigation of
biomedical samples and has a high potential for use in clinical applications,
it is not common in clinical routines. One of the factors that obstruct the
integration of Raman spectroscopic tools into clinical routines is the
complexity of the data processing workflow. Software tools that simplify
spectroscopic data handling may facilitate such integration by familiarizing
clinical experts with the advantages of Raman spectroscopy.
  Here, RAMANMETRIX is introduced as a user-friendly software with an intuitive
web-based graphical user interface (GUI) that incorporates a complete workflow
for chemometric analysis of Raman spectra, from raw data pretreatment to a
robust validation of machine learning models. The software can be used both for
model training and for the application of the pretrained models onto new data
sets. Users have full control of the parameters during model training, but the
testing data flow is frozen and does not require additional user input.
RAMANMETRIX is available in two versions: as standalone software and web
application. Due to the modern software architecture, the computational backend
part can be executed separately from the GUI and accessed through an
application programming interface (API) for applying a preconstructed model to
the measured data. This opens up possibilities for using the software as a data
processing backend for the measurement devices in real-time.
  The models preconstructed by more experienced users can be exported and
reused for easy one-click data preprocessing and prediction, which requires
minimal interaction between the user and the software. The results of such
prediction and graphical outputs of the different data processing steps can be
exported and saved.","['Darina Storozhuk', 'Oleg Ryabchykov', 'Juergen Popp', 'Thomas Bocklitz']","['physics.data-an', 'stat.AP', 'stat.ML']",2022-01-19 13:20:28+00:00
http://arxiv.org/abs/2201.09986v3,Bayesian Inference with Nonlinear Generative Models: Comments on Secure Learning,"Unlike the classical linear model, nonlinear generative models have been
addressed sparsely in the literature of statistical learning. This work aims to
bringing attention to these models and their secrecy potential. To this end, we
invoke the replica method to derive the asymptotic normalized cross entropy in
an inverse probability problem whose generative model is described by a
Gaussian random field with a generic covariance function. Our derivations
further demonstrate the asymptotic statistical decoupling of the Bayesian
estimator and specify the decoupled setting for a given nonlinear model.
  The replica solution depicts that strictly nonlinear models establish an
all-or-nothing phase transition: There exists a critical load at which the
optimal Bayesian inference changes from perfect to an uncorrelated learning.
Based on this finding, we design a new secure coding scheme which achieves the
secrecy capacity of the wiretap channel. This interesting result implies that
strictly nonlinear generative models are perfectly secured without any secure
coding. We justify this latter statement through the analysis of an
illustrative model for perfectly secure and reliable inference.","['Ali Bereyhi', 'Bruno Loureiro', 'Florent Krzakala', 'Ralf R. Müller', 'Hermann Schulz-Baldes']","['cs.IT', 'cs.CR', 'cs.LG', 'math.IT', 'stat.ML']",2022-01-19 08:29:53+00:00
http://arxiv.org/abs/2201.07427v1,Lifted Primal-Dual Method for Bilinearly Coupled Smooth Minimax Optimization,"We study the bilinearly coupled minimax problem: $\min_{x} \max_{y} f(x) +
y^\top A x - h(y)$, where $f$ and $h$ are both strongly convex smooth functions
and admit first-order gradient oracles. Surprisingly, no known first-order
algorithms have hitherto achieved the lower complexity bound of
$\Omega((\sqrt{\frac{L_x}{\mu_x}} + \frac{\|A\|}{\sqrt{\mu_x \mu_y}} +
\sqrt{\frac{L_y}{\mu_y}}) \log(\frac1{\varepsilon}))$ for solving this problem
up to an $\varepsilon$ primal-dual gap in the general parameter regime, where
$L_x, L_y,\mu_x,\mu_y$ are the corresponding smoothness and strongly convexity
constants.
  We close this gap by devising the first optimal algorithm, the Lifted
Primal-Dual (LPD) method. Our method lifts the objective into an extended form
that allows both the smooth terms and the bilinear term to be handled optimally
and seamlessly with the same primal-dual framework. Besides optimality, our
method yields a desirably simple single-loop algorithm that uses only one
gradient oracle call per iteration. Moreover, when $f$ is just convex, the same
algorithm applied to a smoothed objective achieves the nearly optimal iteration
complexity. We also provide a direct single-loop algorithm, using the LPD
method, that achieves the iteration complexity of
$O(\sqrt{\frac{L_x}{\varepsilon}} + \frac{\|A\|}{\sqrt{\mu_y \varepsilon}} +
\sqrt{\frac{L_y}{\varepsilon}})$. Numerical experiments on quadratic minimax
problems and policy evaluation problems further demonstrate the fast
convergence of our algorithm in practice.","['Kiran Koshy Thekumparampil', 'Niao He', 'Sewoong Oh']","['math.OC', 'cs.LG', 'stat.ML']",2022-01-19 05:56:19+00:00
http://arxiv.org/abs/2201.07401v2,Multiway Spherical Clustering via Degree-Corrected Tensor Block Models,"We consider the problem of multiway clustering in the presence of unknown
degree heterogeneity. Such data problems arise commonly in applications such as
recommendation system, neuroimaging, community detection, and hypergraph
partitions in social networks. The allowance of degree heterogeneity provides
great flexibility in clustering models, but the extra complexity poses
significant challenges in both statistics and computation. Here, we develop a
degree-corrected tensor block model with estimation accuracy guarantees. We
present the phase transition of clustering performance based on the notion of
angle separability, and we characterize three signal-to-noise regimes
corresponding to different statistical-computational behaviors. In particular,
we demonstrate that an intrinsic statistical-to-computational gap emerges only
for tensors of order three or greater. Further, we develop an efficient
polynomial-time algorithm that provably achieves exact clustering under mild
signal conditions. The efficacy of our procedure is demonstrated through two
data applications, one on human brain connectome project, and another on Peru
Legislation network dataset.","['Jiaxin Hu', 'Miaoyan Wang']","['math.ST', 'stat.ML', 'stat.TH']",2022-01-19 03:40:22+00:00
http://arxiv.org/abs/2201.07396v3,Ordinal Causal Discovery,"Causal discovery for purely observational, categorical data is a
long-standing challenging problem. Unlike continuous data, the vast majority of
existing methods for categorical data focus on inferring the Markov equivalence
class only, which leaves the direction of some causal relationships
undetermined. This paper proposes an identifiable ordinal causal discovery
method that exploits the ordinal information contained in many real-world
applications to uniquely identify the causal structure. The proposed method is
applicable beyond ordinal data via data discretization. Through real-world and
synthetic experiments, we demonstrate that the proposed ordinal causal
discovery method combined with simple score-and-search algorithms has favorable
and robust performance compared to state-of-the-art alternative methods in both
ordinal categorical and non-categorical data. An accompanied R package OrdCD is
freely available on CRAN and at
https://web.stat.tamu.edu/~yni/files/OrdCD_1.0.0.tar.gz.","['Yang Ni', 'Bani Mallick']","['stat.ME', 'stat.ML']",2022-01-19 03:11:26+00:00
http://arxiv.org/abs/2201.07794v5,A Non-Expert's Introduction to Data Ethics for Mathematicians,"I give a short introduction to data ethics. I begin with some background
information and societal context for data ethics. I then discuss data ethics in
mathematical-science education and indicate some available course material. I
briefly highlight a few efforts -- at my home institution and elsewhere -- on
data ethics, society, and social good. I then discuss open data in research,
research replicability and some other ethical issues in research, and the
tension between privacy and open data and code, and a few controversial studies
and reactions to studies. I then discuss ethical principles, institutional
review boards, and a few other considerations in the scientific use of human
data. I then briefly survey a variety of research and lay articles that are
relevant to data ethics and data privacy. I conclude with a brief summary and
some closing remarks.
  My focal audience is mathematicians, but I hope that this chapter will also
be useful to others. I am not an expert about data ethics, and this chapter
provides only a starting point on this wide-ranging topic. I encourage you to
examine the resources that I discuss and to reflect carefully on data ethics,
its role in mathematics education, and the societal implications of data and
data analysis. As data and technology continue to evolve, I hope that such
careful reflection will continue throughout your life.",['Mason A. Porter'],"['math.HO', 'cs.CY', 'cs.LG', 'physics.soc-ph', 'stat.ML']",2022-01-18 23:31:06+00:00
http://arxiv.org/abs/2201.07348v1,Learning Tensor Representations for Meta-Learning,"We introduce a tensor-based model of shared representation for meta-learning
from a diverse set of tasks. Prior works on learning linear representations for
meta-learning assume that there is a common shared representation across
different tasks, and do not consider the additional task-specific observable
side information. In this work, we model the meta-parameter through an
order-$3$ tensor, which can adapt to the observed task features of the task. We
propose two methods to estimate the underlying tensor. The first method solves
a tensor regression problem and works under natural assumptions on the data
generating process. The second method uses the method of moments under
additional distributional assumptions and has an improved sample complexity in
terms of the number of tasks.
  We also focus on the meta-test phase, and consider estimating task-specific
parameters on a new task. Substituting the estimated tensor from the first step
allows us estimating the task-specific parameters with very few samples of the
new task, thereby showing the benefits of learning tensor representations for
meta-learning. Finally, through simulation and several real-world datasets, we
evaluate our methods and show that it improves over previous linear models of
shared representations for meta-learning.","['Samuel Deng', 'Yilin Guo', 'Daniel Hsu', 'Debmalya Mandal']","['cs.LG', 'stat.ML']",2022-01-18 23:01:35+00:00
http://arxiv.org/abs/2201.07296v2,Convergence of Policy Gradient for Entropy Regularized MDPs with Neural Network Approximation in the Mean-Field Regime,"We study the global convergence of policy gradient for infinite-horizon,
continuous state and action space, and entropy-regularized Markov decision
processes (MDPs). We consider a softmax policy with (one-hidden layer) neural
network approximation in a mean-field regime. Additional entropic
regularization in the associated mean-field probability measure is added, and
the corresponding gradient flow is studied in the 2-Wasserstein metric. We show
that the objective function is increasing along the gradient flow. Further, we
prove that if the regularization in terms of the mean-field measure is
sufficient, the gradient flow converges exponentially fast to the unique
stationary solution, which is the unique maximizer of the regularized MDP
objective. Lastly, we study the sensitivity of the value function along the
gradient flow with respect to regularization parameters and the initial
condition. Our results rely on the careful analysis of the non-linear
Fokker-Planck-Kolmogorov equation and extend the pioneering work of Mei et al.
2020 and Agarwal et al. 2020, which quantify the global convergence rate of
policy gradient for entropy-regularized MDPs in the tabular setting.","['Bekzhan Kerimkulov', 'James-Michael Leahy', 'David Šiška', 'Lukasz Szpruch']","['math.OC', 'cs.AI', 'cs.LG', 'math.PR', 'stat.ML']",2022-01-18 20:17:16+00:00
http://arxiv.org/abs/2201.07206v1,Minimax Optimality (Probably) Doesn't Imply Distribution Learning for GANs,"Arguably the most fundamental question in the theory of generative
adversarial networks (GANs) is to understand to what extent GANs can actually
learn the underlying distribution. Theoretical and empirical evidence suggests
local optimality of the empirical training objective is insufficient. Yet, it
does not rule out the possibility that achieving a true population minimax
optimal solution might imply distribution learning.
  In this paper, we show that standard cryptographic assumptions imply that
this stronger condition is still insufficient. Namely, we show that if local
pseudorandom generators (PRGs) exist, then for a large family of natural
continuous target distributions, there are ReLU network generators of constant
depth and polynomial size which take Gaussian random seeds so that (i) the
output is far in Wasserstein distance from the target distribution, but (ii) no
polynomially large Lipschitz discriminator ReLU network can detect this. This
implies that even achieving a population minimax optimal solution to the
Wasserstein GAN objective is likely insufficient for distribution learning in
the usual statistical sense. Our techniques reveal a deep connection between
GANs and PRGs, which we believe will lead to further insights into the
computational landscape of GANs.","['Sitan Chen', 'Jerry Li', 'Yuanzhi Li', 'Raghu Meka']","['cs.LG', 'cs.CC', 'cs.CR', 'stat.ML']",2022-01-18 18:59:21+00:00
http://arxiv.org/abs/2201.07164v1,Low Regret Binary Sampling Method for Efficient Global Optimization of Univariate Functions,"In this work, we propose a computationally efficient algorithm for the
problem of global optimization in univariate loss functions. For the
performance evaluation, we study the cumulative regret of the algorithm instead
of the simple regret between our best query and the optimal value of the
objective function. Although our approach has similar regret results with the
traditional lower-bounding algorithms such as the Piyavskii-Shubert method for
the Lipschitz continuous or Lipschitz smooth functions, it has a major
computational cost advantage. In Piyavskii-Shubert method, for certain types of
functions, the query points may be hard to determine (as they are solutions to
additional optimization problems). However, this issue is circumvented in our
binary sampling approach, where the sampling set is predetermined irrespective
of the function characteristics. For a search space of $[0,1]$, our approach
has at most $L\log (3T)$ and $2.25H$ regret for $L$-Lipschitz continuous and
$H$-Lipschitz smooth functions respectively. We also analytically extend our
results for a broader class of functions that covers more complex regularity
conditions.","['Kaan Gokcesu', 'Hakan Gokcesu']","['cs.LG', 'cs.CC', 'math.OC', 'stat.ML']",2022-01-18 18:11:48+00:00
http://arxiv.org/abs/2201.07136v4,Incompleteness of graph neural networks for points clouds in three dimensions,"Graph neural networks (GNN) are very popular methods in machine learning and
have been applied very successfully to the prediction of the properties of
molecules and materials. First-order GNNs are well known to be incomplete,
i.e., there exist graphs that are distinct but appear identical when seen
through the lens of the GNN. More complicated schemes have thus been designed
to increase their resolving power. Applications to molecules (and more
generally, point clouds), however, add a geometric dimension to the problem.
The most straightforward and prevalent approach to construct graph
representation for molecules regards atoms as vertices in a graph and draws a
bond between each pair of atoms within a chosen cutoff. Bonds can be decorated
with the distance between atoms, and the resulting ""distance graph NNs"" (dGNN)
have empirically demonstrated excellent resolving power and are widely used in
chemical ML, with all known indistinguishable configurations being resolved in
the fully-connected limit, which is equivalent to infinite or sufficiently
large cutoff. Here we present a counterexample that proves that dGNNs are not
complete even for the restricted case of fully-connected graphs induced by 3D
atom clouds. We construct pairs of distinct point clouds whose associated
graphs are, for any cutoff radius, equivalent based on a first-order
Weisfeiler-Lehman test. This class of degenerate structures includes
chemically-plausible configurations, both for isolated structures and for
infinite structures that are periodic in 1, 2, and 3 dimensions. The existence
of indistinguishable configurations sets an ultimate limit to the expressive
power of some of the well-established GNN architectures for atomistic machine
learning. Models that explicitly use angular or directional information in the
description of atomic environments can resolve this class of degeneracies.","['Sergey N. Pozdnyakov', 'Michele Ceriotti']","['stat.ML', 'cs.LG', 'physics.chem-ph']",2022-01-18 17:18:26+00:00
http://arxiv.org/abs/2201.07130v2,"Online, Informative MCMC Thinning with Kernelized Stein Discrepancy","A fundamental challenge in Bayesian inference is efficient representation of
a target distribution. Many non-parametric approaches do so by sampling a large
number of points using variants of Markov Chain Monte Carlo (MCMC). We propose
an MCMC variant that retains only those posterior samples which exceed a KSD
threshold, which we call KSD Thinning. We establish the convergence and
complexity tradeoffs for several settings of KSD Thinning as a function of the
KSD threshold parameter, sample size, and other problem parameters. Finally, we
provide experimental comparisons against other online nonparametric Bayesian
methods that generate low-complexity posterior representations, and observe
superior consistency/complexity tradeoffs. Code is available at
github.com/colehawkins/KSD-Thinning.","['Cole Hawkins', 'Alec Koppel', 'Zheng Zhang']","['cs.LG', 'stat.ML']",2022-01-18 17:13:21+00:00
http://arxiv.org/abs/2201.07083v2,A Short Tutorial on The Weisfeiler-Lehman Test And Its Variants,"Graph neural networks are designed to learn functions on graphs. Typically,
the relevant target functions are invariant with respect to actions by
permutations. Therefore the design of some graph neural network architectures
has been inspired by graph-isomorphism algorithms. The classical
Weisfeiler-Lehman algorithm (WL) -- a graph-isomorphism test based on color
refinement -- became relevant to the study of graph neural networks. The WL
test can be generalized to a hierarchy of higher-order tests, known as $k$-WL.
This hierarchy has been used to characterize the expressive power of graph
neural networks, and to inspire the design of graph neural network
architectures. A few variants of the WL hierarchy appear in the literature. The
goal of this short note is pedagogical and practical: We explain the
differences between the WL and folklore-WL formulations, with pointers to
existing discussions in the literature. We illuminate the differences between
the formulations by visualizing an example.","['Ningyuan Huang', 'Soledad Villar']","['stat.ML', 'cs.LG']",2022-01-18 16:04:35+00:00
http://arxiv.org/abs/2201.07072v4,Who Increases Emergency Department Use? New Insights from the Oregon Health Insurance Experiment,"We provide new insights regarding the headline result that Medicaid increased
emergency department (ED) use from the Oregon experiment. We find meaningful
heterogeneous impacts of Medicaid on ED use using causal machine learning
methods. The individualized treatment effect distribution includes a wide range
of negative and positive values, suggesting the average effect masks
substantial heterogeneity. A small group-about 14% of participants-in the right
tail of the distribution drives the overall effect. We identify priority groups
with economically significant increases in ED usage based on demographics and
previous utilization. Intensive margin effects are an important driver of
increases in ED utilization.","['Augustine Denteh', 'Helge Liebert']","['econ.EM', 'stat.ML']",2022-01-18 15:53:28+00:00
http://arxiv.org/abs/2201.07051v2,System-Agnostic Meta-Learning for MDP-based Dynamic Scheduling via Descriptive Policy,"Dynamic scheduling is an important problem in applications from queuing to
wireless networks. It addresses how to choose an item among multiple scheduling
items in each timestep to achieve a long-term goal. Conventional approaches for
dynamic scheduling find the optimal policy for a given specific system so that
the policy from these approaches is usable only for the corresponding system
characteristics. Hence, it is hard to use such approaches for a practical
system in which system characteristics dynamically change. This paper proposes
a novel policy structure for MDP-based dynamic scheduling, a descriptive
policy, which has a system-agnostic capability to adapt to unseen system
characteristics for an identical task (dynamic scheduling). To this end, the
descriptive policy learns a system-agnostic scheduling principle--in a
nutshell, ""which condition of items should have a higher priority in
scheduling"". The scheduling principle can be applied to any system so that the
descriptive policy learned in one system can be used for another system.
Experiments with simple explanatory and realistic application scenarios
demonstrate that it enables system-agnostic meta-learning with very little
performance degradation compared with the system-specific conventional
policies.",['Hyun-Suk Lee'],"['cs.LG', 'stat.AP', 'stat.ML']",2022-01-18 15:24:11+00:00
http://arxiv.org/abs/2201.07026v1,Socioeconomic disparities and COVID-19: the causal connections,"The analysis of causation is a challenging task that can be approached in
various ways. With the increasing use of machine learning based models in
computational socioeconomics, explaining these models while taking causal
connections into account is a necessity. In this work, we advocate the use of
an explanatory framework from cooperative game theory augmented with $do$
calculus, namely causal Shapley values. Using causal Shapley values, we analyze
socioeconomic disparities that have a causal link to the spread of COVID-19 in
the USA. We study several phases of the disease spread to show how the causal
connections change over time. We perform a causal analysis using random effects
models and discuss the correspondence between the two methods to verify our
results. We show the distinct advantages a non-linear machine learning models
have over linear models when performing a multivariate analysis, especially
since the machine learning models can map out non-linear correlations in the
data. In addition, the causal Shapley values allow for including the causal
structure in the variable importance computed for the machine learning model.","['Tannista Banerjee', 'Ayan Paul', 'Vishak Srikanth', 'Inga Strümke']","['cs.LG', 'econ.EM', 'stat.ML']",2022-01-18 14:46:32+00:00
http://arxiv.org/abs/2201.06854v5,Convergence of a robust deep FBSDE method for stochastic control,"In this paper, we propose a deep learning based numerical scheme for strongly
coupled FBSDEs, stemming from stochastic control. It is a modification of the
deep BSDE method in which the initial value to the backward equation is not a
free parameter, and with a new loss function being the weighted sum of the cost
of the control problem, and a variance term which coincides with the mean
squared error in the terminal condition. We show by a numerical example that a
direct extension of the classical deep BSDE method to FBSDEs, fails for a
simple linear-quadratic control problem, and motivate why the new method works.
Under regularity and boundedness assumptions on the exact controls of time
continuous and time discrete control problems, we provide an error analysis for
our method. We show empirically that the method converges for three different
problems, one being the one that failed for a direct extension of the deep BSDE
method.","['Kristoffer Andersson', 'Adam Andersson', 'Cornelis W. Oosterlee']","['math.OC', 'cs.NA', 'math.NA', 'math.PR', 'stat.ML', '49M25, 60H30, 60H35, 65C30']",2022-01-18 10:06:57+00:00
http://arxiv.org/abs/2201.06763v1,Online Time Series Anomaly Detection with State Space Gaussian Processes,"We propose r-ssGPFA, an unsupervised online anomaly detection model for uni-
and multivariate time series building on the efficient state space formulation
of Gaussian processes. For high-dimensional time series, we propose an
extension of Gaussian process factor analysis to identify the common latent
processes of the time series, allowing us to detect anomalies efficiently in an
interpretable manner. We gain explainability while speeding up computations by
imposing an orthogonality constraint on the mapping from the latent to the
observed. Our model's robustness is improved by using a simple heuristic to
skip Kalman updates when encountering anomalous observations. We investigate
the behaviour of our model on synthetic data and show on standard benchmark
datasets that our method is competitive with state-of-the-art methods while
being computationally cheaper.","['Christian Bock', 'François-Xavier Aubet', 'Jan Gasthaus', 'Andrey Kan', 'Ming Chen', 'Laurent Callot']","['cs.LG', 'stat.ML']",2022-01-18 06:43:32+00:00
http://arxiv.org/abs/2201.06676v2,Observing how deep neural networks understand physics through the energy spectrum of one-dimensional quantum mechanics,"We investigate how neural networks (NNs) understand physics using 1D quantum
mechanics. After training an NN to accurately predict energy eigenvalues from
potentials, we used it to confirm the NN's understanding of physics from four
different aspects. The trained NN could predict energy eigenvalues of different
kinds of potentials than the ones learned, predict the probability distribution
of the existence of particles not used during training, reproduce untrained
physical phenomena, and predict the energy eigenvalues of potentials with an
unknown matter effect. These results show that NNs can learn physical laws from
experimental data, predict the results of experiments under conditions
different from those used for training, and predict physical quantities of
types not provided during training. Because NNs understand physics in a
different way than humans, they will be a powerful tool for advancing physics
by complementing the human way of understanding.",['Kenzo Ogure'],"['physics.comp-ph', 'quant-ph', 'stat.ML']",2022-01-18 00:35:28+00:00
http://arxiv.org/abs/2201.06652v2,Equitable Community Resilience: The Case of Winter Storm Uri in Texas,"Community resilience in the face of natural hazards relies on a community's
potential to bounce back. A failure to integrate equity into resilience
considerations results in unequal recovery and disproportionate impacts on
vulnerable populations, which has long been a concern in the United States.
This research investigated aspects of equity related to community resilience in
the aftermath of Winter Storm Uri in Texas which led to extended power outages
for more than 4 million households. County level outage and recovery data was
analyzed to explore potential significant links between various county
attributes and their share of the outages during the recovery and restoration
phases. Next, satellite imagery was used to examine data at a much higher
geographical resolution focusing on census tracts in the city of Houston. The
goal was to use computer vision to extract the extent of outages within census
tracts and investigate their linkages to census tracts attributes. Results from
various statistical procedures revealed statistically significant negative
associations between counties' percentage of non-Hispanic whites and median
household income with the ratio of outages. Additionally, at census tract
level, variables including percentages of linguistically isolated population
and public transport users exhibited positive associations with the group of
census tracts that were affected by the outage as detected by computer vision
analysis. Informed by these results, engineering solutions such as the
applicability of grid modernization technologies, together with distributed and
renewable energy resources, when controlled for the region's topographical
characteristics, are proposed to enhance equitable power grid resiliency in the
face of natural hazards.","['Ali Nejat', 'Laura Solitare', 'Edward Pettitt', 'Hamed Mohsenian-Rad']","['stat.ML', 'cs.LG', 'stat.AP']",2022-01-17 22:54:07+00:00
http://arxiv.org/abs/2201.06616v2,Improving the quality control of seismic data through active learning,"In image denoising problems, the increasing density of available images makes
an exhaustive visual inspection impossible and therefore automated methods
based on machine-learning must be deployed for this purpose. This is
particulary the case in seismic signal processing. Engineers/geophysicists have
to deal with millions of seismic time series. Finding the sub-surface
properties useful for the oil industry may take up to a year and is very costly
in terms of computing/human resources. In particular, the data must go through
different steps of noise attenuation. Each denoise step is then ideally
followed by a quality control (QC) stage performed by means of human expertise.
To learn a quality control classifier in a supervised manner, labeled training
data must be available, but collecting the labels from human experts is
extremely time-consuming. We therefore propose a novel active learning
methodology to sequentially select the most relevant data, which are then given
back to a human expert for labeling. Beyond the application in geophysics, the
technique we promote in this paper, based on estimates of the local error and
its uncertainty, is generic. Its performance is supported by strong empirical
evidence, as illustrated by the numerical experiments presented in this
article, where it is compared to alternative active learning strategies both on
synthetic and real seismic datasets.","['Mathieu Chambefort', 'Raphaël Butez', 'Emilie Chautru', 'Stephan Clémençon']","['stat.ML', 'cs.LG']",2022-01-17 20:15:37+00:00
http://arxiv.org/abs/2201.06605v2,Inferential Theory for Granular Instrumental Variables in High Dimensions,"The Granular Instrumental Variables (GIV) methodology exploits panels with
factor error structures to construct instruments to estimate structural time
series models with endogeneity even after controlling for latent factors. We
extend the GIV methodology in several dimensions. First, we extend the
identification procedure to a large $N$ and large $T$ framework, which depends
on the asymptotic Herfindahl index of the size distribution of $N$
cross-sectional units. Second, we treat both the factors and loadings as
unknown and show that the sampling error in the estimated instrument and
factors is negligible when considering the limiting distribution of the
structural parameters. Third, we show that the sampling error in the
high-dimensional precision matrix is negligible in our estimation algorithm.
Fourth, we overidentify the structural parameters with additional constructed
instruments, which leads to efficiency gains. Monte Carlo evidence is presented
to support our asymptotic theory and application to the global crude oil market
leads to new results.","['Saman Banafti', 'Tae-Hwy Lee']","['econ.EM', 'stat.ML']",2022-01-17 19:41:24+00:00
http://arxiv.org/abs/2201.06532v3,A New Look at Dynamic Regret for Non-Stationary Stochastic Bandits,"We study the non-stationary stochastic multi-armed bandit problem, where the
reward statistics of each arm may change several times during the course of
learning. The performance of a learning algorithm is evaluated in terms of
their dynamic regret, which is defined as the difference between the expected
cumulative reward of an agent choosing the optimal arm in every time step and
the cumulative reward of the learning algorithm. One way to measure the
hardness of such environments is to consider how many times the identity of the
optimal arm can change. We propose a method that achieves, in $K$-armed bandit
problems, a near-optimal $\widetilde O(\sqrt{K N(S+1)})$ dynamic regret, where
$N$ is the time horizon of the problem and $S$ is the number of times the
identity of the optimal arm changes, without prior knowledge of $S$. Previous
works for this problem obtain regret bounds that scale with the number of
changes (or the amount of change) in the reward functions, which can be much
larger, or assume prior knowledge of $S$ to achieve similar bounds.","['Yasin Abbasi-Yadkori', 'Andras Gyorgy', 'Nevena Lazic']","['cs.LG', 'stat.ML']",2022-01-17 17:23:56+00:00
http://arxiv.org/abs/2201.06515v2,Differentiable Rule Induction with Learned Relational Features,"Rule-based decision models are attractive due to their interpretability.
However, existing rule induction methods often result in long and consequently
less interpretable rule models. This problem can often be attributed to the
lack of appropriately expressive vocabulary, i.e., relevant predicates used as
literals in the decision model. Most existing rule induction algorithms presume
pre-defined literals, naturally decoupling the definition of the literals from
the rule learning phase. In contrast, we propose the Relational Rule Network
(R2N), a neural architecture that learns literals that represent a linear
relationship among numerical input features along with the rules that use them.
This approach opens the door to increasing the expressiveness of induced
decision models by coupling literal learning directly with rule learning in an
end-to-end differentiable fashion. On benchmark tasks, we show that these
learned literals are simple enough to retain interpretability, yet improve
prediction accuracy and provide sets of rules that are more concise compared to
state-of-the-art rule induction algorithms.","['Remy Kusters', 'Yusik Kim', 'Marine Collery', 'Christian de Sainte Marie', 'Shubham Gupta']","['stat.ML', 'cs.LG', 'stat.ME']",2022-01-17 16:46:50+00:00
http://arxiv.org/abs/2201.06487v6,Minimax risk classifiers with 0-1 loss,"Supervised classification techniques use training samples to learn a
classification rule with small expected 0-1 loss (error probability).
Conventional methods enable tractable learning and provide out-of-sample
generalization by using surrogate losses instead of the 0-1 loss and
considering specific families of rules (hypothesis classes). This paper
presents minimax risk classifiers (MRCs) that minize the worst-case 0-1 loss
with respect to uncertainty sets of distributions that can include the
underlying distribution, with a tunable confidence. We show that MRCs can
provide tight performance guarantees at learning and are strongly universally
consistent using feature mappings given by characteristic kernels. The paper
also proposes efficient optimization techniques for MRC learning and shows that
the methods presented can provide accurate classification together with tight
performance guarantees in practice.","['Santiago Mazuelas', 'Mauricio Romero', 'Peter Grünwald']","['stat.ML', 'cs.LG']",2022-01-17 16:00:07+00:00
http://arxiv.org/abs/2201.06468v2,Chaining Value Functions for Off-Policy Learning,"To accumulate knowledge and improve its policy of behaviour, a reinforcement
learning agent can learn `off-policy' about policies that differ from the
policy used to generate its experience. This is important to learn
counterfactuals, or because the experience was generated out of its own
control. However, off-policy learning is non-trivial, and standard
reinforcement-learning algorithms can be unstable and divergent.
  In this paper we discuss a novel family of off-policy prediction algorithms
which are convergent by construction. The idea is to first learn on-policy
about the data-generating behaviour, and then bootstrap an off-policy value
estimate on this on-policy estimate, thereby constructing a value estimate that
is partially off-policy. This process can be repeated to build a chain of value
functions, each time bootstrapping a new estimate on the previous estimate in
the chain. Each step in the chain is stable and hence the complete algorithm is
guaranteed to be stable. Under mild conditions this comes arbitrarily close to
the off-policy TD solution when we increase the length of the chain. Hence it
can compute the solution even in cases where off-policy TD diverges.
  We prove that the proposed scheme is convergent and corresponds to an
iterative decomposition of the inverse key matrix. Furthermore it can be
interpreted as estimating a novel objective -- that we call a `k-step
expedition' -- of following the target policy for finitely many steps before
continuing indefinitely with the behaviour policy. Empirically we evaluate the
idea on challenging MDPs such as Baird's counter example and observe favourable
results.","['Simon Schmitt', 'John Shawe-Taylor', 'Hado van Hasselt']","['cs.LG', 'cs.AI', 'stat.ML']",2022-01-17 15:26:47+00:00
http://arxiv.org/abs/2201.06463v4,Bayesian Calibration of Imperfect Computer Models using Physics-Informed Priors,"We introduce a computational efficient data-driven framework suitable for
quantifying the uncertainty in physical parameters and model formulation of
computer models, represented by differential equations. We construct
physics-informed priors, which are multi-output GP priors that encode the
model's structure in the covariance function. This is extended into a fully
Bayesian framework that quantifies the uncertainty of physical parameters and
model predictions. Since physical models often are imperfect descriptions of
the real process, we allow the model to deviate from the observed data by
considering a discrepancy function. For inference, Hamiltonian Monte Carlo is
used. Further, approximations for big data are developed that reduce the
computational complexity from $\mathcal{O}(N^3)$ to $\mathcal{O}(N\cdot m^2),$
where $m \ll N.$ Our approach is demonstrated in simulation and real data case
studies where the physics are described by time-dependent ODEs describe
(cardiovascular models) and space-time dependent PDEs (heat equation). In the
studies, it is shown that our modelling framework can recover the true
parameters of the physical models in cases where 1) the reality is more complex
than our modelling choice and 2) the data acquisition process is biased while
also producing accurate predictions. Furthermore, it is demonstrated that our
approach is computationally faster than traditional Bayesian calibration
methods.","['Michail Spitieris', 'Ingelin Steinsland']","['stat.ML', 'cs.LG', 'stat.ME']",2022-01-17 15:16:26+00:00
http://arxiv.org/abs/2201.06461v1,Using machine learning to parametrize postmerger signals from binary neutron stars,"There is growing interest in the detection and characterization of
gravitational waves from postmerger oscillations of binary neutron stars. These
signals contain information about the nature of the remnant and the
high-density and out-of-equilibrium physics of the postmerger processes, which
would complement any electromagnetic signal. However, the construction of
binary neutron star postmerger waveforms is much more complicated than for
binary black holes: (i) there are theoretical uncertainties in the neutron-star
equation of state and other aspects of the high-density physics, (ii) numerical
simulations are expensive and available ones only cover a small fraction of the
parameter space with limited numerical accuracy, and (iii) it is unclear how to
parametrize the theoretical uncertainties and interpolate across parameter
space. In this work, we describe the use of a machine-learning method called a
conditional variational autoencoder (CVAE) to construct postmerger models for
hyper/massive neutron star remnant signals based on numerical-relativity
simulations. The CVAE provides a probabilistic model, which encodes
uncertainties in the training data within a set of latent parameters. We
estimate that training such a model will ultimately require $\sim 10^4$
waveforms. However, using synthetic training waveforms as a proof-of-principle,
we show that the CVAE can be used as an accurate generative model and that it
encodes the equation of state in a useful latent representation.","['Tim Whittaker', 'William E. East', 'Stephen R. Green', 'Luis Lehner', 'Huan Yang']","['gr-qc', 'astro-ph.HE', 'cs.LG', 'stat.ML']",2022-01-17 15:14:35+00:00
http://arxiv.org/abs/2201.06438v2,Matrix Reordering for Noisy Disordered Matrices: Optimality and Computationally Efficient Algorithms,"Motivated by applications in single-cell biology and metagenomics, we
investigate the problem of matrix reordering based on a noisy disordered
monotone Toeplitz matrix model. We establish the fundamental statistical limit
for this problem in a decision-theoretic framework and demonstrate that a
constrained least squares estimator achieves the optimal rate. However, due to
its computational complexity, we analyze a popular polynomial-time algorithm,
spectral seriation, and show that it is suboptimal. To address this, we propose
a novel polynomial-time adaptive sorting algorithm with guaranteed performance
improvement. Simulations and analyses of two real single-cell RNA sequencing
datasets demonstrate the superiority of our algorithm over existing methods.","['T. Tony Cai', 'Rong Ma']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2022-01-17 14:53:52+00:00
http://arxiv.org/abs/2201.06391v1,Tk-merge: Computationally Efficient Robust Clustering Under General Assumptions,"We address general-shaped clustering problems under very weak parametric
assumptions with a two-step hybrid robust clustering algorithm based on trimmed
k-means and hierarchical agglomeration. The algorithm has low computational
complexity and effectively identifies the clusters also in presence of data
contamination. We also present natural generalizations of the approach as well
as an adaptive procedure to estimate the amount of contamination in a
data-driven fashion. Our proposal outperforms state-of-the-art robust,
model-based methods in our numerical simulations and real-world applications
related to color quantization for image analysis, human mobility patterns based
on GPS data, biomedical images of diabetic retinopathy, and functional data
across weather stations.","['Luca Insolia', 'Domenico Perrotta']","['stat.ME', 'stat.AP', 'stat.ML']",2022-01-17 13:05:05+00:00
http://arxiv.org/abs/2201.06343v1,Fair Interpretable Learning via Correction Vectors,"Neural network architectures have been extensively employed in the fair
representation learning setting, where the objective is to learn a new
representation for a given vector which is independent of sensitive
information. Various ""representation debiasing"" techniques have been proposed
in the literature. However, as neural networks are inherently opaque, these
methods are hard to comprehend, which limits their usefulness. We propose a new
framework for fair representation learning which is centered around the
learning of ""correction vectors"", which have the same dimensionality as the
given data vectors. The corrections are then simply summed up to the original
features, and can therefore be analyzed as an explicit penalty or bonus to each
feature. We show experimentally that a fair representation learning problem
constrained in such a way does not impact performance.","['Mattia Cerrato', 'Marius Köppel', 'Alexander Segner', 'Stefan Kramer']","['cs.LG', 'stat.ML']",2022-01-17 10:59:33+00:00
http://arxiv.org/abs/2201.06314v1,Efficient Hyperparameter Tuning for Large Scale Kernel Ridge Regression,"Kernel methods provide a principled approach to nonparametric learning. While
their basic implementations scale poorly to large problems, recent advances
showed that approximate solvers can efficiently handle massive datasets. A
shortcoming of these solutions is that hyperparameter tuning is not taken care
of, and left for the user to perform. Hyperparameters are crucial in practice
and the lack of automated tuning greatly hinders efficiency and usability. In
this paper, we work to fill in this gap focusing on kernel ridge regression
based on the Nystr\""om approximation. After reviewing and contrasting a number
of hyperparameter tuning strategies, we propose a complexity regularization
criterion based on a data dependent penalty, and discuss its efficient
optimization. Then, we proceed to a careful and extensive empirical evaluation
highlighting strengths and weaknesses of the different tuning strategies. Our
analysis shows the benefit of the proposed approach, that we hence incorporate
in a library for large scale kernel methods to derive adaptively tuned
solutions.","['Giacomo Meanti', 'Luigi Carratino', 'Ernesto De Vito', 'Lorenzo Rosasco']","['cs.LG', 'stat.ML']",2022-01-17 09:57:32+00:00
http://arxiv.org/abs/2201.06277v1,Risk bounds for PU learning under Selected At Random assumption,"Positive-unlabeled learning (PU learning) is known as a special case of
semi-supervised binary classification where only a fraction of positive
examples are labeled. The challenge is then to find the correct classifier
despite this lack of information. Recently, new methodologies have been
introduced to address the case where the probability of being labeled may
depend on the covariates. In this paper, we are interested in establishing risk
bounds for PU learning under this general assumption. In addition, we quantify
the impact of label noise on PU learning compared to standard classification
setting. Finally, we provide a lower bound on minimax risk proving that the
upper bound is almost optimal.","['Olivier Coudray', 'Christine Keribin', 'Pascal Massart', 'Patrick Pamphile']","['math.ST', 'stat.ML', 'stat.TH']",2022-01-17 08:45:39+00:00
http://arxiv.org/abs/2201.06247v2,Contrastive Regularization for Semi-Supervised Learning,"Consistency regularization on label predictions becomes a fundamental
technique in semi-supervised learning, but it still requires a large number of
training iterations for high performance. In this study, we analyze that the
consistency regularization restricts the propagation of labeling information
due to the exclusion of samples with unconfident pseudo-labels in the model
updates. Then, we propose contrastive regularization to improve both efficiency
and accuracy of the consistency regularization by well-clustered features of
unlabeled data. In specific, after strongly augmented samples are assigned to
clusters by their pseudo-labels, our contrastive regularization updates the
model so that the features with confident pseudo-labels aggregate the features
in the same cluster, while pushing away features in different clusters. As a
result, the information of confident pseudo-labels can be effectively
propagated into more unlabeled samples during training by the well-clustered
features. On benchmarks of semi-supervised learning tasks, our contrastive
regularization improves the previous consistency-based methods and achieves
state-of-the-art results, especially with fewer training iterations. Our method
also shows robust performance on open-set semi-supervised learning where
unlabeled data includes out-of-distribution samples.","['Doyup Lee', 'Sungwoong Kim', 'Ildoo Kim', 'Yeongjae Cheon', 'Minsu Cho', 'Wook-Shin Han']","['cs.LG', 'stat.ML']",2022-01-17 07:20:11+00:00
http://arxiv.org/abs/2201.06229v2,Targeted Optimal Treatment Regime Learning Using Summary Statistics,"Personalized decision-making, aiming to derive optimal treatment regimes
based on individual characteristics, has recently attracted increasing
attention in many fields, such as medicine, social services, and economics.
Current literature mainly focuses on estimating treatment regimes from a single
source population. In real-world applications, the distribution of a target
population can be different from that of the source population. Therefore,
treatment regimes learned by existing methods may not generalize well to the
target population. Due to privacy concerns and other practical issues,
individual-level data from the target population is often not available, which
makes treatment regime learning more challenging. We consider the problem of
treatment regime estimation when the source and target populations may be
heterogeneous, individual-level data is available from the source population,
and only the summary information of covariates, such as moments, is accessible
from the target population. We develop a weighting framework that tailors a
treatment regime for a given target population by leveraging the available
summary statistics. Specifically, we propose a calibrated augmented inverse
probability weighted estimator of the value function for the target population
and estimate an optimal treatment regime by maximizing this estimator within a
class of pre-specified regimes. We show that the proposed calibrated estimator
is consistent and asymptotically normal even with flexible semi/nonparametric
models for nuisance function approximation, and the variance of the value
estimator can be consistently estimated. We demonstrate the empirical
performance of the proposed method using simulation studies and a real
application to an eICU dataset as the source sample and a MIMIC-III dataset as
the target sample.","['Jianing Chu', 'Wenbin Lu', 'Shu Yang']","['stat.ME', 'stat.AP', 'stat.ML']",2022-01-17 06:11:31+00:00
http://arxiv.org/abs/2201.06169v3,On Well-posedness and Minimax Optimal Rates of Nonparametric Q-function Estimation in Off-policy Evaluation,"We study the off-policy evaluation (OPE) problem in an infinite-horizon
Markov decision process with continuous states and actions. We recast the
$Q$-function estimation into a special form of the nonparametric instrumental
variables (NPIV) estimation problem. We first show that under one mild
condition the NPIV formulation of $Q$-function estimation is well-posed in the
sense of $L^2$-measure of ill-posedness with respect to the data generating
distribution, bypassing a strong assumption on the discount factor $\gamma$
imposed in the recent literature for obtaining the $L^2$ convergence rates of
various $Q$-function estimators. Thanks to this new well-posed property, we
derive the first minimax lower bounds for the convergence rates of
nonparametric estimation of $Q$-function and its derivatives in both sup-norm
and $L^2$-norm, which are shown to be the same as those for the classical
nonparametric regression (Stone, 1982). We then propose a sieve two-stage least
squares estimator and establish its rate-optimality in both norms under some
mild conditions. Our general results on the well-posedness and the minimax
lower bounds are of independent interest to study not only other nonparametric
estimators for $Q$-function but also efficient estimation on the value of any
target policy in off-policy settings.","['Xiaohong Chen', 'Zhengling Qi']","['math.ST', 'cs.LG', 'econ.EM', 'stat.ML', 'stat.TH']",2022-01-17 01:09:38+00:00
http://arxiv.org/abs/2201.06153v1,Reconstruction of Incomplete Wildfire Data using Deep Generative Models,"We present our submission to the Extreme Value Analysis 2021 Data Challenge
in which teams were asked to accurately predict distributions of wildfire
frequency and size within spatio-temporal regions of missing data. For the
purpose of this competition we developed a variant of the powerful variational
autoencoder models dubbed the Conditional Missing data Importance-Weighted
Autoencoder (CMIWAE). Our deep latent variable generative model requires little
to no feature engineering and does not necessarily rely on the specifics of
scoring in the Data Challenge. It is fully trained on incomplete data, with the
single objective to maximize log-likelihood of the observed wildfire
information. We mitigate the effects of the relatively low number of training
samples by stochastic sampling from a variational latent variable distribution,
as well as by ensembling a set of CMIWAE models trained and validated on
different splits of the provided data. The presented approach is not
domain-specific and is amenable to application in other missing data recovery
tasks with tabular or image-like information conditioned on auxiliary
information.","['Tomislav Ivek', 'Domagoj Vlah']","['stat.ML', 'cs.LG', 'cs.NE']",2022-01-16 23:27:31+00:00
http://arxiv.org/abs/2201.06142v1,Towards Sample-efficient Overparameterized Meta-learning,"An overarching goal in machine learning is to build a generalizable model
with few samples. To this end, overparameterization has been the subject of
immense interest to explain the generalization ability of deep nets even when
the size of the dataset is smaller than that of the model. While the prior
literature focuses on the classical supervised setting, this paper aims to
demystify overparameterization for meta-learning. Here we have a sequence of
linear-regression tasks and we ask: (1) Given earlier tasks, what is the
optimal linear representation of features for a new downstream task? and (2)
How many samples do we need to build this representation? This work shows that
surprisingly, overparameterization arises as a natural answer to these
fundamental meta-learning questions. Specifically, for (1), we first show that
learning the optimal representation coincides with the problem of designing a
task-aware regularization to promote inductive bias. We leverage this inductive
bias to explain how the downstream task actually benefits from
overparameterization, in contrast to prior works on few-shot learning. For (2),
we develop a theory to explain how feature covariance can implicitly help
reduce the sample complexity well below the degrees of freedom and lead to
small estimation error. We then integrate these findings to obtain an overall
performance guarantee for our meta-learning algorithm. Numerical experiments on
real and synthetic data verify our insights on overparameterized meta-learning.","['Yue Sun', 'Adhyyan Narang', 'Halil Ibrahim Gulluk', 'Samet Oymak', 'Maryam Fazel']","['cs.LG', 'stat.ML']",2022-01-16 21:57:17+00:00
http://arxiv.org/abs/2201.06133v1,On Maximum-a-Posteriori estimation with Plug & Play priors and stochastic gradient descent,"Bayesian methods to solve imaging inverse problems usually combine an
explicit data likelihood function with a prior distribution that explicitly
models expected properties of the solution. Many kinds of priors have been
explored in the literature, from simple ones expressing local properties to
more involved ones exploiting image redundancy at a non-local scale. In a
departure from explicit modelling, several recent works have proposed and
studied the use of implicit priors defined by an image denoising algorithm.
This approach, commonly known as Plug & Play (PnP) regularisation, can deliver
remarkably accurate results, particularly when combined with state-of-the-art
denoisers based on convolutional neural networks. However, the theoretical
analysis of PnP Bayesian models and algorithms is difficult and works on the
topic often rely on unrealistic assumptions on the properties of the image
denoiser. This papers studies maximum-a-posteriori (MAP) estimation for
Bayesian models with PnP priors. We first consider questions related to
existence, stability and well-posedness, and then present a convergence proof
for MAP computation by PnP stochastic gradient descent (PnP-SGD) under
realistic assumptions on the denoiser used. We report a range of imaging
experiments demonstrating PnP-SGD as well as comparisons with other PnP
schemes.","['Rémi Laumont', 'Valentin de Bortoli', 'Andrés Almansa', 'Julie Delon', 'Alain Durmus', 'Marcelo Pereyra']","['stat.ML', 'cs.CV', 'cs.LG', 'eess.IV', 'math.OC', '65K10 (Primary) 65K05, 62F15, 62C10, 68Q25, 68U10, 90C26 (Secondary)\n  65K10, 65K05, 62F15, 62C10, 68Q25, 68U10, 90C26']",2022-01-16 20:50:08+00:00
http://arxiv.org/abs/2201.06064v2,Neighborhood Region Smoothing Regularization for Finding Flat Minima In Deep Neural Networks,"Due to diverse architectures in deep neural networks (DNNs) with severe
overparameterization, regularization techniques are critical for finding
optimal solutions in the huge hypothesis space. In this paper, we propose an
effective regularization technique, called Neighborhood Region Smoothing (NRS).
NRS leverages the finding that models would benefit from converging to flat
minima, and tries to regularize the neighborhood region in weight space to
yield approximate outputs. Specifically, gap between outputs of models in the
neighborhood region is gauged by a defined metric based on Kullback-Leibler
divergence. This metric provides similar insights with the minimum description
length principle on interpreting flat minima. By minimizing both this
divergence and empirical loss, NRS could explicitly drive the optimizer towards
converging to flat minima. We confirm the effectiveness of NRS by performing
image classification tasks across a wide range of model architectures on
commonly-used datasets such as CIFAR and ImageNet, where generalization ability
could be universally improved. Also, we empirically show that the minima found
by NRS would have relatively smaller Hessian eigenvalues compared to the
conventional method, which is considered as the evidence of flat minima.","['Yang Zhao', 'Hao Zhang']","['cs.LG', 'stat.ML', 'I.2.10']",2022-01-16 15:11:00+00:00
http://arxiv.org/abs/2201.05974v2,Fractional SDE-Net: Generation of Time Series Data with Long-term Memory,"In this paper, we focus on the generation of time-series data using neural
networks. It is often the case that input time-series data have only one
realized (and usually irregularly sampled) path, which makes it difficult to
extract time-series characteristics, and its noise structure is more
complicated than i.i.d. type. Time series data, especially from hydrology,
telecommunications, economics, and finance, exhibit long-term memory also
called long-range dependency (LRD). The main purpose of this paper is to
artificially generate time series with the help of neural networks, making the
LRD of paths into account. We propose fSDE-Net: neural fractional Stochastic
Differential Equation Network. It generalizes the neural stochastic
differential equation model by using fractional Brownian motion with a Hurst
index larger than half, which exhibits the LRD property. We derive the solver
of fSDE-Net and theoretically analyze the existence and uniqueness of the
solution to fSDE-Net. Our experiments with artificial and real time-series data
demonstrate that the fSDE-Net model can replicate distributional properties
well.","['Kohei Hayashi', 'Kei Nakagawa']","['cs.LG', 'q-fin.CP', 'stat.ML']",2022-01-16 05:37:02+00:00
http://arxiv.org/abs/2201.05947v2,Universal Online Learning: an Optimistically Universal Learning Rule,"We study the subject of universal online learning with non-i.i.d. processes
for bounded losses. The notion of an universally consistent learning was
defined by Hanneke in an effort to study learning theory under minimal
assumptions, where the objective is to obtain low long-run average loss for any
target function. We are interested in characterizing processes for which
learning is possible and whether there exist learning rules guaranteed to be
universally consistent given the only assumption that such learning is
possible. The case of unbounded losses is very restrictive, since the learnable
processes almost surely visit a finite number of points and as a result, simple
memorization is optimistically universal. We focus on the bounded setting and
give a complete characterization of the processes admitting strong and weak
universal learning. We further show that k-nearest neighbor algorithm (kNN) is
not optimistically universal and present a novel variant of 1NN which is
optimistically universal for general input and value spaces in both strong and
weak setting. This closes all COLT 2021 open problems posed by Hanneke on
universal online learning.",['Moïse Blanchard'],"['cs.LG', 'stat.ML']",2022-01-16 02:13:47+00:00
http://arxiv.org/abs/2201.05923v1,Theoretical analysis and computation of the sample Frechet mean for sets of large graphs based on spectral information,"To characterize the location (mean, median) of a set of graphs, one needs a
notion of centrality that is adapted to metric spaces, since graph sets are not
Euclidean spaces. A standard approach is to consider the Frechet mean. In this
work, we equip a set of graphs with the pseudometric defined by the norm
between the eigenvalues of their respective adjacency matrix. Unlike the edit
distance, this pseudometric reveals structural changes at multiple scales, and
is well adapted to studying various statistical problems for graph-valued data.
We describe an algorithm to compute an approximation to the sample Frechet mean
of a set of undirected unweighted graphs with a fixed size using this
pseudometric.","['Daniel Ferguson', 'Francois G. Meyer']","['stat.ML', 'cs.LG', 'cs.SI', 'physics.data-an']",2022-01-15 20:53:29+00:00
http://arxiv.org/abs/2201.05893v2,Treatment Effect Risk: Bounds and Inference,"Since the average treatment effect (ATE) measures the change in social
welfare, even if positive, there is a risk of negative effect on, say, some 10%
of the population. Assessing such risk is difficult, however, because any one
individual treatment effect (ITE) is never observed, so the 10% worst-affected
cannot be identified, while distributional treatment effects only compare the
first deciles within each treatment group, which does not correspond to any
10%-subpopulation. In this paper we consider how to nonetheless assess this
important risk measure, formalized as the conditional value at risk (CVaR) of
the ITE-distribution. We leverage the availability of pre-treatment covariates
and characterize the tightest-possible upper and lower bounds on ITE-CVaR given
by the covariate-conditional average treatment effect (CATE) function. We then
proceed to study how to estimate these bounds efficiently from data and
construct confidence intervals. This is challenging even in randomized
experiments as it requires understanding the distribution of the unknown CATE
function, which can be very complex if we use rich covariates so as to best
control for heterogeneity. We develop a debiasing method that overcomes this
and prove it enjoys favorable statistical properties even when CATE and other
nuisances are estimated by black-box machine learning or even inconsistently.
Studying a hypothetical change to French job-search counseling services, our
bounds and inference demonstrate a small social benefit entails a negative
impact on a substantial subpopulation.",['Nathan Kallus'],"['stat.ME', 'econ.EM', 'math.OC', 'stat.ML']",2022-01-15 17:21:26+00:00
http://arxiv.org/abs/2201.05890v1,Robust uncertainty estimates with out-of-distribution pseudo-inputs training,"Probabilistic models often use neural networks to control their predictive
uncertainty. However, when making out-of-distribution (OOD)} predictions, the
often-uncontrollable extrapolation properties of neural networks yield poor
uncertainty predictions. Such models then don't know what they don't know,
which directly limits their robustness w.r.t unexpected inputs. To counter
this, we propose to explicitly train the uncertainty predictor where we are not
given data to make it reliable. As one cannot train without data, we provide
mechanisms for generating pseudo-inputs in informative low-density regions of
the input space, and show how to leverage these in a practical Bayesian
framework that casts a prior distribution over the model uncertainty. With a
holistic evaluation, we demonstrate that this yields robust and interpretable
predictions of uncertainty while retaining state-of-the-art performance on
diverse tasks such as regression and generative modelling","['Pierre Segonne', 'Yevgen Zainchkovskyy', 'Søren Hauberg']","['cs.LG', 'stat.ML']",2022-01-15 17:15:07+00:00
http://arxiv.org/abs/2201.05830v1,Physical Derivatives: Computing policy gradients by physical forward-propagation,"Model-free and model-based reinforcement learning are two ends of a spectrum.
Learning a good policy without a dynamic model can be prohibitively expensive.
Learning the dynamic model of a system can reduce the cost of learning the
policy, but it can also introduce bias if it is not accurate. We propose a
middle ground where instead of the transition model, the sensitivity of the
trajectories with respect to the perturbation of the parameters is learned.
This allows us to predict the local behavior of the physical system around a
set of nominal policies without knowing the actual model. We assay our method
on a custom-built physical robot in extensive experiments and show the
feasibility of the approach in practice. We investigate potential challenges
when applying our method to physical systems and propose solutions to each of
them.","['Arash Mehrjou', 'Ashkan Soleymani', 'Stefan Bauer', 'Bernhard Schölkopf']","['cs.RO', 'math.DS', 'stat.ML']",2022-01-15 11:27:42+00:00
http://arxiv.org/abs/2201.05759v2,FairIF: Boosting Fairness in Deep Learning via Influence Functions with Validation Set Sensitive Attributes,"Most fair machine learning methods either highly rely on the sensitive
information of the training samples or require a large modification on the
target models, which hinders their practical application. To address this
issue, we propose a two-stage training algorithm named FAIRIF. It minimizes the
loss over the reweighted data set (second stage) where the sample weights are
computed to balance the model performance across different demographic groups
(first stage). FAIRIF can be applied on a wide range of models trained by
stochastic gradient descent without changing the model, while only requiring
group annotations on a small validation set to compute sample weights.
Theoretically, we show that, in the classification setting, three notions of
disparity among different groups can be mitigated by training with the weights.
Experiments on synthetic data sets demonstrate that FAIRIF yields models with
better fairness-utility trade-offs against various types of bias; and on
real-world data sets, we show the effectiveness and scalability of FAIRIF.
Moreover, as evidenced by the experiments with pretrained models, FAIRIF is
able to alleviate the unfairness issue of pretrained models without hurting
their performance.","['Haonan Wang', 'Ziwei Wu', 'Jingrui He']","['cs.LG', 'cs.CY', 'stat.ML']",2022-01-15 05:14:48+00:00
http://arxiv.org/abs/2201.05666v1,Reliable Causal Discovery with Improved Exact Search and Weaker Assumptions,"Many of the causal discovery methods rely on the faithfulness assumption to
guarantee asymptotic correctness. However, the assumption can be approximately
violated in many ways, leading to sub-optimal solutions. Although there is a
line of research in Bayesian network structure learning that focuses on
weakening the assumption, such as exact search methods with well-defined score
functions, they do not scale well to large graphs. In this work, we introduce
several strategies to improve the scalability of exact score-based methods in
the linear Gaussian setting. In particular, we develop a super-structure
estimation method based on the support of inverse covariance matrix which
requires assumptions that are strictly weaker than faithfulness, and apply it
to restrict the search space of exact search. We also propose a local search
strategy that performs exact search on the local clusters formed by each
variable and its neighbors within two hops in the super-structure. Numerical
experiments validate the efficacy of the proposed procedure, and demonstrate
that it scales up to hundreds of nodes with a high accuracy.","['Ignavier Ng', 'Yujia Zheng', 'Jiji Zhang', 'Kun Zhang']","['cs.LG', 'stat.ME', 'stat.ML']",2022-01-14 20:52:30+00:00
http://arxiv.org/abs/2201.05634v1,Imputing Missing Observations with Time Sliced Synthetic Minority Oversampling Technique,"We present a simple yet novel time series imputation technique with the goal
of constructing an irregular time series that is uniform across every sample in
a data set. Specifically, we fix a grid defined by the midpoints of
non-overlapping bins (dubbed ""slices"") of observation times and ensure that
each sample has values for all of the features at that given time. This allows
one to both impute fully missing observations to allow uniform time series
classification across the entire data and, in special cases, to impute
individually missing features. To do so, we slightly generalize the well-known
class imbalance algorithm SMOTE \cite{smote} to allow component wise nearest
neighbor interpolation that preserves correlations when there are no missing
features. We visualize the method in the simplified setting of 2-dimensional
uncoupled harmonic oscillators. Next, we use tSMOTE to train an Encoder/Decoder
long-short term memory (LSTM) model with Logistic Regression for predicting and
classifying distinct trajectories of different 2D oscillators. After
illustrating the the utility of tSMOTE in this context, we use the same
architecture to train a clinical model for COVID-19 disease severity on an
imputed data set. Our experiments show an improvement over standard mean and
median imputation techniques by allowing a wider class of patient trajectories
to be recognized by the model, as well as improvement over aggregated
classification models.","['Andrew Baumgartner', 'Sevda Molani', 'Qi Wei', 'Jennifer Hadlock']","['cs.LG', 'physics.bio-ph', 'q-bio.QM', 'stat.ML']",2022-01-14 19:23:24+00:00
http://arxiv.org/abs/2201.05565v1,Estimating Gaussian Copulas with Missing Data,"In this work we present a rigorous application of the Expectation
Maximization algorithm to determine the marginal distributions and the
dependence structure in a Gaussian copula model with missing data. We further
show how to circumvent a priori assumptions on the marginals with
semiparametric modelling. The joint distribution learned through this algorithm
is considerably closer to the underlying distribution than existing methods.","['Maximilian Kertel', 'Markus Pauly']","['stat.ML', 'cs.LG', 'stat.ME']",2022-01-14 17:20:44+00:00
http://arxiv.org/abs/2201.05464v1,Bayesian sense of time in biological and artificial brains,"Enquiries concerning the underlying mechanisms and the emergent properties of
a biological brain have a long history of theoretical postulates and
experimental findings. Today, the scientific community tends to converge to a
single interpretation of the brain's cognitive underpinnings -- that it is a
Bayesian inference machine. This contemporary view has naturally been a strong
driving force in recent developments around computational and cognitive
neurosciences. Of particular interest is the brain's ability to process the
passage of time -- one of the fundamental dimensions of our experience. How can
we explain empirical data on human time perception using the Bayesian brain
hypothesis? Can we replicate human estimation biases using Bayesian models?
What insights can the agent-based machine learning models provide for the study
of this subject? In this chapter, we review some of the recent advancements in
the field of time perception and discuss the role of Bayesian processing in the
construction of temporal models.","['Zafeirios Fountas', 'Alexey Zakharov']","['q-bio.NC', 'cs.AI', 'stat.ML']",2022-01-14 14:05:30+00:00
http://arxiv.org/abs/2201.05405v1,The Implicit Regularization of Momentum Gradient Descent with Early Stopping,"The study on the implicit regularization induced by gradient-based
optimization is a longstanding pursuit. In the present paper, we characterize
the implicit regularization of momentum gradient descent (MGD) with early
stopping by comparing with the explicit $\ell_2$-regularization (ridge). In
details, we study MGD in the continuous-time view, so-called momentum gradient
flow (MGF), and show that its tendency is closer to ridge than the gradient
descent (GD) [Ali et al., 2019] for least squares regression. Moreover, we
prove that, under the calibration $t=\sqrt{2/\lambda}$, where $t$ is the time
parameter in MGF and $\lambda$ is the tuning parameter in ridge regression, the
risk of MGF is no more than 1.54 times that of ridge. In particular, the
relative Bayes risk of MGF to ridge is between 1 and 1.035 under the optimal
tuning. The numerical experiments support our theoretical results strongly.","['Li Wang', 'Yingcong Zhou', 'Zhiguo Fu']","['cs.LG', 'stat.ML']",2022-01-14 11:50:54+00:00
