id,title,abstract,authors,categories,date
http://arxiv.org/abs/1608.06622v2,The discriminative Kalman filter for nonlinear and non-Gaussian sequential Bayesian filtering,"The Kalman filter (KF) is used in a variety of applications for computing the
posterior distribution of latent states in a state space model. The model
requires a linear relationship between states and observations. Extensions to
the Kalman filter have been proposed that incorporate linear approximations to
nonlinear models, such as the extended Kalman filter (EKF) and the unscented
Kalman filter (UKF). However, we argue that in cases where the dimensionality
of observed variables greatly exceeds the dimensionality of state variables, a
model for $p(\text{state}|\text{observation})$ proves both easier to learn and
more accurate for latent space estimation. We derive and validate what we call
the discriminative Kalman filter (DKF): a closed-form discriminative version of
Bayesian filtering that readily incorporates off-the-shelf discriminative
learning techniques. Further, we demonstrate that given mild assumptions,
highly non-linear models for $p(\text{state}|\text{observation})$ can be
specified. We motivate and validate on synthetic datasets and in neural
decoding from non-human primates, showing substantial increases in decoding
performance versus the standard Kalman filter.","['Michael C. Burkhart', 'David M. Brandman', 'Carlos E. Vargas-Irwin', 'Matthew T. Harrison']",['stat.ML'],2016-08-23 19:53:20+00:00
http://arxiv.org/abs/1608.06582v2,Approximation and inference methods for stochastic biochemical kinetics - a tutorial review,"Stochastic fluctuations of molecule numbers are ubiquitous in biological
systems. Important examples include gene expression and enzymatic processes in
living cells. Such systems are typically modelled as chemical reaction networks
whose dynamics are governed by the Chemical Master Equation. Despite its simple
structure, no analytic solutions to the Chemical Master Equation are known for
most systems. Moreover, stochastic simulations are computationally expensive,
making systematic analysis and statistical inference a challenging task.
Consequently, significant effort has been spent in recent decades on the
development of efficient approximation and inference methods. This article
gives an introduction to basic modelling concepts as well as an overview of
state of the art methods. First, we motivate and introduce deterministic and
stochastic methods for modelling chemical networks, and give an overview of
simulation and exact solution methods. Next, we discuss several approximation
methods, including the chemical Langevin equation, the system size expansion,
moment closure approximations, time-scale separation approximations and hybrid
methods. We discuss their various properties and review recent advances and
remaining challenges for these methods. We present a comparison of several of
these methods by means of a numerical case study and highlight some of their
respective advantages and disadvantages. Finally, we discuss the problem of
inference from experimental data in the Bayesian framework and review recent
methods developed the literature. In summary, this review gives a
self-contained introduction to modelling, approximations and inference methods
for stochastic chemical kinetics.","['David Schnoerr', 'Guido Sanguinetti', 'Ramon Grima']","['q-bio.QM', 'cond-mat.stat-mech', 'physics.bio-ph', 'q-bio.MN', 'stat.ML']",2016-08-23 17:13:05+00:00
http://arxiv.org/abs/1608.06412v1,Stability revisited: new generalisation bounds for the Leave-one-Out,"The present paper provides a new generic strategy leading to non-asymptotic
theoretical guarantees on the Leave-one-Out procedure applied to a broad class
of learning algorithms. This strategy relies on two main ingredients: the new
notion of $L^q$ stability, and the strong use of moment inequalities. $L^q$
stability extends the ongoing notion of hypothesis stability while remaining
weaker than the uniform stability. It leads to new PAC exponential
generalisation bounds for Leave-one-Out under mild assumptions. In the
literature, such bounds are available only for uniform stable algorithms under
boundedness for instance. Our generic strategy is applied to the Ridge
regression algorithm as a first step.","['Alain Celisse', 'Benjamin Guedj']","['stat.ML', 'math.ST', 'stat.TH']",2016-08-23 08:11:29+00:00
http://arxiv.org/abs/1608.06383v1,Softplus Regressions and Convex Polytopes,"To construct flexible nonlinear predictive distributions, the paper
introduces a family of softplus function based regression models that convolve,
stack, or combine both operations by convolving countably infinite stacked
gamma distributions, whose scales depend on the covariates. Generalizing
logistic regression that uses a single hyperplane to partition the covariate
space into two halves, softplus regressions employ multiple hyperplanes to
construct a confined space, related to a single convex polytope defined by the
intersection of multiple half-spaces or a union of multiple convex polytopes,
to separate one class from the other. The gamma process is introduced to
support the convolution of countably infinite (stacked) covariate-dependent
gamma distributions. For Bayesian inference, Gibbs sampling derived via novel
data augmentation and marginalization techniques is used to deconvolve and/or
demix the highly complex nonlinear predictive distribution. Example results
demonstrate that softplus regressions provide flexible nonlinear decision
boundaries, achieving classification accuracies comparable to that of kernel
support vector machine while requiring significant less computation for
out-of-sample prediction.",['Mingyuan Zhou'],"['stat.ML', 'stat.ME']",2016-08-23 05:15:25+00:00
http://arxiv.org/abs/1608.06315v1,LFADS - Latent Factor Analysis via Dynamical Systems,"Neuroscience is experiencing a data revolution in which many hundreds or
thousands of neurons are recorded simultaneously. Currently, there is little
consensus on how such data should be analyzed. Here we introduce LFADS (Latent
Factor Analysis via Dynamical Systems), a method to infer latent dynamics from
simultaneously recorded, single-trial, high-dimensional neural spiking data.
LFADS is a sequential model based on a variational auto-encoder. By making a
dynamical systems hypothesis regarding the generation of the observed data,
LFADS reduces observed spiking to a set of low-dimensional temporal factors,
per-trial initial conditions, and inferred inputs. We compare LFADS to existing
methods on synthetic data and show that it significantly out-performs them in
inferring neural firing rates and latent dynamics.","['David Sussillo', 'Rafal Jozefowicz', 'L. F. Abbott', 'Chethan Pandarinath']","['cs.LG', 'q-bio.NC', 'stat.ML']",2016-08-22 21:15:00+00:00
http://arxiv.org/abs/1608.06296v2,Neural networks for the prediction organic chemistry reactions,"Reaction prediction remains one of the major challenges for organic
chemistry, and is a pre-requisite for efficient synthetic planning. It is
desirable to develop algorithms that, like humans, ""learn"" from being exposed
to examples of the application of the rules of organic chemistry. We explore
the use of neural networks for predicting reaction types, using a new reaction
fingerprinting method. We combine this predictor with SMARTS transformations to
build a system which, given a set of reagents and re- actants, predicts the
likely products. We test this method on problems from a popular organic
chemistry textbook.","['Jennifer N. Wei', 'David Duvenaud', 'Al√°n Aspuru-Guzik']","['physics.chem-ph', 'q-bio.QM', 'stat.ML']",2016-08-22 20:01:05+00:00
http://arxiv.org/abs/1608.06253v1,Multi-Dueling Bandits and Their Application to Online Ranker Evaluation,"New ranking algorithms are continually being developed and refined,
necessitating the development of efficient methods for evaluating these
rankers. Online ranker evaluation focuses on the challenge of efficiently
determining, from implicit user feedback, which ranker out of a finite set of
rankers is the best. Online ranker evaluation can be modeled by dueling ban-
dits, a mathematical model for online learning under limited feedback from
pairwise comparisons. Comparisons of pairs of rankers is performed by
interleaving their result sets and examining which documents users click on.
The dueling bandits model addresses the key issue of which pair of rankers to
compare at each iteration, thereby providing a solution to the
exploration-exploitation trade-off. Recently, methods for simultaneously
comparing more than two rankers have been developed. However, the question of
which rankers to compare at each iteration was left open. We address this
question by proposing a generalization of the dueling bandits model that uses
simultaneous comparisons of an unrestricted number of rankers. We evaluate our
algorithm on synthetic data and several standard large-scale online ranker
evaluation datasets. Our experimental results show that the algorithm yields
orders of magnitude improvement in performance compared to stateof- the-art
dueling bandit algorithms.","['Brian Brost', 'Yevgeny Seldin', 'Ingemar J. Cox', 'Christina Lioma']","['cs.IR', 'cs.LG', 'stat.ML']",2016-08-22 18:20:18+00:00
http://arxiv.org/abs/1608.06238v1,Single-shot Adaptive Measurement for Quantum-enhanced Metrology,"Quantum-enhanced metrology aims to estimate an unknown parameter such that
the precision scales better than the shot-noise bound. Single-shot adaptive
quantum-enhanced metrology (AQEM) is a promising approach that uses feedback to
tweak the quantum process according to previous measurement outcomes.
Techniques and formalism for the adaptive case are quite different from the
usual non-adaptive quantum metrology approach due to the causal relationship
between measurements and outcomes. We construct a formal framework for AQEM by
modeling the procedure as a decision-making process, and we derive the
imprecision and the Cram\'{e}r-Rao lower bound with explicit dependence on the
feedback policy. We also explain the reinforcement learning approach for
generating quantum control policies, which is adopted due to the optimal policy
being non-trivial to devise. Applying a learning algorithm based on
differential evolution enables us to attain imprecision for adaptive
interferometric phase estimation, which turns out to be SQL when non-entangled
particles are used in the scheme.","['Pantita Palittapongarnpim', 'Peter Wittek', 'Barry C. Sanders']","['quant-ph', 'stat.ML']",2016-08-22 17:52:28+00:00
http://arxiv.org/abs/1608.06203v1,Computational and Statistical Tradeoffs in Learning to Rank,"For massive and heterogeneous modern datasets, it is of fundamental interest
to provide guarantees on the accuracy of estimation when computational
resources are limited. In the application of learning to rank, we provide a
hierarchy of rank-breaking mechanisms ordered by the complexity in thus
generated sketch of the data. This allows the number of data points collected
to be gracefully traded off against computational resources available, while
guaranteeing the desired level of accuracy. Theoretical guarantees on the
proposed generalized rank-breaking implicitly provide such trade-offs, which
can be explicitly characterized under certain canonical scenarios on the
structure of the data.","['Ashish Khetan', 'Sewoong Oh']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2016-08-22 15:58:31+00:00
http://arxiv.org/abs/1608.06072v2,"Uniform Generalization, Concentration, and Adaptive Learning","One fundamental goal in any learning algorithm is to mitigate its risk for
overfitting. Mathematically, this requires that the learning algorithm enjoys a
small generalization risk, which is defined either in expectation or in
probability. Both types of generalization are commonly used in the literature.
For instance, generalization in expectation has been used to analyze
algorithms, such as ridge regression and SGD, whereas generalization in
probability is used in the VC theory, among others. Recently, a third notion of
generalization has been studied, called uniform generalization, which requires
that the generalization risk vanishes uniformly in expectation across all
bounded parametric losses. It has been shown that uniform generalization is, in
fact, equivalent to an information-theoretic stability constraint, and that it
recovers classical results in learning theory. It is achievable under various
settings, such as sample compression schemes, finite hypothesis spaces, finite
domains, and differential privacy. However, the relationship between uniform
generalization and concentration remained unknown. In this paper, we answer
this question by proving that, while a generalization in expectation does not
imply a generalization in probability, a uniform generalization in expectation
does imply concentration. We establish a chain rule for the uniform
generalization risk of the composition of hypotheses and use it to derive a
large deviation bound. Finally, we prove that the bound is tight.",['Ibrahim Alabdulmohsin'],"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML', '68T05, 94A15', 'I.2.6']",2016-08-22 07:47:56+00:00
http://arxiv.org/abs/1608.06048v1,Survey of resampling techniques for improving classification performance in unbalanced datasets,"A number of classification problems need to deal with data imbalance between
classes. Often it is desired to have a high recall on the minority class while
maintaining a high precision on the majority class. In this paper, we review a
number of resampling techniques proposed in literature to handle unbalanced
datasets and study their effect on classification performance.",['Ajinkya More'],"['stat.AP', 'cs.LG', 'stat.ML']",2016-08-22 04:27:28+00:00
http://arxiv.org/abs/1608.06031v2,Towards Instance Optimal Bounds for Best Arm Identification,"In the classical best arm identification (Best-$1$-Arm) problem, we are given
$n$ stochastic bandit arms, each associated with a reward distribution with an
unknown mean. We would like to identify the arm with the largest mean with
probability at least $1-\delta$, using as few samples as possible.
Understanding the sample complexity of Best-$1$-Arm has attracted significant
attention since the last decade. However, the exact sample complexity of the
problem is still unknown.
  Recently, Chen and Li made the gap-entropy conjecture concerning the instance
sample complexity of Best-$1$-Arm. Given an instance $I$, let $\mu_{[i]}$ be
the $i$th largest mean and $\Delta_{[i]}=\mu_{[1]}-\mu_{[i]}$ be the
corresponding gap. $H(I)=\sum_{i=2}^n\Delta_{[i]}^{-2}$ is the complexity of
the instance. The gap-entropy conjecture states that
$\Omega\left(H(I)\cdot\left(\ln\delta^{-1}+\mathsf{Ent}(I)\right)\right)$ is an
instance lower bound, where $\mathsf{Ent}(I)$ is an entropy-like term
determined by the gaps, and there is a $\delta$-correct algorithm for
Best-$1$-Arm with sample complexity
$O\left(H(I)\cdot\left(\ln\delta^{-1}+\mathsf{Ent}(I)\right)+\Delta_{[2]}^{-2}\ln\ln\Delta_{[2]}^{-1}\right)$.
If the conjecture is true, we would have a complete understanding of the
instance-wise sample complexity of Best-$1$-Arm.
  We make significant progress towards the resolution of the gap-entropy
conjecture. For the upper bound, we provide a highly nontrivial algorithm which
requires \[O\left(H(I)\cdot\left(\ln\delta^{-1}
+\mathsf{Ent}(I)\right)+\Delta_{[2]}^{-2}\ln\ln\Delta_{[2]}^{-1}\mathrm{polylog}(n,\delta^{-1})\right)\]
samples in expectation. For the lower bound, we show that for any Gaussian
Best-$1$-Arm instance with gaps of the form $2^{-k}$, any $\delta$-correct
monotone algorithm requires $\Omega\left(H(I)\cdot\left(\ln\delta^{-1} +
\mathsf{Ent}(I)\right)\right)$ samples in expectation.","['Lijie Chen', 'Jian Li', 'Mingda Qiao']","['cs.LG', 'cs.DS', 'stat.ML']",2016-08-22 02:05:10+00:00
http://arxiv.org/abs/1608.06014v2,The Symmetry of a Simple Optimization Problem in Lasso Screening,"Recently dictionary screening has been proposed as an effective way to
improve the computational efficiency of solving the lasso problem, which is one
of the most commonly used method for learning sparse representations. To
address today's ever increasing large dataset, effective screening relies on a
tight region bound on the solution to the dual lasso. Typical region bounds are
in the form of an intersection of a sphere and multiple half spaces. One way to
tighten the region bound is using more half spaces, which however, adds to the
overhead of solving the high dimensional optimization problem in lasso
screening. This paper reveals the interesting property that the optimization
problem only depends on the projection of features onto the subspace spanned by
the normals of the half spaces. This property converts an optimization problem
in high dimension to much lower dimension, and thus sheds light on reducing the
computation overhead of lasso screening based on tighter region bounds.","['Yun Wang', 'Peter J. Ramadge']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2016-08-21 23:48:43+00:00
http://arxiv.org/abs/1608.06010v2,Feedback-Controlled Sequential Lasso Screening,"One way to solve lasso problems when the dictionary does not fit into
available memory is to first screen the dictionary to remove unneeded features.
Prior research has shown that sequential screening methods offer the greatest
promise in this endeavor. Most existing work on sequential screening targets
the context of tuning parameter selection, where one screens and solves a
sequence of $N$ lasso problems with a fixed grid of geometrically spaced
regularization parameters. In contrast, we focus on the scenario where a target
regularization parameter has already been chosen via cross-validated model
selection, and we then need to solve many lasso instances using this fixed
value. In this context, we propose and explore a feedback controlled sequential
screening scheme. Feedback is used at each iteration to select the next problem
to be solved. This allows the sequence of problems to be adapted to the
instance presented and the number of intermediate problems to be automatically
selected. We demonstrate our feedback scheme using several datasets including a
dictionary of approximate size 100,000 by 300,000.","['Yun Wang', 'Xu Chen', 'Peter J. Ramadge']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2016-08-21 23:40:56+00:00
http://arxiv.org/abs/1608.05995v5,A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing,"We develop an efficient alternating framework for learning a generalized
version of Factorization Machine (gFM) on steaming data with provable
guarantees. When the instances are sampled from $d$ dimensional random Gaussian
vectors and the target second order coefficient matrix in gFM is of rank $k$,
our algorithm converges linearly, achieves $O(\epsilon)$ recovery error after
retrieving $O(k^{3}d\log(1/\epsilon))$ training instances, consumes $O(kd)$
memory in one-pass of dataset and only requires matrix-vector product
operations in each iteration. The key ingredient of our framework is a
construction of an estimation sequence endowed with a so-called Conditionally
Independent RIP condition (CI-RIP). As special cases of gFM, our framework can
be applied to symmetric or asymmetric rank-one matrix sensing problems, such as
inductive matrix completion and phase retrieval.","['Ming Lin', 'Jieping Ye']","['stat.ML', 'cs.LG']",2016-08-21 20:28:29+00:00
http://arxiv.org/abs/1608.05983v2,Inverting Variational Autoencoders for Improved Generative Accuracy,"Recent advances in semi-supervised learning with deep generative models have
shown promise in generalizing from small labeled datasets
($\mathbf{x},\mathbf{y}$) to large unlabeled ones ($\mathbf{x}$). In the case
where the codomain has known structure, a large unfeatured dataset
($\mathbf{y}$) is potentially available. We develop a parameter-efficient, deep
semi-supervised generative model for the purpose of exploiting this untapped
data source. Empirical results show improved performance in disentangling
latent variable semantics as well as improved discriminative prediction on
Martian spectroscopic and handwritten digit domains.","['Ian Gemp', 'Ishan Durugkar', 'Mario Parente', 'M. Darby Dyar', 'Sridhar Mahadevan']","['cs.LG', 'stat.ML']",2016-08-21 19:02:27+00:00
http://arxiv.org/abs/1608.05934v1,Spatial Modeling of Oil Exploration Areas Using Neural Networks and ANFIS in GIS,"Exploration of hydrocarbon resources is a highly complicated and expensive
process where various geological, geochemical and geophysical factors are
developed then combined together. It is highly significant how to design the
seismic data acquisition survey and locate the exploratory wells since
incorrect or imprecise locations lead to waste of time and money during the
operation. The objective of this study is to locate high-potential oil and gas
field in 1: 250,000 sheet of Ahwaz including 20 oil fields to reduce both time
and costs in exploration and production processes. In this regard, 17 maps were
developed using GIS functions for factors including: minimum and maximum of
total organic carbon (TOC), yield potential for hydrocarbons production (PP),
Tmax peak, production index (PI), oxygen index (OI), hydrogen index (HI) as
well as presence or proximity to high residual Bouguer gravity anomalies,
proximity to anticline axis and faults, topography and curvature maps obtained
from Asmari Formation subsurface contours. To model and to integrate maps, this
study employed artificial neural network and adaptive neuro-fuzzy inference
system (ANFIS) methods. The results obtained from model validation demonstrated
that the 17x10x5 neural network with R=0.8948, RMS=0.0267, and kappa=0.9079 can
be trained better than other models such as ANFIS and predicts the potential
areas more accurately. However, this method failed to predict some oil fields
and wrongly predict some areas as potential zones.","['Nouraddin Misagh', 'Mohammadreza Ashouri']",['stat.ML'],2016-08-21 13:00:45+00:00
http://arxiv.org/abs/1608.05921v2,Probabilistic Knowledge Graph Construction: Compositional and Incremental Approaches,"Knowledge graph construction consists of two tasks: extracting information
from external resources (knowledge population) and inferring missing
information through a statistical analysis on the extracted information
(knowledge completion). In many cases, insufficient external resources in the
knowledge population hinder the subsequent statistical inference. The gap
between these two processes can be reduced by an incremental population
approach. We propose a new probabilistic knowledge graph factorisation method
that benefits from the path structure of existing knowledge (e.g. syllogism)
and enables a common modelling approach to be used for both incremental
population and knowledge completion tasks. More specifically, the probabilistic
formulation allows us to develop an incremental population algorithm that
trades off exploitation-exploration. Experiments on three benchmark datasets
show that the balanced exploitation-exploration helps the incremental
population, and the additional path structure helps to predict missing
information in knowledge completion.","['Dongwoo Kim', 'Lexing Xie', 'Cheng Soon Ong']","['stat.ML', 'cs.AI', 'cs.LG']",2016-08-21 11:49:53+00:00
http://arxiv.org/abs/1608.05910v2,Mining of health and disease events on Twitter: validating search protocols within the setting of Indonesia,"This study seeks to validate a search protocol of ill health-related terms
using Twitter data which can later be used to understand if, and how, Twitter
can reveal information on the current health situation. We extracted
conversations related to health and disease postings on Twitter using a set of
pre-defined keywords, assessed the prevalence, frequency, and timing of such
content in these conversations, and validated how this search protocol was able
to detect relevant disease tweets. Classification and Regression Trees (CART)
algorithm was used to train and test search protocols of disease and health
hits comparing to those identified by our team. The accuracy of predictions
showed a good validity with AUC beyond 0.8. Our study shows that monitoring of
public sentiment on Twitter can be used as a real-time proxy for health events.","['Aditya L. Ramadona', 'Rendra Agusta', 'Sulistyawati', 'Lutfan Lazuardi', 'Anwar D. Cahyono', '√Ösa Holmner', 'Fatwa S. T. Dewi', 'Hari Kusnanto', 'Joacim R√∂cklov']","['stat.ML', 'cs.CY', 'cs.SI']",2016-08-21 09:57:12+00:00
http://arxiv.org/abs/1608.05889v1,Online Feature Selection with Group Structure Analysis,"Online selection of dynamic features has attracted intensive interest in
recent years. However, existing online feature selection methods evaluate
features individually and ignore the underlying structure of feature stream.
For instance, in image analysis, features are generated in groups which
represent color, texture and other visual information. Simply breaking the
group structure in feature selection may degrade performance. Motivated by this
fact, we formulate the problem as an online group feature selection. The
problem assumes that features are generated individually but there are group
structure in the feature stream. To the best of our knowledge, this is the
first time that the correlation among feature stream has been considered in the
online feature selection process. To solve this problem, we develop a novel
online group feature selection method named OGFS. Our proposed approach
consists of two stages: online intra-group selection and online inter-group
selection. In the intra-group selection, we design a criterion based on
spectral analysis to select discriminative features in each group. In the
inter-group selection, we utilize a linear regression model to select an
optimal subset. This two-stage procedure continues until there are no more
features arriving or some predefined stopping conditions are met. %Our method
has been applied Finally, we apply our method to multiple tasks including image
classification %, face verification and face verification. Extensive empirical
studies performed on real-world and benchmark data sets demonstrate that our
method outperforms other state-of-the-art online feature selection %method
methods.","['Jing Wang', 'Meng Wang', 'Peipei Li', 'Luoqi Liu', 'Zhongqiu Zhao', 'Xuegang Hu', 'Xindong Wu']","['cs.CV', 'cs.LG', 'stat.ML']",2016-08-21 02:39:48+00:00
http://arxiv.org/abs/1608.05878v2,The ground truth about metadata and community detection in networks,"Across many scientific domains, there is a common need to automatically
extract a simplified view or coarse-graining of how a complex system's
components interact. This general task is called community detection in
networks and is analogous to searching for clusters in independent vector data.
It is common to evaluate the performance of community detection algorithms by
their ability to find so-called ""ground truth"" communities. This works well in
synthetic networks with planted communities because such networks' links are
formed explicitly based on those known communities. However, there are no
planted communities in real world networks. Instead, it is standard practice to
treat some observed discrete-valued node attributes, or metadata, as ground
truth. Here, we show that metadata are not the same as ground truth, and that
treating them as such induces severe theoretical and practical problems. We
prove that no algorithm can uniquely solve community detection, and we prove a
general No Free Lunch theorem for community detection, which implies that there
can be no algorithm that is optimal for all possible community detection tasks.
However, community detection remains a powerful tool and node metadata still
have value so a careful exploration of their relationship with network
structure can yield insights of genuine worth. We illustrate this point by
introducing two statistical techniques that can quantify the relationship
between metadata and community structure for a broad class of models. We
demonstrate these techniques using both synthetic and real-world networks, and
for multiple types of metadata and community structure.","['Leto Peel', 'Daniel B. Larremore', 'Aaron Clauset']","['cs.SI', 'physics.data-an', 'physics.soc-ph', 'stat.ML']",2016-08-20 23:57:12+00:00
http://arxiv.org/abs/1608.05806v1,Reweighting with Boosted Decision Trees,"Machine learning tools are commonly used in modern high energy physics (HEP)
experiments. Different models, such as boosted decision trees (BDT) and
artificial neural networks (ANN), are widely used in analyses and even in the
software triggers.
  In most cases, these are classification models used to select the ""signal""
events from data. Monte Carlo simulated events typically take part in training
of these models. While the results of the simulation are expected to be close
to real data, in practical cases there is notable disagreement between
simulated and observed data. In order to use available simulation in training,
corrections must be introduced to generated data. One common approach is
reweighting - assigning weights to the simulated events. We present a novel
method of event reweighting based on boosted decision trees. The problem of
checking the quality of reweighting step in analyses is also discussed.",['A. Rogozhnikov'],"['physics.data-an', 'hep-ex', 'stat.ML']",2016-08-20 09:30:29+00:00
http://arxiv.org/abs/1608.07251v1,Large-scale Collaborative Imaging Genetics Studies of Risk Genetic Factors for Alzheimer's Disease Across Multiple Institutions,"Genome-wide association studies (GWAS) offer new opportunities to identify
genetic risk factors for Alzheimer's disease (AD). Recently, collaborative
efforts across different institutions emerged that enhance the power of many
existing techniques on individual institution data. However, a major barrier to
collaborative studies of GWAS is that many institutions need to preserve
individual data privacy. To address this challenge, we propose a novel
distributed framework, termed Local Query Model (LQM) to detect risk SNPs for
AD across multiple research institutions. To accelerate the learning process,
we propose a Distributed Enhanced Dual Polytope Projection (D-EDPP) screening
rule to identify irrelevant features and remove them from the optimization. To
the best of our knowledge, this is the first successful run of the
computationally intensive model selection procedure to learn a consistent model
across different institutions without compromising their privacy while ranking
the SNPs that may collectively affect AD. Empirical studies are conducted on
809 subjects with 5.9 million SNP features which are distributed across three
individual institutions. D-EDPP achieved a 66-fold speed-up by effectively
identifying irrelevant features.","['Qingyang Li', 'Tao Yang', 'Liang Zhan', 'Derrek Paul Hibar', 'Neda Jahanshad', 'Yalin Wang', 'Jieping Ye', 'Paul M. Thompson', 'Jie Wang']","['cs.LG', 'stat.ML']",2016-08-19 23:21:49+00:00
http://arxiv.org/abs/1608.05749v1,Solving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization,"We consider the problem of solving mixed random linear equations with $k$
components. This is the noiseless setting of mixed linear regression. The goal
is to estimate multiple linear models from mixed samples in the case where the
labels (which sample corresponds to which model) are not observed. We give a
tractable algorithm for the mixed linear equation problem, and show that under
some technical conditions, our algorithm is guaranteed to solve the problem
exactly with sample complexity linear in the dimension, and polynomial in $k$,
the number of components. Previous approaches have required either exponential
dependence on $k$, or super-linear dependence on the dimension. The proposed
algorithm is a combination of tensor decomposition and alternating
minimization. Our analysis involves proving that the initialization provided by
the tensor method allows alternating minimization, which is equivalent to EM in
our setting, to converge to the global optimum at a linear rate.","['Xinyang Yi', 'Constantine Caramanis', 'Sujay Sanghavi']","['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2016-08-19 22:10:46+00:00
http://arxiv.org/abs/1608.05747v1,Space-Filling Curves as a Novel Crystal Structure Representation for Machine Learning Models,"A fundamental problem in applying machine learning techniques for chemical
problems is to find suitable representations for molecular and crystal
structures. While the structure representations based on atom connectivities
are prevalent for molecules, two-dimensional descriptors are not suitable for
describing molecular crystals. In this work, we introduce the SFC-M family of
feature representations, which are based on Morton space-filling curves, as an
alternative means of representing crystal structures. Latent Semantic Indexing
(LSI) was employed in a novel setting to reduce sparsity of feature
representations. The quality of the SFC-M representations were assessed by
using them in combination with artificial neural networks to predict Density
Functional Theory (DFT) single point, Ewald summed, lattice, and many-body
dispersion energies of 839 organic molecular crystal unit cells from the
Cambridge Structural Database that consist of the elements C, H, N, and O.
Promising initial results suggest that the SFC-M representations merit further
exploration to improve its ability to predict solid-state properties of organic
crystal structures","['Dipti Jasrasaria', 'Edward O. Pyzer-Knapp', 'Dmitrij Rappoport', 'Alan Aspuru-Guzik']","['stat.ML', 'physics.chem-ph']",2016-08-19 22:01:43+00:00
http://arxiv.org/abs/1608.05610v2,A Strongly Quasiconvex PAC-Bayesian Bound,"We propose a new PAC-Bayesian bound and a way of constructing a hypothesis
space, so that the bound is convex in the posterior distribution and also
convex in a trade-off parameter between empirical performance of the posterior
distribution and its complexity. The complexity is measured by the
Kullback-Leibler divergence to a prior. We derive an alternating procedure for
minimizing the bound. We show that the bound can be rewritten as a
one-dimensional function of the trade-off parameter and provide sufficient
conditions under which the function has a single global minimum. When the
conditions are satisfied the alternating minimization is guaranteed to converge
to the global minimum of the bound. We provide experimental results
demonstrating that rigorous minimization of the bound is competitive with
cross-validation in tuning the trade-off between complexity and empirical
performance. In all our experiments the trade-off turned to be quasiconvex even
when the sufficient conditions were violated.","['Niklas Thiemann', 'Christian Igel', 'Olivier Wintenberger', 'Yevgeny Seldin']","['cs.LG', 'stat.ML']",2016-08-19 14:21:18+00:00
http://arxiv.org/abs/1608.05581v5,Unsupervised Feature Selection Based on the Morisita Estimator of Intrinsic Dimension,"This paper deals with a new filter algorithm for selecting the smallest
subset of features carrying all the information content of a data set (i.e. for
removing redundant features). It is an advanced version of the fractal
dimension reduction technique, and it relies on the recently introduced
Morisita estimator of Intrinsic Dimension (ID). Here, the ID is used to
quantify dependencies between subsets of features, which allows the effective
processing of highly non-linear data. The proposed algorithm is successfully
tested on simulated and real world case studies. Different levels of sample
size and noise are examined along with the variability of the results. In
addition, a comprehensive procedure based on random forests shows that the data
dimensionality is significantly reduced by the algorithm without loss of
relevant information. And finally, comparisons with benchmark feature selection
techniques demonstrate the promising performance of this new filter.","['Jean Golay', 'Mikhail Kanevski']","['stat.ML', 'cs.LG']",2016-08-19 12:28:21+00:00
http://arxiv.org/abs/1608.05560v1,Iterative Views Agreement: An Iterative Low-Rank based Structured Optimization Method to Multi-View Spectral Clustering,"Multi-view spectral clustering, which aims at yielding an agreement or
consensus data objects grouping across multi-views with their graph laplacian
matrices, is a fundamental clustering problem. Among the existing methods,
Low-Rank Representation (LRR) based method is quite superior in terms of its
effectiveness, intuitiveness and robustness to noise corruptions. However, it
aggressively tries to learn a common low-dimensional subspace for multi-view
data, while inattentively ignoring the local manifold structure in each view,
which is critically important to the spectral clustering; worse still, the
low-rank minimization is enforced to achieve the data correlation consensus
among all views, failing to flexibly preserve the local manifold structure for
each view. In this paper, 1) we propose a multi-graph laplacian regularized LRR
with each graph laplacian corresponding to one view to characterize its local
manifold structure. 2) Instead of directly enforcing the low-rank minimization
among all views for correlation consensus, we separately impose low-rank
constraint on each view, coupled with a mutual structural consensus constraint,
where it is able to not only well preserve the local manifold structure but
also serve as a constraint for that from other views, which iteratively makes
the views more agreeable. Extensive experiments on real-world multi-view data
sets demonstrate its superiority.","['Yang Wang', 'Wenjie Zhang', 'Lin Wu', 'Xuemin Lin', 'Meng Fang', 'Shirui Pan']","['cs.LG', 'stat.ML']",2016-08-19 10:25:46+00:00
http://arxiv.org/abs/1608.05493v1,Network Volume Anomaly Detection and Identification in Large-scale Networks based on Online Time-structured Traffic Tensor Tracking,"This paper addresses network anomography, that is, the problem of inferring
network-level anomalies from indirect link measurements. This problem is cast
as a low-rank subspace tracking problem for normal flows under incomplete
observations, and an outlier detection problem for abnormal flows. Since
traffic data is large-scale time-structured data accompanied with noise and
outliers under partial observations, an efficient modeling method is essential.
To this end, this paper proposes an online subspace tracking of a Hankelized
time-structured traffic tensor for normal flows based on the Candecomp/PARAFAC
decomposition exploiting the recursive least squares (RLS) algorithm. We
estimate abnormal flows as outlier sparse flows via sparsity maximization in
the underlying under-constrained linear-inverse problem. A major advantage is
that our algorithm estimates normal flows by low-dimensional matrices with
time-directional features as well as the spatial correlation of multiple links
without using the past observed measurements and the past model parameters.
Extensive numerical evaluations show that the proposed algorithm achieves
faster convergence per iteration of model approximation, and better volume
anomaly detection performance compared to state-of-the-art algorithms.","['Hiroyuki Kasai', 'Wolfgang Kellerer', 'Martin Kleinsteuber']","['cs.NI', 'stat.ML']",2016-08-19 05:06:58+00:00
http://arxiv.org/abs/1608.05347v1,Probabilistic Data Analysis with Probabilistic Programming,"Probabilistic techniques are central to data analysis, but different
approaches can be difficult to apply, combine, and compare. This paper
introduces composable generative population models (CGPMs), a computational
abstraction that extends directed graphical models and can be used to describe
and compose a broad class of probabilistic data analysis techniques. Examples
include hierarchical Bayesian models, multivariate kernel methods,
discriminative machine learning, clustering algorithms, dimensionality
reduction, and arbitrary probabilistic programs. We also demonstrate the
integration of CGPMs into BayesDB, a probabilistic programming platform that
can express data analysis tasks using a modeling language and a structured
query language. The practical value is illustrated in two ways. First, CGPMs
are used in an analysis that identifies satellite data records which probably
violate Kepler's Third Law, by composing causal probabilistic programs with
non-parametric Bayes in under 50 lines of probabilistic code. Second, for
several representative data analysis tasks, we report on lines of code and
accuracy measurements of various CGPMs, plus comparisons with standard baseline
solutions from Python and MATLAB libraries.","['Feras Saad', 'Vikash Mansinghka']","['cs.AI', 'cs.LG', 'stat.ML']",2016-08-18 17:47:53+00:00
http://arxiv.org/abs/1608.05275v1,A Tight Convex Upper Bound on the Likelihood of a Finite Mixture,"The likelihood function of a finite mixture model is a non-convex function
with multiple local maxima and commonly used iterative algorithms such as EM
will converge to different solutions depending on initial conditions. In this
paper we ask: is it possible to assess how far we are from the global maximum
of the likelihood? Since the likelihood of a finite mixture model can grow
unboundedly by centering a Gaussian on a single datapoint and shrinking the
covariance, we constrain the problem by assuming that the parameters of the
individual models are members of a large discrete set (e.g. estimating a
mixture of two Gaussians where the means and variances of both Gaussians are
members of a set of a million possible means and variances). For this setting
we show that a simple upper bound on the likelihood can be computed using
convex optimization and we analyze conditions under which the bound is
guaranteed to be tight. This bound can then be used to assess the quality of
solutions found by EM (where the final result is projected on the discrete set)
or any other mixture estimation algorithm. For any dataset our method allows us
to find a finite mixture model together with a dataset-specific bound on how
far the likelihood of this mixture is from the global optimum of the likelihood","['Elad Mezuman', 'Yair Weiss']","['cs.LG', 'stat.ML']",2016-08-18 14:27:45+00:00
http://arxiv.org/abs/1608.05258v1,Parameter Learning for Log-supermodular Distributions,"We consider log-supermodular models on binary variables, which are
probabilistic models with negative log-densities which are submodular. These
models provide probabilistic interpretations of common combinatorial
optimization tasks such as image segmentation. In this paper, we focus
primarily on parameter estimation in the models from known upper-bounds on the
intractable log-partition function. We show that the bound based on separable
optimization on the base polytope of the submodular function is always inferior
to a bound based on ""perturb-and-MAP"" ideas. Then, to learn parameters, given
that our approximation of the log-partition function is an expectation (over
our own randomization), we use a stochastic subgradient technique to maximize a
lower-bound on the log-likelihood. This can also be extended to conditional
maximum likelihood. We illustrate our new results in a set of experiments in
binary image denoising, where we highlight the flexibility of a probabilistic
model to learn with missing data.","['Tatiana Shpakova', 'Francis Bach']","['stat.ML', 'cs.LG']",2016-08-18 13:55:41+00:00
http://arxiv.org/abs/1608.05225v1,Active Learning for Approximation of Expensive Functions with Normal Distributed Output Uncertainty,"When approximating a black-box function, sampling with active learning
focussing on regions with non-linear responses tends to improve accuracy. We
present the FLOLA-Voronoi method introduced previously for deterministic
responses, and theoretically derive the impact of output uncertainty. The
algorithm automatically puts more emphasis on exploration to provide more
information to the models.","['Joachim van der Herten', 'Ivo Couckuyt', 'Dirk Deschrijver', 'Tom Dhaene']","['cs.LG', 'stat.ML']",2016-08-18 10:15:54+00:00
http://arxiv.org/abs/1608.05182v2,A Bayesian Nonparametric Approach for Estimating Individualized Treatment-Response Curves,"We study the problem of estimating the continuous response over time to
interventions using observational time series---a retrospective dataset where
the policy by which the data are generated is unknown to the learner. We are
motivated by applications where response varies by individuals and therefore,
estimating responses at the individual-level is valuable for personalizing
decision-making. We refer to this as the problem of estimating individualized
treatment response (ITR) curves. In statistics, G-computation formula (Robins,
1986) has been commonly used for estimating treatment responses from
observational data containing sequential treatment assignments. However, past
studies have focused predominantly on obtaining point-in-time estimates at the
population level. We leverage the G-computation formula and develop a novel
Bayesian nonparametric (BNP) method that can flexibly model functional data and
provide posterior inference over the treatment response curves at both the
individual and population level. On a challenging dataset containing time
series from patients admitted to a hospital, we estimate responses to
treatments used in managing kidney function and show that the resulting fits
are more accurate than alternative approaches. Accurate methods for obtaining
ITRs from observational data can dramatically accelerate the pace at which
personalized treatment plans become possible.","['Yanbo Xu', 'Yanxun Xu', 'Suchi Saria']","['cs.LG', 'stat.ML']",2016-08-18 05:31:53+00:00
http://arxiv.org/abs/1608.05152v1,Conditional Sparse Linear Regression,"Machine learning and statistics typically focus on building models that
capture the vast majority of the data, possibly ignoring a small subset of data
as ""noise"" or ""outliers."" By contrast, here we consider the problem of jointly
identifying a significant (but perhaps small) segment of a population in which
there is a highly sparse linear regression fit, together with the coefficients
for the linear fit. We contend that such tasks are of interest both because the
models themselves may be able to achieve better predictions in such special
cases, but also because they may aid our understanding of the data. We give
algorithms for such problems under the sup norm, when this unknown segment of
the population is described by a k-DNF condition and the regression fit is
s-sparse for constant k and s. For the variants of this problem when the
regression fit is not so sparse or using expected error, we also give a
preliminary algorithm and highlight the question as a challenge for future
work.",['Brendan Juba'],"['cs.LG', 'cs.DS', 'stat.ML']",2016-08-18 01:30:49+00:00
http://arxiv.org/abs/1608.05138v2,Hybrid CPU-GPU Framework for Network Motifs,"Massively parallel architectures such as the GPU are becoming increasingly
important due to the recent proliferation of data. In this paper, we propose a
key class of hybrid parallel graphlet algorithms that leverages multiple CPUs
and GPUs simultaneously for computing k-vertex induced subgraph statistics
(called graphlets). In addition to the hybrid multi-core CPU-GPU framework, we
also investigate single GPU methods (using multiple cores) and multi-GPU
methods that leverage all available GPUs simultaneously for computing induced
subgraph statistics. Both methods leverage GPU devices only, whereas the hybrid
multi-core CPU-GPU framework leverages all available multi-core CPUs and
multiple GPUs for computing graphlets in large networks. Compared to recent
approaches, our methods are orders of magnitude faster, while also more cost
effective enjoying superior performance per capita and per watt. In particular,
the methods are up to 300 times faster than the recent state-of-the-art method.
To the best of our knowledge, this is the first work to leverage multiple CPUs
and GPUs simultaneously for computing induced subgraph statistics.","['Ryan A. Rossi', 'Rong Zhou']","['cs.DC', 'cs.SI', 'stat.ML', 'H.2.8; I.2.6; G.1.0']",2016-08-18 00:29:21+00:00
http://arxiv.org/abs/1608.05127v1,A Bayesian Network approach to County-Level Corn Yield Prediction using historical data and expert knowledge,"Crop yield forecasting is the methodology of predicting crop yields prior to
harvest. The availability of accurate yield prediction frameworks have enormous
implications from multiple standpoints, including impact on the crop commodity
futures markets, formulation of agricultural policy, as well as crop insurance
rating. The focus of this work is to construct a corn yield predictor at the
county scale. Corn yield (forecasting) depends on a complex, interconnected set
of variables that include economic, agricultural, management and meteorological
factors. Conventional forecasting is either knowledge-based computer programs
(that simulate plant-weather-soil-management interactions) coupled with
targeted surveys or statistical model based. The former is limited by the need
for painstaking calibration, while the latter is limited to univariate analysis
or similar simplifying assumptions that fail to capture the complex
interdependencies affecting yield. In this paper, we propose a data-driven
approach that is ""gray box"" i.e. that seamlessly utilizes expert knowledge in
constructing a statistical network model for corn yield forecasting. Our
multivariate gray box model is developed on Bayesian network analysis to build
a Directed Acyclic Graph (DAG) between predictors and yield. Starting from a
complete graph connecting various carefully chosen variables and yield, expert
knowledge is used to prune or strengthen edges connecting variables.
Subsequently the structure (connectivity and edge weights) of the DAG that
maximizes the likelihood of observing the training data is identified via
optimization. We curated an extensive set of historical data (1948-2012) for
each of the 99 counties in Iowa as data to train the model.","['Vikas Chawla', 'Hsiang Sing Naik', 'Adedotun Akintayo', 'Dermot Hayes', 'Patrick Schnable', 'Baskar Ganapathysubramanian', 'Soumik Sarkar']","['cs.LG', 'stat.AP', 'stat.ML']",2016-08-17 23:30:04+00:00
http://arxiv.org/abs/1608.05081v4,BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems,"We present a new algorithm that significantly improves the efficiency of
exploration for deep Q-learning agents in dialogue systems. Our agents explore
via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop
neural network. Our algorithm learns much faster than common exploration
strategies such as $\epsilon$-greedy, Boltzmann, bootstrapping, and
intrinsic-reward-based ones. Additionally, we show that spiking the replay
buffer with experiences from just a few successful episodes can make Q-learning
feasible when it might otherwise fail.","['Zachary C. Lipton', 'Xiujun Li', 'Jianfeng Gao', 'Lihong Li', 'Faisal Ahmed', 'Li Deng']","['cs.LG', 'cs.NE', 'stat.ML']",2016-08-17 20:00:04+00:00
http://arxiv.org/abs/1608.04972v1,A Three Spatial Dimension Wave Latent Force Model for Describing Excitation Sources and Electric Potentials Produced by Deep Brain Stimulation,"Deep brain stimulation (DBS) is a surgical treatment for Parkinson's Disease.
Static models based on quasi-static approximation are common approaches for DBS
modeling. While this simplification has been validated for bioelectric sources,
its application to rapid stimulation pulses, which contain more high-frequency
power, may not be appropriate, as DBS therapeutic results depend on stimulus
parameters such as frequency and pulse width, which are related to time
variations of the electric field. We propose an alternative hybrid approach
based on probabilistic models and differential equations, by using Gaussian
processes and wave equation. Our model avoids quasi-static approximation,
moreover, it is able to describe dynamic behavior of DBS. Therefore, the
proposed model may be used to obtain a more realistic phenomenon description.
The proposed model can also solve inverse problems, i.e. to recover the
corresponding source of excitation, given electric potential distribution. The
electric potential produced by a time-varying source was predicted using
proposed model. For static sources, the electric potential produced by
different electrode configurations were modeled. Four different sources of
excitation were recovered by solving the inverse problem. We compare our
outcomes with the electric potential obtained by solving Poisson's equation
using the Finite Element Method (FEM). Our approach is able to take into
account time variations of the source and the produced field. Also, inverse
problem can be addressed using the proposed model. The electric potential
calculated with the proposed model is close to the potential obtained by
solving Poisson's equation using FEM.","['Pablo A. Alvarado', 'Mauricio A. √Ålvarez', '√Ålvaro A. Orozco']","['q-bio.NC', 'stat.ML']",2016-08-17 14:19:35+00:00
http://arxiv.org/abs/1608.04961v3,Clustering Mixed Datasets Using Homogeneity Analysis with Applications to Big Data,"Datasets with a mixture of numerical and categorical attributes are routinely
encountered in many application domains. In this work we examine an approach to
clustering such datasets using homogeneity analysis. Homogeneity analysis
determines a euclidean representation of the data. This can be analyzed by
leveraging the large body of tools and techniques for data with a euclidean
representation. Experiments conducted as part of this study suggest that this
approach can be useful in the analysis and exploration of big datasets with a
mixture of numerical and categorical attributes.","['Rajiv Sambasivan', 'Sourish Das']",['stat.ML'],2016-08-17 13:40:31+00:00
http://arxiv.org/abs/1608.04846v1,A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation,"Finding the most effective way to aggregate multi-subject fMRI data is a
long-standing and challenging problem. It is of increasing interest in
contemporary fMRI studies of human cognition due to the scarcity of data per
subject and the variability of brain anatomy and functional response across
subjects. Recent work on latent factor models shows promising results in this
task but this approach does not preserve spatial locality in the brain. We
examine two ways to combine the ideas of a factor model and a searchlight based
analysis to aggregate multi-subject fMRI data while preserving spatial
locality. We first do this directly by combining a recent factor method known
as a shared response model with searchlight analysis. Then we design a
multi-view convolutional autoencoder for the same task. Both approaches
preserve spatial locality and have competitive or better performance compared
with standard searchlight analysis and the shared response model applied across
the whole brain. We also report a system design to handle the computational
challenge of training the convolutional autoencoder.","['Po-Hsuan Chen', 'Xia Zhu', 'Hejia Zhang', 'Javier S. Turek', 'Janice Chen', 'Theodore L. Willke', 'Uri Hasson', 'Peter J. Ramadge']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']",2016-08-17 03:49:56+00:00
http://arxiv.org/abs/1608.04845v1,Lecture Notes on Spectral Graph Methods,"These are lecture notes that are based on the lectures from a class I taught
on the topic of Spectral Graph Methods at UC Berkeley during the Spring 2015
semester.",['Michael W. Mahoney'],"['cs.DS', 'stat.ML']",2016-08-17 03:38:37+00:00
http://arxiv.org/abs/1608.04839v3,Dynamic Collaborative Filtering with Compound Poisson Factorization,"Model-based collaborative filtering analyzes user-item interactions to infer
latent factors that represent user preferences and item characteristics in
order to predict future interactions. Most collaborative filtering algorithms
assume that these latent factors are static, although it has been shown that
user preferences and item perceptions drift over time. In this paper, we
propose a conjugate and numerically stable dynamic matrix factorization (DCPF)
based on compound Poisson matrix factorization that models the smoothly
drifting latent factors using Gamma-Markov chains. We propose a numerically
stable Gamma chain construction, and then present a stochastic variational
inference approach to estimate the parameters of our model. We apply our model
to time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF
achieves a higher predictive accuracy than state-of-the-art static and dynamic
factorization models.","['Ghassen Jerfel', 'Mehmet E. Basbug', 'Barbara E. Engelhardt']","['cs.LG', 'cs.AI', 'stat.ML']",2016-08-17 02:38:44+00:00
http://arxiv.org/abs/1608.04830v1,Outlier Detection on Mixed-Type Data: An Energy-based Approach,"Outlier detection amounts to finding data points that differ significantly
from the norm. Classic outlier detection methods are largely designed for
single data type such as continuous or discrete. However, real world data is
increasingly heterogeneous, where a data point can have both discrete and
continuous attributes. Handling mixed-type data in a disciplined way remains a
great challenge. In this paper, we propose a new unsupervised outlier detection
method for mixed-type data based on Mixed-variate Restricted Boltzmann Machine
(Mv.RBM). The Mv.RBM is a principled probabilistic method that models data
density. We propose to use \emph{free-energy} derived from Mv.RBM as outlier
score to detect outliers as those data points lying in low density regions. The
method is fast to learn and compute, is scalable to massive datasets. At the
same time, the outlier score is identical to data negative log-density up-to an
additive constant. We evaluate the proposed method on synthetic and real-world
datasets and demonstrate that (a) a proper handling mixed-types is necessary in
outlier detection, and (b) free-energy of Mv.RBM is a powerful and efficient
outlier scoring method, which is highly competitive against state-of-the-arts.","['Kien Do', 'Truyen Tran', 'Dinh Phung', 'Svetha Venkatesh']","['stat.ML', 'cs.LG']",2016-08-17 01:41:40+00:00
http://arxiv.org/abs/1608.04802v2,Scalable Learning of Non-Decomposable Objectives,"Modern retrieval systems are often driven by an underlying machine learning
model. The goal of such systems is to identify and possibly rank the few most
relevant items for a given query or context. Thus, such systems are typically
evaluated using a ranking-based performance metric such as the area under the
precision-recall curve, the $F_\beta$ score, precision at fixed recall, etc.
Obviously, it is desirable to train such systems to optimize the metric of
interest.
  In practice, due to the scalability limitations of existing approaches for
optimizing such objectives, large-scale retrieval systems are instead trained
to maximize classification accuracy, in the hope that performance as measured
via the true objective will also be favorable. In this work we present a
unified framework that, using straightforward building block bounds, allows for
highly scalable optimization of a wide range of ranking-based objectives. We
demonstrate the advantage of our approach on several real-life retrieval
problems that are significantly larger than those considered in the literature,
while achieving substantial improvement in performance over the
accuracy-objective baseline.","['Elad ET. Eban', 'Mariano Schain', 'Alan Mackey', 'Ariel Gordon', 'Rif A. Saurous', 'Gal Elidan']","['stat.ML', 'cs.LG']",2016-08-16 23:11:14+00:00
http://arxiv.org/abs/1608.04783v1,Application of multiview techniques to NHANES dataset,"Disease prediction or classification using health datasets involve using
well-known predictors associated with the disease as features for the models.
This study considers multiple data components of an individual's health, using
the relationship between variables to generate features that may improve the
performance of disease classification models. In order to capture information
from different aspects of the data, this project uses a multiview learning
approach, using Canonical Correlation Analysis (CCA), a technique that finds
projections with maximum correlations between two data views. Data categories
collected from the NHANES survey (1999-2014) are used as views to learn the
multiview representations. The usefulness of the representations is
demonstrated by applying them as features in a Diabetes classification task.",['Aileme Omogbai'],"['cs.LG', 'stat.ML']",2016-08-16 21:20:30+00:00
http://arxiv.org/abs/1608.04773v2,Faster Principal Component Regression and Stable Matrix Chebyshev Approximation,"We solve principal component regression (PCR), up to a multiplicative
accuracy $1+\gamma$, by reducing the problem to $\tilde{O}(\gamma^{-1})$
black-box calls of ridge regression. Therefore, our algorithm does not require
any explicit construction of the top principal components, and is suitable for
large-scale PCR instances. In contrast, previous result requires
$\tilde{O}(\gamma^{-2})$ such black-box calls.
  We obtain this result by developing a general stable recurrence formula for
matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to
the matrix sign function. Our techniques may be of independent interests,
especially when designing iterative methods.","['Zeyuan Allen-Zhu', 'Yuanzhi Li']","['stat.ML', 'cs.DS', 'cs.LG', 'math.NA', 'math.OC']",2016-08-16 20:48:02+00:00
http://arxiv.org/abs/1608.04689v1,A Shallow High-Order Parametric Approach to Data Visualization and Compression,"Explicit high-order feature interactions efficiently capture essential
structural knowledge about the data of interest and have been used for
constructing generative models. We present a supervised discriminative
High-Order Parametric Embedding (HOPE) approach to data visualization and
compression. Compared to deep embedding models with complicated deep
architectures, HOPE generates more effective high-order feature mapping through
an embarrassingly simple shallow model. Furthermore, two approaches to
generating a small number of exemplars conveying high-order interactions to
represent large-scale data sets are proposed. These exemplars in combination
with the feature mapping learned by HOPE effectively capture essential data
variations. Moreover, through HOPE, these exemplars are employed to increase
the computational efficiency of kNN classification for fast information
retrieval by thousands of times. For classification in two-dimensional
embedding space on MNIST and USPS datasets, our shallow method HOPE with simple
Sigmoid transformations significantly outperforms state-of-the-art supervised
deep embedding models based on deep neural networks, and even achieved
historically low test error rate of 0.65% in two-dimensional space on MNIST,
which demonstrates the representational efficiency and power of supervised
shallow models with high-order feature interactions.","['Martin Renqiang Min', 'Hongyu Guo', 'Dongjin Song']","['cs.AI', 'cs.LG', 'stat.ML']",2016-08-16 17:54:40+00:00
http://arxiv.org/abs/1608.04674v1,Shape Constrained Tensor Decompositions using Sparse Representations in Over-Complete Libraries,"We consider $N$-way data arrays and low-rank tensor factorizations where the
time mode is coded as a sparse linear combination of temporal elements from an
over-complete library. Our method, Shape Constrained Tensor Decomposition
(SCTD) is based upon the CANDECOMP/PARAFAC (CP) decomposition which produces
$r$-rank approximations of data tensors via outer products of vectors in each
dimension of the data. By constraining the vector in the temporal dimension to
known analytic forms which are selected from a large set of candidate
functions, more readily interpretable decompositions are achieved and analytic
time dependencies discovered. The SCTD method circumvents traditional {\em
flattening} techniques where an $N$-way array is reshaped into a matrix in
order to perform a singular value decomposition. A clear advantage of the SCTD
algorithm is its ability to extract transient and intermittent phenomena which
is often difficult for SVD-based methods. We motivate the SCTD method using
several intuitively appealing results before applying it on a number of
high-dimensional, real-world data sets in order to illustrate the efficiency of
the algorithm in extracting interpretable spatio-temporal modes. With the rise
of data-driven discovery methods, the decomposition proposed provides a viable
technique for analyzing multitudes of data in a more comprehensible fashion.","['Bethany Lusch', 'Eric C. Chi', 'J. Nathan Kutz']","['stat.ML', 'cs.LG', 'stat.ME']",2016-08-16 17:00:48+00:00
http://arxiv.org/abs/1608.04667v2,Medical image denoising using convolutional denoising autoencoders,"Image denoising is an important pre-processing step in medical image
analysis. Different algorithms have been proposed in past three decades with
varying denoising performances. More recently, having outperformed all
conventional methods, deep learning based models have shown a great promise.
These methods are however limited for requirement of large training sample size
and high computational costs. In this paper we show that using small sample
size, denoising autoencoders constructed using convolutional layers can be used
for efficient denoising of medical images. Heterogeneous images can be combined
to boost sample size for increased denoising performance. Simplest of networks
can reconstruct images with corruption levels so high that noise and signal are
not differentiable to human eye.",['Lovedeep Gondara'],"['cs.CV', 'stat.ML']",2016-08-16 16:39:20+00:00
http://arxiv.org/abs/1608.04664v2,Variational Gaussian Process Auto-Encoder for Ordinal Prediction of Facial Action Units,"We address the task of simultaneous feature fusion and modeling of discrete
ordinal outputs. We propose a novel Gaussian process(GP) auto-encoder modeling
approach. In particular, we introduce GP encoders to project multiple observed
features onto a latent space, while GP decoders are responsible for
reconstructing the original features. Inference is performed in a novel
variational framework, where the recovered latent representations are further
constrained by the ordinal output labels. In this way, we seamlessly integrate
the ordinal structure in the learned manifold, while attaining robust fusion of
the input features. We demonstrate the representation abilities of our model on
benchmark datasets from machine learning and affect analysis. We further
evaluate the model on the tasks of feature fusion and joint ordinal prediction
of facial action units. Our experiments demonstrate the benefits of the
proposed approach compared to the state of the art.","['Stefanos Eleftheriadis', 'Ognjen Rudovic', 'Marc P. Deisenroth', 'Maja Pantic']","['stat.ML', 'cs.CV']",2016-08-16 16:31:39+00:00
http://arxiv.org/abs/1608.04647v2,Enabling Factor Analysis on Thousand-Subject Neuroimaging Datasets,"The scale of functional magnetic resonance image data is rapidly increasing
as large multi-subject datasets are becoming widely available and
high-resolution scanners are adopted. The inherent low-dimensionality of the
information in this data has led neuroscientists to consider factor analysis
methods to extract and analyze the underlying brain activity. In this work, we
consider two recent multi-subject factor analysis methods: the Shared Response
Model and Hierarchical Topographic Factor Analysis. We perform analytical,
algorithmic, and code optimization to enable multi-node parallel
implementations to scale. Single-node improvements result in 99x and 1812x
speedups on these two methods, and enables the processing of larger datasets.
Our distributed implementations show strong scaling of 3.3x and 5.5x
respectively with 20 nodes on real datasets. We also demonstrate weak scaling
on a synthetic dataset with 1024 subjects, on up to 1024 nodes and 32,768
cores.","['Michael J. Anderson', 'Mihai CapotƒÉ', 'Javier S. Turek', 'Xia Zhu', 'Theodore L. Willke', 'Yida Wang', 'Po-Hsuan Chen', 'Jeremy R. Manning', 'Peter J. Ramadge', 'Kenneth A. Norman']","['stat.ML', 'cs.DC', 'cs.LG', '68W15', 'I.2']",2016-08-16 16:05:14+00:00
http://arxiv.org/abs/1608.04636v4,Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-≈Åojasiewicz Condition,"In 1963, Polyak proposed a simple condition that is sufficient to show a
global linear convergence rate for gradient descent. This condition is a
special case of the \L{}ojasiewicz inequality proposed in the same year, and it
does not require strong convexity (or even convexity). In this work, we show
that this much-older Polyak-\L{}ojasiewicz (PL) inequality is actually weaker
than the main conditions that have been explored to show linear convergence
rates without strong convexity over the last 25 years. We also use the PL
inequality to give new analyses of randomized and greedy coordinate descent
methods, sign-based gradient descent methods, and stochastic gradient methods
in the classic setting (with decreasing or constant step-sizes) as well as the
variance-reduced setting. We further propose a generalization that applies to
proximal-gradient methods for non-smooth optimization, leading to simple proofs
of linear convergence of these methods. Along the way, we give simple
convergence results for a wide variety of problems in machine learning: least
squares, logistic regression, boosting, resilient backpropagation,
L1-regularization, support vector machines, stochastic dual coordinate ascent,
and stochastic variance-reduced gradient methods.","['Hamed Karimi', 'Julie Nutini', 'Mark Schmidt']","['cs.LG', 'math.OC', 'stat.CO', 'stat.ML', '65K10', 'G.1.6; I.2.6']",2016-08-16 15:28:24+00:00
http://arxiv.org/abs/1608.04615v1,Scalable Modeling of Multivariate Longitudinal Data for Prediction of Chronic Kidney Disease Progression,"Prediction of the future trajectory of a disease is an important challenge
for personalized medicine and population health management. However, many
complex chronic diseases exhibit large degrees of heterogeneity, and
furthermore there is not always a single readily available biomarker to
quantify disease severity. Even when such a clinical variable exists, there are
often additional related biomarkers routinely measured for patients that may
better inform the predictions of their future disease state. To this end, we
propose a novel probabilistic generative model for multivariate longitudinal
data that captures dependencies between multivariate trajectories. We use a
Gaussian process based regression model for each individual trajectory, and
build off ideas from latent class models to induce dependence between their
mean functions. We fit our method using a scalable variational inference
algorithm to a large dataset of longitudinal electronic patient health records,
and find that it improves dynamic predictions compared to a recent state of the
art method. Our local accountable care organization then uses the model
predictions during chart reviews of high risk patients with chronic kidney
disease.","['Joseph Futoma', 'Mark Sendak', 'C. Blake Cameron', 'Katherine Heller']","['stat.ML', 'stat.AP', 'stat.ME']",2016-08-16 14:30:07+00:00
http://arxiv.org/abs/1608.04585v1,Conformalized density- and distance-based anomaly detection in time-series data,"Anomalies (unusual patterns) in time-series data give essential, and often
actionable information in critical situations. Examples can be found in such
fields as healthcare, intrusion detection, finance, security and flight safety.
In this paper we propose new conformalized density- and distance-based anomaly
detection algorithms for a one-dimensional time-series data. The algorithms use
a combination of a feature extraction method, an approach to assess a score
whether a new observation differs significantly from a previously observed
data, and a probabilistic interpretation of this score based on the conformal
paradigm.","['Evgeny Burnaev', 'Vladislav Ishimtsev']","['stat.AP', 'cs.LG', 'stat.ML']",2016-08-16 13:32:05+00:00
http://arxiv.org/abs/1608.04581v1,A novel transfer learning method based on common space mapping and weighted domain matching,"In this paper, we propose a novel learning framework for the problem of
domain transfer learning. We map the data of two domains to one single common
space, and learn a classifier in this common space. Then we adapt the common
classifier to the two domains by adding two adaptive functions to it
respectively. In the common space, the target domain data points are weighted
and matched to the target domain in term of distributions. The weighting terms
of source domain data points and the target domain classification responses are
also regularized by the local reconstruction coefficients. The novel transfer
learning framework is evaluated over some benchmark cross-domain data sets, and
it outperforms the existing state-of-the-art transfer learning methods.","['Ru-Ze Liang', 'Wei Xie', 'Weizhi Li', 'Hongqi Wang', 'Jim Jing-Yan Wang', 'Lisa Taylor']","['cs.LG', 'stat.ML']",2016-08-16 13:17:51+00:00
http://arxiv.org/abs/1608.04550v1,Fast Calculation of the Knowledge Gradient for Optimization of Deterministic Engineering Simulations,"A novel efficient method for computing the Knowledge-Gradient policy for
Continuous Parameters (KGCP) for deterministic optimization is derived. The
differences with Expected Improvement (EI), a popular choice for Bayesian
optimization of deterministic engineering simulations, are explored. Both
policies and the Upper Confidence Bound (UCB) policy are compared on a number
of benchmark functions including a problem from structural dynamics. It is
empirically shown that KGCP has similar performance as the EI policy for many
problems, but has better convergence properties for complex (multi-modal)
optimization problems as it emphasizes more on exploration when the model is
confident about the shape of optimal regions. In addition, the relationship
between Maximum Likelihood Estimation (MLE) and slice sampling for estimation
of the hyperparameters of the underlying models, and the complexity of the
problem at hand, is studied.","['Joachim van der Herten', 'Ivo Couckuyt', 'Dirk Deschrijver', 'Tom Dhaene']","['cs.CE', 'cs.LG', 'stat.ML']",2016-08-16 11:26:25+00:00
http://arxiv.org/abs/1608.04738v2,An Efficient Character-Level Neural Machine Translation,"Neural machine translation aims at building a single large neural network
that can be trained to maximize translation performance. The encoder-decoder
architecture with an attention mechanism achieves a translation performance
comparable to the existing state-of-the-art phrase-based systems on the task of
English-to-French translation. However, the use of large vocabulary becomes the
bottleneck in both training and improving the performance. In this paper, we
propose an efficient architecture to train a deep character-level neural
machine translation by introducing a decimator and an interpolator. The
decimator is used to sample the source sequence before encoding while the
interpolator is used to resample after decoding. Such a deep model has two
major advantages. It avoids the large vocabulary issue radically; at the same
time, it is much faster and more memory-efficient in training than conventional
character-based models. More interestingly, our model is able to translate the
misspelled word like human beings.","['Shenjian Zhao', 'Zhihua Zhang']","['cs.CL', 'stat.ML']",2016-08-16 07:44:02+00:00
http://arxiv.org/abs/1608.04481v1,Lecture Notes on Randomized Linear Algebra,"These are lecture notes that are based on the lectures from a class I taught
on the topic of Randomized Linear Algebra (RLA) at UC Berkeley during the Fall
2013 semester.",['Michael W. Mahoney'],"['cs.DS', 'stat.ML']",2016-08-16 04:55:26+00:00
http://arxiv.org/abs/1608.04478v1,A Geometrical Approach to Topic Model Estimation,"In the probabilistic topic models, the quantity of interest---a low-rank
matrix consisting of topic vectors---is hidden in the text corpus matrix,
masked by noise, and the Singular Value Decomposition (SVD) is a potentially
useful tool for learning such a low-rank matrix. However, the connection
between this low-rank matrix and the singular vectors of the text corpus matrix
are usually complicated and hard to spell out, so how to use SVD for learning
topic models faces challenges. In this paper, we overcome the challenge by
revealing a surprising insight: there is a low-dimensional simplex structure
which can be viewed as a bridge between the low-rank matrix of interest and the
SVD of the text corpus matrix, and allows us to conveniently reconstruct the
former using the latter. Such an insight motivates a new SVD approach to
learning topic models, which we analyze with delicate random matrix theory and
derive the rate of convergence. We support our methods and theory numerically,
using both simulated data and real data.",['Zheng Tracy Ke'],"['stat.ME', 'cs.LG', 'stat.ML']",2016-08-16 04:31:52+00:00
http://arxiv.org/abs/1608.04471v3,Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm,"We propose a general purpose variational inference algorithm that forms a
natural counterpart of gradient descent for optimization. Our method
iteratively transports a set of particles to match the target distribution, by
applying a form of functional gradient descent that minimizes the KL
divergence. Empirical studies are performed on various real world models and
datasets, on which our method is competitive with existing state-of-the-art
methods. The derivation of our method is based on a new theoretical result that
connects the derivative of KL divergence under smooth transforms with Stein's
identity and a recently proposed kernelized Stein discrepancy, which is of
independent interest.","['Qiang Liu', 'Dilin Wang']","['stat.ML', 'cs.LG']",2016-08-16 03:24:20+00:00
http://arxiv.org/abs/1608.04414v3,Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back,"In stochastic convex optimization the goal is to minimize a convex function
$F(x) \doteq {\mathbf E}_{{\mathbf f}\sim D}[{\mathbf f}(x)]$ over a convex set
$\cal K \subset {\mathbb R}^d$ where $D$ is some unknown distribution and each
$f(\cdot)$ in the support of $D$ is convex over $\cal K$. The optimization is
commonly based on i.i.d.~samples $f^1,f^2,\ldots,f^n$ from $D$. A standard
approach to such problems is empirical risk minimization (ERM) that optimizes
$F_S(x) \doteq \frac{1}{n}\sum_{i\leq n} f^i(x)$. Here we consider the question
of how many samples are necessary for ERM to succeed and the closely related
question of uniform convergence of $F_S$ to $F$ over $\cal K$. We demonstrate
that in the standard $\ell_p/\ell_q$ setting of Lipschitz-bounded functions
over a $\cal K$ of bounded radius, ERM requires sample size that scales
linearly with the dimension $d$. This nearly matches standard upper bounds and
improves on $\Omega(\log d)$ dependence proved for $\ell_2/\ell_2$ setting by
Shalev-Shwartz et al. (2009). In stark contrast, these problems can be solved
using dimension-independent number of samples for $\ell_2/\ell_2$ setting and
$\log d$ dependence for $\ell_1/\ell_\infty$ setting using other approaches. We
further show that our lower bound applies even if the functions in the support
of $D$ are smooth and efficiently computable and even if an $\ell_1$
regularization term is added. Finally, we demonstrate that for a more general
class of bounded-range (but not Lipschitz-bounded) stochastic convex programs
an infinite gap appears already in dimension 2.",['Vitaly Feldman'],"['cs.LG', 'stat.ML']",2016-08-15 21:19:51+00:00
http://arxiv.org/abs/1608.04374v2,A Geometric Framework for Convolutional Neural Networks,"In this paper, a geometric framework for neural networks is proposed. This
framework uses the inner product space structure underlying the parameter set
to perform gradient descent not in a component-based form, but in a
coordinate-free manner. Convolutional neural networks are described in this
framework in a compact form, with the gradients of standard --- and
higher-order --- loss functions calculated for each layer of the network. This
approach can be applied to other network structures and provides a basis on
which to create new networks.","['Anthony L. Caterini', 'Dong Eui Chang']","['stat.ML', 'cs.AI', 'cs.NE', 'I.5.1; I.2.6']",2016-08-15 19:38:35+00:00
http://arxiv.org/abs/1608.04331v1,Consistency constraints for overlapping data clustering,"We examine overlapping clustering schemes with functorial constraints, in the
spirit of Carlsson--Memoli. This avoids issues arising from the chaining
required by partition-based methods. Our principal result shows that any
clustering functor is naturally constrained to refine single-linkage clusters
and be refined by maximal-linkage clusters. We work in the context of metric
spaces with non-expansive maps, which is appropriate for modeling data
processing which does not increase information content.","['Jared Culbertson', 'Dan P. Guralnik', 'Jakob Hansen', 'Peter F. Stiller']","['cs.LG', 'stat.ML', '51K05, 68P01', 'H.3.3']",2016-08-15 17:12:09+00:00
http://arxiv.org/abs/1608.04290v1,Robust Volume Minimization-Based Matrix Factorization for Remote Sensing and Document Clustering,"This paper considers \emph{volume minimization} (VolMin)-based structured
matrix factorization (SMF). VolMin is a factorization criterion that decomposes
a given data matrix into a basis matrix times a structured coefficient matrix
via finding the minimum-volume simplex that encloses all the columns of the
data matrix. Recent work showed that VolMin guarantees the identifiability of
the factor matrices under mild conditions that are realistic in a wide variety
of applications. This paper focuses on both theoretical and practical aspects
of VolMin. On the theory side, exact equivalence of two independently developed
sufficient conditions for VolMin identifiability is proven here, thereby
providing a more comprehensive understanding of this aspect of VolMin. On the
algorithm side, computational complexity and sensitivity to outliers are two
key challenges associated with real-world applications of VolMin. These are
addressed here via a new VolMin algorithm that handles volume regularization in
a computationally simple way, and automatically detects and {iteratively
downweights} outliers, simultaneously. Simulations and real-data experiments
using a remotely sensed hyperspectral image and the Reuters document corpus are
employed to showcase the effectiveness of the proposed algorithm.","['Xiao Fu', 'Kejun Huang', 'Bo Yang', 'Wing-Kin Ma', 'Nicholas D. Sidiropoulos']",['stat.ML'],2016-08-15 14:51:10+00:00
http://arxiv.org/abs/1608.04245v2,The Bayesian Low-Rank Determinantal Point Process Mixture Model,"Determinantal point processes (DPPs) are an elegant model for encoding
probabilities over subsets, such as shopping baskets, of a ground set, such as
an item catalog. They are useful for a number of machine learning tasks,
including product recommendation. DPPs are parametrized by a positive
semi-definite kernel matrix. Recent work has shown that using a low-rank
factorization of this kernel provides remarkable scalability improvements that
open the door to training on large-scale datasets and computing online
recommendations, both of which are infeasible with standard DPP models that use
a full-rank kernel. In this paper we present a low-rank DPP mixture model that
allows us to represent the latent structure present in observed subsets as a
mixture of a number of component low-rank DPPs, where each component DPP is
responsible for representing a portion of the observed data. The mixture model
allows us to effectively address the capacity constraints of the low-rank DPP
model. We present an efficient and scalable Markov Chain Monte Carlo (MCMC)
learning algorithm for our model that uses Gibbs sampling and stochastic
gradient Hamiltonian Monte Carlo (SGHMC). Using an evaluation on several
real-world product recommendation datasets, we show that our low-rank DPP
mixture model provides substantially better predictive performance than is
possible with a single low-rank or full-rank DPP, and significantly better
performance than several other competing recommendation methods in many cases.","['Mike Gartrell', 'Ulrich Paquet', 'Noam Koenigstein']","['stat.ML', 'cs.LG']",2016-08-15 11:42:51+00:00
http://arxiv.org/abs/1608.04236v2,Generative and Discriminative Voxel Modeling with Convolutional Neural Networks,"When working with three-dimensional data, choice of representation is key. We
explore voxel-based models, and present evidence for the viability of
voxellated representations in applications including shape modeling and object
classification. Our key contributions are methods for training voxel-based
variational autoencoders, a user interface for exploring the latent space
learned by the autoencoder, and a deep convolutional neural network
architecture for object classification. We address challenges unique to
voxel-based representations, and empirically evaluate our models on the
ModelNet benchmark, where we demonstrate a 51.5% relative improvement in the
state of the art for object classification.","['Andrew Brock', 'Theodore Lim', 'J. M. Ritchie', 'Nick Weston']","['cs.CV', 'cs.HC', 'cs.LG', 'stat.ML']",2016-08-15 11:14:35+00:00
http://arxiv.org/abs/1608.04123v1,The Spectral Condition Number Plot for Regularization Parameter Determination,"Many modern statistical applications ask for the estimation of a covariance
(or precision) matrix in settings where the number of variables is larger than
the number of observations. There exists a broad class of ridge-type estimators
that employs regularization to cope with the subsequent singularity of the
sample covariance matrix. These estimators depend on a penalty parameter and
choosing its value can be hard, in terms of being computationally unfeasible or
tenable only for a restricted set of ridge-type estimators. Here we introduce a
simple graphical tool, the spectral condition number plot, for informed
heuristic penalty parameter selection. The proposed tool is computationally
friendly and can be employed for the full class of ridge-type covariance
(precision) estimators.","['Carel F. W. Peeters', 'Mark A. van de Wiel', 'Wessel N. van Wieringen']","['stat.CO', 'stat.ML']",2016-08-14 18:50:32+00:00
http://arxiv.org/abs/1608.04109v1,Depth and depth-based classification with R-package ddalpha,"Following the seminal idea of Tukey, data depth is a function that measures
how close an arbitrary point of the space is located to an implicitly defined
center of a data cloud. Having undergone theoretical and computational
developments, it is now employed in numerous applications with classification
being the most popular one. The R-package ddalpha is a software directed to
fuse experience of the applicant with recent achievements in the area of data
depth and depth-based classification.
  ddalpha provides an implementation for exact and approximate computation of
most reasonable and widely applied notions of data depth. These can be further
used in the depth-based multivariate and functional classifiers implemented in
the package, where the $DD\alpha$-procedure is in the main focus. The package
is expandable with user-defined custom depth methods and separators. The
implemented functions for depth visualization and the built-in benchmark
procedures may also serve to provide insights into the geometry of the data and
the quality of pattern recognition.","['Oleksii Pokotylo', 'Pavlo Mozharovskyi', 'Rainer Dyckerhoff']","['stat.CO', 'stat.ML']",2016-08-14 14:44:17+00:00
http://arxiv.org/abs/1608.04089v1,Viewpoint and Topic Modeling of Current Events,"There are multiple sides to every story, and while statistical topic models
have been highly successful at topically summarizing the stories in corpora of
text documents, they do not explicitly address the issue of learning the
different sides, the viewpoints, expressed in the documents. In this paper, we
show how these viewpoints can be learned completely unsupervised and
represented in a human interpretable form. We use a novel approach of applying
CorrLDA2 for this purpose, which learns topic-viewpoint relations that can be
used to form groups of topics, where each group represents a viewpoint. A
corpus of documents about the Israeli-Palestinian conflict is then used to
demonstrate how a Palestinian and an Israeli viewpoint can be learned. By
leveraging the magnitudes and signs of the feature weights of a linear SVM, we
introduce a principled method to evaluate associations between topics and
viewpoints. With this, we demonstrate, both quantitatively and qualitatively,
that the learned topic groups are contextually coherent, and form consistently
correct topic-viewpoint associations.","['Kerry Zhang', 'Jussi Karlgren', 'Cheng Zhang', 'Jens Lagergren']","['cs.CL', 'cs.IR', 'stat.ML']",2016-08-14 11:36:52+00:00
http://arxiv.org/abs/1608.04063v1,Bayesian Model Selection Methods for Mutual and Symmetric $k$-Nearest Neighbor Classification,"The $k$-nearest neighbor classification method ($k$-NNC) is one of the
simplest nonparametric classification methods. The mutual $k$-NN classification
method (M$k$NNC) is a variant of $k$-NNC based on mutual neighborship. We
propose another variant of $k$-NNC, the symmetric $k$-NN classification method
(S$k$NNC) based on both mutual neighborship and one-sided neighborship. The
performance of M$k$NNC and S$k$NNC depends on the parameter $k$ as the one of
$k$-NNC does. We propose the ways how M$k$NN and S$k$NN classification can be
performed based on Bayesian mutual and symmetric $k$-NN regression methods with
the selection schemes for the parameter $k$. Bayesian mutual and symmetric
$k$-NN regression methods are based on Gaussian process models, and it turns
out that they can do M$k$NN and S$k$NN classification with new encodings of
target values (class labels). The simulation results show that the proposed
methods are better than or comparable to $k$-NNC, M$k$NNC and S$k$NNC with the
parameter $k$ selected by the leave-one-out cross validation method not only
for an artificial data set but also for real world data sets.",['Hyun-Chul Kim'],"['cs.LG', 'stat.ML']",2016-08-14 06:01:21+00:00
http://arxiv.org/abs/1608.04048v1,Ultra High-Dimensional Nonlinear Feature Selection for Big Biological Data,"Machine learning methods are used to discover complex nonlinear relationships
in biological and medical data. However, sophisticated learning models are
computationally unfeasible for data with millions of features. Here we
introduce the first feature selection method for nonlinear learning problems
that can scale up to large, ultra-high dimensional biological data. More
specifically, we scale up the novel Hilbert-Schmidt Independence Criterion
Lasso (HSIC Lasso) to handle millions of features with tens of thousand
samples. The proposed method is guaranteed to find an optimal subset of
maximally predictive features with minimal redundancy, yielding higher
predictive power and improved interpretability. Its effectiveness is
demonstrated through applications to classify phenotypes based on module
expression in human prostate cancer patients and to detect enzymes among
protein structures. We achieve high accuracy with as few as 20 out of one
million features --- a dimensionality reduction of 99.998%. Our algorithm can
be implemented on commodity cloud computing platforms. The dramatic reduction
of features may lead to the ubiquitous deployment of sophisticated prediction
models in mobile health care applications.","['Makoto Yamada', 'Jiliang Tang', 'Jose Lugo-Martinez', 'Ermin Hodzic', 'Raunak Shrestha', 'Avishek Saha', 'Hua Ouyang', 'Dawei Yin', 'Hiroshi Mamitsuka', 'Cenk Sahinalp', 'Predrag Radivojac', 'Filippo Menczer', 'Yi Chang']",['stat.ML'],2016-08-14 01:56:22+00:00
http://arxiv.org/abs/1608.04037v1,An approach to dealing with missing values in heterogeneous data using k-nearest neighbors,"Techniques such as clusterization, neural networks and decision making
usually rely on algorithms that are not well suited to deal with missing
values. However, real world data frequently contains such cases. The simplest
solution is to either substitute them by a best guess value or completely
disregard the missing values. Unfortunately, both approaches can lead to biased
results. In this paper, we propose a technique for dealing with missing values
in heterogeneous data using imputation based on the k-nearest neighbors
algorithm. It can handle real (which we refer to as crisp henceforward),
interval and fuzzy data. The effectiveness of the algorithm is tested on
several datasets and the numerical results are promising.","['Davi E. N. Frossard', 'Igor O. Nunes', 'Renato A. Krohling']","['cs.LG', 'cs.IR', 'stat.ML']",2016-08-13 23:45:21+00:00
http://arxiv.org/abs/1608.03974v1,Recurrent Fully Convolutional Neural Networks for Multi-slice MRI Cardiac Segmentation,"In cardiac magnetic resonance imaging, fully-automatic segmentation of the
heart enables precise structural and functional measurements to be taken, e.g.
from short-axis MR images of the left-ventricle. In this work we propose a
recurrent fully-convolutional network (RFCN) that learns image representations
from the full stack of 2D slices and has the ability to leverage inter-slice
spatial dependences through internal memory units. RFCN combines anatomical
detection and segmentation into a single architecture that is trained
end-to-end thus significantly reducing computational time, simplifying the
segmentation pipeline, and potentially enabling real-time applications. We
report on an investigation of RFCN using two datasets, including the publicly
available MICCAI 2009 Challenge dataset. Comparisons have been carried out
between fully convolutional networks and deep restricted Boltzmann machines,
including a recurrent version that leverages inter-slice spatial correlation.
Our studies suggest that RFCN produces state-of-the-art results and can
substantially improve the delineation of contours near the apex of the heart.","['Rudra P K Poudel', 'Pablo Lamata', 'Giovanni Montana']","['stat.ML', 'cs.CV', 'cs.LG']",2016-08-13 11:19:22+00:00
http://arxiv.org/abs/1608.03928v2,Hybrid Jacobian and Gauss-Seidel proximal block coordinate update methods for linearly constrained convex programming,"Recent years have witnessed the rapid development of block coordinate update
(BCU) methods, which are particularly suitable for problems involving
large-sized data and/or variables. In optimization, BCU first appears as the
coordinate descent method that works well for smooth problems or those with
separable nonsmooth terms and/or separable constraints. As nonseparable
constraints exist, BCU can be applied under primal-dual settings.
  In the literature, it has been shown that for weakly convex problems with
nonseparable linear constraint, BCU with fully Gauss-Seidel updating rule may
fail to converge and that with fully Jacobian rule can converge sublinearly.
However, empirically the method with Jacobian update is usually slower than
that with Gauss-Seidel rule. To maintain their advantages, we propose a hybrid
Jacobian and Gauss-Seidel BCU method for solving linearly constrained
multi-block structured convex programming, where the objective may have a
nonseparable quadratic term and separable nonsmooth terms. At each primal block
variable update, the method approximates the augmented Lagrangian function at
an affine combination of the previous two iterates, and the affinely mixing
matrix with desired nice properties can be chosen through solving a
semidefinite programming. We show that the hybrid method enjoys the theoretical
convergence guarantee as Jacobian BCU. In addition, we numerically demonstrate
that the method can perform as well as Gauss-Seidel method and better than a
recently proposed randomized primal-dual BCU method.",['Yangyang Xu'],"['math.OC', 'math.NA', 'stat.ML', '9008, 90C25, 90C06, 68W40']",2016-08-13 00:30:32+00:00
http://arxiv.org/abs/1608.03817v3,Scaling Factorial Hidden Markov Models: Stochastic Variational Inference without Messages,"Factorial Hidden Markov Models (FHMMs) are powerful models for sequential
data but they do not scale well with long sequences. We propose a scalable
inference and learning algorithm for FHMMs that draws on ideas from the
stochastic variational inference, neural network and copula literatures. Unlike
existing approaches, the proposed algorithm requires no message passing
procedure among latent variables and can be distributed to a network of
computers to speed up learning. Our experiments corroborate that the proposed
algorithm does not introduce further approximation bias compared to the proven
structured mean-field algorithm, and achieves better performance with long
sequences and large FHMMs.","['Yin Cheng Ng', 'Pawel Chilinski', 'Ricardo Silva']",['stat.ML'],2016-08-12 15:02:10+00:00
http://arxiv.org/abs/1608.03811v1,Content-based image retrieval tutorial,"This paper functions as a tutorial for individuals interested to enter the
field of information retrieval but wouldn't know where to begin from. It
describes two fundamental yet efficient image retrieval techniques, the first
being k - nearest neighbors (knn) and the second support vector machines(svm).
The goal is to provide the reader with both the theoretical and practical
aspects in order to acquire a better understanding. Along with this tutorial we
have also developed the equivalent software1 using the MATLAB environment in
order to illustrate the techniques, so that the reader can have a hands-on
experience.",['Joani Mitro'],"['stat.ML', 'cs.IR', 'cs.LG']",2016-08-12 14:40:46+00:00
http://arxiv.org/abs/1608.03665v4,Learning Structured Sparsity in Deep Neural Networks,"High demand for computation resources severely hinders deployment of
large-scale Deep Neural Networks (DNN) in resource constrained devices. In this
work, we propose a Structured Sparsity Learning (SSL) method to regularize the
structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.
SSL can: (1) learn a compact structure from a bigger DNN to reduce computation
cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently
accelerate the DNNs evaluation. Experimental results show that SSL achieves on
average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet
against CPU and GPU, respectively, with off-the-shelf libraries. These speedups
are about twice speedups of non-structured sparsity; (3) regularize the DNN
structure to improve classification accuracy. The results show that for
CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual
Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,
which is still slightly higher than that of original ResNet with 32 layers. For
AlexNet, structure regularization by SSL also reduces the error by around ~1%.
Open source code is in https://github.com/wenwei202/caffe/tree/scnn","['Wei Wen', 'Chunpeng Wu', 'Yandan Wang', 'Yiran Chen', 'Hai Li']","['cs.NE', 'cs.LG', 'stat.ML', 'I.2.6; I.5.1']",2016-08-12 03:20:43+00:00
http://arxiv.org/abs/1608.03643v2,Chi-squared Amplification: Identifying Hidden Hubs,"We consider the following general hidden hubs model: an $n \times n$ random
matrix $A$ with a subset $S$ of $k$ special rows (hubs): entries in rows
outside $S$ are generated from the probability distribution $p_0 \sim
N(0,\sigma_0^2)$; for each row in $S$, some $k$ of its entries are generated
from $p_1 \sim N(0,\sigma_1^2)$, $\sigma_1>\sigma_0$, and the rest of the
entries from $p_0$. The problem is to identify the high-degree hubs
efficiently. This model includes and significantly generalizes the planted
Gaussian Submatrix Model, where the special entries are all in a $k \times k$
submatrix. There are two well-known barriers: if $k\geq c\sqrt{n\ln n}$, just
the row sums are sufficient to find $S$ in the general model. For the submatrix
problem, this can be improved by a $\sqrt{\ln n}$ factor to $k \ge c\sqrt{n}$
by spectral methods or combinatorial methods. In the variant with $p_0=\pm 1$
(with probability $1/2$ each) and $p_1\equiv 1$, neither barrier has been
broken.
  We give a polynomial-time algorithm to identify all the hidden hubs with high
probability for $k \ge n^{0.5-\delta}$ for some $\delta >0$, when
$\sigma_1^2>2\sigma_0^2$. The algorithm extends to the setting where planted
entries might have different variances each at least as large as $\sigma_1^2$.
We also show a nearly matching lower bound: for $\sigma_1^2 \le 2\sigma_0^2$,
there is no polynomial-time Statistical Query algorithm for distinguishing
between a matrix whose entries are all from $N(0,\sigma_0^2)$ and a matrix with
$k=n^{0.5-\delta}$ hidden hubs for any $\delta >0$. The lower bound as well as
the algorithm are related to whether the chi-squared distance of the two
distributions diverges. At the critical value $\sigma_1^2=2\sigma_0^2$, we show
that the general hidden hubs problem can be solved for $k\geq c\sqrt n(\ln
n)^{1/4}$, improving on the naive row sum-based method.","['Ravi Kannan', 'Santosh Vempala']","['cs.LG', 'cs.DS', 'stat.ML']",2016-08-12 00:36:42+00:00
http://arxiv.org/abs/1608.03639v1,Faster Training of Very Deep Networks Via p-Norm Gates,"A major contributing factor to the recent advances in deep neural networks is
structural units that let sensory information and gradients to propagate
easily. Gating is one such structure that acts as a flow control. Gates are
employed in many recent state-of-the-art recurrent models such as LSTM and GRU,
and feedforward models such as Residual Nets and Highway Networks. This enables
learning in very deep networks with hundred layers and helps achieve
record-breaking results in vision (e.g., ImageNet with Residual Nets) and NLP
(e.g., machine translation with GRU). However, there is limited work in
analysing the role of gating in the learning process. In this paper, we propose
a flexible $p$-norm gating scheme, which allows user-controllable flow and as a
consequence, improve the learning speed. This scheme subsumes other existing
gating schemes, including those in GRU, Highway Networks and Residual Nets as
special cases. Experiments on large sequence and vector datasets demonstrate
that the proposed gating scheme helps improve the learning speed significantly
without extra overhead.","['Trang Pham', 'Truyen Tran', 'Dinh Phung', 'Svetha Venkatesh']","['stat.ML', 'cs.LG', 'cs.NE']",2016-08-11 23:48:44+00:00
http://arxiv.org/abs/1608.03585v1,Warm Starting Bayesian Optimization,"We develop a framework for warm-starting Bayesian optimization, that reduces
the solution time required to solve an optimization problem that is one in a
sequence of related problems. This is useful when optimizing the output of a
stochastic simulator that fails to provide derivative information, for which
Bayesian optimization methods are well-suited. Solving sequences of related
optimization problems arises when making several business decisions using one
optimization model and input data collected over different time periods or
markets. While many gradient-based methods can be warm started by initiating
optimization at the solution to the previous problem, this warm start approach
does not apply to Bayesian optimization methods, which carry a full metamodel
of the objective function from iteration to iteration. Our approach builds a
joint statistical model of the entire collection of related objective
functions, and uses a value of information calculation to recommend points to
evaluate.","['Matthias Poloczek', 'Jialei Wang', 'Peter I. Frazier']","['stat.ML', 'cs.LG', 'stat.AP']",2016-08-11 19:56:27+00:00
http://arxiv.org/abs/1608.03533v15,Sequence Graph Transform (SGT): A Feature Embedding Function for Sequence Data Mining,"Sequence feature embedding is a challenging task due to the unstructuredness
of sequence, i.e., arbitrary strings of arbitrary length. Existing methods are
efficient in extracting short-term dependencies but typically suffer from
computation issues for the long-term. Sequence Graph Transform (SGT), a feature
embedding function, that can extract a varying amount of short- to long-term
dependencies without increasing the computation is proposed. SGT's properties
are analytically proved for interpretation under normal and uniform
distribution assumptions. SGT features yield significantly superior results in
sequence clustering and classification with higher accuracy and lower
computation as compared to the existing methods, including the state-of-the-art
sequence/string Kernels and LSTM.","['Chitta Ranjan', 'Samaneh Ebrahimi', 'Kamran Paynabar']","['stat.ML', 'cs.LG']",2016-08-11 16:59:19+00:00
http://arxiv.org/abs/1608.03530v1,Semi-Supervised Prediction of Gene Regulatory Networks Using Machine Learning Algorithms,"Use of computational methods to predict gene regulatory networks (GRNs) from
gene expression data is a challenging task. Many studies have been conducted
using unsupervised methods to fulfill the task; however, such methods usually
yield low prediction accuracies due to the lack of training data. In this
article, we propose semi-supervised methods for GRN prediction by utilizing two
machine learning algorithms, namely support vector machines (SVM) and random
forests (RF). The semi-supervised methods make use of unlabeled data for
training. We investigate inductive and transductive learning approaches, both
of which adopt an iterative procedure to obtain reliable negative training data
from the unlabeled data. We then apply our semi-supervised methods to gene
expression data of Escherichia coli and Saccharomyces cerevisiae, and evaluate
the performance of our methods using the expression data. Our analysis
indicated that the transductive learning approach outperformed the inductive
learning approach for both organisms. However, there was no conclusive
difference identified in the performance of SVM and RF. Experimental results
also showed that the proposed semi-supervised methods performed better than
existing supervised methods for both organisms.","['Nihir Patel', 'Jason T. L. Wang']","['cs.LG', 'q-bio.QM', 'stat.ML']",2016-08-11 16:52:03+00:00
http://arxiv.org/abs/1608.03487v2,A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates,"This paper focuses on convex constrained optimization problems, where the
solution is subject to a convex inequality constraint. In particular, we aim at
challenging problems for which both projection into the constrained domain and
a linear optimization under the inequality constraint are time-consuming, which
render both projected gradient methods and conditional gradient methods (a.k.a.
the Frank-Wolfe algorithm) expensive. In this paper, we develop projection
reduced optimization algorithms for both smooth and non-smooth optimization
with improved convergence rates under a certain regularity condition of the
constraint function. We first present a general theory of optimization with
only one projection. Its application to smooth optimization with only one
projection yields $O(1/\epsilon)$ iteration complexity, which improves over the
$O(1/\epsilon^2)$ iteration complexity established before for non-smooth
optimization and can be further reduced under strong convexity. Then we
introduce a local error bound condition and develop faster algorithms for
non-strongly convex optimization at the price of a logarithmic number of
projections. In particular, we achieve an iteration complexity of $\widetilde
O(1/\epsilon^{2(1-\theta)})$ for non-smooth optimization and $\widetilde
O(1/\epsilon^{1-\theta})$ for smooth optimization, where $\theta\in(0,1]$
appearing the local error bound condition characterizes the functional local
growth rate around the optimal solutions. Novel applications in solving the
constrained $\ell_1$ minimization problem and a positive semi-definite
constrained distance metric learning problem demonstrate that the proposed
algorithms achieve significant speed-up compared with previous algorithms.","['Tianbao Yang', 'Qihang Lin', 'Lijun Zhang']","['math.OC', 'stat.ML']",2016-08-11 14:48:18+00:00
http://arxiv.org/abs/1608.03339v2,Distributed learning with regularized least squares,"We study distributed learning with the least squares regularization scheme in
a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach,
the algorithm partitions a data set into disjoint data subsets, applies the
least squares regularization scheme to each data subset to produce an output
function, and then takes an average of the individual output functions as a
final global estimator or predictor. We show with error bounds in expectation
in both the $L^2$-metric and RKHS-metric that the global output function of
this distributed learning is a good approximation to the algorithm processing
the whole data in one single machine. Our error bounds are sharp and stated in
a general setting without any eigenfunction assumption. The analysis is
achieved by a novel second order decomposition of operator differences in our
integral operator approach. Even for the classical least squares regularization
scheme in the RKHS associated with a general kernel, we give the best learning
rate in the literature.","['Shao-Bo Lin', 'Xin Guo', 'Ding-Xuan Zhou']","['cs.LG', 'stat.ML', '68T05, 94A20, 41A35', 'F.2.3']",2016-08-11 01:20:23+00:00
http://arxiv.org/abs/1608.03333v1,Temporal Learning and Sequence Modeling for a Job Recommender System,"We present our solution to the job recommendation task for RecSys Challenge
2016. The main contribution of our work is to combine temporal learning with
sequence modeling to capture complex user-item activity patterns to improve job
recommendations. First, we propose a time-based ranking model applied to
historical observations and a hybrid matrix factorization over time re-weighted
interactions. Second, we exploit sequence properties in user-items activities
and develop a RNN-based recommendation model. Our solution achieved 5$^{th}$
place in the challenge among more than 100 participants. Notably, the strong
performance of our RNN approach shows a promising new direction in employing
sequence modeling for recommendation systems.","['Kuan Liu', 'Xing Shi', 'Anoop Kumar', 'Linhong Zhu', 'Prem Natarajan']","['cs.LG', 'stat.ML']",2016-08-11 00:48:00+00:00
http://arxiv.org/abs/1608.03100v1,Estimation from Indirect Supervision with Linear Moments,"In structured prediction problems where we have indirect supervision of the
output, maximum marginal likelihood faces two computational obstacles:
non-convexity of the objective and intractability of even a single gradient
computation. In this paper, we bypass both obstacles for a class of what we
call linear indirectly-supervised problems. Our approach is simple: we solve a
linear system to estimate sufficient statistics of the model, which we then use
to estimate parameters via convex optimization. We analyze the statistical
properties of our approach and show empirically that it is effective in two
settings: learning with local privacy constraints and learning from low-cost
count-based annotations.","['Aditi Raghunathan', 'Roy Frostig', 'John Duchi', 'Percy Liang']","['stat.ML', 'cs.LG']",2016-08-10 09:19:07+00:00
http://arxiv.org/abs/1608.03045v3,Combinatorial Inference for Graphical Models,"We propose a new family of combinatorial inference problems for graphical
models. Unlike classical statistical inference where the main interest is point
estimation or parameter testing, combinatorial inference aims at testing the
global structure of the underlying graph. Examples include testing the graph
connectivity, the presence of a cycle of certain size, or the maximum degree of
the graph. To begin with, we develop a unified theory for the fundamental
limits of a large family of combinatorial inference problems. We propose new
concepts including structural packing and buffer entropies to characterize how
the complexity of combinatorial graph structures impacts the corresponding
minimax lower bounds. On the other hand, we propose a family of novel and
practical structural testing algorithms to match the lower bounds. We provide
thorough numerical results on both synthetic graphical models and brain
networks to illustrate the usefulness of these proposed methods.","['Matey Neykov', 'Junwei Lu', 'Han Liu']","['math.ST', 'stat.ML', 'stat.TH']",2016-08-10 04:47:29+00:00
http://arxiv.org/abs/1608.03023v3,Stochastic Rank-1 Bandits,"We propose stochastic rank-$1$ bandits, a class of online learning problems
where at each step a learning agent chooses a pair of row and column arms, and
receives the product of their values as a reward. The main challenge of the
problem is that the individual values of the row and column are unobserved. We
assume that these values are stochastic and drawn independently. We propose a
computationally-efficient algorithm for solving our problem, which we call
Rank1Elim. We derive a $O((K + L) (1 / \Delta) \log n)$ upper bound on its
$n$-step regret, where $K$ is the number of rows, $L$ is the number of columns,
and $\Delta$ is the minimum of the row and column gaps; under the assumption
that the mean row and column rewards are bounded away from zero. To the best of
our knowledge, we present the first bandit algorithm that finds the maximum
entry of a rank-$1$ matrix whose regret is linear in $K + L$, $1 / \Delta$, and
$\log n$. We also derive a nearly matching lower bound. Finally, we evaluate
Rank1Elim empirically on multiple problems. We observe that it leverages the
structure of our problems and can learn near-optimal solutions even if our
modeling assumptions are mildly violated.","['Sumeet Katariya', 'Branislav Kveton', 'Csaba Szepesvari', 'Claire Vernade', 'Zheng Wen']","['cs.LG', 'stat.ML']",2016-08-10 01:51:36+00:00
http://arxiv.org/abs/1608.03022v1,Dynamic Principal Component Analysis: Identifying the Relationship between Multiple Air Pollutants,"The dynamic nature of air quality chemistry and transport makes it difficult
to identify the mixture of air pollutants for a region. In this study of air
quality in the Houston metropolitan area we apply dynamic principal component
analysis (DPCA) to a normalized multivariate time series of daily concentration
measurements of five pollutants (O3, CO, NO2, SO2, PM2.5) from January 1, 2009
through December 31, 2011 for each of the 24 hours in a day. The resulting
dynamic components are examined by hour across days for the 3 year period.
Diurnal and seasonal patterns are revealed underlining times when DPCA performs
best and two principal components (PCs) explain most variability in the
multivariate series. DPCA is shown to be superior to static principal component
analysis (PCA) in discovery of linear relations among transformed pollutant
measurements. DPCA captures the time-dependent correlation structure of the
underlying pollutants recorded at up to 34 monitoring sites in the region. In
winter mornings the first principal component (PC1) (mainly CO and NO2)
explains up to 70% of variability. Augmenting with the second principal
component (PC2) (mainly driven by SO2) the explained variability rises to 90%.
In the afternoon, O3 gains prominence in the second principal component. The
seasonal profile of PCs' contribution to variance loses its distinction in the
afternoon, yet cumulatively PC1 and PC2 still explain up to 65% of variability
in ambient air data. DPCA provides a strategy for identifying the changing air
quality profile for the region studied.","['Oleg Melnikov', 'Loren H. Raun', 'Katherine B. Ensor']","['stat.AP', 'stat.ML']",2016-08-10 01:51:17+00:00
http://arxiv.org/abs/1608.02902v1,Linear Regression with an Unknown Permutation: Statistical and Computational Limits,"Consider a noisy linear observation model with an unknown permutation, based
on observing $y = \Pi^* A x^* + w$, where $x^* \in \mathbb{R}^d$ is an unknown
vector, $\Pi^*$ is an unknown $n \times n$ permutation matrix, and $w \in
\mathbb{R}^n$ is additive Gaussian noise. We analyze the problem of permutation
recovery in a random design setting in which the entries of the matrix $A$ are
drawn i.i.d. from a standard Gaussian distribution, and establish sharp
conditions on the SNR, sample size $n$, and dimension $d$ under which $\Pi^*$
is exactly and approximately recoverable. On the computational front, we show
that the maximum likelihood estimate of $\Pi^*$ is NP-hard to compute, while
also providing a polynomial time algorithm when $d =1$.","['Ashwin Pananjady', 'Martin J. Wainwright', 'Thomas A. Courtade']","['math.ST', 'cs.IT', 'math.IT', 'stat.ML', 'stat.TH']",2016-08-09 18:24:50+00:00
http://arxiv.org/abs/1608.02861v1,Classification with the pot-pot plot,"We propose a procedure for supervised classification that is based on
potential functions. The potential of a class is defined as a kernel density
estimate multiplied by the class's prior probability. The method transforms the
data to a potential-potential (pot-pot) plot, where each data point is mapped
to a vector of potentials. Separation of the classes, as well as classification
of new data points, is performed on this plot. For this, either the
$\alpha$-procedure ($\alpha$-P) or $k$-nearest neighbors ($k$-NN) are employed.
For data that are generated from continuous distributions, these classifiers
prove to be strongly Bayes-consistent. The potentials depend on the kernel and
its bandwidth used in the density estimate. We investigate several variants of
bandwidth selection, including joint and separate pre-scaling and a bandwidth
regression approach. The new method is applied to benchmark data from the
literature, including simulated data sets as well as 50 sets of real data. It
compares favorably to known classification methods such as LDA, QDA, max kernel
density estimates, $k$-NN, and $DD$-plot classification using depth functions.","['Oleksii Pokotylo', 'Karl Mosler']","['stat.ML', 'cs.LG', '62H30, 62G07']",2016-08-09 16:46:53+00:00
http://arxiv.org/abs/1608.02732v1,On Lower Bounds for Regret in Reinforcement Learning,"This is a brief technical note to clarify the state of lower bounds on regret
for reinforcement learning. In particular, this paper:
  - Reproduces a lower bound on regret for reinforcement learning, similar to
the result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010).
  - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett
and Tewari 2009) does not hold using the standard techniques without further
work. We suggest that this result should instead be considered a conjecture as
it has no rigorous proof.
  - Suggests that the conjectured lower bound given by (Bartlett and Tewari
2009) is incorrect and, in fact, it is possible to improve the scaling of the
upper bound to match the weaker lower bounds presented in this paper.
  We hope that this note serves to clarify existing results in the field of
reinforcement learning and provides interesting motivation for future work.","['Ian Osband', 'Benjamin Van Roy']","['stat.ML', 'cs.LG']",2016-08-09 09:02:01+00:00
http://arxiv.org/abs/1608.02731v1,Posterior Sampling for Reinforcement Learning Without Episodes,"This is a brief technical note to clarify some of the issues with applying
the application of the algorithm posterior sampling for reinforcement learning
(PSRL) in environments without fixed episodes. In particular, this paper aims
to:
  - Review some of results which have been proven for finite horizon MDPs
(Osband et al 2013, 2014a, 2014b, 2016) and also for MDPs with finite ergodic
structure (Gopalan et al 2014).
  - Review similar results for optimistic algorithms in infinite horizon
problems (Jaksch et al 2010, Bartlett and Tewari 2009, Abbasi-Yadkori and
Szepesvari 2011), with particular attention to the dynamic episode growth.
  - Highlight the delicate technical issue which has led to a fault in the
proof of the lazy-PSRL algorithm (Abbasi-Yadkori and Szepesvari 2015). We
present an explicit counterexample to this style of argument. Therefore, we
suggest that the Theorem 2 in (Abbasi-Yadkori and Szepesvari 2015) be instead
considered a conjecture, as it has no rigorous proof.
  - Present pragmatic approaches to apply PSRL in infinite horizon problems. We
conjecture that, under some additional assumptions, it will be possible to
obtain bounds $O( \sqrt{T} )$ even without episodic reset.
  We hope that this note serves to clarify existing results in the field of
reinforcement learning and provides interesting motivation for future work.","['Ian Osband', 'Benjamin Van Roy']","['stat.ML', 'cs.LG']",2016-08-09 09:01:13+00:00
http://arxiv.org/abs/1608.02715v1,A deep language model for software code,"Existing language models such as n-grams for software code often fail to
capture a long context where dependent code elements scatter far apart. In this
paper, we propose a novel approach to build a language model for software code
to address this particular issue. Our language model, partly inspired by human
memory, is built upon the powerful deep learning-based Long Short Term Memory
architecture that is capable of learning long-term dependencies which occur
frequently in software code. Results from our intrinsic evaluation on a corpus
of Java projects have demonstrated the effectiveness of our language model.
This work contributes to realizing our vision for DeepSoft, an end-to-end,
generic deep learning-based framework for modeling software and its development
process.","['Hoa Khanh Dam', 'Truyen Tran', 'Trang Pham']","['cs.SE', 'stat.ML']",2016-08-09 08:16:42+00:00
http://arxiv.org/abs/1608.02658v3,Revisiting Causality Inference in Memory-less Transition Networks,"Several methods exist to infer causal networks from massive volumes of
observational data. However, almost all existing methods require a considerable
length of time series data to capture cause and effect relationships. In
contrast, memory-less transition networks or Markov Chain data, which refers to
one-step transitions to and from an event, have not been explored for causality
inference even though such data is widely available. We find that causal
network can be inferred from characteristics of four unique distribution zones
around each event. We call this Composition of Transitions and show that cause,
effect, and random events exhibit different behavior in their compositions. We
applied machine learning models to learn these different behaviors and to infer
causality. We name this new method Causality Inference using Composition of
Transitions (CICT). To evaluate CICT, we used an administrative inpatient
healthcare dataset to set up a network of patients transitions between
different diagnoses. We show that CICT is highly accurate in inferring whether
the transition between a pair of events is causal or random and performs well
in identifying the direction of causality in a bi-directional association.","['Abbas Shojaee', 'Isuru Ranasinghe', 'Alireza Ani']","['stat.ML', 'cs.AI', 'nlin.CD', 'physics.data-an']",2016-08-08 23:46:59+00:00
http://arxiv.org/abs/1608.02554v1,Sparse recovery via Orthogonal Least-Squares under presence of Noise,"We consider the Orthogonal Least-Squares (OLS) algorithm for the recovery of
a $m$-dimensional $k$-sparse signal from a low number of noisy linear
measurements. The Exact Recovery Condition (ERC) in bounded noisy scenario is
established for OLS under certain condition on nonzero elements of the signal.
The new result also improves the existing guarantees for Orthogonal Matching
Pursuit (OMP) algorithm. In addition, This framework is employed to provide
probabilistic guarantees for the case that the coefficient matrix is drawn at
random according to Gaussian or Bernoulli distribution where we exploit some
concentration properties. It is shown that under certain conditions, OLS
recovers the true support in $k$ iterations with high probability. This in turn
demonstrates that ${\cal O}\left(k\log m\right)$ measurements is sufficient for
exact recovery of sparse signals via OLS.","['Abolfazl Hashemi', 'Haris Vikalo']","['stat.ML', 'cs.IT', 'math.IT']",2016-08-08 18:52:33+00:00
http://arxiv.org/abs/1608.02549v2,Sampling Requirements and Accelerated Schemes for Sparse Linear Regression with Orthogonal Least-Squares,"We study the problem of inferring a sparse vector from random linear
combinations of its components. We propose the Accelerated Orthogonal
Least-Squares (AOLS) algorithm that improves performance of the well-known
Orthogonal Least-Squares (OLS) algorithm while requiring significantly lower
computational costs. While OLS greedily selects columns of the coefficient
matrix that correspond to non-zero components of the sparse vector, AOLS
employs a novel computationally efficient procedure that speeds up the search
by anticipating future selections via choosing $L$ columns in each step, where
$L$ is an adjustable hyper-parameter. We analyze the performance of AOLS and
establish lower bounds on the probability of exact recovery for both noiseless
and noisy random linear measurements. In the noiseless scenario, it is shown
that when the coefficients are samples from a Gaussian distribution, AOLS with
high probability recovers a $k$-sparse $m$-dimensional sparse vector using
${\cal O}(k\log \frac{m}{k+L-1})$ measurements. Similar result is established
for the bounded-noise scenario where an additional condition on the smallest
nonzero element of the unknown vector is required. The asymptotic sampling
complexity of AOLS is lower than the asymptotic sampling complexity of the
existing sparse reconstruction algorithms. In simulations, AOLS is compared to
state-of-the-art sparse recovery techniques and shown to provide better
performance in terms of accuracy, running time, or both. Finally, we consider
an application of AOLS to clustering high-dimensional data lying on the union
of low-dimensional subspaces and demonstrate its superiority over existing
methods.","['Abolfazl Hashemi', 'Haris Vikalo']","['stat.ML', 'cs.IT', 'math.IT']",2016-08-08 18:36:30+00:00
http://arxiv.org/abs/1608.02485v2,Boosting as a kernel-based method,"Boosting combines weak (biased) learners to obtain effective learning
algorithms for classification and prediction. In this paper, we show a
connection between boosting and kernel-based methods, highlighting both
theoretical and practical applications. In the context of $\ell_2$ boosting, we
start with a weak linear learner defined by a kernel $K$. We show that boosting
with this learner is equivalent to estimation with a special {\it boosting
kernel} that depends on $K$, as well as on the regression matrix, noise
variance, and hyperparameters. The number of boosting iterations is modeled as
a continuous hyperparameter, and fit along with other parameters using standard
techniques.
  We then generalize the boosting kernel to a broad new class of boosting
approaches for more general weak learners, including those based on the
$\ell_1$, hinge and Vapnik losses. The approach allows fast hyperparameter
tuning for this general class, and has a wide range of applications, including
robust regression and classification. We illustrate some of these applications
with numerical examples on synthetic and real data.","['Aleksandr Y. Aravkin', 'Giulio Bottegal', 'Gianluigi Pillonetto']",['stat.ML'],2016-08-08 15:23:37+00:00
http://arxiv.org/abs/1608.03532v1,QPass: a Merit-based Evaluation of Soccer Passes,"Quantitative analysis of soccer players' passing ability focuses on
descriptive statistics without considering the players' real contribution to
the passing and ball possession strategy of their team. Which player is able to
help the build-up of an attack, or to maintain the possession of the ball? We
introduce a novel methodology called QPass to answer questions like these
quantitatively. Based on the analysis of an entire season, we rank the players
based on the intrinsic value of their passes using QPass. We derive an album of
pass trajectories for different gaming styles. Our methodology reveals a quite
counterintuitive paradigm: losing the ball possession could lead to better
chances to win a game.","['Laszlo Gyarmati', 'Rade Stanojevic']","['cs.AI', 'stat.AP', 'stat.ML', 'H.4']",2016-08-08 12:54:57+00:00
