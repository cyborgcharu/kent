id,title,abstract,authors,categories,date
http://arxiv.org/abs/1905.04398v2,Few-Shot Learning with Embedded Class Models and Shot-Free Meta Training,"We propose a method for learning embeddings for few-shot learning that is
suitable for use with any number of ways and any number of shots (shot-free).
Rather than fixing the class prototypes to be the Euclidean average of sample
embeddings, we allow them to live in a higher-dimensional space (embedded class
models) and learn the prototypes along with the model parameters. The class
representation function is defined implicitly, which allows us to deal with a
variable number of shots per each class with a simple constant-size
architecture. The class embedding encompasses metric learning, that facilitates
adding new classes without crowding the class representation space. Despite
being general and not tuned to the benchmark, our approach achieves
state-of-the-art performance on the standard few-shot benchmark datasets.","['Avinash Ravichandran', 'Rahul Bhotika', 'Stefano Soatto']","['cs.LG', 'cs.CV', 'stat.ML']",2019-05-10 23:21:31+00:00
http://arxiv.org/abs/1905.04396v3,Prediction and outlier detection in classification problems,"We consider the multi-class classification problem when the training data and
the out-of-sample test data may have different distributions and propose a
method called BCOPS (balanced and conformal optimized prediction sets). BCOPS
constructs a prediction set $C(x)$ as a subset of class labels, possibly empty.
It tries to optimize the out-of-sample performance, aiming to include the
correct class as often as possible, but also detecting outliers $x$, for which
the method returns no prediction (corresponding to $C(x)$ equal to the empty
set). The proposed method combines supervised-learning algorithms with the
method of conformal prediction to minimize a misclassification loss averaged
over the out-of-sample distribution. The constructed prediction sets have a
finite-sample coverage guarantee without distributional assumptions.
  We also propose a method to estimate the outlier detection rate of a given
method. We prove asymptotic consistency and optimality of our proposals under
suitable assumptions and illustrate our methods on real data examples.","['Leying Guan', 'Rob Tibshirani']","['stat.ME', 'math.ST', 'stat.AP', 'stat.ML', 'stat.TH']",2019-05-10 22:56:39+00:00
http://arxiv.org/abs/1905.04388v1,Multi-Pass Q-Networks for Deep Reinforcement Learning with Parameterised Action Spaces,"Parameterised actions in reinforcement learning are composed of discrete
actions with continuous action-parameters. This provides a framework for
solving complex domains that require combining high-level actions with flexible
control. The recent P-DQN algorithm extends deep Q-networks to learn over such
action spaces. However, it treats all action-parameters as a single joint input
to the Q-network, invalidating its theoretical foundations. We analyse the
issues with this approach and propose a novel method, multi-pass deep
Q-networks, or MP-DQN, to address them. We empirically demonstrate that MP-DQN
significantly outperforms P-DQN and other previous algorithms in terms of data
efficiency and converged policy performance on the Platform, Robot Soccer Goal,
and Half Field Offense domains.","['Craig J. Bester', 'Steven D. James', 'George D. Konidaris']","['cs.LG', 'stat.ML']",2019-05-10 21:57:41+00:00
http://arxiv.org/abs/1905.04363v2,Active embedding search via noisy paired comparisons,"Suppose that we wish to estimate a user's preference vector $w$ from paired
comparisons of the form ""does user $w$ prefer item $p$ or item $q$?,"" where
both the user and items are embedded in a low-dimensional Euclidean space with
distances that reflect user and item similarities. Such observations arise in
numerous settings, including psychometrics and psychology experiments, search
tasks, advertising, and recommender systems. In such tasks, queries can be
extremely costly and subject to varying levels of response noise; thus, we aim
to actively choose pairs that are most informative given the results of
previous comparisons. We provide new theoretical insights into the benefits and
challenges of greedy information maximization in this setting, and develop two
novel strategies that maximize lower bounds on information gain and are simpler
to analyze and compute respectively. We use simulated responses from a
real-world dataset to validate our strategies through their similar performance
to greedy information maximization, and their superior preference estimation
over state-of-the-art selection methods as well as random queries.","['Gregory H. Canal', 'Andrew K. Massimino', 'Mark A. Davenport', 'Christopher J. Rozell']","['stat.ML', 'cs.LG']",2019-05-10 19:55:50+00:00
http://arxiv.org/abs/1905.06134v1,Recommending Dream Jobs in a Biased Real World,"Machine learning models learn what we teach them to learn. Machine learning
is at the heart of recommender systems. If a machine learning model is trained
on biased data, the resulting recommender system may reflect the biases in its
recommendations. Biases arise at different stages in a recommender system, from
existing societal biases in the data such as the professional gender gap, to
biases introduced by the data collection or modeling processes. These biases
impact the performance of various components of recommender systems, from
offline training, to evaluation and online serving of recommendations in
production systems. Specific techniques can help reduce bias at each stage of a
recommender system. Reducing bias in our recommender systems is crucial to
successfully recommending dream jobs to hundreds of millions members worldwide,
while being true to LinkedIn's vision: ""To create economic opportunity for
every member of the global workforce"".",['Nadia Fawaz'],"['cs.IR', 'cs.LG', 'cs.SI', 'stat.ML']",2019-05-10 19:26:01+00:00
http://arxiv.org/abs/1905.04351v1,Solving Irregular and Data-enriched Differential Equations using Deep Neural Networks,"Recent work has introduced a simple numerical method for solving partial
differential equations (PDEs) with deep neural networks (DNNs). This paper
reviews and extends the method while applying it to analyze one of the most
fundamental features in numerical PDEs and nonlinear analysis: irregular
solutions. First, the Sod shock tube solution to compressible Euler equations
is discussed, analyzed, and then compared to conventional finite element and
finite volume methods. These methods are extended to consider performance
improvements and simultaneous parameter space exploration. Next, a shock
solution to compressible magnetohydrodynamics (MHD) is solved for, and used in
a scenario where experimental data is utilized to enhance a PDE system that is
\emph{a priori} insufficient to validate against the observed/experimental
data. This is accomplished by enriching the model PDE system with source terms
and using supervised training on synthetic experimental data. The resulting DNN
framework for PDEs seems to demonstrate almost fantastical ease of system
prototyping, natural integration of large data sets (be they synthetic or
experimental), all while simultaneously enabling single-pass exploration of the
entire parameter space.","['Craig Michoski', 'Milos Milosavljevic', 'Todd Oliver', 'David Hatch']","['cs.LG', 'cs.NA', 'physics.comp-ph', 'physics.data-an', 'stat.ML']",2019-05-10 19:25:53+00:00
http://arxiv.org/abs/1905.04337v1,Learning in structured MDPs with convex cost functions: Improved regret bounds for inventory management,"We consider a stochastic inventory control problem under censored demands,
lost sales, and positive lead times. This is a fundamental problem in inventory
management, with significant literature establishing near-optimality of a
simple class of policies called ``base-stock policies'' for the underlying
Markov Decision Process (MDP), as well as convexity of long run average-cost
under those policies. We consider the relatively less studied problem of
designing a learning algorithm for this problem when the underlying demand
distribution is unknown. The goal is to bound regret of the algorithm when
compared to the best base-stock policy. We utilize the convexity properties and
a newly derived bound on bias of base-stock policies to establish a connection
to stochastic convex bandit optimization.
  Our main contribution is a learning algorithm with a regret bound of
$\tilde{O}(L\sqrt{T}+D)$ for the inventory control problem. Here $L$ is the
fixed and known lead time, and $D$ is an unknown parameter of the demand
distribution described roughly as the number of time steps needed to generate
enough demand for depleting one unit of inventory. Notably, even though the
state space of the underlying MDP is continuous and $L$-dimensional, our regret
bounds depend linearly on $L$. Our results significantly improve the previously
best known regret bounds for this problem where the dependence on $L$ was
exponential and many further assumptions on demand distribution were required.
The techniques presented here may be of independent interest for other settings
that involve large structured MDPs but with convex cost functions.","['Shipra Agrawal', 'Randy Jia']","['cs.LG', 'stat.ML']",2019-05-10 18:38:38+00:00
http://arxiv.org/abs/1905.04271v1,Mutual Information Scaling and Expressive Power of Sequence Models,"Sequence models assign probabilities to variable-length sequences such as
natural language texts. The ability of sequence models to capture temporal
dependence can be characterized by the temporal scaling of correlation and
mutual information. In this paper, we study the mutual information of recurrent
neural networks (RNNs) including long short-term memories and self-attention
networks such as Transformers. Through a combination of theoretical study of
linear RNNs and empirical study of nonlinear RNNs, we find their mutual
information decays exponentially in temporal distance. On the other hand,
Transformers can capture long-range mutual information more efficiently, making
them preferable in modeling sequences with slow power-law mutual information,
such as natural languages and stock prices. We discuss the connection of these
results with statistical mechanics. We also point out the non-uniformity
problem in many natural language datasets. We hope this work provides a new
perspective in understanding the expressive power of sequence models and shed
new light on improving the architecture of them.",['Huitao Shen'],"['cs.LG', 'cond-mat.dis-nn', 'cs.IT', 'math.IT', 'stat.ML']",2019-05-10 17:21:21+00:00
http://arxiv.org/abs/1905.04270v1,Interpreting and Evaluating Neural Network Robustness,"Recently, adversarial deception becomes one of the most considerable threats
to deep neural networks. However, compared to extensive research in new designs
of various adversarial attacks and defenses, the neural networks' intrinsic
robustness property is still lack of thorough investigation. This work aims to
qualitatively interpret the adversarial attack and defense mechanism through
loss visualization, and establish a quantitative metric to evaluate the neural
network model's intrinsic robustness. The proposed robustness metric identifies
the upper bound of a model's prediction divergence in the given domain and thus
indicates whether the model can maintain a stable prediction. With extensive
experiments, our metric demonstrates several advantages over conventional
adversarial testing accuracy based robustness estimation: (1) it provides a
uniformed evaluation to models with different structures and parameter scales;
(2) it over-performs conventional accuracy based robustness estimation and
provides a more reliable evaluation that is invariant to different test
settings; (3) it can be fast generated without considerable testing cost.","['Fuxun Yu', 'Zhuwei Qin', 'Chenchen Liu', 'Liang Zhao', 'Yanzhi Wang', 'Xiang Chen']","['cs.LG', 'cs.CV', 'stat.ML']",2019-05-10 17:21:15+00:00
http://arxiv.org/abs/1905.04241v1,Hybrid Predictive Model: When an Interpretable Model Collaborates with a Black-box Model,"Interpretable machine learning has become a strong competitor for traditional
black-box models. However, the possible loss of the predictive performance for
gaining interpretability is often inevitable, putting practitioners in a
dilemma of choosing between high accuracy (black-box models) and
interpretability (interpretable models). In this work, we propose a novel
framework for building a Hybrid Predictive Model (HPM) that integrates an
interpretable model with any black-box model to combine their strengths. The
interpretable model substitutes the black-box model on a subset of data where
the black-box is overkill or nearly overkill, gaining transparency at no or low
cost of the predictive accuracy. We design a principled objective function that
considers predictive accuracy, model interpretability, and model transparency
(defined as the percentage of data processed by the interpretable substitute.)
Under this framework, we propose two hybrid models, one substituting with
association rules and the other with linear models, and we design customized
training algorithms for both models. We test the hybrid models on structured
data and text data where interpretable models collaborate with various
state-of-the-art black-box models. Results show that hybrid models obtain an
efficient trade-off between transparency and predictive performance,
characterized by our proposed efficient frontiers.","['Tong Wang', 'Qihang Lin']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-10 16:21:00+00:00
http://arxiv.org/abs/1905.04223v1,"Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges","Machine learning has evolved into an enabling technology for a wide range of
highly successful applications. The potential for this success to continue and
accelerate has placed machine learning (ML) at the top of research, economic
and political agendas. Such unprecedented interest is fuelled by a vision of ML
applicability extending to healthcare, transportation, defence and other
domains of great societal importance. Achieving this vision requires the use of
ML in safety-critical applications that demand levels of assurance beyond those
needed for current ML applications. Our paper provides a comprehensive survey
of the state-of-the-art in the assurance of ML, i.e. in the generation of
evidence that ML is sufficiently safe for its intended use. The survey covers
the methods capable of providing such evidence at different stages of the
machine learning lifecycle, i.e. of the complex, iterative process that starts
with the collection of the data used to train an ML component for a system, and
ends with the deployment of that component within the system. The paper begins
with a systematic presentation of the ML lifecycle and its stages. We then
define assurance desiderata for each stage, review existing methods that
contribute to achieving these desiderata, and identify open challenges that
require further research.","['Rob Ashmore', 'Radu Calinescu', 'Colin Paterson']","['cs.LG', 'cs.SE', 'stat.ML']",2019-05-10 15:44:38+00:00
http://arxiv.org/abs/1905.04206v2,The Regression Tsetlin Machine: A Tsetlin Machine for Continuous Output Problems,"The recently introduced Tsetlin Machine (TM) has provided competitive pattern
classification accuracy in several benchmarks, composing patterns with
easy-to-interpret conjunctive clauses in propositional logic. In this paper, we
go beyond pattern classification by introducing a new type of TMs, namely, the
Regression Tsetlin Machine (RTM). In all brevity, we modify the inner inference
mechanism of the TM so that input patterns are transformed into a single
continuous output, rather than to distinct categories. We achieve this by: (1)
using the conjunctive clauses of the TM to capture arbitrarily complex
patterns; (2) mapping these patterns to a continuous output through a novel
voting and normalization mechanism; and (3) employing a feedback scheme that
updates the TM clauses to minimize the regression error. The feedback scheme
uses a new activation probability function that stabilizes the updating of
clauses, while the overall system converges towards an accurate input-output
mapping. The performance of the RTM is evaluated using six different artificial
datasets with and without noise, in comparison with the Classic Tsetlin Machine
(CTM) and the Multiclass Tsetlin Machine (MTM). Our empirical results indicate
that the RTM obtains the best training and testing results for both noisy and
noise-free datasets, with a smaller number of clauses. This, in turn,
translates to higher regression accuracy, using significantly less
computational resources.","['K. Darshana Abeyrathna', 'Ole-Christoffer Granmo', 'Lei Jiao', 'Morten Goodwin']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-10 15:05:30+00:00
http://arxiv.org/abs/1905.04194v2,Formal Verification of Input-Output Mappings of Tree Ensembles,"Recent advances in machine learning and artificial intelligence are now being
considered in safety-critical autonomous systems where software defects may
cause severe harm to humans and the environment. Design organizations in these
domains are currently unable to provide convincing arguments that their systems
are safe to operate when machine learning algorithms are used to implement
their software.
  In this paper, we present an efficient method to extract equivalence classes
from decision trees and tree ensembles, and to formally verify that their
input-output mappings comply with requirements. The idea is that, given that
safety requirements can be traced to desirable properties on system
input-output patterns, we can use positive verification outcomes in safety
arguments. This paper presents the implementation of the method in the tool
VoTE (Verifier of Tree Ensembles), and evaluates its scalability on two case
studies presented in current literature.
  We demonstrate that our method is practical for tree ensembles trained on
low-dimensional data with up to 25 decision trees and tree depths of up to 20.
Our work also studies the limitations of the method with high-dimensional data
and preliminarily investigates the trade-off between large number of trees and
time taken for verification.","['John Törnblom', 'Simin Nadjm-Tehrani']","['cs.LG', 'stat.ML']",2019-05-10 14:30:59+00:00
http://arxiv.org/abs/1905.04192v1,Do Autonomous Agents Benefit from Hearing?,"Mapping states to actions in deep reinforcement learning is mainly based on
visual information. The commonly used approach for dealing with visual
information is to extract pixels from images and use them as state
representation for reinforcement learning agent. But, any vision only agent is
handicapped by not being able to sense audible cues. Using hearing, animals are
able to sense targets that are outside of their visual range. In this work, we
propose the use of audio as complementary information to visual only in state
representation. We assess the impact of such multi-modal setup in
reach-the-goal tasks in ViZDoom environment. Results show that the agent
improves its behavior when visual information is accompanied with audio
features.","['Abraham Woubie', 'Anssi Kanervisto', 'Janne Karttunen', 'Ville Hautamaki']","['cs.LG', 'cs.AI', 'cs.SD', 'stat.ML']",2019-05-10 14:25:49+00:00
http://arxiv.org/abs/1905.04191v1,Multiple Independent Subspace Clusterings,"Multiple clustering aims at discovering diverse ways of organizing data into
clusters. Despite the progress made, it's still a challenge for users to
analyze and understand the distinctive structure of each output clustering. To
ease this process, we consider diverse clusterings embedded in different
subspaces, and analyze the embedding subspaces to shed light into the structure
of each clustering. To this end, we provide a two-stage approach called MISC
(Multiple Independent Subspace Clusterings). In the first stage, MISC uses
independent subspace analysis to seek multiple and statistical independent
(i.e. non-redundant) subspaces, and determines the number of subspaces via the
minimum description length principle. In the second stage, to account for the
intrinsic geometric structure of samples embedded in each subspace, MISC
performs graph regularized semi-nonnegative matrix factorization to explore
clusters. It additionally integrates the kernel trick into matrix factorization
to handle non-linearly separable clusters. Experimental results on synthetic
datasets show that MISC can find different interesting clusterings from the
sought independent subspaces, and it also outperforms other related and
competitive approaches on real-world datasets.","['Xing Wang', 'Jun Wang', 'Carlotta Domeniconi', 'Guoxian Yu', 'Guoqiang Xiao', 'Maozu Guo']","['cs.LG', 'stat.ML']",2019-05-10 14:24:44+00:00
http://arxiv.org/abs/1905.04172v1,On the Connection Between Adversarial Robustness and Saliency Map Interpretability,"Recent studies on the adversarial vulnerability of neural networks have shown
that models trained to be more robust to adversarial attacks exhibit more
interpretable saliency maps than their non-robust counterparts. We aim to
quantify this behavior by considering the alignment between input image and
saliency map. We hypothesize that as the distance to the decision boundary
grows,so does the alignment. This connection is strictly true in the case of
linear models. We confirm these theoretical findings with experiments based on
models trained with a local Lipschitz regularization and identify where the
non-linear nature of neural networks weakens the relation.","['Christian Etmann', 'Sebastian Lunz', 'Peter Maass', 'Carola-Bibiane Schönlieb']","['stat.ML', 'cs.CV', 'cs.LG']",2019-05-10 13:45:21+00:00
http://arxiv.org/abs/1905.04159v1,Single-Path NAS: Device-Aware Efficient ConvNet Design,"Can we automatically design a Convolutional Network (ConvNet) with the
highest image classification accuracy under the latency constraint of a mobile
device? Neural Architecture Search (NAS) for ConvNet design is a challenging
problem due to the combinatorially large design space and search time (at least
200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a
novel differentiable NAS method for designing device-efficient ConvNets in less
than 4 hours. 1. Novel NAS formulation: our method introduces a single-path,
over-parameterized ConvNet to encode all architectural decisions with shared
convolutional kernel parameters. 2. NAS efficiency: Our method decreases the
NAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster
compared to prior work. 3. On-device image classification: Single-Path NAS
achieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a
Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with
similar latency (<80ms).","['Dimitrios Stamoulis', 'Ruizhou Ding', 'Di Wang', 'Dimitrios Lymberopoulos', 'Bodhi Priyantha', 'Jie Liu', 'Diana Marculescu']","['cs.LG', 'cs.CV', 'stat.ML']",2019-05-10 13:23:48+00:00
http://arxiv.org/abs/1905.04152v1,Massive Autonomous UAV Path Planning: A Neural Network Based Mean-Field Game Theoretic Approach,"This paper investigates the autonomous control of massive unmanned aerial
vehicles (UAVs) for mission-critical applications (e.g., dispatching many UAVs
from a source to a destination for firefighting). Achieving their fast travel
and low motion energy without inter-UAV collision under wind perturbation is a
daunting control task, which incurs huge communication energy for exchanging
UAV states in real time. We tackle this problem by exploiting a mean-field game
(MFG) theoretic control method that requires the UAV state exchanges only once
at the initial source. Afterwards, each UAV can control its acceleration by
locally solving two partial differential equations (PDEs), known as the
Hamilton-Jacobi-Bellman (HJB) and Fokker-Planck-Kolmogorov (FPK) equations.
This approach, however, brings about huge computation energy for solving the
PDEs, particularly under multi-dimensional UAV states. We address this issue by
utilizing a machine learning (ML) method where two separate ML models
approximate the solutions of the HJB and FPK equations. These ML models are
trained and exploited using an online gradient descent method with low
computational complexity. Numerical evaluations validate that the proposed ML
aided MFG theoretic algorithm, referred to as MFG learning control, is
effective in collision avoidance with low communication energy and acceptable
computation energy.","['Hamid Shiri', 'Jihong Park', 'Mehdi Bennis']","['cs.SY', 'cs.LG', 'cs.NI', 'stat.ML']",2019-05-10 13:07:00+00:00
http://arxiv.org/abs/1905.04121v1,"The sharp, the flat and the shallow: Can weakly interacting agents learn to escape bad minima?","An open problem in machine learning is whether flat minima generalize better
and how to compute such minima efficiently. This is a very challenging problem.
As a first step towards understanding this question we formalize it as an
optimization problem with weakly interacting agents. We review appropriate
background material from the theory of stochastic processes and provide
insights that are relevant to practitioners. We propose an algorithmic
framework for an extended stochastic gradient Langevin dynamics and illustrate
its potential. The paper is written as a tutorial, and presents an alternative
use of multi-agent learning. Our primary focus is on the design of algorithms
for machine learning applications; however the underlying mathematical
framework is suitable for the understanding of large scale systems of agent
based models that are popular in the social sciences, economics and finance.","['Nikolas Kantas', 'Panos Parpas', 'Grigorios A. Pavliotis']","['stat.ML', 'cs.LG', 'cs.MA']",2019-05-10 12:37:51+00:00
http://arxiv.org/abs/1905.04105v1,Which Contrast Does Matter? Towards a Deep Understanding of MR Contrast using Collaborative GAN,"Thanks to the recent success of generative adversarial network (GAN) for
image synthesis, there are many exciting GAN approaches that successfully
synthesize MR image contrast from other images with different contrasts. These
approaches are potentially important for image imputation problems, where
complete set of data is often difficult to obtain and image synthesis is one of
the key solutions for handling the missing data problem. Unfortunately, the
lack of the scalability of the existing GAN-based image translation approaches
poses a fundamental challenge to understand the nature of the MR contrast
imputation problem: which contrast does matter? Here, we present a systematic
approach using Collaborative Generative Adversarial Networks (CollaGAN), which
enable the learning of the joint image manifold of multiple MR contrasts to
investigate which contrasts are essential. Our experimental results showed that
the exogenous contrast from contrast agents is not replaceable, but other
endogenous contrast such as T1, T2, etc can be synthesized from other contrast.
These findings may give important guidance to the acquisition protocol design
for MR in real clinical environment.","['Dongwook Lee', 'Won-Jin Moon', 'Jong Chul Ye']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2019-05-10 12:18:19+00:00
http://arxiv.org/abs/1905.06109v1,A New Anchor Word Selection Method for the Separable Topic Discovery,"Separable Non-negative Matrix Factorization (SNMF) is an important method for
topic modeling, where ""separable"" assumes every topic contains at least one
anchor word, defined as a word that has non-zero probability only on that
topic. SNMF focuses on the word co-occurrence patterns to reveal topics by two
steps: anchor word selection and topic recovery. The quality of the anchor
words strongly influences the quality of the extracted topics. Existing anchor
word selection algorithm is to greedily find an approximate convex hull in a
high-dimensional word co-occurrence space. In this work, we propose a new
method for the anchor word selection by associating the word co-occurrence
probability with the words similarity and assuming that the most different
words on semantic are potential candidates for the anchor words. Therefore, if
the similarity of a word-pair is very low, then the two words are very likely
to be the anchor words. According to the statistical information of text
corpora, we can get the similarity of all word-pairs. We build the word
similarity graph where the nodes correspond to words and weights on edges stand
for the word-pair similarity. Following this way, we design a greedy method to
find a minimum edge-weight anchor clique of a given size in the graph for the
anchor word selection. Extensive experiments on real-world corpus demonstrate
the effectiveness of the proposed anchor word selection method that outperforms
the common convex hull-based methods on the revealed topic quality. Meanwhile,
our method is much faster than typical SNMF based method.","['Kun He', 'Wu Wang', 'Xiaosen Wang', 'John E. Hopcroft']","['cs.IR', 'cs.CL', 'cs.LG', 'stat.ML']",2019-05-10 12:16:10+00:00
http://arxiv.org/abs/1905.04094v1,Domain Adversarial Reinforcement Learning for Partial Domain Adaptation,"Partial domain adaptation aims to transfer knowledge from a label-rich source
domain to a label-scarce target domain which relaxes the fully shared label
space assumption across different domains. In this more general and practical
scenario, a major challenge is how to select source instances in the shared
classes across different domains for positive transfer. To address this issue,
we propose a Domain Adversarial Reinforcement Learning (DARL) framework to
automatically select source instances in the shared classes for circumventing
negative transfer as well as to simultaneously learn transferable features
between domains by reducing the domain shift. Specifically, in this framework,
we employ deep Q-learning to learn policies for an agent to make selection
decisions by approximating the action-value function. Moreover, domain
adversarial learning is introduced to learn domain-invariant features for the
selected source instances by the agent and the target instances, and also to
determine rewards for the agent based on how relevant the selected source
instances are to the target domain. Experiments on several benchmark datasets
demonstrate that the superior performance of our DARL method over existing
state of the arts for partial domain adaptation.","['Jin Chen', 'Xinxiao Wu', 'Lixin Duan', 'Shenghua Gao']","['cs.LG', 'cs.CV', 'stat.ML']",2019-05-10 12:02:32+00:00
http://arxiv.org/abs/1905.04079v2,Compressing Weight-updates for Image Artifacts Removal Neural Networks,"In this paper, we present a novel approach for fine-tuning a decoder-side
neural network in the context of image compression, such that the
weight-updates are better compressible. At encoder side, we fine-tune a
pre-trained artifact removal network on target data by using a compression
objective applied on the weight-update. In particular, the compression
objective encourages weight-updates which are sparse and closer to quantized
values. This way, the final weight-update can be compressed more efficiently by
pruning and quantization, and can be included into the encoded bitstream
together with the image bitstream of a traditional codec. We show that this
approach achieves reconstruction quality which is on-par or slightly superior
to a traditional codec, at comparable bitrates. To our knowledge, this is the
first attempt to combine image compression and neural network's weight update
compression.","['Yat Hong Lam', 'Alireza Zare', 'Caglar Aytekin', 'Francesco Cricri', 'Jani Lainema', 'Emre Aksu', 'Miska Hannuksela']","['cs.LG', 'cs.MM', 'stat.ML']",2019-05-10 11:36:36+00:00
http://arxiv.org/abs/1905.05004v2,Capturing Evolution Genes for Time Series Data,"The modeling of time series is becoming increasingly critical in a wide
variety of applications. Overall, data evolves by following different patterns,
which are generally caused by different user behaviors. Given a time series, we
define the evolution gene to capture the latent user behaviors and to describe
how the behaviors lead to the generation of time series. In particular, we
propose a uniform framework that recognizes different evolution genes of
segments by learning a classifier, and adopt an adversarial generator to
implement the evolution gene by estimating the segments' distribution.
Experimental results based on a synthetic dataset and five real-world datasets
show that our approach can not only achieve a good prediction results (e.g.,
averagely +10.56% in terms of F1), but is also able to provide explanations of
the results.","['Wenjie Hu', 'Jianping Huang', 'Liang Wu', 'Yang Yang', 'Zongtao Liu', 'Zhanlin Sun', 'Bingshen Yao', 'Ke Chen']","['cs.LG', 'stat.ML']",2019-05-10 11:09:34+00:00
http://arxiv.org/abs/1905.04062v2,A Contrastive Divergence for Combining Variational Inference and MCMC,"We develop a method to combine Markov chain Monte Carlo (MCMC) and
variational inference (VI), leveraging the advantages of both inference
approaches. Specifically, we improve the variational distribution by running a
few MCMC steps. To make inference tractable, we introduce the variational
contrastive divergence (VCD), a new divergence that replaces the standard
Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of
discrepancy between the initial variational distribution and its improved
version (obtained after running the MCMC steps), and it converges
asymptotically to the symmetrized KL divergence between the variational
distribution and the posterior of interest. The VCD objective can be optimized
efficiently with respect to the variational parameters via stochastic
optimization. We show experimentally that optimizing the VCD leads to better
predictive performance on two latent variable models: logistic matrix
factorization and variational autoencoders (VAEs).","['Francisco J. R. Ruiz', 'Michalis K. Titsias']","['stat.ML', 'cs.LG']",2019-05-10 10:45:23+00:00
http://arxiv.org/abs/1905.04042v2,Prototype Propagation Networks (PPN) for Weakly-supervised Few-shot Learning on Category Graph,"A variety of machine learning applications expect to achieve rapid learning
from a limited number of labeled data. However, the success of most current
models is the result of heavy training on big data. Meta-learning addresses
this problem by extracting common knowledge across different tasks that can be
quickly adapted to new tasks. However, they do not fully explore
weakly-supervised information, which is usually free or cheap to collect. In
this paper, we show that weakly-labeled data can significantly improve the
performance of meta-learning on few-shot classification. We propose prototype
propagation network (PPN) trained on few-shot tasks together with data
annotated by coarse-label. Given a category graph of the targeted fine-classes
and some weakly-labeled coarse-classes, PPN learns an attention mechanism which
propagates the prototype of one class to another on the graph, so that the
K-nearest neighbor (KNN) classifier defined on the propagated prototypes
results in high accuracy across different few-shot tasks. The training tasks
are generated by subgraph sampling, and the training objective is obtained by
accumulating the level-wise classification loss on the subgraph. The resulting
graph of prototypes can be continually re-used and updated for new tasks and
classes. We also introduce two practical test/inference settings which differ
according to whether the test task can leverage any weakly-supervised
information as in training. On two benchmarks, PPN significantly outperforms
most recent few-shot learning methods in different settings, even when they are
also allowed to train on weakly-labeled data.","['Lu Liu', 'Tianyi Zhou', 'Guodong Long', 'Jing Jiang', 'Lina Yao', 'Chengqi Zhang']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2019-05-10 09:57:23+00:00
http://arxiv.org/abs/1905.04022v4,Evaluating probabilistic forecasts of extremes using continuous ranked probability score distributions,"Verifying probabilistic forecasts for extreme events is a highly active
research area because popular media and public opinions are naturally focused
on extreme events, and biased conclusions are readily made. In this context,
classical verification methods tailored for extreme events, such as thresholded
and weighted scoring rules, have undesirable properties that cannot be
mitigated, and the well-known continuous ranked probability score (CRPS) is no
exception.
  In this paper, we define a formal framework for assessing the behavior of
forecast evaluation procedures with respect to extreme events, which we use to
demonstrate that assessment based on the expectation of a proper score is not
suitable for extremes. Alternatively, we propose studying the properties of the
CRPS as a random variable by using extreme value theory to address extreme
event verification. An index is introduced to compare calibrated forecasts,
which summarizes the ability of probabilistic forecasts for predicting
extremes. The strengths and limitations of this method are discussed using both
theoretical arguments and simulations.","['Maxime Taillardat', 'Anne-Laure Fougères', 'Philippe Naveau', 'Raphaël de Fondeville']","['stat.ME', 'math.ST', 'stat.AP', 'stat.ML', 'stat.TH']",2019-05-10 09:15:38+00:00
http://arxiv.org/abs/1905.04014v2,Supervized Segmentation with Graph-Structured Deep Metric Learning,"We present a fully-supervized method for learning to segment data structured
by an adjacency graph. We introduce the graph-structured contrastive loss, a
loss function structured by a ground truth segmentation. It promotes learning
vertex embeddings which are homogeneous within desired segments, and have high
contrast at their interface. Thus, computing a piecewise-constant approximation
of such embeddings produces a graph-partition close to the objective
segmentation. This loss is fully backpropagable, which allows us to learn
vertex embeddings with deep learning algorithms. We evaluate our methods on a
3D point cloud oversegmentation task, defining a new state-of-the-art by a
large margin. These results are based on the published work of Landrieu and
Boussaha 2019.","['Loic Landrieu', 'Mohamed Boussaha']","['cs.LG', 'cs.CV', 'stat.ML']",2019-05-10 08:55:49+00:00
http://arxiv.org/abs/1905.03994v2,Predicting Path Failure In Time-Evolving Graphs,"In this paper we use a time-evolving graph which consists of a sequence of
graph snapshots over time to model many real-world networks. We study the path
classification problem in a time-evolving graph, which has many applications in
real-world scenarios, for example, predicting path failure in a
telecommunication network and predicting path congestion in a traffic network
in the near future. In order to capture the temporal dependency and graph
structure dynamics, we design a novel deep neural network named Long Short-Term
Memory R-GCN (LRGCN). LRGCN considers temporal dependency between time-adjacent
graph snapshots as a special relation with memory, and uses relational GCN to
jointly process both intra-time and inter-time relations. We also propose a new
path representation method named self-attentive path embedding (SAPE), to embed
paths of arbitrary length into fixed-length vectors. Through experiments on a
real-world telecommunication network and a traffic network in California, we
demonstrate the superiority of LRGCN to other competing methods in path failure
prediction, and prove the effectiveness of SAPE on path representation.","['Jia Li', 'Zhichao Han', 'Hong Cheng', 'Jiao Su', 'Pengyun Wang', 'Jianfeng Zhang', 'Lujia Pan']","['cs.LG', 'stat.ML']",2019-05-10 08:03:12+00:00
http://arxiv.org/abs/1905.03985v1,Attention-based Deep Reinforcement Learning for Multi-view Environments,"In reinforcement learning algorithms, it is a common practice to account for
only a single view of the environment to make the desired decisions; however,
utilizing multiple views of the environment can help to promote the learning of
complicated policies. Since the views may frequently suffer from partial
observability, their provided observation can have different levels of
importance. In this paper, we present a novel attention-based deep
reinforcement learning method in a multi-view environment in which each view
can provide various representative information about the environment.
Specifically, our method learns a policy to dynamically attend to views of the
environment based on their importance in the decision-making process. We
evaluate the performance of our method on TORCS racing car simulator and three
other complex 3D environments with obstacles.","['Elaheh Barati', 'Xuewen Chen', 'Zichun Zhong']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-10 07:39:39+00:00
http://arxiv.org/abs/1905.03980v1,Bayesian Optimized Continual Learning with Attention Mechanism,"Though neural networks have achieved much progress in various applications,
it is still highly challenging for them to learn from a continuous stream of
tasks without forgetting. Continual learning, a new learning paradigm, aims to
solve this issue. In this work, we propose a new model for continual learning,
called Bayesian Optimized Continual Learning with Attention Mechanism (BOCL)
that dynamically expands the network capacity upon the arrival of new tasks by
Bayesian optimization and selectively utilizes previous knowledge (e.g. feature
maps of previous tasks) via attention mechanism. Our experiments on variants of
MNIST and CIFAR-100 demonstrate that our methods outperform the
state-of-the-art in preventing catastrophic forgetting and fitting new tasks
better.","['Ju Xu', 'Jin Ma', 'Zhanxing Zhu']","['cs.LG', 'stat.ML']",2019-05-10 07:30:53+00:00
http://arxiv.org/abs/1905.05006v4,Time-Series Event Prediction with Evolutionary State Graph,"The accurate and interpretable prediction of future events in time-series
data often requires the capturing of representative patterns (or referred to as
states) underpinning the observed data. To this end, most existing studies
focus on the representation and recognition of states, but ignore the changing
transitional relations among them. In this paper, we present evolutionary state
graph, a dynamic graph structure designed to systematically represent the
evolving relations (edges) among states (nodes) along time. We conduct analysis
on the dynamic graphs constructed from the time-series data and show that
changes on the graph structures (e.g., edges connecting certain state nodes)
can inform the occurrences of events (i.e., time-series fluctuation). Inspired
by this, we propose a novel graph neural network model, Evolutionary State
Graph Network (EvoNet), to encode the evolutionary state graph for accurate and
interpretable time-series event prediction. Specifically, Evolutionary State
Graph Network models both the node-level (state-to-state) and graph-level
(segment-to-segment) propagation, and captures the node-graph
(state-to-segment) interactions over time. Experimental results based on five
real-world datasets show that our approach not only achieves clear improvements
compared with 11 baselines, but also provides more insights towards explaining
the results of event predictions.","['Wenjie Hu', 'Yang Yang', 'Ziqiang Cheng', 'Carl Yang', 'Xiang Ren']","['cs.LG', 'stat.ML']",2019-05-10 07:11:17+00:00
http://arxiv.org/abs/1905.03970v4,Reinforcement Learning in Non-Stationary Environments,"Reinforcement learning (RL) methods learn optimal decisions in the presence
of a stationary environment. However, the stationary assumption on the
environment is very restrictive. In many real world problems like traffic
signal control, robotic applications, one often encounters situations with
non-stationary environments and in these scenarios, RL methods yield
sub-optimal decisions. In this paper, we thus consider the problem of
developing RL methods that obtain optimal decisions in a non-stationary
environment. The goal of this problem is to maximize the long-term discounted
reward achieved when the underlying model of the environment changes over time.
To achieve this, we first adapt a change point algorithm to detect change in
the statistics of the environment and then develop an RL algorithm that
maximizes the long-run reward accrued. We illustrate that our change point
method detects change in the model of the environment effectively and thus
facilitates the RL algorithm in maximizing the long-run reward. We further
validate the effectiveness of the proposed solution on non-stationary random
Markov decision processes, a sensor energy management problem and a traffic
signal control problem.","['Sindhu Padakandla', 'Prabuchandran K. J', 'Shalabh Bhatnagar']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-10 07:05:27+00:00
http://arxiv.org/abs/1905.03946v1,Credit Scoring for Micro-Loans,"Credit Scores are ubiquitous and instrumental for loan providers and
regulators. In this paper we showcase how micro-loan credit system can be
developed in real setting. We show what challenges arise and discuss solutions.
Particularly, we are concerned about model interpretability and data quality.
In the final section, we introduce semi-supervised algorithm that aids model
development and evaluate its performance","['Nikolay Dubina', 'Dasom Kang', 'Alex Suh']","['cs.LG', 'stat.ML']",2019-05-10 05:26:03+00:00
http://arxiv.org/abs/1905.03929v3,GAN-powered Deep Distributional Reinforcement Learning for Resource Management in Network Slicing,"Network slicing is a key technology in 5G communications system. Its purpose
is to dynamically and efficiently allocate resources for diversified services
with distinct requirements over a common underlying physical infrastructure.
Therein, demand-aware resource allocation is of significant importance to
network slicing. In this paper, we consider a scenario that contains several
slices in a radio access network with base stations that share the same
physical resources (e.g., bandwidth or slots). We leverage deep reinforcement
learning (DRL) to solve this problem by considering the varying service demands
as the environment state and the allocated resources as the environment action.
In order to reduce the effects of the annoying randomness and noise embedded in
the received service level agreement (SLA) satisfaction ratio (SSR) and
spectrum efficiency (SE), we primarily propose generative adversarial
network-powered deep distributional Q network (GAN-DDQN) to learn the
action-value distribution driven by minimizing the discrepancy between the
estimated action-value distribution and the target action-value distribution.
We put forward a reward-clipping mechanism to stabilize GAN-DDQN training
against the effects of widely-spanning utility values. Moreover, we further
develop Dueling GAN-DDQN, which uses a specially designed dueling generator, to
learn the action-value distribution by estimating the state-value distribution
and the action advantage function. Finally, we verify the performance of the
proposed GAN-DDQN and Dueling GAN-DDQN algorithms through extensive
simulations.","['Yuxiu Hua', 'Rongpeng Li', 'Zhifeng Zhao', 'Xianfu Chen', 'Honggang Zhang']","['cs.LG', 'cs.AI', 'cs.NI', 'stat.ML']",2019-05-10 04:10:43+00:00
http://arxiv.org/abs/1905.03927v3,Generalized Second Order Value Iteration in Markov Decision Processes,"Value iteration is a fixed point iteration technique utilized to obtain the
optimal value function and policy in a discounted reward Markov Decision
Process (MDP). Here, a contraction operator is constructed and applied
repeatedly to arrive at the optimal solution. Value iteration is a first order
method and therefore it may take a large number of iterations to converge to
the optimal solution. Successive relaxation is a popular technique that can be
applied to solve a fixed point equation. It has been shown in the literature
that, under a special structure of the MDP, successive over-relaxation
technique computes the optimal value function faster than standard value
iteration. In this work, we propose a second order value iteration procedure
that is obtained by applying the Newton-Raphson method to the successive
relaxation value iteration scheme. We prove the global convergence of our
algorithm to the optimal solution asymptotically and show the second order
convergence. Through experiments, we demonstrate the effectiveness of our
proposed approach.","['Chandramouli Kamanchi', 'Raghuram Bharadwaj Diddigi', 'Shalabh Bhatnagar']","['cs.LG', 'stat.ML']",2019-05-10 04:02:50+00:00
http://arxiv.org/abs/1905.03920v2,Integrating Tensor Similarity to Enhance Clustering Performance,"The performance of most the clustering methods hinges on the used pairwise
affinity, which is usually denoted by a similarity matrix. However, the
pairwise similarity is notoriously known for its vulnerability of noise
contamination or the imbalance in samples or features, and thus hinders
accurate clustering. To tackle this issue, we propose to use information among
samples to boost the clustering performance. We proved that a simplified
similarity for pairs, denoted by a fourth order tensor, equals to the Kronecker
product of pairwise similarity matrices under decomposable assumption, or
provide complementary information for which the pairwise similarity missed
under indecomposable assumption. Then a high order similarity matrix is
obtained from the tensor similarity via eigenvalue decomposition. The high
order similarity capturing spatial information serves as a robust complement
for the pairwise similarity. It is further integrated with the popular pairwise
similarity, named by IPS2, to boost the clustering performance. Extensive
experiments demonstrated that the proposed IPS2 significantly outperformed
previous similarity-based methods on real-world datasets and it was capable of
handling the clustering task over under-sampled and noisy datasets.","['Hong Peng', 'Yu Hu', 'Jiazhou Chen', 'Haiyan Wang', 'Yang Li', 'Hongmin Cai']","['cs.LG', 'stat.ML', '68U99']",2019-05-10 03:15:27+00:00
http://arxiv.org/abs/1905.07320v3,EENA: Efficient Evolution of Neural Architecture,"Latest algorithms for automatic neural architecture search perform remarkable
but are basically directionless in search space and computational expensive in
training of every intermediate architecture. In this paper, we propose a method
for efficient architecture search called EENA (Efficient Evolution of Neural
Architecture). Due to the elaborately designed mutation and crossover
operations, the evolution process can be guided by the information have already
been learned. Therefore, less computational effort will be required while the
searching and training time can be reduced significantly. On CIFAR-10
classification, EENA using minimal computational resources (0.65 GPU-days) can
design highly effective neural architecture which achieves 2.56% test error
with 8.47M parameters. Furthermore, the best architecture discovered is also
transferable for CIFAR-100.","['Hui Zhu', 'Zhulin An', 'Chuanguang Yang', 'Kaiqiang Xu', 'Erhu Zhao', 'Yongjun Xu']","['cs.NE', 'cs.CV', 'cs.LG', 'stat.ML']",2019-05-10 02:34:23+00:00
http://arxiv.org/abs/1905.03911v3,Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning,"Dimensionality reduction (DR) is frequently used for analyzing and
visualizing high-dimensional data as it provides a good first glance of the
data. However, to interpret the DR result for gaining useful insights from the
data, it would take additional analysis effort such as identifying clusters and
understanding their characteristics. While there are many automatic methods
(e.g., density-based clustering methods) to identify clusters, effective
methods for understanding a cluster's characteristics are still lacking. A
cluster can be mostly characterized by its distribution of feature values.
Reviewing the original feature values is not a straightforward task when the
number of features is large. To address this challenge, we present a visual
analytics method that effectively highlights the essential features of a
cluster in a DR result. To extract the essential features, we introduce an
enhanced usage of contrastive principal component analysis (cPCA). Our method,
called ccPCA (contrasting clusters in PCA), can calculate each feature's
relative contribution to the contrast between one cluster and other clusters.
With ccPCA, we have created an interactive system including a scalable
visualization of clusters' feature contributions. We demonstrate the
effectiveness of our method and system with case studies using several publicly
available datasets.","['Takanori Fujiwara', 'Oh-Hyun Kwon', 'Kwan-Liu Ma']","['cs.LG', 'cs.HC', 'stat.ML', 'I.3.8']",2019-05-10 02:07:58+00:00
http://arxiv.org/abs/1905.03871v5,Differentially Private Learning with Adaptive Clipping,"Existing approaches for training neural networks with user-level differential
privacy (e.g., DP Federated Averaging) in federated learning (FL) settings
involve bounding the contribution of each user's model update by clipping it to
some constant value. However there is no good a priori setting of the clipping
norm across tasks and learning settings: the update norm distribution depends
on the model architecture and loss, the amount of data on each device, the
client learning rate, and possibly various other parameters. We propose a
method wherein instead of a fixed clipping norm, one clips to a value at a
specified quantile of the update norm distribution, where the value at the
quantile is itself estimated online, with differential privacy. The method
tracks the quantile closely, uses a negligible amount of privacy budget, is
compatible with other federated learning technologies such as compression and
secure aggregation, and has a straightforward joint DP analysis with DP-FedAvg.
Experiments demonstrate that adaptive clipping to the median update norm works
well across a range of realistic federated learning tasks, sometimes
outperforming even the best fixed clip chosen in hindsight, and without the
need to tune any clipping hyperparameter.","['Galen Andrew', 'Om Thakkar', 'H. Brendan McMahan', 'Swaroop Ramaswamy']","['cs.LG', 'stat.ML']",2019-05-09 21:50:15+00:00
http://arxiv.org/abs/1905.13149v1,The Art of Food: Meal Image Synthesis from Ingredients,"In this work we propose a new computational framework, based on generative
deep models, for synthesis of photo-realistic food meal images from textual
descriptions of its ingredients. Previous works on synthesis of images from
text typically rely on pre-trained text models to extract text features,
followed by a generative neural networks (GANs) aimed to generate realistic
images conditioned on the text features. These works mainly focus on generating
spatially compact and well-defined categories of objects, such as birds or
flowers. In contrast, meal images are significantly more complex, consisting of
multiple ingredients whose appearance and spatial qualities are further
modified by cooking methods. We propose a method that first builds an
attention-based ingredients-image association model, which is then used to
condition a generative neural network tasked with synthesizing meal images.
Furthermore, a cycle-consistent constraint is added to further improve image
quality and control appearance. Extensive experiments show our model is able to
generate meal image corresponding to the ingredients, which could be used to
augment existing dataset for solving other computational food analysis
problems.","['Fangda Han', 'Ricardo Guerrero', 'Vladimir Pavlovic']","['cs.CV', 'cs.GR', 'cs.LG', 'stat.ML']",2019-05-09 20:57:51+00:00
http://arxiv.org/abs/1905.05701v2,Modeling user context for valence prediction from narratives,"Automated prediction of valence, one key feature of a person's emotional
state, from individuals' personal narratives may provide crucial information
for mental healthcare (e.g. early diagnosis of mental diseases, supervision of
disease course, etc.). In the Interspeech 2018 ComParE Self-Assessed Affect
challenge, the task of valence prediction was framed as a three-class
classification problem using 8 seconds fragments from individuals' narratives.
As such, the task did not allow for exploring contextual information of the
narratives. In this work, we investigate the intrinsic information from
multiple narratives recounted by the same individual in order to predict their
current state-of-mind. Furthermore, with generalizability in mind, we decided
to focus our experiments exclusively on textual information as the public
availability of audio narratives is limited compared to text. Our hypothesis
is, that context modeling might provide insights about emotion triggering
concepts (e.g. events, people, places) mentioned in the narratives that are
linked to an individual's state of mind. We explore multiple machine learning
techniques to model narratives. We find that the models are able to capture
inter-individual differences, leading to more accurate predictions of an
individual's emotional state, as compared to single narratives.","['Aniruddha Tammewar', 'Alessandra Cervone', 'Eva-Maria Messner', 'Giuseppe Riccardi']","['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']",2019-05-09 20:57:14+00:00
http://arxiv.org/abs/1905.03837v1,Exploring the Hyperparameter Landscape of Adversarial Robustness,"Adversarial training shows promise as an approach for training models that
are robust towards adversarial perturbation. In this paper, we explore some of
the practical challenges of adversarial training. We present a sensitivity
analysis that illustrates that the effectiveness of adversarial training hinges
on the settings of a few salient hyperparameters. We show that the robustness
surface that emerges across these salient parameters can be surprisingly
complex and that therefore no effective one-size-fits-all parameter settings
exist. We then demonstrate that we can use the same salient hyperparameters as
tuning knob to navigate the tension that can arise between robustness and
accuracy. Based on these findings, we present a practical approach that
leverages hyperparameter optimization techniques for tuning adversarial
training to maximize robustness while keeping the loss in accuracy within a
defined budget.","['Evelyn Duesterwald', 'Anupama Murthi', 'Ganesh Venkataraman', 'Mathieu Sinn', 'Deepak Vijaykeerthy']","['cs.LG', 'stat.ML']",2019-05-09 20:06:02+00:00
http://arxiv.org/abs/1905.03828v2,Universal Adversarial Perturbations for Speech Recognition Systems,"In this work, we demonstrate the existence of universal adversarial audio
perturbations that cause mis-transcription of audio signals by automatic speech
recognition (ASR) systems. We propose an algorithm to find a single
quasi-imperceptible perturbation, which when added to any arbitrary speech
signal, will most likely fool the victim speech recognition model. Our
experiments demonstrate the application of our proposed technique by crafting
audio-agnostic universal perturbations for the state-of-the-art ASR system --
Mozilla DeepSpeech. Additionally, we show that such perturbations generalize to
a significant extent across models that are not available during training, by
performing a transferability test on a WaveNet based ASR system.","['Paarth Neekhara', 'Shehzeen Hussain', 'Prakhar Pandey', 'Shlomo Dubnov', 'Julian McAuley', 'Farinaz Koushanfar']","['cs.LG', 'cs.SD', 'eess.AS', 'stat.ML']",2019-05-09 19:35:30+00:00
http://arxiv.org/abs/1905.03826v2,Random Function Priors for Correlation Modeling,"The likelihood model of high dimensional data $X_n$ can often be expressed as
$p(X_n|Z_n,\theta)$, where $\theta\mathrel{\mathop:}=(\theta_k)_{k\in[K]}$ is a
collection of hidden features shared across objects, indexed by $n$, and $Z_n$
is a non-negative factor loading vector with $K$ entries where $Z_{nk}$
indicates the strength of $\theta_k$ used to express $X_n$. In this paper, we
introduce random function priors for $Z_n$ for modeling correlations among its
$K$ dimensions $Z_{n1}$ through $Z_{nK}$, which we call \textit{population
random measure embedding} (PRME). Our model can be viewed as a generalized
paintbox model~\cite{Broderick13} using random functions, and can be learned
efficiently with neural networks via amortized variational inference. We derive
our Bayesian nonparametric method by applying a representation theorem on
separately exchangeable discrete random measures.","['Aonan Zhang', 'John Paisley']","['cs.LG', 'stat.ML']",2019-05-09 19:32:48+00:00
http://arxiv.org/abs/1905.03818v1,Beta Survival Models,"This article analyzes the problem of estimating the time until an event
occurs, also known as survival modeling. We observe through substantial
experiments on large real-world datasets and use-cases that populations are
largely heterogeneous. Sub-populations have different mean and variance in
their survival rates requiring flexible models that capture heterogeneity. We
leverage a classical extension of the logistic function into the survival
setting to characterize unobserved heterogeneity using the beta distribution.
This yields insights into the geometry of the problem as well as efficient
estimation methods for linear, tree and neural network models that adjust the
beta distribution based on observed covariates. We also show that the
additional information captured by the beta distribution leads to interesting
ranking implications as we determine who is most-at-risk. We show theoretically
that the ranking is variable as we forecast forward in time and prove that
pairwise comparisons of survival remain transitive. Empirical results using
large-scale datasets across two use-cases (online conversions and retention
modeling), demonstrate the competitiveness of the method. The simplicity of the
method and its ability to capture skew in the data makes it a viable
alternative to standard techniques particularly when we are interested in the
time to event and when the underlying probabilities are heterogeneous.","['David Hubbard', 'Benoit Rostykus', 'Yves Raimond', 'Tony Jebara']","['cs.LG', 'stat.ML']",2019-05-09 19:10:49+00:00
http://arxiv.org/abs/1905.03814v2,Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs,"This paper establishes that optimistic algorithms attain gap-dependent and
non-asymptotic logarithmic regret for episodic MDPs. In contrast to prior work,
our bounds do not suffer a dependence on diameter-like quantities or
ergodicity, and smoothly interpolate between the gap dependent
logarithmic-regret, and the $\widetilde{\mathcal{O}}(\sqrt{HSAT})$-minimax
rate. The key technique in our analysis is a novel ""clipped"" regret
decomposition which applies to a broad family of recent optimistic algorithms
for episodic MDPs.","['Max Simchowitz', 'Kevin Jamieson']","['cs.LG', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']",2019-05-09 19:00:31+00:00
http://arxiv.org/abs/1905.03806v2,"Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting","Forecasting high-dimensional time series plays a crucial role in many
applications such as demand forecasting and financial predictions. Modern
datasets can have millions of correlated time-series that evolve together, i.e
they are extremely high dimensional (one dimension for each individual
time-series). There is a need for exploiting global patterns and coupling them
with local calibration for better prediction. However, most recent deep
learning approaches in the literature are one-dimensional, i.e, even though
they are trained on the whole dataset, during prediction, the future forecast
for a single dimension mainly depends on past values from the same dimension.
In this paper, we seek to correct this deficiency and propose DeepGLO, a deep
forecasting model which thinks globally and acts locally. In particular,
DeepGLO is a hybrid model that combines a global matrix factorization model
regularized by a temporal convolution network, along with another temporal
network that can capture local properties of each time-series and associated
covariates. Our model can be trained effectively on high-dimensional but
diverse time series, where different time series can have vastly different
scales, without a priori normalization or rescaling. Empirical results
demonstrate that DeepGLO can outperform state-of-the-art approaches; for
example, we see more than 25% improvement in WAPE over other methods on a
public dataset that contains more than 100K-dimensional time series.","['Rajat Sen', 'Hsiang-Fu Yu', 'Inderjit Dhillon']","['stat.ML', 'cs.LG']",2019-05-09 18:24:34+00:00
http://arxiv.org/abs/1905.03776v1,The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study,"We investigate how the final parameters found by stochastic gradient descent
are influenced by over-parameterization. We generate families of models by
increasing the number of channels in a base network, and then perform a large
hyper-parameter search to study how the test error depends on learning rate,
batch size, and network width. We find that the optimal SGD hyper-parameters
are determined by a ""normalized noise scale,"" which is a function of the batch
size, learning rate, and initialization conditions. In the absence of batch
normalization, the optimal normalized noise scale is directly proportional to
width. Wider networks, with their higher optimal noise scale, also achieve
higher test accuracy. These observations hold for MLPs, ConvNets, and ResNets,
and for two different parameterization schemes (""Standard"" and ""NTK""). We
observe a similar trend with batch normalization for ResNets. Surprisingly,
since the largest stable learning rate is bounded, the largest batch size
consistent with the optimal normalized noise scale decreases as the width
increases.","['Daniel S. Park', 'Jascha Sohl-Dickstein', 'Quoc V. Le', 'Samuel L. Smith']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2019-05-09 17:58:13+00:00
http://arxiv.org/abs/1905.03729v1,Best-scored Random Forest Density Estimation,"This paper presents a brand new nonparametric density estimation strategy
named the best-scored random forest density estimation whose effectiveness is
supported by both solid theoretical analysis and significant experimental
performance. The terminology best-scored stands for selecting one density tree
with the best estimation performance out of a certain number of purely random
density tree candidates and we then name the selected one the best-scored
random density tree. In this manner, the ensemble of these selected trees that
is the best-scored random density forest can achieve even better estimation
results than simply integrating trees without selection. From the theoretical
perspective, by decomposing the error term into two, we are able to carry out
the following analysis: First of all, we establish the consistency of the
best-scored random density trees under $L_1$-norm. Secondly, we provide the
convergence rates of them under $L_1$-norm concerning with three different tail
assumptions, respectively. Thirdly, the convergence rates under
$L_{\infty}$-norm is presented. Last but not least, we also achieve the above
convergence rates analysis for the best-scored random density forest. When
conducting comparative experiments with other state-of-the-art density
estimation approaches on both synthetic and real data sets, it turns out that
our algorithm has not only significant advantages in terms of estimation
accuracy over other methods, but also stronger resistance to the curse of
dimensionality.","['Hanyuan Hang', 'Hongwei Wen']","['stat.ML', 'cs.LG']",2019-05-09 16:11:55+00:00
http://arxiv.org/abs/1905.03684v3,Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation,"Existing Rademacher complexity bounds for neural networks rely only on norm
control of the weight matrices and depend exponentially on depth via a product
of the matrix norms. Lower bounds show that this exponential dependence on
depth is unavoidable when no additional properties of the training data are
considered. We suspect that this conundrum comes from the fact that these
bounds depend on the training data only through the margin. In practice, many
data-dependent techniques such as Batchnorm improve the generalization
performance. For feedforward neural nets as well as RNNs, we obtain tighter
Rademacher complexity bounds by considering additional data-dependent
properties of the network: the norms of the hidden layers of the network, and
the norms of the Jacobians of each layer with respect to all previous layers.
Our bounds scale polynomially in depth when these empirical quantities are
small, as is usually the case in practice. To obtain these bounds, we develop
general tools for augmenting a sequence of functions to make their composition
Lipschitz and then covering the augmented functions. Inspired by our theory, we
directly regularize the network's Jacobians during training and empirically
demonstrate that this improves test performance.","['Colin Wei', 'Tengyu Ma']","['cs.LG', 'stat.ML']",2019-05-09 15:18:41+00:00
http://arxiv.org/abs/1905.03680v1,A Bayesian Finite Mixture Model with Variable Selection for Data with Mixed-type Variables,"Finite mixture model is an important branch of clustering methods and can be
applied on data sets with mixed types of variables. However, challenges exist
in its applications. First, it typically relies on the EM algorithm which could
be sensitive to the choice of initial values. Second, biomarkers subject to
limits of detection (LOD) are common to encounter in clinical data, which
brings censored variables into finite mixture model. Additionally, researchers
are recently getting more interest in variable importance due to the increasing
number of variables that become available for clustering.
  To address these challenges, we propose a Bayesian finite mixture model to
simultaneously conduct variable selection, account for biomarker LOD and obtain
clustering results. We took a Bayesian approach to obtain parameter estimates
and the cluster membership to bypass the limitation of the EM algorithm. To
account for LOD, we added one more step in Gibbs sampling to iteratively fill
in biomarker values below or above LODs. In addition, we put a spike-and-slab
type of prior on each variable to obtain variable importance. Simulations
across various scenarios were conducted to examine the performance of this
method. Real data application on electronic health records was also conducted.","['Shu Wang', 'Jonathan G. Yabes', 'Chung-Chou H. Chang']","['stat.ML', 'cs.LG', 'stat.AP']",2019-05-09 15:13:13+00:00
http://arxiv.org/abs/1905.03679v2,Adversarial Defense Framework for Graph Neural Network,"Graph neural network (GNN), as a powerful representation learning model on
graph data, attracts much attention across various disciplines. However, recent
studies show that GNN is vulnerable to adversarial attacks. How to make GNN
more robust? What are the key vulnerabilities in GNN? How to address the
vulnerabilities and defense GNN against the adversarial attacks? In this paper,
we propose DefNet, an effective adversarial defense framework for GNNs. In
particular, we first investigate the latent vulnerabilities in every layer of
GNNs and propose corresponding strategies including dual-stage aggregation and
bottleneck perceptron. Then, to cope with the scarcity of training data, we
propose an adversarial contrastive learning method to train the GNN in a
conditional GAN manner by leveraging the high-level graph representation.
Extensive experiments on three public datasets demonstrate the effectiveness of
DefNet in improving the robustness of popular GNN variants, such as Graph
Convolutional Network and GraphSAGE, under various types of adversarial
attacks.","['Shen Wang', 'Zhengzhang Chen', 'Jingchao Ni', 'Xiao Yu', 'Zhichun Li', 'Haifeng Chen', 'Philip S. Yu']","['cs.LG', 'cs.CR', 'stat.ML']",2019-05-09 15:10:30+00:00
http://arxiv.org/abs/1905.03674v3,Proportionally Fair Clustering,"We extend the fair machine learning literature by considering the problem of
proportional centroid clustering in a metric context. For clustering $n$ points
with $k$ centers, we define fairness as proportionality to mean that any $n/k$
points are entitled to form their own cluster if there is another center that
is closer in distance for all $n/k$ points. We seek clustering solutions to
which there are no such justified complaints from any subsets of agents,
without assuming any a priori notion of protected subsets. We present and
analyze algorithms to efficiently compute, optimize, and audit proportional
solutions. We conclude with an empirical examination of the tradeoff between
proportional solutions and the $k$-means objective.","['Xingyu Chen', 'Brandon Fain', 'Liang Lyu', 'Kamesh Munagala']","['cs.LG', 'cs.DS', 'cs.GT', 'stat.ML']",2019-05-09 14:58:43+00:00
http://arxiv.org/abs/1905.03673v2,Stein Point Markov Chain Monte Carlo,"An important task in machine learning and statistics is the approximation of
a probability measure by an empirical measure supported on a discrete point
set. Stein Points are a class of algorithms for this task, which proceed by
sequentially minimising a Stein discrepancy between the empirical measure and
the target and, hence, require the solution of a non-convex optimisation
problem to obtain each new point. This paper removes the need to solve this
optimisation problem by, instead, selecting each new point based on a Markov
chain sample path. This significantly reduces the computational cost of Stein
Points and leads to a suite of algorithms that are straightforward to
implement. The new algorithms are illustrated on a set of challenging Bayesian
inference problems, and rigorous theoretical guarantees of consistency are
established.","['Wilson Ye Chen', 'Alessandro Barp', 'François-Xavier Briol', 'Jackson Gorham', 'Mark Girolami', 'Lester Mackey', 'Chris. J. Oates']","['stat.CO', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2019-05-09 14:57:02+00:00
http://arxiv.org/abs/1905.03658v3,Improving Discrete Latent Representations With Differentiable Approximation Bridges,"Modern neural network training relies on piece-wise (sub-)differentiable
functions in order to use backpropagation to update model parameters. In this
work, we introduce a novel method to allow simple non-differentiable functions
at intermediary layers of deep neural networks. We do so by training with a
differentiable approximation bridge (DAB) neural network which approximates the
non-differentiable forward function and provides gradient updates during
backpropagation. We present strong empirical results (performing over 600
experiments) in four different domains: unsupervised (image) representation
learning, variational (image) density estimation, image classification, and
sequence sorting to demonstrate that our proposed method improves state of the
art performance. We demonstrate that training with DAB aided discrete
non-differentiable functions improves image reconstruction quality and
posterior linear separability by 10% against the Gumbel-Softmax relaxed
estimator [37, 26] as well as providing a 9% improvement in the test
variational lower bound in comparison to the state of the art RELAX [16]
discrete estimator. We also observe an accuracy improvement of 77% in neural
sequence sorting and a 25% improvement against the straight-through estimator
[5] in an image classification setting. The DAB network is not used for
inference and expands the class of functions that are usable in neural
networks.","['Jason Ramapuram', 'Russ Webb']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2019-05-09 14:31:59+00:00
http://arxiv.org/abs/1905.03652v1,Stochastic Iterative Hard Thresholding for Graph-structured Sparsity Optimization,"Stochastic optimization algorithms update models with cheap per-iteration
costs sequentially, which makes them amenable for large-scale data analysis.
Such algorithms have been widely studied for structured sparse models where the
sparsity information is very specific, e.g., convex sparsity-inducing norms or
$\ell^0$-norm. However, these norms cannot be directly applied to the problem
of complex (non-convex) graph-structured sparsity models, which have important
application in disease outbreak and social networks, etc. In this paper, we
propose a stochastic gradient-based method for solving graph-structured
sparsity constraint problems, not restricted to the least square loss. We prove
that our algorithm enjoys a linear convergence up to a constant error, which is
competitive with the counterparts in the batch learning setting. We conduct
extensive experiments to show the efficiency and effectiveness of the proposed
algorithms.","['Baojian Zhou', 'Feng Chen', 'Yiming Ying']","['cs.LG', 'cs.AI', 'stat.ML', '46N10', 'I.2']",2019-05-09 14:24:43+00:00
http://arxiv.org/abs/1906.00951v1,User Traffic Prediction for Proactive Resource Management: Learning-Powered Approaches,"Traffic prediction plays a vital role in efficient planning and usage of
network resources in wireless networks. While traffic prediction in wired
networks is an established field, there is a lack of research on the analysis
of traffic in cellular networks, especially in a content-blind manner at the
user level. Here, we shed light into this problem by designing traffic
prediction tools that employ either statistical, rule-based, or deep machine
learning methods. First, we present an extensive experimental evaluation of the
designed tools over a real traffic dataset. Within this analysis, the impact of
different parameters, such as length of prediction, feature set used in
analyses, and granularity of data, on accuracy of prediction are investigated.
Second, regarding the coupling observed between behavior of traffic and its
generating application, we extend our analysis to the blind classification of
applications generating the traffic based on the statistics of traffic
arrival/departure. The results demonstrate presence of a threshold number of
previous observations, beyond which, deep machine learning can outperform
linear statistical learning, and before which, statistical learning outperforms
deep learning approaches. Further analysis of this threshold value represents a
strong coupling between this threshold, the length of future prediction, and
the feature set in use. Finally, through a case study, we present how the
experienced delay could be decreased by traffic arrival prediction.","['Amin Azari', 'Panagiotis Papapetrou', 'Stojan Denic', 'Gunnar Peters']","['cs.NI', 'cs.LG', 'stat.ML']",2019-05-09 13:56:51+00:00
http://arxiv.org/abs/1905.03546v1,A Novel Adaptive Kernel for the RBF Neural Networks,"In this paper, we propose a novel adaptive kernel for the radial basis
function (RBF) neural networks. The proposed kernel adaptively fuses the
Euclidean and cosine distance measures to exploit the reciprocating properties
of the two. The proposed framework dynamically adapts the weights of the
participating kernels using the gradient descent method thereby alleviating the
need for predetermined weights. The proposed method is shown to outperform the
manual fusion of the kernels on three major problems of estimation namely
nonlinear system identification, pattern classification and function
approximation.","['Shujaat Khan', 'Imran Naseem', 'Roberto Togneri', 'Mohammed Bennamoun']","['stat.ML', 'cs.CV', 'cs.LG', 'math.OC']",2019-05-09 11:38:57+00:00
http://arxiv.org/abs/1905.03501v1,Pretrain Soft Q-Learning with Imperfect Demonstrations,"Pretraining reinforcement learning methods with demonstrations has been an
important concept in the study of reinforcement learning since a large amount
of computing power is spent on online simulations with existing reinforcement
learning algorithms. Pretraining reinforcement learning remains a significant
challenge in exploiting expert demonstrations whilst keeping exploration
potentials, especially for value based methods. In this paper, we propose a
pretraining method for soft Q-learning. Our work is inspired by pretraining
methods for actor-critic algorithms since soft Q-learning is a value based
algorithm that is equivalent to policy gradient. The proposed method is based
on $\gamma$-discounted biased policy evaluation with entropy regularization,
which is also the updating target of soft Q-learning. Our method is evaluated
on various tasks from Atari 2600. Experiments show that our method effectively
learns from imperfect demonstrations, and outperforms other state-of-the-art
methods that learn from expert demonstrations.","['Xiaoqin Zhang', 'Yunfei Li', 'Huimin Ma', 'Xiong Luo']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-09 09:23:53+00:00
http://arxiv.org/abs/1905.03493v1,Limits of Deepfake Detection: A Robust Estimation Viewpoint,"Deepfake detection is formulated as a hypothesis testing problem to classify
an image as genuine or GAN-generated. A robust statistics view of GANs is
considered to bound the error probability for various GAN implementations in
terms of their performance. The bounds are further simplified using a Euclidean
approximation for the low error regime. Lastly, relationships between error
probability and epidemic thresholds for spreading processes in networks are
established.","['Sakshi Agarwal', 'Lav R. Varshney']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",2019-05-09 09:01:08+00:00
http://arxiv.org/abs/1905.03454v1,Bidirectional RNN-based Few-shot Training for Detecting Multi-stage Attack,"""Feint Attack"", as a new type of APT attack, has become the focus of
attention. It adopts a multi-stage attacks mode which can be concluded as a
combination of virtual attacks and real attacks. Under the cover of virtual
attacks, real attacks can achieve the real purpose of the attacker, as a
result, it often caused huge losses inadvertently. However, to our knowledge,
all previous works use common methods such as Causal-Correlation or Cased-based
to detect outdated multi-stage attacks. Few attentions have been paid to detect
the ""Feint Attack"", because the difficulty of detection lies in the
diversification of the concept of ""Feint Attack"" and the lack of professional
datasets, many detection methods ignore the semantic relationship in the
attack. Aiming at the existing challenge, this paper explores a new method to
solve the problem. In the attack scenario, the fuzzy clustering method based on
attribute similarity is used to mine multi-stage attack chains. Then we use a
few-shot deep learning algorithm (SMOTE&CNN-SVM) and bidirectional Recurrent
Neural Network model (Bi-RNN) to obtain the ""Feint Attack"" chains. ""Feint
Attack"" is simulated by the real attack inserted in the normal causal attack
chain, and the addition of the real attack destroys the causal relationship of
the original attack chain. So we used Bi-RNN coding to obtain the hidden
feature of ""Feint Attack"" chain. In the end, our method achieved the goal to
detect the ""Feint Attack"" accurately by using the LLDoS1.0 and LLDoS2.0 of
DARPA2000 and CICIDS2017 of Canadian Institute for Cybersecurity.","['Di Zhao', 'Jiqiang Liu', 'Jialin Wang', 'Wenjia Niu', 'Endong Tong', 'Tong Chen', 'Gang Li']","['cs.CR', 'cs.LG', 'stat.ML']",2019-05-09 06:38:12+00:00
http://arxiv.org/abs/1905.03438v1,Two-stage Best-scored Random Forest for Large-scale Regression,"We propose a novel method designed for large-scale regression problems,
namely the two-stage best-scored random forest (TBRF). ""Best-scored"" means to
select one regression tree with the best empirical performance out of a certain
number of purely random regression tree candidates, and ""two-stage"" means to
divide the original random tree splitting procedure into two: In stage one, the
feature space is partitioned into non-overlapping cells; in stage two, child
trees grow separately on these cells. The strengths of this algorithm can be
summarized as follows: First of all, the pure randomness in TBRF leads to the
almost optimal learning rates, and also makes ensemble learning possible, which
resolves the boundary discontinuities long plaguing the existing algorithms.
Secondly, the two-stage procedure paves the way for parallel computing, leading
to computational efficiency. Last but not least, TBRF can serve as an inclusive
framework where different mainstream regression strategies such as linear
predictor and least squares support vector machines (LS-SVMs) can also be
incorporated as value assignment approaches on leaves of the child trees,
depending on the characteristics of the underlying data sets. Numerical
assessments on comparisons with other state-of-the-art methods on several
large-scale real data sets validate the promising prediction accuracy and high
computational efficiency of our algorithm.","['Hanyuan Hang', 'Yingyi Chen', 'Johan A. K. Suykens']","['stat.ML', 'cs.LG']",2019-05-09 04:20:48+00:00
http://arxiv.org/abs/1905.03421v3,Adversarial Image Translation: Unrestricted Adversarial Examples in Face Recognition Systems,"Thanks to recent advances in deep neural networks (DNNs), face recognition
systems have become highly accurate in classifying a large number of face
images. However, recent studies have found that DNNs could be vulnerable to
adversarial examples, raising concerns about the robustness of such systems.
Adversarial examples that are not restricted to small perturbations could be
more serious since conventional certified defenses might be ineffective against
them. To shed light on the vulnerability to such adversarial examples, we
propose a flexible and efficient method for generating unrestricted adversarial
examples using image translation techniques. Our method enables us to translate
a source image into any desired facial appearance with large perturbations to
deceive target face recognition systems. Our experimental results indicate that
our method achieved about $90$ and $80\%$ attack success rates under white- and
black-box settings, respectively, and that the translated images are
perceptually realistic and maintain the identifiability of the individual while
the perturbations are large enough to bypass certified defenses.","['Kazuya Kakizaki', 'Kosuke Yoshida']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']",2019-05-09 02:58:45+00:00
http://arxiv.org/abs/1905.03410v4,Learning Erdős-Rényi Random Graphs via Edge Detecting Queries,"In this paper, we consider the problem of learning an unknown graph via
queries on groups of nodes, with the result indicating whether or not at least
one edge is present among those nodes. While learning arbitrary graphs with $n$
nodes and $k$ edges is known to be hard in the sense of requiring $\Omega(
\min\{ k^2 \log n, n^2\})$ tests (even when a small probability of error is
allowed), we show that learning an Erd\H{o}s-R\'enyi random graph with an
average of $\bar{k}$ edges is much easier; namely, one can attain
asymptotically vanishing error probability with only $O(\bar{k}\log n)$ tests.
We establish such bounds for a variety of algorithms inspired by the group
testing problem, with explicit constant factors indicating a near-optimal
number of tests, and in some cases asymptotic optimality including constant
factors. In addition, we present an alternative design that permits a
near-optimal sublinear decoding time of $O(\bar{k} \log^2 \bar{k} + \bar{k}
\log n)$.","['Zihan Li', 'Matthias Fresacher', 'Jonathan Scarlett']","['cs.IT', 'cs.DM', 'cs.LG', 'math.IT', 'math.PR', 'stat.ML']",2019-05-09 02:10:17+00:00
http://arxiv.org/abs/1905.03406v1,Multi-fidelity classification using Gaussian processes: accelerating the prediction of large-scale computational models,"Machine learning techniques typically rely on large datasets to create
accurate classifiers. However, there are situations when data is scarce and
expensive to acquire. This is the case of studies that rely on state-of-the-art
computational models which typically take days to run, thus hindering the
potential of machine learning tools. In this work, we present a novel
classifier that takes advantage of lower fidelity models and inexpensive
approximations to predict the binary output of expensive computer simulations.
We postulate an autoregressive model between the different levels of fidelity
with Gaussian process priors. We adopt a fully Bayesian treatment for the
hyper-parameters and use Markov Chain Mont Carlo samplers. We take advantage of
the probabilistic nature of the classifier to implement active learning
strategies. We also introduce a sparse approximation to enhance the ability of
themulti-fidelity classifier to handle large datasets. We test these
multi-fidelity classifiers against their single-fidelity counterpart with
synthetic data, showing a median computational cost reduction of 23% for a
target accuracy of 90%. In an application to cardiac electrophysiology, the
multi-fidelity classifier achieves an F1 score, the harmonic mean of precision
and recall, of 99.6% compared to 74.1% of a single-fidelity classifier when
both are trained with 50 samples. In general, our results show that the
multi-fidelity classifiers outperform their single-fidelity counterpart in
terms of accuracy in all cases. We envision that this new tool will enable
researchers to study classification problems that would otherwise be
prohibitively expensive. Source code is available at
https://github.com/fsahli/MFclass.","['Francisco Sahli Costabal', 'Paris Perdikaris', 'Ellen Kuhl', 'Daniel E. Hurtado']","['cs.LG', 'stat.ML']",2019-05-09 01:52:47+00:00
http://arxiv.org/abs/1905.03389v1,Learning to Evolve,"Evolution and learning are two of the fundamental mechanisms by which life
adapts in order to survive and to transcend limitations. These biological
phenomena inspired successful computational methods such as evolutionary
algorithms and deep learning. Evolution relies on random mutations and on
random genetic recombination. Here we show that learning to evolve, i.e.
learning to mutate and recombine better than at random, improves the result of
evolution in terms of fitness increase per generation and even in terms of
attainable fitness. We use deep reinforcement learning to learn to dynamically
adjust the strategy of evolutionary algorithms to varying circumstances. Our
methods outperform classical evolutionary algorithms on combinatorial and
continuous optimization problems.","['Jan Schuchardt', 'Vladimir Golkov', 'Daniel Cremers']","['cs.NE', 'cs.AI', 'cs.CV', 'cs.LG', 'stat.ML', '62M45, 68T05, 68W25, 68T20, 90C40, 91A22, 92D15, 92D25', 'G.1.6; I.2.6; I.2.8; G.3; I.5.1']",2019-05-08 23:35:02+00:00
http://arxiv.org/abs/1905.03381v1,AutoAssist: A Framework to Accelerate Training of Deep Neural Networks,"Deep neural networks have yielded superior performance in many applications;
however, the gradient computation in a deep model with millions of instances
lead to a lengthy training process even with modern GPU/TPU hardware
acceleration. In this paper, we propose AutoAssist, a simple framework to
accelerate training of a deep neural network. Typically, as the training
procedure evolves, the amount of improvement in the current model by a
stochastic gradient update on each instance varies dynamically. In AutoAssist,
we utilize this fact and design a simple instance shrinking operation, which is
used to filter out instances with relatively low marginal improvement to the
current model; thus the computationally intensive gradient computations are
performed on informative instances as much as possible. We prove that the
proposed technique outperforms vanilla SGD with existing importance sampling
approaches for linear SVM problems, and establish an O(1/k) convergence for
strongly convex problems. In order to apply the proposed techniques to
accelerate training of deep models, we propose to jointly train a very
lightweight Assistant network in addition to the original deep network referred
to as Boss. The Assistant network is designed to gauge the importance of a
given instance with respect to the current Boss such that a shrinking operation
can be applied in the batch generator. With careful design, we train the Boss
and Assistant in a nonblocking and asynchronous fashion such that overhead is
minimal. We demonstrate that AutoAssist reduces the number of epochs by 40% for
training a ResNet to reach the same test accuracy on an image classification
data set and saves 30% training time needed for a transformer model to yield
the same BLEU scores on a translation dataset.","['Jiong Zhang', 'Hsiang-fu Yu', 'Inderjit S. Dhillon']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-08 22:36:37+00:00
http://arxiv.org/abs/1905.03375v1,Embarrassingly Shallow Autoencoders for Sparse Data,"Combining simple elements from the literature, we define a linear model that
is geared toward sparse data, in particular implicit feedback data for
recommender systems. We show that its training objective has a closed-form
solution, and discuss the resulting conceptual insights. Surprisingly, this
simple model achieves better ranking accuracy than various state-of-the-art
collaborative-filtering approaches, including deep non-linear models, on most
of the publicly available data-sets used in our experiments.",['Harald Steck'],"['cs.IR', 'cs.LG', 'stat.ML']",2019-05-08 22:16:59+00:00
http://arxiv.org/abs/1905.03356v2,QSMGAN: Improved Quantitative Susceptibility Mapping using 3D Generative Adversarial Networks with Increased Receptive Field,"Quantitative susceptibility mapping (QSM) is a powerful MRI technique that
has shown great potential in quantifying tissue susceptibility in numerous
neurological disorders. However, the intrinsic ill-posed dipole inversion
problem greatly affects the accuracy of the susceptibility map. We propose
QSMGAN: a 3D deep convolutional neural network approach based on a 3D U-Net
architecture with increased receptive field of the input phase compared to the
output and further refined the network using the WGAN with gradient penalty
training strategy. Our method generates accurate QSM maps from single
orientation phase maps efficiently and performs significantly better than
traditional non-learning-based dipole inversion algorithms. The generalization
capability was verified by applying the algorithm to an unseen pathology--brain
tumor patients with radiation-induced cerebral microbleeds.","['Yicheng Chen', 'Angela Jakary', 'Sivakami Avadiappan', 'Christopher P. Hess', 'Janine M. Lupo']","['eess.IV', 'cs.LG', 'stat.ML']",2019-05-08 21:19:02+00:00
http://arxiv.org/abs/1905.03353v2,Regression from Dependent Observations,"The standard linear and logistic regression models assume that the response
variables are independent, but share the same linear relationship to their
corresponding vectors of covariates. The assumption that the response variables
are independent is, however, too strong. In many applications, these responses
are collected on nodes of a network, or some spatial or temporal domain, and
are dependent. Examples abound in financial and meteorological applications,
and dependencies naturally arise in social networks through peer effects.
Regression with dependent responses has thus received a lot of attention in the
Statistics and Economics literature, but there are no strong consistency
results unless multiple independent samples of the vectors of dependent
responses can be collected from these models. We present computationally and
statistically efficient methods for linear and logistic regression models when
the response variables are dependent on a network. Given one sample from a
networked linear or logistic regression model and under mild assumptions, we
prove strong consistency results for recovering the vector of coefficients and
the strength of the dependencies, recovering the rates of standard regression
under independent observations. We use projected gradient descent on the
negative log-likelihood, or negative log-pseudolikelihood, and establish their
strong convexity and consistency using concentration of measure for dependent
random variables.","['Constantinos Daskalakis', 'Nishanth Dikkala', 'Ioannis Panageas']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2019-05-08 21:11:50+00:00
http://arxiv.org/abs/1905.03330v2,Universal Sound Separation,"Recent deep learning approaches have achieved impressive performance on
speech enhancement and separation tasks. However, these approaches have not
been investigated for separating mixtures of arbitrary sounds of different
types, a task we refer to as universal sound separation, and it is unknown how
performance on speech tasks carries over to non-speech tasks. To study this
question, we develop a dataset of mixtures containing arbitrary sounds, and use
it to investigate the space of mask-based separation architectures, varying
both the overall network architecture and the framewise analysis-synthesis
basis for signal transformations. These network architectures include
convolutional long short-term memory networks and time-dilated convolution
stacks inspired by the recent success of time-domain enhancement networks like
ConvTasNet. For the latter architecture, we also propose novel modifications
that further improve separation performance. In terms of the framewise
analysis-synthesis basis, we explore both a short-time Fourier transform (STFT)
and a learnable basis, as used in ConvTasNet. For both of these bases, we also
examine the effect of window size. In particular, for STFTs, we find that
longer windows (25-50 ms) work best for speech/non-speech separation, while
shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases,
shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal
sound separation, STFTs outperform learnable bases. Our best methods produce an
improvement in scale-invariant signal-to-distortion ratio of over 13 dB for
speech/non-speech separation and close to 10 dB for universal sound separation.","['Ilya Kavalerov', 'Scott Wisdom', 'Hakan Erdogan', 'Brian Patton', 'Kevin Wilson', 'Jonathan Le Roux', 'John R. Hershey']","['cs.SD', 'cs.LG', 'eess.AS', 'stat.ML']",2019-05-08 20:48:49+00:00
http://arxiv.org/abs/1905.03329v1,Learning Embeddings into Entropic Wasserstein Spaces,"Euclidean embeddings of data are fundamentally limited in their ability to
capture latent semantic structures, which need not conform to Euclidean spatial
assumptions. Here we consider an alternative, which embeds data as discrete
probability distributions in a Wasserstein space, endowed with an optimal
transport metric. Wasserstein spaces are much larger and more flexible than
Euclidean spaces, in that they can successfully embed a wider variety of metric
structures. We exploit this flexibility by learning an embedding that captures
semantic information in the Wasserstein distance between embedded
distributions. We examine empirically the representational capacity of our
learned Wasserstein embeddings, showing that they can embed a wide variety of
metric structures with smaller distortion than an equivalent Euclidean
embedding. We also investigate an application to word embedding, demonstrating
a unique advantage of Wasserstein embeddings: We can visualize the
high-dimensional embedding directly, since it is a probability distribution on
a low-dimensional space. This obviates the need for dimensionality reduction
techniques like t-SNE for visualization.","['Charlie Frogner', 'Farzaneh Mirzazadeh', 'Justin Solomon']","['cs.LG', 'stat.ML']",2019-05-08 20:48:28+00:00
http://arxiv.org/abs/1905.06115v1,Naive Bayes with Correlation Factor for Text Classification Problem,"Naive Bayes estimator is widely used in text classification problems.
However, it doesn't perform well with small-size training dataset. We propose a
new method based on Naive Bayes estimator to solve this problem. A correlation
factor is introduced to incorporate the correlation among different classes.
Experimental results show that our estimator achieves a better accuracy
compared with traditional Naive Bayes in real world data.","['Jiangning Chen', 'Zhibo Dai', 'Juntao Duan', 'Heinrich Matzinger', 'Ionel Popescu']","['cs.IR', 'cs.LG', 'stat.ML']",2019-05-08 20:27:00+00:00
http://arxiv.org/abs/1905.03319v2,Data-Efficient Mutual Information Neural Estimator,"Measuring Mutual Information (MI) between high-dimensional, continuous,
random variables from observed samples has wide theoretical and practical
applications. Recent work, MINE (Belghazi et al. 2018), focused on estimating
tight variational lower bounds of MI using neural networks, but assumed
unlimited supply of samples to prevent overfitting. In real world applications,
data is not always available at a surplus. In this work, we focus on improving
data efficiency and propose a Data-Efficient MINE Estimator (DEMINE), by
developing a relaxed predictive MI lower bound that can be estimated at higher
data efficiency by orders of magnitudes. The predictive MI lower bound also
enables us to develop a new meta-learning approach using task augmentation,
Meta-DEMINE, to improve generalization of the network and further boost
estimation accuracy empirically. With improved data-efficiency, our estimators
enables statistical testing of dependency at practical dataset sizes. We
demonstrate the effectiveness of our estimators on synthetic benchmarks and a
real world fMRI data, with application of inter-subject correlation analysis.","['Xiao Lin', 'Indranil Sur', 'Samuel A. Nastase', 'Ajay Divakaran', 'Uri Hasson', 'Mohamed R. Amer']","['cs.LG', 'stat.ML']",2019-05-08 20:22:13+00:00
http://arxiv.org/abs/1905.03302v2,PerceptNet: Learning Perceptual Similarity of Haptic Textures in Presence of Unorderable Triplets,"In order to design haptic icons or build a haptic vocabulary, we require a
set of easily distinguishable haptic signals to avoid perceptual ambiguity,
which in turn requires a way to accurately estimate the perceptual
(dis)similarity of such signals. In this work, we present a novel method to
learn such a perceptual metric based on data from human studies. Our method is
based on a deep neural network that projects signals to an embedding space
where the natural Euclidean distance accurately models the degree of
dissimilarity between two signals. The network is trained only on non-numerical
comparisons of triplets of signals, using a novel triplet loss that considers
both types of triplets that are easy to order (inequality constraints), as well
as those that are unorderable/ambiguous (equality constraints). Unlike prior
MDS-based non-parametric approaches, our method can be trained on a partial set
of comparisons and can embed new haptic signals without retraining the model
from scratch. Extensive experimental evaluations show that our method is
significantly more effective at modeling perceptual dissimilarity than
alternatives.","['Priyadarshini Kumari', 'Siddhartha Chaudhuri', 'Subhasis Chaudhuri']","['cs.LG', 'cs.HC', 'stat.ML']",2019-05-08 19:06:39+00:00
http://arxiv.org/abs/1905.03297v3,Interpretable Subgroup Discovery in Treatment Effect Estimation with Application to Opioid Prescribing Guidelines,"The dearth of prescribing guidelines for physicians is one key driver of the
current opioid epidemic in the United States. In this work, we analyze medical
and pharmaceutical claims data to draw insights on characteristics of patients
who are more prone to adverse outcomes after an initial synthetic opioid
prescription. Toward this end, we propose a generative model that allows
discovery from observational data of subgroups that demonstrate an enhanced or
diminished causal effect due to treatment. Our approach models these
sub-populations as a mixture distribution, using sparsity to enhance
interpretability, while jointly learning nonlinear predictors of the potential
outcomes to better adjust for confounding. The approach leads to
human-interpretable insights on discovered subgroups, improving the practical
utility for decision support","['Chirag Nagpal', 'Dennis Wei', 'Bhanukiran Vinzamuri', 'Monica Shekhar', 'Sara E. Berger', 'Subhro Das', 'Kush R. Varshney']","['cs.LG', 'stat.ML']",2019-05-08 19:00:09+00:00
http://arxiv.org/abs/1905.03290v1,Importance Weighted Hierarchical Variational Inference,"Variational Inference is a powerful tool in the Bayesian modeling toolkit,
however, its effectiveness is determined by the expressivity of the utilized
variational distributions in terms of their ability to match the true posterior
distribution. In turn, the expressivity of the variational family is largely
limited by the requirement of having a tractable density function. To overcome
this roadblock, we introduce a new family of variational upper bounds on a
marginal log density in the case of hierarchical models (also known as latent
variable models). We then give an upper bound on the Kullback-Leibler
divergence and derive a family of increasingly tighter variational lower bounds
on the otherwise intractable standard evidence lower bound for hierarchical
variational distributions, enabling the use of more expressive approximate
posteriors. We show that previously known methods, such as Hierarchical
Variational Models, Semi-Implicit Variational Inference and Doubly
Semi-Implicit Variational Inference can be seen as special cases of the
proposed approach, and empirically demonstrate superior performance of the
proposed method in a set of experiments.","['Artem Sobolev', 'Dmitry Vetrov']","['stat.ML', 'cs.LG']",2019-05-08 18:38:51+00:00
http://arxiv.org/abs/1905.03282v1,Reconstruction of Privacy-Sensitive Data from Protected Templates,"In this paper, we address the problem of data reconstruction from
privacy-protected templates, based on recent concept of sparse ternary coding
with ambiguization (STCA). The STCA is a generalization of randomization
techniques which includes random projections, lossy quantization, and addition
of ambiguization noise to satisfy the privacy-utility trade-off requirements.
The theoretical privacy-preserving properties of STCA have been validated on
synthetic data. However, the applicability of STCA to real data and potential
threats linked to reconstruction based on recent deep reconstruction algorithms
are still open problems. Our results demonstrate that STCA still achieves the
claimed theoretical performance when facing deep reconstruction attacks for the
synthetic i.i.d. data, while for real images special measures are required to
guarantee proper protection of the templates.","['Shideh Rezaeifar', 'Behrooz Razeghi', 'Olga Taran', 'Taras Holotyak', 'Slava Voloshynovskiy']","['cs.LG', 'cs.CR', 'cs.IR', 'stat.ML']",2019-05-08 18:17:01+00:00
http://arxiv.org/abs/1905.03239v1,Generative Model with Dynamic Linear Flow,"Flow-based generative models are a family of exact log-likelihood models with
tractable sampling and latent-variable inference, hence conceptually attractive
for modeling complex distributions. However, flow-based models are limited by
density estimation performance issues as compared to state-of-the-art
autoregressive models. Autoregressive models, which also belong to the family
of likelihood-based methods, however suffer from limited parallelizability. In
this paper, we propose Dynamic Linear Flow (DLF), a new family of invertible
transformations with partially autoregressive structure. Our method benefits
from the efficient computation of flow-based methods and high density
estimation performance of autoregressive methods. We demonstrate that the
proposed DLF yields state-of-theart performance on ImageNet 32x32 and 64x64 out
of all flow-based methods, and is competitive with the best autoregressive
model. Additionally, our model converges 10 times faster than Glow (Kingma and
Dhariwal, 2018). The code is available at https://github.com/naturomics/DLF.","['Huadong Liao', 'Jiawei He', 'Kunxian Shu']","['cs.LG', 'stat.ML']",2019-05-08 17:51:11+00:00
http://arxiv.org/abs/1905.03231v2,Smoothing Policies and Safe Policy Gradients,"Policy Gradient (PG) algorithms are among the best candidates for the
much-anticipated applications of reinforcement learning to real-world control
tasks, such as robotics. However, the trial-and-error nature of these methods
poses safety issues whenever the learning process itself must be performed on a
physical system or involves any form of human-computer interaction. In this
paper, we address a specific safety formulation, where both goals and dangers
are encoded in a scalar reward signal and the learning agent is constrained to
never worsen its performance, measured as the expected sum of rewards. By
studying actor-only policy gradient from a stochastic optimization perspective,
we establish improvement guarantees for a wide class of parametric policies,
generalizing existing results on Gaussian policies. This, together with novel
upper bounds on the variance of policy gradient estimators, allows us to
identify meta-parameter schedules that guarantee monotonic improvement with
high probability. The two key meta-parameters are the step size of the
parameter updates and the batch size of the gradient estimates. Through a
joint, adaptive selection of these meta-parameters, we obtain a policy gradient
algorithm with monotonic improvement guarantees.","['Matteo Papini', 'Matteo Pirotta', 'Marcello Restelli']","['cs.LG', 'stat.ML']",2019-05-08 17:40:46+00:00
http://arxiv.org/abs/1905.03222v1,Conformalized Quantile Regression,"Conformal prediction is a technique for constructing prediction intervals
that attain valid coverage in finite samples, without making distributional
assumptions. Despite this appeal, existing conformal methods can be
unnecessarily conservative because they form intervals of constant or weakly
varying length across the input space. In this paper we propose a new method
that is fully adaptive to heteroscedasticity. It combines conformal prediction
with classical quantile regression, inheriting the advantages of both. We
establish a theoretical guarantee of valid coverage, supplemented by extensive
experiments on popular regression datasets. We compare the efficiency of
conformalized quantile regression to other conformal methods, showing that our
method tends to produce shorter intervals.","['Yaniv Romano', 'Evan Patterson', 'Emmanuel J. Candès']","['stat.ME', 'stat.ML']",2019-05-08 17:21:11+00:00
http://arxiv.org/abs/1905.03218v1,MetaPred: Meta-Learning for Clinical Risk Prediction with Limited Patient Electronic Health Records,"In recent years, increasingly augmentation of health data, such as patient
Electronic Health Records (EHR), are becoming readily available. This provides
an unprecedented opportunity for knowledge discovery and data mining algorithms
to dig insights from them, which can, later on, be helpful to the improvement
of the quality of care delivery. Predictive modeling of clinical risk,
including in-hospital mortality, hospital readmission, chronic disease onset,
condition exacerbation, etc., from patient EHR, is one of the health data
analytic problems that attract most of the interests. The reason is not only
because the problem is important in clinical settings, but also there are
challenges working with EHR such as sparsity, irregularity, temporality, etc.
Different from applications in other domains such as computer vision and
natural language processing, the labeled data samples in medicine (patients)
are relatively limited, which creates lots of troubles for effective predictive
model learning, especially for complicated models such as deep learning. In
this paper, we propose MetaPred, a meta-learning for clinical risk prediction
from longitudinal patient EHRs. In particular, in order to predict the target
risk where there are limited data samples, we train a meta-learner from a set
of related risk prediction tasks which learns how a good predictor is learned.
The meta-learned can then be directly used in target risk prediction, and the
limited available samples can be used for further fine-tuning the model
performance. The effectiveness of MetaPred is tested on a real patient EHR
repository from Oregon Health & Science University. We are able to demonstrate
that with CNN and RNN as base predictors, MetaPred can achieve much better
performance for predicting target risk with low resources comparing with the
predictor trained on the limited samples available for this risk.","['Xi Sheryl Zhang', 'Fengyi Tang', 'Hiroko Dodge', 'Jiayu Zhou', 'Fei Wang']","['cs.LG', 'stat.ML']",2019-05-08 17:07:51+00:00
http://arxiv.org/abs/1905.03177v1,Does Data Augmentation Lead to Positive Margin?,"Data augmentation (DA) is commonly used during model training, as it
significantly improves test error and model robustness. DA artificially expands
the training set by applying random noise, rotations, crops, or even
adversarial perturbations to the input data. Although DA is widely used, its
capacity to provably improve robustness is not fully understood. In this work,
we analyze the robustness that DA begets by quantifying the margin that DA
enforces on empirical risk minimizers. We first focus on linear separators, and
then a class of nonlinear models whose labeling is constant within small convex
hulls of data points. We present lower bounds on the number of augmented data
points required for non-zero margin, and show that commonly used DA techniques
may only introduce significant margin after adding exponentially many points to
the data set.","['Shashank Rajput', 'Zhili Feng', 'Zachary Charles', 'Po-Ling Loh', 'Dimitris Papailiopoulos']","['cs.LG', 'stat.ML']",2019-05-08 16:03:06+00:00
http://arxiv.org/abs/1905.03135v2,Optimal Statistical Rates for Decentralised Non-Parametric Regression with Linear Speed-Up,"We analyse the learning performance of Distributed Gradient Descent in the
context of multi-agent decentralised non-parametric regression with the square
loss function when i.i.d. samples are assigned to agents. We show that if
agents hold sufficiently many samples with respect to the network size, then
Distributed Gradient Descent achieves optimal statistical rates with a number
of iterations that scales, up to a threshold, with the inverse of the spectral
gap of the gossip matrix divided by the number of samples owned by each agent
raised to a problem-dependent power. The presence of the threshold comes from
statistics. It encodes the existence of a ""big data"" regime where the number of
required iterations does not depend on the network topology. In this regime,
Distributed Gradient Descent achieves optimal statistical rates with the same
order of iterations as gradient descent run with all the samples in the
network. Provided the communication delay is sufficiently small, the
distributed protocol yields a linear speed-up in runtime compared to the
single-machine protocol. This is in contrast to decentralised optimisation
algorithms that do not exploit statistics and only yield a linear speed-up in
graphs where the spectral gap is bounded away from zero. Our results exploit
the statistical concentration of quantities held by agents and shed new light
on the interplay between statistics and communication in decentralised methods.
Bounds are given in the standard non-parametric setting with source/capacity
assumptions.","['Dominic Richards', 'Patrick Rebeschini']","['stat.ML', 'cs.DC', 'cs.LG', 'math.OC']",2019-05-08 15:08:28+00:00
http://arxiv.org/abs/1905.03125v4,Batch-Size Independent Regret Bounds for the Combinatorial Multi-Armed Bandit Problem,"We consider the combinatorial multi-armed bandit (CMAB) problem, where the
reward function is nonlinear. In this setting, the agent chooses a batch of
arms on each round and receives feedback from each arm of the batch. The reward
that the agent aims to maximize is a function of the selected arms and their
expectations. In many applications, the reward function is highly nonlinear,
and the performance of existing algorithms relies on a global Lipschitz
constant to encapsulate the function's nonlinearity. This may lead to loose
regret bounds, since by itself, a large gradient does not necessarily cause a
large regret, but only in regions where the uncertainty in the reward's
parameters is high. To overcome this problem, we introduce a new smoothness
criterion, which we term \emph{Gini-weighted smoothness}, that takes into
account both the nonlinearity of the reward and concentration properties of the
arms. We show that a linear dependence of the regret in the batch size in
existing algorithms can be replaced by this smoothness parameter. This, in
turn, leads to much tighter regret bounds when the smoothness parameter is
batch-size independent. For example, in the probabilistic maximum coverage
(PMC) problem, that has many applications, including influence maximization,
diverse recommendations and more, we achieve dramatic improvements in the upper
bounds. We also prove matching lower bounds for the PMC problem and show that
our algorithm is tight, up to a logarithmic factor in the problem's parameters.","['Nadav Merlis', 'Shie Mannor']","['cs.LG', 'stat.ML']",2019-05-08 14:58:24+00:00
http://arxiv.org/abs/1905.03100v1,Unsupervised Learning through Temporal Smoothing and Entropy Maximization,"This paper proposes a method for machine learning from unlabeled data in the
form of a time-series. The mapping that is learned is shown to extract slowly
evolving information that would be useful for control applications, while
efficiently filtering out unwanted, higher-frequency noise.
  The method consists of training a feedforward artificial neural network with
backpropagation using two opposing objectives.
  The first of these is to minimize the squared changes in activations between
time steps of each unit in the network. This ""temporal smoothing"" has the
effect of correlating inputs that occur close in time with outputs that are
close in the L2-norm.
  The second objective is to maximize the log determinant of the covariance
matrix of activations in each layer of the network. This objective ensures that
information from each layer is passed through to the next. This second
objective acts as a balance to the first, which on its own would result in a
network with all input weights equal to zero.",['Per Rutquist'],"['cs.LG', 'cs.CV', 'stat.ML']",2019-05-08 14:37:38+00:00
http://arxiv.org/abs/1905.03053v1,Multi-modal Graph Fusion for Inductive Disease Classification in Incomplete Datasets,"Clinical diagnostic decision making and population-based studies often rely
on multi-modal data which is noisy and incomplete. Recently, several works
proposed geometric deep learning approaches to solve disease classification, by
modeling patients as nodes in a graph, along with graph signal processing of
multi-modal features. Many of these approaches are limited by assuming
modality- and feature-completeness, and by transductive inference, which
requires re-training of the entire model for each new test sample. In this
work, we propose a novel inductive graph-based approach that can generalize to
out-of-sample patients, despite missing features from entire modalities per
patient. We propose multi-modal graph fusion which is trained end-to-end
towards node-level classification. We demonstrate the fundamental working
principle of this method on a simplified MNIST toy dataset. In experiments on
medical data, our method outperforms single static graph approach in
multi-modal disease classification.","['Gerome Vivar', 'Hendrik Burwinkel', 'Anees Kazi', 'Andreas Zwergal', 'Nassir Navab', 'Seyed-Ahmad Ahmadi']","['cs.LG', 'stat.ML', '68T99']",2019-05-08 13:10:14+00:00
http://arxiv.org/abs/1905.03046v1,PiNet: A Permutation Invariant Graph Neural Network for Graph Classification,"We propose an end-to-end deep learning learning model for graph
classification and representation learning that is invariant to permutation of
the nodes of the input graphs. We address the challenge of learning a fixed
size graph representation for graphs of varying dimensions through a
differentiable node attention pooling mechanism. In addition to a theoretical
proof of its invariance to permutation, we provide empirical evidence
demonstrating the statistically significant gain in accuracy when faced with an
isomorphic graph classification task given only a small number of training
examples. We analyse the effect of four different matrices to facilitate the
local message passing mechanism by which graph convolutions are performed vs. a
matrix parametrised by a learned parameter pair able to transition smoothly
between the former. Finally, we show that our model achieves competitive
classification performance with existing techniques on a set of molecule
datasets.","['Peter Meltzer', 'Marcelo Daniel Gutierrez Mallea', 'Peter J. Bentley']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2019-05-08 12:51:52+00:00
http://arxiv.org/abs/1905.03036v2,Adaptive Image-Feature Learning for Disease Classification Using Inductive Graph Networks,"Recently, Geometric Deep Learning (GDL) has been introduced as a novel and
versatile framework for computer-aided disease classification. GDL uses patient
meta-information such as age and gender to model patient cohort relations in a
graph structure. Concepts from graph signal processing are leveraged to learn
the optimal mapping of multi-modal features, e.g. from images to disease
classes. Related studies so far have considered image features that are
extracted in a pre-processing step. We hypothesize that such an approach
prevents the network from optimizing feature representations towards achieving
the best performance in the graph network. We propose a new network
architecture that exploits an inductive end-to-end learning approach for
disease classification, where filters from both the CNN and the graph are
trained jointly. We validate this architecture against state-of-the-art
inductive graph networks and demonstrate significantly improved classification
scores on a modified MNIST toy dataset, as well as comparable classification
results with higher stability on a chest X-ray image dataset. Additionally, we
explain how the structural information of the graph affects both the image
filters and the feature learning.","['Hendrik Burwinkel', 'Anees Kazi', 'Gerome Vivar', 'Shadi Albarqouni', 'Guillaume Zahnd', 'Nassir Navab', 'Seyed-Ahmad Ahmadi']","['cs.LG', 'eess.IV', 'stat.ML', '68T99']",2019-05-08 12:39:43+00:00
http://arxiv.org/abs/1905.03030v2,Meta-learning of Sequential Strategies,"In this report we review memory-based meta-learning as a tool for building
sample-efficient strategies that learn from past experience to adapt to any
task within a target class. Our goal is to equip the reader with the conceptual
foundations of this tool for building new, scalable agents that operate on
broad domains. To do so, we present basic algorithmic templates for building
near-optimal predictors and reinforcement learners which behave as if they had
a probabilistic model that allowed them to efficiently exploit task structure.
Furthermore, we recast memory-based meta-learning within a Bayesian framework,
showing that the meta-learned strategies are near-optimal because they amortize
Bayes-filtered data, where the adaptation is implemented in the memory dynamics
as a state-machine of sufficient statistics. Essentially, memory-based
meta-learning translates the hard problem of probabilistic sequential inference
into a regression problem.","['Pedro A. Ortega', 'Jane X. Wang', 'Mark Rowland', 'Tim Genewein', 'Zeb Kurth-Nelson', 'Razvan Pascanu', 'Nicolas Heess', 'Joel Veness', 'Alex Pritzel', 'Pablo Sprechmann', 'Siddhant M. Jayakumar', 'Tom McGrath', 'Kevin Miller', 'Mohammad Azar', 'Ian Osband', 'Neil Rabinowitz', 'András György', 'Silvia Chiappa', 'Simon Osindero', 'Yee Whye Teh', 'Hado van Hasselt', 'Nando de Freitas', 'Matthew Botvinick', 'Shane Legg']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-08 12:27:20+00:00
http://arxiv.org/abs/1905.03026v1,3d-SMRnet: Achieving a new quality of MPI system matrix recovery by deep learning,"Magnetic particle imaging (MPI) data is commonly reconstructed using a system
matrix acquired in a time-consuming calibration measurement. The calibration
approach has the important advantage over model-based reconstruction that it
takes the complex particle physics as well as system imperfections into
account. This benefit comes for the cost that the system matrix needs to be
re-calibrated whenever the scan parameters, particle types or even the particle
environment (e.g. viscosity or temperature) changes. One route for reducing the
calibration time is the sampling of the system matrix at a subset of the
spatial positions of the intended field-of-view and employing system matrix
recovery. Recent approaches used compressed sensing (CS) and achieved
subsampling factors up to 28 that still allowed reconstructing MPI images of
sufficient quality. In this work, we propose a novel framework with a 3d-System
Matrix Recovery Network and demonstrate it to recover a 3d system matrix with a
subsampling factor of 64 in less than one minute and to outperform CS in terms
of system matrix quality, reconstructed image quality, and processing time. The
advantage of our method is demonstrated by reconstructing open access MPI
datasets. The model is further shown to be capable of inferring system matrices
for different particle types.","['Ivo Matteo Baltruschat', 'Patryk Szwargulski', 'Florian Griese', 'Mirco Grosser', 'René Werner', 'Tobias Knopp']","['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML']",2019-05-08 12:21:39+00:00
http://arxiv.org/abs/1905.02961v1,Generalized Dilation Neural Networks,"Vanilla convolutional neural networks are known to provide superior
performance not only in image recognition tasks but also in natural language
processing and time series analysis. One of the strengths of convolutional
layers is the ability to learn features about spatial relations in the input
domain using various parameterized convolutional kernels. However, in time
series analysis learning such spatial relations is not necessarily required nor
effective. In such cases, kernels which model temporal dependencies or kernels
with broader spatial resolutions are recommended for more efficient training as
proposed by dilation kernels. However, the dilation has to be fixed a priori
which limits the flexibility of the kernels. We propose generalized dilation
networks which generalize the initial dilations in two aspects. First we derive
an end-to-end learnable architecture for dilation layers where also the
dilation rate can be learned. Second we break up the strict dilation structure,
in that we develop kernels operating independently in the input space.","['Gavneet Singh Chadha', 'Jan Niclas Reimann', 'Andreas Schwung']","['cs.LG', 'stat.ML']",2019-05-08 08:46:04+00:00
http://arxiv.org/abs/1905.02957v1,SAdam: A Variant of Adam for Strongly Convex Functions,"The Adam algorithm has become extremely popular for large-scale machine
learning. Under convexity condition, it has been proved to enjoy a
data-dependant $O(\sqrt{T})$ regret bound where $T$ is the time horizon.
However, whether strong convexity can be utilized to further improve the
performance remains an open problem. In this paper, we give an affirmative
answer by developing a variant of Adam (referred to as SAdam) which achieves a
data-dependant $O(\log T)$ regret bound for strongly convex functions. The
essential idea is to maintain a faster decaying yet under controlled step size
for exploiting strong convexity. In addition, under a special configuration of
hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant
of RMSprop for strongly convex functions, for which we provide the first
data-dependent logarithmic regret bound. Empirical results on optimizing
strongly convex functions and training deep networks demonstrate the
effectiveness of our method.","['Guanghui Wang', 'Shiyin Lu', 'Weiwei Tu', 'Lijun Zhang']","['cs.LG', 'stat.ML']",2019-05-08 08:38:30+00:00
http://arxiv.org/abs/1905.06256v1,A Scalable Learned Index Scheme in Storage Systems,"Index structures are important for efficient data access, which have been
widely used to improve the performance in many in-memory systems. Due to high
in-memory overheads, traditional index structures become difficult to process
the explosive growth of data, let alone providing low latency and high
throughput performance with limited system resources. The promising learned
indexes leverage deep-learning models to complement existing index structures
and obtain significant memory savings. However, the learned indexes fail to
become scalable due to the heavy inter-model dependency and expensive
retraining. To address these problems, we propose a scalable learned index
scheme to construct different linear regression models according to the data
distribution. Moreover, the used models are independent so as to reduce the
complexity of retraining and become easy to partition and store the data into
different pages, blocks or distributed systems. Our experimental results show
that compared with state-of-the-art schemes, AIDEL improves the insertion
performance by about 2$\times$ and provides comparable lookup performance,
while efficiently supporting scalability.","['Pengfei Li', 'Yu Hua', 'Pengfei Zuo', 'Jingnan Jia']","['cs.DB', 'cs.LG', 'stat.ML']",2019-05-08 08:14:19+00:00
http://arxiv.org/abs/1905.02941v1,Robust Federated Training via Collaborative Machine Teaching using Trusted Instances,"Federated learning performs distributed model training using local data
hosted by agents. It shares only model parameter updates for iterative
aggregation at the server. Although it is privacy-preserving by design,
federated learning is vulnerable to noise corruption of local agents, as
demonstrated in the previous study on adversarial data poisoning threat against
federated learning systems. Even a single noise-corrupted agent can bias the
model training. In our work, we propose a collaborative and privacy-preserving
machine teaching paradigm with multiple distributed teachers, to improve
robustness of the federated training process against local data corruption. We
assume that each local agent (teacher) have the resources to verify a small
portions of trusted instances, which may not by itself be adequate for
learning. In the proposed collaborative machine teaching method, these trusted
instances guide the distributed agents to jointly select a compact while
informative training subset from data hosted by their own. Simultaneously, the
agents learn to add changes of limited magnitudes into the selected data
instances, in order to improve the testing performances of the federally
trained model despite of the training data corruption. Experiments on toy and
real data demonstrate that our approach can identify training set bugs
effectively and suggest appropriate changes to the labels. Our algorithm is a
step toward trustworthy machine learning.","['Yufei Han', 'Xiangliang Zhang']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-08 07:27:04+00:00
http://arxiv.org/abs/1905.02850v3,Understanding Attention and Generalization in Graph Neural Networks,"We aim to better understand attention over nodes in graph neural networks
(GNNs) and identify factors influencing its effectiveness. We particularly
focus on the ability of attention GNNs to generalize to larger, more complex or
noisy graphs. Motivated by insights from the work on Graph Isomorphism
Networks, we design simple graph reasoning tasks that allow us to study
attention in a controlled environment. We find that under typical conditions
the effect of attention is negligible or even harmful, but under certain
conditions it provides an exceptional gain in performance of more than 60% in
some of our classification tasks. Satisfying these conditions in practice is
challenging and often requires optimal initialization or supervised training of
attention. We propose an alternative recipe and train attention in a
weakly-supervised fashion that approaches the performance of supervised models,
and, compared to unsupervised models, improves results on several synthetic as
well as real datasets. Source code and datasets are available at
https://github.com/bknyaz/graph_attention_pool.","['Boris Knyazev', 'Graham W. Taylor', 'Mohamed R. Amer']","['cs.LG', 'cs.AI', 'stat.ML']",2019-05-08 00:30:25+00:00
http://arxiv.org/abs/1905.02845v1,Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review,"Pattern analysis often requires a pre-processing stage for extracting or
selecting features in order to help the classification, prediction, or
clustering stage discriminate or represent the data in a better way. The reason
for this requirement is that the raw data are complex and difficult to process
without extracting or selecting appropriate features beforehand. This paper
reviews theory and motivation of different common methods of feature selection
and extraction and introduces some of their applications. Some numerical
implementations are also shown for these methods. Finally, the methods in
feature selection and extraction are compared.","['Benyamin Ghojogh', 'Maria N. Samad', 'Sayema Asif Mashhadi', 'Tania Kapoor', 'Wahab Ali', 'Fakhri Karray', 'Mark Crowley']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",2019-05-07 23:41:34+00:00
http://arxiv.org/abs/1905.03420v1,A deep learning approach for analyzing the composition of chemometric data,"We propose novel deep learning based chemometric data analysis technique. We
trained L2 regularized sparse autoencoder end-to-end for reducing the size of
the feature vector to handle the classic problem of the curse of dimensionality
in chemometric data analysis. We introduce a novel technique of automatic
selection of nodes inside the hidden layer of an autoencoder through Pareto
optimization. Moreover, Gaussian process regressor is applied on the reduced
size feature vector for the regression. We evaluated our technique on orange
juice and wine dataset and results are compared against 3 state-of-the-art
methods. Quantitative results are shown on Normalized Mean Square Error (NMSE)
and the results show considerable improvement in the state-of-the-art.","['Muhammad Bilal', 'Mohib Ullah']","['cs.LG', 'stat.ML']",2019-05-07 23:20:47+00:00
http://arxiv.org/abs/1905.02841v2,Accelerated Target Updates for Q-learning,"This paper studies accelerations in Q-learning algorithms. We propose an
accelerated target update scheme by incorporating the historical iterates of Q
functions. The idea is conceptually inspired by the momentum-based accelerated
methods in the optimization theory. Conditions under which the proposed
accelerated algorithms converge are established. The algorithms are validated
using commonly adopted testing problems in reinforcement learning, including
the FrozenLake grid world game, two discrete-time LQR problems from the
Deepmind Control Suite, and the Atari 2600 games. Simulation results show that
the proposed accelerated algorithms can improve the convergence performance
compared with the vanilla Q-learning algorithm.","['Bowen Weng', 'Huaqing Xiong', 'Wei Zhang']","['cs.LG', 'math.OC', 'stat.ML']",2019-05-07 23:14:23+00:00
