id,title,abstract,authors,categories,date
http://arxiv.org/abs/2401.07298v1,Efficient Frameworks for Generalized Low-Rank Matrix Bandit Problems,"In the stochastic contextual low-rank matrix bandit problem, the expected
reward of an action is given by the inner product between the action's feature
matrix and some fixed, but initially unknown $d_1$ by $d_2$ matrix $\Theta^*$
with rank $r \ll \{d_1, d_2\}$, and an agent sequentially takes actions based
on past experience to maximize the cumulative reward. In this paper, we study
the generalized low-rank matrix bandit problem, which has been recently
proposed in \cite{lu2021low} under the Generalized Linear Model (GLM)
framework. To overcome the computational infeasibility and theoretical restrain
of existing algorithms on this problem, we first propose the G-ESTT framework
that modifies the idea from \cite{jun2019bilinear} by using Stein's method on
the subspace estimation and then leverage the estimated subspaces via a
regularization idea. Furthermore, we remarkably improve the efficiency of
G-ESTT by using a novel exclusion idea on the estimated subspace instead, and
propose the G-ESTS framework. We also show that G-ESTT can achieve the
$\tilde{O}(\sqrt{(d_1+d_2)MrT})$ bound of regret while G-ESTS can achineve the
$\tilde{O}(\sqrt{(d_1+d_2)^{3/2}Mr^{3/2}T})$ bound of regret under mild
assumption up to logarithm terms, where $M$ is some problem dependent value.
Under a reasonable assumption that $M = O((d_1+d_2)^2)$ in our problem setting,
the regret of G-ESTT is consistent with the current best regret of
$\tilde{O}((d_1+d_2)^{3/2} \sqrt{rT}/D_{rr})$~\citep{lu2021low} ($D_{rr}$ will
be defined later). For completeness, we conduct experiments to illustrate that
our proposed algorithms, especially G-ESTS, are also computationally tractable
and consistently outperform other state-of-the-art (generalized) linear matrix
bandit methods based on a suite of simulations.","['Yue Kang', 'Cho-Jui Hsieh', 'Thomas C. M. Lee']","['stat.ML', 'cs.LG']",2024-01-14 14:14:19+00:00
http://arxiv.org/abs/2401.07231v3,Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data,"This paper proposes two methods for causal additive models with unobserved
variables (CAM-UV). CAM-UV assumes that the causal functions take the form of
generalized additive models and that latent confounders are present. First, we
propose a method that leverages prior knowledge for efficient causal discovery.
Then, we propose an extension of this method for inferring causality in time
series data. The original CAM-UV algorithm differs from other existing causal
function models in that it does not seek the causal order between observed
variables, but rather aims to identify the causes for each observed variable.
Therefore, the first proposed method in this paper utilizes prior knowledge,
such as understanding that certain variables cannot be causes of specific
others. Moreover, by incorporating the prior knowledge that causes precedes
their effects in time, we extend the first algorithm to the second method for
causal discovery in time series data. We validate the first proposed method by
using simulated data to demonstrate that the accuracy of causal discovery
increases as more prior knowledge is accumulated. Additionally, we test the
second proposed method by comparing it with existing time series causal
discovery methods, using both simulated data and real-world data.","['Takashi Nicholas Maeda', 'Shohei Shimizu']","['cs.LG', 'stat.ME', 'stat.ML']",2024-01-14 08:32:32+00:00
http://arxiv.org/abs/2401.07206v1,Probabilistic Reduced-Dimensional Vector Autoregressive Modeling with Oblique Projections,"In this paper, we propose a probabilistic reduced-dimensional vector
autoregressive (PredVAR) model to extract low-dimensional dynamics from
high-dimensional noisy data. The model utilizes an oblique projection to
partition the measurement space into a subspace that accommodates the
reduced-dimensional dynamics and a complementary static subspace. An optimal
oblique decomposition is derived for the best predictability regarding
prediction error covariance. Building on this, we develop an iterative PredVAR
algorithm using maximum likelihood and the expectation-maximization (EM)
framework. This algorithm alternately updates the estimates of the latent
dynamics and optimal oblique projection, yielding dynamic latent variables with
rank-ordered predictability and an explicit latent VAR model that is consistent
with the outer projection model. The superior performance and efficiency of the
proposed approach are demonstrated using data sets from a synthesized Lorenz
system and an industrial process from Eastman Chemical.","['Yanfang Mo', 'S. Joe Qin']","['stat.ML', 'cs.LG', 'cs.SY', 'eess.SY']",2024-01-14 05:38:10+00:00
http://arxiv.org/abs/2401.07187v3,"A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models","In this article, we review the literature on statistical theories of neural
networks from three perspectives: approximation, training dynamics and
generative models. In the first part, results on excess risks for neural
networks are reviewed in the nonparametric framework of regression (and
classification in Appendix~{\color{blue}B}). These results rely on explicit
constructions of neural networks, leading to fast convergence rates of excess
risks. Nonetheless, their underlying analysis only applies to the global
minimizer in the highly non-convex landscape of deep neural networks. This
motivates us to review the training dynamics of neural networks in the second
part. Specifically, we review papers that attempt to answer ``how the neural
network trained via gradient-based methods finds the solution that can
generalize well on unseen data.'' In particular, two well-known paradigms are
reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF)
paradigm. Last but not least, we review the most recent theoretical
advancements in generative models including Generative Adversarial Networks
(GANs), diffusion models, and in-context learning (ICL) in the Large Language
Models (LLMs) from two perpsectives reviewed previously, i.e., approximation
and training dynamics.","['Namjoon Suh', 'Guang Cheng']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2024-01-14 02:30:19+00:00
http://arxiv.org/abs/2401.07174v1,On the (In)Compatibility between Group Fairness and Individual Fairness,"We study the compatibility between the optimal statistical parity solutions
and individual fairness. While individual fairness seeks to treat similar
individuals similarly, optimal statistical parity aims to provide similar
treatment to individuals who share relative similarity within their respective
sensitive groups. The two fairness perspectives, while both desirable from a
fairness perspective, often come into conflict in applications. Our goal in
this work is to analyze the existence of this conflict and its potential
solution. In particular, we establish sufficient (sharp) conditions for the
compatibility between the optimal (post-processing) statistical parity $L^2$
learning and the ($K$-Lipschitz or $(\epsilon,\delta)$) individual fairness
requirements. Furthermore, when there exists a conflict between the two, we
first relax the former to the Pareto frontier (or equivalently the optimal
trade-off) between $L^2$ error and statistical disparity, and then analyze the
compatibility between the frontier and the individual fairness requirements.
Our analysis identifies regions along the Pareto frontier that satisfy
individual fairness requirements. (Lastly, we provide individual fairness
guarantees for the composition of a trained model and the optimal
post-processing step so that one can determine the compatibility of the
post-processed model.) This provides practitioners with a valuable approach to
attain Pareto optimality for statistical parity while adhering to the
constraints of individual fairness.","['Shizhou Xu', 'Thomas Strohmer']","['math.ST', 'cs.CY', 'cs.LG', 'stat.ML', 'stat.TH']",2024-01-13 23:38:10+00:00
http://arxiv.org/abs/2401.07110v2,Hebbian Learning from First Principles,"Recently, the original storage prescription for the Hopfield model of neural
networks -- as well as for its dense generalizations -- has been turned into a
genuine Hebbian learning rule by postulating the expression of its Hamiltonian
for both the supervised and unsupervised protocols. In these notes, first, we
obtain these explicit expressions by relying upon maximum entropy extremization
\`a la Jaynes. Beyond providing a formal derivation of these recipes for
Hebbian learning, this construction also highlights how Lagrangian constraints
within entropy extremization force network's outcomes on neural correlations:
these try to mimic the empirical counterparts hidden in the datasets provided
to the network for its training and, the denser the network, the longer the
correlations that it is able to capture. Next, we prove that, in the big data
limit, whatever the presence of a teacher (or its lacking), not only these
Hebbian learning rules converge to the original storage prescription of the
Hopfield model but also their related free energies (and, thus, the statistical
mechanical picture provided by Amit, Gutfreund and Sompolinsky is fully
recovered). As a sideline, we show mathematical equivalence among standard Cost
functions (Hamiltonian), preferred in Statistical Mechanical jargon, and
quadratic Loss Functions, preferred in Machine Learning terminology. Remarks on
the exponential Hopfield model (as the limit of dense networks with diverging
density) and semi-supervised protocols are also provided.","['Linda Albanese', 'Adriano Barra', 'Pierluigi Bianco', 'Fabrizio Durante', 'Diego Pallara']","['cond-mat.dis-nn', 'stat.ML']",2024-01-13 16:19:49+00:00
http://arxiv.org/abs/2401.08691v1,Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making,"In an era characterized by the pervasive integration of artificial
intelligence into decision-making processes across diverse industries, the
demand for trust has never been more pronounced. This thesis embarks on a
comprehensive exploration of bias and fairness, with a particular emphasis on
their ramifications within the banking sector, where AI-driven decisions bear
substantial societal consequences. In this context, the seamless integration of
fairness, explainability, and human oversight is of utmost importance,
culminating in the establishment of what is commonly referred to as
""Responsible AI"". This emphasizes the critical nature of addressing biases
within the development of a corporate culture that aligns seamlessly with both
AI regulations and universal human rights standards, particularly in the realm
of automated decision-making systems. Nowadays, embedding ethical principles
into the development, training, and deployment of AI models is crucial for
compliance with forthcoming European regulations and for promoting societal
good. This thesis is structured around three fundamental pillars: understanding
bias, mitigating bias, and accounting for bias. These contributions are
validated through their practical application in real-world scenarios, in
collaboration with Intesa Sanpaolo. This collaborative effort not only
contributes to our understanding of fairness but also provides practical tools
for the responsible implementation of AI-based decision-making systems. In line
with open-source principles, we have released Bias On Demand and FairView as
accessible Python packages, further promoting progress in the field of AI
fairness.",['Alessandro Castelnovo'],"['stat.ML', 'cs.CY', 'cs.LG', 'stat.AP']",2024-01-13 14:07:09+00:00
http://arxiv.org/abs/2401.07012v1,An ADRC-Incorporated Stochastic Gradient Descent Algorithm for Latent Factor Analysis,"High-dimensional and incomplete (HDI) matrix contains many complex
interactions between numerous nodes. A stochastic gradient descent (SGD)-based
latent factor analysis (LFA) model is remarkably effective in extracting
valuable information from an HDI matrix. However, such a model commonly
encounters the problem of slow convergence because a standard SGD algorithm
only considers the current learning error to compute the stochastic gradient
without considering the historical and future state of the learning error. To
address this critical issue, this paper innovatively proposes an
ADRC-incorporated SGD (ADS) algorithm by refining the instance learning error
by considering the historical and future state by following the principle of an
ADRC controller. With it, an ADS-based LFA model is further achieved for fast
and accurate latent factor analysis on an HDI matrix. Empirical studies on two
HDI datasets demonstrate that the proposed model outperforms the
state-of-the-art LFA models in terms of computational efficiency and accuracy
for predicting the missing data of an HDI matrix.","['Jinli Li', 'Ye Yuan']","['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",2024-01-13 08:38:54+00:00
http://arxiv.org/abs/2401.06980v1,Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization,"In this paper, we present a novel bilevel optimization-based training
approach to training acoustic models for automatic speech recognition (ASR)
tasks that we term {bi-level joint unsupervised and supervised training
(BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an
unsupervised loss and a supervised loss respectively, leveraging recent
advances in penalty-based bilevel optimization to solve this challenging ASR
problem with affordable complexity and rigorous convergence guarantees.} To
evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2
datasets have been conducted. BL-JUST achieves superior performance over the
commonly used pre-training followed by fine-tuning strategy.","['A F M Saif', 'Xiaodong Cui', 'Han Shen', 'Songtao Lu', 'Brian Kingsbury', 'Tianyi Chen']","['cs.CL', 'cs.LG', 'stat.ML']",2024-01-13 05:01:47+00:00
http://arxiv.org/abs/2401.06925v2,Modeling Latent Selection with Structural Causal Models,"Selection bias is ubiquitous in real-world data, and can lead to misleading
results if not dealt with properly. We introduce a conditioning operation on
Structural Causal Models (SCMs) to model latent selection from a causal
perspective. We show that the conditioning operation transforms an SCM with the
presence of an explicit latent selection mechanism into an SCM without such
selection mechanism, which partially encodes the causal semantics of the
selected subpopulation according to the original SCM. Furthermore, we show that
this conditioning operation preserves the simplicity, acyclicity, and linearity
of SCMs, and commutes with marginalization. Thanks to these properties,
combined with marginalization and intervention, the conditioning operation
offers a valuable tool for conducting causal reasoning tasks within causal
models where latent details have been abstracted away. We demonstrate by
example how classical results of causal inference can be generalized to include
selection bias and how the conditioning operation helps with modeling of
real-world problems.","['Leihao Chen', 'Onno Zoeter', 'Joris M. Mooij']","['cs.AI', 'cs.LG', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2024-01-12 23:14:34+00:00
http://arxiv.org/abs/2401.06922v1,Open RAN LSTM Traffic Prediction and Slice Management using Deep Reinforcement Learning,"With emerging applications such as autonomous driving, smart cities, and
smart factories, network slicing has become an essential component of 5G and
beyond networks as a means of catering to a service-aware network. However,
managing different network slices while maintaining quality of services (QoS)
is a challenge in a dynamic environment. To address this issue, this paper
leverages the heterogeneous experiences of distributed units (DUs) in ORAN
systems and introduces a novel approach to ORAN slicing xApp using distributed
deep reinforcement learning (DDRL). Additionally, to enhance the
decision-making performance of the RL agent, a prediction rApp based on long
short-term memory (LSTM) is incorporated to provide additional information from
the dynamic environment to the xApp. Simulation results demonstrate significant
improvements in network performance, particularly in reducing QoS violations.
This emphasizes the importance of using the prediction rApp and distributed
actors' information jointly as part of a dynamic xApp.","['Fatemeh Lotfi', 'Fatemeh Afghah']","['cs.LG', 'cs.AI', 'cs.NI', 'cs.SY', 'eess.SY', 'stat.ML']",2024-01-12 22:43:07+00:00
http://arxiv.org/abs/2401.06864v1,Deep Learning With DAGs,"Social science theories often postulate causal relationships among a set of
variables or events. Although directed acyclic graphs (DAGs) are increasingly
used to represent these theories, their full potential has not yet been
realized in practice. As non-parametric causal models, DAGs require no
assumptions about the functional form of the hypothesized relationships.
Nevertheless, to simplify the task of empirical evaluation, researchers tend to
invoke such assumptions anyway, even though they are typically arbitrary and do
not reflect any theoretical content or prior knowledge. Moreover, functional
form assumptions can engender bias, whenever they fail to accurately capture
the complexity of the causal system under investigation. In this article, we
introduce causal-graphical normalizing flows (cGNFs), a novel approach to
causal inference that leverages deep neural networks to empirically evaluate
theories represented as DAGs. Unlike conventional approaches, cGNFs model the
full joint distribution of the data according to a DAG supplied by the analyst,
without relying on stringent assumptions about functional form. In this way,
the method allows for flexible, semi-parametric estimation of any causal
estimand that can be identified from the DAG, including total effects,
conditional effects, direct and indirect effects, and path-specific effects. We
illustrate the method with a reanalysis of Blau and Duncan's (1967) model of
status attainment and Zhou's (2019) model of conditional versus controlled
mobility. To facilitate adoption, we provide open-source software together with
a series of online tutorials for implementing cGNFs. The article concludes with
a discussion of current limitations and directions for future development.","['Sourabh Balgi', 'Adel Daoud', 'Jose M. Peña', 'Geoffrey T. Wodtke', 'Jesse Zhou']","['stat.ML', 'cs.LG', 'econ.EM', 'stat.ME']",2024-01-12 19:35:54+00:00
http://arxiv.org/abs/2401.06740v1,A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models,"We develop a novel deep learning approach for pricing European basket options
written on assets that follow jump-diffusion dynamics. The option pricing
problem is formulated as a partial integro-differential equation, which is
approximated via a new implicit-explicit minimizing movement time-stepping
approach, involving approximation by deep, residual-type Artificial Neural
Networks (ANNs) for each time step. The integral operator is discretized via
two different approaches: a) a sparse-grid Gauss--Hermite approximation
following localised coordinate axes arising from singular value decompositions,
and b) an ANN-based high-dimensional special-purpose quadrature rule.
Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of
the solution for large values of the underlyings and also leads to consistent
outputs with respect to a priori known qualitative properties of the solution.
The performance and robustness with respect to the dimension of the methods are
assessed in a series of numerical experiments involving the Merton
jump-diffusion model.","['Emmanuil H. Georgoulis', 'Antonis Papapantoleon', 'Costas Smaragdakis']","['q-fin.CP', 'cs.LG', 'cs.NA', 'math.NA', 'math.PR', 'stat.ML', '68T07, 65C20, 91G60, 91G20, 65M12']",2024-01-12 18:21:01+00:00
http://arxiv.org/abs/2401.06738v2,(Accelerated) Noise-adaptive Stochastic Heavy-Ball Momentum,"Stochastic heavy ball momentum (SHB) is commonly used to train machine
learning models, and often provides empirical improvements over stochastic
gradient descent. By primarily focusing on strongly-convex quadratics, we aim
to better understand the theoretical advantage of SHB and subsequently improve
the method. For strongly-convex quadratics, Kidambi et al. (2018) show that SHB
(with a mini-batch of size $1$) cannot attain accelerated convergence, and
hence has no theoretical benefit over SGD. They conjecture that the practical
gain of SHB is a by-product of using larger mini-batches. We first substantiate
this claim by showing that SHB can attain an accelerated rate when the
mini-batch size is larger than a threshold $b^*$ that depends on the condition
number $\kappa$. Specifically, we prove that with the same step-size and
momentum parameters as in the deterministic setting, SHB with a sufficiently
large mini-batch size results in an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) +
\sigma \right)$ convergence, where $T$ is the number of iterations and
$\sigma^2$ is the variance in the stochastic gradients. We prove a lower-bound
which demonstrates that a $\kappa$ dependence in $b^*$ is necessary. To ensure
convergence to the minimizer, we design a noise-adaptive multi-stage algorithm
that results in an $O\left(\exp\left(-\frac{T}{\sqrt{\kappa}}\right) +
\frac{\sigma}{T}\right)$ rate. We also consider the general smooth,
strongly-convex setting and propose the first noise-adaptive SHB variant that
converges to the minimizer at an $O(\exp(-\frac{T}{\kappa}) +
\frac{\sigma^2}{T})$ rate. We empirically demonstrate the effectiveness of the
proposed algorithms.","['Anh Dang', 'Reza Babanezhad', 'Sharan Vaswani']","['math.OC', 'cs.LG', 'stat.ML']",2024-01-12 18:17:28+00:00
http://arxiv.org/abs/2401.06564v1,Valid causal inference with unobserved confounding in high-dimensional settings,"Various methods have recently been proposed to estimate causal effects with
confidence intervals that are uniformly valid over a set of data generating
processes when high-dimensional nuisance models are estimated by
post-model-selection or machine learning estimators. These methods typically
require that all the confounders are observed to ensure identification of the
effects. We contribute by showing how valid semiparametric inference can be
obtained in the presence of unobserved confounders and high-dimensional
nuisance models. We propose uncertainty intervals which allow for unobserved
confounding, and show that the resulting inference is valid when the amount of
unobserved confounding is small relative to the sample size; the latter is
formalized in terms of convergence rates. Simulation experiments illustrate the
finite sample properties of the proposed intervals and investigate an
alternative procedure that improves the empirical coverage of the intervals
when the amount of unobserved confounding is large. Finally, a case study on
the effect of smoking during pregnancy on birth weight is used to illustrate
the use of the methods introduced to perform a sensitivity analysis to
unobserved confounding.","['Niloofar Moosavi', 'Tetiana Gorbach', 'Xavier de Luna']","['stat.ME', 'stat.ML', '62D20, 62D10']",2024-01-12 13:21:20+00:00
http://arxiv.org/abs/2401.06523v1,Boosting Causal Additive Models,"We present a boosting-based method to learn additive Structural Equation
Models (SEMs) from observational data, with a focus on the theoretical aspects
of determining the causal order among variables. We introduce a family of score
functions based on arbitrary regression techniques, for which we establish
necessary conditions to consistently favor the true causal ordering. Our
analysis reveals that boosting with early stopping meets these criteria and
thus offers a consistent score function for causal orderings. To address the
challenges posed by high-dimensional data sets, we adapt our approach through a
component-wise gradient descent in the space of additive SEMs. Our simulation
study underlines our theoretical results for lower dimensions and demonstrates
that our high-dimensional adaptation is competitive with state-of-the-art
methods. In addition, it exhibits robustness with respect to the choice of the
hyperparameters making the procedure easy to tune.","['Maximilian Kertel', 'Nadja Klein']","['stat.ML', 'cs.LG', 'math.PR', 'math.ST', 'stat.TH']",2024-01-12 11:43:11+00:00
http://arxiv.org/abs/2401.06481v2,Machine learning a fixed point action for SU(3) gauge theory with a gauge equivariant convolutional neural network,"Fixed point lattice actions are designed to have continuum classical
properties unaffected by discretization effects and reduced lattice artifacts
at the quantum level. They provide a possible way to extract continuum physics
with coarser lattices, thereby allowing one to circumvent problems with
critical slowing down and topological freezing toward the continuum limit. A
crucial ingredient for practical applications is to find an accurate and
compact parametrization of a fixed point action, since many of its properties
are only implicitly defined. Here we use machine learning methods to revisit
the question of how to parametrize fixed point actions. In particular, we
obtain a fixed point action for four-dimensional SU(3) gauge theory using
convolutional neural networks with exact gauge invariance. The large operator
space allows us to find superior parametrizations compared to previous studies,
a necessary first step for future Monte Carlo simulations and scaling studies.","['Kieran Holland', 'Andreas Ipp', 'David I. Müller', 'Urs Wenger']","['hep-lat', 'cs.LG', 'hep-ph', 'stat.ML']",2024-01-12 10:03:00+00:00
http://arxiv.org/abs/2401.06447v2,Uncertainty-aware multi-fidelity surrogate modeling with noisy data,"Emulating high-accuracy computationally expensive models is crucial for tasks
requiring numerous model evaluations, such as uncertainty quantification and
optimization. When lower-fidelity models are available, they can be used to
improve the predictions of high-fidelity models. Multi-fidelity surrogate
models combine information from sources of varying fidelities to construct an
efficient surrogate model. However, in real-world applications, uncertainty is
present in both high- and low-fidelity models due to measurement or numerical
noise, as well as lack of knowledge due to the limited experimental design
budget. This paper introduces a comprehensive framework for multi-fidelity
surrogate modeling that handles noise-contaminated data and is able to estimate
the underlying noise-free high-fidelity model. Our methodology quantitatively
incorporates the different types of uncertainty affecting the problem and
emphasizes on delivering precise estimates of the uncertainty in its
predictions both with respect to the underlying high-fidelity model and unseen
noise-contaminated high-fidelity observations, presented through confidence and
prediction intervals, respectively. Additionally, the proposed framework offers
a natural approach to combining physical experiments and computational models
by treating noisy experimental data as high-fidelity sources and white-box
computational models as their low-fidelity counterparts. The effectiveness of
our methodology is showcased through synthetic examples and a wind turbine
application.","['Katerina Giannoukou', 'Stefano Marelli', 'Bruno Sudret']","['stat.ME', 'stat.CO', 'stat.ML']",2024-01-12 08:37:41+00:00
http://arxiv.org/abs/2401.06325v1,Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo,"To sample from a general target distribution $p_*\propto e^{-f_*}$ beyond the
isoperimetric condition, Huang et al. (2023) proposed to perform sampling
through reverse diffusion, giving rise to Diffusion-based Monte Carlo (DMC).
Specifically, DMC follows the reverse SDE of a diffusion process that
transforms the target distribution to the standard Gaussian, utilizing a
non-parametric score estimation. However, the original DMC algorithm
encountered high gradient complexity, resulting in an exponential dependency on
the error tolerance $\epsilon$ of the obtained samples. In this paper, we
demonstrate that the high complexity of DMC originates from its redundant
design of score estimation, and proposed a more efficient algorithm, called
RS-DMC, based on a novel recursive score estimation method. In particular, we
first divide the entire diffusion process into multiple segments and then
formulate the score estimation step (at any time step) as a series of
interconnected mean estimation and sampling subproblems accordingly, which are
correlated in a recursive manner. Importantly, we show that with a proper
design of the segment decomposition, all sampling subproblems will only need to
tackle a strongly log-concave distribution, which can be very efficient to
solve using the Langevin-based samplers with a provably rapid convergence rate.
As a result, we prove that the gradient complexity of RS-DMC only has a
quasi-polynomial dependency on $\epsilon$, which significantly improves
exponential gradient complexity in Huang et al. (2023). Furthermore, under
commonly used dissipative conditions, our algorithm is provably much faster
than the popular Langevin-based algorithms. Our algorithm design and
theoretical framework illuminate a novel direction for addressing sampling
problems, which could be of broader applicability in the community.","['Xunpeng Huang', 'Difan Zou', 'Hanze Dong', 'Yian Ma', 'Tong Zhang']","['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",2024-01-12 02:33:57+00:00
http://arxiv.org/abs/2401.05982v3,A tree-based varying coefficient model,"The paper introduces a tree-based varying coefficient model (VCM) where the
varying coefficients are modelled using the cyclic gradient boosting machine
(CGBM) from Delong et al. (2023). Modelling the coefficient functions using a
CGBM allows for dimension-wise early stopping and feature importance scores.
The dimension-wise early stopping not only reduces the risk of
dimension-specific overfitting, but also reveals differences in model
complexity across dimensions. The use of feature importance scores allows for
simple feature selection and easy model interpretation. The model is evaluated
on the same simulated and real data examples as those used in Richman and
W\""uthrich (2023), and the results show that it produces results in terms of
out of sample loss that are comparable to those of their neural network-based
VCM called LocalGLMnet.","['Henning Zakrisson', 'Mathias Lindholm']","['stat.ML', 'cs.LG']",2024-01-11 15:35:32+00:00
http://arxiv.org/abs/2401.05934v1,Combining Normalizing Flows and Quasi-Monte Carlo,"Recent advances in machine learning have led to the development of new
methods for enhancing Monte Carlo methods such as Markov chain Monte Carlo
(MCMC) and importance sampling (IS). One such method is normalizing flows,
which use a neural network to approximate a distribution by evaluating it
pointwise. Normalizing flows have been shown to improve the performance of MCMC
and IS. On the other side, (randomized) quasi-Monte Carlo methods are used to
perform numerical integration. They replace the random sampling of Monte Carlo
by a sequence which cover the hypercube more uniformly, resulting in better
convergence rates for the error that plain Monte Carlo. In this work, we
combine these two methods by using quasi-Monte Carlo to sample the initial
distribution that is transported by the flow. We demonstrate through numerical
experiments that this combination can lead to an estimator with significantly
lower variance than if the flow was sampled with a classic Monte Carlo.",['Charly Andral'],"['stat.CO', 'stat.ML']",2024-01-11 14:17:06+00:00
http://arxiv.org/abs/2401.05765v2,A new computationally efficient algorithm to solve Feature Selection for Functional Data Classification in high-dimensional spaces,"This paper introduces a novel methodology for Feature Selection for
Functional Classification, FSFC, that addresses the challenge of jointly
performing feature selection and classification of functional data in scenarios
with categorical responses and multivariate longitudinal features. FSFC tackles
a newly defined optimization problem that integrates logistic loss and
functional features to identify the most crucial variables for classification.
To address the minimization procedure, we employ functional principal
components and develop a new adaptive version of the Dual Augmented Lagrangian
algorithm. The computational efficiency of FSFC enables handling
high-dimensional scenarios where the number of features may considerably exceed
the number of statistical units. Simulation experiments demonstrate that FSFC
outperforms other machine learning and deep learning methods in computational
time and classification accuracy. Furthermore, the FSFC feature selection
capability can be leveraged to significantly reduce the problem's
dimensionality and enhance the performances of other classification algorithms.
The efficacy of FSFC is also demonstrated through a real data application,
analyzing relationships between four chronic diseases and other health and
demographic factors.","['Tobia Boschi', 'Francesca Bonin', 'Rodrigo Ordonez-Hurtado', 'Alessandra Pascale', 'Jonathan Epperlein']","['stat.ML', 'cs.LG']",2024-01-11 09:17:25+00:00
http://arxiv.org/abs/2401.05716v1,Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature and Bayesian Optimization,"In this paper, we study the problem of estimating the normalizing constant
$\int e^{-\lambda f(x)}dx$ through queries to the black-box function $f$, where
$f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\lambda$ is a
problem parameter. We show that to estimate the normalizing constant within a
small relative error, the level of difficulty depends on the value of
$\lambda$: When $\lambda$ approaches zero, the problem is similar to Bayesian
quadrature (BQ), while when $\lambda$ approaches infinity, the problem is
similar to Bayesian optimization (BO). More generally, the problem varies
between BQ and BO. We find that this pattern holds true even when the function
evaluations are noisy, bringing new aspects to this topic. Our findings are
supported by both algorithm-independent lower bounds and algorithmic upper
bounds, as well as simulation studies conducted on a variety of benchmark
functions.","['Xu Cai', 'Jonathan Scarlett']","['cs.LG', 'stat.ML']",2024-01-11 07:45:09+00:00
http://arxiv.org/abs/2401.05579v1,An Augmented Surprise-guided Sequential Learning Framework for Predicting the Melt Pool Geometry,"Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry,
offering benefits like intricate design, minimal waste, rapid prototyping,
material versatility, and customized solutions. However, its full industry
adoption faces hurdles, particularly in achieving consistent product quality. A
crucial aspect for MAM's success is understanding the relationship between
process parameters and melt pool characteristics. Integrating Artificial
Intelligence (AI) into MAM is essential. Traditional machine learning (ML)
methods, while effective, depend on large datasets to capture complex
relationships, a significant challenge in MAM due to the extensive time and
resources required for dataset creation. Our study introduces a novel
surprise-guided sequential learning framework, SurpriseAF-BO, signaling a
significant shift in MAM. This framework uses an iterative, adaptive learning
process, modeling the dynamics between process parameters and melt pool
characteristics with limited data, a key benefit in MAM's cyber manufacturing
context. Compared to traditional ML models, our sequential learning method
shows enhanced predictive accuracy for melt pool dimensions. Further improving
our approach, we integrated a Conditional Tabular Generative Adversarial
Network (CTGAN) into our framework, forming the CT-SurpriseAF-BO. This produces
synthetic data resembling real experimental data, improving learning
effectiveness. This enhancement boosts predictive precision without requiring
additional physical experiments. Our study demonstrates the power of advanced
data-driven techniques in cyber manufacturing and the substantial impact of
sequential AI and ML, particularly in overcoming MAM's traditional challenges.","['Ahmed Shoyeb Raihan', 'Hamed Khosravi', 'Tanveer Hossain Bhuiyan', 'Imtiaz Ahmed']","['cs.LG', 'stat.ML']",2024-01-10 23:05:23+00:00
http://arxiv.org/abs/2401.05574v3,A provable initialization and robust clustering method for general mixture models,"Clustering is a fundamental tool in statistical machine learning in the
presence of heterogeneous data. Most recent results focus primarily on optimal
mislabeling guarantees when data are distributed around centroids with
sub-Gaussian errors. Yet, the restrictive sub-Gaussian model is often invalid
in practice since various real-world applications exhibit heavy tail
distributions around the centroids or suffer from possible adversarial attacks
that call for robust clustering with a robust data-driven initialization. In
this paper, we present initialization and subsequent clustering methods that
provably guarantee near-optimal mislabeling for general mixture models when the
number of clusters and data dimensions are finite. We first introduce a hybrid
clustering technique with a novel multivariate trimmed mean type centroid
estimate to produce mislabeling guarantees under a weak initialization
condition for general error distributions around the centroids. A matching
lower bound is derived, up to factors depending on the number of clusters. In
addition, our approach also produces similar mislabeling guarantees even in the
presence of adversarial outliers. Our results reduce to the sub-Gaussian case
in finite dimensions when errors follow sub-Gaussian distributions. To solve
the problem thoroughly, we also present novel data-driven robust initialization
techniques and show that, with probabilities approaching one, these initial
centroid estimates are sufficiently good for the subsequent clustering
algorithm to achieve the optimal mislabeling rates. Furthermore, we demonstrate
that the Lloyd algorithm is suboptimal for more than two clusters even when
errors are Gaussian and for two clusters when error distributions have heavy
tails. Both simulated data and real data examples further support our robust
initialization procedure and clustering algorithm.","['Soham Jana', 'Jianqing Fan', 'Sanjeev Kulkarni']","['math.ST', 'stat.ML', 'stat.TH', '62H30 (Primary), 62G35, 62G05 (Secondary)']",2024-01-10 22:56:44+00:00
http://arxiv.org/abs/2401.05535v3,Theoretical and Empirical Advances in Forest Pruning,"Decades after their inception, regression forests continue to provide
state-of-the-art accuracy, outperforming in this respect alternative machine
learning models such as regression trees or even neural networks. However,
being an ensemble method, the one aspect where regression forests tend to
severely underperform regression trees is interpretability. In the present
work, we revisit forest pruning, an approach that aims to have the best of both
worlds: the accuracy of regression forests and the interpretability of
regression trees. This pursuit, whose foundation lies at the core of random
forest theory, has seen vast success in empirical studies. In this paper, we
contribute theoretical results that support and qualify those empirical
findings; namely, we prove the asymptotic advantage of a Lasso-pruned forest
over its unpruned counterpart under extremely weak assumptions, as well as
high-probability finite-sample generalization bounds for regression forests
pruned according to the main methods, which we then validate by way of
simulation. Then, we test the accuracy of pruned regression forests against
their unpruned counterparts on 19 different datasets (16 synthetic, 3 real). We
find that in the vast majority of scenarios tested, there is at least one
forest-pruning method that yields equal or better accuracy than the original
full forest (in expectation), while just using a small fraction of the trees.
We show that, in some cases, the reduction in the size of the forest is so
dramatic that the resulting sub-forest can be meaningfully merged into a single
tree, obtaining a level of interpretability that is qualitatively superior to
that of the original regression forest, which remains a black box.",['Albert Dorador'],"['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']",2024-01-10 20:02:47+00:00
http://arxiv.org/abs/2401.05330v2,Hierarchical Causal Models,"Scientists often want to learn about cause and effect from hierarchical data,
collected from subunits nested inside units. Consider students in schools,
cells in patients, or cities in states. In such settings, unit-level variables
(e.g. each school's budget) may affect subunit-level variables (e.g. the test
scores of each student in each school) and vice versa. To address causal
questions with hierarchical data, we propose hierarchical causal models, which
extend structural causal models and causal graphical models by adding inner
plates. We develop a general graphical identification technique for
hierarchical causal models that extends do-calculus. We find many situations in
which hierarchical data can enable causal identification even when it would be
impossible with non-hierarchical data, that is, if we had only unit-level
summaries of subunit-level variables (e.g. the school's average test score,
rather than each student's score). We develop estimation techniques for
hierarchical causal models, using methods including hierarchical Bayesian
models. We illustrate our results in simulation and via a reanalysis of the
classic ""eight schools"" study.","['Eli N. Weinstein', 'David M. Blei']","['stat.ME', 'stat.ML']",2024-01-10 18:56:44+00:00
http://arxiv.org/abs/2401.05244v1,Reliability Analysis of Complex Systems using Subset Simulations with Hamiltonian Neural Networks,"We present a new Subset Simulation approach using Hamiltonian neural
network-based Monte Carlo sampling for reliability analysis. The proposed
strategy combines the superior sampling of the Hamiltonian Monte Carlo method
with computationally efficient gradient evaluations using Hamiltonian neural
networks. This combination is especially advantageous because the neural
network architecture conserves the Hamiltonian, which defines the acceptance
criteria of the Hamiltonian Monte Carlo sampler. Hence, this strategy achieves
high acceptance rates at low computational cost. Our approach estimates small
failure probabilities using Subset Simulations. However, in low-probability
sample regions, the gradient evaluation is particularly challenging. The
remarkable accuracy of the proposed strategy is demonstrated on different
reliability problems, and its efficiency is compared to the traditional
Hamiltonian Monte Carlo method. We note that this approach can reach its
limitations for gradient estimations in low-probability regions of complex and
high-dimensional distributions. Thus, we propose techniques to improve gradient
prediction in these particular situations and enable accurate estimations of
the probability of failure. The highlight of this study is the reliability
analysis of a system whose parameter distributions must be inferred with
Bayesian inference problems. In such a case, the Hamiltonian Monte Carlo method
requires a full model evaluation for each gradient evaluation and, therefore,
comes at a very high cost. However, using Hamiltonian neural networks in this
framework replaces the expensive model evaluation, resulting in tremendous
improvements in computational efficiency.","['Denny Thaler', 'Somayajulu L. N. Dhulipala', 'Franz Bamer', 'Bernd Markert', 'Michael D. Shields']","['stat.ML', 'cs.LG', 'stat.AP', 'stat.CO']",2024-01-10 16:15:42+00:00
http://arxiv.org/abs/2401.05233v1,"Taming ""data-hungry"" reinforcement learning? Stability in continuous state-action spaces","We introduce a novel framework for analyzing reinforcement learning (RL) in
continuous state-action spaces, and use it to prove fast rates of convergence
in both off-line and on-line settings. Our analysis highlights two key
stability properties, relating to how changes in value functions and/or
policies affect the Bellman operator and occupation measures. We argue that
these properties are satisfied in many continuous state-action Markov decision
processes, and demonstrate how they arise naturally when using linear function
approximation methods. Our analysis offers fresh perspectives on the roles of
pessimism and optimism in off-line and on-line RL, and highlights the
connection between off-line RL and transfer learning.","['Yaqi Duan', 'Martin J. Wainwright']","['cs.LG', 'cs.IT', 'cs.SY', 'eess.SY', 'math.IT', 'math.OC', 'stat.ML']",2024-01-10 16:01:08+00:00
http://arxiv.org/abs/2401.05193v1,Experiment Planning with Function Approximation,"We study the problem of experiment planning with function approximation in
contextual bandit problems. In settings where there is a significant overhead
to deploying adaptive algorithms -- for example, when the execution of the data
collection policies is required to be distributed, or a human in the loop is
needed to implement these policies -- producing in advance a set of policies
for data collection is paramount. We study the setting where a large dataset of
contexts but not rewards is available and may be used by the learner to design
an effective data collection strategy. Although when rewards are linear this
problem has been well studied, results are still missing for more complex
reward models. In this work we propose two experiment planning strategies
compatible with function approximation. The first is an eluder planning and
sampling procedure that can recover optimality guarantees depending on the
eluder dimension of the reward function class. For the second, we show that a
uniform sampler achieves competitive optimality rates in the setting where the
number of actions is small. We finalize our results introducing a statistical
gap fleshing out the fundamental differences between planning and adaptive
learning and provide results for planning with model selection.","['Aldo Pacchiano', 'Jonathan N. Lee', 'Emma Brunskill']","['cs.LG', 'cs.AI', 'stat.ML']",2024-01-10 14:40:23+00:00
http://arxiv.org/abs/2401.04933v1,Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection,"While likelihood is attractive in theory, its estimates by deep generative
models (DGMs) are often broken in practice, and perform poorly for out of
distribution (OOD) Detection. Various recent works started to consider
alternative scores and achieved better performances. However, such recipes do
not come with provable guarantees, nor is it clear that their choices extract
sufficient information.
  We attempt to change this by conducting a case study on variational
autoencoders (VAEs). First, we introduce the likelihood path (LPath) principle,
generalizing the likelihood principle. This narrows the search for informative
summary statistics down to the minimal sufficient statistics of VAEs'
conditional likelihoods. Second, introducing new theoretic tools such as nearly
essential support, essential distance and co-Lipschitzness, we obtain
non-asymptotic provable OOD detection guarantees for certain distillation of
the minimal sufficient statistics. The corresponding LPath algorithm
demonstrates SOTA performances, even using simple and small VAEs with poor
likelihood estimates. To our best knowledge, this is the first provable
unsupervised OOD method that delivers excellent empirical results, better than
any other VAEs based techniques. We use the same model as
\cite{xiao2020likelihood}, open sourced from:
https://github.com/XavierXiao/Likelihood-Regret","['Sicong Huang', 'Jiawei He', 'Kry Yik Chau Lui']","['cs.LG', 'stat.ML']",2024-01-10 05:07:14+00:00
http://arxiv.org/abs/2401.04900v1,SPT: Spectral Transformer for Red Giant Stars Age and Mass Estimation,"The age and mass of red giants are essential for understanding the structure
and evolution of the Milky Way. Traditional isochrone methods for these
estimations are inherently limited due to overlapping isochrones in the
Hertzsprung-Russell diagram, while asteroseismology, though more precise,
requires high-precision, long-term observations. In response to these
challenges, we developed a novel framework, Spectral Transformer (SPT), to
predict the age and mass of red giants aligned with asteroseismology from their
spectra. A key component of SPT, the Multi-head Hadamard Self-Attention
mechanism, designed specifically for spectra, can capture complex relationships
across different wavelength. Further, we introduced a Mahalanobis
distance-based loss function to address scale imbalance and interaction mode
loss, and incorporated Monte Carlo dropout for quantitative analysis of
prediction uncertainty.Trained and tested on 3,880 red giant spectra from
LAMOST, the SPT achieved remarkable age and mass estimations with average
percentage errors of 17.64% and 6.61%, respectively, and provided uncertainties
for each corresponding prediction. The results significantly outperform those
of traditional machine learning algorithms and demonstrate a high level of
consistency with asteroseismology methods and isochrone fitting techniques. In
the future, our work will leverage datasets from the Chinese Space Station
Telescope and the Large Synoptic Survey Telescope to enhance the precision of
the model and broaden its applicability in the field of astronomy and
astrophysics.","['Mengmeng Zhang', 'Fan Wu', 'Yude Bu', 'Shanshan Li', 'Zhenping Yi', 'Meng Liu', 'Xiaoming Kong']","['astro-ph.SR', 'astro-ph.IM', 'cs.LG', 'stat.ML']",2024-01-10 03:03:12+00:00
http://arxiv.org/abs/2401.04890v1,"Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse Actions, Interventions and Sparse Temporal Dependencies","This work introduces a novel principle for disentanglement we call mechanism
sparsity regularization, which applies when the latent factors of interest
depend sparsely on observed auxiliary variables and/or past latent factors. We
propose a representation learning method that induces disentanglement by
simultaneously learning the latent factors and the sparse causal graphical
model that explains them. We develop a nonparametric identifiability theory
that formalizes this principle and shows that the latent factors can be
recovered by regularizing the learned causal graph to be sparse. More
precisely, we show identifiablity up to a novel equivalence relation we call
""consistency"", which allows some latent factors to remain entangled (hence the
term partial disentanglement). To describe the structure of this entanglement,
we introduce the notions of entanglement graphs and graph preserving functions.
We further provide a graphical criterion which guarantees complete
disentanglement, that is identifiability up to permutations and element-wise
transformations. We demonstrate the scope of the mechanism sparsity principle
as well as the assumptions it relies on with several worked out examples. For
instance, the framework shows how one can leverage multi-node interventions
with unknown targets on the latent factors to disentangle them. We further draw
connections between our nonparametric results and the now popular exponential
family assumption. Lastly, we propose an estimation procedure based on
variational autoencoders and a sparsity constraint and demonstrate it on
various synthetic datasets. This work is meant to be a significantly extended
version of Lachapelle et al. (2022).","['Sébastien Lachapelle', 'Pau Rodríguez López', 'Yash Sharma', 'Katie Everett', 'Rémi Le Priol', 'Alexandre Lacoste', 'Simon Lacoste-Julien']","['stat.ML', 'cs.LG', 'I.2.6; I.5.1']",2024-01-10 02:38:21+00:00
http://arxiv.org/abs/2401.04874v1,Feature Network Methods in Machine Learning and Applications,"A machine learning (ML) feature network is a graph that connects ML features
in learning tasks based on their similarity. This network representation allows
us to view feature vectors as functions on the network. By leveraging function
operations from Fourier analysis and from functional analysis, one can easily
generate new and novel features, making use of the graph structure imposed on
the feature vectors. Such network structures have previously been studied
implicitly in image processing and computational biology. We thus describe
feature networks as graph structures imposed on feature vectors, and provide
applications in machine learning. One application involves graph-based
generalizations of convolutional neural networks, involving structured deep
learning with hierarchical representations of features that have varying depth
or complexity. This extends also to learning algorithms that are able to
generate useful new multilevel features. Additionally, we discuss the use of
feature networks to engineer new features, which can enhance the expressiveness
of the model. We give a specific example of a deep tree-structured feature
network, where hierarchical connections are formed through feature clustering
and feed-forward learning. This results in low learning complexity and
computational efficiency. Unlike ""standard"" neural features which are limited
to modulated (thresholded) linear combinations of adjacent ones, feature
networks offer more general feedforward dependencies among features. For
example, radial basis functions or graph structure-based dependencies between
features can be utilized.","['Xinying Mu', 'Mark Kon']","['stat.ML', 'cs.LG']",2024-01-10 01:57:12+00:00
http://arxiv.org/abs/2401.04856v2,A Good Score Does not Lead to A Good Generative Model,"Score-based Generative Models (SGMs) is one leading method in generative
modeling, renowned for their ability to generate high-quality samples from
complex, high-dimensional data distributions. The method enjoys empirical
success and is supported by rigorous theoretical convergence properties. In
particular, it has been shown that SGMs can generate samples from a
distribution that is close to the ground-truth if the underlying score function
is learned well, suggesting the success of SGM as a generative model. We
provide a counter-example in this paper. Through the sample complexity
argument, we provide one specific setting where the score function is learned
well. Yet, SGMs in this setting can only output samples that are Gaussian
blurring of training data points, mimicking the effects of kernel density
estimation. The finding resonates a series of recent finding that reveal that
SGMs can demonstrate strong memorization effect and fail to generate.","['Sixu Li', 'Shi Chen', 'Qin Li']","['cs.LG', 'stat.ML']",2024-01-10 00:17:36+00:00
http://arxiv.org/abs/2401.04847v2,On the Correctness of the Generalized Isotonic Recursive Partitioning Algorithm,"This paper presents an in-depth analysis of the generalized isotonic
recursive partitioning (GIRP) algorithm for fitting isotonic models under
separable convex losses, proposed by Luss and Rosset [J. Comput. Graph.
Statist., 23 (2014), pp. 192--201] for differentiable losses and extended by
Painsky and Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp.
308-321] for nondifferentiable losses. The GIRP algorithm poseses an attractive
feature that in each step of the algorithm, the intermediate solution satisfies
the isotonicity constraint. The paper begins with an example showing that the
GIRP algorithm as described in the literature may fail to produce an isotonic
model, suggesting that the existence and uniqueness of the solution to the
isotonic regression problem must be carefully addressed. It proceeds with
showing that, among possibly many solutions, there indeed exists a solution
that can be found by recursive binary partitioning of the set of observed data.
A small modification of the GIRP algorithm suffices to obtain a correct
solution and preserve the desired property that all the intermediate solutions
are isotonic. This proposed modification includes a proper choice of
intermediate solutions and a simplification of the partitioning step from
ternary to binary.","['Joong-Ho Won', 'Jihan Jung']","['stat.ML', 'cs.LG', 'math.OC']",2024-01-09 23:17:05+00:00
http://arxiv.org/abs/2401.04778v2,Generative neural networks for characteristic functions,"We provide a simulation algorithm to simulate from a (multivariate)
characteristic function, which is only accessible in a black-box format. The
method is based on a generative neural network, whose loss function exploits a
specific representation of the Maximum-Mean-Discrepancy metric to directly
incorporate the targeted characteristic function. The algorithm is universal in
the sense that it is independent of the dimension and that it does not require
any assumptions on the given characteristic function. Furthermore, finite
sample guarantees on the approximation quality in terms of the Maximum-Mean
Discrepancy metric are derived. The method is illustrated in a simulation
study.",['Florian Brück'],"['stat.ML', 'cs.LG', 'stat.ME']",2024-01-09 19:07:59+00:00
http://arxiv.org/abs/2401.04682v1,Mixture of multilayer stochastic block models for multiview clustering,"In this work, we propose an original method for aggregating multiple
clustering coming from different sources of information. Each partition is
encoded by a co-membership matrix between observations. Our approach uses a
mixture of multilayer Stochastic Block Models (SBM) to group co-membership
matrices with similar information into components and to partition observations
into different clusters, taking into account their specificities within the
components. The identifiability of the model parameters is established and a
variational Bayesian EM algorithm is proposed for the estimation of these
parameters. The Bayesian framework allows for selecting an optimal number of
clusters and components. The proposed approach is compared using synthetic data
with consensus clustering and tensor-based algorithms for community detection
in large-scale complex networks. Finally, the method is utilized to analyze
global food trading networks, leading to structures of interest.","['Kylliann De Santiago', 'Marie Szafranski', 'Christophe Ambroise']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2024-01-09 17:15:47+00:00
http://arxiv.org/abs/2401.04553v1,Linear Recursive Feature Machines provably recover low-rank matrices,"A fundamental problem in machine learning is to understand how neural
networks make accurate predictions, while seemingly bypassing the curse of
dimensionality. A possible explanation is that common training algorithms for
neural networks implicitly perform dimensionality reduction - a process called
feature learning. Recent work posited that the effects of feature learning can
be elicited from a classical statistical estimator called the average gradient
outer product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as
an algorithm that explicitly performs feature learning by alternating between
(1) reweighting the feature vectors by the AGOP and (2) learning the prediction
function in the transformed space. In this work, we develop the first
theoretical guarantees for how RFM performs dimensionality reduction by
focusing on the class of overparametrized problems arising in sparse linear
regression and low-rank matrix recovery. Specifically, we show that RFM
restricted to linear models (lin-RFM) generalizes the well-studied Iteratively
Reweighted Least Squares (IRLS) algorithm. Our results shed light on the
connection between feature learning in neural networks and classical sparse
recovery algorithms. In addition, we provide an implementation of lin-RFM that
scales to matrices with millions of missing entries. Our implementation is
faster than the standard IRLS algorithm as it is SVD-free. It also outperforms
deep linear networks for sparse linear regression and low-rank matrix
completion.","['Adityanarayanan Radhakrishnan', 'Mikhail Belkin', 'Dmitriy Drusvyatskiy']","['stat.ML', 'cs.LG']",2024-01-09 13:44:12+00:00
http://arxiv.org/abs/2401.04535v1,"Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection and Beyond","We propose SDORE, a semi-supervised deep Sobolev regressor, for the
nonparametric estimation of the underlying regression function and its
gradient. SDORE employs deep neural networks to minimize empirical risk with
gradient norm regularization, allowing computation of the gradient norm on
unlabeled data. We conduct a comprehensive analysis of the convergence rates of
SDORE and establish a minimax optimal rate for the regression function.
Crucially, we also derive a convergence rate for the associated plug-in
gradient estimator, even in the presence of significant domain shift. These
theoretical findings offer valuable prior guidance for selecting regularization
parameters and determining the size of the neural network, while showcasing the
provable advantage of leveraging unlabeled data in semi-supervised learning. To
the best of our knowledge, SDORE is the first provable neural network-based
approach that simultaneously estimates the regression function and its
gradient, with diverse applications including nonparametric variable selection
and inverse problems. The effectiveness of SDORE is validated through an
extensive range of numerical simulations and real data analysis.","['Zhao Ding', 'Chenguang Duan', 'Yuling Jiao', 'Jerry Zhijian Yang']","['stat.ML', 'cs.LG', '62G05, 62G08, 65N21']",2024-01-09 13:10:30+00:00
http://arxiv.org/abs/2401.04372v3,Stable generative modeling using Schrödinger bridges,"We consider the problem of sampling from an unknown distribution for which
only a sufficiently large number of training samples are available. Such
settings have recently drawn considerable interest in the context of generative
modelling and Bayesian inference. In this paper, we propose a generative model
combining Schr\""odinger bridges and Langevin dynamics. Schr\""odinger bridges
over an appropriate reversible reference process are used to approximate the
conditional transition probability from the available training samples, which
is then implemented in a discrete-time reversible Langevin sampler to generate
new samples. By setting the kernel bandwidth in the reference process to match
the time step size used in the unadjusted Langevin algorithm, our method
effectively circumvents any stability issues typically associated with the
time-stepping of stiff stochastic differential equations. Moreover, we
introduce a novel split-step scheme, ensuring that the generated samples remain
within the convex hull of the training samples. Our framework can be naturally
extended to generate conditional samples and to Bayesian inference problems. We
demonstrate the performance of our proposed scheme through experiments on
synthetic datasets with increasing dimensions and on a stochastic subgrid-scale
parametrization conditional sampling problem as well as generating sample
trajectories of a dynamical system using conditional sampling.","['Georg A. Gottwald', 'Fengyi Li', 'Youssef Marzouk', 'Sebastian Reich']","['stat.ML', 'cs.LG', 'cs.NA', 'math.NA', 'stat.CO', '60H10, 62F15, 62F30, 65C05, 65C40']",2024-01-09 06:15:45+00:00
http://arxiv.org/abs/2401.04286v2,Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes,"In this paper, we prove the universal consistency of wide and deep ReLU
neural network classifiers trained on the logistic loss. We also give
sufficient conditions for a class of probability measures for which classifiers
based on neural networks achieve minimax optimal rates of convergence. The
result applies to a wide range of known function classes. In particular, while
most previous works impose explicit smoothness assumptions on the regression
function, our framework encompasses more general settings. The proposed neural
networks are either the minimizers of the logistic loss or the $0$-$1$ loss. In
the former case, they are interpolating classifiers that exhibit a benign
overfitting behavior.","['Hyunouk Ko', 'Xiaoming Huo']","['stat.ML', 'cs.LG']",2024-01-08 23:54:46+00:00
http://arxiv.org/abs/2401.04280v2,Predicting the structure of dynamic graphs,"Many aspects of graphs have been studied in depth. However, forecasting the
structure of a graph at future time steps incorporating unseen, new nodes and
edges has not gained much attention. In this paper, we present such an
approach. Using a time series of graphs, we forecast graphs at future time
steps. We use time series forecasting methods to predict the node degree at
future time points and combine these forecasts with flux balance analysis -- a
linear programming method used in biochemistry -- to obtain the structure of
future graphs. We evaluate this approach using synthetic and real-world
datasets and demonstrate its utility and applicability.","['Sevvandi Kandanaarachchi', 'Ziqi Xu', 'Stefan Westerlund']","['cs.LG', 'cs.SI', 'stat.ML']",2024-01-08 23:25:43+00:00
http://arxiv.org/abs/2401.04082v2,Improved motif-scaffolding with SE(3) flow matching,"Protein design often begins with the knowledge of a desired function from a
motif which motif-scaffolding aims to construct a functional protein around.
Recently, generative models have achieved breakthrough success in designing
scaffolds for a range of motifs. However, generated scaffolds tend to lack
structural diversity, which can hinder success in wet-lab validation. In this
work, we extend FrameFlow, an SE(3) flow matching model for protein backbone
generation, to perform motif-scaffolding with two complementary approaches. The
first is motif amortization, in which FrameFlow is trained with the motif as
input using a data augmentation strategy. The second is motif guidance, which
performs scaffolding using an estimate of the conditional score from FrameFlow
without additional training. On a benchmark of 24 biologically meaningful
motifs, we show our method achieves 2.5 times more designable and unique
motif-scaffolds compared to state-of-the-art. Code:
https://github.com/microsoft/protein-frame-flow","['Jason Yim', 'Andrew Campbell', 'Emile Mathieu', 'Andrew Y. K. Foong', 'Michael Gastegger', 'José Jiménez-Luna', 'Sarah Lewis', 'Victor Garcia Satorras', 'Bastiaan S. Veeling', 'Frank Noé', 'Regina Barzilay', 'Tommi S. Jaakkola']","['q-bio.QM', 'cs.LG', 'stat.ML']",2024-01-08 18:38:00+00:00
http://arxiv.org/abs/2401.04071v4,Fun with Flags: Robust Principal Directions via Flag Manifolds,"Principal component analysis (PCA), along with its extensions to manifolds
and outlier contaminated data, have been indispensable in computer vision and
machine learning. In this work, we present a unifying formalism for PCA and its
variants, and introduce a framework based on the flags of linear subspaces, ie
a hierarchy of nested linear subspaces of increasing dimension, which not only
allows for a common implementation but also yields novel variants, not explored
previously. We begin by generalizing traditional PCA methods that either
maximize variance or minimize reconstruction error. We expand these
interpretations to develop a wide array of new dimensionality reduction
algorithms by accounting for outliers and the data manifold. To devise a common
computational approach, we recast robust and dual forms of PCA as optimization
problems on flag manifolds. We then integrate tangent space approximations of
principal geodesic analysis (tangent-PCA) into this flag-based framework,
creating novel robust and dual geodesic PCA variations. The remarkable
flexibility offered by the 'flagification' introduced here enables even more
algorithmic variants identified by specific flag types. Last but not least, we
propose an effective convergent solver for these flag-formulations employing
the Stiefel manifold. Our empirical results on both real-world and synthetic
scenarios, demonstrate the superiority of our novel algorithms, especially in
terms of robustness to outliers on manifolds.","['Nathan Mankovich', 'Gustau Camps-Valls', 'Tolga Birdal']","['cs.CV', 'cs.LG', 'math.DG', 'math.OC', 'stat.ML']",2024-01-08 18:18:02+00:00
http://arxiv.org/abs/2401.04013v1,Weak Correlations as the Underlying Principle for Linearization of Gradient-Based Learning Systems,"Deep learning models, such as wide neural networks, can be conceptualized as
nonlinear dynamical physical systems characterized by a multitude of
interacting degrees of freedom. Such systems in the infinite limit, tend to
exhibit simplified dynamics. This paper delves into gradient descent-based
learning algorithms, that display a linear structure in their parameter
dynamics, reminiscent of the neural tangent kernel. We establish this apparent
linearity arises due to weak correlations between the first and higher-order
derivatives of the hypothesis function, concerning the parameters, taken around
their initial values. This insight suggests that these weak correlations could
be the underlying reason for the observed linearization in such systems. As a
case in point, we showcase this weak correlations structure within neural
networks in the large width limit. Exploiting the relationship between
linearity and weak correlations, we derive a bound on deviations from linearity
observed during the training trajectory of stochastic gradient descent. To
facilitate our proof, we introduce a novel method to characterise the
asymptotic behavior of random tensors.","['Ori Shem-Ur', 'Yaron Oz']","['cs.LG', 'cond-mat.stat-mech', 'hep-th', 'math.PR', 'stat.ML']",2024-01-08 16:44:23+00:00
http://arxiv.org/abs/2401.03923v1,A non-asymptotic distributional theory of approximate message passing for sparse and robust regression,"Characterizing the distribution of high-dimensional statistical estimators is
a challenging task, due to the breakdown of classical asymptotic theory in high
dimension. This paper makes progress towards this by developing non-asymptotic
distributional characterizations for approximate message passing (AMP) -- a
family of iterative algorithms that prove effective as both fast estimators and
powerful theoretical machinery -- for both sparse and robust regression. Prior
AMP theory, which focused on high-dimensional asymptotics for the most part,
failed to describe the behavior of AMP when the number of iterations exceeds
$o\big({\log n}/{\log \log n}\big)$ (with $n$ the sample size). We establish
the first finite-sample non-asymptotic distributional theory of AMP for both
sparse and robust regression that accommodates a polynomial number of
iterations. Our results derive approximate accuracy of Gaussian approximation
of the AMP iterates, which improves upon all prior results and implies enhanced
distributional characterizations for both optimally tuned Lasso and robust
M-estimator.","['Gen Li', 'Yuting Wei']","['math.ST', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT', 'stat.ML', 'stat.TH']",2024-01-08 14:34:35+00:00
http://arxiv.org/abs/2401.03921v1,Design a Metric Robust to Complicated High Dimensional Noise for Efficient Manifold Denoising,"In this manuscript, we propose an efficient manifold denoiser based on
landmark diffusion and optimal shrinkage under the complicated high dimensional
noise and compact manifold setup. It is flexible to handle several setups,
including the high ambient space dimension with a manifold embedding that
occupies a subspace of high or low dimensions, and the noise could be colored
and dependent. A systematic comparison with other existing algorithms on both
simulated and real datasets is provided. This manuscript is mainly algorithmic
and we report several existing tools and numerical results. Theoretical
guarantees and more comparisons will be reported in the official paper of this
manuscript.",['Hau-Tieng Wu'],"['stat.ML', 'cs.LG', 'stat.AP']",2024-01-08 14:30:47+00:00
http://arxiv.org/abs/2401.03893v1,Finite-Time Decoupled Convergence in Nonlinear Two-Time-Scale Stochastic Approximation,"In two-time-scale stochastic approximation (SA), two iterates are updated at
varying speeds using different step sizes, with each update influencing the
other. Previous studies in linear two-time-scale SA have found that the
convergence rates of the mean-square errors for these updates are dependent
solely on their respective step sizes, leading to what is referred to as
decoupled convergence. However, the possibility of achieving this decoupled
convergence in nonlinear SA remains less understood. Our research explores the
potential for finite-time decoupled convergence in nonlinear two-time-scale SA.
We find that under a weaker Lipschitz condition, traditional analyses are
insufficient for achieving decoupled convergence. This finding is further
numerically supported by a counterexample. But by introducing an additional
condition of nested local linearity, we show that decoupled convergence is
still feasible, contingent on the appropriate choice of step sizes associated
with smoothness parameters. Our analysis depends on a refined characterization
of the matrix cross term between the two iterates and utilizes fourth-order
moments to control higher-order approximation errors induced by the local
linearity assumption.","['Yuze Han', 'Xiang Li', 'Zhihua Zhang']","['math.OC', 'stat.ML']",2024-01-08 13:44:35+00:00
http://arxiv.org/abs/2401.03892v3,Sampling in Unit Time with Kernel Fisher-Rao Flow,"We introduce a new mean-field ODE and corresponding interacting particle
systems (IPS) for sampling from an unnormalized target density. The IPS are
gradient-free, available in closed form, and only require the ability to sample
from a reference density and compute the (unnormalized) target-to-reference
density ratio. The mean-field ODE is obtained by solving a Poisson equation for
a velocity field that transports samples along the geometric mixture of the two
densities, which is the path of a particular Fisher-Rao gradient flow. We
employ a RKHS ansatz for the velocity field, which makes the Poisson equation
tractable and enables discretization of the resulting mean-field ODE over
finite samples. The mean-field ODE can be additionally be derived from a
discrete-time perspective as the limit of successive linearizations of the
Monge-Amp\`ere equations within a framework known as sample-driven optimal
transport. We introduce a stochastic variant of our approach and demonstrate
empirically that our IPS can produce high-quality samples from varied target
distributions, outperforming comparable gradient-free particle systems and
competitive with gradient-based alternatives.","['Aimee Maurais', 'Youssef Marzouk']","['stat.CO', 'cs.LG', 'stat.ML']",2024-01-08 13:43:56+00:00
http://arxiv.org/abs/2401.03824v1,A topological description of loss surfaces based on Betti Numbers,"In the context of deep learning models, attention has recently been paid to
studying the surface of the loss function in order to better understand
training with methods based on gradient descent. This search for an appropriate
description, both analytical and topological, has led to numerous efforts to
identify spurious minima and characterize gradient dynamics. Our work aims to
contribute to this field by providing a topological measure to evaluate loss
complexity in the case of multilayer neural networks. We compare deep and
shallow architectures with common sigmoidal activation functions by deriving
upper and lower bounds on the complexity of their loss function and revealing
how that complexity is influenced by the number of hidden units, training
models, and the activation function used. Additionally, we found that certain
variations in the loss function or model architecture, such as adding an
$\ell_2$ regularization term or implementing skip connections in a feedforward
network, do not affect loss topology in specific cases.","['Maria Sofia Bucarelli', ""Giuseppe Alessio D'Inverno"", 'Monica Bianchini', 'Franco Scarselli', 'Fabrizio Silvestri']","['cs.LG', 'stat.ML']",2024-01-08 11:20:04+00:00
http://arxiv.org/abs/2401.03820v2,Optimal Differentially Private PCA and Estimation for Spiked Covariance Matrices,"Estimating a covariance matrix and its associated principal components is a
fundamental problem in contemporary statistics. While optimal estimation
procedures have been developed with well-understood properties, the increasing
demand for privacy preservation introduces new complexities to this classical
problem. In this paper, we study optimal differentially private Principal
Component Analysis (PCA) and covariance estimation within the spiked covariance
model. We precisely characterize the sensitivity of eigenvalues and
eigenvectors under this model and establish the minimax rates of convergence
for estimating both the principal components and covariance matrix. These rates
hold up to logarithmic factors and encompass general Schatten norms, including
spectral norm, Frobenius norm, and nuclear norm as special cases. We propose
computationally efficient differentially private estimators and prove their
minimax optimality for sub-Gaussian distributions, up to logarithmic factors.
Additionally, matching minimax lower bounds are established. Notably, compared
to the existing literature, our results accommodate a diverging rank, a broader
range of signal strengths, and remain valid even when the sample size is much
smaller than the dimension, provided the signal strength is sufficiently
strong. Both simulation studies and real data experiments demonstrate the
merits of our method.","['T. Tony Cai', 'Dong Xia', 'Mengyue Zha']","['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']",2024-01-08 11:18:14+00:00
http://arxiv.org/abs/2401.03756v3,Adaptive Experimental Design for Policy Learning,"Evidence-based targeting has been a topic of growing interest among the
practitioners of policy and business. Formulating decision-maker's policy
learning as a fixed-budget best arm identification (BAI) problem with
contextual information, we study an optimal adaptive experimental design for
policy learning with multiple treatment arms. In the sampling stage, the
planner assigns treatment arms adaptively over sequentially arriving
experimental units upon observing their contextual information (covariates).
After the experiment, the planner recommends an individualized assignment rule
to the population. Setting the worst-case expected regret as the performance
criterion of adaptive sampling and recommended policies, we derive its
asymptotic lower bounds, and propose a strategy, Adaptive Sampling-Policy
Learning strategy (PLAS), whose leading factor of the regret upper bound aligns
with the lower bound as the size of experimental units increases.","['Masahiro Kato', 'Kyohei Okumura', 'Takuya Ishihara', 'Toru Kitagawa']","['cs.LG', 'cs.AI', 'econ.EM', 'stat.ME', 'stat.ML']",2024-01-08 09:29:07+00:00
http://arxiv.org/abs/2401.03482v2,Uncertainty Quantification on Clinical Trial Outcome Prediction,"The importance of uncertainty quantification is increasingly recognized in
the diverse field of machine learning. Accurately assessing model prediction
uncertainty can help provide deeper understanding and confidence for
researchers and practitioners. This is especially critical in medical diagnosis
and drug discovery areas, where reliable predictions directly impact research
quality and patient health.
  In this paper, we proposed incorporating uncertainty quantification into
clinical trial outcome predictions. Our main goal is to enhance the model's
ability to discern nuanced differences, thereby significantly improving its
overall performance.
  We have adopted a selective classification approach to fulfill our objective,
integrating it seamlessly with the Hierarchical Interaction Network (HINT),
which is at the forefront of clinical trial prediction modeling. Selective
classification, encompassing a spectrum of methods for uncertainty
quantification, empowers the model to withhold decision-making in the face of
samples marked by ambiguity or low confidence, thereby amplifying the accuracy
of predictions for the instances it chooses to classify. A series of
comprehensive experiments demonstrate that incorporating selective
classification into clinical trial predictions markedly enhances the model's
performance, as evidenced by significant upticks in pivotal metrics such as
PR-AUC, F1, ROC-AUC, and overall accuracy.
  Specifically, the proposed method achieved 32.37\%, 21.43\%, and 13.27\%
relative improvement on PR-AUC over the base model (HINT) in phase I, II, and
III trial outcome prediction, respectively. When predicting phase III, our
method reaches 0.9022 PR-AUC scores.
  These findings illustrate the robustness and prospective utility of this
strategy within the area of clinical trial predictions, potentially setting a
new benchmark in the field.","['Tianyi Chen', 'Yingzhou Lu', 'Nan Hao', 'Capucine Van Rechem', 'Jintai Chen', 'Tianfan Fu']","['cs.LG', 'stat.ML']",2024-01-07 13:48:05+00:00
http://arxiv.org/abs/2401.03350v1,Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks,"While graph neural networks (GNNs) are widely used for node and graph
representation learning tasks, the reliability of GNN uncertainty estimates
under distribution shifts remains relatively under-explored. Indeed, while
post-hoc calibration strategies can be used to improve in-distribution
calibration, they need not also improve calibration under distribution shift.
However, techniques which produce GNNs with better intrinsic uncertainty
estimates are particularly valuable, as they can always be combined with
post-hoc strategies later. Therefore, in this work, we propose G-$\Delta$UQ, a
novel training framework designed to improve intrinsic GNN uncertainty
estimates. Our framework adapts the principle of stochastic data centering to
graph data through novel graph anchoring strategies, and is able to support
partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic
networks are necessary to obtain reliable estimates, we find that the
functional diversity induced by our anchoring strategies when sampling
hypotheses renders this unnecessary and allows us to support G-$\Delta$UQ on
pretrained models. Indeed, through extensive evaluation under covariate,
concept and graph size shifts, we show that G-$\Delta$UQ leads to better
calibrated GNNs for node and graph classification. Further, it also improves
performance on the uncertainty-based tasks of out-of-distribution detection and
generalization gap estimation. Overall, our work provides insights into
uncertainty estimation for GNNs, and demonstrates the utility of G-$\Delta$UQ
in obtaining reliable estimates.","['Puja Trivedi', 'Mark Heimann', 'Rushil Anirudh', 'Danai Koutra', 'Jayaraman J. Thiagarajan']","['cs.LG', 'stat.ML']",2024-01-07 00:58:33+00:00
http://arxiv.org/abs/2401.03341v1,Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection,"Due to their unsupervised training and uncertainty estimation, deep
Variational Autoencoders (VAEs) have become powerful tools for
reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based
TSAD methods, either statistical or deep, tune meta-priors to estimate the
likelihood probability for effectively capturing spatiotemporal dependencies in
the data. However, these methods confront the challenge of inherent data
scarcity, which is often the case in anomaly detection tasks. Such scarcity
easily leads to latent holes, discontinuous regions in latent space, resulting
in non-robust reconstructions on these discontinuous spaces. We propose a novel
generative framework that combines VAEs with self-supervised learning (SSL) to
address this issue.","['Zhangkai Wu', 'Longbing Cao', 'Qi Zhang', 'Junxian Zhou', 'Hui Chen']","['cs.LG', 'stat.ML']",2024-01-07 00:23:05+00:00
http://arxiv.org/abs/2401.03302v3,Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT,"In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.","['Seyed Mohammad Hossein Hashemi', 'Leila Safari', 'Amirhossein Dadashzadeh Taromi']","['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG', 'stat.ML']",2024-01-06 20:53:02+00:00
http://arxiv.org/abs/2401.03301v2,"On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond","We seek to understand what facilitates sample-efficient learning from
historical datasets for sequential decision-making, a problem that is popularly
known as offline reinforcement learning (RL). Further, we are interested in
algorithms that enjoy sample efficiency while leveraging (value) function
approximation. In this paper, we address these fundamental questions by (i)
proposing a notion of data diversity that subsumes the previous notions of
coverage measures in offline RL and (ii) using this notion to {unify} three
distinct classes of offline RL algorithms based on version spaces (VS),
regularized optimization (RO), and posterior sampling (PS). We establish that
VS-based, RO-based, and PS-based algorithms, under standard assumptions,
achieve \emph{comparable} sample efficiency, which recovers the
state-of-the-art sub-optimality bounds for finite and linear model classes with
the standard assumptions. This result is surprising, given that the prior work
suggested an unfavorable sample complexity of the RO-based algorithm compared
to the VS-based algorithm, whereas posterior sampling is rarely considered in
offline RL due to its explorative nature. Notably, our proposed model-free
PS-based algorithm for offline RL is {novel}, with sub-optimality bounds that
are {frequentist} (i.e., worst-case) in nature.","['Thanh Nguyen-Tang', 'Raman Arora']","['cs.LG', 'cs.AI', 'stat.ML']",2024-01-06 20:52:04+00:00
http://arxiv.org/abs/2401.03251v1,TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in End-to-End ASR,"Confidence estimation of predictions from an End-to-End (E2E) Automatic
Speech Recognition (ASR) model benefits ASR's downstream and upstream tasks.
Class-probability-based confidence scores do not accurately represent the
quality of overconfident ASR predictions. An ancillary Confidence Estimation
Model (CEM) calibrates the predictions. State-of-the-art (SOTA) solutions use
binary target scores for CEM training. However, the binary labels do not reveal
the granular information of predicted words, such as temporal alignment between
reference and hypothesis and whether the predicted word is entirely incorrect
or contains spelling errors. Addressing this issue, we propose a novel
Temporal-Lexeme Similarity (TeLeS) confidence score to train CEM. To address
the data imbalance of target scores while training CEM, we use shrinkage loss
to focus on hard-to-learn data points and minimise the impact of easily learned
data points. We conduct experiments with ASR models trained in three languages,
namely Hindi, Tamil, and Kannada, with varying training data sizes. Experiments
show that TeLeS generalises well across domains. To demonstrate the
applicability of the proposed method, we formulate a TeLeS-based Acquisition
(TeLeS-A) function for sampling uncertainty in active learning. We observe a
significant reduction in the Word Error Rate (WER) as compared to SOTA methods.","['Nagarathna Ravi', 'Thishyan Raj T', 'Vipul Arora']","['eess.AS', 'cs.LG', 'cs.SD', 'stat.ML']",2024-01-06 16:29:13+00:00
http://arxiv.org/abs/2401.03248v1,Neuronal Temporal Filters as Normal Mode Extractors,"To generate actions in the face of physiological delays, the brain must
predict the future. Here we explore how prediction may lie at the core of brain
function by considering a neuron predicting the future of a scalar time series
input. Assuming that the dynamics of the lag vector (a vector composed of
several consecutive elements of the time series) are locally linear, Normal
Mode Decomposition decomposes the dynamics into independently evolving
(eigen-)modes allowing for straightforward prediction. We propose that a neuron
learns the top mode and projects its input onto the associated subspace. Under
this interpretation, the temporal filter of a neuron corresponds to the left
eigenvector of a generalized eigenvalue problem. We mathematically analyze the
operation of such an algorithm on noisy observations of synthetic data
generated by a linear system. Interestingly, the shape of the temporal filter
varies with the signal-to-noise ratio (SNR): a noisy input yields a monophasic
filter and a growing SNR leads to multiphasic filters with progressively
greater number of phases. Such variation in the temporal filter with input SNR
resembles that observed experimentally in biological neurons.","['Siavash Golkar', 'Jules Berman', 'David Lipshutz', 'Robert Mihai Haret', 'Tim Gollisch', 'Dmitri B. Chklovskii']","['q-bio.NC', 'cs.SY', 'eess.SY', 'nlin.CD', 'stat.ML']",2024-01-06 16:10:02+00:00
http://arxiv.org/abs/2401.03228v1,Reflected Schrödinger Bridge for Constrained Generative Modeling,"Diffusion models have become the go-to method for large-scale generative
models in real-world applications. These applications often involve data
distributions confined within bounded domains, typically requiring ad-hoc
thresholding techniques for boundary enforcement. Reflected diffusion models
(Lou23) aim to enhance generalizability by generating the data distribution
through a backward process governed by reflected Brownian motion. However,
reflected diffusion models may not easily adapt to diverse domains without the
derivation of proper diffeomorphic mappings and do not guarantee optimal
transport properties. To overcome these limitations, we introduce the Reflected
Schrodinger Bridge algorithm: an entropy-regularized optimal transport approach
tailored for generating data within diverse bounded domains. We derive elegant
reflected forward-backward stochastic differential equations with Neumann and
Robin boundary conditions, extend divergence-based likelihood training to
bounded domains, and explore natural connections to entropic optimal transport
for the study of approximate linear convergence - a valuable insight for
practical training. Our algorithm yields robust generative modeling in diverse
domains, and its scalability is demonstrated in real-world constrained
generative modeling through standard image benchmarks.","['Wei Deng', 'Yu Chen', 'Nicole Tianjiao Yang', 'Hengrong Du', 'Qi Feng', 'Ricky T. Q. Chen']","['stat.ML', 'cs.LG']",2024-01-06 14:39:58+00:00
http://arxiv.org/abs/2401.03206v1,A Robbins--Monro Sequence That Can Exploit Prior Information For Faster Convergence,"We propose a new method to improve the convergence speed of the Robbins-Monro
algorithm by introducing prior information about the target point into the
Robbins-Monro iteration. We achieve the incorporation of prior information
without the need of a -- potentially wrong -- regression model, which would
also entail additional constraints. We show that this prior-information
Robbins-Monro sequence is convergent for a wide range of prior distributions,
even wrong ones, such as Gaussian, weighted sum of Gaussians, e.g., in a kernel
density estimate, as well as bounded arbitrary distribution functions greater
than zero. We furthermore analyse the sequence numerically to understand its
performance and the influence of parameters. The results demonstrate that the
prior-information Robbins-Monro sequence converges faster than the standard
one, especially during the first steps, which are particularly important for
applications where the number of function measurements is limited, and when the
noise of observing the underlying function is large. We finally propose a rule
to select the parameters of the sequence.","['Siwei Liu', 'Ke Ma', 'Stephan M. Goetz']","['cs.LG', 'cs.NA', 'math.NA', 'math.OC', 'math.PR', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH', '62L20, 62L05, 62L10, 60G99, 60-08, 65B99, 65C99, 90C15']",2024-01-06 12:42:58+00:00
http://arxiv.org/abs/2401.03137v1,SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning,"Alleviating overestimation bias is a critical challenge for deep
reinforcement learning to achieve successful performance on more complex tasks
or offline datasets containing out-of-distribution data. In order to overcome
overestimation bias, ensemble methods for Q-learning have been investigated to
exploit the diversity of multiple Q-functions. Since network initialization has
been the predominant approach to promote diversity in Q-functions,
heuristically designed diversity injection methods have been studied in the
literature. However, previous studies have not attempted to approach guaranteed
independence over an ensemble from a theoretical perspective. By introducing a
novel regularization loss for Q-ensemble independence based on random matrix
theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR)
for reinforcement learning. Specifically, we modify the intractable hypothesis
testing criterion for the Q-ensemble independence into a tractable KL
divergence between the spectral distribution of the Q-ensemble and the target
Wigner's semicircle distribution. We implement SPQR in several online and
offline ensemble Q-learning algorithms. In the experiments, SPQR outperforms
the baseline algorithms in both online and offline RL benchmarks.","['Dohyeok Lee', 'Seungyub Han', 'Taehyun Cho', 'Jungwoo Lee']","['cs.LG', 'cs.AI', 'stat.ML']",2024-01-06 06:39:06+00:00
http://arxiv.org/abs/2401.03058v1,Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate,"Second-order optimization methods, such as cubic regularized Newton methods,
are known for their rapid convergence rates; nevertheless, they become
impractical in high-dimensional problems due to their substantial memory
requirements and computational costs. One promising approach is to execute
second-order updates within a lower-dimensional subspace, giving rise to
subspace second-order methods. However, the majority of existing subspace
second-order methods randomly select subspaces, consequently resulting in
slower convergence rates depending on the problem's dimension $d$. In this
paper, we introduce a novel subspace cubic regularized Newton method that
achieves a dimension-independent global convergence rate of
${O}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$ for solving convex optimization
problems. Here, $m$ represents the subspace dimension, which can be
significantly smaller than $d$. Instead of adopting a random subspace, our
primary innovation involves performing the cubic regularized Newton update
within the Krylov subspace associated with the Hessian and the gradient of the
objective function. This result marks the first instance of a
dimension-independent convergence rate for a subspace second-order method.
Furthermore, when specific spectral conditions of the Hessian are met, our
method recovers the convergence rate of a full-dimensional cubic regularized
Newton method. Numerical experiments show our method converges faster than
existing random subspace methods, especially for high-dimensional problems.","['Ruichen Jiang', 'Parameswaran Raman', 'Shoham Sabach', 'Aryan Mokhtari', 'Mingyi Hong', 'Volkan Cevher']","['math.OC', 'cs.LG', 'stat.ML']",2024-01-05 20:24:18+00:00
http://arxiv.org/abs/2401.02930v1,"Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery","We introduce Dagma-DCE, an interpretable and model-agnostic scheme for
differentiable causal discovery. Current non- or over-parametric methods in
differentiable causal discovery use opaque proxies of ``independence'' to
justify the inclusion or exclusion of a causal relationship. We show
theoretically and empirically that these proxies may be arbitrarily different
than the actual causal strength. Juxtaposed to existing differentiable causal
discovery algorithms, \textsc{Dagma-DCE} uses an interpretable measure of
causal strength to define weighted adjacency matrices. In a number of simulated
datasets, we show our method achieves state-of-the-art level performance. We
additionally show that \textsc{Dagma-DCE} allows for principled thresholding
and sparsity penalties by domain-experts. The code for our method is available
open-source at https://github.com/DanWaxman/DAGMA-DCE, and can easily be
adapted to arbitrary differentiable models.","['Daniel Waxman', 'Kurt Butler', 'Petar M. Djuric']","['cs.LG', 'stat.ME', 'stat.ML']",2024-01-05 18:15:19+00:00
http://arxiv.org/abs/2401.02904v1,Class-wise Generalization Error: an Information-Theoretic Analysis,"Existing generalization theories of supervised learning typically take a
holistic approach and provide bounds for the expected generalization over the
whole data distribution, which implicitly assumes that the model generalizes
similarly for all the classes. In practice, however, there are significant
variations in generalization performance among different classes, which cannot
be captured by the existing generalization bounds. In this work, we tackle this
problem by theoretically studying the class-generalization error, which
quantifies the generalization performance of each individual class. We derive a
novel information-theoretic bound for class-generalization error using the KL
divergence, and we further obtain several tighter bounds using the conditional
mutual information (CMI), which are significantly easier to estimate in
practice. We empirically validate our proposed bounds in different neural
networks and show that they accurately capture the complex class-generalization
error behavior. Moreover, we show that the theoretical tools developed in this
paper can be applied in several applications beyond this context.","['Firas Laakom', 'Yuheng Bu', 'Moncef Gabbouj']","['cs.LG', 'stat.ML']",2024-01-05 17:05:14+00:00
http://arxiv.org/abs/2401.02890v1,Nonlinear functional regression by functional deep neural network with kernel embedding,"With the rapid development of deep learning in various fields of science and
technology, such as speech recognition, image classification, and natural
language processing, recently it is also widely applied in the functional data
analysis (FDA) with some empirical success. However, due to the infinite
dimensional input, we need a powerful dimension reduction method for functional
learning tasks, especially for the nonlinear functional regression. In this
paper, based on the idea of smooth kernel integral transformation, we propose a
functional deep neural network with an efficient and fully data-dependent
dimension reduction method. The architecture of our functional net consists of
a kernel embedding step: an integral transformation with a data-dependent
smooth kernel; a projection step: a dimension reduction by projection with
eigenfunction basis based on the embedding kernel; and finally an expressive
deep ReLU neural network for the prediction. The utilization of smooth kernel
embedding enables our functional net to be discretization invariant, efficient,
and robust to noisy observations, capable of utilizing information in both
input functions and responses data, and have a low requirement on the number of
discrete points for an unimpaired generalization performance. We conduct
theoretical analysis including approximation error and generalization error
analysis, and numerical simulations to verify these advantages of our
functional net.","['Zhongjie Shi', 'Jun Fan', 'Linhao Song', 'Ding-Xuan Zhou', 'Johan A. K. Suykens']","['stat.ML', 'cs.LG']",2024-01-05 16:43:39+00:00
http://arxiv.org/abs/2401.02739v3,Denoising Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors,"We propose denoising diffusion variational inference (DDVI), a black-box
variational inference algorithm for latent variable models which relies on
diffusion models as flexible approximate posteriors. Specifically, our method
introduces an expressive class of diffusion-based variational posteriors that
perform iterative refinement in latent space; we train these posteriors with a
novel regularized evidence lower bound (ELBO) on the marginal likelihood
inspired by the wake-sleep algorithm. Our method is easy to implement (it fits
a regularized extension of the ELBO), is compatible with black-box variational
inference, and outperforms alternative classes of approximate posteriors based
on normalizing flows or adversarial networks. We find that DDVI improves
inference and learning in deep latent variable models across common benchmarks
as well as on a motivating task in biology -- inferring latent ancestry from
human genomes -- where it outperforms strong baselines on the Thousand Genomes
dataset.","['Top Piriyakulkij', 'Yingheng Wang', 'Volodymyr Kuleshov']","['cs.LG', 'q-bio.QM', 'stat.ML']",2024-01-05 10:27:44+00:00
http://arxiv.org/abs/2401.02736v2,On the numerical reliability of nonsmooth autodiff: a MaxPool case study,"This paper considers the reliability of automatic differentiation (AD) for
neural networks involving the nonsmooth MaxPool operation. We investigate the
behavior of AD across different precision levels (16, 32, 64 bits) and
convolutional architectures (LeNet, VGG, and ResNet) on various datasets
(MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent
research has shown that it coincides with the derivative almost everywhere,
even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the
other hand, in practice, AD operates with floating-point numbers (not real
numbers), and there is, therefore, a need to explore subsets on which AD can be
numerically incorrect. These subsets include a bifurcation zone (where AD is
incorrect over reals) and a compensation zone (where AD is incorrect over
floating-point numbers but correct over reals). Using SGD for the training
process, we study the impact of different choices of the nonsmooth Jacobian for
the MaxPool function on the precision of 16 and 32 bits. These findings suggest
that nonsmooth MaxPool Jacobians with lower norms help maintain stable and
efficient test accuracy, whereas those with higher norms can result in
instability and decreased performance. We also observe that the influence of
MaxPool's nonsmooth Jacobians on learning can be reduced by using batch
normalization, Adam-like optimizers, or increasing the precision level.",['Ryan Boustany'],"['cs.LG', 'cs.NA', 'math.NA', 'math.OC', 'stat.ML']",2024-01-05 10:14:39+00:00
http://arxiv.org/abs/2401.02735v1,Shared active subspace for multivariate vector-valued functions,"This paper proposes several approaches as baselines to compute a shared
active subspace for multivariate vector-valued functions. The goal is to
minimize the deviation between the function evaluations on the original space
and those on the reconstructed one. This is done either by manipulating the
gradients or the symmetric positive (semi-)definite (SPD) matrices computed
from the gradients of each component function so as to get a single structure
common to all component functions. These approaches can be applied to any data
irrespective of the underlying distribution unlike the existing vector-valued
approach that is constrained to a normal distribution. We test the
effectiveness of these methods on five optimization problems. The experiments
show that, in general, the SPD-level methods are superior to the gradient-level
ones, and are close to the vector-valued approach in the case of a normal
distribution. Interestingly, in most cases it suffices to take the sum of the
SPD matrices to identify the best shared active subspace.","['Khadija Musayeva', 'Mickael Binois']","['stat.ME', 'cs.LG', 'stat.ML']",2024-01-05 10:08:38+00:00
http://arxiv.org/abs/2401.02708v1,TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis,"A core challenge in survival analysis is to model the distribution of
censored time-to-event data, where the event of interest may be a death,
failure, or occurrence of a specific event. Previous studies have showed that
ranking and maximum likelihood estimation (MLE)loss functions are widely-used
for survival analysis. However, ranking loss only focus on the ranking of
survival time and does not consider potential effect of samples for exact
survival time values. Furthermore, the MLE is unbounded and easily subject to
outliers (e.g., censored data), which may cause poor performance of modeling.
To handle the complexities of learning process and exploit valuable survival
time values, we propose a time-adaptive coordinate loss function, TripleSurv,
to achieve adaptive adjustments by introducing the differences in the survival
time between sample pairs into the ranking, which can encourage the model to
quantitatively rank relative risk of pairs, ultimately enhancing the accuracy
of predictions. Most importantly, the TripleSurv is proficient in quantifying
the relative risk between samples by ranking ordering of pairs, and consider
the time interval as a trade-off to calibrate the robustness of model over
sample distribution. Our TripleSurv is evaluated on three real-world survival
datasets and a public synthetic dataset. The results show that our method
outperforms the state-of-the-art methods and exhibits good model performance
and robustness on modeling various sophisticated data distributions with
different censor rates. Our code will be available upon acceptance.","['Liwen Zhang', 'Lianzhen Zhong', 'Fan Yang', 'Di Dong', 'Hui Hui', 'Jie Tian']","['cs.LG', 'cs.AI', 'stat.ML']",2024-01-05 08:37:57+00:00
http://arxiv.org/abs/2401.02650v1,Improving sample efficiency of high dimensional Bayesian optimization with MCMC,"Sequential optimization methods are often confronted with the curse of
dimensionality in high-dimensional spaces. Current approaches under the
Gaussian process framework are still burdened by the computational complexity
of tracking Gaussian process posteriors and need to partition the optimization
problem into small regions to ensure exploration or assume an underlying
low-dimensional structure. With the idea of transiting the candidate points
towards more promising positions, we propose a new method based on Markov Chain
Monte Carlo to efficiently sample from an approximated posterior. We provide
theoretical guarantees of its convergence in the Gaussian process Thompson
sampling setting. We also show experimentally that both the Metropolis-Hastings
and the Langevin Dynamics version of our algorithm outperform state-of-the-art
methods in high-dimensional sequential optimization and reinforcement learning
benchmarks.","['Zeji Yi', 'Yunyue Wei', 'Chu Xin Cheng', 'Kaibo He', 'Yanan Sui']","['cs.LG', 'stat.ML']",2024-01-05 05:56:42+00:00
http://arxiv.org/abs/2401.02592v1,Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery,"In this paper, we provide the first convergence guarantee for the
factorization approach. Specifically, to avoid the scaling ambiguity and to
facilitate theoretical analysis, we optimize over the so-called left-orthogonal
TT format which enforces orthonormality among most of the factors. To ensure
the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for
optimizing those factors over the Stiefel manifold. We first delve into the TT
factorization problem and establish the local linear convergence of RGD.
Notably, the rate of convergence only experiences a linear decline as the
tensor order increases. We then study the sensing problem that aims to recover
a TT format tensor from linear measurements. Assuming the sensing operator
satisfies the restricted isometry property (RIP), we show that with a proper
initialization, which could be obtained through spectral initialization, RGD
also converges to the ground-truth tensor at a linear rate. Furthermore, we
expand our analysis to encompass scenarios involving Gaussian noise in the
measurements. We prove that RGD can reliably recover the ground truth at a
linear rate, with the recovery error exhibiting only polynomial growth in
relation to the tensor order. We conduct various experiments to validate our
theoretical findings.","['Zhen Qin', 'Michael B. Wakin', 'Zhihui Zhu']","['stat.ML', 'cs.LG', 'eess.SP', 'math.OC']",2024-01-05 01:17:16+00:00
http://arxiv.org/abs/2401.02520v1,Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel,"The problem of structured matrix estimation has been studied mostly under
strong noise dependence assumptions. This paper considers a general framework
of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come
from any joint distribution with arbitrary dependence across entries. We
propose an incoherent-constrained least-square estimator and prove its
tightness both in the sense of deterministic lower bound and matching minimax
risks under various noise distributions. To attain this, we establish a novel
result asserting that the difference between two arbitrary low-rank incoherent
matrices must spread energy out across its entries, in other words cannot be
too sparse, which sheds light on the structure of incoherent low-rank matrices
and may be of independent interest. We then showcase the applications of our
framework to several important statistical machine learning problems. In the
problem of estimating a structured Markov transition kernel, the proposed
method achieves the minimax optimality and the result can be extended to
estimating the conditional mean operator, a crucial component in reinforcement
learning. The applications to multitask regression and structured covariance
estimation are also presented. We propose an alternating minimization algorithm
to approximately solve the potentially hard optimization problem. Numerical
results corroborate the effectiveness of our method which typically converges
in a few steps.","['Jinhang Chai', 'Jianqing Fan']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2024-01-04 20:13:23+00:00
http://arxiv.org/abs/2401.02413v2,Simulation-Based Inference with Quantile Regression,"We present Neural Quantile Estimation (NQE), a novel Simulation-Based
Inference (SBI) method based on conditional quantile regression. NQE
autoregressively learns individual one dimensional quantiles for each posterior
dimension, conditioned on the data and previous posterior dimensions. Posterior
samples are obtained by interpolating the predicted quantiles using monotonic
cubic Hermite spline, with specific treatment for the tail behavior and
multi-modal distributions. We introduce an alternative definition for the
Bayesian credible region using the local Cumulative Density Function (CDF),
offering substantially faster evaluation than the traditional Highest Posterior
Density Region (HPDR). In case of limited simulation budget and/or known model
misspecification, a post-processing calibration step can be integrated into NQE
to ensure the unbiasedness of the posterior estimation with negligible
additional computational cost. We demonstrate that NQE achieves
state-of-the-art performance on a variety of benchmark problems.",['He Jia'],"['stat.ML', 'cs.LG']",2024-01-04 18:53:50+00:00
http://arxiv.org/abs/2401.02349v2,A Survey Analyzing Generalization in Deep Reinforcement Learning,"Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to large language models, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will formalize and analyze generalization in deep
reinforcement learning. We will explain the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
generalization capabilities. Furthermore, we will categorize and explain the
manifold solution approaches to increase generalization, and overcome
overfitting in deep reinforcement learning policies. From exploration to
adversarial analysis and from regularization to robustness our paper provides
an analysis on a wide range of subfields within deep reinforcement learning
with a broad scope and in-depth view. We believe our study can provide a
compact guideline for the current advancements in deep reinforcement learning,
and help to construct robust deep neural policies with higher generalization
skills.",['Ezgi Korkmaz'],"['cs.LG', 'cs.AI', 'stat.ML']",2024-01-04 16:45:01+00:00
http://arxiv.org/abs/2401.02325v2,A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning,"Distributional Reinforcement Learning (RL) estimates return distribution
mainly by learning quantile values via minimizing the quantile Huber loss
function, entailing a threshold parameter often selected heuristically or via
hyperparameter search, which may not generalize well and can be suboptimal.
This paper introduces a generalized quantile Huber loss function derived from
Wasserstein distance (WD) calculation between Gaussian distributions, capturing
noise in predicted (current) and target (Bellman-updated) quantile values.
Compared to the classical quantile Huber loss, this innovative loss function
enhances robustness against outliers. Notably, the classical Huber loss
function can be seen as an approximation of our proposed loss, enabling
parameter adjustment by approximating the amount of noise in the data during
the learning process. Empirical tests on Atari games, a common application in
distributional RL, and a recent hedging strategy using distributional RL,
validate the effectiveness of our proposed loss function and its potential for
parameter adjustments in distributional RL. The implementation of the proposed
loss function is available here.","['Parvin Malekzadeh', 'Konstantinos N. Plataniotis', 'Zissis Poulos', 'Zeyu Wang']","['cs.LG', 'stat.ML']",2024-01-04 15:51:49+00:00
http://arxiv.org/abs/2401.02203v1,Robust bilinear factor analysis based on the matrix-variate $t$ distribution,"Factor Analysis based on multivariate $t$ distribution ($t$fa) is a useful
robust tool for extracting common factors on heavy-tailed or contaminated data.
However, $t$fa is only applicable to vector data. When $t$fa is applied to
matrix data, it is common to first vectorize the matrix observations. This
introduces two challenges for $t$fa: (i) the inherent matrix structure of the
data is broken, and (ii) robustness may be lost, as vectorized matrix data
typically results in a high data dimension, which could easily lead to the
breakdown of $t$fa. To address these issues, starting from the intrinsic matrix
structure of matrix data, a novel robust factor analysis model, namely bilinear
factor analysis built on the matrix-variate $t$ distribution ($t$bfa), is
proposed in this paper. The novelty is that it is capable to simultaneously
extract common factors for both row and column variables of interest on
heavy-tailed or contaminated matrix data. Two efficient algorithms for maximum
likelihood estimation of $t$bfa are developed. Closed-form expression for the
Fisher information matrix to calculate the accuracy of parameter estimates are
derived. Empirical studies are conducted to understand the proposed $t$bfa
model and compare with related competitors. The results demonstrate the
superiority and practicality of $t$bfa. Importantly, $t$bfa exhibits a
significantly higher breakdown point than $t$fa, making it more suitable for
matrix data.","['Xuan Ma', 'Jianhua Zhao', 'Changchun Shang', 'Fen Jiang', 'Philip L. H. Yu']","['stat.ML', 'cs.LG']",2024-01-04 11:15:44+00:00
http://arxiv.org/abs/2401.02080v2,Energy based diffusion generator for efficient sampling of Boltzmann distributions,"Sampling from Boltzmann distributions, particularly those tied to
high-dimensional and complex energy functions, poses a significant challenge in
many fields. In this work, we present the Energy-Based Diffusion Generator
(EDG), a novel approach that integrates ideas from variational autoencoders and
diffusion models. EDG leverages a decoder to transform latent variables from a
simple distribution into samples approximating the target Boltzmann
distribution, while the diffusion-based encoder provides an accurate estimate
of the Kullback-Leibler divergence during training. Notably, EDG is
simulation-free, eliminating the need to solve ordinary or stochastic
differential equations during training. Furthermore, by removing constraints
such as bijectivity in the decoder, EDG allows for flexible network design.
Through empirical evaluation, we demonstrate the superior performance of EDG
across a variety of complex distribution tasks, outperforming existing methods.","['Yan Wang', 'Ling Guo', 'Hao Wu', 'Tao Zhou']","['cs.LG', 'stat.CO', 'stat.ML']",2024-01-04 06:03:46+00:00
http://arxiv.org/abs/2401.02062v1,"U-Trustworthy Models.Reliability, Competence, and Confidence in Decision-Making","With growing concerns regarding bias and discrimination in predictive models,
the AI community has increasingly focused on assessing AI system
trustworthiness. Conventionally, trustworthy AI literature relies on the
probabilistic framework and calibration as prerequisites for trustworthiness.
In this work, we depart from this viewpoint by proposing a novel trust
framework inspired by the philosophy literature on trust. We present a precise
mathematical definition of trustworthiness, termed
$\mathcal{U}$-trustworthiness, specifically tailored for a subset of tasks
aimed at maximizing a utility function. We argue that a model's
$\mathcal{U}$-trustworthiness is contingent upon its ability to maximize Bayes
utility within this task subset. Our first set of results challenges the
probabilistic framework by demonstrating its potential to favor less
trustworthy models and introduce the risk of misleading trustworthiness
assessments. Within the context of $\mathcal{U}$-trustworthiness, we prove that
properly-ranked models are inherently $\mathcal{U}$-trustworthy. Furthermore,
we advocate for the adoption of the AUC metric as the preferred measure of
trustworthiness. By offering both theoretical guarantees and experimental
validation, AUC enables robust evaluation of trustworthiness, thereby enhancing
model selection and hyperparameter tuning to yield more trustworthy outcomes.","['Ritwik Vashistha', 'Arya Farahi']","['stat.ML', 'cs.LG']",2024-01-04 04:58:02+00:00
http://arxiv.org/abs/2401.02058v2,Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model,"The current paradigm of training deep neural networks for classification
tasks includes minimizing the empirical risk that pushes the training loss
value towards zero, even after the training error has been vanished. In this
terminal phase of training, it has been observed that the last-layer features
collapse to their class-means and these class-means converge to the vertices of
a simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural
Collapse (NC). To theoretically understand this phenomenon, recent works employ
a simplified unconstrained feature model to prove that NC emerges at the global
solutions of the training problem. However, when the training dataset is
class-imbalanced, some NC properties will no longer be true. For example, the
class-means geometry will skew away from the simplex ETF when the loss
converges. In this paper, we generalize NC to imbalanced regime for
cross-entropy loss under the unconstrained ReLU feature model. We prove that,
while the within-class features collapse property still holds in this setting,
the class-means will converge to a structure consisting of orthogonal vectors
with different lengths. Furthermore, we find that the classifier weights are
aligned to the scaled and centered class-means with scaling factors depend on
the number of training samples of each class, which generalizes NC in the
class-balanced setting. We empirically prove our results through experiments on
practical architectures and dataset.","['Hien Dang', 'Tho Tran', 'Tan Nguyen', 'Nhat Ho']","['cs.LG', 'stat.ML']",2024-01-04 04:53:31+00:00
http://arxiv.org/abs/2401.01988v2,Hierarchical Clustering in $Λ$CDM Cosmologies via Persistence Energy,"In this research, we investigate the structural evolution of the cosmic web,
employing advanced methodologies from Topological Data Analysis. Our approach
involves leveraging LITE, an innovative method from recent literature that
embeds persistence diagrams into elements of vector spaces. Utilizing this
methodology, we analyze three quintessential cosmic structures: clusters,
filaments, and voids. A central discovery is the correlation between
\textit{Persistence Energy} and redshift values, linking persistent homology
with cosmic evolution and providing insights into the dynamics of cosmic
structures.","['Michael Etienne Van Huffel', 'Leonardo Aldo Alejandro Barberi', 'Tobias Sagis']","['astro-ph.CO', 'cs.CG', 'math.AT', 'stat.ML']",2024-01-03 21:36:57+00:00
http://arxiv.org/abs/2401.01981v3,Beyond Regrets: Geometric Metrics for Bayesian Optimization,"Bayesian optimization is a principled optimization strategy for a black-box
objective function. It shows its effectiveness in a wide variety of real-world
applications such as scientific discovery and experimental design. In general,
the performance of Bayesian optimization is reported through regret-based
metrics such as instantaneous, simple, and cumulative regrets. These metrics
only rely on function evaluations, so that they do not consider geometric
relationships between query points and global solutions, or query points
themselves. Notably, they cannot discriminate if multiple global solutions are
successfully found. Moreover, they do not evaluate Bayesian optimization's
abilities to exploit and explore a search space given. To tackle these issues,
we propose four new geometric metrics, i.e., precision, recall, average degree,
and average distance. These metrics allow us to compare Bayesian optimization
algorithms considering the geometry of both query points and global optima, or
query points. However, they are accompanied by an extra parameter, which needs
to be carefully determined. We therefore devise the parameter-free forms of the
respective metrics by integrating out the additional parameter. Finally, we
validate that our proposed metrics can provide more delicate interpretation of
Bayesian optimization, on top of assessment via the conventional metrics.",['Jungtaek Kim'],"['cs.LG', 'stat.ML']",2024-01-03 20:59:52+00:00
http://arxiv.org/abs/2401.01869v1,On the hardness of learning under symmetries,"We study the problem of learning equivariant neural networks via gradient
descent. The incorporation of known symmetries (""equivariance"") into neural
nets has empirically improved the performance of learning pipelines, in domains
ranging from biology to computer vision. However, a rich yet separate line of
learning theoretic research has demonstrated that actually learning shallow,
fully-connected (i.e. non-symmetric) networks has exponential complexity in the
correlational statistical query (CSQ) model, a framework encompassing gradient
descent. In this work, we ask: are known problem symmetries sufficient to
alleviate the fundamental hardness of learning neural nets with gradient
descent? We answer this question in the negative. In particular, we give lower
bounds for shallow graph neural networks, convolutional networks, invariant
polynomials, and frame-averaged networks for permutation subgroups, which all
scale either superpolynomially or exponentially in the relevant input
dimension. Therefore, in spite of the significant inductive bias imparted via
symmetry, actually learning the complete classes of functions represented by
equivariant neural networks via gradient descent remains hard.","['Bobak T. Kiani', 'Thien Le', 'Hannah Lawrence', 'Stefanie Jegelka', 'Melanie Weber']","['cs.LG', 'cs.DS', 'math.ST', 'stat.ML', 'stat.TH']",2024-01-03 18:24:18+00:00
http://arxiv.org/abs/2401.01857v1,Optimal cross-learning for contextual bandits with unknown context distributions,"We consider the problem of designing contextual bandit algorithms in the
``cross-learning'' setting of Balseiro et al., where the learner observes the
loss for the action they play in all possible contexts, not just the context of
the current round. We specifically consider the setting where losses are chosen
adversarially and contexts are sampled i.i.d. from an unknown distribution. In
this setting, we resolve an open problem of Balseiro et al. by providing an
efficient algorithm with a nearly tight (up to logarithmic factors) regret
bound of $\widetilde{O}(\sqrt{TK})$, independent of the number of contexts. As
a consequence, we obtain the first nearly tight regret bounds for the problems
of learning to bid in first-price auctions (under unknown value distributions)
and sleeping bandits with a stochastic action set.
  At the core of our algorithm is a novel technique for coordinating the
execution of a learning algorithm over multiple epochs in such a way to remove
correlations between estimation of the unknown distribution and the actions
played by the algorithm. This technique may be of independent interest for
other learning problems involving estimation of an unknown context
distribution.","['Jon Schneider', 'Julian Zimmert']","['cs.LG', 'stat.ML']",2024-01-03 18:02:13+00:00
http://arxiv.org/abs/2401.01804v1,Efficient Computation of Confidence Sets Using Classification on Equidistributed Grids,"Economic models produce moment inequalities, which can be used to form tests
of the true parameters. Confidence sets (CS) of the true parameters are derived
by inverting these tests. However, they often lack analytical expressions,
necessitating a grid search to obtain the CS numerically by retaining the grid
points that pass the test. When the statistic is not asymptotically pivotal,
constructing the critical value for each grid point in the parameter space adds
to the computational burden. In this paper, we convert the computational issue
into a classification problem by using a support vector machine (SVM)
classifier. Its decision function provides a faster and more systematic way of
dividing the parameter space into two regions: inside vs. outside of the
confidence set. We label those points in the CS as 1 and those outside as -1.
Researchers can train the SVM classifier on a grid of manageable size and use
it to determine whether points on denser grids are in the CS or not. We
establish certain conditions for the grid so that there is a tuning that allows
us to asymptotically reproduce the test in the CS. This means that in the
limit, a point is classified as belonging to the confidence set if and only if
it is labeled as 1 by the SVM.",['Lujie Zhou'],"['econ.EM', 'stat.ML']",2024-01-03 16:04:14+00:00
http://arxiv.org/abs/2401.01789v1,Deep learning the Hurst parameter of linear fractional processes and assessing its reliability,"This research explores the reliability of deep learning, specifically Long
Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in
fractional stochastic processes. The study focuses on three types of processes:
fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process,
and linear fractional stable motions (lfsm). The work involves a fast
generation of extensive datasets for fBm and fOU to train the LSTM network on a
large volume of data in a feasible time. The study analyses the accuracy of the
LSTM network's Hurst parameter estimation regarding various performance
measures like RMSE, MAE, MRE, and quantiles of the absolute and relative
errors. It finds that LSTM outperforms the traditional statistical methods in
the case of fBm and fOU processes; however, it has limited accuracy on lfsm
processes. The research also delves into the implications of training length
and valuation sequence length on the LSTM's performance. The methodology is
applied by estimating the Hurst parameter in Li-ion battery degradation data
and obtaining confidence bounds for the estimation. The study concludes that
while deep learning methods show promise in parameter estimation of fractional
processes, their effectiveness is contingent on the process type and the
quality of training data.","['Dániel Boros', 'Bálint Csanády', 'Iván Ivkovic', 'Lóránt Nagy', 'András Lukács', 'László Márkus']","['stat.ML', 'cs.AI', 'cs.LG', '68T07']",2024-01-03 15:42:45+00:00
http://arxiv.org/abs/2401.01645v2,Model Averaging and Double Machine Learning,"This paper discusses pairing double/debiased machine learning (DDML) with
stacking, a model averaging method for combining multiple candidate learners,
to estimate structural parameters. In addition to conventional stacking, we
consider two stacking variants available for DDML: short-stacking exploits the
cross-fitting step of DDML to substantially reduce the computational burden and
pooled stacking enforces common stacking weights over cross-fitting folds.
Using calibrated simulation studies and two applications estimating gender gaps
in citations and wages, we show that DDML with stacking is more robust to
partially unknown functional forms than common alternative approaches based on
single pre-selected learners. We provide Stata and R software implementing our
proposals.","['Achim Ahrens', 'Christian B. Hansen', 'Mark E. Schaffer', 'Thomas Wiemann']","['econ.EM', 'stat.ML']",2024-01-03 09:38:13+00:00
http://arxiv.org/abs/2401.01460v1,Point Cloud Classification via Deep Set Linearized Optimal Transport,"We introduce Deep Set Linearized Optimal Transport, an algorithm designed for
the efficient simultaneous embedding of point clouds into an $L^2-$space. This
embedding preserves specific low-dimensional structures within the Wasserstein
space while constructing a classifier to distinguish between various classes of
point clouds. Our approach is motivated by the observation that $L^2-$distances
between optimal transport maps for distinct point clouds, originating from a
shared fixed reference distribution, provide an approximation of the
Wasserstein-2 distance between these point clouds, under certain assumptions.
To learn approximations of these transport maps, we employ input convex neural
networks (ICNNs) and establish that, under specific conditions, Euclidean
distances between samples from these ICNNs closely mirror Wasserstein-2
distances between the true distributions. Additionally, we train a
discriminator network that attaches weights these samples and creates a
permutation invariant classifier to differentiate between different classes of
point clouds. We showcase the advantages of our algorithm over the standard
deep set approach through experiments on a flow cytometry dataset with a
limited number of labeled point clouds.","['Scott Mahan', 'Caroline Moosmüller', 'Alexander Cloninger']","['cs.LG', 'stat.ML']",2024-01-02 23:26:33+00:00
http://arxiv.org/abs/2401.01426v2,Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference,"Sound and complete algorithms have been proposed to compute identifiable
causal queries using the causal structure and data. However, most of these
algorithms assume accurate estimation of the data distribution, which is
impractical for high-dimensional variables such as images. On the other hand,
modern deep generative architectures can be trained to sample from
high-dimensional distributions. However, training these networks are typically
very costly. Thus, it is desirable to leverage pre-trained models to answer
causal queries using such high-dimensional data. To address this, we propose
modular training of deep causal generative models that not only makes learning
more efficient, but also allows us to utilize large, pre-trained conditional
generative models. To the best of our knowledge, our algorithm, Modular-DCM is
the first algorithm that, given the causal structure, uses adversarial training
to learn the network weights, and can make use of pre-trained models to
provably sample from any identifiable causal query in the presence of latent
confounders. With extensive experiments on the Colored-MNIST dataset, we
demonstrate that our algorithm outperforms the baselines. We also show our
algorithm's convergence on the COVIDx dataset and its utility with a causal
invariant prediction problem on CelebA-HQ.","['Md Musfiqur Rahman', 'Murat Kocaoglu']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML']",2024-01-02 20:31:15+00:00
http://arxiv.org/abs/2401.01404v5,Scalable network reconstruction in subquadratic time,"Network reconstruction consists in determining the unobserved pairwise
couplings between $N$ nodes given only observational data on the resulting
behavior that is conditioned on those couplings -- typically a time-series or
independent samples from a graphical model. A major obstacle to the scalability
of algorithms proposed for this problem is a seemingly unavoidable quadratic
complexity of $\Omega(N^2)$, corresponding to the requirement of each possible
pairwise coupling being contemplated at least once, despite the fact that most
networks of interest are sparse, with a number of non-zero couplings that is
only $O(N)$. Here we present a general algorithm applicable to a broad range of
reconstruction problems that significantly outperforms this quadratic baseline.
Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011)
that produces the best edge candidates with high probability, thus bypassing an
exhaustive quadratic search. If we rely on the conjecture that the
second-neighbor search finishes in log-linear time (Baron & Darling, 2020;
2022), we demonstrate theoretically that our algorithm finishes in subquadratic
time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log
N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. In
practice, we show that our algorithm achieves a performance that is many orders
of magnitude faster than the quadratic baseline -- in a manner consistent with
our theoretical analysis -- allows for easy parallelization, and thus enables
the reconstruction of networks with hundreds of thousands and even millions of
nodes and edges.",['Tiago P. Peixoto'],"['cs.DS', 'cs.LG', 'physics.data-an', 'stat.CO', 'stat.ML']",2024-01-02 19:00:13+00:00
http://arxiv.org/abs/2401.01335v3,Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models,"Harnessing the power of human-annotated data through Supervised Fine-Tuning
(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we
delve into the prospect of growing a strong LLM out of a weak one without the
need for acquiring additional human-annotated data. We propose a new
fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a
supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,
where the LLM refines its capability by playing against instances of itself.
More specifically, the LLM generates its own training data from its previous
iterations, refining its policy by discerning these self-generated responses
from those obtained from human-annotated data. Our method progressively
elevates the LLM from a nascent model to a formidable one, unlocking the full
potential of human-annotated demonstration data for SFT. Theoretically, we
prove that the global optimum to the training objective function of our method
is achieved only when the LLM policy aligns with the target data distribution.
Empirically, we evaluate our method on several benchmark datasets including the
HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our
results show that SPIN can significantly improve the LLM's performance across a
variety of benchmarks and even outperform models trained through direct
preference optimization (DPO) supplemented with extra GPT-4 preference data.
This sheds light on the promise of self-play, enabling the achievement of
human-level performance in LLMs without the need for expert opponents. Codes
are available at https://github.com/uclaml/SPIN.","['Zixiang Chen', 'Yihe Deng', 'Huizhuo Yuan', 'Kaixuan Ji', 'Quanquan Gu']","['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",2024-01-02 18:53:13+00:00
http://arxiv.org/abs/2401.01294v1,Efficient Sparse Least Absolute Deviation Regression with Differential Privacy,"In recent years, privacy-preserving machine learning algorithms have
attracted increasing attention because of their important applications in many
scientific fields. However, in the literature, most privacy-preserving
algorithms demand learning objectives to be strongly convex and Lipschitz
smooth, which thus cannot cover a wide class of robust loss functions (e.g.,
quantile/least absolute loss). In this work, we aim to develop a fast
privacy-preserving learning solution for a sparse robust regression problem.
Our learning loss consists of a robust least absolute loss and an $\ell_1$
sparse penalty term. To fast solve the non-smooth loss under a given privacy
budget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)
algorithm for least absolute deviation regression. Our algorithm achieves a
fast estimation by reformulating the sparse LAD problem as a penalized least
square estimation problem and adopts a three-stage noise injection to guarantee
the $(\epsilon,\delta)$-differential privacy. We show that our algorithm can
achieve better privacy and statistical accuracy trade-off compared with the
state-of-the-art privacy-preserving regression algorithms. In the end, we
conduct experiments to verify the efficiency of our proposed FRAPPE algorithm.","['Weidong Liu', 'Xiaojun Mao', 'Xiaofei Zhang', 'Xin Zhang']","['stat.ML', 'cs.LG', 'stat.ME', '62J07']",2024-01-02 17:13:34+00:00
http://arxiv.org/abs/2401.01242v1,Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning,"Broadband infrastructure owners do not always know how their customers are
connected in the local networks, which are structured as rooted trees. A recent
study is able to infer the topology of a local network using discrete time
series data from the leaves of the tree (customers). In this study we propose a
contrastive approach for learning a binary event encoder from continuous time
series data. As a preliminary result, we show that our approach has some
potential in learning a valuable encoder.","['Tobias Engelhardt Rasmussen', 'Siv Sørensen']","['cs.LG', 'cs.AI', 'cs.SI', 'stat.ML']",2024-01-02 15:18:23+00:00
http://arxiv.org/abs/2401.01148v4,PAC-Bayes-Chernoff bounds for unbounded losses,"We introduce a new PAC-Bayes oracle bound for unbounded losses that extends
Cram\'er-Chernoff bounds to the PAC-Bayesian setting. The proof technique
relies on controlling the tails of certain random variables involving the
Cram\'er transform of the loss. Our approach naturally leverages properties of
Cram\'er-Chernoff bounds, such as exact optimization of the free parameter in
many PAC-Bayes bounds. We highlight several applications of the main theorem.
Firstly, we show that our bound recovers and generalizes previous results.
Additionally, our approach allows working with richer assumptions that result
in more informative and potentially tighter bounds. In this direction, we
provide a general bound under a new \textit{model-dependent} assumption from
which we obtain bounds based on parameter norms and log-Sobolev inequalities.
Notably, many of these bounds can be minimized to obtain distributions beyond
the Gibbs posterior and provide novel theoretical coverage to existing
regularization techniques.","['Ioar Casado', 'Luis A. Ortega', 'Aritz Pérez', 'Andrés R. Masegosa']","['stat.ML', 'cs.LG']",2024-01-02 10:58:54+00:00
http://arxiv.org/abs/2401.01048v1,PAC-Bayesian Domain Adaptation Bounds for Multi-view learning,"This paper presents a series of new results for domain adaptation in the
multi-view learning setting. The incorporation of multiple views in the domain
adaptation was paid little attention in the previous studies. In this way, we
propose an analysis of generalization bounds with Pac-Bayesian theory to
consolidate the two paradigms, which are currently treated separately. Firstly,
building on previous work by Germain et al., we adapt the distance between
distribution proposed by Germain et al. for domain adaptation with the concept
of multi-view learning. Thus, we introduce a novel distance that is tailored
for the multi-view domain adaptation setting. Then, we give Pac-Bayesian bounds
for estimating the introduced divergence. Finally, we compare the different new
bounds with the previous studies.","['Mehdi Hennequin', 'Khalid Benabdeslem', 'Haytham Elghazel']","['cs.LG', 'stat.ML']",2024-01-02 06:00:47+00:00
http://arxiv.org/abs/2401.01047v1,Sharp Analysis of Power Iteration for Tensor PCA,"We investigate the power iteration algorithm for the tensor PCA model
introduced in Richard and Montanari (2014). Previous work studying the
properties of tensor power iteration is either limited to a constant number of
iterations, or requires a non-trivial data-independent initialization. In this
paper, we move beyond these limitations and analyze the dynamics of randomly
initialized tensor power iteration up to polynomially many steps. Our
contributions are threefold: First, we establish sharp bounds on the number of
iterations required for power method to converge to the planted signal, for a
broad range of the signal-to-noise ratios. Second, our analysis reveals that
the actual algorithmic threshold for power iteration is smaller than the one
conjectured in literature by a polylog(n) factor, where n is the ambient
dimension. Finally, we propose a simple and effective stopping criterion for
power iteration, which provably outputs a solution that is highly correlated
with the true signal. Extensive numerical experiments verify our theoretical
results.","['Yuchen Wu', 'Kangjie Zhou']","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']",2024-01-02 05:55:27+00:00
http://arxiv.org/abs/2401.00987v2,Inverting estimating equations for causal inference on quantiles,"The causal inference literature frequently focuses on estimating the mean of
the potential outcome, whereas quantiles of the potential outcome may carry
important additional information. We propose a unified approach, based on the
inverse estimating equations, to generalize a class of causal inference
solutions from estimating the mean of the potential outcome to its quantiles.
We assume that a moment function is available to identify the mean of the
threshold-transformed potential outcome, based on which a convenient
construction of the estimating equation of quantiles of potential outcome is
proposed. In addition, we give a general construction of the efficient
influence functions of the mean and quantiles of potential outcomes, and
explicate their connection. We motivate estimators for the quantile estimands
with the efficient influence function, and develop their asymptotic properties
when either parametric models or data-adaptive machine learners are used to
estimate the nuisance functions. A broad implication of our results is that one
can rework the existing result for mean causal estimands to facilitate causal
inference on quantiles. Our general results are illustrated by several
analytical and numerical examples.","['Chao Cheng', 'Fan Li']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2024-01-02 01:52:28+00:00
http://arxiv.org/abs/2401.00953v1,Families of costs with zero and nonnegative MTW tensor in optimal transport,"We compute explicitly the MTW tensor (or cross curvature) for the optimal
transport problem on $\mathbb{R}^n$ with a cost function of form $\mathsf{c}(x,
y) = \mathsf{u}(x^{\mathfrak{t}}y)$, where $\mathsf{u}$ is a scalar function
with inverse $\mathsf{s}$, $x^{\ft}y$ is a nondegenerate bilinear pairing of
vectors $x, y$ belonging to an open subset of $\mathbb{R}^n$. The condition
that the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a
fourth-order nonlinear ODE, which could be reduced to a linear ODE of the form
$\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$ with constant
coefficients $P$ and $S$. The resulting inverse functions include {\it Lambert}
and {\it generalized inverse hyperbolic\slash trigonometric} functions. The
square Euclidean metric and $\log$-type costs are equivalent to instances of
these solutions. The optimal map for the family is also explicit. For cost
functions of a similar form on a hyperboloid model of the hyperbolic space and
unit sphere, we also express this tensor in terms of algebraic expressions in
derivatives of $\mathsf{s}$ using the Gauss-Codazzi equation, obtaining new
families of strictly regular costs for these manifolds, including new families
of {\it power function costs}. We analyze the $\sinh$-type hyperbolic cost,
providing examples of $\mathsf{c}$-convex functions and divergence.",['Du Nguyen'],"['math.AP', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML', '58C05, 49Q22, 53C80, 57Z20, 57Z25, 68T05, 26B25']",2024-01-01 20:33:27+00:00
http://arxiv.org/abs/2401.00828v4,Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows,"We consider the problem of sampling lattice field configurations on a lattice
from the Boltzmann distribution corresponding to some action. Since such
densities arise as approximationw of an underlying functional density, we frame
the task as an instance of operator learning. We propose to approximate a
time-dependent neural operator whose time integral provides a mapping between
the functional distributions of the free and target theories. Once a particular
lattice is chosen, the neural operator can be discretized to a
finite-dimensional, time-dependent vector field which in turn induces a
continuous normalizing flow between finite dimensional distributions over the
chosen lattice. This flow can then be trained to be a diffeormorphism between
the discretized free and target theories on the chosen lattice, and, by
construction, can be evaluated on different discretizations of spacetime. We
experimentally validate the proposal on the 2-dimensional $\phi^4$-theory to
explore to what extent such operator-based flow architectures generalize to
lattice sizes they were not trained on, and show that pretraining on smaller
lattices can lead to a speedup over training directly on the target lattice
size.","['Bálint Máté', 'François Fleuret']","['cs.LG', 'hep-lat', 'stat.ML']",2024-01-01 17:56:24+00:00
