id,title,abstract,authors,categories,date
http://arxiv.org/abs/1712.06028v1,"A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms","This paper proposes a new approach to construct high quality space-filling
sample designs. First, we propose a novel technique to quantify the
space-filling property and optimally trade-off uniformity and randomness in
sample designs in arbitrary dimensions. Second, we connect the proposed metric
(defined in the spatial domain) to the objective measure of the design
performance (defined in the spectral domain). This connection serves as an
analytic framework for evaluating the qualitative properties of space-filling
designs in general. Using the theoretical insights provided by this
spatial-spectral analysis, we derive the notion of optimal space-filling
designs, which we refer to as space-filling spectral designs. Third, we propose
an efficient estimator to evaluate the space-filling properties of sample
designs in arbitrary dimensions and use it to develop an optimization framework
to generate high quality space-filling designs. Finally, we carry out a
detailed performance comparison on two different applications in 2 to 6
dimensions: a) image reconstruction and b) surrogate modeling on several
benchmark optimization functions and an inertial confinement fusion (ICF)
simulation code. We demonstrate that the propose spectral designs significantly
outperform existing approaches especially in high dimensions.","['Bhavya Kailkhura', 'Jayaraman J. Thiagarajan', 'Charvi Rastogi', 'Pramod K. Varshney', 'Peer-Timo Bremer']","['stat.ML', 'cs.AI']",2017-12-16 22:31:52+00:00
http://arxiv.org/abs/1712.06006v1,How well does your sampler really work?,"We present a new data-driven benchmark system to evaluate the performance of
new MCMC samplers. Taking inspiration from the COCO benchmark in optimization,
we view this task as having critical importance to machine learning and
statistics given the rate at which new samplers are proposed. The common
hand-crafted examples to test new samplers are unsatisfactory; we take a
meta-learning-like approach to generate benchmark examples from a large corpus
of data sets and models. Surrogates of posteriors found in real problems are
created using highly flexible density models including modern neural network
based approaches. We provide new insights into the real effective sample size
of various samplers per unit time and the estimation efficiency of the samplers
per sample. Additionally, we provide a meta-analysis to assess the predictive
utility of various MCMC diagnostics and perform a nonparametric regression to
combine them.","['Ryan Turner', 'Brady Neal']","['stat.ML', 'stat.CO']",2017-12-16 18:56:20+00:00
http://arxiv.org/abs/1712.05999v1,Characterizing Political Fake News in Twitter by its Meta-Data,"This article presents a preliminary approach towards characterizing political
fake news on Twitter through the analysis of their meta-data. In particular, we
focus on more than 1.5M tweets collected on the day of the election of Donald
Trump as 45th president of the United States of America. We use the meta-data
embedded within those tweets in order to look for differences between tweets
containing fake news and tweets not containing them. Specifically, we perform
our analysis only on tweets that went viral, by studying proxies for users'
exposure to the tweets, by characterizing accounts spreading fake news, and by
looking at their polarization. We found significant differences on the
distribution of followers, the number of URLs on tweets, and the verification
of the users.","['Julio Amador', 'Axel Oehmichen', 'Miguel Molina-Solana']","['cs.CL', 'cs.SI', 'stat.ML']",2017-12-16 18:07:58+00:00
http://arxiv.org/abs/1712.05997v1,Taming Wild High Dimensional Text Data with a Fuzzy Lash,"The bag of words (BOW) represents a corpus in a matrix whose elements are the
frequency of words. However, each row in the matrix is a very high-dimensional
sparse vector. Dimension reduction (DR) is a popular method to address sparsity
and high-dimensionality issues. Among different strategies to develop DR
method, Unsupervised Feature Transformation (UFT) is a popular strategy to map
all words on a new basis to represent BOW. The recent increase of text data and
its challenges imply that DR area still needs new perspectives. Although a wide
range of methods based on the UFT strategy has been developed, the fuzzy
approach has not been considered for DR based on this strategy. This research
investigates the application of fuzzy clustering as a DR method based on the
UFT strategy to collapse BOW matrix to provide a lower-dimensional
representation of documents instead of the words in a corpus. The quantitative
evaluation shows that fuzzy clustering produces superior performance and
features to Principal Components Analysis (PCA) and Singular Value
Decomposition (SVD), two popular DR methods based on the UFT strategy.",['Amir Karami'],"['stat.ML', 'cs.CL', 'cs.IR', 'cs.LG', 'stat.AP']",2017-12-16 17:57:57+00:00
http://arxiv.org/abs/1712.05907v2,"Parallel Markov Chain Monte Carlo for Bayesian Hierarchical Models with Big Data, in Two Stages","Due to the escalating growth of big data sets in recent years, new Bayesian
Markov chain Monte Carlo (MCMC) parallel computing methods have been developed.
These methods partition large data sets by observations into subsets. However,
for Bayesian nested hierarchical models, typically only a few parameters are
common for the full data set, with most parameters being group-specific. Thus,
parallel Bayesian MCMC methods that take into account the structure of the
model and split the full data set by groups rather than by observations are a
more natural approach for analysis. Here, we adapt and extend a recently
introduced two-stage Bayesian hierarchical modeling approach, and we partition
complete data sets by groups. In stage 1, the group-specific parameters are
estimated independently in parallel. The stage 1 posteriors are used as
proposal distributions in stage 2, where the target distribution is the full
model. Using three-level and four-level models, we show in both simulation and
real data studies that results of our method agree closely with the full data
analysis, with greatly increased MCMC efficiency and greatly reduced
computation times. The advantages of our method versus existing parallel MCMC
computing methods are also described.","['Zheng Wei', 'Erin M. Conlon']","['stat.ME', 'cs.DC', 'stat.CO', 'stat.ML']",2017-12-16 06:14:18+00:00
http://arxiv.org/abs/1712.05901v1,Automatic Music Highlight Extraction using Convolutional Recurrent Attention Networks,"Music highlights are valuable contents for music services. Most methods
focused on low-level signal features. We propose a method for extracting
highlights using high-level features from convolutional recurrent attention
networks (CRAN). CRAN utilizes convolution and recurrent layers for sequential
learning with an attention mechanism. The attention allows CRAN to capture
significant snippets for distinguishing between genres, thus being used as a
high-level feature. CRAN was evaluated on over 32,000 popular tracks in Korea
for two months. Experimental results show our method outperforms three baseline
methods through quantitative and qualitative evaluations. Also, we analyze the
effects of attention and sequence information on performance.","['Jung-Woo Ha', 'Adrian Kim', 'Chanju Kim', 'Jangyeon Park', 'Sunghun Kim']","['cs.LG', 'cs.MM', 'cs.SD', 'stat.ML']",2017-12-16 04:27:36+00:00
http://arxiv.org/abs/1712.08655v3,Travel time tomography with adaptive dictionaries,"We develop a 2D travel time tomography method which regularizes the inversion
by modeling groups of slowness pixels from discrete slowness maps, called
patches, as sparse linear combinations of atoms from a dictionary. We propose
to use dictionary learning during the inversion to adapt dictionaries to
specific slowness maps. This patch regularization, called the local model, is
integrated into the overall slowness map, called the global model. The local
model considers small-scale variations using a sparsity constraint and the
global model considers larger-scale features constrained using $\ell_2$
regularization. This strategy in a locally-sparse travel time tomography (LST)
approach enables simultaneous modeling of smooth and discontinuous slowness
features. This is in contrast to conventional tomography methods, which
constrain models to be exclusively smooth or discontinuous. We develop a
$\textit{maximum a posteriori}$ formulation for LST and exploit the sparsity of
slowness patches using dictionary learning. The LST approach compares favorably
with smoothness and total variation regularization methods on densely, but
irregularly sampled synthetic slowness maps.","['Michael Bianco', 'Peter Gerstoft']","['physics.geo-ph', 'eess.IV', 'stat.ML']",2017-12-16 02:28:40+00:00
http://arxiv.org/abs/1712.05889v2,Ray: A Distributed Framework for Emerging AI Applications,"The next generation of AI applications will continuously interact with the
environment and learn from these interactions. These applications impose new
and demanding systems requirements, both in terms of performance and
flexibility. In this paper, we consider these requirements and present Ray---a
distributed system to address them. Ray implements a unified interface that can
express both task-parallel and actor-based computations, supported by a single
dynamic execution engine. To meet the performance requirements, Ray employs a
distributed scheduler and a distributed and fault-tolerant store to manage the
system's control state. In our experiments, we demonstrate scaling beyond 1.8
million tasks per second and better performance than existing specialized
systems for several challenging reinforcement learning applications.","['Philipp Moritz', 'Robert Nishihara', 'Stephanie Wang', 'Alexey Tumanov', 'Richard Liaw', 'Eric Liang', 'Melih Elibol', 'Zongheng Yang', 'William Paul', 'Michael I. Jordan', 'Ion Stoica']","['cs.DC', 'cs.AI', 'cs.LG', 'stat.ML']",2017-12-16 01:29:49+00:00
http://arxiv.org/abs/1712.05882v1,On reproduction of On the regularization of Wasserstein GANs,"This report has several purposes. First, our report is written to investigate
the reproducibility of the submitted paper On the regularization of Wasserstein
GANs (2018). Second, among the experiments performed in the submitted paper,
five aspects were emphasized and reproduced: learning speed, stability,
robustness against hyperparameter, estimating the Wasserstein distance, and
various sampling method. Finally, we identify which parts of the contribution
can be reproduced, and at what cost in terms of resources. All source code for
reproduction is open to the public.","['Junghoon Seo', 'Taegyun Jeon']","['cs.LG', 'stat.ML']",2017-12-16 00:37:20+00:00
http://arxiv.org/abs/1712.05878v1,An MPI-Based Python Framework for Distributed Training with Keras,"We present a lightweight Python framework for distributed training of neural
networks on multiple GPUs or CPUs. The framework is built on the popular Keras
machine learning library. The Message Passing Interface (MPI) protocol is used
to coordinate the training process, and the system is well suited for job
submission at supercomputing sites. We detail the software's features, describe
its use, and demonstrate its performance on systems of varying sizes on a
benchmark problem drawn from high-energy physics research.","['Dustin Anderson', 'Jean-Roch Vlimant', 'Maria Spiropulu']","['cs.DC', 'stat.ML']",2017-12-16 00:01:27+00:00
http://arxiv.org/abs/1712.05877v1,Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,"The rising popularity of intelligent mobile devices and the daunting
computational cost of deep learning-based models call for efficient and
accurate on-device inference schemes. We propose a quantization scheme that
allows inference to be carried out using integer-only arithmetic, which can be
implemented more efficiently than floating point inference on commonly
available integer-only hardware. We also co-design a training procedure to
preserve end-to-end model accuracy post quantization. As a result, the proposed
quantization scheme improves the tradeoff between accuracy and on-device
latency. The improvements are significant even on MobileNets, a model family
known for run-time efficiency, and are demonstrated in ImageNet classification
and COCO detection on popular CPUs.","['Benoit Jacob', 'Skirmantas Kligys', 'Bo Chen', 'Menglong Zhu', 'Matthew Tang', 'Andrew Howard', 'Hartwig Adam', 'Dmitry Kalenichenko']","['cs.LG', 'stat.ML']",2017-12-15 23:56:52+00:00
http://arxiv.org/abs/1712.05861v1,WACSF - Weighted Atom-Centered Symmetry Functions as Descriptors in Machine Learning Potentials,"We introduce weighted atom-centered symmetry functions (wACSFs) as
descriptors of a chemical system's geometry for use in the prediction of
chemical properties such as enthalpies or potential energies via machine
learning. The wACSFs are based on conventional atom-centered symmetry functions
(ACSFs) but overcome the undesirable scaling of the latter with increasing
number of different elements in a chemical system. The performance of these two
descriptors is compared using them as inputs in high-dimensional neural network
potentials (HDNNPs), employing the molecular structures and associated
enthalpies of the 133855 molecules containing up to five different elements
reported in the QM9 database as reference data. A substantially smaller number
of wACSFs than ACSFs is needed to obtain a comparable spatial resolution of the
molecular structures. At the same time, this smaller set of wACSFs leads to
significantly better generalization performance in the machine learning
potential than the large set of conventional ACSFs. Furthermore, we show that
the intrinsic parameters of the descriptors can in principle be optimized with
a genetic algorithm in a highly automated manner. For the wACSFs employed here,
we find however that using a simple empirical parametrization scheme is
sufficient in order to obtain HDNNPs with high accuracy.","['Michael Gastegger', 'Ludwig Schwiedrzik', 'Marius Bittermann', 'Florian Berzsenyi', 'Philipp Marquetand']","['physics.chem-ph', 'stat.ML']",2017-12-15 22:15:27+00:00
http://arxiv.org/abs/1712.05813v1,Realistic Traffic Generation for Web Robots,"Critical to evaluating the capacity, scalability, and availability of web
systems are realistic web traffic generators. Web traffic generation is a
classic research problem, no generator accounts for the characteristics of web
robots or crawlers that are now the dominant source of traffic to a web server.
Administrators are thus unable to test, stress, and evaluate how their systems
perform in the face of ever increasing levels of web robot traffic. To resolve
this problem, this paper introduces a novel approach to generate synthetic web
robot traffic with high fidelity. It generates traffic that accounts for both
the temporal and behavioral qualities of robot traffic by statistical and
Bayesian models that are fitted to the properties of robot traffic seen in web
logs from North America and Europe. We evaluate our traffic generator by
comparing the characteristics of generated traffic to those of the original
data. We look at session arrival rates, inter-arrival times and session
lengths, comparing and contrasting them between generated and real traffic.
Finally, we show that our generated traffic affects cache performance similarly
to actual traffic, using the common LRU and LFU eviction policies.","['Kyle Brown', 'Derek Doran']","['cs.NI', 'stat.ML']",2017-12-15 19:16:17+00:00
http://arxiv.org/abs/1712.05790v1,Deep Burst Denoising,"Noise is an inherent issue of low-light image capture, one which is
exacerbated on mobile devices due to their narrow apertures and small sensors.
One strategy for mitigating noise in a low-light situation is to increase the
shutter time of the camera, thus allowing each photosite to integrate more
light and decrease noise variance. However, there are two downsides of long
exposures: (a) bright regions can exceed the sensor range, and (b) camera and
scene motion will result in blurred images. Another way of gathering more light
is to capture multiple short (thus noisy) frames in a ""burst"" and intelligently
integrate the content, thus avoiding the above downsides. In this paper, we use
the burst-capture strategy and implement the intelligent integration via a
recurrent fully convolutional deep neural net (CNN). We build our novel,
multiframe architecture to be a simple addition to any single frame denoising
model, and design to handle an arbitrary number of noisy input frames. We show
that it achieves state of the art denoising results on our burst dataset,
improving on the best published multi-frame techniques, such as VBM4D and
FlexISP. Finally, we explore other applications of image enhancement by
integrating content from multiple frames and demonstrate that our DNN
architecture generalizes well to image super-resolution.","['Clément Godard', 'Kevin Matzen', 'Matt Uyttendaele']","['cs.CV', 'cs.LG', 'stat.ML']",2017-12-15 18:55:16+00:00
http://arxiv.org/abs/1712.05754v1,Understanding Career Progression in Baseball Through Machine Learning,"Professional baseball players are increasingly guaranteed expensive long-term
contracts, with over 70 deals signed in excess of \$90 million, mostly in the
last decade. These are substantial sums compared to a typical franchise
valuation of \$1-2 billion. Hence, the players to whom a team chooses to give
such a contract can have an enormous impact on both competitiveness and profit.
Despite this, most published approaches examining career progression in
baseball are fairly simplistic. We applied four machine learning algorithms to
the problem and soundly improved upon existing approaches, particularly for
batting data.","['Brian Bierig', 'Jonathan Hollenbeck', 'Alexander Stroud']","['stat.ML', 'cs.OH']",2017-12-15 17:02:03+00:00
http://arxiv.org/abs/1712.05690v2,Sockeye: A Toolkit for Neural Machine Translation,"We describe Sockeye (version 1.12), an open-source sequence-to-sequence
toolkit for Neural Machine Translation (NMT). Sockeye is a production-ready
framework for training and applying models as well as an experimental platform
for researchers. Written in Python and built on MXNet, the toolkit offers
scalable training and inference for the three most prominent encoder-decoder
architectures: attentional recurrent neural networks, self-attentional
transformers, and fully convolutional networks. Sockeye also supports a wide
range of optimizers, normalization and regularization techniques, and inference
improvements from current NMT literature. Users can easily run standard
training recipes, explore different model settings, and incorporate new ideas.
In this paper, we highlight Sockeye's features and benchmark it against other
NMT toolkits on two language arcs from the 2017 Conference on Machine
Translation (WMT): English-German and Latvian-English. We report competitive
BLEU scores across all three architectures, including an overall best score for
Sockeye's transformer implementation. To facilitate further comparison, we
release all system outputs and training scripts used in our experiments. The
Sockeye toolkit is free software released under the Apache 2.0 license.","['Felix Hieber', 'Tobias Domhan', 'Michael Denkowski', 'David Vilar', 'Artem Sokolov', 'Ann Clifton', 'Matt Post']","['cs.CL', 'cs.LG', 'stat.ML']",2017-12-15 14:44:28+00:00
http://arxiv.org/abs/1712.05689v1,BT-Nets: Simplifying Deep Neural Networks via Block Term Decomposition,"Recently, deep neural networks (DNNs) have been regarded as the
state-of-the-art classification methods in a wide range of applications,
especially in image classification. Despite the success, the huge number of
parameters blocks its deployment to situations with light computing resources.
Researchers resort to the redundancy in the weights of DNNs and attempt to find
how fewer parameters can be chosen while preserving the accuracy at the same
time. Although several promising results have been shown along this research
line, most existing methods either fail to significantly compress a
well-trained deep network or require a heavy fine-tuning process for the
compressed network to regain the original performance. In this paper, we
propose the \textit{Block Term} networks (BT-nets) in which the commonly used
fully-connected layers (FC-layers) are replaced with block term layers
(BT-layers). In BT-layers, the inputs and the outputs are reshaped into two
low-dimensional high-order tensors, then block-term decomposition is applied as
tensor operators to connect them. We conduct extensive experiments on benchmark
datasets to demonstrate that BT-layers can achieve a very large compression
ratio on the number of parameters while preserving the representation power of
the original FC-layers as much as possible. Specifically, we can get a higher
performance while requiring fewer parameters compared with the tensor train
method.","['Guangxi Li', 'Jinmian Ye', 'Haiqin Yang', 'Di Chen', 'Shuicheng Yan', 'Zenglin Xu']","['stat.ML', 'cs.LG']",2017-12-15 14:42:32+00:00
http://arxiv.org/abs/1712.05654v2,Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice,"We introduce a generic scheme for accelerating gradient-based optimization
methods in the sense of Nesterov. The approach, called Catalyst, builds upon
the inexact accelerated proximal point algorithm for minimizing a convex
objective function, and consists of approximately solving a sequence of
well-chosen auxiliary problems, leading to faster convergence. One of the keys
to achieve acceleration in theory and in practice is to solve these
sub-problems with appropriate accuracy by using the right stopping criterion
and the right warm-start strategy. We give practical guidelines to use Catalyst
and present a comprehensive analysis of its global complexity. We show that
Catalyst applies to a large class of algorithms, including gradient descent,
block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG,
MISO/Finito, and their proximal variants. For all of these methods, we
establish faster rates using the Catalyst acceleration, for strongly convex and
non-strongly convex objectives. We conclude with extensive experiments showing
that acceleration is useful in practice, especially for ill-conditioned
problems.","['Hongzhou Lin', 'Julien Mairal', 'Zaid Harchaoui']","['stat.ML', 'math.OC']",2017-12-15 13:05:18+00:00
http://arxiv.org/abs/1712.05630v4,Sparse principal component analysis via axis-aligned random projections,"We introduce a new method for sparse principal component analysis, based on
the aggregation of eigenvector information from carefully-selected axis-aligned
random projections of the sample covariance matrix. Unlike most alternative
approaches, our algorithm is non-iterative, so is not vulnerable to a bad
choice of initialisation. We provide theoretical guarantees under which our
principal subspace estimator can attain the minimax optimal rate of convergence
in polynomial time. In addition, our theory provides a more refined
understanding of the statistical and computational trade-off in the problem of
sparse principal component estimation, revealing a subtle interplay between the
effective sample size and the number of random projections that are required to
achieve the minimax optimal rate. Numerical studies provide further insight
into the procedure and confirm its highly competitive finite-sample
performance.","['Milana Gataric', 'Tengyao Wang', 'Richard J. Samworth']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH', '62H25']",2017-12-15 11:55:39+00:00
http://arxiv.org/abs/1712.05556v3,Safe Policy Search with Gaussian Process Models,"We propose a method to optimise the parameters of a policy which will be used
to safely perform a given task in a data-efficient manner. We train a Gaussian
process model to capture the system dynamics, based on the PILCO framework. Our
model has useful analytic properties, which allow closed form computation of
error gradients and estimating the probability of violating given state space
constraints. During training, as well as operation, only policies that are
deemed safe are implemented on the real system, minimising the risk of failure.","['Kyriakos Polymenakos', 'Alessandro Abate', 'Stephen Roberts']","['stat.ML', 'cs.LG']",2017-12-15 06:34:53+00:00
http://arxiv.org/abs/1712.05510v1,Graph-Sparse Logistic Regression,"We introduce Graph-Sparse Logistic Regression, a new algorithm for
classification for the case in which the support should be sparse but connected
on a graph. We val- idate this algorithm against synthetic data and benchmark
it against L1-regularized Logistic Regression. We then explore our technique in
the bioinformatics context of proteomics data on the interactome graph. We make
all our experimental code public and provide GSLR as an open source package.","['Alexander LeNail', 'Ludwig Schmidt', 'Johnathan Li', 'Tobias Ehrenberger', 'Karen Sachs', 'Stefanie Jegelka', 'Ernest Fraenkel']","['cs.LG', 'stat.ML']",2017-12-15 02:17:06+00:00
http://arxiv.org/abs/1712.05438v1,Stochastic Particle Gradient Descent for Infinite Ensembles,"The superior performance of ensemble methods with infinite models are well
known. Most of these methods are based on optimization problems in
infinite-dimensional spaces with some regularization, for instance, boosting
methods and convex neural networks use $L^1$-regularization with the
non-negative constraint. However, due to the difficulty of handling
$L^1$-regularization, these problems require early stopping or a rough
approximation to solve it inexactly. In this paper, we propose a new ensemble
learning method that performs in a space of probability measures, that is, our
method can handle the $L^1$-constraint and the non-negative constraint in a
rigorous way. Such an optimization is realized by proposing a general purpose
stochastic optimization method for learning probability measures via
parameterization using transport maps on base models. As a result of running
the method, a transport map to output an infinite ensemble is obtained, which
forms a residual-type network. From the perspective of functional gradient
methods, we give a convergence rate as fast as that of a stochastic
optimization method for finite dimensional nonconvex problems. Moreover, we
show an interior optimality property of a local optimality condition used in
our analysis.","['Atsushi Nitanda', 'Taiji Suzuki']","['stat.ML', 'cs.LG', 'math.OC']",2017-12-14 20:12:02+00:00
http://arxiv.org/abs/1712.05382v2,Monotonic Chunkwise Attention,"Sequence-to-sequence models with soft attention have been successfully
applied to a wide variety of problems, but their decoding process incurs a
quadratic time and space cost and is inapplicable to real-time sequence
transduction. To address these issues, we propose Monotonic Chunkwise Attention
(MoChA), which adaptively splits the input sequence into small chunks over
which soft attention is computed. We show that models utilizing MoChA can be
trained efficiently with standard backpropagation while allowing online and
linear-time decoding at test time. When applied to online speech recognition,
we obtain state-of-the-art results and match the performance of a model using
an offline soft attention mechanism. In document summarization experiments
where we do not expect monotonic alignments, we show significantly improved
performance compared to a baseline monotonic attention-based model.","['Chung-Cheng Chiu', 'Colin Raffel']","['cs.CL', 'stat.ML']",2017-12-14 18:29:42+00:00
http://arxiv.org/abs/1712.05279v1,Strictly proper kernel scores and characteristic kernels on compact spaces,"Strictly proper kernel scores are well-known tool in probabilistic
forecasting, while characteristic kernels have been extensively investigated in
the machine learning literature. We first show that both notions coincide, so
that insights from one part of the literature can be used in the other. We then
show that the metric induced by a characteristic kernel cannot reliably
distinguish between distributions that are far apart in the total variation
norm as soon as the underlying space of measures is infinite dimensional. In
addition, we provide a characterization of characteristic kernels in terms of
eigenvalues and -functions and apply this characterization to the case of
continuous kernels on (locally) compact spaces. In the compact case we further
show that characteristic kernels exist if and only if the space is metrizable.
As special cases of our general theory we investigate translation-invariant
kernels on compact Abelian groups and isotropic kernels on spheres. The latter
are of particular interest for forecast evaluation of probabilistic predictions
on spherical domains as frequently encountered in meteorology and climatology.","['Ingo Steinwart', 'Johanna F. Ziegel']","['math.FA', 'math.ST', 'stat.ML', 'stat.TH']",2017-12-14 15:18:29+00:00
http://arxiv.org/abs/1712.05134v2,Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition,"Recurrent Neural Networks (RNNs) are powerful sequence modeling tools.
However, when dealing with high dimensional inputs, the training of RNNs
becomes computational expensive due to the large number of model parameters.
This hinders RNNs from solving many important computer vision tasks, such as
Action Recognition in Videos and Image Captioning. To overcome this problem, we
propose a compact and flexible structure, namely Block-Term tensor
decomposition, which greatly reduces the parameters of RNNs and improves their
training efficiency. Compared with alternative low-rank approximations, such as
tensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only
more concise (when using the same rank), but also able to attain a better
approximation to the original RNNs with much fewer parameters. On three
challenging tasks, including Action Recognition in Videos, Image Captioning and
Image Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of
both prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes
17,388 times fewer parameters than the standard LSTM to achieve an accuracy
improvement over 15.6\% in the Action Recognition task on the UCF11 dataset.","['Jinmian Ye', 'Linnan Wang', 'Guangxi Li', 'Di Chen', 'Shandian Zhe', 'Xinqi Chu', 'Zenglin Xu']","['cs.LG', 'stat.ML']",2017-12-14 09:24:27+00:00
http://arxiv.org/abs/1712.05016v2,Deep Prior,"The recent literature on deep learning offers new tools to learn a rich
probability distribution over high dimensional data such as images or sounds.
In this work we investigate the possibility of learning the prior distribution
over neural network parameters using such tools. Our resulting variational
Bayes algorithm generalizes well to new tasks, even when very few training
examples are provided. Furthermore, this learned prior allows the model to
extrapolate correctly far from a given task's training data on a meta-dataset
of periodic signals.","['Alexandre Lacoste', 'Thomas Boquet', 'Negar Rostamzadeh', 'Boris Oreshkin', 'Wonchang Chung', 'David Krueger']","['stat.ML', 'cs.LG']",2017-12-13 21:41:56+00:00
http://arxiv.org/abs/1712.04997v2,Predicting Station-level Hourly Demands in a Large-scale Bike-sharing Network: A Graph Convolutional Neural Network Approach,"This study proposes a novel Graph Convolutional Neural Network with
Data-driven Graph Filter (GCNN-DDGF) model that can learn hidden heterogeneous
pairwise correlations between stations to predict station-level hourly demand
in a large-scale bike-sharing network. Two architectures of the GCNN-DDGF model
are explored; GCNNreg-DDGF is a regular GCNN-DDGF model which contains the
convolution and feedforward blocks, and GCNNrec-DDGF additionally contains a
recurrent block from the Long Short-term Memory neural network architecture to
capture temporal dependencies in the bike-sharing demand series. Furthermore,
four types of GCNN models are proposed whose adjacency matrices are based on
various bike-sharing system data, including Spatial Distance matrix (SD),
Demand matrix (DE), Average Trip Duration matrix (ATD), and Demand Correlation
matrix (DC). These six types of GCNN models and seven other benchmark models
are built and compared on a Citi Bike dataset from New York City which includes
272 stations and over 28 million transactions from 2013 to 2016. Results show
that the GCNNrec-DDGF performs the best in terms of the Root Mean Square Error,
the Mean Absolute Error and the coefficient of determination (R2), followed by
the GCNNreg-DDGF. They outperform the other models. Through a more detailed
graph network analysis based on the learned DDGF, insights are obtained on the
black box of the GCNN-DDGF model. It is found to capture some information
similar to details embedded in the SD, DE and DC matrices. More importantly, it
also uncovers hidden heterogeneous pairwise correlations between stations that
are not revealed by any of those matrices.","['Lei Lin', 'Zhengbing He', 'Srinivas Peeta']","['stat.ML', 'cs.LG']",2017-12-13 20:26:50+00:00
http://arxiv.org/abs/1801.08196v1,Incremental Eigenpair Computation for Graph Laplacian Matrices: Theory and Applications,"The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)
of a graph Laplacian matrix have been widely used in spectral clustering and
community detection. However, in real-life applications the number of clusters
or communities (say, $K$) is generally unknown a-priori. Consequently, the
majority of the existing methods either choose $K$ heuristically or they repeat
the clustering method with different choices of $K$ and accept the best
clustering result. The first option, more often, yields suboptimal result,
while the second option is computationally expensive. In this work, we propose
an incremental method for constructing the eigenspectrum of the graph Laplacian
matrix. This method leverages the eigenstructure of graph Laplacian matrix to
obtain the $K$-th smallest eigenpair of the Laplacian matrix given a collection
of all previously computed $K-1$ smallest eigenpairs. Our proposed method
adapts the Laplacian matrix such that the batch eigenvalue decomposition
problem transforms into an efficient sequential leading eigenpair computation
problem. As a practical application, we consider user-guided spectral
clustering. Specifically, we demonstrate that users can utilize the proposed
incremental method for effective eigenpair computation and for determining the
desired number of clusters based on multiple clustering metrics.","['Pin-Yu Chen', 'Baichuan Zhang', 'Mohammad Al Hasan']","['cs.LG', 'cs.SI', 'stat.ML']",2017-12-13 19:04:35+00:00
http://arxiv.org/abs/1712.04912v4,Quasi-Oracle Estimation of Heterogeneous Treatment Effects,"Flexible estimation of heterogeneous treatment effects lies at the heart of
many statistical challenges, such as personalized medicine and optimal resource
allocation. In this paper, we develop a general class of two-step algorithms
for heterogeneous treatment effect estimation in observational studies. We
first estimate marginal effects and treatment propensities in order to form an
objective function that isolates the causal component of the signal. Then, we
optimize this data-adaptive objective function. Our approach has several
advantages over existing methods. From a practical perspective, our method is
flexible and easy to use: In both steps, we can use any loss-minimization
method, e.g., penalized regression, deep neural networks, or boosting;
moreover, these methods can be fine-tuned by cross validation. Meanwhile, in
the case of penalized kernel regression, we show that our method has a
quasi-oracle property: Even if the pilot estimates for marginal effects and
treatment propensities are not particularly accurate, we achieve the same error
bounds as an oracle who has a priori knowledge of these two nuisance
components. We implement variants of our approach based on penalized
regression, kernel ridge regression, and boosting in a variety of simulation
setups, and find promising performance relative to existing baselines.","['Xinkun Nie', 'Stefan Wager']","['stat.ML', 'econ.EM', 'math.ST', 'stat.TH']",2017-12-13 18:32:13+00:00
http://arxiv.org/abs/1712.04910v1,FFT-Based Deep Learning Deployment in Embedded Systems,"Deep learning has delivered its powerfulness in many application domains,
especially in image and speech recognition. As the backbone of deep learning,
deep neural networks (DNNs) consist of multiple layers of various types with
hundreds to thousands of neurons. Embedded platforms are now becoming essential
for deep learning deployment due to their portability, versatility, and energy
efficiency. The large model size of DNNs, while providing excellent accuracy,
also burdens the embedded platforms with intensive computation and storage.
Researchers have investigated on reducing DNN model size with negligible
accuracy loss. This work proposes a Fast Fourier Transform (FFT)-based DNN
training and inference model suitable for embedded platforms with reduced
asymptotic complexity of both computation and storage, making our approach
distinguished from existing approaches. We develop the training and inference
algorithms based on FFT as the computing kernel and deploy the FFT-based
inference model on embedded platforms achieving extraordinary processing speed.","['Sheng Lin', 'Ning Liu', 'Mahdi Nazemi', 'Hongjia Li', 'Caiwen Ding', 'Yanzhi Wang', 'Massoud Pedram']","['cs.LG', 'stat.ML']",2017-12-13 18:26:17+00:00
http://arxiv.org/abs/1712.04828v1,Ballpark Crowdsourcing: The Wisdom of Rough Group Comparisons,"Crowdsourcing has become a popular method for collecting labeled training
data. However, in many practical scenarios traditional labeling can be
difficult for crowdworkers (for example, if the data is high-dimensional or
unintuitive, or the labels are continuous).
  In this work, we develop a novel model for crowdsourcing that can complement
standard practices by exploiting people's intuitions about groups and relations
between them. We employ a recent machine learning setting, called Ballpark
Learning, that can estimate individual labels given only coarse, aggregated
signal over groups of data points. To address the important case of continuous
labels, we extend the Ballpark setting (which focused on classification) to
regression problems. We formulate the problem as a convex optimization problem
and propose fast, simple methods with an innate robustness to outliers.
  We evaluate our methods on real-world datasets, demonstrating how useful
constraints about groups can be harnessed from a crowd of non-experts. Our
methods can rival supervised models trained on many true labels, and can obtain
considerably better results from the crowd than a standard label-collection
process (for a lower price). By collecting rough guesses on groups of instances
and using machine learning to infer the individual labels, our lightweight
framework is able to address core crowdsourcing challenges and train machine
learning models in a cost-effective way.","['Tom Hope', 'Dafna Shahaf']","['stat.ML', 'cs.LG']",2017-12-13 15:56:27+00:00
http://arxiv.org/abs/1712.04802v8,"Fisher-Schultz Lecture: Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments, with an Application to Immunization in India","We propose strategies to estimate and make inference on key features of
heterogeneous effects in randomized experiments. These key features include
best linear predictors of the effects using machine learning proxies, average
effects sorted by impact groups, and average characteristics of most and least
impacted units. The approach is valid in high dimensional settings, where the
effects are proxied (but not necessarily consistently estimated) by predictive
and causal machine learning methods. We post-process these proxies into
estimates of the key features. Our approach is generic, it can be used in
conjunction with penalized methods, neural networks, random forests, boosted
trees, and ensemble methods, both predictive and causal. Estimation and
inference are based on repeated data splitting to avoid overfitting and achieve
validity. We use quantile aggregation of the results across many potential
splits, in particular taking medians of p-values and medians and other
quantiles of confidence intervals. We show that quantile aggregation lowers
estimation risks over a single split procedure, and establish its principal
inferential properties. Finally, our analysis reveals ways to build provably
better machine learning proxies through causal learning: we can use the
objective functions that we develop to construct the best linear predictors of
the effects, to obtain better machine learning proxies in the initial step. We
illustrate the use of both inferential tools and causal learners with a
randomized field experiment that evaluates a combination of nudges to stimulate
demand for immunization in India.","['Victor Chernozhukov', 'Mert Demirer', 'Esther Duflo', 'Iván Fernández-Val']","['stat.ML', 'econ.EM', 'math.ST', 'stat.TH']",2017-12-13 14:47:57+00:00
http://arxiv.org/abs/1712.04775v1,Multiple testing for outlier detection in functional data,"We propose a novel procedure for outlier detection in functional data, in a
semi-supervised framework. As the data is functional, we consider the
coefficients obtained after projecting the observations onto orthonormal bases
(wavelet, PCA). A multiple testing procedure based on the two-sample test is
defined in order to highlight the levels of the coefficients on which the
outliers appear as significantly different to the normal data. The selected
coefficients are then called features for the outlier detection, on which we
compute the Local Outlier Factor to highlight the outliers. This procedure to
select the features is applied on simulated data that mimic the behaviour of
space telemetries, and compared with existing dimension reduction techniques.","['Clémentine Barreyre', 'Béatrice Laurent', 'Jean-Michel Loubes', 'Bertrand Cabon', 'Loïc Boussouf']","['stat.ML', 'stat.AP', 'stat.ME']",2017-12-13 14:07:55+00:00
http://arxiv.org/abs/1712.04755v4,Exponential convergence of testing error for stochastic gradient methods,"We consider binary classification problems with positive definite kernels and
square loss, and study the convergence rates of stochastic gradient methods. We
show that while the excess testing loss (squared loss) converges slowly to zero
as the number of observations (and thus iterations) goes to infinity, the
testing error (classification error) converges exponentially fast if low-noise
conditions are assumed.","['Loucas Pillaud-Vivien', 'Alessandro Rudi', 'Francis Bach']","['cs.LG', 'stat.ML']",2017-12-13 13:35:27+00:00
http://arxiv.org/abs/1712.04709v1,A Quantum Extension of Variational Bayes Inference,"Variational Bayes (VB) inference is one of the most important algorithms in
machine learning and widely used in engineering and industry. However, VB is
known to suffer from the problem of local optima. In this Letter, we generalize
VB by using quantum mechanics, and propose a new algorithm, which we call
quantum annealing variational Bayes (QAVB) inference. We then show that QAVB
drastically improve the performance of VB by applying them to a clustering
problem described by a Gaussian mixture model. Finally, we discuss an intuitive
understanding on how QAVB works well.","['Hideyuki Miyahara', 'Yuki Sughiyama']","['stat.ML', 'cond-mat.stat-mech', 'quant-ph']",2017-12-13 11:18:22+00:00
http://arxiv.org/abs/1712.04688v1,Stability Selection for Structured Variable Selection,"In variable or graph selection problems, finding a right-sized model or
controlling the number of false positives is notoriously difficult. Recently, a
meta-algorithm called Stability Selection was proposed that can provide
reliable finite-sample control of the number of false positives. Its benefits
were demonstrated when used in conjunction with the lasso and orthogonal
matching pursuit algorithms.
  In this paper, we investigate the applicability of stability selection to
structured selection algorithms: the group lasso and the structured
input-output lasso. We find that using stability selection often increases the
power of both algorithms, but that the presence of complex structure reduces
the reliability of error control under stability selection. We give strategies
for setting tuning parameters to obtain a good model size under stability
selection, and highlight its strengths and weaknesses compared to competing
methods screen and clean and cross-validation. We give guidelines about when to
use which error control method.","['George Philipp', 'Seunghak Lee', 'Eric P. Xing']","['stat.ML', 'cs.LG']",2017-12-13 10:20:15+00:00
http://arxiv.org/abs/1712.04667v5,Empirical Variance Minimization with Applications in Variance Reduction and Optimal Control,"We study the problem of empirical minimization for variance-type functionals
over functional classes. Sharp non-asymptotic bounds for the excess variance
are derived under mild conditions. In particular, it is shown that under some
restrictions imposed on the functional class fast convergence rates can be
achieved including the optimal non-parametric rates for expressive classes in
the non-Donsker regime under some additional assumptions. Our main applications
include variance reduction and optimal control.","['D. Belomestny', 'L. Iosipoi', 'Q. Paris', 'N. Zhivotovskiy']","['math.NA', 'cs.NA', 'stat.ML']",2017-12-13 09:09:09+00:00
http://arxiv.org/abs/1712.04644v1,Stochastic Low-Rank Bandits,"Many problems in computer vision and recommender systems involve low-rank
matrices. In this work, we study the problem of finding the maximum entry of a
stochastic low-rank matrix from sequential observations. At each step, a
learning agent chooses pairs of row and column arms, and receives the noisy
product of their latent values as a reward. The main challenge is that the
latent values are unobserved. We identify a class of non-negative matrices
whose maximum entry can be found statistically efficiently and propose an
algorithm for finding them, which we call LowRankElim. We derive a
$\DeclareMathOperator{\poly}{poly} O((K + L) \poly(d) \Delta^{-1} \log n)$
upper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the
number of columns, $d$ is the rank of the matrix, and $\Delta$ is the minimum
gap. The bound depends on other problem-specific constants that clearly do not
depend $K L$. To the best of our knowledge, this is the first such result in
the literature.","['Branislav Kveton', 'Csaba Szepesvari', 'Anup Rao', 'Zheng Wen', 'Yasin Abbasi-Yadkori', 'S. Muthukrishnan']","['cs.LG', 'stat.ML']",2017-12-13 07:59:48+00:00
http://arxiv.org/abs/1712.04603v1,Multi-focus Attention Network for Efficient Deep Reinforcement Learning,"Deep reinforcement learning (DRL) has shown incredible performance in
learning various tasks to the human level. However, unlike human perception,
current DRL models connect the entire low-level sensory input to the
state-action values rather than exploiting the relationship between and among
entities that constitute the sensory input. Because of this difference, DRL
needs vast amount of experience samples to learn. In this paper, we propose a
Multi-focus Attention Network (MANet) which mimics human ability to spatially
abstract the low-level sensory input into multiple entities and attend to them
simultaneously. The proposed method first divides the low-level input into
several segments which we refer to as partial states. After this segmentation,
parallel attention layers attend to the partial states relevant to solving the
task. Our model estimates state-action values using these attended partial
states. In our experiments, MANet attains highest scores with significantly
less experience samples. Additionally, the model shows higher performance
compared to the Deep Q-network and the single attention model as benchmarks.
Furthermore, we extend our model to attentive communication model for
performing multi-agent cooperative tasks. In multi-agent cooperative task
experiments, our model shows 20% faster learning than existing state-of-the-art
model.","['Jinyoung Choi', 'Beom-Jin Lee', 'Byoung-Tak Zhang']","['cs.LG', 'cs.AI', 'stat.ML']",2017-12-13 04:04:29+00:00
http://arxiv.org/abs/1712.04567v1,Practical Bayesian optimization in the presence of outliers,"Inference in the presence of outliers is an important field of research as
outliers are ubiquitous and may arise across a variety of problems and domains.
Bayesian optimization is method that heavily relies on probabilistic inference.
This allows outstanding sample efficiency because the probabilistic machinery
provides a memory of the whole optimization process. However, that virtue
becomes a disadvantage when the memory is populated with outliers, inducing
bias in the estimation. In this paper, we present an empirical evaluation of
Bayesian optimization methods in the presence of outliers. The empirical
evidence shows that Bayesian optimization with robust regression often produces
suboptimal results. We then propose a new algorithm which combines robust
regression (a Gaussian process with Student-t likelihood) with outlier
diagnostics to classify data points as outliers or inliers. By using an
scheduler for the classification of outliers, our method is more efficient and
has better convergence over the standard robust regression. Furthermore, we
show that even in controlled situations with no expected outliers, our method
is able to produce better results.","['Ruben Martinez-Cantin', 'Kevin Tee', 'Michael McCourt']","['cs.LG', 'stat.ML', '90C26, 62K25, 62F35']",2017-12-12 23:31:45+00:00
http://arxiv.org/abs/1712.04543v2,A Mathematical Programming Approach for Integrated Multiple Linear Regression Subset Selection and Validation,"Subset selection for multiple linear regression aims to construct a
regression model that minimizes errors by selecting a small number of
explanatory variables. Once a model is built, various statistical tests and
diagnostics are conducted to validate the model and to determine whether the
regression assumptions are met. Most traditional approaches require human
decisions at this step. For example, the user adding or removing a variable
until a satisfactory model is obtained. However, this trial-and-error strategy
cannot guarantee that a subset that minimizes the errors while satisfying all
regression assumptions will be found. In this paper, we propose a fully
automated model building procedure for multiple linear regression subset
selection that integrates model building and validation based on mathematical
programming. The proposed model minimizes mean squared errors while ensuring
that the majority of the important regression assumptions are met. We also
propose an efficient constraint to approximate the constraint for the
coefficient t-test. When no subset satisfies all of the considered regression
assumptions, our model provides an alternative subset that satisfies most of
these assumptions. Computational results show that our model yields better
solutions (i.e., satisfying more regression assumptions) compared to the
state-of-the-art benchmark models while maintaining similar explanatory power.","['Seokhyun Chung', 'Young Woong Park', 'Taesu Cheong']",['stat.ML'],2017-12-12 21:42:01+00:00
http://arxiv.org/abs/1712.04542v2,Learning Sparse Graphs for Prediction and Filtering of Multivariate Data Processes,"We address the problem of prediction of multivariate data process using an
underlying graph model. We develop a method that learns a sparse partial
correlation graph in a tuning-free and computationally efficient manner.
Specifically, the graph structure is learned recursively without the need for
cross-validation or parameter tuning by building upon a hyperparameter-free
framework. Our approach does not require the graph to be undirected and also
accommodates varying noise levels across different nodes.Experiments using
real-world datasets show that the proposed method offers significant
performance gains in prediction, in comparison with the graphs frequently
associated with these datasets.","['Arun Venkitaraman', 'Dave Zachariah']","['stat.ML', 'stat.CO']",2017-12-12 21:39:18+00:00
http://arxiv.org/abs/1712.04432v4,"Integrated Model, Batch and Domain Parallelism in Training Neural Networks","We propose a new integrated method of exploiting model, batch and domain
parallelism for the training of deep neural networks (DNNs) on large
distributed-memory computers using minibatch stochastic gradient descent (SGD).
Our goal is to find an efficient parallelization strategy for a fixed batch
size using $P$ processes. Our method is inspired by the communication-avoiding
algorithms in numerical linear algebra. We see $P$ processes as logically
divided into a $P_r \times P_c$ grid where the $P_r$ dimension is implicitly
responsible for model/domain parallelism and the $P_c$ dimension is implicitly
responsible for batch parallelism. In practice, the integrated matrix-based
parallel algorithm encapsulates these types of parallelism automatically. We
analyze the communication complexity and analytically demonstrate that the
lowest communication costs are often achieved neither with pure model nor with
pure data parallelism. We also show how the domain parallel approach can help
in extending the theoretical scaling limit of the typical batch parallel
method.","['Amir Gholami', 'Ariful Azad', 'Peter Jin', 'Kurt Keutzer', 'Aydin Buluc']","['cs.LG', 'stat.ML']",2017-12-12 18:42:07+00:00
http://arxiv.org/abs/1712.04407v1,Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks,"Designing a logo for a new brand is a lengthy and tedious back-and-forth
process between a designer and a client. In this paper we explore to what
extent machine learning can solve the creative task of the designer. For this,
we build a dataset -- LLD -- of 600k+ logos crawled from the world wide web.
Training Generative Adversarial Networks (GANs) for logo synthesis on such
multi-modal data is not straightforward and results in mode collapse for some
state-of-the-art methods. We propose the use of synthetic labels obtained
through clustering to disentangle and stabilize GAN training. We are able to
generate a high diversity of plausible logos and we demonstrate latent space
exploration techniques to ease the logo design task in an interactive manner.
Moreover, we validate the proposed clustered GAN training on CIFAR 10,
achieving state-of-the-art Inception scores when using synthetic labels
obtained via clustering the features of an ImageNet classifier. GANs can cope
with multi-modal data by means of synthetic labels achieved through clustering,
and our results show the creative potential of such techniques for logo
synthesis and manipulation. Our dataset and models will be made publicly
available at https://data.vision.ee.ethz.ch/cvl/lld/.","['Alexander Sage', 'Eirikur Agustsson', 'Radu Timofte', 'Luc Van Gool']","['cs.CV', 'cs.LG', 'stat.ML']",2017-12-12 17:51:23+00:00
http://arxiv.org/abs/1712.04356v1,CUSBoost: Cluster-based Under-sampling with Boosting for Imbalanced Classification,"Class imbalance classification is a challenging research problem in data
mining and machine learning, as most of the real-life datasets are often
imbalanced in nature. Existing learning algorithms maximise the classification
accuracy by correctly classifying the majority class, but misclassify the
minority class. However, the minority class instances are representing the
concept with greater interest than the majority class instances in real-life
applications. Recently, several techniques based on sampling methods
(under-sampling of the majority class and over-sampling the minority class),
cost-sensitive learning methods, and ensemble learning have been used in the
literature for classifying imbalanced datasets. In this paper, we introduce a
new clustering-based under-sampling approach with boosting (AdaBoost)
algorithm, called CUSBoost, for effective imbalanced classification. The
proposed algorithm provides an alternative to RUSBoost (random under-sampling
with AdaBoost) and SMOTEBoost (synthetic minority over-sampling with AdaBoost)
algorithms. We evaluated the performance of CUSBoost algorithm with the
state-of-the-art methods based on ensemble learning like AdaBoost, RUSBoost,
SMOTEBoost on 13 imbalance binary and multi-class datasets with various
imbalance ratios. The experimental results show that the CUSBoost is a
promising and effective approach for dealing with highly imbalanced datasets.","['Farshid Rayhan', 'Sajid Ahmed', 'Asif Mahbub', 'Md. Rafsan Jani', 'Swakkhar Shatabda', 'Dewan Md. Farid']","['cs.LG', 'stat.ML']",2017-12-12 15:33:26+00:00
http://arxiv.org/abs/1712.04323v4,Deep Echo State Network (DeepESN): A Brief Survey,"The study of deep recurrent neural networks (RNNs) and, in particular, of
deep Reservoir Computing (RC) is gaining an increasing research attention in
the neural networks community. The recently introduced Deep Echo State Network
(DeepESN) model opened the way to an extremely efficient approach for designing
deep neural networks for temporal data. At the same time, the study of DeepESNs
allowed to shed light on the intrinsic properties of state dynamics developed
by hierarchical compositions of recurrent layers, i.e. on the bias of depth in
RNNs architectural design. In this paper, we summarize the advancements in the
development, analysis and applications of DeepESNs.","['Claudio Gallicchio', 'Alessio Micheli']","['cs.LG', 'cs.AI', 'stat.ML']",2017-12-12 14:50:51+00:00
http://arxiv.org/abs/1712.04276v1,Multi-Speaker Localization Using Convolutional Neural Network Trained with Noise,"The problem of multi-speaker localization is formulated as a multi-class
multi-label classification problem, which is solved using a convolutional
neural network (CNN) based source localization method. Utilizing the common
assumption of disjoint speaker activities, we propose a novel method to train
the CNN using synthesized noise signals. The proposed localization method is
evaluated for two speakers and compared to a well-known steered response power
method.","['Soumitro Chakrabarty', 'Emanuël A. P. Habets']","['cs.SD', 'eess.AS', 'stat.ML']",2017-12-12 13:17:30+00:00
http://arxiv.org/abs/1712.04248v2,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,"Many machine learning algorithms are vulnerable to almost imperceptible
perturbations of their inputs. So far it was unclear how much risk adversarial
perturbations carry for the safety of real-world machine learning applications
because most methods used to generate such perturbations rely either on
detailed model information (gradient-based attacks) or on confidence scores
such as class probabilities (score-based attacks), neither of which are
available in most real-world scenarios. In many such cases one currently needs
to retreat to transfer-based attacks which rely on cumbersome substitute
models, need access to the training data and can be defended against. Here we
emphasise the importance of attacks which solely rely on the final model
decision. Such decision-based attacks are (1) applicable to real-world
black-box models such as autonomous cars, (2) need less knowledge and are
easier to apply than transfer-based attacks and (3) are more robust to simple
defences than gradient- or score-based attacks. Previous attacks in this
category were limited to simple models or simple datasets. Here we introduce
the Boundary Attack, a decision-based attack that starts from a large
adversarial perturbation and then seeks to reduce the perturbation while
staying adversarial. The attack is conceptually simple, requires close to no
hyperparameter tuning, does not rely on substitute models and is competitive
with the best gradient-based attacks in standard computer vision tasks like
ImageNet. We apply the attack on two black-box algorithms from Clarifai.com.
The Boundary Attack in particular and the class of decision-based attacks in
general open new avenues to study the robustness of machine learning models and
raise new questions regarding the safety of deployed machine learning systems.
An implementation of the attack is available as part of Foolbox at
https://github.com/bethgelab/foolbox .","['Wieland Brendel', 'Jonas Rauber', 'Matthias Bethge']","['stat.ML', 'cs.CR', 'cs.CV', 'cs.LG', 'cs.NE']",2017-12-12 11:36:26+00:00
http://arxiv.org/abs/1712.04221v1,Causal Patterns: Extraction of multiple causal relationships by Mixture of Probabilistic Partial Canonical Correlation Analysis,"In this paper, we propose a mixture of probabilistic partial canonical
correlation analysis (MPPCCA) that extracts the Causal Patterns from two
multivariate time series. Causal patterns refer to the signal patterns within
interactions of two elements having multiple types of mutually causal
relationships, rather than a mixture of simultaneous correlations or the
absence of presence of a causal relationship between the elements. In
multivariate statistics, partial canonical correlation analysis (PCCA)
evaluates the correlation between two multivariates after subtracting the
effect of the third multivariate. PCCA can calculate the Granger Causal- ity
Index (which tests whether a time-series can be predicted from an- other
time-series), but is not applicable to data containing multiple partial
canonical correlations. After introducing the MPPCCA, we propose an
expectation-maxmization (EM) algorithm that estimates the parameters and latent
variables of the MPPCCA. The MPPCCA is expected to ex- tract multiple partial
canonical correlations from data series without any supervised signals to split
the data as clusters. The method was then eval- uated in synthetic data
experiments. In the synthetic dataset, our method estimated the multiple
partial canonical correlations more accurately than the existing method. To
determine the types of patterns detectable by the method, experiments were also
conducted on real datasets. The method estimated the communication patterns In
motion-capture data. The MP- PCCA is applicable to various type of signals such
as brain signals, human communication and nonlinear complex multibody systems.","['Hiroki Mori', 'Keisuke Kawano', 'Hiroki Yokoyama']","['stat.ME', 'stat.ML']",2017-12-12 10:46:27+00:00
http://arxiv.org/abs/1712.04195v1,Concept Formation and Dynamics of Repeated Inference in Deep Generative Models,"Deep generative models are reported to be useful in broad applications
including image generation. Repeated inference between data space and latent
space in these models can denoise cluttered images and improve the quality of
inferred results. However, previous studies only qualitatively evaluated image
outputs in data space, and the mechanism behind the inference has not been
investigated. The purpose of the current study is to numerically analyze
changes in activity patterns of neurons in the latent space of a deep
generative model called a ""variational auto-encoder"" (VAE). What kinds of
inference dynamics the VAE demonstrates when noise is added to the input data
are identified. The VAE embeds a dataset with clear cluster structures in the
latent space and the center of each cluster of multiple correlated data points
(memories) is referred as the concept. Our study demonstrated that transient
dynamics of inference first approaches a concept, and then moves close to a
memory. Moreover, the VAE revealed that the inference dynamics approaches a
more abstract concept to the extent that the uncertainty of input data
increases due to noise. It was demonstrated that by increasing the number of
the latent variables, the trend of the inference dynamics to approach a concept
can be enhanced, and the generalization ability of the VAE can be improved.","['Yoshihiro Nagano', 'Ryo Karakida', 'Masato Okada']","['stat.ML', 'cs.LG', 'cs.NE', 'q-bio.NC']",2017-12-12 09:55:11+00:00
http://arxiv.org/abs/1712.04165v3,Temporal Stability in Predictive Process Monitoring,"Predictive process monitoring is concerned with the analysis of events
produced during the execution of a business process in order to predict as
early as possible the final outcome of an ongoing case. Traditionally,
predictive process monitoring methods are optimized with respect to accuracy.
However, in environments where users make decisions and take actions in
response to the predictions they receive, it is equally important to optimize
the stability of the successive predictions made for each case. To this end,
this paper defines a notion of temporal stability for binary classification
tasks in predictive process monitoring and evaluates existing methods with
respect to both temporal stability and accuracy. We find that methods based on
XGBoost and LSTM neural networks exhibit the highest temporal stability. We
then show that temporal stability can be enhanced by hyperparameter-optimizing
random forests and XGBoost classifiers with respect to inter-run stability.
Finally, we show that time series smoothing techniques can further enhance
temporal stability at the expense of slightly lower accuracy.","['Irene Teinemaa', 'Marlon Dumas', 'Anna Leontjeva', 'Fabrizio Maria Maggi']","['cs.LG', 'stat.ML']",2017-12-12 08:20:56+00:00
http://arxiv.org/abs/1712.04146v2,A Random Sample Partition Data Model for Big Data Analysis,"Big data sets must be carefully partitioned into statistically similar data
subsets that can be used as representative samples for big data analysis tasks.
In this paper, we propose the random sample partition (RSP) data model to
represent a big data set as a set of non-overlapping data subsets, called RSP
data blocks, where each RSP data block has a probability distribution similar
to the whole big data set. Under this data model, efficient block level
sampling is used to randomly select RSP data blocks, replacing expensive record
level sampling to select sample data from a big distributed data set on a
computing cluster. We show how RSP data blocks can be employed to estimate
statistics of a big data set and build models which are equivalent to those
built from the whole big data set. In this approach, analysis of a big data set
becomes analysis of few RSP data blocks which have been generated in advance on
the computing cluster. Therefore, the new method for data analysis based on RSP
data blocks is scalable to big data.","['Salman Salloum', 'Yulin He', 'Joshua Zhexue Huang', 'Xiaoliang Zhang', 'Tamer Z. Emara', 'Chenghao Wei', 'Heping He']","['cs.DC', 'cs.DS', 'physics.data-an', 'stat.ML']",2017-12-12 06:49:28+00:00
http://arxiv.org/abs/1712.04145v1,Transportation analysis of denoising autoencoders: a novel method for analyzing deep neural networks,"The feature map obtained from the denoising autoencoder (DAE) is investigated
by determining transportation dynamics of the DAE, which is a cornerstone for
deep learning. Despite the rapid development in its application, deep neural
networks remain analytically unexplained, because the feature maps are nested
and parameters are not faithful. In this paper, we address the problem of the
formulation of nested complex of parameters by regarding the feature map as a
transport map. Even when a feature map has different dimensions between input
and output, we can regard it as a transportation map by considering that both
the input and output spaces are embedded in a common high-dimensional space. In
addition, the trajectory is a geometric object and thus, is independent of
parameterization. In this manner, transportation can be regarded as a universal
character of deep neural networks. By determining and analyzing the
transportation dynamics, we can understand the behavior of a deep neural
network. In this paper, we investigate a fundamental case of deep neural
networks: the DAE. We derive the transport map of the DAE, and reveal that the
infinitely deep DAE transports mass to decrease a certain quantity, such as
entropy, of the data distribution. These results though analytically simple,
shed light on the correspondence between deep neural networks and the
Wasserstein gradient flows.","['Sho Sonoda', 'Noboru Murata']","['cs.LG', 'stat.ML']",2017-12-12 06:37:28+00:00
http://arxiv.org/abs/1712.04144v1,Information Perspective to Probabilistic Modeling: Boltzmann Machines versus Born Machines,"We compare and contrast the statistical physics and quantum physics inspired
approaches for unsupervised generative modeling of classical data. The two
approaches represent probabilities of observed data using energy-based models
and quantum states respectively.Classical and quantum information patterns of
the target datasets therefore provide principled guidelines for structural
design and learning in these two approaches. Taking the restricted Boltzmann
machines (RBM) as an example, we analyze the information theoretical bounds of
the two approaches. We verify our reasonings by comparing the performance of
RBMs of various architectures on the standard MNIST datasets.","['Song Cheng', 'Jing Chen', 'Lei Wang']","['physics.data-an', 'cond-mat.stat-mech', 'quant-ph', 'stat.ML']",2017-12-12 06:34:10+00:00
http://arxiv.org/abs/1712.04135v1,Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems,"Intelligent transportation systems (ITSs) will be a major component of
tomorrow's smart cities. However, realizing the true potential of ITSs requires
ultra-low latency and reliable data analytics solutions that can combine, in
real-time, a heterogeneous mix of data stemming from the ITS network and its
environment. Such data analytics capabilities cannot be provided by
conventional cloud-centric data processing techniques whose communication and
computing latency can be high. Instead, edge-centric solutions that are
tailored to the unique ITS environment must be developed. In this paper, an
edge analytics architecture for ITSs is introduced in which data is processed
at the vehicle or roadside smart sensor level in order to overcome the ITS
latency and reliability challenges. With a higher capability of passengers'
mobile devices and intra-vehicle processors, such a distributed edge computing
architecture can leverage deep learning techniques for reliable mobile sensing
in ITSs. In this context, the ITS mobile edge analytics challenges pertaining
to heterogeneous data, autonomous control, vehicular platoon control, and
cyber-physical security are investigated. Then, different deep learning
solutions for such challenges are proposed. The proposed deep learning
solutions will enable ITS edge analytics by endowing the ITS devices with
powerful computer vision and signal processing functions. Preliminary results
show that the proposed edge analytics architecture, coupled with the power of
deep learning algorithms, can provide a reliable, secure, and truly smart
transportation environment.","['Aidin Ferdowsi', 'Ursula Challita', 'Walid Saad']","['cs.IT', 'math.IT', 'stat.ML']",2017-12-12 05:12:44+00:00
http://arxiv.org/abs/1712.04129v2,Outlier Detection by Consistent Data Selection Method,"Often the challenge associated with tasks like fraud and spam detection[1] is
the lack of all likely patterns needed to train suitable supervised learning
models. In order to overcome this limitation, such tasks are attempted as
outlier or anomaly detection tasks. We also hypothesize that out- liers have
behavioral patterns that change over time. Limited data and continuously
changing patterns makes learning significantly difficult. In this work we are
proposing an approach that detects outliers in large data sets by relying on
data points that are consistent. The primary contribution of this work is that
it will quickly help retrieve samples for both consistent and non-outlier data
sets and is also mindful of new outlier patterns. No prior knowledge of each
set is required to extract the samples. The method consists of two phases, in
the first phase, consistent data points (non- outliers) are retrieved by an
ensemble method of unsupervised clustering techniques and in the second phase a
one class classifier trained on the consistent data point set is ap- plied on
the remaining sample set to identify the outliers. The approach is tested on
three publicly available data sets and the performance scores are competitive.","['Utkarsh Porwal', 'Smruthi Mukund']","['cs.LG', 'stat.ML']",2017-12-12 04:57:40+00:00
http://arxiv.org/abs/1712.04120v1,GibbsNet: Iterative Adversarial Inference for Deep Graphical Models,"Directed latent variable models that formulate the joint distribution as
$p(x,z) = p(z) p(x \mid z)$ have the advantage of fast and exact sampling.
However, these models have the weakness of needing to specify $p(z)$, often
with a simple fixed prior that limits the expressiveness of the model.
Undirected latent variable models discard the requirement that $p(z)$ be
specified with a prior, yet sampling from them generally requires an iterative
procedure such as blocked Gibbs-sampling that may require many steps to draw
samples from the joint distribution $p(x, z)$. We propose a novel approach to
learning the joint distribution between the data and a latent code which uses
an adversarially learned iterative procedure to gradually refine the joint
distribution, $p(x, z)$, to better match with the data distribution on each
step. GibbsNet is the best of both worlds both in theory and in practice.
Achieving the speed and simplicity of a directed latent variable model, it is
guaranteed (assuming the adversarial game reaches the virtual training criteria
global minimum) to produce samples from $p(x, z)$ with only a few sampling
iterations. Achieving the expressiveness and flexibility of an undirected
latent variable model, GibbsNet does away with the need for an explicit $p(z)$
and has the ability to do attribute prediction, class-conditional generation,
and joint image-attribute modeling in a single model which is not trained for
any of these specific tasks. We show empirically that GibbsNet is able to learn
a more complex $p(z)$ and show that this leads to improved inpainting and
iterative refinement of $p(x, z)$ for dozens of steps and stable generation
without collapse for thousands of steps, despite being trained on only a few
steps.","['Alex Lamb', 'Devon Hjelm', 'Yaroslav Ganin', 'Joseph Paul Cohen', 'Aaron Courville', 'Yoshua Bengio']","['stat.ML', 'cs.LG']",2017-12-12 04:16:52+00:00
http://arxiv.org/abs/1712.04118v1,Neural Component Analysis for Fault Detection,"Principal component analysis (PCA) is largely adopted for chemical process
monitoring and numerous PCA-based systems have been developed to solve various
fault detection and diagnosis problems. Since PCA-based methods assume that the
monitored process is linear, nonlinear PCA models, such as autoencoder models
and kernel principal component analysis (KPCA), has been proposed and applied
to nonlinear process monitoring. However, KPCA-based methods need to perform
eigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on
the number of training data. Moreover, prefixed kernel parameters cannot be
most effective for different faults which may need different parameters to
maximize their respective detection performances. Autoencoder models lack the
consideration of orthogonal constraints which is crucial for PCA-based
algorithms. To address these problems, this paper proposes a novel nonlinear
method, called neural component analysis (NCA), which intends to train a
feedforward neural work with orthogonal constraints such as those used in PCA.
NCA can adaptively learn its parameters through backpropagation and the
dimensionality of the nonlinear features has no relationship with the number of
training samples. Extensive experimental results on the Tennessee Eastman (TE)
benchmark process show the superiority of NCA in terms of missed detection rate
(MDR) and false alarm rate (FAR). The source code of NCA can be found in
https://github.com/haitaozhao/Neural-Component-Analysis.git.",['Haitao Zhao'],"['cs.LG', 'cs.NE', 'stat.ML']",2017-12-12 04:11:37+00:00
http://arxiv.org/abs/1712.04086v3,PacGAN: The power of two samples in generative adversarial networks,"Generative adversarial networks (GANs) are innovative techniques for learning
generative models of complex data distributions from samples. Despite
remarkable recent improvements in generating realistic images, one of their
major shortcomings is the fact that in practice, they tend to produce samples
with little diversity, even when trained on diverse datasets. This phenomenon,
known as mode collapse, has been the main focus of several recent advances in
GANs. Yet there is little understanding of why mode collapse happens and why
existing approaches are able to mitigate mode collapse. We propose a principled
approach to handling mode collapse, which we call packing. The main idea is to
modify the discriminator to make decisions based on multiple samples from the
same class, either real or artificially generated. We borrow analysis tools
from binary hypothesis testing---in particular the seminal result of Blackwell
[Bla53]---to prove a fundamental connection between packing and mode collapse.
We show that packing naturally penalizes generators with mode collapse, thereby
favoring generator distributions with less mode collapse during the training
process. Numerical experiments on benchmark datasets suggests that packing
provides significant improvements in practice as well.","['Zinan Lin', 'Ashish Khetan', 'Giulia Fanti', 'Sewoong Oh']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2017-12-12 00:57:52+00:00
http://arxiv.org/abs/1712.04046v3,Character-Based Handwritten Text Transcription with Attention Networks,"The paper approaches the task of handwritten text recognition (HTR) with
attentional encoder-decoder networks trained on sequences of characters, rather
than words. We experiment on lines of text from popular handwriting datasets
and compare different activation functions for the attention mechanism used for
aligning image pixels and target characters. We find that softmax attention
focuses heavily on individual characters, while sigmoid attention focuses on
multiple characters at each step of the decoding. When the sequence alignment
is one-to-one, softmax attention is able to learn a more precise alignment at
each step of the decoding, whereas the alignment generated by sigmoid attention
is much less precise. When a linear function is used to obtain attention
weights, the model predicts a character by looking at the entire sequence of
characters and performs poorly because it lacks a precise alignment between the
source and target. Future research may explore HTR in natural scene images,
since the model is capable of transcribing handwritten text without the need
for producing segmentations or bounding boxes of text in images.","['Jason Poulos', 'Rafael Valle']","['cs.CV', 'cs.CL', 'stat.ML']",2017-12-11 21:57:03+00:00
http://arxiv.org/abs/1712.07682v1,Deep metric learning for multi-labelled radiographs,"Many radiological studies can reveal the presence of several co-existing
abnormalities, each one represented by a distinct visual pattern. In this
article we address the problem of learning a distance metric for plain
radiographs that captures a notion of ""radiological similarity"": two chest
radiographs are considered to be similar if they share similar abnormalities.
Deep convolutional neural networks (DCNs) are used to learn a low-dimensional
embedding for the radiographs that is equipped with the desired metric. Two
loss functions are proposed to deal with multi-labelled images and potentially
noisy labels. We report on a large-scale study involving over 745,000 chest
radiographs whose labels were automatically extracted from free-text
radiological reports through a natural language processing system. Using 4,500
validated exams, we demonstrate that the methodology performs satisfactorily on
clustering and image retrieval tasks. Remarkably, the learned metric separates
normal exams from those having radiological abnormalities.","['Mauro Annarumma', 'Giovanni Montana']","['stat.ML', 'cs.CV']",2017-12-11 20:55:08+00:00
http://arxiv.org/abs/1712.03999v1,Eye In-Painting with Exemplar Generative Adversarial Networks,"This paper introduces a novel approach to in-painting where the identity of
the object to remove or change is preserved and accounted for at inference
time: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize
exemplar information to produce high-quality, personalized in painting results.
We propose using exemplar information in the form of a reference image of the
region to in-paint, or a perceptual code describing that object. Unlike
previous conditional GAN formulations, this extra information can be inserted
at multiple points within the adversarial network, thus increasing its
descriptive power. We show that ExGANs can produce photo-realistic personalized
in-painting results that are both perceptually and semantically plausible by
applying them to the task of closed to-open eye in-painting in natural
pictures. A new benchmark dataset is also introduced for the task of eye
in-painting for future comparisons.","['Brian Dolhansky', 'Cristian Canton Ferrer']","['cs.CV', 'cs.LG', 'stat.ML']",2017-12-11 19:40:55+00:00
http://arxiv.org/abs/1712.04350v1,Predicting Yelp Star Reviews Based on Network Structure with Deep Learning,"In this paper, we tackle the real-world problem of predicting Yelp
star-review rating based on business features (such as images, descriptions),
user features (average previous ratings), and, of particular interest, network
properties (which businesses has a user rated before). We compare multiple
models on different sets of features -- from simple linear regression on
network features only to deep learning models on network and item features.
  In recent years, breakthroughs in deep learning have led to increased
accuracy in common supervised learning tasks, such as image classification,
captioning, and language understanding. However, the idea of combining deep
learning with network feature and structure appears to be novel. While the
problem of predicting future interactions in a network has been studied at
length, these approaches have often ignored either node-specific data or global
structure.
  We demonstrate that taking a mixed approach combining both node-level
features and network information can effectively be used to predict Yelp-review
star ratings. We evaluate on the Yelp dataset by splitting our data along the
time dimension (as would naturally occur in the real-world) and comparing our
model against others which do no take advantage of the network structure and/or
deep learning.",['Luis Perez'],"['cs.LG', 'stat.ML']",2017-12-11 18:54:23+00:00
http://arxiv.org/abs/1712.03893v1,Towards reduction of autocorrelation in HMC by machine learning,"In this paper we propose new algorithm to reduce autocorrelation in Markov
chain Monte-Carlo algorithms for euclidean field theories on the lattice. Our
proposing algorithm is the Hybrid Monte-Carlo algorithm (HMC) with restricted
Boltzmann machine. We examine the validity of the algorithm by employing the
phi-fourth theory in three dimension. We observe reduction of the
autocorrelation both in symmetric and broken phase as well. Our proposing
algorithm provides consistent central values of expectation values of the
action density and one-point Green's function with ones from the original HMC
in both the symmetric phase and broken phase within the statistical error. On
the other hand, two-point Green's functions have slight difference between one
calculated by the HMC and one by our proposing algorithm in the symmetric
phase. Furthermore, near the criticality, the distribution of the one-point
Green's function differs from the one from HMC. We discuss the origin of
discrepancies and its improvement.","['Akinori Tanaka', 'Akio Tomiya']","['hep-lat', 'cond-mat.dis-nn', 'stat.ML']",2017-12-11 17:12:11+00:00
http://arxiv.org/abs/1712.03878v5,Generalized Zero-Shot Learning via Synthesized Examples,"We present a generative framework for generalized zero-shot learning where
the training and test classes are not necessarily disjoint. Built upon a
variational autoencoder based architecture, consisting of a probabilistic
encoder and a probabilistic conditional decoder, our model can generate novel
exemplars from seen/unseen classes, given their respective class attributes.
These exemplars can subsequently be used to train any off-the-shelf
classification model. One of the key aspects of our encoder-decoder
architecture is a feedback-driven mechanism in which a discriminator (a
multivariate regressor) learns to map the generated exemplars to the
corresponding class attribute vectors, leading to an improved generator. Our
model's ability to generate and leverage examples from unseen classes to train
the classification model naturally helps to mitigate the bias towards
predicting seen classes in generalized zero-shot learning settings. Through a
comprehensive set of experiments, we show that our model outperforms several
state-of-the-art methods, on several benchmark datasets, for both standard as
well as generalized zero-shot learning.","['Vinay Kumar Verma', 'Gundeep Arora', 'Ashish Mishra', 'Piyush Rai']","['cs.LG', 'cs.CV', 'stat.ML']",2017-12-11 16:44:12+00:00
http://arxiv.org/abs/1712.03847v1,On Quadratic Penalties in Elastic Weight Consolidation,"Elastic weight consolidation (EWC, Kirkpatrick et al, 2017) is a novel
algorithm designed to safeguard against catastrophic forgetting in neural
networks. EWC can be seen as an approximation to Laplace propagation (Eskin et
al, 2004), and this view is consistent with the motivation given by Kirkpatrick
et al (2017). In this note, I present an extended derivation that covers the
case when there are more than two tasks. I show that the quadratic penalties in
EWC are inconsistent with this derivation and might lead to double-counting
data from earlier tasks.",['Ferenc Huszár'],"['stat.ML', 'cs.LG']",2017-12-11 15:49:02+00:00
http://arxiv.org/abs/1712.03660v3,Parallel Mapper,"The construction of Mapper has emerged in the last decade as a powerful and
effective topological data analysis tool that approximates and generalizes
other topological summaries, such as the Reeb graph, the contour tree, split,
and joint trees. In this paper, we study the parallel analysis of the
construction of Mapper. We give a provably correct parallel algorithm to
execute Mapper on multiple processors and discuss the performance results that
compare our approach to a reference sequential Mapper implementation. We report
the performance experiments that demonstrate the efficiency of our method.","['Mustafa Hajij', 'Basem Assiri', 'Paul Rosen']","['cs.CV', 'cs.CG', 'cs.DC', 'stat.ML']",2017-12-11 07:02:06+00:00
http://arxiv.org/abs/1712.03638v2,Lifting high-dimensional nonlinear models with Gaussian regressors,"We study the problem of recovering a structured signal $\mathbf{x}_0$ from
high-dimensional data $\mathbf{y}_i=f(\mathbf{a}_i^T\mathbf{x}_0)$ for some
nonlinear (and potentially unknown) link function $f$, when the regressors
$\mathbf{a}_i$ are iid Gaussian. Brillinger (1982) showed that ordinary
least-squares estimates $\mathbf{x}_0$ up to a constant of proportionality
$\mu_\ell$, which depends on $f$. Recently, Plan & Vershynin (2015) extended
this result to the high-dimensional setting deriving sharp error bounds for the
generalized Lasso. Unfortunately, both least-squares and the Lasso fail to
recover $\mathbf{x}_0$ when $\mu_\ell=0$. For example, this includes all even
link functions. We resolve this issue by proposing and analyzing an alternative
convex recovery method. In a nutshell, our method treats such link functions as
if they were linear in a lifted space of higher-dimension. Interestingly, our
error analysis captures the effect of both the nonlinearity and the problem's
geometry in a few simple summary parameters.","['Christos Thrampoulidis', 'Ankit Singh Rawat']",['stat.ML'],2017-12-11 03:59:03+00:00
http://arxiv.org/abs/1712.03607v2,Gradient Normalization & Depth Based Decay For Deep Learning,"In this paper we introduce a novel method of gradient normalization and decay
with respect to depth. Our method leverages the simple concept of normalizing
all gradients in a deep neural network, and then decaying said gradients with
respect to their depth in the network. Our proposed normalization and decay
techniques can be used in conjunction with most current state of the art
optimizers and are a very simple addition to any network. This method, although
simple, showed improvements in convergence time on state of the art networks
such as DenseNet and ResNet on image classification tasks, as well as on an
LSTM for natural language processing tasks.","['Robert Kwiatkowski', 'Oscar Chang']","['cs.LG', 'stat.ML']",2017-12-10 23:01:13+00:00
http://arxiv.org/abs/1712.03605v1,Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural Networks,"We derive a novel sensitivity analysis of input variables for predictive
epistemic and aleatoric uncertainty. We use Bayesian neural networks with
latent variables as a model class and illustrate the usefulness of our
sensitivity analysis on real-world datasets. Our method increases the
interpretability of complex black-box probabilistic models.","['Stefan Depeweg', 'José Miguel Hernández-Lobato', 'Steffen Udluft', 'Thomas Runkler']",['stat.ML'],2017-12-10 22:57:42+00:00
http://arxiv.org/abs/1712.03563v1,DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model,"Convolutional neural networks (CNNs) can be applied to graph similarity
matching, in which case they are called graph CNNs. Graph CNNs are attracting
increasing attention due to their effectiveness and efficiency. However, the
existing convolution approaches focus only on regular data forms and require
the transfer of the graph or key node neighborhoods of the graph into the same
fixed form. During this transfer process, structural information of the graph
can be lost, and some redundant information can be incorporated. To overcome
this problem, we propose the disordered graph convolutional neural network
(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a
preprocessing layer called the disordered graph convolutional layer (DGCL). The
DGCL uses a mixed Gaussian function to realize the mapping between the
convolution kernel and the nodes in the neighborhood of the graph. The output
of the DGCL is the input of the CNN. We further implement a
backward-propagation optimization process of the convolutional layer by which
we incorporate the feature-learning model of the irregular node neighborhood
structure into the network. Thereafter, the optimization of the convolution
kernel becomes part of the neural network learning process. The DGCNN can
accept arbitrary scaled and disordered neighborhood graph structures as the
receptive fields of CNNs, which reduces information loss during graph
transformation. Finally, we perform experiments on multiple standard graph
datasets. The results show that the proposed method outperforms the
state-of-the-art methods in graph classification and retrieval.","['Bo Wu', 'Yang Liu', 'Bo Lang', 'Lei Huang']","['cs.LG', 'stat.ML']",2017-12-10 17:38:25+00:00
http://arxiv.org/abs/1712.03553v7,"RNN-based counterfactual prediction, with an application to homestead policy and public schooling","This paper proposes a method for estimating the effect of a policy
intervention on an outcome over time. We train recurrent neural networks (RNNs)
on the history of control unit outcomes to learn a useful representation for
predicting future outcomes. The learned representation of control units is then
applied to the treated units for predicting counterfactual outcomes. RNNs are
specifically structured to exploit temporal dependencies in panel data, and are
able to learn negative and nonlinear interactions between control unit
outcomes. We apply the method to the problem of estimating the long-run impact
of U.S. homestead policy on public school spending.","['Jason Poulos', 'Shuxi Zeng']","['stat.ML', 'econ.EM', 'stat.AP']",2017-12-10 16:00:18+00:00
http://arxiv.org/abs/1712.03541v2,An Architecture Combining Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for Image Classification,"Convolutional neural networks (CNNs) are similar to ""ordinary"" neural
networks in the sense that they are made up of hidden layers consisting of
neurons with ""learnable"" parameters. These neurons receive inputs, performs a
dot product, and then follows it with a non-linearity. The whole network
expresses the mapping between raw image pixels and their class scores.
Conventionally, the Softmax function is the classifier used at the last layer
of this network. However, there have been studies (Alalshekmubarak and Smith,
2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited
studies introduce the usage of linear support vector machine (SVM) in an
artificial neural network architecture. This project is yet another take on the
subject, and is inspired by (Tang, 2013). Empirical data has shown that the
CNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST
dataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax
was able to achieve a test accuracy of ~99.23% using the same dataset. Both
models were also tested on the recently-published Fashion-MNIST dataset (Xiao,
Rasul, and Vollgraf, 2017), which is suppose to be a more difficult image
classification dataset than MNIST (Zalandoresearch, 2017). This proved to be
the case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax
reached a test accuracy of ~91.86%. The said results may be improved if data
preprocessing techniques were employed on the datasets, and if the base CNN
model was a relatively more sophisticated than the one used in this study.",['Abien Fred Agarap'],"['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']",2017-12-10 14:50:28+00:00
http://arxiv.org/abs/1712.03483v1,Improving Malware Detection Accuracy by Extracting Icon Information,"Detecting PE malware files is now commonly approached using statistical and
machine learning models. While these models commonly use features extracted
from the structure of PE files, we propose that icons from these files can also
help better predict malware. We propose an innovative machine learning approach
to extract information from icons. Our proposed approach consists of two steps:
1) extracting icon features using summary statics, histogram of gradients
(HOG), and a convolutional autoencoder, 2) clustering icons based on the
extracted icon features. Using publicly available data and by using machine
learning experiments, we show our proposed icon clusters significantly boost
the efficacy of malware prediction models. In particular, our experiments show
an average accuracy increase of 10% when icon clusters are used in the
prediction model.","['Pedro Silva', 'Sepehr Akhavan-Masouleh', 'Li Li']","['cs.CR', 'stat.ML']",2017-12-10 08:15:12+00:00
http://arxiv.org/abs/1712.03480v1,Capsule Network Performance on Complex Data,"In recent years, convolutional neural networks (CNN) have played an important
role in the field of deep learning. Variants of CNN's have proven to be very
successful in classification tasks across different domains. However, there are
two big drawbacks to CNN's: their failure to take into account of important
spatial hierarchies between features, and their lack of rotational invariance.
As long as certain key features of an object are present in the test data,
CNN's classify the test data as the object, disregarding features' relative
spatial orientation to each other. This causes false positives. The lack of
rotational invariance in CNN's would cause the network to incorrectly assign
the object another label, causing false negatives. To address this concern,
Hinton et al. propose a novel type of neural network using the concept of
capsules in a recent paper. With the use of dynamic routing and reconstruction
regularization, the capsule network model would be both rotation invariant and
spatially aware. The capsule network has shown its potential by achieving a
state-of-the-art result of 0.25% test error on MNIST without data augmentation
such as rotation and scaling, better than the previous baseline of 0.39%. To
further test out the application of capsule networks on data with higher
dimensionality, we attempt to find the best set of configurations that yield
the optimal test error on CIFAR10 dataset.","['Edgar Xi', 'Selina Bing', 'Yang Jin']","['stat.ML', 'cs.LG']",2017-12-10 07:50:04+00:00
http://arxiv.org/abs/1712.03471v3,Identifiability of Kronecker-structured Dictionaries for Tensor Data,"This paper derives sufficient conditions for local recovery of coordinate
dictionaries comprising a Kronecker-structured dictionary that is used for
representing $K$th-order tensor data. Tensor observations are assumed to be
generated from a Kronecker-structured dictionary multiplied by sparse
coefficient tensors that follow the separable sparsity model. This work
provides sufficient conditions on the underlying coordinate dictionaries,
coefficient and noise distributions, and number of samples that guarantee
recovery of the individual coordinate dictionaries up to a specified error, as
a local minimum of the objective function, with high probability. In
particular, the sample complexity to recover $K$ coordinate dictionaries with
dimensions $m_k \times p_k$ up to estimation error $\varepsilon_k$ is shown to
be $\max_{k \in [K]}\mathcal{O}(m_kp_k^3\varepsilon_k^{-2})$.","['Zahra Shakeri', 'Anand D. Sarwate', 'Waheed U. Bajwa']","['stat.ML', 'cs.IT', 'math.IT']",2017-12-10 05:28:48+00:00
http://arxiv.org/abs/1712.03428v1,Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent,"In this paper, we propose a novel approach to automatically determine the
batch size in stochastic gradient descent methods. The choice of the batch size
induces a trade-off between the accuracy of the gradient estimate and the cost
in terms of samples of each update. We propose to determine the batch size by
optimizing the ratio between a lower bound to a linear or quadratic Taylor
approximation of the expected improvement and the number of samples used to
estimate the gradient. The performance of the proposed approach is empirically
compared with related methods on popular classification tasks.
  The work was presented at the NIPS workshop on Optimizing the Optimizers.
Barcelona, Spain, 2016.","['Matteo Pirotta', 'Marcello Restelli']","['cs.LG', 'stat.ML']",2017-12-09 19:59:25+00:00
http://arxiv.org/abs/1712.03412v4,Elastic-net Regularized High-dimensional Negative Binomial Regression: Consistency and Weak Signals Detection,"We study a sparse negative binomial regression (NBR) for count data by
showing the non-asymptotic advantages of using the elastic-net estimator. Two
types of oracle inequalities are derived for the NBR's elastic-net estimates by
using the Compatibility Factor Condition and the Stabil Condition. The second
type of oracle inequality is for the random design and can be extended to many
$\ell_1 + \ell_2$ regularized M-estimations, with the corresponding empirical
process having stochastic Lipschitz properties. We derive the concentration
inequality for the suprema empirical processes for the weighted sum of negative
binomial variables to show some high--probability events. We apply the method
by showing the sign consistency, provided that the nonzero components in the
true sparse vector are larger than a proper choice of the weakest signal
detection threshold. In the second application, we show the grouping effect
inequality with high probability. Third, under some assumptions for a design
matrix, we can recover the true variable set with a high probability if the
weakest signal detection threshold is large than the turning parameter up to a
known constant. Lastly, we briefly discuss the de-biased elastic-net estimator,
and numerical studies are given to support the proposal.","['Huiming Zhang', 'Jinzhu Jia']","['stat.ML', 'math.PR', 'math.ST', 'stat.TH']",2017-12-09 17:08:08+00:00
http://arxiv.org/abs/1712.03353v1,Variational Inference over Non-differentiable Cardiac Simulators using Bayesian Optimization,"Performing inference over simulators is generally intractable as their
runtime means we cannot compute a marginal likelihood. We develop a
likelihood-free inference method to infer parameters for a cardiac simulator,
which replicates electrical flow through the heart to the body surface. We
improve the fit of a state-of-the-art simulator to an electrocardiogram (ECG)
recorded from a real patient.","['Adam McCarthy', 'Blanca Rodriguez', 'Ana Minchole']",['stat.ML'],2017-12-09 08:11:17+00:00
http://arxiv.org/abs/1712.03351v1,Peephole: Predicting Network Performance Before Training,"The quest for performant networks has been a significant force that drives
the advancements of deep learning in recent years. While rewarding, improving
network design has never been an easy journey. The large design space combined
with the tremendous cost required for network training poses a major obstacle
to this endeavor. In this work, we propose a new approach to this problem,
namely, predicting the performance of a network before training, based on its
architecture. Specifically, we develop a unified way to encode individual
layers into vectors and bring them together to form an integrated description
via LSTM. Taking advantage of the recurrent network's strong expressive power,
this method can reliably predict the performances of various network
architectures. Our empirical studies showed that it not only achieved accurate
predictions but also produced consistent rankings across datasets -- a key
desideratum in performance prediction.","['Boyang Deng', 'Junjie Yan', 'Dahua Lin']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2017-12-09 07:50:27+00:00
http://arxiv.org/abs/1712.03337v1,Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise,"Matrix decomposition is a popular and fundamental approach in machine
learning and data mining. It has been successfully applied into various fields.
Most matrix decomposition methods focus on decomposing a data matrix from one
single source. However, it is common that data are from different sources with
heterogeneous noise. A few of matrix decomposition methods have been extended
for such multi-view data integration and pattern discovery. While only few
methods were designed to consider the heterogeneity of noise in such multi-view
data for data integration explicitly. To this end, we propose a joint matrix
decomposition framework (BJMD), which models the heterogeneity of noise by
Gaussian distribution in a Bayesian framework. We develop two algorithms to
solve this model: one is a variational Bayesian inference algorithm, which
makes full use of the posterior distribution; and another is a maximum a
posterior algorithm, which is more scalable and can be easily paralleled.
Extensive experiments on synthetic and real-world datasets demonstrate that
BJMD considering the heterogeneity of noise is superior or competitive to the
state-of-the-art methods.","['Chihao Zhang', 'Shihua Zhang']","['cs.CV', 'cs.LG', 'stat.ML']",2017-12-09 02:54:17+00:00
http://arxiv.org/abs/1712.03298v1,Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,"Progress in deep learning is slowed by the days or weeks it takes to train
large models. The natural solution of using more hardware is limited by
diminishing returns, and leads to inefficient use of additional resources. In
this paper, we present a large batch, stochastic optimization algorithm that is
both faster than widely used algorithms for fixed amounts of computation, and
also scales up substantially better as more computational resources become
available. Our algorithm implicitly computes the inverse Hessian of each
mini-batch to produce descent directions; we do so without either an explicit
approximation to the Hessian or Hessian-vector products. We demonstrate the
effectiveness of our algorithm by successfully training large ImageNet models
(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch
sizes of up to 32000 with no loss in validation error relative to current
baselines, and no increase in the total number of steps. At smaller mini-batch
sizes, our optimizer improves the validation error in these models by 0.8-0.9%.
Alternatively, we can trade off this accuracy to reduce the number of training
steps needed by roughly 10-30%. Our work is practical and easily usable by
others -- only one hyperparameter (learning rate) needs tuning, and
furthermore, the algorithm is as computationally cheap as the commonly used
Adam optimizer.","['Shankar Krishnan', 'Ying Xiao', 'Rif A. Saurous']","['cs.LG', 'stat.ML']",2017-12-08 22:26:58+00:00
http://arxiv.org/abs/1712.03281v1,Fast Low-Rank Matrix Estimation without the Condition Number,"In this paper, we study the general problem of optimizing a convex function
$F(L)$ over the set of $p \times p$ matrices, subject to rank constraints on
$L$. However, existing first-order methods for solving such problems either are
too slow to converge, or require multiple invocations of singular value
decompositions. On the other hand, factorization-based non-convex algorithms,
while being much faster, require stringent assumptions on the \emph{condition
number} of the optimum. In this paper, we provide a novel algorithmic framework
that achieves the best of both worlds: asymptotically as fast as factorization
methods, while requiring no dependency on the condition number.
  We instantiate our general framework for three important matrix estimation
problems that impact several practical applications; (i) a \emph{nonlinear}
variant of affine rank minimization, (ii) logistic PCA, and (iii) precision
matrix estimation in probabilistic graphical model learning. We then derive
explicit bounds on the sample complexity as well as the running time of our
approach, and show that it achieves the best possible bounds for both cases. We
also provide an extensive range of experimental results, and demonstrate that
our algorithm provides a very attractive tradeoff between estimation accuracy
and running time.","['Mohammadreza Soltani', 'Chinmay Hegde']",['stat.ML'],2017-12-08 21:10:42+00:00
http://arxiv.org/abs/1712.03134v2,On Adaptive Estimation for Dynamic Bernoulli Bandits,"The multi-armed bandit (MAB) problem is a classic example of the
exploration-exploitation dilemma. It is concerned with maximising the total
rewards for a gambler by sequentially pulling an arm from a multi-armed slot
machine where each arm is associated with a reward distribution. In static
MABs, the reward distributions do not change over time, while in dynamic MABs,
each arm's reward distribution can change, and the optimal arm can switch over
time. Motivated by many real applications where rewards are binary, we focus on
dynamic Bernoulli bandits. Standard methods like $\epsilon$-Greedy and Upper
Confidence Bound (UCB), which rely on the sample mean estimator, often fail to
track changes in the underlying reward for dynamic problems. In this paper, we
overcome the shortcoming of slow response to change by deploying adaptive
estimation in the standard methods and propose a new family of algorithms,
which are adaptive versions of $\epsilon$-Greedy, UCB, and Thompson sampling.
These new methods are simple and easy to implement. Moreover, they do not
require any prior knowledge about the dynamic reward process, which is
important for real applications. We examine the new algorithms numerically in
different scenarios and the results show solid improvements of our algorithms
in dynamic environments.","['Xue Lu', 'Niall Adams', 'Nikolas Kantas']","['stat.ML', 'cs.LG']",2017-12-08 15:45:28+00:00
http://arxiv.org/abs/1712.03133v1,Building competitive direct acoustics-to-word models for English conversational speech recognition,"Direct acoustics-to-word (A2W) models in the end-to-end paradigm have
received increasing attention compared to conventional sub-word based automatic
speech recognition models using phones, characters, or context-dependent hidden
Markov model states. This is because A2W models recognize words from speech
without any decoder, pronunciation lexicon, or externally-trained language
model, making training and decoding with such models simple. Prior work has
shown that A2W models require orders of magnitude more training data in order
to perform comparably to conventional models. Our work also showed this
accuracy gap when using the English Switchboard-Fisher data set. This paper
describes a recipe to train an A2W model that closes this gap and is at-par
with state-of-the-art sub-word based models. We achieve a word error rate of
8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder
or language model. We find that model initialization, training data order, and
regularization have the most impact on the A2W model performance. Next, we
present a joint word-character A2W model that learns to first spell the word
and then recognize it. This model provides a rich output to the user instead of
simple word hypotheses, making it especially useful in the case of words unseen
or rarely-seen during training.","['Kartik Audhkhasi', 'Brian Kingsbury', 'Bhuvana Ramabhadran', 'George Saon', 'Michael Picheny']","['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']",2017-12-08 15:43:21+00:00
http://arxiv.org/abs/1712.03834v2,Crime prediction through urban metrics and statistical learning,"Understanding the causes of crime is a longstanding issue in researcher's
agenda. While it is a hard task to extract causality from data, several linear
models have been proposed to predict crime through the existing correlations
between crime and urban metrics. However, because of non-Gaussian distributions
and multicollinearity in urban indicators, it is common to find controversial
conclusions about the influence of some urban indicators on crime. Machine
learning ensemble-based algorithms can handle well such problems. Here, we use
a random forest regressor to predict crime and quantify the influence of urban
indicators on homicides. Our approach can have up to 97% of accuracy on crime
prediction, and the importance of urban indicators is ranked and clustered in
groups of equal influence, which are robust under slightly changes in the data
sample analyzed. Our results determine the rank of importance of urban
indicators to predict crime, unveiling that unemployment and illiteracy are the
most important variables for describing homicides in Brazilian cities. We
further believe that our approach helps in producing more robust conclusions
regarding the effects of urban indicators on crime, having potential
applications for guiding public policies for crime control.","['Luiz G A Alves', 'Haroldo V Ribeiro', 'Francisco A Rodrigues']","['physics.soc-ph', 'stat.AP', 'stat.ML']",2017-12-08 13:38:23+00:00
http://arxiv.org/abs/1712.03010v2,Coordinate Descent with Bandit Sampling,"Coordinate descent methods usually minimize a cost function by updating a
random decision variable (corresponding to one coordinate) at a time. Ideally,
we would update the decision variable that yields the largest decrease in the
cost function. However, finding this coordinate would require checking all of
them, which would effectively negate the improvement in computational
tractability that coordinate descent is intended to afford. To address this, we
propose a new adaptive method for selecting a coordinate. First, we find a
lower bound on the amount the cost function decreases when a coordinate is
updated. We then use a multi-armed bandit algorithm to learn which coordinates
result in the largest lower bound by interleaving this learning with
conventional coordinate descent updates except that the coordinate is selected
proportionately to the expected decrease. We show that our approach improves
the convergence of coordinate descent methods both theoretically and
experimentally.","['Farnood Salehi', 'Patrick Thiran', 'L. Elisa Celis']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",2017-12-08 10:23:30+00:00
http://arxiv.org/abs/1712.02950v2,"CycleGAN, a Master of Steganography","CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a
transformation between two image distributions. In a series of experiments, we
demonstrate an intriguing property of the model: CycleGAN learns to ""hide""
information about a source image into the images it generates in a nearly
imperceptible, high-frequency signal. This trick ensures that the generator can
recover the original sample and thus satisfy the cyclic consistency
requirement, while the generated image remains realistic. We connect this
phenomenon with adversarial attacks by viewing CycleGAN's training procedure as
training a generator of adversarial examples and demonstrate that the cyclic
consistency loss causes CycleGAN to be especially vulnerable to adversarial
attacks.","['Casey Chu', 'Andrey Zhmoginov', 'Mark Sandler']","['cs.CV', 'cs.LG', 'stat.ML']",2017-12-08 06:07:52+00:00
http://arxiv.org/abs/1712.03779v1,Artificial Intelligence and Statistics,"Artificial intelligence (AI) is intrinsically data-driven. It calls for the
application of statistical concepts through human-machine collaboration during
generation of data, development of algorithms, and evaluation of results. This
paper discusses how such human-machine collaboration can be approached through
the statistical concepts of population, question of interest,
representativeness of training data, and scrutiny of results (PQRS). The PQRS
workflow provides a conceptual framework for integrating statistical ideas with
human input into AI products and research. These ideas include experimental
design principles of randomization and local control as well as the principle
of stability to gain reproducibility and interpretability of algorithms and
data results. We discuss the use of these principles in the contexts of
self-driving cars, automated medical diagnoses, and examples from the authors'
collaborative research.","['Bin Yu', 'Karl Kumbier']","['stat.ML', 'cs.AI']",2017-12-08 02:18:43+00:00
http://arxiv.org/abs/1712.02903v2,Blind Multiclass Ensemble Classification,"The rising interest in pattern recognition and data analytics has spurred the
development of innovative machine learning algorithms and tools. However, as
each algorithm has its strengths and limitations, one is motivated to
judiciously fuse multiple algorithms in order to find the ""best"" performing
one, for a given dataset. Ensemble learning aims at such high-performance
meta-algorithm, by combining the outputs from multiple algorithms. The present
work introduces a blind scheme for learning from ensembles of classifiers,
using a moment matching method that leverages joint tensor and matrix
factorization. Blind refers to the combiner who has no knowledge of the
ground-truth labels that each classifier has been trained on. A rigorous
performance analysis is derived and the proposed scheme is evaluated on
synthetic and real datasets.","['Panagiotis A. Traganitis', 'Alba Pagès-Zamora', 'Georgios B. Giannakis']","['stat.ML', 'cs.LG', 'eess.SP']",2017-12-08 01:01:37+00:00
http://arxiv.org/abs/1712.02902v1,Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start,"Bayesian optimization (BO) is a model-based approach for gradient-free
black-box function optimization. Typically, BO is powered by a Gaussian process
(GP), whose algorithmic complexity is cubic in the number of evaluations.
Hence, GP-based BO cannot leverage large amounts of past or related function
evaluations, for example, to warm start the BO procedure. We develop a multiple
adaptive Bayesian linear regression model as a scalable alternative whose
complexity is linear in the number of observations. The multiple Bayesian
linear regression models are coupled through a shared feedforward neural
network, which learns a joint representation and transfers knowledge across
machine learning problems.","['Valerio Perrone', 'Rodolphe Jenatton', 'Matthias Seeger', 'Cedric Archambeau']",['stat.ML'],2017-12-08 01:00:19+00:00
http://arxiv.org/abs/1712.04332v1,Scaling Limit: Exact and Tractable Analysis of Online Learning Algorithms with Applications to Regularized Regression and PCA,"We present a framework for analyzing the exact dynamics of a class of online
learning algorithms in the high-dimensional scaling limit. Our results are
applied to two concrete examples: online regularized linear regression and
principal component analysis. As the ambient dimension tends to infinity, and
with proper time scaling, we show that the time-varying joint empirical
measures of the target feature vector and its estimates provided by the
algorithms will converge weakly to a deterministic measured-valued process that
can be characterized as the unique solution of a nonlinear PDE. Numerical
solutions of this PDE can be efficiently obtained. These solutions lead to
precise predictions of the performance of the algorithms, as many practical
performance metrics are linear functionals of the joint empirical measures. In
addition to characterizing the dynamic performance of online learning
algorithms, our asymptotic analysis also provides useful insights. In
particular, in the high-dimensional limit, and due to exchangeability, the
original coupled dynamics associated with the algorithms will be asymptotically
""decoupled"", with each coordinate independently solving a 1-D effective
minimization problem via stochastic gradient descent. Exploiting this insight
for nonconvex optimization problems may prove an interesting line of future
research.","['Chuang Wang', 'Jonathan Mattingly', 'Yue M. Lu']","['cs.LG', 'cs.IT', 'math.IT', 'math.PR', 'stat.ML']",2017-12-08 00:20:18+00:00
http://arxiv.org/abs/1712.02854v1,Stochastic reconstruction of an oolitic limestone by generative adversarial networks,"Stochastic image reconstruction is a key part of modern digital rock physics
and materials analysis that aims to create numerous representative samples of
material micro-structures for upscaling, numerical computation of effective
properties and uncertainty quantification. We present a method of
three-dimensional stochastic image reconstruction based on generative
adversarial neural networks (GANs). GANs represent a framework of unsupervised
learning methods that require no a priori inference of the probability
distribution associated with the training data. Using a fully convolutional
neural network allows fast sampling of large volumetric images.We apply a GAN
based workflow of network training and image generation to an oolitic Ketton
limestone micro-CT dataset. Minkowski functionals, effective permeability as
well as velocity distributions of simulated flow within the acquired images are
compared with the synthetic reconstructions generated by the deep neural
network. While our results show that GANs allow a fast and accurate
reconstruction of the evaluated image dataset, we address a number of open
questions and challenges involved in the evaluation of generative network-based
methods.","['Lukas Mosser', 'Olivier Dubrule', 'Martin J. Blunt']","['cs.CV', 'physics.geo-ph', 'stat.ML']",2017-12-07 20:21:01+00:00
http://arxiv.org/abs/1712.02831v1,RelNN: A Deep Neural Model for Relational Learning,"Statistical relational AI (StarAI) aims at reasoning and learning in noisy
domains described in terms of objects and relationships by combining
probability with first-order logic. With huge advances in deep learning in the
current years, combining deep networks with first-order logic has been the
focus of several recent studies. Many of the existing attempts, however, only
focus on relations and ignore object properties. The attempts that do consider
object properties are limited in terms of modelling power or scalability. In
this paper, we develop relational neural networks (RelNNs) by adding hidden
layers to relational logistic regression (the relational counterpart of
logistic regression). We learn latent properties for objects both directly and
through general rules. Back-propagation is used for training these models. A
modular, layer-wise architecture facilitates utilizing the techniques developed
within deep learning community to our architecture. Initial experiments on
eight tasks over three real-world datasets show that RelNNs are promising
models for relational learning.","['Seyed Mehran Kazemi', 'David Poole']","['stat.ML', 'cs.LG']",2017-12-07 19:40:01+00:00
http://arxiv.org/abs/1712.02779v4,Exploring the Landscape of Spatial Robustness,"The study of adversarial robustness has so far largely focused on
perturbations bound in p-norms. However, state-of-the-art models turn out to be
also vulnerable to other, more natural classes of perturbations such as
translations and rotations. In this work, we thoroughly investigate the
vulnerability of neural network--based classifiers to rotations and
translations. While data augmentation offers relatively small robustness, we
use ideas from robust optimization and test-time input aggregation to
significantly improve robustness. Finally we find that, in contrast to the
p-norm case, first-order methods cannot reliably find worst-case perturbations.
This highlights spatial robustness as a fundamentally different setting
requiring additional study. Code available at
https://github.com/MadryLab/adversarial_spatial and
https://github.com/MadryLab/spatial-pytorch.","['Logan Engstrom', 'Brandon Tran', 'Dimitris Tsipras', 'Ludwig Schmidt', 'Aleksander Madry']","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",2017-12-07 18:53:52+00:00
http://arxiv.org/abs/1712.02743v1,End-to-end Learning of Deterministic Decision Trees,"Conventional decision trees have a number of favorable properties, including
interpretability, a small computational footprint and the ability to learn from
little training data. However, they lack a key quality that has helped fuel the
deep learning revolution: that of being end-to-end trainable, and to learn from
scratch those features that best allow to solve a given supervised learning
problem. Recent work (Kontschieder 2015) has addressed this deficit, but at the
cost of losing a main attractive trait of decision trees: the fact that each
sample is routed along a small subset of tree nodes only. We here propose a
model and Expectation-Maximization training scheme for decision trees that are
fully probabilistic at train time, but after a deterministic annealing process
become deterministic at test time. We also analyze the learned oblique split
parameters on image datasets and show that Neural Networks can be trained at
each split node. In summary, we present the first end-to-end learning scheme
for deterministic decision trees and present results on par with or superior to
published standard oblique decision tree algorithms.","['Thomas Hehn', 'Fred A. Hamprecht']","['stat.ML', 'cs.CV', 'cs.LG']",2017-12-07 17:40:25+00:00
http://arxiv.org/abs/1712.02734v2,Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for Transferable Chemical Property Prediction,"With access to large datasets, deep neural networks (DNN) have achieved
human-level accuracy in image and speech recognition tasks. However, in
chemistry, data is inherently small and fragmented. In this work, we develop an
approach of using rule-based knowledge for training ChemNet, a transferable and
generalizable deep neural network for chemical property prediction that learns
in a weak-supervised manner from large unlabeled chemical databases. When
coupled with transfer learning approaches to predict other smaller datasets for
chemical properties that it was not originally trained on, we show that
ChemNet's accuracy outperforms contemporary DNN models that were trained using
conventional supervised learning. Furthermore, we demonstrate that the ChemNet
pre-training approach is equally effective on both CNN (Chemception) and RNN
(SMILES2vec) models, indicating that this approach is network architecture
agnostic and is effective across multiple data modalities. Our results indicate
a pre-trained ChemNet that incorporates chemistry domain knowledge, enables the
development of generalizable neural networks for more accurate prediction of
novel chemical properties.","['Garrett B. Goh', 'Charles Siegel', 'Abhinav Vishnu', 'Nathan O. Hodas']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']",2017-12-07 17:25:48+00:00
http://arxiv.org/abs/1712.02679v1,AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training,"Highly distributed training of Deep Neural Networks (DNNs) on future compute
platforms (offering 100 of TeraOps/s of computational capacity) is expected to
be severely communication constrained. To overcome this limitation, new
gradient compression techniques are needed that are computationally friendly,
applicable to a wide variety of layers seen in Deep Neural Networks and
adaptable to variations in network architectures as well as their
hyper-parameters. In this paper we introduce a novel technique - the Adaptive
Residual Gradient Compression (AdaComp) scheme. AdaComp is based on localized
selection of gradient residues and automatically tunes the compression rate
depending on local activity. We show excellent results on a wide spectrum of
state of the art Deep Learning models in multiple domains (vision, speech,
language), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers
(SGD with momentum, Adam) and network parameters (number of learners,
minibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate
end-to-end compression rates of ~200X for fully-connected and recurrent layers,
and ~40X for convolutional layers, without any noticeable degradation in model
accuracies.","['Chia-Yu Chen', 'Jungwook Choi', 'Daniel Brand', 'Ankur Agrawal', 'Wei Zhang', 'Kailash Gopalakrishnan']","['cs.LG', 'stat.ML']",2017-12-07 15:44:19+00:00
http://arxiv.org/abs/1712.02675v2,How consistent is my model with the data? Information-Theoretic Model Check,"The choice of model class is fundamental in statistical learning and system
identification, no matter whether the class is derived from physical principles
or is a generic black-box. We develop a method to evaluate the specified model
class by assessing its capability of reproducing data that is similar to the
observed data record. This model check is based on the information-theoretic
properties of models viewed as data generators and is applicable to e.g.
sequential data and nonlinear dynamical models. The method can be understood as
a specific two-sided posterior predictive test. We apply the
information-theoretic model check to both synthetic and real data and compare
it with a classical whiteness test.","['Andreas Svensson', 'Dave Zachariah', 'Thomas B. Schön']","['stat.ML', 'cs.LG', 'eess.SP', 'stat.ME']",2017-12-07 15:40:17+00:00
http://arxiv.org/abs/1712.02658v1,Using SVDD in SimpleMKL for 3D-Shapes Filtering,"This paper proposes the adaptation of Support Vector Data Description (SVDD)
to the multiple kernel case (MK-SVDD), based on SimpleMKL. It also introduces a
variant called Slim-MK-SVDD that is able to produce a tighter frontier around
the data. For the sake of comparison, the equivalent methods are also developed
for One-Class SVM, known to be very similar to SVDD for certain shapes of
kernels.
  Those algorithms are illustrated in the context of 3D-shapes filtering and
outliers detection. For the 3D-shapes problem, the objective is to be able to
select a sub-category of 3D-shapes, each sub-category being learned with our
algorithm in order to create a filter. For outliers detection, we apply the
proposed algorithms for unsupervised outliers detection as well as for the
supervised case.","['Gaëlle Loosli', 'Hattoibe Aboubacar']","['stat.ML', 'cs.CV']",2017-12-07 14:56:53+00:00
