id,title,abstract,authors,categories,date
http://arxiv.org/abs/1511.08768v2,Gradient Estimation with Simultaneous Perturbation and Compressive Sensing,"This paper aims at achieving a ""good"" estimator for the gradient of a
function on a high-dimensional space. Often such functions are not sensitive in
all coordinates and the gradient of the function is almost sparse. We propose a
method for gradient estimation that combines ideas from Spall's Simultaneous
Perturbation Stochastic Approximation with compressive sensing. The aim is to
obtain ""good"" estimator without too many function evaluations. Application to
estimating gradient outer product matrix as well as standard optimization
problems are illustrated via simulations.","['Vivek S. Borkar', 'Vikranth R. Dwaracherla', 'Neeraja Sahasrabudhe']",['stat.ML'],2015-11-27 18:51:29+00:00
http://arxiv.org/abs/1511.08681v1,Algorithms for Differentially Private Multi-Armed Bandits,"We present differentially private algorithms for the stochastic Multi-Armed
Bandit (MAB) problem. This is a problem for applications such as adaptive
clinical trials, experiment design, and user-targeted advertising where private
information is connected to individual rewards. Our major contribution is to
show that there exist $(\epsilon, \delta)$ differentially private variants of
Upper Confidence Bound algorithms which have optimal regret, $O(\epsilon^{-1} +
\log T)$. This is a significant improvement over previous results, which only
achieve poly-log regret $O(\epsilon^{-2} \log^{2} T)$, because of our use of a
novel interval-based mechanism. We also substantially improve the bounds of
previous family of algorithms which use a continual release mechanism.
Experiments clearly validate our theoretical bounds.","['Aristide Tossou', 'Christos Dimitrakakis']","['stat.ML', 'cs.CR', 'cs.LG']",2015-11-27 14:16:00+00:00
http://arxiv.org/abs/1511.08551v2,Regularized EM Algorithms: A Unified Framework and Statistical Guarantees,"Latent variable models are a fundamental modeling tool in machine learning
applications, but they present significant computational and analytical
challenges. The popular EM algorithm and its variants, is a much used
algorithmic tool; yet our rigorous understanding of its performance is highly
incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that
for an important class of problems, EM exhibits linear local convergence. In
the high-dimensional setting, however, the M-step may not be well defined. We
address precisely this setting through a unified treatment using
regularization. While regularization for high-dimensional problems is by now
well understood, the iterative EM algorithm requires a careful balancing of
making progress towards the solution while identifying the right structure
(e.g., sparsity or low-rank). In particular, regularizing the M-step using the
state-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) is
not guaranteed to provide this balance. Our algorithm and analysis are linked
in a way that reveals the balance between optimization and statistical errors.
We specialize our general framework to sparse gaussian mixture models,
high-dimensional mixed regression, and regression with missing variables,
obtaining statistical guarantees for each of these examples.","['Xinyang Yi', 'Constantine Caramanis']","['cs.LG', 'stat.ML']",2015-11-27 03:46:36+00:00
http://arxiv.org/abs/1511.08405v1,Gains and Losses are Fundamentally Different in Regret Minimization: The Sparse Case,"We demonstrate that, in the classical non-stochastic regret minimization
problem with $d$ decisions, gains and losses to be respectively maximized or
minimized are fundamentally different. Indeed, by considering the additional
sparsity assumption (at each stage, at most $s$ decisions incur a nonzero
outcome), we derive optimal regret bounds of different orders. Specifically,
with gains, we obtain an optimal regret guarantee after $T$ stages of order
$\sqrt{T\log s}$, so the classical dependency in the dimension is replaced by
the sparsity size. With losses, we provide matching upper and lower bounds of
order $\sqrt{Ts\log(d)/d}$, which is decreasing in $d$. Eventually, we also
study the bandit setting, and obtain an upper bound of order $\sqrt{Ts\log
(d/s)}$ when outcomes are losses. This bound is proven to be optimal up to the
logarithmic factor $\sqrt{\log(d/s)}$.","['Joon Kwon', 'Vianney Perchet']","['cs.LG', 'stat.ML']",2015-11-26 14:53:00+00:00
http://arxiv.org/abs/1511.08400v7,Regularizing RNNs by Stabilizing Activations,"We stabilize the activations of Recurrent Neural Networks (RNNs) by
penalizing the squared distance between successive hidden states' norms.
  This penalty term is an effective regularizer for RNNs including LSTMs and
IRNNs, improving performance on character-level language modeling and phoneme
recognition, and outperforming weight noise and dropout.
  We achieve competitive performance (18.6\% PER) on the TIMIT phoneme
recognition task for RNNs evaluated without beam search or an RNN transducer.
  With this penalty term, IRNN can achieve similar performance to LSTM on
language modeling, although adding the penalty term to the LSTM results in
superior performance.
  Our penalty term also prevents the exponential growth of IRNN's activations
outside of their training horizon, allowing them to generalize to much longer
sequences.","['David Krueger', 'Roland Memisevic']","['cs.NE', 'cs.CL', 'cs.LG', 'stat.ML']",2015-11-26 14:35:27+00:00
http://arxiv.org/abs/1511.08343v2,The Automatic Statistician: A Relational Perspective,"Gaussian Processes (GPs) provide a general and analytically tractable way of
modeling complex time-varying, nonparametric functions. The Automatic Bayesian
Covariance Discovery (ABCD) system constructs natural-language description of
time-series data by treating unknown time-series data nonparametrically using
GP with a composite covariance kernel function. Unfortunately, learning a
composite covariance kernel with a single time-series data set often results in
less informative kernel that may not give qualitative, distinctive descriptions
of data. We address this challenge by proposing two relational kernel learning
methods which can model multiple time-series data sets by finding common,
shared causes of changes. We show that the relational kernel learning methods
find more accurate models for regression problems on several real-world data
sets; US stock data, US house price index data and currency exchange rate data.","['Yunseong Hwang', 'Anh Tong', 'Jaesik Choi']","['cs.LG', 'stat.ML']",2015-11-26 10:26:51+00:00
http://arxiv.org/abs/1511.08327v2,Random Forests for Big Data,"Big Data is one of the major challenges of statistical science and has
numerous consequences from algorithmic and theoretical viewpoints. Big Data
always involve massive data but they also often include online data and data
heterogeneity. Recently some statistical methods have been adapted to process
Big Data, like linear regression models, clustering methods and bootstrapping
schemes. Based on decision trees combined with aggregation and bootstrap ideas,
random forests were introduced by Breiman in 2001. They are a powerful
nonparametric statistical method allowing to consider in a single and versatile
framework regression problems, as well as two-class and multi-class
classification problems. Focusing on classification problems, this paper
proposes a selective review of available proposals that deal with scaling
random forests to Big Data problems. These proposals rely on parallel
environments or on online adaptations of random forests. We also describe how
related quantities -- such as out-of-bag error and variable importance -- are
addressed in these methods. Then, we formulate various remarks for random
forests in the Big Data context. Finally, we experiment five variants on two
massive datasets (15 and 120 millions of observations), a simulated one as well
as real world data. One variant relies on subsampling while three others are
related to parallel implementations of random forests and involve either
various adaptations of bootstrap to Big Data or to ""divide-and-conquer""
approaches. The fifth variant relates on online learning of random forests.
These numerical experiments lead to highlight the relative performance of the
different variants, as well as some of their limitations.","['Robin Genuer', 'Jean-Michel Poggi', 'Christine Tuleau-Malot', 'Nathalie Villa-Vialaneix']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2015-11-26 09:04:47+00:00
http://arxiv.org/abs/1511.08136v5,Unifying Decision Trees Split Criteria Using Tsallis Entropy,"The construction of efficient and effective decision trees remains a key
topic in machine learning because of their simplicity and flexibility. A lot of
heuristic algorithms have been proposed to construct near-optimal decision
trees. ID3, C4.5 and CART are classical decision tree algorithms and the split
criteria they used are Shannon entropy, Gain Ratio and Gini index respectively.
All the split criteria seem to be independent, actually, they can be unified in
a Tsallis entropy framework. Tsallis entropy is a generalization of Shannon
entropy and provides a new approach to enhance decision trees' performance with
an adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC)
algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index,
which generalizes the split criteria of decision trees. More importantly, we
reveal the relations between Tsallis entropy with different $q$ and other split
criteria. Experimental results on UCI data sets indicate that the TEC algorithm
achieves statistically significant improvement over the classical algorithms.","['Yisen Wang', 'Chaobing Song', 'Shu-Tao Xia']","['stat.ML', 'cs.AI', 'cs.LG']",2015-11-25 17:49:55+00:00
http://arxiv.org/abs/1511.08102v3,L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs,"It is known that for a certain class of single index models (SIMs) $Y =
f(\boldsymbol{X}_{p \times 1}^\intercal\boldsymbol{\beta}_0, \varepsilon)$,
support recovery is impossible when $\boldsymbol{X} \sim \mathcal{N}(0,
\mathbb{I}_{p \times p})$ and a model complexity adjusted sample size is below
a critical threshold. Recently, optimal algorithms based on Sliced Inverse
Regression (SIR) were suggested. These algorithms work provably under the
assumption that the design $\boldsymbol{X}$ comes from an i.i.d. Gaussian
distribution. In the present paper we analyze algorithms based on covariance
screening and least squares with $L_1$ penalization (i.e. LASSO) and
demonstrate that they can also enjoy optimal (up to a scalar) rescaled sample
size in terms of support recovery, albeit under slightly different assumptions
on $f$ and $\varepsilon$ compared to the SIR based algorithms. Furthermore, we
show more generally, that LASSO succeeds in recovering the signed support of
$\boldsymbol{\beta}_0$ if $\boldsymbol{X} \sim \mathcal{N}(0,
\boldsymbol{\Sigma})$, and the covariance $\boldsymbol{\Sigma}$ satisfies the
irrepresentable condition. Our work extends existing results on the support
recovery of LASSO for the linear model, to a more general class of SIMs.","['Matey Neykov', 'Jun S. Liu', 'Tianxi Cai']","['math.ST', 'stat.ML', 'stat.TH']",2015-11-25 16:00:44+00:00
http://arxiv.org/abs/1511.09123v1,A Short Survey on Data Clustering Algorithms,"With rapidly increasing data, clustering algorithms are important tools for
data analytics in modern research. They have been successfully applied to a
wide range of domains; for instance, bioinformatics, speech recognition, and
financial analysis. Formally speaking, given a set of data instances, a
clustering algorithm is expected to divide the set of data instances into the
subsets which maximize the intra-subset similarity and inter-subset
dissimilarity, where a similarity measure is defined beforehand. In this work,
the state-of-the-arts clustering algorithms are reviewed from design concept to
methodology; Different clustering paradigms are discussed. Advanced clustering
algorithms are also discussed. After that, the existing clustering evaluation
metrics are reviewed. A summary with future insights is provided at the end.",['Ka-Chun Wong'],"['cs.DS', 'cs.CV', 'cs.LG', 'stat.CO', 'stat.ML']",2015-11-25 08:02:37+00:00
http://arxiv.org/abs/1511.07944v1,Maximum Likelihood Estimation for Single Linkage Hierarchical Clustering,"We derive a statistical model for estimation of a dendrogram from single
linkage hierarchical clustering (SLHC) that takes account of uncertainty
through noise or corruption in the measurements of separation of data. Our
focus is on just the estimation of the hierarchy of partitions afforded by the
dendrogram, rather than the heights in the latter. The concept of estimating
this ""dendrogram structure'' is introduced, and an approximate maximum
likelihood estimator (MLE) for the dendrogram structure is described. These
ideas are illustrated by a simple Monte Carlo simulation that, at least for
small data sets, suggests the method outperforms SLHC in the presence of noise.","['Dekang Zhu', 'Dan P. Guralnik', 'Xuezhi Wang', 'Xiang Li', 'Bill Moran']",['stat.ML'],2015-11-25 03:35:46+00:00
http://arxiv.org/abs/1511.07916v1,Natural Language Understanding with Distributed Representation,"This is a lecture note for the course DS-GA 3001 <Natural Language
Understanding with Distributed Representation> at the Center for Data Science ,
New York University in Fall, 2015. As the name of the course suggests, this
lecture note introduces readers to a neural network based approach to natural
language understanding/processing. In order to make it as self-contained as
possible, I spend much time on describing basics of machine learning and neural
networks, only after which how they are used for natural languages is
introduced. On the language front, I almost solely focus on language modelling
and machine translation, two of which I personally find most fascinating and
most fundamental to natural language understanding.",['Kyunghyun Cho'],"['cs.CL', 'stat.ML']",2015-11-24 23:23:13+00:00
http://arxiv.org/abs/1511.07902v4,"Performance Limits of Stochastic Sub-Gradient Learning, Part I: Single Agent Case","In this work and the supporting Part II, we examine the performance of
stochastic sub-gradient learning strategies under weaker conditions than
usually considered in the literature. The new conditions are shown to be
automatically satisfied by several important cases of interest including SVM,
LASSO, and Total-Variation denoising formulations. In comparison, these
problems do not satisfy the traditional assumptions used in prior analyses and,
therefore, conclusions derived from these earlier treatments are not directly
applicable to these problems. The results in this article establish that
stochastic sub-gradient strategies can attain linear convergence rates, as
opposed to sub-linear rates, to the steady-state regime. A realizable
exponential-weighting procedure is employed to smooth the intermediate iterates
and guarantee useful performance bounds in terms of convergence rate and
excessive risk performance. Part I of this work focuses on single-agent
scenarios, which are common in stand-alone learning applications, while Part II
extends the analysis to networked learners. The theoretical conclusions are
illustrated by several examples and simulations, including comparisons with the
FISTA procedure.","['Bicheng Ying', 'Ali H. Sayed']","['stat.ML', 'cs.LG', 'cs.MA']",2015-11-24 22:31:19+00:00
http://arxiv.org/abs/1511.07896v1,Private Posterior distributions from Variational approximations,"Privacy preserving mechanisms such as differential privacy inject additional
randomness in the form of noise in the data, beyond the sampling mechanism.
Ignoring this additional noise can lead to inaccurate and invalid inferences.
In this paper, we incorporate the privacy mechanism explicitly into the
likelihood function by treating the original data as missing, with an end goal
of estimating posterior distributions over model parameters. This leads to a
principled way of performing valid statistical inference using private data,
however, the corresponding likelihoods are intractable. In this paper, we
derive fast and accurate variational approximations to tackle such intractable
likelihoods that arise due to privacy. We focus on estimating posterior
distributions of parameters of the naive Bayes log-linear model, where the
sufficient statistics of this model are shared using a differentially private
interface. Using a simulation study, we show that the posterior approximations
outperform the naive method of ignoring the noise addition mechanism.","['Vishesh Karwa', 'Dan Kifer', 'Aleksandra B. Slavković']","['stat.ML', 'cs.CR', 'cs.LG']",2015-11-24 21:49:02+00:00
http://arxiv.org/abs/1511.07837v3,Generalized Conjugate Gradient Methods for $\ell_1$ Regularized Convex Quadratic Programming with Finite Convergence,"The conjugate gradient (CG) method is an efficient iterative method for
solving large-scale strongly convex quadratic programming (QP). In this paper
we propose some generalized CG (GCG) methods for solving the
$\ell_1$-regularized (possibly not strongly) convex QP that terminate at an
optimal solution in a finite number of iterations. At each iteration, our
methods first identify a face of an orthant and then either perform an exact
line search along the direction of the negative projected minimum-norm
subgradient of the objective function or execute a CG subroutine that conducts
a sequence of CG iterations until a CG iterate crosses the boundary of this
face or an approximate minimizer of over this face or a subface is found. We
determine which type of step should be taken by comparing the magnitude of some
components of the minimum-norm subgradient of the objective function to that of
its rest components. Our analysis on finite convergence of these methods makes
use of an error bound result and some key properties of the aforementioned
exact line search and the CG subroutine. We also show that the proposed methods
are capable of finding an approximate solution of the problem by allowing some
inexactness on the execution of the CG subroutine. The overall arithmetic
operation cost of our GCG methods for finding an $\epsilon$-optimal solution
depends on $\epsilon$ in $O(\log(1/\epsilon))$, which is superior to the
accelerated proximal gradient method [2,23] that depends on $\epsilon$ in
$O(1/\sqrt{\epsilon})$. In addition, our GCG methods can be extended
straightforwardly to solve box-constrained convex QP with finite convergence.
Numerical results demonstrate that our methods are very favorable for solving
ill-conditioned problems.","['Zhaosong Lu', 'Xiaojun Chen']","['math.OC', 'cs.LG', 'math.NA', 'stat.CO', 'stat.ML', '65C60, 65K05, 65Y20, 90C06, 90C20, 90C25']",2015-11-24 19:28:09+00:00
http://arxiv.org/abs/1511.07827v2,Stopping criteria for boosting automatic experimental design using real-time fMRI with Bayesian optimization,"Bayesian optimization has been proposed as a practical and efficient tool
through which to tune parameters in many difficult settings. Recently, such
techniques have been combined with real-time fMRI to propose a novel framework
which turns on its head the conventional functional neuroimaging approach. This
closed-loop method automatically designs the optimal experiment to evoke a
desired target brain pattern. One of the challenges associated with extending
such methods to real-time brain imaging is the need for adequate stopping
criteria, an aspect of Bayesian optimization which has received limited
attention. In light of high scanning costs and limited attentional capacities
of subjects an accurate and reliable stopping criteria is essential. In order
to address this issue we propose and empirically study the performance of two
stopping criteria.","['Romy Lorenz', 'Ricardo P Monti', 'Ines R Violante', 'Aldo A Faisal', 'Christoforos Anagnostopoulos', 'Robert Leech', 'Giovanni Montana']","['q-bio.NC', 'stat.ML']",2015-11-24 18:28:54+00:00
http://arxiv.org/abs/1511.07715v2,Statistical Properties of the Single Linkage Hierarchical Clustering Estimator,"Distance-based hierarchical clustering (HC) methods are widely used in
unsupervised data analysis but few authors take account of uncertainty in the
distance data. We incorporate a statistical model of the uncertainty through
corruption or noise in the pairwise distances and investigate the problem of
estimating the HC as unknown parameters from measurements. Specifically, we
focus on single linkage hierarchical clustering (SLHC) and study its geometry.
We prove that under fairly reasonable conditions on the probability
distribution governing measurements, SLHC is equivalent to maximum partial
profile likelihood estimation (MPPLE) with some of the information contained in
the data ignored. At the same time, we show that direct evaluation of SLHC on
maximum likelihood estimation (MLE) of pairwise distances yields a consistent
estimator. Consequently, a full MLE is expected to perform better than SLHC in
getting the correct HC results for the ground truth metric.","['Dekang Zhu', 'Dan P. Guralnik', 'Xuezhi Wang', 'Xiang Li', 'Bill Moran']",['stat.ML'],2015-11-24 14:15:11+00:00
http://arxiv.org/abs/1511.07551v1,Transductive Log Opinion Pool of Gaussian Process Experts,"We introduce a framework for analyzing transductive combination of Gaussian
process (GP) experts, where independently trained GP experts are combined in a
way that depends on test point location, in order to scale GPs to big data. The
framework provides some theoretical justification for the generalized product
of GP experts (gPoE-GP) which was previously shown to work well in practice but
lacks theoretical basis. Based on the proposed framework, an improvement over
gPoE-GP is introduced and empirically validated.","['Yanshuai Cao', 'David J. Fleet']","['cs.LG', 'stat.ML']",2015-11-24 03:08:59+00:00
http://arxiv.org/abs/1511.07528v1,The Limitations of Deep Learning in Adversarial Settings,"Deep learning takes advantage of large datasets and computationally efficient
training algorithms to outperform other approaches at various machine learning
tasks. However, imperfections in the training phase of deep neural networks
make them vulnerable to adversarial samples: inputs crafted by adversaries with
the intent of causing deep neural networks to misclassify. In this work, we
formalize the space of adversaries against deep neural networks (DNNs) and
introduce a novel class of algorithms to craft adversarial samples based on a
precise understanding of the mapping between inputs and outputs of DNNs. In an
application to computer vision, we show that our algorithms can reliably
produce samples correctly classified by human subjects but misclassified in
specific targets by a DNN with a 97% adversarial success rate while only
modifying on average 4.02% of the input features per sample. We then evaluate
the vulnerability of different sample classes to adversarial perturbations by
defining a hardness measure. Finally, we describe preliminary work outlining
defenses against adversarial samples by defining a predictive measure of
distance between a benign input and a target classification.","['Nicolas Papernot', 'Patrick McDaniel', 'Somesh Jha', 'Matt Fredrikson', 'Z. Berkay Celik', 'Ananthram Swami']","['cs.CR', 'cs.LG', 'cs.NE', 'stat.ML']",2015-11-24 01:07:08+00:00
http://arxiv.org/abs/1511.07428v3,Estimating the number of unseen species: A bird in the hand is worth $\log n $ in the bush,"Estimating the number of unseen species is an important problem in many
scientific endeavors. Its most popular formulation, introduced by Fisher, uses
$n$ samples to predict the number $U$ of hitherto unseen species that would be
observed if $t\cdot n$ new samples were collected. Of considerable interest is
the largest ratio $t$ between the number of new and existing samples for which
$U$ can be accurately predicted.
  In seminal works, Good and Toulmin constructed an intriguing estimator that
predicts $U$ for all $t\le 1$, thereby showing that the number of species can
be estimated for a population twice as large as that observed. Subsequently
Efron and Thisted obtained a modified estimator that empirically predicts $U$
even for some $t>1$, but without provable guarantees.
  We derive a class of estimators that $\textit{provably}$ predict $U$ not just
for constant $t>1$, but all the way up to $t$ proportional to $\log n$. This
shows that the number of species can be estimated for a population $\log n$
times larger than that observed, a factor that grows arbitrarily large as $n$
increases. We also show that this range is the best possible and that the
estimators' mean-square error is optimal up to constants for any $t$. Our
approach yields the first provable guarantee for the Efron-Thisted estimator
and, in addition, a variant which achieves stronger theoretical and
experimental performance than existing methodologies on a variety of synthetic
and real datasets.
  The estimators we derive are simple linear estimators that are computable in
time proportional to $n$. The performance guarantees hold uniformly for all
distributions, and apply to all four standard sampling models commonly used
across various scientific disciplines: multinomial, Poisson, hypergeometric,
and Bernoulli product.","['Alon Orlitsky', 'Ananda Theertha Suresh', 'Yihong Wu']","['math.ST', 'stat.ML', 'stat.TH']",2015-11-23 20:58:55+00:00
http://arxiv.org/abs/1511.07367v1,Black box variational inference for state space models,"Latent variable time-series models are among the most heavily used tools from
machine learning and applied statistics. These models have the advantage of
learning latent structure both from noisy observations and from the temporal
ordering in the data, where it is assumed that meaningful correlation structure
exists across time. A few highly-structured models, such as the linear
dynamical system with linear-Gaussian observations, have closed-form inference
procedures (e.g. the Kalman Filter), but this case is an exception to the
general rule that exact posterior inference in more complex generative models
is intractable. Consequently, much work in time-series modeling focuses on
approximate inference procedures for one particular class of models. Here, we
extend recent developments in stochastic variational inference to develop a
`black-box' approximate inference technique for latent variable models with
latent dynamical structure. We propose a structured Gaussian variational
approximate posterior that carries the same intuition as the standard Kalman
filter-smoother but, importantly, permits us to use the same inference approach
to approximate the posterior of much more general, nonlinear latent variable
generative models. We show that our approach recovers accurate estimates in the
case of basic models with closed-form posteriors, and more interestingly
performs well in comparison to variational approaches that were designed in a
bespoke fashion for specific non-conjugate models.","['Evan Archer', 'Il Memming Park', 'Lars Buesing', 'John Cunningham', 'Liam Paninski']",['stat.ML'],2015-11-23 19:08:08+00:00
http://arxiv.org/abs/1511.07334v2,Switched latent force models for reverse-engineering transcriptional regulation in gene expression data,"To survive environmental conditions, cells transcribe their response
activities into encoded mRNA sequences in order to produce certain amounts of
protein concentrations. The external conditions are mapped into the cell
through the activation of special proteins called transcription factors (TFs).
Due to the difficult task to measure experimentally TF behaviours, and the
challenges to capture their quick-time dynamics, different types of models
based on differential equations have been proposed. However, those approaches
usually incur in costly procedures, and they present problems to describe
sudden changes in TF regulators. In this paper, we present a switched dynamical
latent force model for reverse-engineering transcriptional regulation in gene
expression data which allows the exact inference over latent TF activities
driving some observed gene expressions through a linear differential equation.
To deal with discontinuities in the dynamics, we introduce an approach that
switches between different TF activities and different dynamical systems. This
creates a versatile representation of transcription networks that can capture
discrete changes and non-linearities We evaluate our model on both simulated
data and real-data (e.g. microaerobic shift in E. coli, yeast respiration),
concluding that our framework allows for the fitting of the expression data
while being able to infer continuous-time TF profiles.","['Andrés F. López-Lopera', 'Mauricio A. Álvarez']","['physics.bio-ph', 'physics.data-an', 'stat.ML']",2015-11-23 17:38:38+00:00
http://arxiv.org/abs/1511.07294v1,Stochastic Parallel Block Coordinate Descent for Large-scale Saddle Point Problems,"We consider convex-concave saddle point problems with a separable structure
and non-strongly convex functions. We propose an efficient stochastic block
coordinate descent method using adaptive primal-dual updates, which enables
flexible parallel optimization for large-scale problems. Our method shares the
efficiency and flexibility of block coordinate descent methods with the
simplicity of primal-dual methods and utilizing the structure of the separable
convex-concave saddle point problem. It is capable of solving a wide range of
machine learning applications, including robust principal component analysis,
Lasso, and feature selection by group Lasso, etc. Theoretically and
empirically, we demonstrate significantly better performance than
state-of-the-art methods in all these applications.","['Zhanxing Zhu', 'Amos J. Storkey']",['stat.ML'],2015-11-23 16:12:11+00:00
http://arxiv.org/abs/1511.07293v1,"Sparse Recovery via Partial Regularization: Models, Theory and Algorithms","In the context of sparse recovery, it is known that most of existing
regularizers such as $\ell_1$ suffer from some bias incurred by some leading
entries (in magnitude) of the associated vector. To neutralize this bias, we
propose a class of models with partial regularizers for recovering a sparse
solution of a linear system. We show that every local minimizer of these models
is sufficiently sparse or the magnitude of all its nonzero entries is above a
uniform constant depending only on the data of the linear system. Moreover, for
a class of partial regularizers, any global minimizer of these models is a
sparsest solution to the linear system. We also establish some sufficient
conditions for local or global recovery of the sparsest solution to the linear
system, among which one of the conditions is weaker than the best known
restricted isometry property (RIP) condition for sparse recovery by $\ell_1$.
In addition, a first-order feasible augmented Lagrangian (FAL) method is
proposed for solving these models, in which each subproblem is solved by a
nonmonotone proximal gradient (NPG) method. Despite the complication of the
partial regularizers, we show that each proximal subproblem in NPG can be
solved as a certain number of one-dimensional optimization problems, which
usually have a closed-form solution. We also show that any accumulation point
of the sequence generated by FAL is a first-order stationary point of the
models. Numerical results on compressed sensing and sparse logistic regression
demonstrate that the proposed models substantially outperform the widely used
ones in the literature in terms of solution quality.","['Zhaosong Lu', 'Xiaorui Li']","['math.OC', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ME', 'stat.ML']",2015-11-23 16:08:24+00:00
http://arxiv.org/abs/1511.07281v1,Sparse Linear Models applied to Power Quality Disturbance Classification,"Power quality (PQ) analysis describes the non-pure electric signals that are
usually present in electric power systems. The automatic recognition of PQ
disturbances can be seen as a pattern recognition problem, in which different
types of waveform distortion are differentiated based on their features.
Similar to other quasi-stationary signals, PQ disturbances can be decomposed
into time-frequency dependent components by using time-frequency or time-scale
transforms, also known as dictionaries. These dictionaries are used in the
feature extraction step in pattern recognition systems. Short-time Fourier,
Wavelets and Stockwell transforms are some of the most common dictionaries used
in the PQ community, aiming to achieve a better signal representation. To the
best of our knowledge, previous works about PQ disturbance classification have
been restricted to the use of one among several available dictionaries. Taking
advantage of the theory behind sparse linear models (SLM), we introduce a
sparse method for PQ representation, starting from overcomplete dictionaries.
In particular, we apply Group Lasso. We employ different types of
time-frequency (or time-scale) dictionaries to characterize the PQ
disturbances, and evaluate their performance under different pattern
recognition algorithms. We show that the SLM reduce the PQ classification
complexity promoting sparse basis selection, and improving the classification
accuracy.","['Andrés F. López-Lopera', 'Mauricio A. Álvarez', 'Ávaro A. Orozco']","['stat.AP', 'stat.ML']",2015-11-23 15:44:16+00:00
http://arxiv.org/abs/1511.07211v2,Noisy Submodular Maximization via Adaptive Sampling with Applications to Crowdsourced Image Collection Summarization,"We address the problem of maximizing an unknown submodular function that can
only be accessed via noisy evaluations. Our work is motivated by the task of
summarizing content, e.g., image collections, by leveraging users' feedback in
form of clicks or ratings. For summarization tasks with the goal of maximizing
coverage and diversity, submodular set functions are a natural choice. When the
underlying submodular function is unknown, users' feedback can provide noisy
evaluations of the function that we seek to maximize. We provide a generic
algorithm -- \submM{} -- for maximizing an unknown submodular function under
cardinality constraints. This algorithm makes use of a novel exploration module
-- \blbox{} -- that proposes good elements based on adaptively sampling noisy
function evaluations. \blbox{} is able to accommodate different kinds of
observation models such as value queries and pairwise comparisons. We provide
PAC-style guarantees on the quality and sampling cost of the solution obtained
by \submM{}. We demonstrate the effectiveness of our approach in an
interactive, crowdsourced image collection summarization application.","['Adish Singla', 'Sebastian Tschiatschek', 'Andreas Krause']","['cs.AI', 'cs.LG', 'stat.ML']",2015-11-23 13:19:05+00:00
http://arxiv.org/abs/1511.07130v1,Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions,"We develop parallel predictive entropy search (PPES), a novel algorithm for
Bayesian optimization of expensive black-box objective functions. At each
iteration, PPES aims to select a batch of points which will maximize the
information gain about the global maximizer of the objective. Well known
strategies exist for suggesting a single evaluation point based on previous
observations, while far fewer are known for selecting batches of points to
evaluate in parallel. The few batch selection schemes that have been studied
all resort to greedy methods to compute an optimal batch. To the best of our
knowledge, PPES is the first non-greedy batch Bayesian optimization strategy.
We demonstrate the benefit of this approach in optimization performance on both
synthetic and real world applications, including problems in machine learning,
rocket science and robotics.","['Amar Shah', 'Zoubin Ghahramani']","['cs.LG', 'stat.ML']",2015-11-23 08:21:17+00:00
http://arxiv.org/abs/1511.07125v1,What Happened to My Dog in That Network: Unraveling Top-down Generators in Convolutional Neural Networks,"Top-down information plays a central role in human perception, but plays
relatively little role in many current state-of-the-art deep networks, such as
Convolutional Neural Networks (CNNs). This work seeks to explore a path by
which top-down information can have a direct impact within current deep
networks. We explore this path by learning and using ""generators"" corresponding
to the network internal effects of three types of transformation (each a
restriction of a general affine transformation): rotation, scaling, and
translation. We demonstrate how these learned generators can be used to
transfer top-down information to novel settings, as mediated by the ""feature
flows"" that the transformations (and the associated generators) correspond to
inside the network. Specifically, we explore three aspects: 1) using generators
as part of a method for synthesizing transformed images --- given a previously
unseen image, produce versions of that image corresponding to one or more
specified transformations, 2) ""zero-shot learning"" --- when provided with a
feature flow corresponding to the effect of a transformation of unknown amount,
leverage learned generators as part of a method by which to perform an accurate
categorization of the amount of transformation, even for amounts never observed
during training, and 3) (inside-CNN) ""data augmentation"" --- improve the
classification performance of an existing network by using the learned
generators to directly provide additional training ""inside the CNN"".","['Patrick W. Gallagher', 'Shuai Tang', 'Zhuowen Tu']","['cs.NE', 'cs.CV', 'cs.LG', 'stat.ML']",2015-11-23 07:48:01+00:00
http://arxiv.org/abs/1511.06909v7,BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies,"We propose BlackOut, an approximation algorithm to efficiently train massive
recurrent neural network language models (RNNLMs) with million word
vocabularies. BlackOut is motivated by using a discriminative loss, and we
describe a new sampling strategy which significantly reduces computation while
improving stability, sample efficiency, and rate of convergence. One way to
understand BlackOut is to view it as an extension of the DropOut strategy to
the output layer, wherein we use a discriminative training loss and a weighted
sampling scheme. We also establish close connections between BlackOut,
importance sampling, and noise contrastive estimation (NCE). Our experiments,
on the recently released one billion word language modeling benchmark,
demonstrate scalability and accuracy of BlackOut; we outperform the
state-of-the art, and achieve the lowest perplexity scores on this dataset.
Moreover, unlike other established methods which typically require GPUs or CPU
clusters, we show that a carefully implemented version of BlackOut requires
only 1-10 days on a single machine to train a RNNLM with a million word
vocabulary and billions of parameters on one billion words. Although we
describe BlackOut in the context of RNNLM training, it can be used to any
networks with large softmax output layers.","['Shihao Ji', 'S. V. N. Vishwanathan', 'Nadathur Satish', 'Michael J. Anderson', 'Pradeep Dubey']","['cs.LG', 'cs.CL', 'cs.NE', 'stat.ML']",2015-11-21 17:49:30+00:00
http://arxiv.org/abs/1511.06891v2,Near-Optimal Active Learning of Multi-Output Gaussian Processes,"This paper addresses the problem of active learning of a multi-output
Gaussian process (MOGP) model representing multiple types of coexisting
correlated environmental phenomena. In contrast to existing works, our active
learning problem involves selecting not just the most informative sampling
locations to be observed but also the types of measurements at each selected
location for minimizing the predictive uncertainty (i.e., posterior joint
entropy) of a target phenomenon of interest given a sampling budget.
Unfortunately, such an entropy criterion scales poorly in the numbers of
candidate sampling locations and selected observations when optimized. To
resolve this issue, we first exploit a structure common to sparse MOGP models
for deriving a novel active learning criterion. Then, we exploit a relaxed form
of submodularity property of our new criterion for devising a polynomial-time
approximation algorithm that guarantees a constant-factor approximation of that
achieved by the optimal set of selected observations. Empirical evaluation on
real-world datasets shows that our proposed approach outperforms existing
algorithms for active learning of MOGP and single-output GP models.","['Yehong Zhang', 'Trong Nghia Hoang', 'Kian Hsiang Low', 'Mohan Kankanhalli']","['stat.ML', 'cs.AI', 'cs.LG']",2015-11-21 15:08:53+00:00
http://arxiv.org/abs/1511.06890v1,"Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond","This paper presents a novel nonmyopic adaptive Gaussian process planning
(GPP) framework endowed with a general class of Lipschitz continuous reward
functions that can unify some active learning/sensing and Bayesian optimization
criteria and offer practitioners some flexibility to specify their desired
choices for defining new tasks/problems. In particular, it utilizes a
principled Bayesian sequential decision problem framework for jointly and
naturally optimizing the exploration-exploitation trade-off. In general, the
resulting induced GPP policy cannot be derived exactly due to an uncountable
set of candidate observations. A key contribution of our work here thus lies in
exploiting the Lipschitz continuity of the reward functions to solve for a
nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real
time, we further propose an asymptotically optimal, branch-and-bound anytime
variant of epsilon-GPP with performance guarantee. We empirically demonstrate
the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian
optimization and an energy harvesting task.","['Chun Kai Ling', 'Kian Hsiang Low', 'Patrick Jaillet']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.RO']",2015-11-21 14:57:48+00:00
http://arxiv.org/abs/1511.06821v1,Kernel Additive Principal Components,"Additive principal components (APCs for short) are a nonlinear generalization
of linear principal components. We focus on smallest APCs to describe additive
nonlinear constraints that are approximately satisfied by the data. Thus APCs
fit data with implicit equations that treat the variables symmetrically, as
opposed to regression analyses which fit data with explicit equations that
treat the data asymmetrically by singling out a response variable. We propose a
regularized data-analytic procedure for APC estimation using kernel methods. In
contrast to existing approaches to APCs that are based on regularization
through subspace restriction, kernel methods achieve regularization through
shrinkage and therefore grant distinctive flexibility in APC estimation by
allowing the use of infinite-dimensional functions spaces for searching APC
transformation while retaining computational feasibility. To connect population
APCs and kernelized finite-sample APCs, we study kernelized population APCs and
their associated eigenproblems, which eventually lead to the establishment of
consistency of the estimated APCs. Lastly, we discuss an iterative algorithm
for computing kernelized finite-sample APCs.","['Xin Lu Tan', 'Andreas Buja', 'Zongming Ma']","['stat.ME', 'stat.ML']",2015-11-21 03:12:04+00:00
http://arxiv.org/abs/1511.06807v1,Adding Gradient Noise Improves Learning for Very Deep Networks,"Deep feedforward and recurrent networks have achieved impressive results in
many perception and language processing applications. This success is partially
attributed to architectural innovations such as convolutional and long
short-term memory networks. The main motivation for these architectural
innovations is that they capture better domain knowledge, and importantly are
easier to optimize than more basic architectures. Recently, more complex
architectures such as Neural Turing Machines and Memory Networks have been
proposed for tasks including question answering and general computation,
creating a new set of optimization challenges. In this paper, we discuss a
low-overhead and easy-to-implement technique of adding gradient noise which we
find to be surprisingly effective when training these very deep architectures.
The technique not only helps to avoid overfitting, but also can result in lower
training loss. This method alone allows a fully-connected 20-layer deep network
to be trained with standard gradient descent, even starting from a poor
initialization. We see consistent improvements for many complex models,
including a 72% relative reduction in error rate over a carefully-tuned
baseline on a challenging question-answering task, and a doubling of the number
of accurate binary multiplication models learned across 7,000 random restarts.
We encourage further application of this technique to additional complex modern
architectures.","['Arvind Neelakantan', 'Luke Vilnis', 'Quoc V. Le', 'Ilya Sutskever', 'Lukasz Kaiser', 'Karol Kurach', 'James Martens']","['stat.ML', 'cs.LG']",2015-11-21 01:11:29+00:00
http://arxiv.org/abs/1511.07422v1,Variational Bayes Factor Analysis for i-Vector Extraction,"In this document we are going to derive the equations needed to implement a
Variational Bayes i-vector extractor. This can be used to extract longer
i-vectors reducing the risk of overfittig or to adapt an i-vector extractor
from a database to another with scarce development data. This work is based on
Patrick Kenny's joint factor analysis and Christopher Bishop's variational
principal components.",['Jesús Villalba'],['stat.ML'],2015-11-20 21:38:25+00:00
http://arxiv.org/abs/1511.07421v1,Unsupervised Adaptation of SPLDA,"State-of-the-art speaker recognition relays on models that need a large
amount of training data. This models are successful in tasks like NIST SRE
because there is sufficient data available. However, in real applications, we
usually do not have so much data and, in many cases, the speaker labels are
unknown. We present a method to adapt a PLDA model from a domain with a large
amount of labeled data to another with unlabeled data. We describe a generative
model that produces both sets of data where the unknown labels are modeled like
latent variables. We used variational Bayes to estimate the hidden variables.
Here, we derive the equations for this model. This model has been used in the
papers: ""UNSUPERVISED ADAPTATION OF PLDA BY USING VARIATIONAL BAYES METHODS""
publised at ICASSP 2014, ""Unsupervised Training of PLDA with Variational Bayes""
published at Iberspeech 2014, and ""VARIATIONAL BAYESIAN PLDA FOR SPEAKER
DIARIZATION IN THE MGB CHALLENGE"" published at ASRU 2015.",['Jesús Villalba'],['stat.ML'],2015-11-20 21:25:59+00:00
http://arxiv.org/abs/1511.06772v1,PLDA with Two Sources of Inter-session Variability,"In some speaker recognition scenarios we find conversations recorded
simultaneously over multiple channels. That is the case of the interviews in
the NIST SRE dataset. To take advantage of that, we propose a modification of
the PLDA model that considers two different inter-session variability terms.
The first term is tied between all the recordings belonging to the same
conversation whereas the second is not. Thus, the former mainly intends to
capture the variability due to the phonetic content of the conversation while
the latter tries to capture the channel variability. In this document, we
derive the equations for this model. This model was applied in the paper
""Handling Recordings Acquired Simultaneously over Multiple Channels with PLDA""
published at Interspeech 2013.",['Jesús Villalba'],['stat.ML'],2015-11-20 21:08:04+00:00
http://arxiv.org/abs/1511.07318v1,Bayesian SPLDA,"In this document we are going to derive the equations needed to implement a
Variational Bayes estimation of the parameters of the simplified probabilistic
linear discriminant analysis (SPLDA) model. This can be used to adapt SPLDA
from one database to another with few development data or to implement the
fully Bayesian recipe. Our approach is similar to Bishop's VB PPCA.",['Jesús Villalba'],['stat.ML'],2015-11-20 20:43:43+00:00
http://arxiv.org/abs/1511.06718v1,Top-N recommendations from expressive recommender systems,"Normalized nonnegative models assign probability distributions to users and
random variables to items; see [Stark, 2015]. Rating an item is regarded as
sampling the random variable assigned to the item with respect to the
distribution assigned to the user who rates the item. Models of that kind are
highly expressive. For instance, using normalized nonnegative models we can
understand users' preferences as mixtures of interpretable user stereotypes,
and we can arrange properties of users and items in a hierarchical manner.
These features would not be useful if the predictive power of normalized
nonnegative models was poor. Thus, we analyze here the performance of
normalized nonnegative models for top-N recommendation and observe that their
performance matches the performance of methods like PureSVD which was
introduced in [Cremonesi et al., 2010]. We conclude that normalized nonnegative
models not only provide accurate recommendations but they also deliver (for
free) representations that are interpretable. We deepen the discussion of
normalized nonnegative models by providing further theoretical insights. In
particular, we introduce total variational distance as an operational
similarity measure, we discover scenarios where normalized nonnegative models
yield unique representations of users and items, we prove that the inference of
optimal normalized nonnegative models is NP-hard and finally, we discuss the
relationship between normalized nonnegative models and nonnegative matrix
factorization.",['Cyril Stark'],"['cs.LG', 'stat.ML']",2015-11-20 18:18:45+00:00
http://arxiv.org/abs/1511.06683v1,Top-k Multiclass SVM,"Class ambiguity is typical in image classification problems with a large
number of classes. When classes are difficult to discriminate, it makes sense
to allow k guesses and evaluate classifiers based on the top-k error instead of
the standard zero-one loss. We propose top-k multiclass SVM as a direct method
to optimize for top-k performance. Our generalization of the well-known
multiclass SVM is based on a tight convex upper bound of the top-k error. We
propose a fast optimization scheme based on an efficient projection onto the
top-k simplex, which is of its own interest. Experiments on five datasets show
consistent improvements in top-k accuracy compared to various baselines.","['Maksim Lapin', 'Matthias Hein', 'Bernt Schiele']","['stat.ML', 'cs.CV', 'cs.LG']",2015-11-20 16:49:33+00:00
http://arxiv.org/abs/1511.06644v6,Recurrent Gaussian Processes,"We define Recurrent Gaussian Processes (RGP) models, a general family of
Bayesian nonparametric models with recurrent GP priors which are able to learn
dynamical patterns from sequential data. Similar to Recurrent Neural Networks
(RNNs), RGPs can have different formulations for their internal states,
distinct inference methods and be extended with deep structures. In such
context, we propose a novel deep RGP model whose autoregressive states are
latent, thereby performing representation and dynamical learning
simultaneously. To fully exploit the Bayesian nature of the RGP model we
develop the Recurrent Variational Bayes (REVARB) framework, which enables
efficient inference and strong regularization through coherent propagation of
uncertainty across the RGP layers and states. We also introduce a RGP extension
where variational parameters are greatly reduced by being reparametrized
through RNN-based sequential recognition models. We apply our model to the
tasks of nonlinear system identification and human motion modeling. The
promising obtained results indicate that our RGP model maintains its highly
flexibility while being able to avoid overfitting and being applicable even
when larger datasets are not available.","['César Lincoln C. Mattos', 'Zhenwen Dai', 'Andreas Damianou', 'Jeremy Forth', 'Guilherme A. Barreto', 'Neil D. Lawrence']","['cs.LG', 'stat.ML']",2015-11-20 15:37:24+00:00
http://arxiv.org/abs/1511.06499v4,The Variational Gaussian Process,"Variational inference is a powerful tool for approximate inference, and it
has been recently applied for representation learning with deep generative
models. We develop the variational Gaussian process (VGP), a Bayesian
nonparametric variational family, which adapts its shape to match complex
posterior distributions. The VGP generates approximate posterior samples by
generating latent inputs and warping them through random non-linear mappings;
the distribution over random mappings is learned during inference, enabling the
transformed outputs to adapt to varying complexity. We prove a universal
approximation theorem for the VGP, demonstrating its representative power for
learning any model. For inference we present a variational objective inspired
by auto-encoders and perform black box inference over a wide class of models.
The VGP achieves new state-of-the-art results for unsupervised learning,
inferring models such as the deep latent Gaussian model and the recently
proposed DRAW.","['Dustin Tran', 'Rajesh Ranganath', 'David M. Blei']","['stat.ML', 'cs.LG', 'cs.NE', 'stat.CO']",2015-11-20 06:01:23+00:00
http://arxiv.org/abs/1511.06481v7,Variance Reduction in SGD by Distributed Importance Sampling,"Humans are able to accelerate their learning by selecting training materials
that are the most informative and at the appropriate level of difficulty. We
propose a framework for distributing deep learning in which one set of workers
search for the most informative examples in parallel while a single worker
updates the model on examples selected by importance sampling. This leads the
model to update using an unbiased estimate of the gradient which also has
minimum variance when the sampling proposal is proportional to the L2-norm of
the gradient. We show experimentally that this method reduces gradient variance
even in a context where the cost of synchronization across machines cannot be
ignored, and where the factors for importance sampling are not updated
instantly across the training set.","['Guillaume Alain', 'Alex Lamb', 'Chinnadhurai Sankar', 'Aaron Courville', 'Yoshua Bengio']","['stat.ML', 'cs.LG']",2015-11-20 03:09:43+00:00
http://arxiv.org/abs/1511.06464v4,Unitary Evolution Recurrent Neural Networks,"Recurrent neural networks (RNNs) are notoriously difficult to train. When the
eigenvalues of the hidden to hidden weight matrix deviate from absolute value
1, optimization becomes difficult due to the well studied issue of vanishing
and exploding gradients, especially when trying to learn long-term
dependencies. To circumvent this problem, we propose a new architecture that
learns a unitary weight matrix, with eigenvalues of absolute value exactly 1.
The challenge we address is that of parametrizing unitary matrices in a way
that does not require expensive computations (such as eigendecomposition) after
each weight update. We construct an expressive unitary weight matrix by
composing several structured matrices that act as building blocks with
parameters to be learned. Optimization with this parameterization becomes
feasible only when considering hidden states in the complex domain. We
demonstrate the potential of this architecture by achieving state of the art
results in several hard tasks involving very long-term dependencies.","['Martin Arjovsky', 'Amar Shah', 'Yoshua Bengio']","['cs.LG', 'cs.NE', 'stat.ML']",2015-11-20 00:37:33+00:00
http://arxiv.org/abs/1511.06462v1,Joint Inverse Covariances Estimation with Mutual Linear Structure,"We consider the problem of joint estimation of structured inverse covariance
matrices. We perform the estimation using groups of measurements with different
covariances of the same unknown structure. Assuming the inverse covariances to
span a low dimensional linear subspace in the space of symmetric matrices, our
aim is to determine this structure. It is then utilized to improve the
estimation of the inverse covariances. We propose a novel optimization
algorithm discovering and exploiting the underlying structure and provide its
efficient implementation. Numerical simulations are presented to illustrate the
performance benefits of the proposed algorithm.","['Ilya Soloveychik', 'Ami Wiesel']","['stat.ML', 'stat.AP']",2015-11-20 00:18:12+00:00
http://arxiv.org/abs/1511.06458v2,Bayesian inference via rejection filtering,"We provide a method for approximating Bayesian inference using rejection
sampling. We not only make the process efficient, but also dramatically reduce
the memory required relative to conventional methods by combining rejection
sampling with particle filtering. We also provide an approximate form of
rejection sampling that makes rejection filtering tractable in cases where
exact rejection sampling is not efficient. Finally, we present several
numerical examples of rejection filtering that show its ability to track time
dependent parameters in online settings and also benchmark its performance on
MNIST classification problems.","['Nathan Wiebe', 'Christopher Granade', 'Ashish Kapoor', 'Krysta M Svore']","['cs.LG', 'quant-ph', 'stat.ML']",2015-11-20 00:08:07+00:00
http://arxiv.org/abs/1511.06455v2,Variational Auto-encoded Deep Gaussian Processes,"We develop a scalable deep non-parametric generative model by augmenting deep
Gaussian processes with a recognition model. Inference is performed in a novel
scalable variational framework where the variational posterior distributions
are reparametrized through a multilayer perceptron. The key aspect of this
reformulation is that it prevents the proliferation of variational parameters
which otherwise grow linearly in proportion to the sample size. We derive a new
formulation of the variational lower bound that allows us to distribute most of
the computation in a way that enables to handle datasets of the size of
mainstream deep learning tasks. We show the efficacy of the method on a variety
of challenges including deep unsupervised learning and deep Bayesian
optimization.","['Zhenwen Dai', 'Andreas Damianou', 'Javier González', 'Neil Lawrence']","['cs.LG', 'stat.ML']",2015-11-19 23:47:34+00:00
http://arxiv.org/abs/1511.06443v2,Neural Network Matrix Factorization,"Data often comes in the form of an array or matrix. Matrix factorization
techniques attempt to recover missing or corrupted entries by assuming that the
matrix can be written as the product of two low-rank matrices. In other words,
matrix factorization approximates the entries of the matrix by a simple, fixed
function---namely, the inner product---acting on the latent feature vectors for
the corresponding row and column. Here we consider replacing the inner product
by an arbitrary function that we learn from the data at the same time as we
learn the latent feature vectors. In particular, we replace the inner product
by a multi-layer feed-forward neural network, and learn by alternating between
optimizing the network for fixed latent features, and optimizing the latent
features for a fixed network. The resulting approach---which we call neural
network matrix factorization or NNMF, for short---dominates standard low-rank
techniques on a suite of benchmark but is dominated by some recent proposals
that take advantage of the graph features. Given the vast range of
architectures, activation functions, regularizers, and optimization techniques
that could be used within the NNMF framework, it seems likely the true
potential of the approach has yet to be reached.","['Gintare Karolina Dziugaite', 'Daniel M. Roy']","['cs.LG', 'stat.ML']",2015-11-19 23:13:29+00:00
http://arxiv.org/abs/1511.06442v5,Fast Metric Learning For Deep Neural Networks,"Similarity metrics are a core component of many information retrieval and
machine learning systems. In this work we propose a method capable of learning
a similarity metric from data equipped with a binary relation. By considering
only the similarity constraints, and initially ignoring the features, we are
able to learn target vectors for each instance using one of several
appropriately designed loss functions. A regression model can then be
constructed that maps novel feature vectors to the same target vector space,
resulting in a feature extractor that computes vectors for which a predefined
metric is a meaningful measure of similarity. We present results on both
multiclass and multi-label classification datasets that demonstrate
considerably faster convergence, as well as higher accuracy on the majority of
the intrinsic evaluation tasks and all extrinsic evaluation tasks.","['Henry Gouk', 'Bernhard Pfahringer', 'Michael Cree']","['cs.LG', 'cs.CV', 'stat.ML']",2015-11-19 23:10:00+00:00
http://arxiv.org/abs/1511.06429v5,Patterns for Learning with Side Information,"Supervised, semi-supervised, and unsupervised learning estimate a function
given input/output samples. Generalization of the learned function to unseen
data can be improved by incorporating side information into learning. Side
information are data that are neither from the input space nor from the output
space of the function, but include useful information for learning it. In this
paper we show that learning with side information subsumes a variety of related
approaches, e.g. multi-task learning, multi-view learning and learning using
privileged information. Our main contributions are (i) a new perspective that
connects these previously isolated approaches, (ii) insights about how these
methods incorporate different types of prior knowledge, and hence implement
different patterns, (iii) facilitating the application of these methods in
novel tasks, as well as (iv) a systematic experimental evaluation of these
patterns in two supervised learning tasks.","['Rico Jonschkowski', 'Sebastian Höfer', 'Oliver Brock']","['cs.LG', 'stat.ML']",2015-11-19 22:39:35+00:00
http://arxiv.org/abs/1511.06423v2,An Information Retrieval Approach to Finding Dependent Subspaces of Multiple Views,"Finding relationships between multiple views of data is essential both for
exploratory analysis and as pre-processing for predictive tasks. A prominent
approach is to apply variants of Canonical Correlation Analysis (CCA), a
classical method seeking correlated components between views. The basic CCA is
restricted to maximizing a simple dependency criterion, correlation, measured
directly between data coordinates. We introduce a new method that finds
dependent subspaces of views directly optimized for the data analysis task of
\textit{neighbor retrieval between multiple views}. We optimize mappings for
each view such as linear transformations to maximize cross-view similarity
between neighborhoods of data samples. The criterion arises directly from the
well-defined retrieval task, detects nonlinear and local similarities, is able
to measure dependency of data relationships rather than only individual data
coordinates, and is related to well understood measures of information
retrieval quality. In experiments we show the proposed method outperforms
alternatives in preserving cross-view neighborhood similarities, and yields
insights into local dependencies between multiple views.","['Ziyuan Lin', 'Jaakko Peltonen']","['stat.ML', 'cs.LG']",2015-11-19 22:20:34+00:00
http://arxiv.org/abs/1511.06421v3,Deep Manifold Traversal: Changing Labels with Convolutional Features,"Many tasks in computer vision can be cast as a ""label changing"" problem,
where the goal is to make a semantic change to the appearance of an image or
some subject in an image in order to alter the class membership. Although
successful task-specific methods have been developed for some label changing
applications, to date no general purpose method exists. Motivated by this we
propose deep manifold traversal, a method that addresses the problem in its
most general form: it first approximates the manifold of natural images then
morphs a test image along a traversal path away from a source class and towards
a target class while staying near the manifold throughout. The resulting
algorithm is surprisingly effective and versatile. It is completely data
driven, requiring only an example set of images from the desired source and
target domains. We demonstrate deep manifold traversal on highly diverse label
changing tasks: changing an individual's appearance (age and hair color),
changing the season of an outdoor image, and transforming a city skyline
towards nighttime.","['Jacob R. Gardner', 'Paul Upchurch', 'Matt J. Kusner', 'Yixuan Li', 'Kilian Q. Weinberger', 'Kavita Bala', 'John E. Hopcroft']","['cs.LG', 'cs.CV', 'stat.ML']",2015-11-19 22:17:20+00:00
http://arxiv.org/abs/1511.06419v1,Canonical Autocorrelation Analysis,"We present an extension of sparse Canonical Correlation Analysis (CCA)
designed for finding multiple-to-multiple linear correlations within a single
set of variables. Unlike CCA, which finds correlations between two sets of data
where the rows are matched exactly but the columns represent separate sets of
variables, the method proposed here, Canonical Autocorrelation Analysis (CAA),
finds multivariate correlations within just one set of variables. This can be
useful when we look for hidden parsimonious structures in data, each involving
only a small subset of all features. In addition, the discovered correlations
are highly interpretable as they are formed by pairs of sparse linear
combinations of the original features. We show how CAA can be of use as a tool
for anomaly detection when the expected structure of correlations is not
followed by anomalous data. We illustrate the utility of CAA in two application
domains where single-class and unsupervised learning of correlation structures
are particularly relevant: breast cancer diagnosis and radiation threat
detection. When applied to the Wisconsin Breast Cancer data, single-class CAA
is competitive with supervised methods used in literature. On the radiation
threat detection task, unsupervised CAA performs significantly better than an
unsupervised alternative prevalent in the domain, while providing valuable
additional insights for threat analysis.","['Maria De-Arteaga', 'Artur Dubrawski', 'Peter Huggins']","['stat.ML', 'cs.LG']",2015-11-19 22:13:43+00:00
http://arxiv.org/abs/1511.06416v1,Fast Parallel SAME Gibbs Sampling on General Discrete Bayesian Networks,"A fundamental task in machine learning and related fields is to perform
inference on Bayesian networks. Since exact inference takes exponential time in
general, a variety of approximate methods are used. Gibbs sampling is one of
the most accurate approaches and provides unbiased samples from the posterior
but it has historically been too expensive for large models. In this paper, we
present an optimized, parallel Gibbs sampler augmented with state replication
(SAME or State Augmented Marginal Estimation) to decrease convergence time. We
find that SAME can improve the quality of parameter estimates while
accelerating convergence. Experiments on both synthetic and real data show that
our Gibbs sampler is substantially faster than the state of the art sampler,
JAGS, without sacrificing accuracy. Our ultimate objective is to introduce the
Gibbs sampler to researchers in many fields to expand their range of feasible
inference problems.","['Daniel Seita', 'Haoyu Chen', 'John Canny']","['cs.LG', 'stat.ML']",2015-11-19 22:08:22+00:00
http://arxiv.org/abs/1511.06391v4,Order Matters: Sequence to sequence for sets,"Sequences have become first class citizens in supervised learning thanks to
the resurgence of recurrent neural networks. Many complex tasks that require
mapping from or to a sequence of observations can now be formulated with the
sequence-to-sequence (seq2seq) framework which employs the chain rule to
efficiently represent the joint probability of sequences. In many cases,
however, variable sized inputs and/or outputs might not be naturally expressed
as sequences. For instance, it is not clear how to input a set of numbers into
a model where the task is to sort them; similarly, we do not know how to
organize outputs when they correspond to random variables and the task is to
model their unknown joint probability. In this paper, we first show using
various examples that the order in which we organize input and/or output data
matters significantly when learning an underlying model. We then discuss an
extension of the seq2seq framework that goes beyond sequences and handles input
sets in a principled way. In addition, we propose a loss which, by searching
over possible orders during training, deals with the lack of structure of
output sets. We show empirical evidence of our claims regarding ordering, and
on the modifications to the seq2seq framework on benchmark language modeling
and parsing tasks, as well as two artificial tasks -- sorting numbers and
estimating the joint probability of unknown graphical models.","['Oriol Vinyals', 'Samy Bengio', 'Manjunath Kudlur']","['stat.ML', 'cs.CL', 'cs.LG']",2015-11-19 21:31:26+00:00
http://arxiv.org/abs/1511.06390v2,Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks,"In this paper we present a method for learning a discriminative classifier
from unlabeled or partially labeled data. Our approach is based on an objective
function that trades-off mutual information between observed examples and their
predicted categorical class distribution, against robustness of the classifier
to an adversarial generative model. The resulting algorithm can either be
interpreted as a natural generalization of the generative adversarial networks
(GAN) framework or as an extension of the regularized information maximization
(RIM) framework to robust classification against an optimal adversary. We
empirically evaluate our method - which we dub categorical generative
adversarial networks (or CatGAN) - on synthetic data as well as on challenging
image classification tasks, demonstrating the robustness of the learned
classifiers. We further qualitatively assess the fidelity of samples generated
by the adversarial generator that is learned alongside the discriminative
classifier, and identify links between the CatGAN objective and discriminative
clustering algorithms (such as RIM).",['Jost Tobias Springenberg'],"['stat.ML', 'cs.LG']",2015-11-19 21:26:58+00:00
http://arxiv.org/abs/1511.06385v1,A Unified Gradient Regularization Family for Adversarial Examples,"Adversarial examples are augmented data points generated by imperceptible
perturbation of input samples. They have recently drawn much attention with the
machine learning and data mining community. Being difficult to distinguish from
real examples, such adversarial examples could change the prediction of many of
the best learning models including the state-of-the-art deep learning models.
Recent attempts have been made to build robust models that take into account
adversarial examples. However, these methods can either lead to performance
drops or lack mathematical motivations. In this paper, we propose a unified
framework to build robust machine learning models against adversarial examples.
More specifically, using the unified framework, we develop a family of gradient
regularization methods that effectively penalize the gradient of loss function
w.r.t. inputs. Our proposed framework is appealing in that it offers a unified
view to deal with adversarial examples. It incorporates another
recently-proposed perturbation based approach as a special case. In addition,
we present some visual effects that reveals semantic meaning in those
perturbations, and thus support our regularization method and provide another
explanation for generalizability of adversarial examples. By applying this
technique to Maxout networks, we conduct a series of experiments and achieve
encouraging results on two benchmark datasets. In particular,we attain the best
accuracy on MNIST data (without data augmentation) and competitive performance
on CIFAR-10 data.","['Chunchuan Lyu', 'Kaizhu Huang', 'Hai-Ning Liang']","['cs.LG', 'stat.ML']",2015-11-19 21:14:43+00:00
http://arxiv.org/abs/1511.06382v6,Iterative Refinement of the Approximate Posterior for Directed Belief Networks,"Variational methods that rely on a recognition network to approximate the
posterior of directed graphical models offer better inference and learning than
previous methods. Recent advances that exploit the capacity and flexibility in
this approach have expanded what kinds of models can be trained. However, as a
proposal for the posterior, the capacity of the recognition network is limited,
which can constrain the representational power of the generative model and
increase the variance of Monte Carlo estimates. To address these issues, we
introduce an iterative refinement procedure for improving the approximate
posterior of the recognition network and show that training with the refined
posterior is competitive with state-of-the-art methods. The advantages of
refinement are further evident in an increased effective sample size, which
implies a lower variance of gradient estimates.","['R Devon Hjelm', 'Kyunghyun Cho', 'Junyoung Chung', 'Russ Salakhutdinov', 'Vince Calhoun', 'Nebojsa Jojic']","['cs.LG', 'stat.ML']",2015-11-19 21:11:12+00:00
http://arxiv.org/abs/1511.06350v3,Structured Prediction Energy Networks,"We introduce structured prediction energy networks (SPENs), a flexible
framework for structured prediction. A deep architecture is used to define an
energy function of candidate labels, and then predictions are produced by using
back-propagation to iteratively optimize the energy with respect to the labels.
This deep architecture captures dependencies between labels that would lead to
intractable graphical models, and performs structure learning by automatically
learning discriminative features of the structured output. One natural
application of our technique is multi-label classification, which traditionally
has required strict prior assumptions about the interactions between labels to
ensure tractable learning and prediction. We are able to apply SPENs to
multi-label problems with substantially larger label sets than previous
applications of structured prediction, while modeling high-order interactions
using minimal structural assumptions. Overall, deep learning provides
remarkable tools for learning features of the inputs to a prediction problem,
and this work extends these techniques to learning features of structured
outputs. Our experiments provide impressive performance on a variety of
benchmark multi-label classification tasks, demonstrate that our technique can
be used to provide interpretable structure learning, and illuminate fundamental
trade-offs between feed-forward and iterative structured prediction.","['David Belanger', 'Andrew McCallum']","['cs.LG', 'stat.ML']",2015-11-19 20:39:59+00:00
http://arxiv.org/abs/1511.06340v2,Robust Classification by Pre-conditioned LASSO and Transductive Diffusion Component Analysis,"Modern machine learning-based recognition approaches require large-scale
datasets with large number of labelled training images. However, such datasets
are inherently difficult and costly to collect and annotate. Hence there is a
great and growing interest in automatic dataset collection methods that can
leverage the web. % which are collected % in a cheap, efficient and yet
unreliable way. Collecting datasets in this way, however, requires robust and
efficient ways for detecting and excluding outliers that are common and
prevalent. % Outliers are thus a % prominent treat of using these dataset. So
far, there have been a limited effort in machine learning community to directly
detect outliers for robust classification. Inspired by the recent work on
Pre-conditioned LASSO, this paper formulates the outlier detection task using
Pre-conditioned LASSO and employs \red{unsupervised} transductive diffusion
component analysis to both integrate the topological structure of the data
manifold, from labeled and unlabeled instances, and reduce the feature
dimensionality. Synthetic experiments as well as results on two real-world
classification tasks show that our framework can robustly detect the outliers
and improve classification.","['Yanwei Fu', 'De-An Huang', 'Leonid Sigal']","['cs.LG', 'cs.CV', 'math.ST', 'stat.ML', 'stat.TH']",2015-11-19 20:13:51+00:00
http://arxiv.org/abs/1511.06321v5,Neural network-based clustering using pairwise constraints,"This paper presents a neural network-based end-to-end clustering framework.
We design a novel strategy to utilize the contrastive criteria for pushing
data-forming clusters directly from raw data, in addition to learning a feature
embedding suitable for such clustering. The network is trained with weak
labels, specifically partial pairwise relationships between data instances. The
cluster assignments and their probabilities are then obtained at the output
layer by feed-forwarding the data. The framework has the interesting
characteristic that no cluster centers need to be explicitly specified, thus
the resulting cluster distribution is purely data-driven and no distance
metrics need to be predefined. The experiments show that the proposed approach
beats the conventional two-stage method (feature embedding with k-means) by a
significant margin. It also compares favorably to the performance of the
standard cross entropy loss for classification. Robustness analysis also shows
that the method is largely insensitive to the number of clusters. Specifically,
we show that the number of dominant clusters is close to the true number of
clusters even when a large k is used for clustering.","['Yen-Chang Hsu', 'Zsolt Kira']","['cs.LG', 'stat.ML']",2015-11-19 19:36:38+00:00
http://arxiv.org/abs/1511.06251v3,Stochastic modified equations and adaptive stochastic gradient algorithms,"We develop the method of stochastic modified equations (SME), in which
stochastic gradient algorithms are approximated in the weak sense by
continuous-time stochastic differential equations. We exploit the continuous
formulation together with optimal control theory to derive novel adaptive
hyper-parameter adjustment policies. Our algorithms have competitive
performance with the added benefit of being robust to varying models and
datasets. This provides a general methodology for the analysis and design of
stochastic gradient algorithms.","['Qianxiao Li', 'Cheng Tai', 'Weinan E']","['cs.LG', 'stat.ML', '68W20']",2015-11-19 16:49:33+00:00
http://arxiv.org/abs/1511.06247v3,Predicting online user behaviour using deep learning algorithms,"We propose a robust classifier to predict buying intentions based on user
behaviour within a large e-commerce website. In this work we compare
traditional machine learning techniques with the most advanced deep learning
approaches. We show that both Deep Belief Networks and Stacked Denoising
auto-Encoders achieved a substantial improvement by extracting features from
high dimensional data during the pre-train phase. They prove also to be more
convenient to deal with severe class imbalance.",['Armando Vieira'],"['cs.LG', 'stat.ML']",2015-11-19 16:47:00+00:00
http://arxiv.org/abs/1511.06238v3,Multimodal sparse representation learning and applications,"Unsupervised methods have proven effective for discriminative tasks in a
single-modality scenario. In this paper, we present a multimodal framework for
learning sparse representations that can capture semantic correlation between
modalities. The framework can model relationships at a higher level by forcing
the shared sparse representation. In particular, we propose the use of joint
dictionary learning technique for sparse coding and formulate the joint
representation for concision, cross-modal representations (in case of a missing
modality), and union of the cross-modal representations. Given the accelerated
growth of multimodal data posted on the Web such as YouTube, Wikipedia, and
Twitter, learning good multimodal features is becoming increasingly important.
We show that the shared representations enabled by our framework substantially
improve the classification performance under both unimodal and multimodal
settings. We further show how deep architectures built on the proposed
framework are effective for the case of highly nonlinear correlations between
modalities. The effectiveness of our approach is demonstrated experimentally in
image denoising, multimedia event detection and retrieval on the TRECVID
dataset (audio-video), category classification on the Wikipedia dataset
(image-text), and sentiment classification on PhotoTweet (image-text).","['Miriam Cha', 'Youngjune Gwon', 'H. T. Kung']","['cs.LG', 'cs.CV', 'stat.ML']",2015-11-19 16:26:24+00:00
http://arxiv.org/abs/1511.06208v1,Diffusion Representations,"Diffusion Maps framework is a kernel based method for manifold learning and
data analysis that defines diffusion similarities by imposing a Markovian
process on the given dataset. Analysis by this process uncovers the intrinsic
geometric structures in the data. Recently, it was suggested to replace the
standard kernel by a measure-based kernel that incorporates information about
the density of the data. Thus, the manifold assumption is replaced by a more
general measure-based assumption.
  The measure-based diffusion kernel incorporates two separate independent
representations. The first determines a measure that correlates with a density
that represents normal behaviors and patterns in the data. The second consists
of the analyzed multidimensional data points.
  In this paper, we present a representation framework for data analysis of
datasets that is based on a closed-form decomposition of the measure-based
kernel. The proposed representation preserves pairwise diffusion distances that
does not depend on the data size while being invariant to scale. For a
stationary data, no out-of-sample extension is needed for embedding newly
arrived data points in the representation space. Several aspects of the
presented methodology are demonstrated on analytically generated data.","['Moshe Salhov', 'Amit Bermanis', 'Guy Wolf', 'Amir Averbuch']","['stat.ML', 'cs.LG', 'math.SP']",2015-11-19 15:30:39+00:00
http://arxiv.org/abs/1511.06201v1,Adjustable Bounded Rectifiers: Towards Deep Binary Representations,"Binary representation is desirable for its memory efficiency, computation
speed and robustness. In this paper, we propose adjustable bounded rectifiers
to learn binary representations for deep neural networks. While hard
constraining representations across layers to be binary makes training
unreasonably difficult, we softly encourage activations to diverge from real
values to binary by approximating step functions. Our final representation is
completely binary. We test our approach on MNIST, CIFAR10, and ILSVRC2012
dataset, and systematically study the training dynamics of the binarization
process. Our approach can binarize the last layer representation without loss
of performance and binarize all the layers with reasonably small degradations.
The memory space that it saves may allow more sophisticated models to be
deployed, thus compensating the loss. To the best of our knowledge, this is the
first work to report results on current deep network architectures using
complete binary middle representations. Given the learned representations, we
find that the firing or inhibition of a binary neuron is usually associated
with a meaningful interpretation across different classes. This suggests that
the semantic structure of a neural network may be manifested through a guided
binarization process.","['Zhirong Wu', 'Dahua Lin', 'Xiaoou Tang']","['cs.LG', 'stat.ML']",2015-11-19 15:14:02+00:00
http://arxiv.org/abs/1511.06198v2,Spherical Cap Packing Asymptotics and Rank-Extreme Detection,"We study the spherical cap packing problem with a probabilistic approach.
Such probabilistic considerations result in an asymptotic sharp universal
uniform bound on the maximal inner product between any set of unit vectors and
a stochastically independent uniformly distributed unit vector. When the set of
unit vectors are themselves independently uniformly distributed, we further
develop the extreme value distribution limit of the maximal inner product,
which characterizes its uncertainty around the bound.
  As applications of the above asymptotic results, we derive (1) an asymptotic
sharp universal uniform bound on the maximal spurious correlation, as well as
its uniform convergence in distribution when the explanatory variables are
independently Gaussian distributed; and (2) an asymptotic sharp universal bound
on the maximum norm of a low-rank elliptically distributed vector, as well as
related limiting distributions. With these results, we develop a fast detection
method for a low-rank structure in high-dimensional Gaussian data without using
the spectrum information.",['Kai Zhang'],"['math.ST', 'cs.IT', 'math.IT', 'physics.data-an', 'stat.ME', 'stat.ML', 'stat.TH']",2015-11-19 15:08:10+00:00
http://arxiv.org/abs/1511.06120v1,The Kernel Two-Sample Test for Brain Networks,"In clinical and neuroscientific studies, systematic differences between two
populations of brain networks are investigated in order to characterize mental
diseases or processes. Those networks are usually represented as graphs built
from neuroimaging data and studied by means of graph analysis methods. The
typical machine learning approach to study these brain graphs creates a
classifier and tests its ability to discriminate the two populations. In
contrast to this approach, in this work we propose to directly test whether two
populations of graphs are different or not, by using the kernel two-sample test
(KTST), without creating the intermediate classifier. We claim that, in
general, the two approaches provides similar results and that the KTST requires
much less computation. Additionally, in the regime of low sample size, we claim
that the KTST has lower frequency of Type II error than the classification
approach. Besides providing algorithmic considerations to support these claims,
we show strong evidence through experiments and one simulation.","['Emanuele Olivetti', 'Sandro Vega-Pons', 'Paolo Avesani']",['stat.ML'],2015-11-19 11:01:54+00:00
http://arxiv.org/abs/1511.06114v4,Multi-task Sequence to Sequence Learning,"Sequence to sequence learning has recently emerged as a new paradigm in
supervised learning. To date, most of its applications focused on only one task
and not much work explored this framework for multiple tasks. This paper
examines three multi-task learning (MTL) settings for sequence to sequence
models: (a) the oneto-many setting - where the encoder is shared between
several tasks such as machine translation and syntactic parsing, (b) the
many-to-one setting - useful when only the decoder can be shared, as in the
case of translation and image caption generation, and (c) the many-to-many
setting - where multiple encoders and decoders are shared, which is the case
with unsupervised objectives and translation. Our results show that training on
a small amount of parsing and image caption data can improve the translation
quality between English and German by up to 1.5 BLEU points over strong
single-task baselines on the WMT benchmarks. Furthermore, we have established a
new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we
reveal interesting properties of the two unsupervised learning objectives,
autoencoder and skip-thought, in the MTL context: autoencoder helps less in
terms of perplexities but more on BLEU scores compared to skip-thought.","['Minh-Thang Luong', 'Quoc V. Le', 'Ilya Sutskever', 'Oriol Vinyals', 'Lukasz Kaiser']","['cs.LG', 'cs.CL', 'stat.ML']",2015-11-19 10:24:14+00:00
http://arxiv.org/abs/1511.06068v4,Reducing Overfitting in Deep Networks by Decorrelating Representations,"One major challenge in training Deep Neural Networks is preventing
overfitting. Many techniques such as data augmentation and novel regularizers
such as Dropout have been proposed to prevent overfitting without requiring a
massive amount of training data. In this work, we propose a new regularizer
called DeCov which leads to significantly reduced overfitting (as indicated by
the difference between train and val performance), and better generalization.
Our regularizer encourages diverse or non-redundant representations in Deep
Neural Networks by minimizing the cross-covariance of hidden activations. This
simple intuition has been explored in a number of past works but surprisingly
has never been applied as a regularizer in supervised learning. Experiments
across a range of datasets and network architectures show that this loss always
reduces overfitting while almost always maintaining or increasing
generalization performance and often improving performance over Dropout.","['Michael Cogswell', 'Faruk Ahmed', 'Ross Girshick', 'Larry Zitnick', 'Dhruv Batra']","['cs.LG', 'stat.ML']",2015-11-19 06:23:09+00:00
http://arxiv.org/abs/1511.06067v3,Convolutional neural networks with low-rank regularization,"Large CNNs have delivered impressive performance in various computer vision
applications. But the storage and computation requirements make it problematic
for deploying these models on mobile devices. Recently, tensor decompositions
have been used for speeding up CNNs. In this paper, we further develop the
tensor decomposition technique. We propose a new algorithm for computing the
low-rank tensor decomposition for removing the redundancy in the convolution
kernels. The algorithm finds the exact global optimizer of the decomposition
and is more effective than iterative methods. Based on the decomposition, we
further propose a new method for training low-rank constrained CNNs from
scratch. Interestingly, while achieving a significant speedup, sometimes the
low-rank constrained CNNs delivers significantly better performance than their
non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank
NIN model achieves $91.31\%$ accuracy (without data augmentation), which also
improves upon state-of-the-art result. We evaluated the proposed method on
CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet,
NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is
reduced by half while the performance is still comparable. Empirical success
suggests that low-rank tensor decompositions can be a very useful tool for
speeding up large CNNs.","['Cheng Tai', 'Tong Xiao', 'Yi Zhang', 'Xiaogang Wang', 'Weinan E']","['cs.LG', 'cs.CV', 'stat.ML']",2015-11-19 06:13:55+00:00
http://arxiv.org/abs/1511.06063v2,A Novel Approach for Phase Identification in Smart Grids Using Graph Theory and Principal Component Analysis,"Consumers with low demand, like households, are generally supplied
single-phase power by connecting their service mains to one of the phases of a
distribution transformer. The distribution companies face the problem of
keeping a record of consumer connectivity to a phase due to uninformed changes
that happen. The exact phase connectivity information is important for the
efficient operation and control of distribution system. We propose a new data
driven approach to the problem based on Principal Component Analysis (PCA) and
its Graph Theoretic interpretations, using energy measurements in equally timed
short intervals, generated from smart meters. We propose an algorithm for
inferring phase connectivity from noisy measurements. The algorithm is
demonstrated using simulated data for phase connectivities in distribution
networks.","['P Satya Jayadev', 'Aravind Rajeswaran', 'Nirav P Bhatt', 'Ramkrishna Pasumarthy']","['cs.LG', 'stat.AP', 'stat.ML']",2015-11-19 05:39:16+00:00
http://arxiv.org/abs/1511.06051v4,SparkNet: Training Deep Networks in Spark,"Training deep networks is a time-consuming process, with networks for object
recognition often requiring multiple days to train. For this reason, leveraging
the resources of a cluster to speed up training is an important area of work.
However, widely-popular batch-processing computational frameworks like
MapReduce and Spark were not designed to support the asynchronous and
communication-intensive workloads of existing distributed deep learning
systems. We introduce SparkNet, a framework for training deep networks in
Spark. Our implementation includes a convenient interface for reading data from
Spark RDDs, a Scala interface to the Caffe deep learning framework, and a
lightweight multi-dimensional tensor library. Using a simple parallelization
scheme for stochastic gradient descent, SparkNet scales well with the cluster
size and tolerates very high-latency communication. Furthermore, it is easy to
deploy and use with no parameter tuning, and it is compatible with existing
Caffe models. We quantify the dependence of the speedup obtained by SparkNet on
the number of machines, the communication frequency, and the cluster's
communication overhead, and we benchmark our system's performance on the
ImageNet dataset.","['Philipp Moritz', 'Robert Nishihara', 'Ion Stoica', 'Michael I. Jordan']","['stat.ML', 'cs.DC', 'cs.LG', 'cs.NE', 'math.OC']",2015-11-19 03:29:56+00:00
http://arxiv.org/abs/1511.06038v4,Neural Variational Inference for Text Processing,"Recent advances in neural variational inference have spawned a renaissance in
deep latent variable models. In this paper we introduce a generic variational
inference framework for generative and conditional models of text. While
traditional variational methods derive an analytic approximation for the
intractable distributions over latent variables, here we construct an inference
network conditioned on the discrete text input to provide the variational
distribution. We validate this framework on two very different text modelling
applications, generative document modelling and supervised question answering.
Our neural variational document model combines a continuous stochastic document
representation with a bag-of-words generative model and achieves the lowest
reported perplexities on two standard test corpora. The neural answer selection
model employs a stochastic representation layer within an attention mechanism
to extract the semantics between a question and answer pair. On two question
answering benchmarks this model exceeds all previous published benchmarks.","['Yishu Miao', 'Lei Yu', 'Phil Blunsom']","['cs.CL', 'cs.LG', 'stat.ML']",2015-11-19 01:23:28+00:00
http://arxiv.org/abs/1511.06036v1,Stochastic gradient method with accelerated stochastic dynamics,"In this paper, we propose a novel technique to implement stochastic gradient
methods, which are beneficial for learning from large datasets, through
accelerated stochastic dynamics. A stochastic gradient method is based on
mini-batch learning for reducing the computational cost when the amount of data
is large. The stochasticity of the gradient can be mitigated by the injection
of Gaussian noise, which yields the stochastic Langevin gradient method; this
method can be used for Bayesian posterior sampling. However, the performance of
the stochastic Langevin gradient method depends on the mixing rate of the
stochastic dynamics. In this study, we propose violating the detailed balance
condition to enhance the mixing rate. Recent studies have revealed that
violating the detailed balance condition accelerates the convergence to a
stationary state and reduces the correlation time between the samplings. We
implement this violation of the detailed balance condition in the stochastic
gradient Langevin method and test our method for a simple model to demonstrate
its performance.",['Masayuki Ohzeki'],"['stat.ML', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.CV']",2015-11-19 01:01:59+00:00
http://arxiv.org/abs/1511.06014v3,Regret Analysis of the Finite-Horizon Gittins Index Strategy for Multi-Armed Bandits,"I analyse the frequentist regret of the famous Gittins index strategy for
multi-armed bandits with Gaussian noise and a finite horizon. Remarkably it
turns out that this approach leads to finite-time regret guarantees comparable
to those available for the popular UCB algorithm. Along the way I derive
finite-time bounds on the Gittins index that are asymptotically exact and may
be of independent interest. I also discuss some computational issues and
present experimental results suggesting that a particular version of the
Gittins index strategy is a modest improvement on existing algorithms with
finite-time regret guarantees such as UCB and Thompson sampling.",['Tor Lattimore'],"['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2015-11-18 22:52:26+00:00
http://arxiv.org/abs/1511.05939v2,Metric Learning with Adaptive Density Discrimination,"Distance metric learning (DML) approaches learn a transformation to a
representation space where distance is in correspondence with a predefined
notion of similarity. While such models offer a number of compelling benefits,
it has been difficult for these to compete with modern classification
algorithms in performance and even in feature extraction.
  In this work, we propose a novel approach explicitly designed to address a
number of subtle yet important issues which have stymied earlier DML
algorithms. It maintains an explicit model of the distributions of the
different classes in representation space. It then employs this knowledge to
adaptively assess similarity, and achieve local discrimination by penalizing
class distribution overlap.
  We demonstrate the effectiveness of this idea on several tasks. Our approach
achieves state-of-the-art classification results on a number of fine-grained
visual recognition datasets, surpassing the standard softmax classifier and
outperforming triplet loss by a relative margin of 30-40%. In terms of
computational performance, it alleviates training inefficiencies in the
traditional triplet loss, reaching the same error in 5-30 times fewer
iterations. Beyond classification, we further validate the saliency of the
learnt representations via their attribute concentration and hierarchy recovery
properties, achieving 10-25% relative gains on the softmax classifier and
25-50% on triplet loss in these tasks.","['Oren Rippel', 'Manohar Paluri', 'Piotr Dollar', 'Lubomir Bourdev']","['stat.ML', 'cs.LG']",2015-11-18 20:41:05+00:00
http://arxiv.org/abs/1511.05932v1,On the Global Linear Convergence of Frank-Wolfe Optimization Variants,"The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity
thanks in particular to its ability to nicely handle the structured constraints
appearing in machine learning applications. However, its convergence rate is
known to be slow (sublinear) when the solution lies at the boundary. A simple
less-known fix is to add the possibility to take 'away steps' during
optimization, an operation that importantly does not require a feasibility
oracle. In this paper, we highlight and clarify several variants of the
Frank-Wolfe optimization algorithm that have been successfully applied in
practice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum
norm point algorithm, and prove for the first time that they all enjoy global
linear convergence, under a weaker condition than strong convexity of the
objective. The constant in the convergence rate has an elegant interpretation
as the product of the (classical) condition number of the function with a novel
geometric quantity that plays the role of a 'condition number' of the
constraint set. We provide pointers to where these algorithms have made a
difference in practice, in particular with the flow polytope, the marginal
polytope and the base polytope for submodular optimization.","['Simon Lacoste-Julien', 'Martin Jaggi']","['math.OC', 'cs.LG', 'stat.ML', '90C52, 90C90, 68T05', 'G.1.6; I.2.6']",2015-11-18 20:24:43+00:00
http://arxiv.org/abs/1511.05897v3,Censoring Representations with an Adversary,"In practice, there are often explicit constraints on what representations or
decisions are acceptable in an application of machine learning. For example it
may be a legal requirement that a decision must not favour a particular group.
Alternatively it can be that that representation of data must not have
identifying information. We address these two related issues by learning
flexible representations that minimize the capability of an adversarial critic.
This adversary is trying to predict the relevant sensitive variable from the
representation, and so minimizing the performance of the adversary ensures
there is little or no information in the representation about the sensitive
variable. We demonstrate this adversarial approach on two problems: making
decisions free from discrimination and removing private information from
images. We formulate the adversarial model as a minimax problem, and optimize
that minimax objective using a stochastic gradient alternate min-max optimizer.
We demonstrate the ability to provide discriminant free representations for
standard test problems, and compare with previous state of the art methods for
fairness, showing statistically significant improvement across most cases. The
flexibility of this method is shown via a novel problem: removing annotations
from images, from unaligned training examples of annotated and unannotated
images, and with no a priori knowledge of the form of annotation provided to
the model.","['Harrison Edwards', 'Amos Storkey']","['cs.LG', 'cs.AI', 'stat.ML']",2015-11-18 18:06:24+00:00
http://arxiv.org/abs/1511.05864v3,Fast Saddle-Point Algorithm for Generalized Dantzig Selector and FDR Control with the Ordered l1-Norm,"In this paper we propose a primal-dual proximal extragradient algorithm to
solve the generalized Dantzig selector (GDS) estimation problem, based on a new
convex-concave saddle-point (SP) reformulation. Our new formulation makes it
possible to adopt recent developments in saddle-point optimization, to achieve
the optimal $O(1/k)$ rate of convergence. Compared to the optimal non-SP
algorithms, ours do not require specification of sensitive parameters that
affect algorithm performance or solution quality. We also provide a new
analysis showing a possibility of local acceleration to achieve the rate of
$O(1/k^2)$ in special cases even without strong convexity or strong smoothness.
As an application, we propose a GDS equipped with the ordered $\ell_1$-norm,
showing its false discovery rate control properties in variable selection.
Algorithm performance is compared between ours and other alternatives,
including the linearized ADMM, Nesterov's smoothing, Nemirovski's mirror-prox,
and the accelerated hybrid proximal extragradient techniques.","['Sangkyun Lee', 'Damian Brzyski', 'Malgorzata Bogdan']","['stat.ML', 'math.OC']",2015-11-18 16:29:51+00:00
http://arxiv.org/abs/1511.05837v1,Using Machine Learning to Predict the Outcome of English County twenty over Cricket Matches,"Cricket betting is a multi-billion dollar market. Therefore, there is a
strong incentive for models that can predict the outcomes of games and beat the
odds provided by bookers. The aim of this study was to investigate to what
degree it is possible to predict the outcome of cricket matches. The target
competition was the English twenty over county cricket cup. The original
features alongside engineered features gave rise to more than 500 team and
player statistics. The models were optimized firstly with team features only
and then both team and player features. The performance of the models was
tested over individual seasons from 2009 to 2014 having been trained over
previous season data in each case. The optimal model was a simple prediction
method combined with complex hierarchical features and was shown to
significantly outperform a gambling industry benchmark.","['Stylianos Kampakis', 'William Thomas']","['stat.ML', 'stat.AP']",2015-11-18 15:39:18+00:00
http://arxiv.org/abs/1511.09392v1,Enhancements in statistical spoken language translation by de-normalization of ASR results,"Spoken language translation (SLT) has become very important in an
increasingly globalized world. Machine translation (MT) for automatic speech
recognition (ASR) systems is a major challenge of great interest. This research
investigates that automatic sentence segmentation of speech that is important
for enriching speech recognition output and for aiding downstream language
processing. This article focuses on the automatic sentence segmentation of
speech and improving MT results. We explore the problem of identifying sentence
boundaries in the transcriptions produced by automatic speech recognition
systems in the Polish language. We also experiment with reverse normalization
of the recognized speech samples.","['Agnieszka Wołk', 'Krzysztof Wołk', 'Krzysztof Marasek']","['cs.CL', 'stat.ML']",2015-11-18 15:34:21+00:00
http://arxiv.org/abs/1511.05835v4,Alternative Markov and Causal Properties for Acyclic Directed Mixed Graphs,"We extend Andersson-Madigan-Perlman chain graphs by (i) relaxing the
semidirected acyclity constraint so that only directed cycles are forbidden,
and (ii) allowing up to two edges between any pair of nodes. We introduce
global, and ordered local and pairwise Markov properties for the new models. We
show the equivalence of these properties for strictly positive probability
distributions. We also show that when the random variables are continuous, the
new models can be interpreted as systems of structural equations with
correlated errors. This enables us to adapt Pearl's do-calculus to them.
Finally, we describe an exact algorithm for learning the new models from
observational and interventional data via answer set programming.",['Jose M. Peña'],"['stat.ML', 'cs.AI']",2015-11-18 15:33:57+00:00
http://arxiv.org/abs/1511.06285v1,Harvesting comparable corpora and mining them for equivalent bilingual sentences using statistical classification and analogy- based heuristics,"Parallel sentences are a relatively scarce but extremely useful resource for
many applications including cross-lingual retrieval and statistical machine
translation. This research explores our new methodologies for mining such data
from previously obtained comparable corpora. The task is highly practical since
non-parallel multilingual data exist in far greater quantities than parallel
corpora, but parallel sentences are a much more useful resource. Here we
propose a web crawling method for building subject-aligned comparable corpora
from e.g. Wikipedia dumps and Euronews web page. The improvements in machine
translation are shown on Polish-English language pair for various text domains.
We also tested another method of building parallel corpora based on comparable
corpora data. It lets automatically broad existing corpus of sentences from
subject of corpora based on analogies between them.","['Krzysztof Wołk', 'Emilia Rejmund', 'Krzysztof Marasek']","['cs.CL', 'stat.ML']",2015-11-18 15:26:06+00:00
http://arxiv.org/abs/1511.05741v1,A Random Forest Guided Tour,"The random forest algorithm, proposed by L. Breiman in 2001, has been
extremely successful as a general-purpose classification and regression method.
The approach, which combines several randomized decision trees and aggregates
their predictions by averaging, has shown excellent performance in settings
where the number of variables is much larger than the number of observations.
Moreover, it is versatile enough to be applied to large-scale problems, is
easily adapted to various ad-hoc learning tasks, and returns measures of
variable importance. The present article reviews the most recent theoretical
and methodological developments for random forests. Emphasis is placed on the
mathematical forces driving the algorithm, with special attention given to the
selection of parameters, the resampling mechanism, and variable importance
measures. This review is intended to provide non-experts easy access to the
main ideas.","['Gérard Biau', 'Erwan Scornet']","['math.ST', 'stat.ML', 'stat.TH']",2015-11-18 11:34:43+00:00
http://arxiv.org/abs/1511.05720v1,Online learning in repeated auctions,"Motivated by online advertising auctions, we consider repeated Vickrey
auctions where goods of unknown value are sold sequentially and bidders only
learn (potentially noisy) information about a good's value once it is
purchased. We adopt an online learning approach with bandit feedback to model
this problem and derive bidding strategies for two models: stochastic and
adversarial. In the stochastic model, the observed values of the goods are
random variables centered around the true value of the good. In this case,
logarithmic regret is achievable when competing against well behaved
adversaries. In the adversarial model, the goods need not be identical and we
simply compare our performance against that of the best fixed bid in hindsight.
We show that sublinear regret is also achievable in this case and prove
matching minimax lower bounds. To our knowledge, this is the first complete set
of strategies for bidders participating in auctions of this type.","['Jonathan Weed', 'Vianney Perchet', 'Philippe Rigollet']","['cs.GT', 'cs.LG', 'stat.ML', 'Primary 62L05, secondary 62C20']",2015-11-18 10:17:33+00:00
http://arxiv.org/abs/1511.05706v1,Efficient Output Kernel Learning for Multiple Tasks,"The paradigm of multi-task learning is that one can achieve better
generalization by learning tasks jointly and thus exploiting the similarity
between the tasks rather than learning them independently of each other. While
previously the relationship between tasks had to be user-defined in the form of
an output kernel, recent approaches jointly learn the tasks and the output
kernel. As the output kernel is a positive semidefinite matrix, the resulting
optimization problems are not scalable in the number of tasks as an
eigendecomposition is required in each step. \mbox{Using} the theory of
positive semidefinite kernels we show in this paper that for a certain class of
regularizers on the output kernel, the constraint of being positive
semidefinite can be dropped as it is automatically satisfied for the relaxed
problem. This leads to an unconstrained dual problem which can be solved
efficiently. Experiments on several multi-task and multi-class data sets
illustrate the efficacy of our approach in terms of computational efficiency as
well as generalization performance.","['Pratik Jawanpuria', 'Maksim Lapin', 'Matthias Hein', 'Bernt Schiele']","['stat.ML', 'cs.LG']",2015-11-18 09:37:54+00:00
http://arxiv.org/abs/1511.05680v2,Wishart Mechanism for Differentially Private Principal Components Analysis,"We propose a new input perturbation mechanism for publishing a covariance
matrix to achieve $(\epsilon,0)$-differential privacy. Our mechanism uses a
Wishart distribution to generate matrix noise. In particular, We apply this
mechanism to principal component analysis. Our mechanism is able to keep the
positive semi-definiteness of the published covariance matrix. Thus, our
approach gives rise to a general publishing framework for input perturbation of
a symmetric positive semidefinite matrix. Moreover, compared with the classic
Laplace mechanism, our method has better utility guarantee. To the best of our
knowledge, Wishart mechanism is the best input perturbation approach for
$(\epsilon,0)$-differentially private PCA. We also compare our work with
previous exponential mechanism algorithms in the literature and provide near
optimal bound while having more flexibility and less computational
intractability.","['Wuxuan Jiang', 'Cong Xie', 'Zhihua Zhang']","['cs.CR', 'cs.DS', 'stat.ML']",2015-11-18 07:34:23+00:00
http://arxiv.org/abs/1511.05660v1,Bayesian hypothesis testing for one bit compressed sensing with sensing matrix perturbation,"This letter proposes a low-computational Bayesian algorithm for noisy sparse
recovery in the context of one bit compressed sensing with sensing matrix
perturbation. The proposed algorithm which is called BHT-MLE comprises a sparse
support detector and an amplitude estimator. The support detector utilizes
Bayesian hypothesis test, while the amplitude estimator uses an ML estimator
which is obtained by solving a convex optimization problem. Simulation results
show that BHT-MLE algorithm offers more reconstruction accuracy than that of an
ML estimator (MLE) at a low computational cost.","['H. Zayyani', 'M. Korki', 'F. Marvasti']","['stat.ML', 'cs.IT', 'math.IT']",2015-11-18 05:28:26+00:00
http://arxiv.org/abs/1511.05650v1,Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models,"Normalized random measures (NRMs) provide a broad class of discrete random
measures that are often used as priors for Bayesian nonparametric models.
Dirichlet process is a well-known example of NRMs. Most of posterior inference
methods for NRM mixture models rely on MCMC methods since they are easy to
implement and their convergence is well studied. However, MCMC often suffers
from slow convergence when the acceptance rate is low. Tree-based inference is
an alternative deterministic posterior inference method, where Bayesian
hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering
(IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively.
Although IBHC is a promising method for posterior inference for NRMM models due
to its efficiency and applicability to online inference, its convergence is not
guaranteed since it uses heuristics that simply selects the best solution after
multiple trials are made. In this paper, we present a hybrid inference
algorithm for NRMM models, which combines the merits of both MCMC and IBHC.
Trees built by IBHC outlines partitions of data, which guides
Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the
nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and
enjoys the fast convergence thanks to the effective proposals guided by trees.
Experiments on both synthetic and real-world datasets demonstrate the benefit
of our method.","['Juho Lee', 'Seungjin Choi']","['stat.ML', 'cs.LG']",2015-11-18 03:16:27+00:00
http://arxiv.org/abs/1511.05634v2,Local entropy as a measure for sampling solutions in Constraint Satisfaction Problems,"We introduce a novel Entropy-driven Monte Carlo (EdMC) strategy to
efficiently sample solutions of random Constraint Satisfaction Problems (CSPs).
First, we extend a recent result that, using a large-deviation analysis, shows
that the geometry of the space of solutions of the Binary Perceptron Learning
Problem (a prototypical CSP), contains regions of very high-density of
solutions. Despite being sub-dominant, these regions can be found by optimizing
a local entropy measure. Building on these results, we construct a fast solver
that relies exclusively on a local entropy estimate, and can be applied to
general CSPs. We describe its performance not only for the Perceptron Learning
Problem but also for the random $K$-Satisfiabilty Problem (another prototypical
CSP with a radically different structure), and show numerically that a simple
zero-temperature Metropolis search in the smooth local entropy landscape can
reach sub-dominant clusters of optimal solutions in a small number of steps,
while standard Simulated Annealing either requires extremely long cooling
procedures or just fails. We also discuss how the EdMC can heuristically be
made even more efficient for the cases we studied.","['Carlo Baldassi', 'Alessandro Ingrosso', 'Carlo Lucibello', 'Luca Saglietti', 'Riccardo Zecchina']","['cond-mat.dis-nn', 'stat.ML', 'G.1.6; I.2.M']",2015-11-18 01:03:59+00:00
http://arxiv.org/abs/1511.05614v3,Model-based Dashboards for Customer Analytics,"Automating the customer analytics process is crucial for companies that
manage distinct customer bases. In such data-rich and dynamic environments,
visualization plays a key role in understanding events of interest. These ideas
have led to the popularity of analytics dashboards, yet academic research has
paid scant attention to these managerial needs. We develop a probabilistic,
nonparametric framework for understanding and predicting individual-level
spending using Gaussian process priors over latent functions that describe
customer spending along calendar time, interpurchase time, and customer
lifetime dimensions. These curves form a dashboard that provides a visual
model-based representation of purchasing dynamics that is easily
comprehensible. The model flexibly and automatically captures the form and
duration of the impact of events that influence spend propensity, even when
such events are unknown a-priori. We illustrate the use of our Gaussian Process
Propensity Model (GPPM) on data from two popular mobile games. We show that the
GPPM generalizes hazard and buy-till-you-die models by incorporating calendar
time dynamics while simultaneously accounting for recency and lifetime effects.
It therefore provides insights about spending propensity beyond those available
from these models. Finally, we show that the GPPM outperforms these benchmarks
both in fitting and forecasting real and simulated spend data.","['Ryan Dew', 'Asim Ansari']","['stat.AP', 'stat.ML']",2015-11-17 23:19:00+00:00
http://arxiv.org/abs/1511.05493v4,Gated Graph Sequence Neural Networks,"Graph-structured data appears frequently in domains including chemistry,
natural language semantics, social networks, and knowledge bases. In this work,
we study feature learning techniques for graph-structured inputs. Our starting
point is previous work on Graph Neural Networks (Scarselli et al., 2009), which
we modify to use gated recurrent units and modern optimization techniques and
then extend to output sequences. The result is a flexible and broadly useful
class of neural network models that has favorable inductive biases relative to
purely sequence-based models (e.g., LSTMs) when the problem is
graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and
graph algorithm learning tasks. We then show it achieves state-of-the-art
performance on a problem from program verification, in which subgraphs need to
be matched to abstract data structures.","['Yujia Li', 'Daniel Tarlow', 'Marc Brockschmidt', 'Richard Zemel']","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",2015-11-17 18:10:12+00:00
http://arxiv.org/abs/1511.05483v1,Accelerating pseudo-marginal Metropolis-Hastings by correlating auxiliary variables,"Pseudo-marginal Metropolis-Hastings (pmMH) is a powerful method for Bayesian
inference in models where the posterior distribution is analytical intractable
or computationally costly to evaluate directly. It operates by introducing
additional auxiliary variables into the model and form an extended target
distribution, which then can be evaluated point-wise. In many cases, the
standard Metropolis-Hastings is then applied to sample from the extended target
and the sought posterior can be obtained by marginalisation. However, in some
implementations this approach suffers from poor mixing as the auxiliary
variables are sampled from an independent proposal. We propose a modification
to the pmMH algorithm in which a Crank-Nicolson (CN) proposal is used instead.
This results in that we introduce a positive correlation in the auxiliary
variables. We investigate how to tune the CN proposal and its impact on the
mixing of the resulting pmMH sampler. The conclusion is that the proposed
modification can have a beneficial effect on both the mixing of the Markov
chain and the computational cost for each iteration of the pmMH algorithm.","['Johan Dahlin', 'Fredrik Lindsten', 'Joel Kronander', 'Thomas B. Schön']","['stat.CO', 'stat.ML']",2015-11-17 17:35:57+00:00
http://arxiv.org/abs/1511.05467v3,Predictive Entropy Search for Multi-objective Bayesian Optimization,"We present PESMO, a Bayesian method for identifying the Pareto set of
multi-objective optimization problems, when the functions are expensive to
evaluate. The central idea of PESMO is to choose evaluation points so as to
maximally reduce the entropy of the posterior distribution over the Pareto set.
Critically, the PESMO multi-objective acquisition function can be decomposed as
a sum of objective-specific acquisition functions, which enables the algorithm
to be used in \emph{decoupled} scenarios in which the objectives can be
evaluated separately and perhaps with different costs. This decoupling
capability also makes it possible to identify difficult objectives that require
more evaluations. PESMO also offers gains in efficiency, as its cost scales
linearly with the number of objectives, in comparison to the exponential cost
of other methods. We compare PESMO with other related methods for
multi-objective Bayesian optimization on synthetic and real-world problems. The
results show that PESMO produces better recommendations with a smaller number
of evaluations of the objectives, and that a decoupled evaluation can lead to
improvements in performance, particularly when the number of objectives is
large.","['Daniel Hernández-Lobato', 'José Miguel Hernández-Lobato', 'Amar Shah', 'Ryan P. Adams']",['stat.ML'],2015-11-17 16:59:33+00:00
http://arxiv.org/abs/1511.05464v1,Extending Gossip Algorithms to Distributed Estimation of U-Statistics,"Efficient and robust algorithms for decentralized estimation in networks are
essential to many distributed systems. Whereas distributed estimation of sample
mean statistics has been the subject of a good deal of attention, computation
of $U$-statistics, relying on more expensive averaging over pairs of
observations, is a less investigated area. Yet, such data functionals are
essential to describe global properties of a statistical population, with
important examples including Area Under the Curve, empirical variance, Gini
mean difference and within-cluster point scatter. This paper proposes new
synchronous and asynchronous randomized gossip algorithms which simultaneously
propagate data across the network and maintain local estimates of the
$U$-statistic of interest. We establish convergence rate bounds of $O(1/t)$ and
$O(\log t / t)$ for the synchronous and asynchronous cases respectively, where
$t$ is the number of iterations, with explicit data and network dependent
terms. Beyond favorable comparisons in terms of rate analysis, numerical
experiments provide empirical evidence the proposed algorithms surpasses the
previously introduced approach.","['Igor Colin', 'Aurélien Bellet', 'Joseph Salmon', 'Stéphan Clémençon']","['stat.ML', 'cs.DC', 'cs.LG', 'cs.SY', 'stat.CO', '68Uxx, 62J15, 68Q32, 62-04,']",2015-11-17 16:49:52+00:00
http://arxiv.org/abs/1511.05440v6,Deep multi-scale video prediction beyond mean square error,"Learning to predict future images from a video sequence involves the
construction of an internal representation that models the image evolution
accurately, and therefore, to some degree, its content and dynamics. This is
why pixel-space video prediction may be viewed as a promising avenue for
unsupervised feature learning. In addition, while optical flow has been a very
studied problem in computer vision for a long time, future frame prediction is
rarely approached. Still, many vision applications could benefit from the
knowledge of the next frames of videos, that does not require the complexity of
tracking every pixel trajectories. In this work, we train a convolutional
network to generate future frames given an input sequence. To deal with the
inherently blurry predictions obtained from the standard Mean Squared Error
(MSE) loss function, we propose three different and complementary feature
learning strategies: a multi-scale architecture, an adversarial training
method, and an image gradient difference loss function. We compare our
predictions to different published results based on recurrent neural networks
on the UCF101 dataset","['Michael Mathieu', 'Camille Couprie', 'Yann LeCun']","['cs.LG', 'cs.CV', 'stat.ML']",2015-11-17 15:36:32+00:00
http://arxiv.org/abs/1511.05432v3,Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization,"We propose a general framework for increasing local stability of Artificial
Neural Nets (ANNs) using Robust Optimization (RO). We achieve this through an
alternating minimization-maximization procedure, in which the loss of the
network is minimized over perturbed examples that are generated at each
parameter update. We show that adversarial training of ANNs is in fact
robustification of the network optimization, and that our proposed framework
generalizes previous approaches for increasing local stability of ANNs.
Experimental results reveal that our approach increases the robustness of the
network to existing adversarial examples, while making it harder to generate
new ones. Furthermore, our algorithm improves the accuracy of the network also
on the original test data.","['Uri Shaham', 'Yutaro Yamada', 'Sahand Negahban']","['stat.ML', 'cs.LG', 'cs.NE']",2015-11-17 15:14:57+00:00
http://arxiv.org/abs/1511.05424v2,Sacrificing information for the greater good: how to select photometric bands for optimal accuracy,"Large-scale surveys make huge amounts of photometric data available. Because
of the sheer amount of objects, spectral data cannot be obtained for all of
them. Therefore it is important to devise techniques for reliably estimating
physical properties of objects from photometric information alone. These
estimates are needed to automatically identify interesting objects worth a
follow-up investigation as well as to produce the required data for a
statistical analysis of the space covered by a survey. We argue that machine
learning techniques are suitable to compute these estimates accurately and
efficiently. This study promotes a feature selection algorithm, which selects
the most informative magnitudes and colours for a given task of estimating
physical quantities from photometric data alone. Using k nearest neighbours
regression, a well-known non-parametric machine learning method, we show that
using the found features significantly increases the accuracy of the
estimations compared to using standard features and standard methods. We
illustrate the usefulness of the approach by estimating specific star formation
rates (sSFRs) and redshifts (photo-z's) using only the broad-band photometry
from the Sloan Digital Sky Survey (SDSS). For estimating sSFRs, we demonstrate
that our method produces better estimates than traditional spectral energy
distribution (SED) fitting. For estimating photo-z's, we show that our method
produces more accurate photo-z's than the method employed by SDSS. The study
highlights the general importance of performing proper model selection to
improve the results of machine learning systems and how feature selection can
provide insights into the predictive relevance of particular input features.","['Kristoffer Stensbo-Smidt', 'Fabian Gieseke', 'Christian Igel', 'Andrew Zirm', 'Kim Steenstrup Pedersen']","['astro-ph.IM', 'stat.ML']",2015-11-17 14:52:40+00:00
http://arxiv.org/abs/1511.05392v3,Learning the Dimensionality of Word Embeddings,"We describe a method for learning word embeddings with data-dependent
dimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic
Dimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of
Mikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is
made dynamic by employing techniques used by Cote & Larochelle (2016) to define
an RBM with an infinite number of hidden units. We show qualitatively and
quantitatively that SD-SG and SD-CBOW are competitive with their
fixed-dimension counterparts while providing a distribution over embedding
dimensionalities, which offers a window into how semantics distribute across
dimensions.","['Eric Nalisnick', 'Sachin Ravi']","['stat.ML', 'cs.CL', 'cs.LG']",2015-11-17 13:28:55+00:00
http://arxiv.org/abs/1511.05385v1,Bayesian Optimization with Dimension Scheduling: Application to Biological Systems,"Bayesian Optimization (BO) is a data-efficient method for global black-box
optimization of an expensive-to-evaluate fitness function. BO typically assumes
that computation cost of BO is cheap, but experiments are time consuming or
costly. In practice, this allows us to optimize ten or fewer critical
parameters in up to 1,000 experiments. But experiments may be less expensive
than BO methods assume: In some simulation models, we may be able to conduct
multiple thousands of experiments in a few hours, and the computational burden
of BO is no longer negligible compared to experimentation time. To address this
challenge we introduce a new Dimension Scheduling Algorithm (DSA), which
reduces the computational burden of BO for many experiments. The key idea is
that DSA optimizes the fitness function only along a small set of dimensions at
each iteration. This DSA strategy (1) reduces the necessary computation time,
(2) finds good solutions faster than the traditional BO method, and (3) can be
parallelized straightforwardly. We evaluate the DSA in the context of
optimizing parameters of dynamic models of microalgae metabolism and show
faster convergence than traditional BO.","['Doniyor Ulmasov', 'Caroline Baroukh', 'Benoit Chachuat', 'Marc Peter Deisenroth', 'Ruth Misener']","['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']",2015-11-17 13:08:10+00:00
