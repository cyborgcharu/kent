id,title,abstract,authors,categories,date
http://arxiv.org/abs/1703.07473v1,Episode-Based Active Learning with Bayesian Neural Networks,"We investigate different strategies for active learning with Bayesian deep
neural networks. We focus our analysis on scenarios where new, unlabeled data
is obtained episodically, such as commonly encountered in mobile robotics
applications. An evaluation of different strategies for acquisition, updating,
and final training on the CIFAR-10 dataset shows that incremental network
updates with final training on the accumulated acquisition set are essential
for best performance, while limiting the amount of required human labeling
labor.","['Feras Dayoub', 'Niko Sünderhauf', 'Peter Corke']","['cs.CV', 'cs.LG', 'stat.ML']",2017-03-21 23:56:51+00:00
http://arxiv.org/abs/1703.07370v4,"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models","Learning in models with discrete latent variables is challenging due to high
variance gradient estimators. Generally, approaches have relied on control
variates to reduce the variance of the REINFORCE estimator. Recent work (Jang
et al. 2016, Maddison et al. 2016) has taken a different approach, introducing
a continuous relaxation of discrete variables to produce low-variance, but
biased, gradient estimates. In this work, we combine the two approaches through
a novel control variate that produces low-variance, \emph{unbiased} gradient
estimates. Then, we introduce a modification to the continuous relaxation and
show that the tightness of the relaxation can be adapted online, removing it as
a hyperparameter. We show state-of-the-art variance reduction on several
benchmark generative modeling tasks, generally leading to faster convergence to
a better final log-likelihood.","['George Tucker', 'Andriy Mnih', 'Chris J. Maddison', 'Dieterich Lawson', 'Jascha Sohl-Dickstein']","['cs.LG', 'stat.ML']",2017-03-21 18:05:31+00:00
http://arxiv.org/abs/1703.07355v1,An Army of Me: Sockpuppets in Online Discussion Communities,"In online discussion communities, users can interact and share information
and opinions on a wide variety of topics. However, some users may create
multiple identities, or sockpuppets, and engage in undesired behavior by
deceiving others or manipulating discussions. In this work, we study
sockpuppetry across nine discussion communities, and show that sockpuppets
differ from ordinary users in terms of their posting behavior, linguistic
traits, as well as social network structure. Sockpuppets tend to start fewer
discussions, write shorter posts, use more personal pronouns such as ""I"", and
have more clustered ego-networks. Further, pairs of sockpuppets controlled by
the same individual are more likely to interact on the same discussion at the
same time than pairs of ordinary users. Our analysis suggests a taxonomy of
deceptive behavior in discussion communities. Pairs of sockpuppets can vary in
their deceptiveness, i.e., whether they pretend to be different users, or their
supportiveness, i.e., if they support arguments of other sockpuppets controlled
by the same user. We apply these findings to a series of prediction tasks,
notably, to identify whether a pair of accounts belongs to the same underlying
user or not. Altogether, this work presents a data-driven view of deception in
online discussion communities and paves the way towards the automatic detection
of sockpuppets.","['Srijan Kumar', 'Justin Cheng', 'Jure Leskovec', 'V. S. Subrahmanian']","['cs.SI', 'cs.CY', 'physics.soc-ph', 'stat.AP', 'stat.ML']",2017-03-21 18:00:02+00:00
http://arxiv.org/abs/1703.07345v2,On The Projection Operator to A Three-view Cardinality Constrained Set,"The cardinality constraint is an intrinsic way to restrict the solution
structure in many domains, for example, sparse learning, feature selection, and
compressed sensing. To solve a cardinality constrained problem, the key
challenge is to solve the projection onto the cardinality constraint set, which
is NP-hard in general when there exist multiple overlapped cardinality
constraints. In this paper, we consider the scenario where the overlapped
cardinality constraints satisfy a Three-view Cardinality Structure (TVCS),
which reflects the natural restriction in many applications, such as
identification of gene regulatory networks and task-worker assignment problem.
We cast the projection into a linear programming, and show that for TVCS, the
vertex solution of this linear programming is the solution for the original
projection problem. We further prove that such solution can be found with the
complexity proportional to the number of variables and constraints. We finally
use synthetic experiments and two interesting applications in bioinformatics
and crowdsourcing to validate the proposed TVCS model and method.","['Haichuan Yang', 'Shupeng Gui', 'Chuyang Ke', 'Daniel Stefankovic', 'Ryohei Fujimaki', 'Ji Liu']","['cs.LG', 'stat.ML']",2017-03-21 17:58:03+00:00
http://arxiv.org/abs/1703.07305v1,Targeting Bayes factors with direct-path non-equilibrium thermodynamic integration,"Thermodynamic integration (TI) for computing marginal likelihoods is based on
an inverse annealing path from the prior to the posterior distribution. In many
cases, the resulting estimator suffers from high variability, which
particularly stems from the prior regime. When comparing complex models with
differences in a comparatively small number of parameters, intrinsic errors
from sampling fluctuations may outweigh the differences in the log marginal
likelihood estimates. In the present article, we propose a thermodynamic
integration scheme that directly targets the log Bayes factor. The method is
based on a modified annealing path between the posterior distributions of the
two models compared, which systematically avoids the high variance prior
regime. We combine this scheme with the concept of non-equilibrium TI to
minimise discretisation errors from numerical integration. Results obtained on
Bayesian regression models applied to standard benchmark data, and a complex
hierarchical model applied to biopathway inference, demonstrate a significant
reduction in estimator variance over state-of-the-art TI methods.","['Marco Grzegorczyk', 'Andrej Aderhold', 'Dirk Husmeier']","['stat.ME', 'stat.ML']",2017-03-21 16:39:28+00:00
http://arxiv.org/abs/1704.08306v1,A Digital Neuromorphic Architecture Efficiently Facilitating Complex Synaptic Response Functions Applied to Liquid State Machines,"Information in neural networks is represented as weighted connections, or
synapses, between neurons. This poses a problem as the primary computational
bottleneck for neural networks is the vector-matrix multiply when inputs are
multiplied by the neural network weights. Conventional processing architectures
are not well suited for simulating neural networks, often requiring large
amounts of energy and time. Additionally, synapses in biological neural
networks are not binary connections, but exhibit a nonlinear response function
as neurotransmitters are emitted and diffuse between neurons. Inspired by
neuroscience principles, we present a digital neuromorphic architecture, the
Spiking Temporal Processing Unit (STPU), capable of modeling arbitrary complex
synaptic response functions without requiring additional hardware components.
We consider the paradigm of spiking neurons with temporally coded information
as opposed to non-spiking rate coded neurons used in most neural networks. In
this paradigm we examine liquid state machines applied to speech recognition
and show how a liquid state machine with temporal dynamics maps onto the
STPU-demonstrating the flexibility and efficiency of the STPU for instantiating
neural algorithms.","['Michael R. Smith', 'Aaron J. Hill', 'Kristofor D. Carlson', 'Craig M. Vineyard', 'Jonathon Donaldson', 'David R. Follett', 'Pamela L. Follett', 'John H. Naegle', 'Conrad D. James', 'James B. Aimone']","['q-bio.NC', 'cs.NE', 'stat.ML']",2017-03-21 16:12:31+00:00
http://arxiv.org/abs/1703.07285v2,From safe screening rules to working sets for faster Lasso-type solvers,"Convex sparsity-promoting regularizations are ubiquitous in modern
statistical learning. By construction, they yield solutions with few non-zero
coefficients, which correspond to saturated constraints in the dual
optimization formulation. Working set (WS) strategies are generic optimization
techniques that consist in solving simpler problems that only consider a subset
of constraints, whose indices form the WS. Working set methods therefore
involve two nested iterations: the outer loop corresponds to the definition of
the WS and the inner loop calls a solver for the subproblems. For the Lasso
estimator a WS is a set of features, while for a Group Lasso it refers to a set
of groups. In practice, WS are generally small in this context so the
associated feature Gram matrix can fit in memory. Here we show that the
Gauss-Southwell rule (a greedy strategy for block coordinate descent
techniques) leads to fast solvers in this case. Combined with a working set
strategy based on an aggressive use of so-called Gap Safe screening rules, we
propose a solver achieving state-of-the-art performance on sparse learning
problems. Results are presented on Lasso and multi-task Lasso estimators.","['Mathurin Massias', 'Alexandre Gramfort', 'Joseph Salmon']","['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",2017-03-21 15:42:38+00:00
http://arxiv.org/abs/1703.07255v2,ZM-Net: Real-time Zero-shot Image Manipulation Network,"Many problems in image processing and computer vision (e.g. colorization,
style transfer) can be posed as 'manipulating' an input image into a
corresponding output image given a user-specified guiding signal. A holy-grail
solution towards generic image manipulation should be able to efficiently alter
an input image with any personalized signals (even signals unseen during
training), such as diverse paintings and arbitrary descriptive attributes.
However, existing methods are either inefficient to simultaneously process
multiple signals (let alone generalize to unseen signals), or unable to handle
signals from other modalities. In this paper, we make the first attempt to
address the zero-shot image manipulation task. We cast this problem as
manipulating an input image according to a parametric model whose key
parameters can be conditionally generated from any guiding signal (even unseen
ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a
fully-differentiable architecture that jointly optimizes an
image-transformation network (TNet) and a parameter network (PNet). The PNet
learns to generate key transformation parameters for the TNet given any guiding
signal while the TNet performs fast zero-shot image manipulation according to
both signal-dependent parameters from the PNet and signal-invariant parameters
from the TNet itself. Extensive experiments show that our ZM-Net can perform
high-quality image manipulation conditioned on different forms of guiding
signals (e.g. style images and attributes) in real-time (tens of milliseconds
per image) even for unseen signals. Moreover, a large-scale style dataset with
over 20,000 style images is also constructed to promote further research.","['Hao Wang', 'Xiaodan Liang', 'Hao Zhang', 'Dit-Yan Yeung', 'Eric P. Xing']","['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'stat.ML']",2017-03-21 15:01:59+00:00
http://arxiv.org/abs/1703.07198v1,Overcoming model simplifications when quantifying predictive uncertainty,"It is generally accepted that all models are wrong -- the difficulty is
determining which are useful. Here, a useful model is considered as one that is
capable of combining data and expert knowledge, through an inversion or
calibration process, to adequately characterize the uncertainty in predictions
of interest. This paper derives conditions that specify which simplified models
are useful and how they should be calibrated. To start, the notion of an
optimal simplification is defined. This relates the model simplifications to
the nature of the data and predictions, and determines when a standard
probabilistic calibration scheme is capable of accurately characterizing
uncertainty. Furthermore, two additional conditions are defined for suboptimal
models that determine when the simplifications can be safely ignored. The first
allows a suboptimally simplified model to be used in a way that replicates the
performance of an optimal model. This is achieved through the judicial
selection of a prior term for the calibration process that explicitly includes
the nature of the data, predictions and modelling simplifications. The second
considers the dependency structure between the predictions and the available
data to gain insights into when the simplifications can be overcome by using
the right calibration data. Furthermore, the derived conditions are related to
the commonly used calibration schemes based on Tikhonov and subspace
regularization. To allow concrete insights to be obtained, the analysis is
performed under a linear expansion of the model equations and where the
predictive uncertainty is characterized via second order moments only.","['George M. Mathews', 'John Vial']","['stat.ML', 'math.PR', 'physics.comp-ph', 'physics.geo-ph', 'stat.ME', '62F15, 62C10, 68U05, 93E12, 93B11, 62P12']",2017-03-21 13:02:35+00:00
http://arxiv.org/abs/1703.07169v1,A Deterministic Global Optimization Method for Variational Inference,"Variational inference methods for latent variable statistical models have
gained popularity because they are relatively fast, can handle large data sets,
and have deterministic convergence guarantees. However, in practice it is
unclear whether the fixed point identified by the variational inference
algorithm is a local or a global optimum. Here, we propose a method for
constructing iterative optimization algorithms for variational inference
problems that are guaranteed to converge to the $\epsilon$-global variational
lower bound on the log-likelihood. We derive inference algorithms for two
variational approximations to a standard Bayesian Gaussian mixture model
(BGMM). We present a minimal data set for empirically testing convergence and
show that a variational inference algorithm frequently converges to a local
optimum while our algorithm always converges to the globally optimal
variational lower bound. We characterize the loss incurred by choosing a
non-optimal variational approximation distribution suggesting that selection of
the approximating variational distribution deserves as much attention as the
selection of the original statistical model for a given data set.","['Hachem Saddiki', 'Andrew C. Trapp', 'Patrick Flaherty']","['stat.ME', 'stat.ML']",2017-03-21 12:33:19+00:00
http://arxiv.org/abs/1703.07131v1,Knowledge distillation using unlabeled mismatched images,"Current approaches for Knowledge Distillation (KD) either directly use
training data or sample from the training data distribution. In this paper, we
demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for
image classification networks. For illustration, we consider scenarios where
this is a complete absence of training data, or mismatched stimulus has to be
used for augmenting a small amount of training data. We demonstrate that
stimulus complexity is a key factor for distillation's good performance. Our
examples include use of various datasets for stimulating MNIST and CIFAR
teachers.","['Mandar Kulkarni', 'Kalpesh Patil', 'Shirish Karande']","['cs.CV', 'cs.LG', 'stat.ML']",2017-03-21 10:34:59+00:00
http://arxiv.org/abs/1703.07056v1,Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling Based on Optimality Violations,"We study primal-dual type stochastic optimization algorithms with non-uniform
sampling. Our main theoretical contribution in this paper is to present a
convergence analysis of Stochastic Primal Dual Coordinate (SPDC) Method with
arbitrary sampling. Based on this theoretical framework, we propose Optimality
Violation-based Sampling SPDC (ovsSPDC), a non-uniform sampling method based on
Optimality Violation. We also propose two efficient heuristic variants of
ovsSPDC called ovsSDPC+ and ovsSDPC++. Through intensive numerical experiments,
we demonstrate that the proposed method and its variants are faster than other
state-of-the-art primal-dual type stochastic optimization methods.","['Atsushi Shibagaki', 'Ichiro Takeuchi']","['stat.ML', 'cs.LG', 'math.OC']",2017-03-21 05:08:33+00:00
http://arxiv.org/abs/1703.07047v3,High-Resolution Breast Cancer Screening with Multi-View Deep Convolutional Neural Networks,"Advances in deep learning for natural images have prompted a surge of
interest in applying similar techniques to medical images. The majority of the
initial attempts focused on replacing the input of a deep convolutional neural
network with a medical image, which does not take into consideration the
fundamental differences between these two types of images. Specifically, fine
details are necessary for detection in medical images, unlike in natural images
where coarse structures matter most. This difference makes it inadequate to use
the existing network architectures developed for natural images, because they
work on heavily downscaled images to reduce the memory requirements. This hides
details necessary to make accurate predictions. Additionally, a single exam in
medical imaging often comes with a set of views which must be fused in order to
reach a correct conclusion. In our work, we propose to use a multi-view deep
convolutional neural network that handles a set of high-resolution medical
images. We evaluate it on large-scale mammography-based breast cancer screening
(BI-RADS prediction) using 886,000 images. We focus on investigating the impact
of the training set size and image size on the prediction accuracy. Our results
highlight that performance increases with the size of training set, and that
the best performance can only be achieved using the original resolution. In the
reader study, performed on a random subset of the test set, we confirmed the
efficacy of our model, which achieved performance comparable to a committee of
radiologists when presented with the same data.","['Krzysztof J. Geras', 'Stacey Wolfson', 'Yiqiu Shen', 'Nan Wu', 'S. Gene Kim', 'Eric Kim', 'Laura Heacock', 'Ujas Parikh', 'Linda Moy', 'Kyunghyun Cho']","['cs.CV', 'cs.LG', 'stat.ML']",2017-03-21 04:11:13+00:00
http://arxiv.org/abs/1703.07027v2,Nonparametric Variational Auto-encoders for Hierarchical Representation Learning,"The recently developed variational autoencoders (VAEs) have proved to be an
effective confluence of the rich representational power of neural networks with
Bayesian methods. However, most work on VAEs use a rather simple prior over the
latent variables such as standard normal distribution, thereby restricting its
applications to relatively simple phenomena. In this work, we propose
hierarchical nonparametric variational autoencoders, which combines
tree-structured Bayesian nonparametric priors with VAEs, to enable infinite
flexibility of the latent representation space. Both the neural parameters and
Bayesian priors are learned jointly using tailored variational inference. The
resulting model induces a hierarchical structure of latent semantic concepts
underlying the data corpus, and infers accurate representations of data
instances. We apply our model in video representation learning. Our method is
able to discover highly interpretable activity hierarchies, and obtain improved
clustering accuracy and generalization capacity based on the learned rich
representations.","['Prasoon Goyal', 'Zhiting Hu', 'Xiaodan Liang', 'Chenyu Wang', 'Eric Xing']","['cs.LG', 'stat.ML']",2017-03-21 02:08:05+00:00
http://arxiv.org/abs/1703.07026v2,Cross-modal Deep Metric Learning with Multi-task Regularization,"DNN-based cross-modal retrieval has become a research hotspot, by which users
can search results across various modalities like image and text. However,
existing methods mainly focus on the pairwise correlation and reconstruction
error of labeled data. They ignore the semantically similar and dissimilar
constraints between different modalities, and cannot take advantage of
unlabeled data. This paper proposes Cross-modal Deep Metric Learning with
Multi-task Regularization (CDMLMR), which integrates quadruplet ranking loss
and semi-supervised contrastive loss for modeling cross-modal semantic
similarity in a unified multi-task learning architecture. The quadruplet
ranking loss can model the semantically similar and dissimilar constraints to
preserve cross-modal relative similarity ranking information. The
semi-supervised contrastive loss is able to maximize the semantic similarity on
both labeled and unlabeled data. Compared to the existing methods, CDMLMR
exploits not only the similarity ranking information but also unlabeled
cross-modal data, and thus boosts cross-modal retrieval accuracy.","['Xin Huang', 'Yuxin Peng']","['cs.LG', 'cs.CV', 'stat.ML']",2017-03-21 02:04:30+00:00
http://arxiv.org/abs/1703.06990v1,Metalearning for Feature Selection,"A general formulation of optimization problems in which various candidate
solutions may use different feature-sets is presented, encompassing supervised
classification, automated program learning and other cases. A novel
characterization of the concept of a ""good quality feature"" for such an
optimization problem is provided; and a proposal regarding the integration of
quality based feature selection into metalearning is suggested, wherein the
quality of a feature for a problem is estimated using knowledge about related
features in the context of related problems. Results are presented regarding
extensive testing of this ""feature metalearning"" approach on supervised text
classification problems; it is demonstrated that, in this context, feature
metalearning can provide significant and sometimes dramatic speedup over
standard feature selection heuristics.","['Ben Goertzel', 'Nil Geisweiller', 'Chris Poulin']","['cs.LG', 'stat.ML']",2017-03-20 22:32:36+00:00
http://arxiv.org/abs/1703.06975v1,Learning to Generate Samples from Noise through Infusion Training,"In this work, we investigate a novel training procedure to learn a generative
model as the transition operator of a Markov chain, such that, when applied
repeatedly on an unstructured random noise sample, it will denoise it into a
sample that matches the target distribution from the training set. The novel
training procedure to learn this progressive denoising operation involves
sampling from a slightly different chain than the model chain used for
generation in the absence of a denoising target. In the training chain we
infuse information from the training target example that we would like the
chains to reach with a high probability. The thus learned transition operator
is able to produce quality and varied samples in a small number of steps.
Experiments show competitive results compared to the samples generated with a
basic Generative Adversarial Net","['Florian Bordes', 'Sina Honari', 'Pascal Vincent']","['stat.ML', 'cs.LG']",2017-03-20 21:29:18+00:00
http://arxiv.org/abs/1703.06934v3,Ensemble representation learning: an analysis of fitness and survival for wrapper-based genetic programming methods,"Recently we proposed a general, ensemble-based feature engineering wrapper
(FEW) that was paired with a number of machine learning methods to solve
regression problems. Here, we adapt FEW for supervised classification and
perform a thorough analysis of fitness and survival methods within this
framework. Our tests demonstrate that two fitness metrics, one introduced as an
adaptation of the silhouette score, outperform the more commonly used Fisher
criterion. We analyze survival methods and demonstrate that $\epsilon$-lexicase
survival works best across our test problems, followed by random survival which
outperforms both tournament and deterministic crowding. We conduct a benchmark
comparison to several classification methods using a large set of problems and
show that FEW can improve the best classifier performance in several cases. We
show that FEW generates consistent, meaningful features for a biomedical
problem with different ML pairings.","['William La Cava', 'Jason H. Moore']","['cs.NE', 'cs.LG', 'stat.ML']",2017-03-20 19:26:00+00:00
http://arxiv.org/abs/1703.06891v3,Dance Dance Convolution,"Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players
perform steps on a dance platform in synchronization with music as directed by
on-screen step charts. While many step charts are available in standardized
packs, players may grow tired of existing charts, or wish to dance to a song
for which no chart exists. We introduce the task of learning to choreograph.
Given a raw audio track, the goal is to produce a new step chart. This task
decomposes naturally into two subtasks: deciding when to place steps and
deciding which steps to select. For the step placement task, we combine
recurrent and convolutional neural networks to ingest spectrograms of low-level
audio features to predict steps, conditioned on chart difficulty. For step
selection, we present a conditional LSTM generative model that substantially
outperforms n-gram and fixed-window approaches.","['Chris Donahue', 'Zachary C. Lipton', 'Julian McAuley']","['cs.LG', 'cs.MM', 'cs.NE', 'cs.SD', 'stat.ML']",2017-03-20 18:00:13+00:00
http://arxiv.org/abs/1703.06857v2,On the Limitation of Convolutional Neural Networks in Recognizing Negative Images,"Convolutional Neural Networks (CNNs) have achieved state-of-the-art
performance on a variety of computer vision tasks, particularly visual
classification problems, where new algorithms reported to achieve or even
surpass the human performance. In this paper, we examine whether CNNs are
capable of learning the semantics of training data. To this end, we evaluate
CNNs on negative images, since they share the same structure and semantics as
regular images and humans can classify them correctly. Our experimental results
indicate that when training on regular images and testing on negative images,
the model accuracy is significantly lower than when it is tested on regular
images. This leads us to the conjecture that current training methods do not
effectively train models to generalize the concepts. We then introduce the
notion of semantic adversarial examples - transformed inputs that semantically
represent the same objects, but the model does not classify them correctly -
and present negative images as one class of such inputs.","['Hossein Hosseini', 'Baicen Xiao', 'Mayoore Jaiswal', 'Radha Poovendran']","['cs.CV', 'cs.LG', 'stat.ML']",2017-03-20 17:21:19+00:00
http://arxiv.org/abs/1703.06856v3,Counterfactual Fairness,"Machine learning can impact people with legal or ethical consequences when it
is used to automate decisions in areas such as insurance, lending, hiring, and
predictive policing. In many of these scenarios, previous decisions have been
made that are unfairly biased against certain subpopulations, for example those
of a particular race, gender, or sexual orientation. Since this past data may
be biased, machine learning predictors must account for this to avoid
perpetuating or creating discriminatory practices. In this paper, we develop a
framework for modeling fairness using tools from causal inference. Our
definition of counterfactual fairness captures the intuition that a decision is
fair towards an individual if it is the same in (a) the actual world and (b) a
counterfactual world where the individual belonged to a different demographic
group. We demonstrate our framework on a real-world problem of fair prediction
of success in law school.","['Matt J. Kusner', 'Joshua R. Loftus', 'Chris Russell', 'Ricardo Silva']","['stat.ML', 'cs.CY', 'cs.LG']",2017-03-20 17:18:57+00:00
http://arxiv.org/abs/1703.06807v2,Guaranteed Sufficient Decrease for Variance Reduced Stochastic Gradient Descent,"In this paper, we propose a novel sufficient decrease technique for variance
reduced stochastic gradient descent methods such as SAG, SVRG and SAGA. In
order to make sufficient decrease for stochastic optimization, we design a new
sufficient decrease criterion, which yields sufficient decrease versions of
variance reduction algorithms such as SVRG-SD and SAGA-SD as a byproduct. We
introduce a coefficient to scale current iterate and satisfy the sufficient
decrease property, which takes the decisions to shrink, expand or move in the
opposite direction, and then give two specific update rules of the coefficient
for Lasso and ridge regression. Moreover, we analyze the convergence properties
of our algorithms for strongly convex problems, which show that both of our
algorithms attain linear convergence rates. We also provide the convergence
guarantees of our algorithms for non-strongly convex problems. Our experimental
results further verify that our algorithms achieve significantly better
performance than their counterparts.","['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Kelvin Kai Wing Ng', 'Yuichi Yoshida']","['cs.LG', 'math.OC', 'stat.ML']",2017-03-20 15:43:10+00:00
http://arxiv.org/abs/1703.06777v1,On the Use of Default Parameter Settings in the Empirical Evaluation of Classification Algorithms,"We demonstrate that, for a range of state-of-the-art machine learning
algorithms, the differences in generalisation performance obtained using
default parameter settings and using parameters tuned via cross-validation can
be similar in magnitude to the differences in performance observed between
state-of-the-art and uncompetitive learning systems. This means that fair and
rigorous evaluation of new learning algorithms requires performance comparison
against benchmark methods with best-practice model selection procedures, rather
than using default parameter settings. We investigate the sensitivity of three
key machine learning algorithms (support vector machine, random forest and
rotation forest) to their default parameter settings, and provide guidance on
determining sensible default parameter values for implementations of these
algorithms. We also conduct an experimental comparison of these three
algorithms on 121 classification problems and find that, perhaps surprisingly,
rotation forest is significantly more accurate on average than both random
forest and a support vector machine.","['Anthony Bagnall', 'Gavin C. Cawley']","['cs.LG', 'stat.ML']",2017-03-20 14:42:27+00:00
http://arxiv.org/abs/1703.06749v2,Efficient variational Bayesian neural network ensembles for outlier detection,"In this work we perform outlier detection using ensembles of neural networks
obtained by variational approximation of the posterior in a Bayesian neural
network setting. The variational parameters are obtained by sampling from the
true posterior by gradient descent. We show our outlier detection results are
comparable to those obtained using other efficient ensembling methods.","['Nick Pawlowski', 'Miguel Jaques', 'Ben Glocker']","['stat.ML', 'cs.LG']",2017-03-20 14:02:11+00:00
http://arxiv.org/abs/1703.06700v1,Independence clustering (without a matrix),"The independence clustering problem is considered in the following
formulation: given a set $S$ of random variables, it is required to find the
finest partitioning $\{U_1,\dots,U_k\}$ of $S$ into clusters such that the
clusters $U_1,\dots,U_k$ are mutually independent. Since mutual independence is
the target, pairwise similarity measurements are of no use, and thus
traditional clustering algorithms are inapplicable. The distribution of the
random variables in $S$ is, in general, unknown, but a sample is available.
Thus, the problem is cast in terms of time series. Two forms of sampling are
considered: i.i.d.\ and stationary time series, with the main emphasis being on
the latter, more general, case. A consistent, computationally tractable
algorithm for each of the settings is proposed, and a number of open directions
for further research are outlined.",['Daniil Ryabko'],"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2017-03-20 12:09:53+00:00
http://arxiv.org/abs/1703.06692v3,QMDP-Net: Deep Learning for Planning under Partial Observability,"This paper introduces the QMDP-net, a neural network architecture for
planning under partial observability. The QMDP-net combines the strengths of
model-free learning and model-based planning. It is a recurrent policy network,
but it represents a policy for a parameterized set of tasks by connecting a
model with a planning algorithm that solves the model, thus embedding the
solution structure of planning in a network learning architecture. The QMDP-net
is fully differentiable and allows for end-to-end training. We train a QMDP-net
on different tasks so that it can generalize to new ones in the parameterized
task set and ""transfer"" to other similar tasks beyond the set. In preliminary
experiments, QMDP-net showed strong performance on several robotic tasks in
simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it
sometimes outperforms the QMDP algorithm in the experiments, as a result of
end-to-end learning.","['Peter Karkus', 'David Hsu', 'Wee Sun Lee']","['cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']",2017-03-20 11:44:00+00:00
http://arxiv.org/abs/1703.06686v5,Copula Index for Detecting Dependence and Monotonicity between Stochastic Signals,"This paper introduces a nonparametric copula-based index for detecting the
strength and monotonicity structure of linear and nonlinear statistical
dependence between pairs of random variables or stochastic signals. Our index,
termed Copula Index for Detecting Dependence and Monotonicity (CIM), satisfies
several desirable properties of measures of association, including Renyi's
properties, the data processing inequality (DPI), and consequently
self-equitability. Synthetic data simulations reveal that the statistical power
of CIM compares favorably to other state-of-the-art measures of association
that are proven to satisfy the DPI. Simulation results with real-world data
reveal the CIM's unique ability to detect the monotonicity structure among
stochastic signals to find interesting dependencies in large datasets.
Additionally, simulations show that the CIM shows favorable performance to
estimators of mutual information when discovering Markov network structure.","['Kiran Karra', 'Lamine Mili']","['stat.ML', 'q-bio.QM']",2017-03-20 11:26:04+00:00
http://arxiv.org/abs/1703.06537v1,A Controlled Set-Up Experiment to Establish Personalized Baselines for Real-Life Emotion Recognition,"We design, conduct and present the results of a highly personalized baseline
emotion recognition experiment, which aims to set reliable ground-truth
estimates for the subject's emotional state for real-life prediction under
similar conditions using a small number of physiological sensors. We also
propose an adaptive stimuli-selection mechanism that would use the user's
feedback as guide for future stimuli selection in the controlled-setup
experiment and generate optimal ground-truth personalized sessions
systematically. Initial results are very promising (85% accuracy) and variable
importance analysis shows that only a few features, which are easy-to-implement
in portable devices, would suffice to predict the subject's emotional state.","['Varvara Kollia', 'Noureddine Tayebi']","['stat.ML', 'cs.HC']",2017-03-19 23:28:39+00:00
http://arxiv.org/abs/1703.06528v1,Universal Consistency and Robustness of Localized Support Vector Machines,"The massive amount of available data potentially used to discover patters in
machine learning is a challenge for kernel based algorithms with respect to
runtime and storage capacities. Local approaches might help to relieve these
issues. From a statistical point of view local approaches allow additionally to
deal with different structures in the data in different ways. This paper
analyses properties of localized kernel based, non-parametric statistical
machine learning methods, in particular of support vector machines (SVMs) and
methods close to them. We will show there that locally learnt kernel methods
are universal consistent. Furthermore, we give an upper bound for the maxbias
in order to show statistical robustness of the proposed method.",['Florian Dumpert'],"['stat.ML', '62G08, 62G20, 62G35']",2017-03-19 22:23:01+00:00
http://arxiv.org/abs/1703.06513v1,Bernoulli Rank-$1$ Bandits for Click Feedback,"The probability that a user will click a search result depends both on its
relevance and its position on the results page. The position based model
explains this behavior by ascribing to every item an attraction probability,
and to every position an examination probability. To be clicked, a result must
be both attractive and examined. The probabilities of an item-position pair
being clicked thus form the entries of a rank-$1$ matrix. We propose the
learning problem of a Bernoulli rank-$1$ bandit where at each step, the
learning agent chooses a pair of row and column arms, and receives the product
of their Bernoulli-distributed values as a reward. This is a special case of
the stochastic rank-$1$ bandit problem considered in recent work that proposed
an elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret
scales linearly with the number of rows and columns on ""benign"" instances.
These are the instances where the minimum of the average row and column rewards
$\mu$ is bounded away from zero. The issue with Rank1Elim is that it fails to
be competitive with straightforward bandit strategies as $\mu \rightarrow 0$.
In this paper we propose Rank1ElimKL which simply replaces the (crude)
confidence intervals of Rank1Elim with confidence intervals based on
Kullback-Leibler (KL) divergences, and with the help of a novel result
concerning the scaling of KL divergences we prove that with this change, our
algorithm will be competitive no matter the value of $\mu$. Experiments with
synthetic data confirm that on benign instances the performance of Rank1ElimKL
is significantly better than that of even Rank1Elim, while experiments with
models derived from real data confirm that the improvements are significant
across the board, regardless of whether the data is benign or not.","['Sumeet Katariya', 'Branislav Kveton', 'Csaba Szepesvári', 'Claire Vernade', 'Zheng Wen']","['cs.LG', 'stat.ML']",2017-03-19 21:06:51+00:00
http://arxiv.org/abs/1703.06476v2,Practical Coreset Constructions for Machine Learning,"We investigate coresets - succinct, small summaries of large data sets - so
that solutions found on the summary are provably competitive with solution
found on the full data set. We provide an overview over the state-of-the-art in
coreset construction for machine learning. In Section 2, we present both the
intuition behind and a theoretically sound framework to construct coresets for
general problems and apply it to $k$-means clustering. In Section 3 we
summarize existing coreset construction algorithms for a variety of machine
learning problems such as maximum likelihood estimation of mixture models,
Bayesian non-parametric models, principal component analysis, regression and
general empirical risk minimization.","['Olivier Bachem', 'Mario Lucic', 'Andreas Krause']",['stat.ML'],2017-03-19 17:45:29+00:00
http://arxiv.org/abs/1703.06327v1,Spectrum Estimation from a Few Entries,"Singular values of a data in a matrix form provide insights on the structure
of the data, the effective dimensionality, and the choice of hyper-parameters
on higher-level data analysis tools. However, in many practical applications
such as collaborative filtering and network analysis, we only get a partial
observation. Under such scenarios, we consider the fundamental problem of
recovering spectral properties of the underlying matrix from a sampling of its
entries. We are particularly interested in directly recovering the spectrum,
which is the set of singular values, and also in sample-efficient approaches
for recovering a spectral sum function, which is an aggregate sum of the same
function applied to each of the singular values. We propose first estimating
the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation
to the spectral sum function or applying moment matching in Wasserstein
distance to recover the singular values. The main technical challenge is in
accurately estimating the Schatten norms from a sampling of a matrix. We
introduce a novel unbiased estimator based on counting small structures in a
graph and provide guarantees that match its empirical performance. Our
theoretical analysis shows that Schatten norms can be recovered accurately from
strictly smaller number of samples compared to what is needed to recover the
underlying low-rank matrix. Numerical experiments suggest that we significantly
improve upon a competing approach of using matrix completion methods.","['Ashish Khetan', 'Sewoong Oh']","['stat.ML', 'cs.DS', 'cs.LG', 'cs.NA']",2017-03-18 18:12:17+00:00
http://arxiv.org/abs/1703.06324v2,Deep Tensor Encoding,"Learning an encoding of feature vectors in terms of an over-complete
dictionary or a information geometric (Fisher vectors) construct is wide-spread
in statistical signal processing and computer vision. In content based
information retrieval using deep-learning classifiers, such encodings are
learnt on the flattened last layer, without adherence to the multi-linear
structure of the underlying feature tensor. We illustrate a variety of feature
encodings incl. sparse dictionary coding and Fisher vectors along with
proposing that a structured tensor factorization scheme enables us to perform
retrieval that can be at par, in terms of average precision, with Fisher vector
encoded image signatures. In short, we illustrate how structural constraints
increase retrieval fidelity.","['B Sengupta', 'E Vasquez', 'Y Qian']","['cs.IR', 'cs.LG', 'stat.ML']",2017-03-18 17:49:42+00:00
http://arxiv.org/abs/1703.06272v1,An Automated Auto-encoder Correlation-based Health-Monitoring and Prognostic Method for Machine Bearings,"This paper studies an intelligent ultimate technique for health-monitoring
and prognostic of common rotary machine components, particularly bearings.
During a run-to-failure experiment, rich unsupervised features from vibration
sensory data are extracted by a trained sparse auto-encoder. Then, the
correlation of the extracted attributes of the initial samples (presumably
healthy at the beginning of the test) with the succeeding samples is calculated
and passed through a moving-average filter. The normalized output is named
auto-encoder correlation-based (AEC) rate which stands for an informative
attribute of the system depicting its health status and precisely identifying
the degradation starting point. We show that AEC technique well-generalizes in
several run-to-failure tests. AEC collects rich unsupervised features form the
vibration data fully autonomous. We demonstrate the superiority of the AEC over
many other state-of-the-art approaches for the health monitoring and prognostic
of machine bearings.","['Ramin M. Hasani', 'Guodong Wang', 'Radu Grosu']","['cs.LG', 'cs.NE', 'stat.ML']",2017-03-18 08:38:51+00:00
http://arxiv.org/abs/1703.06270v3,SIM-CE: An Advanced Simulink Platform for Studying the Brain of Caenorhabditis elegans,"We introduce SIM-CE, an advanced, user-friendly modeling and simulation
environment in Simulink for performing multi-scale behavioral analysis of the
nervous system of Caenorhabditis elegans (C. elegans). SIM-CE contains an
implementation of the mathematical models of C. elegans's neurons and synapses,
in Simulink, which can be easily extended and particularized by the user. The
Simulink model is able to capture both complex dynamics of ion channels and
additional biophysical detail such as intracellular calcium concentration. We
demonstrate the performance of SIM-CE by carrying out neuronal, synaptic and
neural-circuit-level behavioral simulations. Such environment enables the user
to capture unknown properties of the neural circuits, test hypotheses and
determine the origin of many behavioral plasticities exhibited by the worm.","['Ramin M. Hasani', 'Victoria Beneder', 'Magdalena Fuchs', 'David Lung', 'Radu Grosu']","['q-bio.NC', 'cs.NE', 'q-bio.QM', 'stat.ML']",2017-03-18 08:27:42+00:00
http://arxiv.org/abs/1703.06240v1,Multi-fidelity Bayesian Optimisation with Continuous Approximations,"Bandit methods for black-box optimisation, such as Bayesian optimisation, are
used in a variety of applications including hyper-parameter tuning and
experiment design. Recently, \emph{multi-fidelity} methods have garnered
considerable attention since function evaluations have become increasingly
expensive in such applications. Multi-fidelity methods use cheap approximations
to the function of interest to speed up the overall optimisation process.
However, most multi-fidelity methods assume only a finite number of
approximations. In many practical applications however, a continuous spectrum
of approximations might be available. For instance, when tuning an expensive
neural network, one might choose to approximate the cross validation
performance using less data $N$ and/or few training iterations $T$. Here, the
approximations are best viewed as arising out of a continuous two dimensional
space $(N,T)$. In this work, we develop a Bayesian optimisation method, BOCA,
for this setting. We characterise its theoretical properties and show that it
achieves better regret than than strategies which ignore the approximations.
BOCA outperforms several other baselines in synthetic and real experiments.","['Kirthevasan Kandasamy', 'Gautam Dasarathy', 'Jeff Schneider', 'Barnabas Poczos']",['stat.ML'],2017-03-18 03:28:40+00:00
http://arxiv.org/abs/1703.06229v2,Curriculum Dropout,"Dropout is a very effective way of regularizing neural networks.
Stochastically ""dropping out"" units with a certain probability discourages
over-specific co-adaptations of feature detectors, preventing overfitting and
improving network generalization. Besides, Dropout can be interpreted as an
approximate model aggregation technique, where an exponential number of smaller
networks are averaged in order to get a more powerful ensemble. In this paper,
we show that using a fixed dropout probability during training is a suboptimal
choice. We thus propose a time scheduling for the probability of retaining
neurons in the network. This induces an adaptive regularization scheme that
smoothly increases the difficulty of the optimization problem. This idea of
""starting easy"" and adaptively increasing the difficulty of the learning
problem has its roots in curriculum learning and allows one to train better
models. Indeed, we prove that our optimization strategy implements a very
general curriculum scheme, by gradually adding noise to both the input and
intermediate feature representations within the network architecture.
Experiments on seven image classification datasets and different network
architectures show that our method, named Curriculum Dropout, frequently yields
to better generalization and, at worst, performs just as well as the standard
Dropout method.","['Pietro Morerio', 'Jacopo Cavazza', 'Riccardo Volpi', 'Rene Vidal', 'Vittorio Murino']","['cs.NE', 'cs.LG', 'stat.ML']",2017-03-18 00:59:40+00:00
http://arxiv.org/abs/1703.06222v5,A unified treatment of multiple testing with prior knowledge using the p-filter,"There is a significant literature on methods for incorporating knowledge into
multiple testing procedures so as to improve their power and precision. Some
common forms of prior knowledge include (a) beliefs about which hypotheses are
null, modeled by non-uniform prior weights; (b) differing importances of
hypotheses, modeled by differing penalties for false discoveries; (c) multiple
arbitrary partitions of the hypotheses into (possibly overlapping) groups; and
(d) knowledge of independence, positive or arbitrary dependence between
hypotheses or groups, suggesting the use of more aggressive or conservative
procedures. We present a unified algorithmic framework called p-filter for
global null testing and false discovery rate (FDR) control that allows the
scientist to incorporate all four types of prior knowledge (a)-(d)
simultaneously, recovering a variety of known algorithms as special cases.","['Aaditya Ramdas', 'Rina Foygel Barber', 'Martin J. Wainwright', 'Michael I. Jordan']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2017-03-18 00:08:59+00:00
http://arxiv.org/abs/1703.06217v2,Deciding How to Decide: Dynamic Routing in Artificial Neural Networks,"We propose and systematically evaluate three strategies for training
dynamically-routed artificial neural networks: graphs of learned
transformations through which different input signals may take different paths.
Though some approaches have advantages over others, the resulting networks are
often qualitatively similar. We find that, in dynamically-routed networks
trained to classify images, layers and branches become specialized to process
distinct categories of images. Additionally, given a fixed computational
budget, dynamically-routed networks tend to perform better than comparable
statically-routed networks.","['Mason McGill', 'Pietro Perona']","['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']",2017-03-17 23:52:14+00:00
http://arxiv.org/abs/1703.06177v2,On Consistency of Graph-based Semi-supervised Learning,"Graph-based semi-supervised learning is one of the most popular methods in
machine learning. Some of its theoretical properties such as bounds for the
generalization error and the convergence of the graph Laplacian regularizer
have been studied in computer science and statistics literatures. However, a
fundamental statistical property, the consistency of the estimator from this
method has not been proved. In this article, we study the consistency problem
under a non-parametric framework. We prove the consistency of graph-based
learning in the case that the estimated scores are enforced to be equal to the
observed responses for the labeled data. The sample sizes of both labeled and
unlabeled data are allowed to grow in this result. When the estimated scores
are not required to be equal to the observed responses, a tuning parameter is
used to balance the loss function and the graph Laplacian regularizer. We give
a counterexample demonstrating that the estimator for this case can be
inconsistent. The theoretical findings are supported by numerical studies.","['Chengan Du', 'Yunpeng Zhao', 'Feng Wang']",['stat.ML'],2017-03-17 19:24:09+00:00
http://arxiv.org/abs/1703.06131v4,Inference via low-dimensional couplings,"We investigate the low-dimensional structure of deterministic transformations
between random variables, i.e., transport maps between probability measures. In
the context of statistics and machine learning, these transformations can be
used to couple a tractable ""reference"" measure (e.g., a standard Gaussian) with
a target measure of interest. Direct simulation from the desired measure can
then be achieved by pushing forward reference samples through the map. Yet
characterizing such a map---e.g., representing and evaluating it---grows
challenging in high dimensions. The central contribution of this paper is to
establish a link between the Markov properties of the target measure and the
existence of low-dimensional couplings, induced by transport maps that are
sparse and/or decomposable. Our analysis not only facilitates the construction
of transformations in high-dimensional settings, but also suggests new
inference methodologies for continuous non-Gaussian graphical models. For
instance, in the context of nonlinear state-space models, we describe new
variational algorithms for filtering, smoothing, and sequential parameter
inference. These algorithms can be understood as the natural
generalization---to the non-Gaussian case---of the square-root
Rauch-Tung-Striebel Gaussian smoother.","['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']","['stat.ME', 'stat.CO', 'stat.ML']",2017-03-17 17:50:44+00:00
http://arxiv.org/abs/1703.06104v1,Nonconvex One-bit Single-label Multi-label Learning,"We study an extreme scenario in multi-label learning where each training
instance is endowed with a single one-bit label out of multiple labels. We
formulate this problem as a non-trivial special case of one-bit rank-one matrix
sensing and develop an efficient non-convex algorithm based on alternating
power iteration. The proposed algorithm is able to recover the underlying
low-rank matrix model with linear convergence. For a rank-$k$ model with $d_1$
features and $d_2$ classes, the proposed algorithm achieves $O(\epsilon)$
recovery error after retrieving $O(k^{1.5}d_1 d_2/\epsilon)$ one-bit labels
within $O(kd)$ memory. Our bound is nearly optimal in the order of
$O(1/\epsilon)$. This significantly improves the state-of-the-art sampling
complexity of one-bit multi-label learning. We perform experiments to verify
our theory and evaluate the performance of the proposed algorithm.","['Shuang Qiu', 'Tingjin Luo', 'Jieping Ye', 'Ming Lin']","['stat.ML', 'cs.LG']",2017-03-17 17:09:15+00:00
http://arxiv.org/abs/1703.06103v4,Modeling Relational Data with Graph Convolutional Networks,"Knowledge graphs enable a wide variety of applications, including question
answering and information retrieval. Despite the great effort invested in their
creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata)
remain incomplete. We introduce Relational Graph Convolutional Networks
(R-GCNs) and apply them to two standard knowledge base completion tasks: Link
prediction (recovery of missing facts, i.e. subject-predicate-object triples)
and entity classification (recovery of missing entity attributes). R-GCNs are
related to a recent class of neural networks operating on graphs, and are
developed specifically to deal with the highly multi-relational data
characteristic of realistic knowledge bases. We demonstrate the effectiveness
of R-GCNs as a stand-alone model for entity classification. We further show
that factorization models for link prediction such as DistMult can be
significantly improved by enriching them with an encoder model to accumulate
evidence over multiple inference steps in the relational graph, demonstrating a
large improvement of 29.8% on FB15k-237 over a decoder-only baseline.","['Michael Schlichtkrull', 'Thomas N. Kipf', 'Peter Bloem', 'Rianne van den Berg', 'Ivan Titov', 'Max Welling']","['stat.ML', 'cs.AI', 'cs.DB', 'cs.LG']",2017-03-17 17:09:14+00:00
http://arxiv.org/abs/1703.06065v2,Block CUR: Decomposing Matrices using Groups of Columns,"A common problem in large-scale data analysis is to approximate a matrix
using a combination of specifically sampled rows and columns, known as CUR
decomposition. Unfortunately, in many real-world environments, the ability to
sample specific individual rows or columns of the matrix is limited by either
system constraints or cost. In this paper, we consider matrix approximation by
sampling predefined \emph{blocks} of columns (or rows) from the matrix. We
present an algorithm for sampling useful column blocks and provide novel
guarantees for the quality of the approximation. This algorithm has application
in problems as diverse as biometric data analysis to distributed computing. We
demonstrate the effectiveness of the proposed algorithms for computing the
Block CUR decomposition of large matrices in a distributed setting with
multiple nodes in a compute cluster, where such blocks correspond to columns
(or rows) of the matrix stored on the same node, which can be retrieved with
much less overhead than retrieving individual columns stored across different
nodes. In the biometric setting, the rows correspond to different users and
columns correspond to users' biometric reaction to external stimuli, {\em
e.g.,}~watching video content, at a particular time instant. There is
significant cost in acquiring each user's reaction to lengthy content so we
sample a few important scenes to approximate the biometric response. An
individual time sample in this use case cannot be queried in isolation due to
the lack of context that caused that biometric reaction. Instead, collections
of time segments ({\em i.e.,} blocks) must be presented to the user. The
practical application of these algorithms is shown via experimental results
using real-world user biometric data from a content testing environment.","['Urvashi Oswal', 'Swayambhoo Jain', 'Kevin S. Xu', 'Brian Eriksson']","['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG']",2017-03-17 16:08:23+00:00
http://arxiv.org/abs/1703.06043v2,Pattern representation and recognition with accelerated analog neuromorphic systems,"Despite being originally inspired by the central nervous system, artificial
neural networks have diverged from their biological archetypes as they have
been remodeled to fit particular tasks. In this paper, we review several
possibilites to reverse map these architectures to biologically more realistic
spiking networks with the aim of emulating them on fast, low-power neuromorphic
hardware. Since many of these devices employ analog components, which cannot be
perfectly controlled, finding ways to compensate for the resulting effects
represents a key challenge. Here, we discuss three different strategies to
address this problem: the addition of auxiliary network components for
stabilizing activity, the utilization of inherently robust architectures and a
training method for hardware-emulated networks that functions without perfect
knowledge of the system's dynamics and parameters. For all three scenarios, we
corroborate our theoretical considerations with experimental results on
accelerated analog neuromorphic platforms.","['Mihai A. Petrovici', 'Sebastian Schmitt', 'Johann Klähn', 'David Stöckel', 'Anna Schroeder', 'Guillaume Bellec', 'Johannes Bill', 'Oliver Breitwieser', 'Ilja Bytschok', 'Andreas Grübl', 'Maurice Güttler', 'Andreas Hartel', 'Stephan Hartmann', 'Dan Husmann', 'Kai Husmann', 'Sebastian Jeltsch', 'Vitali Karasenko', 'Mitja Kleider', 'Christoph Koke', 'Alexander Kononov', 'Christian Mauch', 'Eric Müller', 'Paul Müller', 'Johannes Partzsch', 'Thomas Pfeil', 'Stefan Schiefer', 'Stefan Scholze', 'Anand Subramoney', 'Vasilis Thanasoulis', 'Bernhard Vogginger', 'Robert Legenstein', 'Wolfgang Maass', 'René Schüffny', 'Christian Mayr', 'Johannes Schemmel', 'Karlheinz Meier']","['q-bio.NC', 'cs.NE', 'stat.ML']",2017-03-17 14:59:17+00:00
http://arxiv.org/abs/1703.05849v3,Estimation and Inference on Nonlinear and Heterogeneous Effects,"Multiple regression has been the go-to method for data analysis for
generations of scholars due to its transparency, interpretability, and
desirable theoretical properties. However, the method's simplicity precludes
the discovery of complex heterogeneities in the data. We introduce the Method
of Direct Estimation and Inference (MDEI) that embraces these potential
complexities, is interpretable, has desirable theoretical guarantees, and,
unlike some existing methods, returns appropriate uncertainty estimates. The
proposed method uses a machine learning regression methodology to estimate the
observation-level effect of a treatment variable. Importantly, we introduce a
robust approach to uncertainty estimates. We provide simulation evidence and an
application illustrating the performance of the method.","['Marc Ratkovic', 'Dustin Tingley']","['stat.ML', 'stat.ME', '62G08, 46N30, 62P20, 62P25']",2017-03-16 23:33:24+00:00
http://arxiv.org/abs/1703.05841v1,Adaptivity to Noise Parameters in Nonparametric Active Learning,"This work addresses various open questions in the theory of active learning
for nonparametric classification. Our contributions are both statistical and
algorithmic: -We establish new minimax-rates for active learning under common
\textit{noise conditions}. These rates display interesting transitions -- due
to the interaction between noise \textit{smoothness and margin} -- not present
in the passive setting. Some such transitions were previously conjectured, but
remained unconfirmed. -We present a generic algorithmic strategy for adaptivity
to unknown noise smoothness and margin; our strategy achieves optimal rates in
many general situations; furthermore, unlike in previous work, we avoid the
need for \textit{adaptive confidence sets}, resulting in strictly milder
distributional requirements.","['Andrea Locatelli', 'Alexandra Carpentier', 'Samory Kpotufe']",['stat.ML'],2017-03-16 22:37:55+00:00
http://arxiv.org/abs/1703.05840v5,Conditional Accelerated Lazy Stochastic Gradient Descent,"In this work we introduce a conditional accelerated lazy stochastic gradient
descent algorithm with optimal number of calls to a stochastic first-order
oracle and convergence rate $O\left(\frac{1}{\varepsilon^2}\right)$ improving
over the projection-free, Online Frank-Wolfe based stochastic gradient descent
of Hazan and Kale [2012] with convergence rate
$O\left(\frac{1}{\varepsilon^4}\right)$.","['Guanghui Lan', 'Sebastian Pokutta', 'Yi Zhou', 'Daniel Zink']","['cs.LG', 'stat.ML', '90C25', 'G.1.6']",2017-03-16 22:15:17+00:00
http://arxiv.org/abs/1703.05785v1,Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and Blind Unmixing of Hyperspectral Images,"Estimation of the number of endmembers existing in a scene constitutes a
critical task in the hyperspectral unmixing process. The accuracy of this
estimate plays a crucial role in subsequent unsupervised unmixing steps i.e.,
the derivation of the spectral signatures of the endmembers (endmembers'
extraction) and the estimation of the abundance fractions of the pixels. A
common practice amply followed in literature is to treat endmembers' number
estimation and unmixing, independently as two separate tasks, providing the
outcome of the former as input to the latter. In this paper, we go beyond this
computationally demanding strategy. More precisely, we set forth a multiple
constrained optimization framework, which encapsulates endmembers' number
estimation and unsupervised unmixing in a single task. This is attained by
suitably formulating the problem via a low-rank and sparse nonnegative matrix
factorization rationale, where low-rankness is promoted with the use of a
sophisticated $\ell_2/\ell_1$ norm penalty term. An alternating proximal
algorithm is then proposed for minimizing the emerging cost function. The
results obtained by simulated and real data experiments verify the
effectiveness of the proposed approach.","['Paris V. Giampouras', 'Athanasios A. Rontogiannis', 'Konstantinos D. Koutroumbas']","['cs.CV', 'stat.ML']",2017-03-16 18:25:21+00:00
http://arxiv.org/abs/1703.05687v2,Gaussian process regression for forecasting battery state of health,"Accurately predicting the future capacity and remaining useful life of
batteries is necessary to ensure reliable system operation and to minimise
maintenance costs. The complex nature of battery degradation has meant that
mechanistic modelling of capacity fade has thus far remained intractable;
however, with the advent of cloud-connected devices, data from cells in various
applications is becoming increasingly available, and the feasibility of
data-driven methods for battery prognostics is increasing. Here we propose
Gaussian process (GP) regression for forecasting battery state of health, and
highlight various advantages of GPs over other data-driven and mechanistic
approaches. GPs are a type of Bayesian non-parametric method, and hence can
model complex systems whilst handling uncertainty in a principled manner. Prior
information can be exploited by GPs in a variety of ways: explicit mean
functions can be used if the functional form of the underlying degradation
model is available, and multiple-output GPs can effectively exploit
correlations between data from different cells. We demonstrate the predictive
capability of GPs for short-term and long-term (remaining useful life)
forecasting on a selection of capacity vs. cycle datasets from lithium-ion
cells.","['Robert R. Richardson', 'Michael A. Osborne', 'David A. Howey']","['stat.AP', 'stat.ML', '62P30', 'J.2; G.3']",2017-03-16 16:00:00+00:00
http://arxiv.org/abs/1703.05667v2,End-to-End Learning for Structured Prediction Energy Networks,"Structured Prediction Energy Networks (SPENs) are a simple, yet expressive
family of structured prediction models (Belanger and McCallum, 2016). An energy
function over candidate structured outputs is given by a deep network, and
predictions are formed by gradient-based optimization. This paper presents
end-to-end learning for SPENs, where the energy function is discriminatively
trained by back-propagating through gradient-based prediction. In our
experience, the approach is substantially more accurate than the structured SVM
method of Belanger and McCallum (2016), as it allows us to use more
sophisticated non-convex energies. We provide a collection of techniques for
improving the speed, accuracy, and memory requirements of end-to-end SPENs, and
demonstrate the power of our method on 7-Scenes image denoising and CoNLL-2005
semantic role labeling tasks. In both, inexact minimization of non-convex SPEN
energies is superior to baseline methods that use simplistic energy functions
that can be minimized exactly.","['David Belanger', 'Bishan Yang', 'Andrew McCallum']","['stat.ML', 'cs.LG']",2017-03-16 15:14:48+00:00
http://arxiv.org/abs/1703.05537v2,Shift Aggregate Extract Networks,"We introduce an architecture based on deep hierarchical decompositions to
learn effective representations of large graphs. Our framework extends classic
R-decompositions used in kernel methods, enabling nested part-of-part
relations. Unlike recursive neural networks, which unroll a template on input
graphs directly, we unroll a neural network template over the decomposition
hierarchy, allowing us to deal with the high degree variability that typically
characterize social network graphs. Deep hierarchical decompositions are also
amenable to domain compression, a technique that reduces both space and time
complexity by exploiting symmetries. We show empirically that our approach is
able to outperform current state-of-the-art graph classification methods on
large social network datasets, while at the same time being competitive on
small chemobiological benchmark datasets.","['Francesco Orsini', 'Daniele Baracchi', 'Paolo Frasconi']","['cs.LG', 'stat.ML']",2017-03-16 09:52:48+00:00
http://arxiv.org/abs/1703.05452v2,Efficient Online Learning for Optimizing Value of Information: Theory and Application to Interactive Troubleshooting,"We consider the optimal value of information (VoI) problem, where the goal is
to sequentially select a set of tests with a minimal cost, so that one can
efficiently make the best decision based on the observed outcomes. Existing
algorithms are either heuristics with no guarantees, or scale poorly (with
exponential run time in terms of the number of available tests). Moreover,
these methods assume a known distribution over the test outcomes, which is
often not the case in practice. We propose an efficient sampling-based online
learning framework to address the above issues. First, assuming the
distribution over hypotheses is known, we propose a dynamic hypothesis
enumeration strategy, which allows efficient information gathering with strong
theoretical guarantees. We show that with sufficient amount of samples, one can
identify a near-optimal decision with high probability. Second, when the
parameters of the hypotheses distribution are unknown, we propose an algorithm
which learns the parameters progressively via posterior sampling in an online
fashion. We further establish a rigorous bound on the expected regret. We
demonstrate the effectiveness of our approach on a real-world interactive
troubleshooting application and show that one can efficiently make high-quality
decisions with low cost.","['Yuxin Chen', 'Jean-Michel Renders', 'Morteza Haghir Chehreghani', 'Andreas Krause']","['cs.AI', 'cs.LG', 'stat.ML']",2017-03-16 01:37:25+00:00
http://arxiv.org/abs/1703.05449v2,Minimax Regret Bounds for Reinforcement Learning,"We consider the problem of provably optimal exploration in reinforcement
learning for finite horizon MDPs. We show that an optimistic modification to
value iteration achieves a regret bound of $\tilde{O}( \sqrt{HSAT} +
H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states,
$A$ the number of actions and $T$ the number of time-steps. This result
improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved
by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new
results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of
$\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of
$\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contains two key
insights. We use careful application of concentration inequalities to the
optimal value function as a whole, rather than to the transitions probabilities
(to improve scaling in $S$), and we define Bernstein-based ""exploration
bonuses"" that use the empirical variance of the estimated values at the next
states (to improve scaling in $H$).","['Mohammad Gheshlaghi Azar', 'Ian Osband', 'Rémi Munos']","['stat.ML', 'cs.AI', 'cs.LG']",2017-03-16 01:31:33+00:00
http://arxiv.org/abs/1703.05430v2,Cost-complexity pruning of random forests,"Random forests perform bootstrap-aggregation by sampling the training samples
with replacement. This enables the evaluation of out-of-bag error which serves
as a internal cross-validation mechanism. Our motivation lies in using the
unsampled training samples to improve each decision tree in the ensemble. We
study the effect of using the out-of-bag samples to improve the generalization
error first of the decision trees and second the random forest by post-pruning.
A preliminary empirical study on four UCI repository datasets show consistent
decrease in the size of the forests without considerable loss in accuracy.","['Kiran Bangalore Ravi', 'Jean Serra']","['stat.ML', 'cs.LG']",2017-03-15 23:58:19+00:00
http://arxiv.org/abs/1703.05411v1,Aggregation of Classifiers: A Justifiable Information Granularity Approach,"In this study, we introduce a new approach to combine multi-classifiers in an
ensemble system. Instead of using numeric membership values encountered in
fixed combining rules, we construct interval membership values associated with
each class prediction at the level of meta-data of observation by using
concepts of information granules. In the proposed method, uncertainty
(diversity) of findings produced by the base classifiers is quantified by
interval-based information granules. The discriminative decision model is
generated by considering both the bounds and the length of the obtained
intervals. We select ten and then fifteen learning algorithms to build a
heterogeneous ensemble system and then conducted the experiment on a number of
UCI datasets. The experimental results demonstrate that the proposed approach
performs better than the benchmark algorithms including six fixed combining
methods, one trainable combining method, AdaBoost, Bagging, and Random
Subspace.","['Tien Thanh Nguyen', 'Xuan Cuong Pham', 'Alan Wee-Chung Liew', 'Witold Pedrycz']","['cs.LG', 'stat.ML']",2017-03-15 22:48:05+00:00
http://arxiv.org/abs/1703.05189v2,Student-t Process Quadratures for Filtering of Non-Linear Systems with Heavy-Tailed Noise,"The aim of this article is to design a moment transformation for Student- t
distributed random variables, which is able to account for the error in the
numerically computed mean. We employ Student-t process quadrature, an instance
of Bayesian quadrature, which allows us to treat the integral itself as a
random variable whose variance provides information about the incurred
integration error. Advantage of the Student- t process quadrature over the
traditional Gaussian process quadrature, is that the integral variance depends
also on the function values, allowing for a more robust modelling of the
integration error. The moment transform is applied in nonlinear sigma-point
filtering and evaluated on two numerical examples, where it is shown to
outperform the state-of-the-art moment transforms.","['Jakub Prüher', 'Filip Tronarp', 'Toni Karvonen', 'Simo Särkkä', 'Ondřej Straka']","['stat.ME', 'stat.ML']",2017-03-15 14:47:02+00:00
http://arxiv.org/abs/1703.05175v2,Prototypical Networks for Few-shot Learning,"We propose prototypical networks for the problem of few-shot classification,
where a classifier must generalize to new classes not seen in the training set,
given only a small number of examples of each new class. Prototypical networks
learn a metric space in which classification can be performed by computing
distances to prototype representations of each class. Compared to recent
approaches for few-shot learning, they reflect a simpler inductive bias that is
beneficial in this limited-data regime, and achieve excellent results. We
provide an analysis showing that some simple design decisions can yield
substantial improvements over recent approaches involving complicated
architectural choices and meta-learning. We further extend prototypical
networks to zero-shot learning and achieve state-of-the-art results on the
CU-Birds dataset.","['Jake Snell', 'Kevin Swersky', 'Richard S. Zemel']","['cs.LG', 'stat.ML']",2017-03-15 14:31:55+00:00
http://arxiv.org/abs/1703.05160v1,A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators for Partition Function Computation in Log-Linear Models,"Log-linear models are arguably the most successful class of graphical models
for large-scale applications because of their simplicity and tractability.
Learning and inference with these models require calculating the partition
function, which is a major bottleneck and intractable for large state spaces.
Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the
condition of having a ""good"" proposal distribution is often not satisfied in
practice.
  In this paper, we add a new dimension to efficient estimation via sampling.
We propose a new sampling scheme and an unbiased estimator that estimates the
partition function accurately in sub-linear time. Our samples are generated in
near-constant time using locality sensitive hashing (LSH), and so are
correlated and unnormalized. We demonstrate the effectiveness of our proposed
approach by comparing the accuracy and speed of estimating the partition
function against other state-of-the-art estimation techniques including IS and
the efficient variant of Gumbel-Max sampling. With our efficient sampling
scheme, we accurately train real-world language models using only 1-2% of
computations.","['Ryan Spring', 'Anshumali Shrivastava']","['stat.ML', 'cs.DB', 'cs.DS', 'cs.LG']",2017-03-15 14:01:21+00:00
http://arxiv.org/abs/1703.05082v1,Selective Harvesting over Networks,"Active search (AS) on graphs focuses on collecting certain labeled nodes
(targets) given global knowledge of the network topology and its edge weights
under a query budget. However, in most networks, nodes, topology and edge
weights are all initially unknown. We introduce selective harvesting, a variant
of AS where the next node to be queried must be chosen among the neighbors of
the current queried node set; the available training data for deciding which
node to query is restricted to the subgraph induced by the queried set (and
their node attributes) and their neighbors (without any node or edge
attributes). Therefore, selective harvesting is a sequential decision problem,
where we must decide which node to query at each step. A classifier trained in
this scenario suffers from a tunnel vision effect: without recourse to
independent sampling, the urge to query promising nodes forces classifiers to
gather increasingly biased training data, which we show significantly hurts the
performance of AS methods and standard classifiers. We find that it is possible
to collect a much larger set of targets by using multiple classifiers, not by
combining their predictions as an ensemble, but switching between classifiers
used at each step, as a way to ease the tunnel vision effect. We discover that
switching classifiers collects more targets by (a) diversifying the training
data and (b) broadening the choices of nodes that can be queried next. This
highlights an exploration, exploitation, and diversification trade-off in our
problem that goes beyond the exploration and exploitation duality found in
classic sequential decision problems. From these observations we propose D3TS,
a method based on multi-armed bandits for non-stationary stochastic processes
that enforces classifier diversity, matching or exceeding the performance of
competing methods on seven real network datasets in our evaluation.","['Fabricio Murai', 'Diogo Rennó', 'Bruno Ribeiro', 'Gisele L. Pappa', 'Don Towsley', 'Krista Gile']","['cs.SI', 'cs.LG', 'stat.ML', 'I.2.6; E.1']",2017-03-15 11:17:02+00:00
http://arxiv.org/abs/1703.05080v1,Tuning Free Orthogonal Matching Pursuit,"Orthogonal matching pursuit (OMP) is a widely used compressive sensing (CS)
algorithm for recovering sparse signals in noisy linear regression models. The
performance of OMP depends on its stopping criteria (SC). SC for OMP discussed
in literature typically assumes knowledge of either the sparsity of the signal
to be estimated $k_0$ or noise variance $\sigma^2$, both of which are
unavailable in many practical applications. In this article we develop a
modified version of OMP called tuning free OMP or TF-OMP which does not require
a SC. TF-OMP is proved to accomplish successful sparse recovery under the usual
assumptions on restricted isometry constants (RIC) and mutual coherence of
design matrix. TF-OMP is numerically shown to deliver a highly competitive
performance in comparison with OMP having \textit{a priori} knowledge of $k_0$
or $\sigma^2$. Greedy algorithm for robust de-noising (GARD) is an OMP like
algorithm proposed for efficient estimation in classical overdetermined linear
regression models corrupted by sparse outliers. However, GARD requires the
knowledge of inlier noise variance which is difficult to estimate. We also
produce a tuning free algorithm (TF-GARD) for efficient estimation in the
presence of sparse outliers by extending the operating principle of TF-OMP to
GARD. TF-GARD is numerically shown to achieve a performance comparable to that
of the existing implementation of GARD.","['Sreejith Kallummil', 'Sheetal Kalyani']","['stat.ML', 'cs.IT', 'math.IT']",2017-03-15 11:07:13+00:00
http://arxiv.org/abs/1703.05060v1,Online Learning for Distribution-Free Prediction,"We develop an online learning method for prediction, which is important in
problems with large and/or streaming data sets. We formulate the learning
approach using a covariance-fitting methodology, and show that the resulting
predictor has desirable computational and distribution-free properties: It is
implemented online with a runtime that scales linearly in the number of
samples; has a constant memory requirement; avoids local minima problems; and
prunes away redundant feature dimensions without relying on restrictive
assumptions on the data distribution. In conjunction with the split conformal
approach, it also produces distribution-free prediction confidence intervals in
a computationally efficient manner. The method is demonstrated on both real and
synthetic datasets.","['Dave Zachariah', 'Petre Stoica', 'Thomas B. Schön']","['cs.LG', 'stat.CO', 'stat.ML']",2017-03-15 10:20:32+00:00
http://arxiv.org/abs/1703.04986v1,Label Stability in Multiple Instance Learning,"We address the problem of \emph{instance label stability} in multiple
instance learning (MIL) classifiers. These classifiers are trained only on
globally annotated images (bags), but often can provide fine-grained
annotations for image pixels or patches (instances). This is interesting for
computer aided diagnosis (CAD) and other medical image analysis tasks for which
only a coarse labeling is provided. Unfortunately, the instance labels may be
unstable. This means that a slight change in training data could potentially
lead to abnormalities being detected in different parts of the image, which is
undesirable from a CAD point of view. Despite MIL gaining popularity in the CAD
literature, this issue has not yet been addressed. We investigate the stability
of instance labels provided by several MIL classifiers on 5 different datasets,
of which 3 are medical image datasets (breast histopathology, diabetic
retinopathy and computed tomography lung images). We propose an unsupervised
measure to evaluate instance stability, and demonstrate that a
performance-stability trade-off can be made when comparing MIL classifiers.","['Veronika Cheplygina', 'Lauge Sørensen', 'David M. J. Tax', 'Marleen de Bruijne', 'Marco Loog']","['cs.CV', 'stat.ML']",2017-03-15 07:46:18+00:00
http://arxiv.org/abs/1703.04981v1,Transfer Learning by Asymmetric Image Weighting for Segmentation across Scanners,"Supervised learning has been very successful for automatic segmentation of
images from a single scanner. However, several papers report deteriorated
performances when using classifiers trained on images from one scanner to
segment images from other scanners. We propose a transfer learning classifier
that adapts to differences between training and test images. This method uses a
weighted ensemble of classifiers trained on individual images. The weight of
each classifier is determined by the similarity between its training image and
the test image.
  We examine three unsupervised similarity measures, which can be used in
scenarios where no labeled data from a newly introduced scanner or scanning
protocol is available. The measures are based on a divergence, a bag distance,
and on estimating the labels with a clustering procedure. These measures are
asymmetric. We study whether the asymmetry can improve classification. Out of
the three similarity measures, the bag similarity measure is the most robust
across different studies and achieves excellent results on four brain tissue
segmentation datasets and three white matter lesion segmentation datasets,
acquired at different centers and with different scanners and scanning
protocols. We show that the asymmetry can indeed be informative, and that
computing the similarity from the test image to the training images is more
appropriate than the opposite direction.","['Veronika Cheplygina', 'Annegreet van Opbroek', 'M. Arfan Ikram', 'Meike W. Vernooij', 'Marleen de Bruijne']","['cs.CV', 'stat.ML']",2017-03-15 07:43:10+00:00
http://arxiv.org/abs/1703.04980v1,Classification of COPD with Multiple Instance Learning,"Chronic obstructive pulmonary disease (COPD) is a lung disease where early
detection benefits the survival rate. COPD can be quantified by classifying
patches of computed tomography images, and combining patch labels into an
overall diagnosis for the image. As labeled patches are often not available,
image labels are propagated to the patches, incorrectly labeling healthy
patches in COPD patients as being affected by the disease. We approach
quantification of COPD from lung images as a multiple instance learning (MIL)
problem, which is more suitable for such weakly labeled data. We investigate
various MIL assumptions in the context of COPD and show that although a concept
region with COPD-related disease patterns is present, considering the whole
distribution of lung tissue patches improves the performance. The best method
is based on averaging instances and obtains an AUC of 0.742, which is higher
than the previously reported best of 0.713 on the same dataset. Using the full
training set further increases performance to 0.776, which is significantly
higher (DeLong test) than previous results.","['Veronika Cheplygina', 'Lauge Sørensen', 'David M. J. Tax', 'Jesper Holst Pedersen', 'Marco Loog', 'Marleen de Bruijne']","['cs.CV', 'stat.ML']",2017-03-15 07:41:49+00:00
http://arxiv.org/abs/1703.04943v1,Matched bipartite block model with covariates,"Community detection or clustering is a fundamental task in the analysis of
network data. Many real networks have a bipartite structure which makes
community detection challenging. In this paper, we consider a model which
allows for matched communities in the bipartite setting, in addition to node
covariates with information about the matching. We derive a simple fast
algorithm for fitting the model based on variational inference ideas and show
its effectiveness on both simulated and real data. A variation of the model to
allow for degree-correction is also considered, in addition to a novel approach
to fitting such degree-corrected models.","['Zahra S. Razaee', 'Arash A. Amini', 'Jingyi Jessica Li']","['cs.SI', 'cs.LG', 'stat.ML']",2017-03-15 05:47:37+00:00
http://arxiv.org/abs/1703.04940v3,Resilience: A Criterion for Learning in the Presence of Arbitrary Outliers,"We introduce a criterion, resilience, which allows properties of a dataset
(such as its mean or best low rank approximation) to be robustly computed, even
in the presence of a large fraction of arbitrary additional data. Resilience is
a weaker condition than most other properties considered so far in the
literature, and yet enables robust estimation in a broader variety of settings.
We provide new information-theoretic results on robust distribution learning,
robust estimation of stochastic block models, and robust mean estimation under
bounded $k$th moments. We also provide new algorithmic results on robust
distribution learning, as well as robust mean estimation in $\ell_p$-norms.
Among our proof techniques is a method for pruning a high-dimensional
distribution with bounded $1$st moments to a stable ""core"" with bounded $2$nd
moments, which may be of independent interest.","['Jacob Steinhardt', 'Moses Charikar', 'Gregory Valiant']","['cs.LG', 'cs.AI', 'cs.CC', 'cs.CR', 'stat.ML']",2017-03-15 05:43:48+00:00
http://arxiv.org/abs/1703.04890v3,Riemannian stochastic quasi-Newton algorithm with variance reduction and its convergence analysis,"Stochastic variance reduction algorithms have recently become popular for
minimizing the average of a large, but finite number of loss functions. The
present paper proposes a Riemannian stochastic quasi-Newton algorithm with
variance reduction (R-SQN-VR). The key challenges of averaging, adding, and
subtracting multiple gradients are addressed with notions of retraction and
vector transport. We present convergence analyses of R-SQN-VR on both
non-convex and retraction-convex functions under retraction and vector
transport operators. The proposed algorithm is evaluated on the Karcher mean
computation on the symmetric positive-definite manifold and the low-rank matrix
completion on the Grassmann manifold. In all cases, the proposed algorithm
outperforms the state-of-the-art Riemannian batch and stochastic gradient
algorithms.","['Hiroyuki Kasai', 'Hiroyuki Sato', 'Bamdev Mishra']","['cs.LG', 'cs.NA', 'math.NA', 'math.OC', 'stat.ML']",2017-03-15 02:34:39+00:00
http://arxiv.org/abs/1703.04864v3,Optimization for L1-Norm Error Fitting via Data Aggregation,"We propose a data aggregation-based algorithm with monotonic convergence to a
global optimum for a generalized version of the L1-norm error fitting model
with an assumption of the fitting function. The proposed algorithm generalizes
the recent algorithm in the literature, aggregate and iterative disaggregate
(AID), which selectively solves three specific L1-norm error fitting problems.
With the proposed algorithm, any L1-norm error fitting model can be solved
optimally if it follows the form of the L1-norm error fitting problem and if
the fitting function satisfies the assumption. The proposed algorithm can also
solve multi-dimensional fitting problems with arbitrary constraints on the
fitting coefficients matrix. The generalized problem includes popular models
such as regression and the orthogonal Procrustes problem. The results of the
computational experiment show that the proposed algorithms are faster than the
state-of-the-art benchmarks for L1-norm regression subset selection and L1-norm
regression over a sphere. Further, the relative performance of the proposed
algorithm improves as data size increases.",['Young Woong Park'],['stat.ML'],2017-03-15 01:16:09+00:00
http://arxiv.org/abs/1703.04832v1,A Random Finite Set Model for Data Clustering,"The goal of data clustering is to partition data points into groups to
minimize a given objective function. While most existing clustering algorithms
treat each data point as vector, in many applications each datum is not a
vector but a point pattern or a set of points. Moreover, many existing
clustering methods require the user to specify the number of clusters, which is
not available in advance. This paper proposes a new class of models for data
clustering that addresses set-valued data as well as unknown number of
clusters, using a Dirichlet Process mixture of Poisson random finite sets. We
also develop an efficient Markov Chain Monte Carlo posterior inference
technique that can learn the number of clusters and mixture parameters
automatically from the data. Numerical studies are presented to demonstrate the
salient features of this new model, in particular its capacity to discover
extremely unbalanced clusters in data.","['Dinh Phung', 'Ba-Ngu Bo']",['stat.ML'],2017-03-14 23:35:57+00:00
http://arxiv.org/abs/1703.04823v1,Classification in biological networks with hypergraphlet kernels,"Biological and cellular systems are often modeled as graphs in which vertices
represent objects of interest (genes, proteins, drugs) and edges represent
relational ties among these objects (binds-to, interacts-with, regulates). This
approach has been highly successful owing to the theory, methodology and
software that support analysis and learning on graphs. Graphs, however, often
suffer from information loss when modeling physical systems due to their
inability to accurately represent multiobject relationships. Hypergraphs, a
generalization of graphs, provide a framework to mitigate information loss and
unify disparate graph-based methodologies. In this paper, we present a
hypergraph-based approach for modeling physical systems and formulate vertex
classification, edge classification and link prediction problems on
(hyper)graphs as instances of vertex classification on (extended, dual)
hypergraphs in a semi-supervised setting. We introduce a novel kernel method on
vertex- and edge-labeled (colored) hypergraphs for analysis and learning. The
method is based on exact and inexact (via hypergraph edit distances)
enumeration of small simple hypergraphs, referred to as hypergraphlets, rooted
at a vertex of interest. We extensively evaluate this method and show its
potential use in a positive-unlabeled setting to estimate the number of missing
and false positive links in protein-protein interaction networks.","['Jose Lugo-Martinez', 'Predrag Radivojac']","['stat.ML', 'cs.LG']",2017-03-14 23:20:17+00:00
http://arxiv.org/abs/1703.04813v4,Learned Optimizers that Scale and Generalize,"Learning to learn has emerged as an important direction for achieving
artificial intelligence. Two of the primary barriers to its adoption are an
inability to scale to larger problems and a limited ability to generalize to
new tasks. We introduce a learned gradient descent optimizer that generalizes
well to new tasks, and which has significantly reduced memory and computation
overhead. We achieve this by introducing a novel hierarchical RNN architecture,
with minimal per-parameter overhead, augmented with additional architectural
features that mirror the known structure of optimization tasks. We also develop
a meta-training ensemble of small, diverse optimization tasks capturing common
properties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM
on problems in this corpus. More importantly, it performs comparably or better
when applied to small convolutional neural networks, despite seeing no neural
networks in its meta-training set. Finally, it generalizes to train Inception
V3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps,
optimization problems that are of a vastly different scale than those it was
trained on. We release an open source implementation of the meta-training
algorithm.","['Olga Wichrowska', 'Niru Maheswaranathan', 'Matthew W. Hoffman', 'Sergio Gomez Colmenarejo', 'Misha Denil', 'Nando de Freitas', 'Jascha Sohl-Dickstein']","['cs.LG', 'cs.NE', 'stat.ML']",2017-03-14 23:05:54+00:00
http://arxiv.org/abs/1703.04782v3,Online Learning Rate Adaptation with Hypergradient Descent,"We introduce a general method for improving the convergence rate of
gradient-based optimizers that is easy to implement and works well in practice.
We demonstrate the effectiveness of the method in a range of optimization
problems by applying it to stochastic gradient descent, stochastic gradient
descent with Nesterov momentum, and Adam, showing that it significantly reduces
the need for the manual tuning of the initial learning rate for these commonly
used algorithms. Our method works by dynamically updating the learning rate
during optimization using the gradient with respect to the learning rate of the
update rule itself. Computing this ""hypergradient"" needs little additional
computation, requires only one extra copy of the original gradient to be stored
in memory, and relies upon nothing more than what is provided by reverse-mode
automatic differentiation.","['Atilim Gunes Baydin', 'Robert Cornish', 'David Martinez Rubio', 'Mark Schmidt', 'Frank Wood']","['cs.LG', 'stat.ML', '68T05', 'G.1.6; I.2.6']",2017-03-14 22:28:27+00:00
http://arxiv.org/abs/1703.04778v1,A statistical model for aggregating judgments by incorporating peer predictions,"We propose a probabilistic model to aggregate the answers of respondents
answering multiple-choice questions. The model does not assume that everyone
has access to the same information, and so does not assume that the consensus
answer is correct. Instead, it infers the most probable world state, even if
only a minority vote for it. Each respondent is modeled as receiving a signal
contingent on the actual world state, and as using this signal to both
determine their own answer and predict the answers given by others. By
incorporating respondent's predictions of others' answers, the model infers
latent parameters corresponding to the prior over world states and the
probability of different signals being received in all possible world states,
including counterfactual ones. Unlike other probabilistic models for
aggregation, our model applies to both single and multiple questions, in which
case it estimates each respondent's expertise. The model shows good
performance, compared to a number of other probabilistic models, on data from
seven studies covering different types of expertise.","['John McCoy', 'Drazen Prelec']",['stat.ML'],2017-03-14 22:23:17+00:00
http://arxiv.org/abs/1703.04775v1,Discriminate-and-Rectify Encoders: Learning from Image Transformation Sets,"The complexity of a learning task is increased by transformations in the
input space that preserve class identity. Visual object recognition for example
is affected by changes in viewpoint, scale, illumination or planar
transformations. While drastically altering the visual appearance, these
changes are orthogonal to recognition and should not be reflected in the
representation or feature encoding used for learning. We introduce a framework
for weakly supervised learning of image embeddings that are robust to
transformations and selective to the class distribution, using sets of
transforming examples (orbit sets), deep parametrizations and a novel
orbit-based loss. The proposed loss combines a discriminative, contrastive part
for orbits with a reconstruction error that learns to rectify orbit
transformations. The learned embeddings are evaluated in distance metric-based
tasks, such as one-shot classification under geometric transformations, as well
as face verification and retrieval under more realistic visual variability. Our
results suggest that orbit sets, suitably computed or observed, can be used for
efficient, weakly-supervised learning of semantically relevant image
embeddings.","['Andrea Tacchetti', 'Stephen Voinea', 'Georgios Evangelopoulos']","['cs.CV', 'cs.LG', 'stat.ML']",2017-03-14 22:21:48+00:00
http://arxiv.org/abs/1703.04757v3,Separation of time scales and direct computation of weights in deep neural networks,"Artificial intelligence is revolutionizing our lives at an ever increasing
pace. At the heart of this revolution is the recent advancements in deep neural
networks (DNN), learning to perform sophisticated, high-level tasks. However,
training DNNs requires massive amounts of data and is very computationally
intensive. Gaining analytical understanding of the solutions found by DNNs can
help us devise more efficient training algorithms, replacing the commonly used
mthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and
show that, indeed, direct computation of the solutions is possible in many
cases. We show that a high performing setup used in DNNs introduces a
separation of time-scales in the training dynamics, allowing SGD to train
layers from the lowest (closest to input) to the highest. We then show that for
each layer, the distribution of solutions found by SGD can be estimated using a
class-based principal component analysis (PCA) of the layer's input. This
finding allows us to forgo SGD entirely and directly derive the DNN parameters
using this class-based PCA, which can be well estimated using significantly
less data than SGD. We implement these results on image datasets MNIST, CIFAR10
and CIFAR100 and find that, in fact, layers derived using our class-based PCA
perform comparable or superior to neural networks of the same size and
architecture trained using SGD. We also confirm that the class-based PCA often
converges using a fraction of the data required for SGD. Thus, using our method
training time can be reduced both by requiring less training data than SGD, and
by eliminating layers in the costly backpropagation step of the training.","['Nima Dehmamy', 'Neda Rohani', 'Aggelos Katsaggelos']","['cs.LG', 'physics.data-an', 'stat.ML']",2017-03-14 22:13:41+00:00
http://arxiv.org/abs/1703.04730v3,Understanding Black-box Predictions via Influence Functions,"How can we explain the predictions of a black-box model? In this paper, we
use influence functions -- a classic technique from robust statistics -- to
trace a model's prediction through the learning algorithm and back to its
training data, thereby identifying training points most responsible for a given
prediction. To scale up influence functions to modern machine learning
settings, we develop a simple, efficient implementation that requires only
oracle access to gradients and Hessian-vector products. We show that even on
non-convex and non-differentiable models where the theory breaks down,
approximations to influence functions can still provide valuable information.
On linear models and convolutional neural networks, we demonstrate that
influence functions are useful for multiple purposes: understanding model
behavior, debugging models, detecting dataset errors, and even creating
visually-indistinguishable training-set attacks.","['Pang Wei Koh', 'Percy Liang']","['stat.ML', 'cs.AI', 'cs.LG']",2017-03-14 21:07:01+00:00
http://arxiv.org/abs/1703.06912v1,Application of backpropagation neural networks to both stages of fingerprinting based WIPS,"We propose a scheme to employ backpropagation neural networks (BPNNs) for
both stages of fingerprinting-based indoor positioning using WLAN/WiFi signal
strengths (FWIPS): radio map construction during the offline stage, and
localization during the online stage. Given a training radio map (TRM), i.e., a
set of coordinate vectors and associated WLAN/WiFi signal strengths of the
available access points, a BPNN can be trained to output the expected signal
strengths for any input position within the region of interest (BPNN-RM). This
can be used to provide a continuous representation of the radio map and to
filter, densify or decimate a discrete radio map. Correspondingly, the TRM can
also be used to train another BPNN to output the expected position within the
region of interest for any input vector of recorded signal strengths and thus
carry out localization (BPNN-LA).Key aspects of the design of such artificial
neural networks for a specific application are the selection of design
parameters like the number of hidden layers and nodes within the network, and
the training procedure. Summarizing extensive numerical simulations, based on
real measurements in a testbed, we analyze the impact of these design choices
on the performance of the BPNN and compare the results in particular to those
obtained using the $k$ nearest neighbors ($k$NN) and weighted $k$ nearest
neighbors approaches to FWIPS.","['Caifa Zhou', 'Andreas Wieser']","['stat.ML', 'cs.LG']",2017-03-14 20:30:50+00:00
http://arxiv.org/abs/1703.04697v1,On the benefits of output sparsity for multi-label classification,"The multi-label classification framework, where each observation can be
associated with a set of labels, has generated a tremendous amount of attention
over recent years. The modern multi-label problems are typically large-scale in
terms of number of observations, features and labels, and the amount of labels
can even be comparable with the amount of observations. In this context,
different remedies have been proposed to overcome the curse of dimensionality.
In this work, we aim at exploiting the output sparsity by introducing a new
loss, called the sparse weighted Hamming loss. This proposed loss can be seen
as a weighted version of classical ones, where active and inactive labels are
weighted separately. Leveraging the influence of sparsity in the loss function,
we provide improved generalization bounds for the empirical risk minimizer, a
suitable property for large-scale problems. For this new loss, we derive rates
of convergence linear in the underlying output-sparsity rather than linear in
the number of labels. In practice, minimizing the associated risk can be
performed efficiently by using convex surrogates and modern convex optimization
algorithms. We provide experiments on various real-world datasets demonstrating
the pertinence of our approach when compared to non-weighted techniques.","['Evgenii Chzhen', 'Christophe Denis', 'Mohamed Hebiri', 'Joseph Salmon']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",2017-03-14 20:19:08+00:00
http://arxiv.org/abs/1703.04691v5,Conditional Time Series Forecasting with Convolutional Neural Networks,"We present a method for conditional time series forecasting based on an
adaptation of the recent deep convolutional WaveNet architecture. The proposed
network contains stacks of dilated convolutions that allow it to access a broad
range of history when forecasting, a ReLU activation function and conditioning
is performed by applying multiple convolutional filters in parallel to separate
time series which allows for the fast processing of data and the exploitation
of the correlation structure between the multivariate time series. We test and
analyze the performance of the convolutional network both unconditionally as
well as conditionally for financial time series forecasting using the S&P500,
the volatility index, the CBOE interest rate and several exchange rates and
extensively compare it to the performance of the well-known autoregressive
model and a long-short term memory network. We show that a convolutional
network is well-suited for regression-type problems and is able to effectively
learn dependencies in and between the series without the need for long
historical time series, is a time-efficient and easy to implement alternative
to recurrent-type networks and tends to outperform linear and recurrent models.","['Anastasia Borovykh', 'Sander Bohte', 'Cornelis W. Oosterlee']",['stat.ML'],2017-03-14 20:07:12+00:00
http://arxiv.org/abs/1703.04599v3,Generalized Self-Concordant Functions: A Recipe for Newton-Type Methods,"We study the smooth structure of convex functions by generalizing a powerful
concept so-called self-concordance introduced by Nesterov and Nemirovskii in
the early 1990s to a broader class of convex functions, which we call
generalized self-concordant functions. This notion allows us to develop a
unified framework for designing Newton-type methods to solve convex optimiza-
tion problems. The proposed theory provides a mathematical tool to analyze both
local and global convergence of Newton-type methods without imposing
unverifiable assumptions as long as the un- derlying functionals fall into our
generalized self-concordant function class. First, we introduce the class of
generalized self-concordant functions, which covers standard self-concordant
functions as a special case. Next, we establish several properties and key
estimates of this function class, which can be used to design numerical
methods. Then, we apply this theory to develop several Newton-type methods for
solving a class of smooth convex optimization problems involving the
generalized self- concordant functions. We provide an explicit step-size for
the damped-step Newton-type scheme which can guarantee a global convergence
without performing any globalization strategy. We also prove a local quadratic
convergence of this method and its full-step variant without requiring the
Lipschitz continuity of the objective Hessian. Then, we extend our result to
develop proximal Newton-type methods for a class of composite convex
minimization problems involving generalized self-concordant functions. We also
achieve both global and local convergence without additional assumption.
Finally, we verify our theoretical results via several numerical examples, and
compare them with existing methods.","['Tianxiao Sun', 'Quoc Tran-Dinh']","['math.OC', 'stat.ML']",2017-03-14 17:23:02+00:00
http://arxiv.org/abs/1703.04455v6,Multivariate Gaussian and Student$-t$ Process Regression for Multi-output Prediction,"Gaussian process model for vector-valued function has been shown to be useful
for multi-output prediction. The existing method for this model is to
re-formulate the matrix-variate Gaussian distribution as a multivariate normal
distribution. Although it is effective in many cases, re-formulation is not
always workable and is difficult to apply to other distributions because not
all matrix-variate distributions can be transformed to respective multivariate
distributions, such as the case for matrix-variate Student$-t$ distribution. In
this paper, we propose a unified framework which is used not only to introduce
a novel multivariate Student$-t$ process regression model (MV-TPR) for
multi-output prediction, but also to reformulate the multivariate Gaussian
process regression (MV-GPR) that overcomes some limitations of the existing
methods. Both MV-GPR and MV-TPR have closed-form expressions for the marginal
likelihoods and predictive distributions under this unified framework and thus
can adopt the same optimization approaches as used in the conventional GPR. The
usefulness of the proposed methods is illustrated through several simulated and
real data examples. In particular, we verify empirically that MV-TPR has
superiority for the datasets considered, including air quality prediction and
bike rent prediction. At last, the proposed methods are shown to produce
profitable investment strategies in the stock markets.","['Zexun Chen', 'Bo Wang', 'Alexander N. Gorban']",['stat.ML'],2017-03-13 15:43:00+00:00
http://arxiv.org/abs/1703.04389v3,Bayesian Optimization with Gradients,"Bayesian optimization has been successful at global optimization of
expensive-to-evaluate multimodal objective functions. However, unlike most
optimization methods, Bayesian optimization typically does not use derivative
information. In this paper we show how Bayesian optimization can exploit
derivative information to decrease the number of objective function evaluations
required for good performance. In particular, we develop a novel Bayesian
optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for
which we show one-step Bayes-optimality, asymptotic consistency, and greater
one-step value of information than is possible in the derivative-free setting.
Our procedure accommodates noisy and incomplete derivative information, comes
in both sequential and batch forms, and can optionally reduce the computational
cost of inference through automatically selected retention of a single
directional derivative. We also compute the d-KG acquisition function and its
gradient using a novel fast discretization-free technique. We show d-KG
provides state-of-the-art performance compared to a wide range of optimization
procedures with and without gradients, on benchmarks including logistic
regression, deep learning, kernel learning, and k-nearest neighbors.","['Jian Wu', 'Matthias Poloczek', 'Andrew Gordon Wilson', 'Peter I. Frazier']","['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']",2017-03-13 13:45:13+00:00
http://arxiv.org/abs/1703.04379v4,Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks,"Minimizing non-convex and high-dimensional objective functions is
challenging, especially when training modern deep neural networks. In this
paper, a novel approach is proposed which divides the training process into two
consecutive phases to obtain better generalization performance: Bayesian
sampling and stochastic optimization. The first phase is to explore the energy
landscape and to capture the ""fat"" modes; and the second one is to fine-tune
the parameter learned from the first phase. In the Bayesian learning phase, we
apply continuous tempering and stochastic approximation into the Langevin
dynamics to create an efficient and effective sampler, in which the temperature
is adjusted automatically according to the designed ""temperature dynamics"".
These strategies can overcome the challenge of early trapping into bad local
minima and have achieved remarkable improvements in various types of neural
networks as shown in our theoretical analysis and empirical experiments.","['Nanyang Ye', 'Zhanxing Zhu', 'Rafal K. Mantiuk']","['cs.LG', 'stat.ML']",2017-03-13 13:27:56+00:00
http://arxiv.org/abs/1703.04335v2,Practical Bayesian Optimization for Variable Cost Objectives,"We propose a novel Bayesian Optimization approach for black-box functions
with an environmental variable whose value determines the tradeoff between
evaluation cost and the fidelity of the evaluations. Further, we use a novel
approach to sampling support points, allowing faster construction of the
acquisition function. This allows us to achieve optimization with lower
overheads than previous approaches and is implemented for a more general class
of problem. We show this approach to be effective on synthetic and real world
benchmark problems.","['Mark McLeod', 'Michael A. Osborne', 'Stephen J. Roberts']",['stat.ML'],2017-03-13 11:19:52+00:00
http://arxiv.org/abs/1703.04334v1,Probabilistic Matching: Causal Inference under Measurement Errors,"The abundance of data produced daily from large variety of sources has
boosted the need of novel approaches on causal inference analysis from
observational data. Observational data often contain noisy or missing entries.
Moreover, causal inference studies may require unobserved high-level
information which needs to be inferred from other observed attributes. In such
cases, inaccuracies of the applied inference methods will result in noisy
outputs. In this study, we propose a novel approach for causal inference when
one or more key variables are noisy. Our method utilizes the knowledge about
the uncertainty of the real values of key variables in order to reduce the bias
induced by noisy measurements. We evaluate our approach in comparison with
existing methods both on simulated and real scenarios and we demonstrate that
our method reduces the bias and avoids false causal inference conclusions in
most cases.","['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']","['stat.ME', 'stat.CO', 'stat.ML']",2017-03-13 11:19:28+00:00
http://arxiv.org/abs/1703.04200v3,Continual Learning Through Synaptic Intelligence,"While deep learning has led to remarkable advances across diverse
applications, it struggles in domains where the data distribution changes over
the course of learning. In stark contrast, biological neural networks
continually adapt to changing domains, possibly by leveraging complex molecular
machinery to solve many tasks simultaneously. In this study, we introduce
intelligent synapses that bring some of this biological complexity into
artificial neural networks. Each synapse accumulates task relevant information
over time, and exploits this information to rapidly store new memories without
forgetting old ones. We evaluate our approach on continual learning of
classification tasks, and show that it dramatically reduces forgetting while
maintaining computational efficiency.","['Friedemann Zenke', 'Ben Poole', 'Surya Ganguli']","['cs.LG', 'q-bio.NC', 'stat.ML']",2017-03-13 00:02:48+00:00
http://arxiv.org/abs/1703.04145v1,Robustness from structure: Inference with hierarchical spiking networks on analog neuromorphic hardware,"How spiking networks are able to perform probabilistic inference is an
intriguing question, not only for understanding information processing in the
brain, but also for transferring these computational principles to neuromorphic
silicon circuits. A number of computationally powerful spiking network models
have been proposed, but most of them have only been tested, under ideal
conditions, in software simulations. Any implementation in an analog, physical
system, be it in vivo or in silico, will generally lead to distorted dynamics
due to the physical properties of the underlying substrate. In this paper, we
discuss several such distortive effects that are difficult or impossible to
remove by classical calibration routines or parameter training. We then argue
that hierarchical networks of leaky integrate-and-fire neurons can offer the
required robustness for physical implementation and demonstrate this with both
software simulations and emulation on an accelerated analog neuromorphic
device.","['Mihai A. Petrovici', 'Anna Schroeder', 'Oliver Breitwieser', 'Andreas Grübl', 'Johannes Schemmel', 'Karlheinz Meier']","['q-bio.NC', 'cs.NE', 'stat.ML']",2017-03-12 17:29:11+00:00
http://arxiv.org/abs/1703.04140v1,Multiscale Hierarchical Convolutional Networks,"Deep neural network algorithms are difficult to analyze because they lack
structure allowing to understand the properties of underlying transforms and
invariants. Multiscale hierarchical convolutional networks are structured deep
convolutional networks where layers are indexed by progressively higher
dimensional attributes, which are learned from training data. Each new layer is
computed with multidimensional convolutions along spatial and attribute
variables. We introduce an efficient implementation of such networks where the
dimensionality is progressively reduced by averaging intermediate layers along
attribute indices. Hierarchical networks are tested on CIFAR image data bases
where they obtain comparable precisions to state of the art networks, with much
fewer parameters. We study some properties of the attributes learned from these
databases.","['Jörn-Henrik Jacobsen', 'Edouard Oyallon', 'Stéphane Mallat', 'Arnold W. M. Smeulders']","['cs.LG', 'stat.ML']",2017-03-12 16:29:44+00:00
http://arxiv.org/abs/1703.04082v2,Sequential Local Learning for Latent Graphical Models,"Learning parameters of latent graphical models (GM) is inherently much harder
than that of no-latent ones since the latent variables make the corresponding
log-likelihood non-concave. Nevertheless, expectation-maximization schemes are
popularly used in practice, but they are typically stuck in local optima. In
the recent years, the method of moments have provided a refreshing angle for
resolving the non-convex issue, but it is applicable to a quite limited class
of latent GMs. In this paper, we aim for enhancing its power via enlarging such
a class of latent GMs. To this end, we introduce two novel concepts, coined
marginalization and conditioning, which can reduce the problem of learning a
larger GM to that of a smaller one. More importantly, they lead to a sequential
learning framework that repeatedly increases the learning portion of given
latent GM, and thus covers a significantly broader and more complicated class
of loopy latent GMs which include convolutional and random regular models.","['Sejun Park', 'Eunho Yang', 'Jinwoo Shin']","['cs.LG', 'stat.ML']",2017-03-12 08:18:11+00:00
http://arxiv.org/abs/1703.04081v2,Feature overwriting as a finite mixture process: Evidence from comprehension data,"The ungrammatical sentence ""The key to the cabinets are on the table"" is
known to lead to an illusion of grammaticality. As discussed in the
meta-analysis by Jaeger et al., 2017, faster reading times are observed at the
verb are in the agreement-attraction sentence above compared to the equally
ungrammatical sentence ""The key to the cabinet are on the table"". One
explanation for this facilitation effect is the feature percolation account:
the plural feature on cabinets percolates up to the head noun key, leading to
the illusion. An alternative account is in terms of cue-based retrieval (Lewis
& Vasishth, 2005), which assumes that the non-subject noun cabinets is
misretrieved due to a partial feature-match when a dependency completion
process at the auxiliary initiates a memory access for a subject with plural
marking. We present evidence for yet another explanation for the observed
facilitation. Because the second sentence has two nouns with identical number,
it is possible that these are, in some proportion of trials, more difficult to
keep distinct, leading to slower reading times at the verb in the first
sentence above; this is the feature overwriting account of Nairne, 1990. We
show that the feature overwriting proposal can be implemented as a finite
mixture process. We reanalysed ten published data-sets, fitting hierarchical
Bayesian mixture models to these data assuming a two-mixture distribution. We
show that in nine out of the ten studies, a mixture distribution corresponding
to feature overwriting furnishes a superior fit over both the feature
percolation and the cue-based retrieval accounts.","['Shravan Vasishth', 'Lena A. Jäger', 'Bruno Nicenboim']","['stat.ML', 'cs.CL', 'stat.AP']",2017-03-12 08:11:29+00:00
http://arxiv.org/abs/1703.04078v1,Prostate Cancer Diagnosis using Deep Learning with 3D Multiparametric MRI,"A novel deep learning architecture (XmasNet) based on convolutional neural
networks was developed for the classification of prostate cancer lesions, using
the 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end
training was performed for XmasNet, with data augmentation done through 3D
rotation and slicing, in order to incorporate the 3D information of the lesion.
XmasNet outperformed traditional machine learning models based on engineered
features, for both train and test data. For the test data, XmasNet outperformed
69 methods from 33 participating groups and achieved the second highest AUC
(0.84) in the PROSTATEx challenge. This study shows the great potential of deep
learning for cancer imaging.","['Saifeng Liu', 'Huaixiu Zheng', 'Yesu Feng', 'Wei Li']","['cs.CV', 'stat.ML']",2017-03-12 07:19:55+00:00
http://arxiv.org/abs/1703.04070v2,Prediction and Control with Temporal Segment Models,"We introduce a method for learning the dynamics of complex nonlinear systems
based on deep generative models over temporal segments of states and actions.
Unlike dynamics models that operate over individual discrete timesteps, we
learn the distribution over future state trajectories conditioned on past
state, past action, and planned future action trajectories, as well as a latent
prior over action trajectories. Our approach is based on convolutional
autoregressive models and variational autoencoders. It makes stable and
accurate predictions over long horizons for complex, stochastic systems,
effectively expressing uncertainty and modeling the effects of collisions,
sensory noise, and action delays. The learned dynamics model and action prior
can be used for end-to-end, fully differentiable trajectory optimization and
model-based policy optimization, which we use to evaluate the performance and
sample-efficiency of our method.","['Nikhil Mishra', 'Pieter Abbeel', 'Igor Mordatch']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2017-03-12 04:59:15+00:00
http://arxiv.org/abs/1703.04046v2,DeepSleepNet: a Model for Automatic Sleep Stage Scoring based on Raw Single-Channel EEG,"The present study proposes a deep learning model, named DeepSleepNet, for
automatic sleep stage scoring based on raw single-channel EEG. Most of the
existing methods rely on hand-engineered features which require prior knowledge
of sleep analysis. Only a few of them encode the temporal information such as
transition rules, which is important for identifying the next sleep stages,
into the extracted features. In the proposed model, we utilize Convolutional
Neural Networks to extract time-invariant features, and bidirectional-Long
Short-Term Memory to learn transition rules among sleep stages automatically
from EEG epochs. We implement a two-step training algorithm to train our model
efficiently. We evaluated our model using different single-channel EEGs
(F4-EOG(Left), Fpz-Cz and Pz-Oz) from two public sleep datasets, that have
different properties (e.g., sampling rate) and scoring standards (AASM and
R&K). The results showed that our model achieved similar overall accuracy and
macro F1-score (MASS: 86.2%-81.7, Sleep-EDF: 82.0%-76.9) compared to the
state-of-the-art methods (MASS: 85.9%-80.5, Sleep-EDF: 78.9%-73.7) on both
datasets. This demonstrated that, without changing the model architecture and
the training algorithm, our model could automatically learn features for sleep
stage scoring from different raw single-channel EEGs from different datasets
without utilizing any hand-engineered features.","['Akara Supratak', 'Hao Dong', 'Chao Wu', 'Yike Guo']",['stat.ML'],2017-03-12 00:15:32+00:00
http://arxiv.org/abs/1703.04025v2,Learning Large-Scale Bayesian Networks with the sparsebn Package,"Learning graphical models from data is an important problem with wide
applications, ranging from genomics to the social sciences. Nowadays datasets
often have upwards of thousands---sometimes tens or hundreds of thousands---of
variables and far fewer samples. To meet this challenge, we have developed a
new R package called sparsebn for learning the structure of large, sparse
graphical models with a focus on Bayesian networks. While there are many
existing software packages for this task, this package focuses on the unique
setting of learning large networks from high-dimensional data, possibly with
interventions. As such, the methods provided place a premium on scalability and
consistency in a high-dimensional setting. Furthermore, in the presence of
interventions, the methods implemented here achieve the goal of learning a
causal network from data. Additionally, the sparsebn package is fully
compatible with existing software packages for network analysis.","['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",2017-03-11 20:07:06+00:00
http://arxiv.org/abs/1703.03888v1,Segmentation of skin lesions based on fuzzy classification of pixels and histogram thresholding,"This paper proposes an innovative method for segmentation of skin lesions in
dermoscopy images developed by the authors, based on fuzzy classification of
pixels and histogram thresholding.","['Jose Luis Garcia-Arroyo', 'Begonya Garcia-Zapirain']","['cs.CV', 'cs.AI', 'stat.ML']",2017-03-11 01:18:14+00:00
http://arxiv.org/abs/1703.03869v1,Deep Learning in Customer Churn Prediction: Unsupervised Feature Learning on Abstract Company Independent Feature Vectors,"As companies increase their efforts in retaining customers, being able to
predict accurately ahead of time, whether a customer will churn in the
foreseeable future is an extremely powerful tool for any marketing team. The
paper describes in depth the application of Deep Learning in the problem of
churn prediction. Using abstract feature vectors, that can generated on any
subscription based company's user event logs, the paper proves that through the
use of the intrinsic property of Deep Neural Networks (learning secondary
features in an unsupervised manner), the complete pipeline can be applied to
any subscription based company with extremely good churn predictive
performance. Furthermore the research documented in the paper was performed for
Framed Data (a company that sells churn prediction as a service for other
companies) in conjunction with the Data Science Institute at Lancaster
University, UK. This paper is the intellectual property of Framed Data.","['Philip Spanoudes', 'Thomson Nguyen']","['cs.LG', 'stat.ML']",2017-03-10 23:26:33+00:00
http://arxiv.org/abs/1703.03864v2,Evolution Strategies as a Scalable Alternative to Reinforcement Learning,"We explore the use of Evolution Strategies (ES), a class of black box
optimization algorithms, as an alternative to popular MDP-based RL techniques
such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show
that ES is a viable solution strategy that scales extremely well with the
number of CPUs available: By using a novel communication strategy based on
common random numbers, our ES implementation only needs to communicate scalars,
making it possible to scale to over a thousand parallel workers. This allows us
to solve 3D humanoid walking in 10 minutes and obtain competitive results on
most Atari games after one hour of training. In addition, we highlight several
advantages of ES as a black box optimization technique: it is invariant to
action frequency and delayed rewards, tolerant of extremely long horizons, and
does not need temporal discounting or value function approximation.","['Tim Salimans', 'Jonathan Ho', 'Xi Chen', 'Szymon Sidor', 'Ilya Sutskever']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.NE']",2017-03-10 23:02:19+00:00
http://arxiv.org/abs/1703.03863v2,Tuning Over-Relaxed ADMM,"The framework of Integral Quadratic Constraints (IQC) reduces the computation
of upper bounds on the convergence rate of several optimization algorithms to a
semi-definite program (SDP). In the case of over-relaxed Alternating Direction
Method of Multipliers (ADMM), an explicit and closed form solution to this SDP
was derived in our recent work [1]. The purpose of this paper is twofold.
First, we summarize these results. Second, we explore one of its consequences
which allows us to obtain general and simple formulas for optimal parameter
selection. These results are valid for arbitrary strongly convex objective
functions.","['Guilherme França', 'José Bento']","['stat.ML', 'math.DS', 'math.OC']",2017-03-10 22:47:31+00:00
http://arxiv.org/abs/1703.03862v4,Joint Embedding of Graphs,"Feature extraction and dimension reduction for networks is critical in a wide
variety of domains. Efficiently and accurately learning features for multiple
graphs has important applications in statistical inference on graphs. We
propose a method to jointly embed multiple undirected graphs. Given a set of
graphs, the joint embedding method identifies a linear subspace spanned by rank
one symmetric matrices and projects adjacency matrices of graphs into this
subspace. The projection coefficients can be treated as features of the graphs,
while the embedding components can represent vertex features. We also propose a
random graph model for multiple graphs that generalizes other classical models
for graphs. We show through theory and numerical experiments that under the
model, the joint embedding method produces estimates of parameters with small
errors. Via simulation experiments, we demonstrate that the joint embedding
method produces features which lead to state of the art performance in
classifying graphs. Applying the joint embedding method to human brain graphs,
we find it extracts interpretable features with good prediction accuracy in
different tasks.","['Shangsi Wang', 'Jesús Arroyo', 'Joshua T. Vogelstein', 'Carey E. Priebe']","['stat.AP', 'cs.LG', 'stat.ML']",2017-03-10 22:46:09+00:00
