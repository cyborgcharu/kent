id,title,abstract,authors,categories,date
http://arxiv.org/abs/1505.00294v1,Monotonous (Semi-)Nonnegative Matrix Factorization,"Nonnegative matrix factorization (NMF) factorizes a non-negative matrix into
product of two non-negative matrices, namely a signal matrix and a mixing
matrix. NMF suffers from the scale and ordering ambiguities. Often, the source
signals can be monotonous in nature. For example, in source separation problem,
the source signals can be monotonously increasing or decreasing while the
mixing matrix can have nonnegative entries. NMF methods may not be effective
for such cases as it suffers from the ordering ambiguity. This paper proposes
an approach to incorporate notion of monotonicity in NMF, labeled as monotonous
NMF. An algorithm based on alternating least-squares is proposed for recovering
monotonous signals from a data matrix. Further, the assumption on mixing matrix
is relaxed to extend monotonous NMF for data matrix with real numbers as
entries. The approach is illustrated using synthetic noisy data. The results
obtained by monotonous NMF are compared with standard NMF algorithms in the
literature, and it is shown that monotonous NMF estimates source signals well
in comparison to standard NMF algorithms when the underlying sources signals
are monotonous.","['Nirav Bhatt', 'Arun Ayyar']","['cs.LG', 'stat.ML', 'I.2']",2015-05-01 23:58:17+00:00
http://arxiv.org/abs/1505.00274v2,Stick-Breaking Policy Learning in Dec-POMDPs,"Expectation maximization (EM) has recently been shown to be an efficient
algorithm for learning finite-state controllers (FSCs) in large decentralized
POMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often
converge to maxima that are far from optimal. This paper considers a
variable-size FSC to represent the local policy of each agent. These
variable-size FSCs are constructed using a stick-breaking prior, leading to a
new framework called \emph{decentralized stick-breaking policy representation}
(Dec-SBPR). This approach learns the controller parameters with a variational
Bayesian algorithm without having to assume that the Dec-POMDP model is
available. The performance of Dec-SBPR is demonstrated on several benchmark
problems, showing that the algorithm scales to large problems while
outperforming other state-of-the-art methods.","['Miao Liu', 'Christopher Amato', 'Xuejun Liao', 'Lawrence Carin', 'Jonathan P. How']","['cs.AI', 'cs.SY', 'stat.ML']",2015-05-01 20:29:27+00:00
http://arxiv.org/abs/1504.08291v5,Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?,"Three important properties of a classification machinery are: (i) the system
preserves the core information of the input data; (ii) the training examples
convey information about unseen data; and (iii) the system is able to treat
differently points from different classes. In this work we show that these
fundamental properties are satisfied by the architecture of deep neural
networks. We formally prove that these networks with random Gaussian weights
perform a distance-preserving embedding of the data, with a special treatment
for in-class and out-of-class data. Similar points at the input of the network
are likely to have a similar output. The theoretical analysis of deep networks
here presented exploits tools used in the compressed sensing and dictionary
learning literature, thereby making a formal connection between these important
topics. The derived results allow drawing conclusions on the metric learning
properties of the network and their relation to its structure, as well as
providing bounds on the required size of the training set such that the
training examples would represent faithfully the unseen data. The results are
validated with state-of-the-art trained networks.","['Raja Giryes', 'Guillermo Sapiro', 'Alex M. Bronstein']","['cs.NE', 'cs.LG', 'stat.ML', '62M45', 'I.5.1']",2015-04-30 16:14:52+00:00
http://arxiv.org/abs/1504.08219v1,Hierarchical Subquery Evaluation for Active Learning on a Graph,"To train good supervised and semi-supervised object classifiers, it is
critical that we not waste the time of the human experts who are providing the
training labels. Existing active learning strategies can have uneven
performance, being efficient on some datasets but wasteful on others, or
inconsistent just between runs on the same dataset. We propose perplexity based
graph construction and a new hierarchical subquery evaluation algorithm to
combat this variability, and to release the potential of Expected Error
Reduction.
  Under some specific circumstances, Expected Error Reduction has been one of
the strongest-performing informativeness criteria for active learning. Until
now, it has also been prohibitively costly to compute for sizeable datasets. We
demonstrate our highly practical algorithm, comparing it to other active
learning measures on classification datasets that vary in sparsity,
dimensionality, and size. Our algorithm is consistent over multiple runs and
achieves high accuracy, while querying the human expert for labels at a
frequency that matches their desired time budget.","['Oisin Mac Aodha', 'Neill D. F. Campbell', 'Jan Kautz', 'Gabriel J. Brostow']","['cs.CV', 'cs.LG', 'stat.ML']",2015-04-30 13:35:59+00:00
http://arxiv.org/abs/1504.08215v1,Lateral Connections in Denoising Autoencoders Support Supervised Learning,"We show how a deep denoising autoencoder with lateral connections can be used
as an auxiliary unsupervised learning task to support supervised learning. The
proposed model is trained to minimize simultaneously the sum of supervised and
unsupervised cost functions by back-propagation, avoiding the need for
layer-wise pretraining. It improves the state of the art significantly in the
permutation-invariant MNIST classification task.","['Antti Rasmus', 'Harri Valpola', 'Tapani Raiko']","['cs.LG', 'cs.NE', 'stat.ML']",2015-04-30 13:26:46+00:00
http://arxiv.org/abs/1504.08196v3,On the estimation of initial conditions in kernel-based system identification,"Recent developments in system identification have brought attention to
regularized kernel-based methods, where, adopting the recently introduced
stable spline kernel, prior information on the unknown process is enforced.
This reduces the variance of the estimates and thus makes kernel-based methods
particularly attractive when few input-output data samples are available. In
such cases however, the influence of the system initial conditions may have a
significant impact on the output dynamics. In this paper, we specifically
address this point. We propose three methods that deal with the estimation of
initial conditions using different types of information. The methods consist in
various mixed maximum likelihood--a posteriori estimators which estimate the
initial conditions and tune the hyperparameters characterizing the stable
spline kernel. To solve the related optimization problems, we resort to the
expectation-maximization method, showing that the solutions can be attained by
iterating among simple update steps. Numerical experiments show the advantages,
in terms of accuracy in reconstructing the system impulse response, of the
proposed strategies, compared to other kernel-based schemes not accounting for
the effect initial conditions.","['Riccardo Sven Risuleo', 'Giulio Bottegal', 'Håkan Hjalmarsson']","['cs.SY', 'stat.ML']",2015-04-30 12:41:49+00:00
http://arxiv.org/abs/1504.08190v2,A new kernel-based approach for overparameterized Hammerstein system identification,"In this paper we propose a new identification scheme for Hammerstein systems,
which are dynamic systems consisting of a static nonlinearity and a linear
time-invariant dynamic system in cascade. We assume that the nonlinear function
can be described as a linear combination of $p$ basis functions. We reconstruct
the $p$ coefficients of the nonlinearity together with the first $n$ samples of
the impulse response of the linear system by estimating an $np$-dimensional
overparameterized vector, which contains all the combinations of the unknown
variables. To avoid high variance in these estimates, we adopt a regularized
kernel-based approach and, in particular, we introduce a new kernel tailored
for Hammerstein system identification. We show that the resulting scheme
provides an estimate of the overparameterized vector that can be uniquely
decomposed as the combination of an impulse response and $p$ coefficients of
the static nonlinearity. We also show, through several numerical experiments,
that the proposed method compares very favorably with two standard methods for
Hammerstein system identification.","['Riccardo Sven Risuleo', 'Giulio Bottegal', 'Håkan Hjalmarsson']","['cs.SY', 'stat.ML']",2015-04-30 12:24:38+00:00
http://arxiv.org/abs/1504.08142v2,Semi-Orthogonal Multilinear PCA with Relaxed Start,"Principal component analysis (PCA) is an unsupervised method for learning
low-dimensional features with orthogonal projections. Multilinear PCA methods
extend PCA to deal with multidimensional data (tensors) directly via
tensor-to-tensor projection or tensor-to-vector projection (TVP). However,
under the TVP setting, it is difficult to develop an effective multilinear PCA
method with the orthogonality constraint. This paper tackles this problem by
proposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA
learns low-dimensional features directly from tensors via TVP by imposing the
orthogonality constraint in only one mode. This formulation results in more
captured variance and more learned features than full orthogonality. For better
generalization, we further introduce a relaxed start (RS) strategy to get
SO-MPCA-RS by fixing the starting projection vectors, which increases the bias
and reduces the variance of the learning model. Experiments on both face (2D)
and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing
algorithms on the whole, and the relaxed start strategy is also effective for
other TVP-based PCA methods.","['Qiquan Shi', 'Haiping Lu']","['stat.ML', 'cs.CV', 'cs.LG', 'I.2.6']",2015-04-30 09:40:09+00:00
http://arxiv.org/abs/1504.07829v2,Market forecasting using Hidden Markov Models,"Working on the daily closing prices and logreturns, in this paper we deal
with the use of Hidden Markov Models (HMMs) to forecast the price of the
EUR/USD Futures. The aim of our work is to understand how the HMMs describe
different financial time series depending on their structure. Subsequently, we
analyse the forecasting methods exposed in the previous literature, putting on
evidence their pros and cons.","['Sara Rebagliati', 'Emanuela Sasso', 'Samuele Soraggi']","['stat.ML', 'cs.LG', '91B84']",2015-04-29 12:21:49+00:00
http://arxiv.org/abs/1504.07676v2,Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers,"There is a large literature explaining why AdaBoost is a successful
classifier. The literature on AdaBoost focuses on classifier margins and
boosting's interpretation as the optimization of an exponential likelihood
function. These existing explanations, however, have been pointed out to be
incomplete. A random forest is another popular ensemble method for which there
is substantially less explanation in the literature. We introduce a novel
perspective on AdaBoost and random forests that proposes that the two
algorithms work for similar reasons. While both classifiers achieve similar
predictive accuracy, random forests cannot be conceived as a direct
optimization procedure. Rather, random forests is a self-averaging,
interpolating algorithm which creates what we denote as a ""spikey-smooth""
classifier, and we view AdaBoost in the same light. We conjecture that both
AdaBoost and random forests succeed because of this mechanism. We provide a
number of examples and some theoretical justification to support this
explanation. In the process, we question the conventional wisdom that suggests
that boosting algorithms for classification require regularization or early
stopping and should be limited to low complexity classes of learners, such as
decision stumps. We conclude that boosting should be used like random forests:
with large decision trees and without direct regularization or early stopping.","['Abraham J. Wyner', 'Matthew Olson', 'Justin Bleich', 'David Mease']","['stat.ML', 'cs.LG', 'stat.ME']",2015-04-28 22:34:25+00:00
http://arxiv.org/abs/1504.07575v1,Becoming the Expert - Interactive Multi-Class Machine Teaching,"Compared to machines, humans are extremely good at classifying images into
categories, especially when they possess prior knowledge of the categories at
hand. If this prior information is not available, supervision in the form of
teaching images is required. To learn categories more quickly, people should
see important and representative images first, followed by less important
images later - or not at all. However, image-importance is individual-specific,
i.e. a teaching image is important to a student if it changes their overall
ability to discriminate between classes. Further, students keep learning, so
while image-importance depends on their current knowledge, it also varies with
time.
  In this work we propose an Interactive Machine Teaching algorithm that
enables a computer to teach challenging visual concepts to a human. Our
adaptive algorithm chooses, online, which labeled images from a teaching set
should be shown to the student as they learn. We show that a teaching strategy
that probabilistically models the student's ability and progress, based on
their correct and incorrect answers, produces better 'experts'. We present
results using real human participants across several varied and challenging
real-world datasets.","['Edward Johns', 'Oisin Mac Aodha', 'Gabriel J. Brostow']","['cs.CV', 'cs.LG', 'stat.ML']",2015-04-28 17:22:29+00:00
http://arxiv.org/abs/1504.07550v6,Deep Neural Networks Regularization for Structured Output Prediction,"A deep neural network model is a powerful framework for learning
representations. Usually, it is used to learn the relation $x \to y$ by
exploiting the regularities in the input $x$. In structured output prediction
problems, $y$ is multi-dimensional and structural relations often exist between
the dimensions. The motivation of this work is to learn the output dependencies
that may lie in the output data in order to improve the prediction accuracy.
Unfortunately, feedforward networks are unable to exploit the relations between
the outputs. In order to overcome this issue, we propose in this paper a
regularization scheme for training neural networks for these particular tasks
using a multi-task framework. Our scheme aims at incorporating the learning of
the output representation $y$ in the training process in an unsupervised
fashion while learning the supervised mapping function $x \to y$.
  We evaluate our framework on a facial landmark detection problem which is a
typical structured output task. We show over two public challenging datasets
(LFPW and HELEN) that our regularization scheme improves the generalization of
deep neural networks and accelerates their training. The use of unlabeled data
and label-only data is also explored, showing an additional improvement of the
results. We provide an opensource implementation
(https://github.com/sbelharbi/structured-output-ae) of our framework.","['Soufiane Belharbi', 'Romain Hérault', 'Clément Chatelain', 'Sébastien Adam']","['cs.LG', 'stat.ML']",2015-04-28 16:11:15+00:00
http://arxiv.org/abs/1504.07468v3,Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood,"We consider the problem of discriminative factor analysis for data that are
in general non-Gaussian. A Bayesian model based on the ranks of the data is
proposed. We first introduce a new {\em max-margin} version of the
rank-likelihood. A discriminative factor model is then developed, integrating
the max-margin rank-likelihood and (linear) Bayesian support vector machines,
which are also built on the max-margin principle. The discriminative factor
model is further extended to the {\em nonlinear} case through mixtures of local
linear classifiers, via Dirichlet processes. Fully local conjugacy of the model
yields efficient inference with both Markov Chain Monte Carlo and variational
Bayes approaches. Extensive experiments on benchmark and real data demonstrate
superior performance of the proposed model and its potential for applications
in computational biology.","['Xin Yuan', 'Ricardo Henao', 'Ephraim L. Tsalik', 'Raymond J. Langley', 'Lawrence Carin']",['stat.ML'],2015-04-28 13:40:18+00:00
http://arxiv.org/abs/1504.07389v1,Building Classifiers to Predict the Start of Glucose-Lowering Pharmacotherapy Using Belgian Health Expenditure Data,"Early diagnosis is important for type 2 diabetes (T2D) to improve patient
prognosis, prevent complications and reduce long-term treatment costs. We
present a novel risk profiling approach based exclusively on health expenditure
data that is available to Belgian mutual health insurers. We used expenditure
data related to drug purchases and medical provisions to construct models that
predict whether a patient will start glucose-lowering pharmacotherapy in the
coming years, based on that patient's recent medical expenditure history. The
design and implementation of the modeling strategy are discussed in detail and
several learning methods are benchmarked for our application. Our best
performing model obtains between 74.9% and 76.8% area under the ROC curve,
which is comparable to state-of-the-art risk prediction approaches for T2D
based on questionnaires. In contrast to other methods, our approach can be
implemented on a population-wide scale at virtually no extra operational cost.
Possibly, our approach can be further improved by additional information about
some risk factors of T2D that is unavailable in health expenditure data.","['Marc Claesen', 'Frank De Smet', 'Pieter Gillard', 'Chantal Mathieu', 'Bart De Moor']","['stat.ML', 'cs.IR', 'I.5.4; J.3']",2015-04-28 09:27:03+00:00
http://arxiv.org/abs/1504.07225v3,Correlational Neural Networks,"Common Representation Learning (CRL), wherein different descriptions (or
views) of the data are embedded in a common subspace, is receiving a lot of
attention recently. Two popular paradigms here are Canonical Correlation
Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA
based approaches learn a joint representation by maximizing correlation of the
views when projected to the common subspace. AE based methods learn a common
representation by minimizing the error of reconstructing the two views. Each of
these approaches has its own advantages and disadvantages. For example, while
CCA based approaches outperform AE based approaches for the task of transfer
learning, they are not as scalable as the latter. In this work we propose an AE
based approach called Correlational Neural Network (CorrNet), that explicitly
maximizes correlation among the views when projected to the common subspace.
Through a series of experiments, we demonstrate that the proposed CorrNet is
better than the above mentioned approaches with respect to its ability to learn
correlated common representations. Further, we employ CorrNet for several cross
language tasks and show that the representations learned using CorrNet perform
better than the ones learned using other state of the art approaches.","['Sarath Chandar', 'Mitesh M. Khapra', 'Hugo Larochelle', 'Balaraman Ravindran']","['cs.CL', 'cs.LG', 'cs.NE', 'stat.ML']",2015-04-27 19:51:34+00:00
http://arxiv.org/abs/1504.07235v1,Sign Stable Random Projections for Large-Scale Learning,"We study the use of ""sign $\alpha$-stable random projections"" (where
$0<\alpha\leq 2$) for building basic data processing tools in the context of
large-scale machine learning applications (e.g., classification, regression,
clustering, and near-neighbor search). After the processing by sign stable
random projections, the inner products of the processed data approximate
various types of nonlinear kernels depending on the value of $\alpha$. Thus,
this approach provides an effective strategy for approximating nonlinear
learning algorithms essentially at the cost of linear learning. When $\alpha
=2$, it is known that the corresponding nonlinear kernel is the arc-cosine
kernel. When $\alpha=1$, the procedure approximates the arc-cos-$\chi^2$ kernel
(under certain condition). When $\alpha\rightarrow0+$, it corresponds to the
resemblance kernel.
  From practitioners' perspective, the method of sign $\alpha$-stable random
projections is ready to be tested for large-scale learning applications, where
$\alpha$ can be simply viewed as a tuning parameter. What is missing in the
literature is an extensive empirical study to show the effectiveness of sign
stable random projections, especially for $\alpha\neq 2$ or 1. The paper
supplies such a study on a wide variety of classification datasets. In
particular, we compare shoulder-by-shoulder sign stable random projections with
the recently proposed ""0-bit consistent weighted sampling (CWS)"" (Li 2015).",['Ping Li'],"['stat.ML', 'cs.LG', 'stat.CO']",2015-04-27 19:50:40+00:00
http://arxiv.org/abs/1504.07218v2,Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons,"This paper explores the preference-based top-$K$ rank aggregation problem.
Suppose that a collection of items is repeatedly compared in pairs, and one
wishes to recover a consistent ordering that emphasizes the top-$K$ ranked
items, based on partially revealed preferences. We focus on the
Bradley-Terry-Luce (BTL) model that postulates a set of latent preference
scores underlying all items, where the odds of paired comparisons depend only
on the relative scores of the items involved.
  We characterize the minimax limits on identifiability of top-$K$ ranked
items, in the presence of random and non-adaptive sampling. Our results
highlight a separation measure that quantifies the gap of preference scores
between the $K^{\text{th}}$ and $(K+1)^{\text{th}}$ ranked items. The minimum
sample complexity required for reliable top-$K$ ranking scales inversely with
the separation measure irrespective of other preference distribution metrics.
To approach this minimax limit, we propose a nearly linear-time ranking scheme,
called \emph{Spectral MLE}, that returns the indices of the top-$K$ items in
accordance to a careful score estimate. In a nutshell, Spectral MLE starts with
an initial score estimate with minimal squared loss (obtained via a spectral
method), and then successively refines each component with the assistance of
coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item
identification under minimal sample complexity. The practical applicability of
Spectral MLE is further corroborated by numerical experiments.","['Yuxin Chen', 'Changho Suh']","['cs.LG', 'cs.DS', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2015-04-27 19:30:01+00:00
http://arxiv.org/abs/1504.07107v5,Fast Sampling for Bayesian Max-Margin Models,"Bayesian max-margin models have shown superiority in various practical
applications, such as text categorization, collaborative prediction, social
network link prediction and crowdsourcing, and they conjoin the flexibility of
Bayesian modeling and predictive strengths of max-margin learning. However,
Monte Carlo sampling for these models still remains challenging, especially for
applications that involve large-scale datasets. In this paper, we present the
stochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to
implement and computationally efficient. We show the approximate detailed
balance property of subgradient HMC which reveals a natural and validated
generalization of the ordinary HMC. Furthermore, we investigate the variants
that use stochastic subsampling and thermostats for better scalability and
mixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we
efficiently solve the posterior inference task of various Bayesian max-margin
models and extensive experimental results demonstrate the effectiveness of our
approach.","['Wenbo Hu', 'Jun Zhu', 'Bo Zhang']","['stat.ML', 'cs.AI', 'cs.LG']",2015-04-27 14:29:40+00:00
http://arxiv.org/abs/1504.07027v2,On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes,"The variational framework for learning inducing variables (Titsias, 2009a)
has had a large impact on the Gaussian process literature. The framework may be
interpreted as minimizing a rigorously defined Kullback-Leibler divergence
between the approximating and posterior processes. To our knowledge this
connection has thus far gone unremarked in the literature. In this paper we
give a substantial generalization of the literature on this topic. We give a
new proof of the result for infinite index sets which allows inducing points
that are not data points and likelihoods that depend on all function values. We
then discuss augmented index sets and show that, contrary to previous works,
marginal consistency of augmentation is not enough to guarantee consistency of
variational inference with the original model. We then characterize an extra
condition where such a guarantee is obtainable. Finally we show how our
framework sheds light on interdomain sparse approximations and sparse
approximations for Cox processes.","['Alexander G. de G. Matthews', 'James Hensman', 'Richard E. Turner', 'Zoubin Ghahramani']",['stat.ML'],2015-04-27 11:01:50+00:00
http://arxiv.org/abs/1504.06964v6,Modeling Recovery Curves With Application to Prostatectomy,"We propose a Bayesian model that predicts recovery curves based on
information available before the disruptive event. A recovery curve of interest
is the quantified sexual function of prostate cancer patients after
prostatectomy surgery. We illustrate the utility of our model as a
pre-treatment medical decision aid, producing personalized predictions that are
both interpretable and accurate. We uncover covariate relationships that agree
with and supplement that in existing medical literature.","['Fulton Wang', 'Tyler H. McCormick', 'Cynthia Rudin', 'John Gore']","['stat.ME', 'stat.AP', 'stat.ML']",2015-04-27 08:14:33+00:00
http://arxiv.org/abs/1504.06937v3,Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits,"We study contextual bandits with budget and time constraints, referred to as
constrained contextual bandits.The time and budget constraints significantly
complicate the exploration and exploitation tradeoff because they introduce
complex coupling among contexts over time.Such coupling effects make it
difficult to obtain oracle solutions that assume known statistics of bandits.
To gain insight, we first study unit-cost systems with known context
distribution. When the expected rewards are known, we develop an approximation
of the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves
near-optimality and only requires the ordering of expected rewards. With these
highly desirable features, we then combine ALP with the upper-confidence-bound
(UCB) method in the general case where the expected rewards are unknown {\it a
priori}. We show that the proposed UCB-ALP algorithm achieves logarithmic
regret except for certain boundary cases. Further, we design algorithms and
obtain similar regret analysis results for more general systems with unknown
context distribution and heterogeneous costs. To the best of our knowledge,
this is the first work that shows how to achieve logarithmic regret in
constrained contextual bandits. Moreover, this work also sheds light on the
study of computationally efficient algorithms for general constrained
contextual bandits.","['Huasen Wu', 'R. Srikant', 'Xin Liu', 'Chong Jiang']","['cs.LG', 'stat.ML']",2015-04-27 06:03:50+00:00
http://arxiv.org/abs/1504.06877v1,Bayesian kernel-based system identification with quantized output data,"In this paper we introduce a novel method for linear system identification
with quantized output data. We model the impulse response as a zero-mean
Gaussian process whose covariance (kernel) is given by the recently proposed
stable spline kernel, which encodes information on regularity and exponential
stability. This serves as a starting point to cast our system identification
problem into a Bayesian framework. We employ Markov Chain Monte Carlo (MCMC)
methods to provide an estimate of the system. In particular, we show how to
design a Gibbs sampler which quickly converges to the target distribution.
Numerical simulations show a substantial improvement in the accuracy of the
estimates over state-of-the-art kernel-based methods when employed in
identification of systems with quantized data.","['Giulio Bottegal', 'Gianluigi Pillonetto', 'Håkan Hjalmarsson']","['cs.SY', 'stat.ML']",2015-04-26 20:08:51+00:00
http://arxiv.org/abs/1504.06848v1,Maximum a Posteriori Estimation by Search in Probabilistic Programs,"We introduce an approximate search algorithm for fast maximum a posteriori
probability estimation in probabilistic programs, which we call Bayesian ascent
Monte Carlo (BaMC). Probabilistic programs represent probabilistic models with
varying number of mutually dependent finite, countable, and continuous random
variables. BaMC is an anytime MAP search algorithm applicable to any
combination of random variables and dependencies. We compare BaMC to other MAP
estimation algorithms and show that BaMC is faster and more robust on a range
of probabilistic models.","['David Tolpin', 'Frank Wood']","['cs.AI', 'stat.ML']",2015-04-26 17:23:06+00:00
http://arxiv.org/abs/1504.06837v2,Assessing binary classifiers using only positive and unlabeled data,"Assessing the performance of a learned model is a crucial part of machine
learning. However, in some domains only positive and unlabeled examples are
available, which prohibits the use of most standard evaluation metrics. We
propose an approach to estimate any metric based on contingency tables,
including ROC and PR curves, using only positive and unlabeled data. Estimating
these performance metrics is essentially reduced to estimating the fraction of
(latent) positives in the unlabeled set, assuming known positives are a random
sample of all positives. We provide theoretical bounds on the quality of our
estimates, illustrate the importance of estimating the fraction of positives in
the unlabeled set and demonstrate empirically that we are able to reliably
estimate ROC and PR curves on real data.","['Marc Claesen', 'Jesse Davis', 'Frank De Smet', 'Bart De Moor']","['stat.ML', 'cs.IR', 'cs.LG', 'I.5.2']",2015-04-26 14:59:12+00:00
http://arxiv.org/abs/1504.06817v2,Relative Error Bound Analysis for Nuclear Norm Regularized Matrix Completion,"In this paper, we develop a relative error bound for nuclear norm regularized
matrix completion, with the focus on the completion of full-rank matrices.
Under the assumption that the top eigenspaces of the target matrix are
incoherent, we derive a relative upper bound for recovering the best low-rank
approximation of the unknown matrix. Although multiple works have been devoted
to analyzing the recovery error of full-rank matrix completion, their error
bounds are usually additive, making it impossible to obtain the perfect
recovery case and more generally difficult to leverage the skewed distribution
of eigenvalues. Our analysis is built upon the optimality condition of the
regularized formulation and existing guarantees for low-rank matrix completion.
To the best of our knowledge, this is the first relative bound that has been
proved for the regularized formulation of matrix completion.","['Lijun Zhang', 'Tianbao Yang', 'Rong Jin', 'Zhi-Hua Zhou']","['cs.LG', 'stat.ML']",2015-04-26 13:12:16+00:00
http://arxiv.org/abs/1504.06796v2,Overlapping Communities Detection via Measure Space Embedding,"We present a new algorithm for community detection. The algorithm uses random
walks to embed the graph in a space of measures, after which a modification of
$k$-means in that space is applied. The algorithm is therefore fast and easily
parallelizable. We evaluate the algorithm on standard random graph benchmarks,
including some overlapping community benchmarks, and find its performance to be
better or at least as good as previously known algorithms. We also prove a
linear time (in number of edges) guarantee for the algorithm on a
$p,q$-stochastic block model with $p \geq c\cdot N^{-\frac{1}{2} + \epsilon}$
and $p-q \geq c' \sqrt{p N^{-\frac{1}{2} + \epsilon} \log N}$.","['Mark Kozdoba', 'Shie Mannor']","['cs.LG', 'cs.SI', 'stat.ML']",2015-04-26 10:00:29+00:00
http://arxiv.org/abs/1504.06785v3,Complete Dictionary Recovery over the Sphere,"We consider the problem of recovering a complete (i.e., square and
invertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb R^{n \times p}$
with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ is
sufficiently sparse. This recovery problem is central to the theoretical
understanding of dictionary learning, which seeks a sparse representation for a
collection of input signals, and finds numerous applications in modern signal
processing and machine learning. We give the first efficient algorithm that
provably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros per
column, under suitable probability model for $\mathbf X_0$. In contrast, prior
results based on efficient algorithms provide recovery guarantees when $\mathbf
X_0$ has only $O(n^{1-\delta})$ nonzeros per column for any constant $\delta
\in (0, 1)$.
  Our algorithmic pipeline centers around solving a certain nonconvex
optimization problem with a spherical constraint, and hence is naturally
phrased in the language of manifold optimization. To show this apparently hard
problem is tractable, we first provide a geometric characterization of the
high-dimensional objective landscape, which shows that with high probability
there are no ""spurious"" local minima. This particular geometric structure
allows us to design a Riemannian trust region algorithm over the sphere that
provably converges to one local minimizer with an arbitrary initialization,
despite the presence of saddle points. The geometric approach we develop here
may also shed light on other problems arising from nonconvex recovery of
structured signals.","['Ju Sun', 'Qing Qu', 'John Wright']","['cs.IT', 'cs.CV', 'cs.LG', 'math.IT', 'math.OC', 'stat.ML', '68P30, 58C05, 94A12, 94A08, 68T05, 90C26, 90C48, 90C55']",2015-04-26 04:57:19+00:00
http://arxiv.org/abs/1504.06779v2,Computational Cost Reduction in Learned Transform Classifications,"We present a theoretical analysis and empirical evaluations of a novel set of
techniques for computational cost reduction of classifiers that are based on
learned transform and soft-threshold. By modifying optimization procedures for
dictionary and classifier training, as well as the resulting dictionary
entries, our techniques allow to reduce the bit precision and to replace each
floating-point multiplication by a single integer bit shift. We also show how
the optimization algorithms in some dictionary training methods can be modified
to penalize higher-energy dictionaries. We applied our techniques with the
classifier Learning Algorithm for Soft-Thresholding, testing on the datasets
used in its original paper. Our results indicate it is feasible to use solely
sums and bit shifts of integers to classify at test time with a limited
reduction of the classification accuracy. These low power operations are a
valuable trade off in FPGA implementations as they increase the classification
throughput while decrease both energy consumption and manufacturing cost.","['Emerson Lopes Machado', 'Cristiano Jacques Miosso', 'Ricardo von Borries', 'Murilo Coutinho', 'Pedro de Azevedo Berger', 'Thiago Marques', 'Ricardo Pezzuol Jacobi']","['cs.CV', 'stat.ML']",2015-04-26 01:16:44+00:00
http://arxiv.org/abs/1504.06701v1,A Prior Distribution over Directed Acyclic Graphs for Sparse Bayesian Networks,"The main contribution of this article is a new prior distribution over
directed acyclic graphs, which gives larger weight to sparse graphs. This
distribution is intended for structured Bayesian networks, where the structure
is given by an ordered block model. That is, the nodes of the graph are objects
which fall into categories (or blocks); the blocks have a natural ordering. The
presence of a relationship between two objects is denoted by an arrow, from the
object of lower category to the object of higher category. The models
considered here were introduced in Kemp et al. (2004) for relational data and
extended to multivariate data in Mansinghka et al. (2006). The prior over graph
structures presented here has an explicit formula. The number of nodes in each
layer of the graph follow a Hoppe Ewens urn model.
  We consider the situation where the nodes of the graph represent random
variables, whose joint probability distribution factorises along the DAG. We
describe Monte Carlo schemes for finding the optimal aposteriori structure
given a data matrix and compare the performance with Mansinghka et al. (2006)
and also with the uniform prior.","['Felix L. Rios', 'John M. Noble', 'Timo J. T. Koski']",['stat.ML'],2015-04-25 08:35:30+00:00
http://arxiv.org/abs/1504.06662v2,Compositional Vector Space Models for Knowledge Base Completion,"Knowledge base (KB) completion adds new facts to a KB by making inferences
from existing facts, for example by inferring with high likelihood
nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop
relational synonyms like this, or use as evidence a multi-hop relational path
treated as an atomic feature, like bornIn(X,Z) -> containedIn(Z,Y). This paper
presents an approach that reasons about conjunctions of multi-hop relations
non-atomically, composing the implications of a path using a recursive neural
network (RNN) that takes as inputs vector embeddings of the binary relation in
the path. Not only does this allow us to generalize to paths unseen at training
time, but also, with a single high-capacity RNN, to predict new relation types
not seen when the compositional model was trained (zero-shot learning). We
assemble a new dataset of over 52M relational triples, and show that our method
improves over a traditional classifier by 11%, and a method leveraging
pre-trained embeddings by 7%.","['Arvind Neelakantan', 'Benjamin Roth', 'Andrew McCallum']","['cs.CL', 'stat.ML']",2015-04-24 23:06:10+00:00
http://arxiv.org/abs/1504.06658v1,Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods,"Most of previous work in knowledge base (KB) completion has focused on the
problem of relation extraction. In this work, we focus on the task of inferring
missing entity type instances in a KB, a fundamental task for KB competition
yet receives little attention. Due to the novelty of this task, we construct a
large-scale dataset and design an automatic evaluation methodology. Our
knowledge base completion method uses information within the existing KB and
external information from Wikipedia. We show that individual methods trained
with a global objective that considers unobserved cells from both the entity
and the type side gives consistently higher quality predictions compared to
baseline methods. We also perform manual evaluation on a small subset of the
data to verify the effectiveness of our knowledge base completion methods and
the correctness of our proposed automatic evaluation method.","['Arvind Neelakantan', 'Ming-Wei Chang']","['cs.CL', 'stat.ML']",2015-04-24 22:32:40+00:00
http://arxiv.org/abs/1504.06654v1,Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space,"There is rising interest in vector-space word embeddings and their use in
NLP, especially given recent methods for their fast estimation at very large
scale. Nearly all this work, however, assumes a single vector per word type
ignoring polysemy and thus jeopardizing their usefulness for downstream tasks.
We present an extension to the Skip-gram model that efficiently learns multiple
embeddings per word type. It differs from recent related work by jointly
performing word sense discrimination and embedding learning, by
non-parametrically estimating the number of senses per word type, and by its
efficiency and scalability. We present new state-of-the-art results in the word
similarity in context task and demonstrate its scalability by training with one
machine on a corpus of nearly 1 billion tokens in less than 6 hours.","['Arvind Neelakantan', 'Jeevan Shankar', 'Alexandre Passos', 'Andrew McCallum']","['cs.CL', 'stat.ML']",2015-04-24 22:12:14+00:00
http://arxiv.org/abs/1504.06650v1,Learning Dictionaries for Named Entity Recognition using Minimal Supervision,"This paper describes an approach for automatic construction of dictionaries
for Named Entity Recognition (NER) using large amounts of unlabeled data and a
few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower
dimensional embeddings (representations) for candidate phrases and classify
these phrases using a small number of labeled examples. Our method achieves
16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER
respectively. We also show that by adding candidate phrase embeddings as
features in a sequence tagger gives better performance compared to using word
embeddings.","['Arvind Neelakantan', 'Michael Collins']","['cs.CL', 'stat.ML']",2015-04-24 21:43:55+00:00
http://arxiv.org/abs/1504.06553v1,A Bayesian approach for structure learning in oscillating regulatory networks,"Oscillations lie at the core of many biological processes, from the cell
cycle, to circadian oscillations and developmental processes. Time-keeping
mechanisms are essential to enable organisms to adapt to varying conditions in
environmental cycles, from day/night to seasonal. Transcriptional regulatory
networks are one of the mechanisms behind these biological oscillations.
However, while identifying cyclically expressed genes from time series
measurements is relatively easy, determining the structure of the interaction
network underpinning the oscillation is a far more challenging problem. Here,
we explicitly leverage the oscillatory nature of the transcriptional signals
and present a method for reconstructing network interactions tailored to this
special but important class of genetic circuits. Our method is based on
projecting the signal onto a set of oscillatory basis functions using a
Discrete Fourier Transform. We build a Bayesian Hierarchical model within a
frequency domain linear model in order to enforce sparsity and incorporate
prior knowledge about the network structure. Experiments on real and simulated
data show that the method can lead to substantial improvements over competing
approaches if the oscillatory assumption is met, and remains competitive also
in cases it is not.","['D Trejo', 'AJ Millar', 'G Sanguinetti']","['stat.ML', 'q-bio.QM']",2015-04-24 16:08:30+00:00
http://arxiv.org/abs/1504.06394v1,Social Trust Prediction via Max-norm Constrained 1-bit Matrix Completion,"Social trust prediction addresses the significant problem of exploring
interactions among users in social networks. Naturally, this problem can be
formulated in the matrix completion framework, with each entry indicating the
trustness or distrustness. However, there are two challenges for the social
trust problem: 1) the observed data are with sign (1-bit) measurements; 2) they
are typically sampled non-uniformly. Most of the previous matrix completion
methods do not well handle the two issues. Motivated by the recent progress of
max-norm, we propose to solve the problem with a 1-bit max-norm constrained
formulation. Since max-norm is not easy to optimize, we utilize a reformulation
of max-norm which facilitates an efficient projected gradient decent algorithm.
We demonstrate the superiority of our formulation on two benchmark datasets.","['Jing Wang', 'Jie Shen', 'Huan Xu']","['cs.SI', 'cs.LG', 'stat.ML']",2015-04-24 05:01:12+00:00
http://arxiv.org/abs/1504.06329v1,Analysis of Stopping Active Learning based on Stabilizing Predictions,"Within the natural language processing (NLP) community, active learning has
been widely investigated and applied in order to alleviate the annotation
bottleneck faced by developers of new NLP systems and technologies. This paper
presents the first theoretical analysis of stopping active learning based on
stabilizing predictions (SP). The analysis has revealed three elements that are
central to the success of the SP method: (1) bounds on Cohen's Kappa agreement
between successively trained models impose bounds on differences in F-measure
performance of the models; (2) since the stop set does not have to be labeled,
it can be made large in practice, helping to guarantee that the results
transfer to previously unseen streams of examples at test/application time; and
(3) good (low variance) sample estimates of Kappa between successive models can
be obtained. Proofs of relationships between the level of Kappa agreement and
the difference in performance between consecutive models are presented.
Specifically, if the Kappa agreement between two models exceeds a threshold T
(where $T>0$), then the difference in F-measure performance between those
models is bounded above by $\frac{4(1-T)}{T}$ in all cases. If precision of the
positive conjunction of the models is assumed to be $p$, then the bound can be
tightened to $\frac{4(1-T)}{(p+1)T}$.","['Michael Bloodgood', 'John Grothendieck']","['cs.LG', 'cs.CL', 'stat.ML', 'I.5.1; I.5.4; G.3; I.2.7; I.2.6']",2015-04-23 20:07:01+00:00
http://arxiv.org/abs/1504.06305v1,Regularization-free estimation in trace regression with symmetric positive semidefinite matrices,"Over the past few years, trace regression models have received considerable
attention in the context of matrix completion, quantum state tomography, and
compressed sensing. Estimation of the underlying matrix from
regularization-based approaches promoting low-rankedness, notably nuclear norm
regularization, have enjoyed great popularity. In the present paper, we argue
that such regularization may no longer be necessary if the underlying matrix is
symmetric positive semidefinite (\textsf{spd}) and the design satisfies certain
conditions. In this situation, simple least squares estimation subject to an
\textsf{spd} constraint may perform as well as regularization-based approaches
with a proper choice of the regularization parameter, which entails knowledge
of the noise level and/or tuning. By contrast, constrained least squares
estimation comes without any tuning parameter and may hence be preferred due to
its simplicity.","['Martin Slawski', 'Ping Li', 'Matthias Hein']","['stat.ML', 'cs.LG', 'stat.ME']",2015-04-23 19:30:38+00:00
http://arxiv.org/abs/1504.06274v1,A new approach for physiological time series,"We developed a new approach for the analysis of physiological time series. An
iterative convolution filter is used to decompose the time series into various
components. Statistics of these components are extracted as features to
characterize the mechanisms underlying the time series. Motivated by the
studies that show many normal physiological systems involve irregularity while
the decrease of irregularity usually implies the abnormality, the statistics
for ""outliers"" in the components are used as features measuring irregularity.
Support vector machines are used to select the most relevant features that are
able to differentiate the time series from normal and abnormal systems. This
new approach is successfully used in the study of congestive heart failure by
heart beat interval time series.","['Dong Mao', 'Yang Wang', 'Qiang Wu']","['cs.LG', 'stat.ML']",2015-04-23 17:56:33+00:00
http://arxiv.org/abs/1504.06043v2,Stability of Stochastic Approximations with `Controlled Markov' Noise and Temporal Difference Learning,"We are interested in understanding stability (almost sure boundedness) of
stochastic approximation algorithms (SAs) driven by a `controlled Markov'
process. Analyzing this class of algorithms is important, since many
reinforcement learning (RL) algorithms can be cast as SAs driven by a
`controlled Markov' process. In this paper, we present easily verifiable
sufficient conditions for stability and convergence of SAs driven by a
`controlled Markov' process. Many RL applications involve continuous state
spaces. While our analysis readily ensures stability for such continuous state
applications, traditional analyses do not. As compared to literature, our
analysis presents a two-fold generalization (a) the Markov process may evolve
in a continuous state space and (b) the process need not be ergodic under any
given stationary policy. Temporal difference learning (TD) is an important
policy evaluation method in reinforcement learning. The theory developed
herein, is used to analyze generalized $TD(0)$, an important variant of TD. Our
theory is also used to analyze a TD formulation of supervised learning for
forecasting problems.","['Arunselvan Ramaswamy', 'Shalabh Bhatnagar']","['cs.SY', 'stat.ML', '62L20, 93E03, 93E35, 34A60']",2015-04-23 04:50:27+00:00
http://arxiv.org/abs/1504.06026v1,Graphical Fermat's Principle and Triangle-Free Graph Estimation,"We consider the problem of estimating undirected triangle-free graphs of high
dimensional distributions. Triangle-free graphs form a rich graph family which
allows arbitrary loopy structures but 3-cliques. For inferential tractability,
we propose a graphical Fermat's principle to regularize the distribution
family. Such principle enforces the existence of a distribution-dependent
pseudo-metric such that any two nodes have a smaller distance than that of two
other nodes who have a geodesic path include these two nodes. Guided by this
principle, we show that a greedy strategy is able to recover the true graph.
The resulting algorithm only requires a pairwise distance matrix as input and
is computationally even more efficient than calculating the minimum spanning
tree. We consider graph estimation problems under different settings, including
discrete and nonparametric distribution families. Thorough numerical results
are provided to illustrate the usefulness of the proposed method.","['Junwei Lu', 'Han Liu']",['stat.ML'],2015-04-23 02:28:16+00:00
http://arxiv.org/abs/1504.05994v1,On the relation between Gaussian process quadratures and sigma-point methods,"This article is concerned with Gaussian process quadratures, which are
numerical integration methods based on Gaussian process regression methods, and
sigma-point methods, which are used in advanced non-linear Kalman filtering and
smoothing algorithms. We show that many sigma-point methods can be interpreted
as Gaussian quadrature based methods with suitably selected covariance
functions. We show that this interpretation also extends to more general
multivariate Gauss--Hermite integration methods and related spherical cubature
rules. Additionally, we discuss different criteria for selecting the
sigma-point locations: exactness for multivariate polynomials up to a given
order, minimum average error, and quasi-random point sets. The performance of
the different methods is tested in numerical experiments.","['Simo Särkkä', 'Jouni Hartikainen', 'Lennart Svensson', 'Fredrik Sandblom']","['stat.ME', 'math.DS', 'stat.ML']",2015-04-22 21:49:42+00:00
http://arxiv.org/abs/1504.05929v2,A Hierarchical Distance-dependent Bayesian Model for Event Coreference Resolution,"We present a novel hierarchical distance-dependent Bayesian model for event
coreference resolution. While existing generative models for event coreference
resolution are completely unsupervised, our model allows for the incorporation
of pairwise distances between event mentions -- information that is widely used
in supervised coreference models to guide the generative clustering processing
for better event clustering both within and across documents. We model the
distances between event mentions using a feature-rich learnable distance
function and encode them as Bayesian priors for nonparametric clustering.
Experiments on the ECB+ corpus show that our model outperforms state-of-the-art
methods for both within- and cross-document event coreference resolution.","['Bishan Yang', 'Claire Cardie', 'Peter Frazier']","['cs.CL', 'stat.ML']",2015-04-22 19:13:49+00:00
http://arxiv.org/abs/1504.05880v1,Spectral Norm of Random Kernel Matrices with Applications to Privacy,"Kernel methods are an extremely popular set of techniques used for many
important machine learning and data analysis applications. In addition to
having good practical performances, these methods are supported by a
well-developed theory. Kernel methods use an implicit mapping of the input data
into a high dimensional feature space defined by a kernel function, i.e., a
function returning the inner product between the images of two data points in
the feature space. Central to any kernel method is the kernel matrix, which is
built by evaluating the kernel function on a given sample dataset.
  In this paper, we initiate the study of non-asymptotic spectral theory of
random kernel matrices. These are n x n random matrices whose (i,j)th entry is
obtained by evaluating the kernel function on $x_i$ and $x_j$, where
$x_1,...,x_n$ are a set of n independent random high-dimensional vectors. Our
main contribution is to obtain tight upper bounds on the spectral norm (largest
eigenvalue) of random kernel matrices constructed by commonly used kernel
functions based on polynomials and Gaussian radial basis.
  As an application of these results, we provide lower bounds on the distortion
needed for releasing the coefficients of kernel ridge regression under
attribute privacy, a general privacy notion which captures a large class of
privacy definitions. Kernel ridge regression is standard method for performing
non-parametric regression that regularly outperforms traditional regression
approaches in various domains. Our privacy distortion lower bounds are the
first for any kernel technique, and our analysis assumes realistic scenarios
for the input, unlike all previous lower bounds for other release problems
which only hold under very restrictive input settings.","['Shiva Prasad Kasiviswanathan', 'Mark Rudelson']","['stat.ML', 'cs.CR', 'cs.LG', 'F.2.1']",2015-04-22 16:54:48+00:00
http://arxiv.org/abs/1504.05823v2,"Normal Bandits of Unknown Means and Variances: Asymptotic Optimality, Finite Horizon Regret Bounds, and a Solution to an Open Problem","Consider the problem of sampling sequentially from a finite number of $N \geq
2$ populations, specified by random variables $X^i_k$, $ i = 1,\ldots , N,$ and
$k = 1, 2, \ldots$; where $X^i_k$ denotes the outcome from population $i$ the
$k^{th}$ time it is sampled. It is assumed that for each fixed $i$,
  $\{ X^i_k \}_{k \geq 1}$ is a sequence of i.i.d. normal random variables,
with unknown mean $\mu_i$ and unknown variance $\sigma_i^2$.
  The objective is to have a policy $\pi$ for deciding from which of the $N$
populations to sample form at any time $n=1,2,\ldots$ so as to maximize the
expected sum of outcomes of $n$ samples or equivalently to minimize the regret
due to lack on information of the parameters $\mu_i$ and $\sigma_i^2$. In this
paper, we present a simple inflated sample mean (ISM) index policy that is
asymptotically optimal in the sense of Theorem 4 below. This resolves a
standing open problem from Burnetas and Katehakis (1996). Additionally, finite
horizon regret bounds are given.","['Wesley Cowan', 'Junya Honda', 'Michael N. Katehakis']","['stat.ML', 'cs.LG']",2015-04-22 14:30:13+00:00
http://arxiv.org/abs/1504.05665v1,Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood,"Factorized information criterion (FIC) is a recently developed approximation
technique for the marginal log-likelihood, which provides an automatic model
selection framework for a few latent variable models (LVMs) with tractable
inference algorithms. This paper reconsiders FIC and fills theoretical gaps of
previous FIC studies. First, we reveal the core idea of FIC that allows
generalization for a broader class of LVMs, including continuous LVMs, in
contrast to previous FICs, which are applicable only to binary LVMs. Second, we
investigate the model selection mechanism of the generalized FIC. Our analysis
provides a formal justification of FIC as a model selection criterion for LVMs
and also a systematic procedure for pruning redundant latent variables that
have been removed heuristically in previous studies. Third, we provide an
interpretation of FIC as a variational free energy and uncover a few
previously-unknown their relationships. A demonstrative study on Bayesian
principal component analysis is provided and numerical experiments support our
theoretical results.","['Kohei Hayashi', 'Shin-ichi Maeda', 'Ryohei Fujimaki']","['cs.LG', 'stat.ML']",2015-04-22 06:27:19+00:00
http://arxiv.org/abs/1504.05487v1,Deep Convolutional Neural Networks Based on Semi-Discrete Frames,"Deep convolutional neural networks have led to breakthrough results in
practical feature extraction applications. The mathematical analysis of these
networks was pioneered by Mallat, 2012. Specifically, Mallat considered
so-called scattering networks based on identical semi-discrete wavelet frames
in each network layer, and proved translation-invariance as well as deformation
stability of the resulting feature extractor. The purpose of this paper is to
develop Mallat's theory further by allowing for different and, most
importantly, general semi-discrete frames (such as, e.g., Gabor frames,
wavelets, curvelets, shearlets, ridgelets) in distinct network layers. This
allows to extract wider classes of features than point singularities resolved
by the wavelet transform. Our generalized feature extractor is proven to be
translation-invariant, and we develop deformation stability results for a
larger class of deformations than those considered by Mallat. For Mallat's
wavelet-based feature extractor, we get rid of a number of technical
conditions. The mathematical engine behind our results is continuous frame
theory, which allows us to completely detach the invariance and deformation
stability proofs from the particular algebraic structure of the underlying
frames.","['Thomas Wiatowski', 'Helmut Bölcskei']","['cs.LG', 'cs.IT', 'math.FA', 'math.IT', 'stat.ML']",2015-04-21 16:01:00+00:00
http://arxiv.org/abs/1504.05473v1,Can FCA-based Recommender System Suggest a Proper Classifier?,"The paper briefly introduces multiple classifier systems and describes a new
algorithm, which improves classification accuracy by means of recommendation of
a proper algorithm to an object classification. This recommendation is done
assuming that a classifier is likely to predict the label of the object
correctly if it has correctly classified its neighbors. The process of
assigning a classifier to each object is based on Formal Concept Analysis. We
explain the idea of the algorithm with a toy example and describe our first
experiments with real-world datasets.","['Yury Kashnitsky', 'Dmitry I. Ignatov']","['cs.IR', 'cs.LG', 'stat.ML', '62-07']",2015-04-21 15:38:23+00:00
http://arxiv.org/abs/1504.05434v1,A local approach to estimation in discrete loglinear models,"We consider two connected aspects of maximum likelihood estimation of the
parameter for high-dimensional discrete graphical models: the existence of the
maximum likelihood estimate (mle) and its computation.
  When the data is sparse, there are many zeros in the contingency table and
the maximum likelihood estimate of the parameter may not exist. Fienberg and
Rinaldo (2012) have shown that the mle does not exists iff the data vector
belongs to a face of the so-called marginal cone spanned by the rows of the
design matrix of the model. Identifying these faces in high-dimension is
challenging. In this paper, we take a local approach : we show that one such
face, albeit possibly not the smallest one, can be identified by looking at a
collection of marginal graphical models generated by induced subgraphs
$G_i,i=1,\ldots,k$ of $G$. This is our first contribution.
  Our second contribution concerns the composite maximum likelihood estimate.
When the dimension of the problem is large, estimating the parameters of a
given graphical model through maximum likelihood is challenging, if not
impossible. The traditional approach to this problem has been local with the
use of composite likelihood based on local conditional likelihoods.
  A more recent development is to have the components of the composite
likelihood be marginal likelihoods centred around each $v$. We first show that
the estimates obtained by consensus through local conditional and marginal
likelihoods are identical. We then study the asymptotic properties of the
composite maximum likelihood estimate when both the dimension of the model and
the sample size $N$ go to infinity.","['Helene Massam', 'Nanwei Wang']","['stat.ML', '62H17, 62M40']",2015-04-21 13:51:47+00:00
http://arxiv.org/abs/1504.05427v2,Signal Recovery on Graphs: Random versus Experimentally Designed Sampling,"We study signal recovery on graphs based on two sampling strategies: random
sampling and experimentally designed sampling. We propose a new class of smooth
graph signals, called approximately bandlimited, which generalizes the
bandlimited class and is similar to the globally smooth class. We then propose
two recovery strategies based on random sampling and experimentally designed
sampling. The proposed recovery strategy based on experimentally designed
sampling is similar to the leverage scores used in the matrix approximation. We
show that while both strategies are unbiased estimators for the low-frequency
components, the convergence rate of experimentally designed sampling is much
faster than that of random sampling when a graph is irregular. We validate the
proposed recovery strategies on three specific graphs: a ring graph, an
Erd\H{o}s-R\'enyi graph, and a star graph. The simulation results support the
theoretical analysis.","['Siheng Chen', 'Rohan Varma', 'Aarti Singh', 'Jelena Kovačević']","['cs.IT', 'math.IT', 'stat.ML']",2015-04-21 13:28:17+00:00
http://arxiv.org/abs/1504.05392v1,Nonparametric Testing for Heterogeneous Correlation,"In the presence of weak overall correlation, it may be useful to investigate
if the correlation is significantly and substantially more pronounced over a
subpopulation. Two different testing procedures are compared. Both are based on
the rankings of the values of two variables from a data set with a large number
n of observations. The first maintains its level against Gaussian copulas; the
second adapts to general alternatives in the sense that that the number of
parameters used in the test grows with n. An analysis of wine quality
illustrates how the methods detect heterogeneity of association between
chemical properties of the wine, which are attributable to a mix of different
cultivars.","['Stephen Bamattre', 'Rex Hu', 'Joseph S. Verducci']",['stat.ML'],2015-04-21 11:48:14+00:00
http://arxiv.org/abs/1504.05287v1,Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares Algorithms,"Tensor rank and low-rank tensor decompositions have many applications in
learning and complexity theory. Most known algorithms use unfoldings of tensors
and can only handle rank up to $n^{\lfloor p/2 \rfloor}$ for a $p$-th order
tensor in $\mathbb{R}^{n^p}$. Previously no efficient algorithm can decompose
3rd order tensors when the rank is super-linear in the dimension. Using ideas
from sum-of-squares hierarchy, we give the first quasi-polynomial time
algorithm that can decompose a random 3rd order tensor decomposition when the
rank is as large as $n^{3/2}/\textrm{polylog} n$.
  We also give a polynomial time algorithm for certifying the injective norm of
random low rank tensors. Our tensor decomposition algorithm exploits the
relationship between injective norm and the tensor components. The proof relies
on interesting tools for decoupling random variables to prove better matrix
concentration bounds, which can be useful in other settings.","['Rong Ge', 'Tengyu Ma']","['cs.DS', 'cs.LG', 'stat.ML']",2015-04-21 03:21:53+00:00
http://arxiv.org/abs/1504.05229v2,Poisson Matrix Recovery and Completion,"We extend the theory of low-rank matrix recovery and completion to the case
when Poisson observations for a linear combination or a subset of the entries
of a matrix are available, which arises in various applications with count
data. We consider the usual matrix recovery formulation through maximum
likelihood with proper constraints on the matrix $M$ of size $d_1$-by-$d_2$,
and establish theoretical upper and lower bounds on the recovery error. Our
bounds for matrix completion are nearly optimal up to a factor on the order of
$\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by combing techniques
for compressed sensing for sparse vectors with Poisson noise and for analyzing
low-rank matrices, as well as adapting the arguments used for one-bit matrix
completion \cite{davenport20121} (although these two problems are different in
nature) and the adaptation requires new techniques exploiting properties of the
Poisson likelihood function and tackling the difficulties posed by the locally
sub-Gaussian characteristic of the Poisson distribution. Our results highlight
a few important distinctions of the Poisson case compared to the prior work
including having to impose a minimum signal-to-noise requirement on each
observed entry and a gap in the upper and lower bounds. We also develop a set
of efficient iterative algorithms and demonstrate their good performance on
synthetic examples and real data.","['Yang Cao', 'Yao Xie']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2015-04-20 21:09:47+00:00
http://arxiv.org/abs/1504.05059v1,Nonparametric Nearest Neighbor Random Process Clustering,"We consider the problem of clustering noisy finite-length observations of
stationary ergodic random processes according to their nonparametric generative
models without prior knowledge of the model statistics and the number of
generative models. Two algorithms, both using the L1-distance between estimated
power spectral densities (PSDs) as a measure of dissimilarity, are analyzed.
The first algorithm, termed nearest neighbor process clustering (NNPC), to the
best of our knowledge, is new and relies on partitioning the nearest neighbor
graph of the observations via spectral clustering. The second algorithm, simply
referred to as k-means (KM), consists of a single k-means iteration with
farthest point initialization and was considered before in the literature,
albeit with a different measure of dissimilarity and with asymptotic
performance results only. We show that both NNPC and KM succeed with high
probability under noise and even when the generative process PSDs overlap
significantly, all provided that the observation length is sufficiently large.
Our results quantify the tradeoff between the overlap of the generative process
PSDs, the noise variance, and the observation length. Finally, we present
numerical performance results for synthetic and real data.","['Michael Tschannen', 'Helmut Bölcskei']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2015-04-20 13:48:45+00:00
http://arxiv.org/abs/1504.05006v2,Partition MCMC for inference on acyclic digraphs,"Acyclic digraphs are the underlying representation of Bayesian networks, a
widely used class of probabilistic graphical models. Learning the underlying
graph from data is a way of gaining insights about the structural properties of
a domain. Structure learning forms one of the inference challenges of
statistical graphical models.
  MCMC methods, notably structure MCMC, to sample graphs from the posterior
distribution given the data are probably the only viable option for Bayesian
model averaging. Score modularity and restrictions on the number of parents of
each node allow the graphs to be grouped into larger collections, which can be
scored as a whole to improve the chain's convergence. Current examples of
algorithms taking advantage of grouping are the biased order MCMC, which acts
on the alternative space of permuted triangular matrices, and non ergodic edge
reversal moves.
  Here we propose a novel algorithm, which employs the underlying combinatorial
structure of DAGs to define a new grouping. As a result convergence is improved
compared to structure MCMC, while still retaining the property of producing an
unbiased sample. Finally the method can be combined with edge reversal moves to
improve the sampler further.","['Jack Kuipers', 'Giusi Moffa']","['stat.ML', 'stat.CO', 'stat.ME']",2015-04-20 10:47:27+00:00
http://arxiv.org/abs/1504.04740v1,On the consistency of Multithreshold Entropy Linear Classifier,"Multithreshold Entropy Linear Classifier (MELC) is a recent classifier idea
which employs information theoretic concept in order to create a multithreshold
maximum margin model. In this paper we analyze its consistency over
multithreshold linear models and show that its objective function upper bounds
the amount of misclassified points in a similar manner like hinge loss does in
support vector machines. For further confirmation we also conduct some
numerical experiments on five datasets.",['Wojciech Marian Czarnecki'],"['cs.LG', 'stat.ML']",2015-04-18 16:29:26+00:00
http://arxiv.org/abs/1504.04739v1,Fast optimization of Multithreshold Entropy Linear Classifier,"Multithreshold Entropy Linear Classifier (MELC) is a density based model
which searches for a linear projection maximizing the Cauchy-Schwarz Divergence
of dataset kernel density estimation. Despite its good empirical results, one
of its drawbacks is the optimization speed. In this paper we analyze how one
can speed it up through solving an approximate problem. We analyze two methods,
both similar to the approximate solutions of the Kernel Density Estimation
querying and provide adaptive schemes for selecting a crucial parameters based
on user-specified acceptable error. Furthermore we show how one can exploit
well known conjugate gradients and L-BFGS optimizers despite the fact that the
original optimization problem should be solved on the sphere. All above methods
and modifications are tested on 10 real life datasets from UCI repository to
confirm their practical usability.","['Rafal Jozefowicz', 'Wojciech Marian Czarnecki']","['cs.LG', 'stat.ML']",2015-04-18 16:19:22+00:00
http://arxiv.org/abs/1504.04599v1,Testing Closeness With Unequal Sized Samples,"We consider the problem of closeness testing for two discrete distributions
in the practically relevant setting of \emph{unequal} sized samples drawn from
each of them. Specifically, given a target error parameter $\varepsilon > 0$,
$m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws from
an unknown distribution $q$, we describe a test for distinguishing the case
that $p=q$ from the case that $||p-q||_1 \geq \varepsilon$. If $p$ and $q$ are
supported on at most $n$ elements, then our test is successful with high
probability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 =
\Omega(\max\{\frac{n}{\sqrt m_1\varepsilon^2}, \frac{\sqrt
n}{\varepsilon^2}\});$ we show that this tradeoff is optimal throughout this
range, to constant factors. These results extend the recent work of Chan et al.
who established the sample complexity when the two samples have equal sizes,
and tightens the results of Acharya et al. by polynomials factors in both $n$
and $\varepsilon$. As a consequence, we obtain an algorithm for estimating the
mixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses
$\tilde{O}(n^{3/2} \tau_{mix})$ queries to a ""next node"" oracle, improving upon
the $\tilde{O}(n^{5/3}\tau_{mix})$ query algorithm of Batu et al. Finally, we
note that the core of our testing algorithm is a relatively simple statistic
that seems to perform well in practice, both on synthetic data and on natural
language data.","['Bhaswar B. Bhattacharya', 'Gregory Valiant']","['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2015-04-17 18:35:35+00:00
http://arxiv.org/abs/1504.04407v2,Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting,"We propose mS2GD: a method incorporating a mini-batching scheme for improving
the theoretical complexity and practical performance of semi-stochastic
gradient descent (S2GD). We consider the problem of minimizing a strongly
convex function represented as the sum of an average of a large number of
smooth convex functions, and a simple nonsmooth convex regularizer. Our method
first performs a deterministic step (computation of the gradient of the
objective function at the starting point), followed by a large number of
stochastic steps. The process is repeated a few times with the last iterate
becoming the new starting point. The novelty of our method is in introduction
of mini-batching into the computation of stochastic steps. In each step,
instead of choosing a single function, we sample $b$ functions, compute their
gradients, and compute the direction based on this. We analyze the complexity
of the method and show that it benefits from two speedup effects. First, we
prove that as long as $b$ is below a certain threshold, we can reach any
predefined accuracy with less overall work than without mini-batching. Second,
our mini-batching scheme admits a simple parallel implementation, and hence is
suitable for further acceleration by parallelization.","['Jakub Konečný', 'Jie Liu', 'Peter Richtárik', 'Martin Takáč']","['cs.LG', 'stat.ML']",2015-04-16 23:31:38+00:00
http://arxiv.org/abs/1504.04406v1,Non-Uniform Stochastic Average Gradient Method for Training Conditional Random Fields,"We apply stochastic average gradient (SAG) algorithms for training
conditional random fields (CRFs). We describe a practical implementation that
uses structure in the CRF gradient to reduce the memory requirement of this
linearly-convergent stochastic gradient method, propose a non-uniform sampling
scheme that substantially improves practical performance, and analyze the rate
of convergence of the SAGA variant under non-uniform sampling. Our experimental
results reveal that our method often significantly outperforms existing methods
in terms of the training objective, and performs as well or better than
optimally-tuned stochastic gradient methods in terms of test error.","['Mark Schmidt', 'Reza Babanezhad', 'Mohamed Osama Ahmed', 'Aaron Defazio', 'Ann Clifton', 'Anoop Sarkar']","['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",2015-04-16 23:26:35+00:00
http://arxiv.org/abs/1504.04343v2,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,"We present Caffe con Troll (CcT), a fully compatible end-to-end version of
the popular framework Caffe with rebuilt internals. We built CcT to examine the
performance characteristics of training and deploying general-purpose
convolutional neural networks across different hardware architectures. We find
that, by employing standard batching optimizations for CPU training, we achieve
a 4.5x throughput improvement over Caffe on popular networks like CaffeNet.
Moreover, with these improvements, the end-to-end training time for CNNs is
directly proportional to the FLOPS delivered by the CPU, which enables us to
efficiently train hybrid CPU-GPU systems for CNNs.","['Stefan Hadjis', 'Firas Abuzaid', 'Ce Zhang', 'Christopher Ré']","['cs.LG', 'cs.CV', 'stat.ML']",2015-04-16 19:11:08+00:00
http://arxiv.org/abs/1504.04184v1,Multichannel sparse recovery of complex-valued signals using Huber's criterion,"In this paper, we generalize Huber's criterion to multichannel sparse
recovery problem of complex-valued measurements where the objective is to find
good recovery of jointly sparse unknown signal vectors from the given multiple
measurement vectors which are different linear combinations of the same known
elementary vectors. This requires careful characterization of robust
complex-valued loss functions as well as Huber's criterion function for the
multivariate sparse regression problem. We devise a greedy algorithm based on
simultaneous normalized iterative hard thresholding (SNIHT) algorithm. Unlike
the conventional SNIHT method, our algorithm, referred to as HUB-SNIHT, is
robust under heavy-tailed non-Gaussian noise conditions, yet has a negligible
performance loss compared to SNIHT under Gaussian noise. Usefulness of the
method is illustrated in source localization application with sensor arrays.",['Esa Ollila'],"['cs.IT', 'math.IT', 'stat.CO', 'stat.ML']",2015-04-16 11:25:40+00:00
http://arxiv.org/abs/1504.04114v1,Actively Learning to Attract Followers on Twitter,"Twitter, a popular social network, presents great opportunities for on-line
machine learning research. However, previous research has focused almost
entirely on learning from passively collected data. We study the problem of
learning to acquire followers through normative user behavior, as opposed to
the mass following policies applied by many bots. We formalize the problem as a
contextual bandit problem, in which we consider retweeting content to be the
action chosen and each tweet (content) is accompanied by context. We design
reward signals based on the change in followers. The result of our month long
experiment with 60 agents suggests that (1) aggregating experience across
agents can adversely impact prediction accuracy and (2) the Twitter community's
response to different actions is non-stationary. Our findings suggest that
actively learning on-line can provide deeper insights about how to attract
followers than machine learning over passively collected data alone.","['Nir Levine', 'Timothy A. Mann', 'Shie Mannor']","['stat.ML', 'cs.LG', 'cs.SI']",2015-04-16 07:26:11+00:00
http://arxiv.org/abs/1504.04054v1,A Generative Model for Deep Convolutional Learning,"A generative model is developed for deep (multi-layered) convolutional
dictionary learning. A novel probabilistic pooling operation is integrated into
the deep model, yielding efficient bottom-up (pretraining) and top-down
(refinement) probabilistic learning. Experimental results demonstrate powerful
capabilities of the model to learn multi-layer features from images, and
excellent classification results are obtained on the MNIST and Caltech 101
datasets.","['Yunchen Pu', 'Xin Yuan', 'Lawrence Carin']","['stat.ML', 'cs.LG', 'cs.NE']",2015-04-15 21:31:58+00:00
http://arxiv.org/abs/1504.03991v4,Theory of Dual-sparse Regularized Randomized Reduction,"In this paper, we study randomized reduction methods, which reduce
high-dimensional features into low-dimensional space by randomized methods
(e.g., random projection, random hashing), for large-scale high-dimensional
classification. Previous theoretical results on randomized reduction methods
hinge on strong assumptions about the data, e.g., low rank of the data matrix
or a large separable margin of classification, which hinder their applications
in broad domains. To address these limitations, we propose dual-sparse
regularized randomized reduction methods that introduce a sparse regularizer
into the reduced dual problem. Under a mild condition that the original dual
solution is a (nearly) sparse vector, we show that the resulting dual solution
is close to the original dual solution and concentrates on its support set. In
numerical experiments, we present an empirical study to support the analysis
and we also present a novel application of the dual-sparse regularized
randomized reduction methods to reducing the communication cost of distributed
learning from large-scale high-dimensional data.","['Tianbao Yang', 'Lijun Zhang', 'Rong Jin', 'Shenghuo Zhu']","['cs.LG', 'stat.ML']",2015-04-15 19:16:54+00:00
http://arxiv.org/abs/1504.03701v1,Probabilistic Clustering of Time-Evolving Distance Data,"We present a novel probabilistic clustering model for objects that are
represented via pairwise distances and observed at different time points. The
proposed method utilizes the information given by adjacent time points to find
the underlying cluster structure and obtain a smooth cluster evolution. This
approach allows the number of objects and clusters to differ at every time
point, and no identification on the identities of the objects is needed.
Further, the model does not require the number of clusters being specified in
advance -- they are instead determined automatically using a Dirichlet process
prior. We validate our model on synthetic data showing that the proposed method
is more accurate than state-of-the-art clustering methods. Finally, we use our
dynamic clustering model to analyze and illustrate the evolution of brain
cancer patients over time.","['Julia E. Vogt', 'Marius Kloft', 'Stefan Stark', 'Sudhir S. Raman', 'Sandhya Prabhakaran', 'Volker Roth', 'Gunnar Rätsch']","['cs.LG', 'stat.ML']",2015-04-14 20:05:45+00:00
http://arxiv.org/abs/1504.03509v2,Regret vs. Communication: Distributed Stochastic Multi-Armed Bandits and Beyond,"In this paper, we consider the distributed stochastic multi-armed bandit
problem, where a global arm set can be accessed by multiple players
independently. The players are allowed to exchange their history of
observations with each other at specific points in time. We study the
relationship between regret and communication. When the time horizon is known,
we propose the Over-Exploration strategy, which only requires one-round
communication and whose regret does not scale with the number of players. When
the time horizon is unknown, we measure the frequency of communication through
a new notion called the density of the communication set, and give an exact
characterization of the interplay between regret and communication.
Specifically, a lower bound is established and stable strategies that match the
lower bound are developed. The results and analyses in this paper are specific
but can be translated into more general settings.","['Shuang Liu', 'Cheng Chen', 'Zhihua Zhang']","['cs.LG', 'stat.ML']",2015-04-14 12:14:46+00:00
http://arxiv.org/abs/1504.03415v1,HHCART: An Oblique Decision Tree,"Decision trees are a popular technique in statistical data classification.
They recursively partition the feature space into disjoint sub-regions until
each sub-region becomes homogeneous with respect to a particular class. The
basic Classification and Regression Tree (CART) algorithm partitions the
feature space using axis parallel splits. When the true decision boundaries are
not aligned with the feature axes, this approach can produce a complicated
boundary structure. Oblique decision trees use oblique decision boundaries to
potentially simplify the boundary structure. The major limitation of this
approach is that the tree induction algorithm is computationally expensive. In
this article we present a new decision tree algorithm, called HHCART. The
method utilizes a series of Householder matrices to reflect the training data
at each node during the tree construction. Each reflection is based on the
directions of the eigenvectors from each classes' covariance matrix.
Considering axis parallel splits in the reflected training data provides an
efficient way of finding oblique splits in the unreflected training data.
Experimental results show that the accuracy and size of the HHCART trees are
comparable with some benchmark methods in the literature. The appealing feature
of HHCART is that it can handle both qualitative and quantitative features in
the same oblique split.","['D. C. Wickramarachchi', 'B. L. Robertson', 'M. Reale', 'C. J. Price', 'J. Brown']","['stat.ML', 'cs.LG']",2015-04-14 04:04:00+00:00
http://arxiv.org/abs/1504.03413v1,Consensus based Detection in the Presence of Data Falsification Attacks,"This paper considers the problem of detection in distributed networks in the
presence of data falsification (Byzantine) attacks. Detection approaches
considered in the paper are based on fully distributed consensus algorithms,
where all of the nodes exchange information only with their neighbors in the
absence of a fusion center. In such networks, we characterize the negative
effect of Byzantines on the steady-state and transient detection performance of
the conventional consensus based detection algorithms. To address this issue,
we study the problem from the network designer's perspective. More
specifically, we first propose a distributed weighted average consensus
algorithm that is robust to Byzantine attacks. We show that, under reasonable
assumptions, the global test statistic for detection can be computed locally at
each node using our proposed consensus algorithm. We exploit the statistical
distribution of the nodes' data to devise techniques for mitigating the
influence of data falsifying Byzantines on the distributed detection system.
Since some parameters of the statistical distribution of the nodes' data might
not be known a priori, we propose learning based techniques to enable an
adaptive design of the local fusion or update rules.","['Bhavya Kailkhura', 'Swastik Brahma', 'Pramod K. Varshney']","['cs.SY', 'cs.DC', 'stat.AP', 'stat.ML']",2015-04-14 03:43:05+00:00
http://arxiv.org/abs/1504.03183v1,Adaptive Randomized Dimension Reduction on Massive Data,"The scalability of statistical estimators is of increasing importance in
modern applications. One approach to implementing scalable algorithms is to
compress data into a low dimensional latent space using dimension reduction
methods. In this paper we develop an approach for dimension reduction that
exploits the assumption of low rank structure in high dimensional data to gain
both computational and statistical advantages. We adapt recent randomized
low-rank approximation algorithms to provide an efficient solution to principal
component analysis (PCA), and we use this efficient solver to improve parameter
estimation in large-scale linear mixed models (LMM) for association mapping in
statistical and quantitative genomics. A key observation in this paper is that
randomization serves a dual role, improving both computational and statistical
performance by implicitly regularizing the covariance matrix estimate of the
random effect in a LMM. These statistical and computational advantages are
highlighted in our experiments on simulated data and large-scale genomic
studies.","['Gregory Darnell', 'Stoyan Georgiev', 'Sayan Mukherjee', 'Barbara E Engelhardt']","['stat.ML', 'q-bio.QM']",2015-04-13 13:52:17+00:00
http://arxiv.org/abs/1504.03156v1,"Streaming, Memory Limited Matrix Completion with Noise","In this paper, we consider the streaming memory-limited matrix completion
problem when the observed entries are noisy versions of a small random fraction
of the original entries. We are interested in scenarios where the matrix size
is very large so the matrix is very hard to store and manipulate. Here, columns
of the observed matrix are presented sequentially and the goal is to complete
the missing entries after one pass on the data with limited memory space and
limited computational complexity. We propose a streaming algorithm which
produces an estimate of the original matrix with a vanishing mean square error,
uses memory space scaling linearly with the ambient dimension of the matrix,
i.e. the memory required to store the output alone, and spends computations as
much as the number of non-zero entries of the input matrix.","['Se-Young Yun', 'Marc Lelarge', 'Alexandre Proutiere']","['math.SP', 'stat.ML']",2015-04-13 12:52:31+00:00
http://arxiv.org/abs/1504.02931v1,Generalized Correntropy for Robust Adaptive Filtering,"As a robust nonlinear similarity measure in kernel space, correntropy has
received increasing attention in domains of machine learning and signal
processing. In particular, the maximum correntropy criterion (MCC) has recently
been successfully applied in robust regression and filtering. The default
kernel function in correntropy is the Gaussian kernel, which is, of course, not
always the best choice. In this work, we propose a generalized correntropy that
adopts the generalized Gaussian density (GGD) function as the kernel (not
necessarily a Mercer kernel), and present some important properties. We further
propose the generalized maximum correntropy criterion (GMCC), and apply it to
adaptive filtering. An adaptive algorithm, called the GMCC algorithm, is
derived, and the mean square convergence performance is studied. We show that
the proposed algorithm is very stable and can achieve zero probability of
divergence (POD). Simulation results confirm the theoretical expectations and
demonstrate the desirable performance of the new algorithm.","['Badong Chen', 'Lei Xing', 'Haiquan Zhao', 'Nanning Zheng', 'José C. Príncipe']","['stat.ML', 'cs.IT', 'math.IT']",2015-04-12 03:47:46+00:00
http://arxiv.org/abs/1504.02870v1,Quick sensitivity analysis for incremental data modification and its application to leave-one-out CV in linear classification problems,"We introduce a novel sensitivity analysis framework for large scale
classification problems that can be used when a small number of instances are
incrementally added or removed. For quickly updating the classifier in such a
situation, incremental learning algorithms have been intensively studied in the
literature. Although they are much more efficient than solving the optimization
problem from scratch, their computational complexity yet depends on the entire
training set size. It means that, if the original training set is large,
completely solving an incremental learning problem might be still rather
expensive. To circumvent this computational issue, we propose a novel framework
that allows us to make an inference about the updated classifier without
actually re-optimizing it. Specifically, the proposed framework can quickly
provide a lower and an upper bounds of a quantity on the unknown updated
classifier. The main advantage of the proposed framework is that the
computational cost of computing these bounds depends only on the number of
updated instances. This property is quite advantageous in a typical sensitivity
analysis task where only a small number of instances are updated. In this paper
we demonstrate that the proposed framework is applicable to various practical
sensitivity analysis tasks, and the bounds provided by the framework are often
sufficiently tight for making desired inferences.","['Shota Okumura', 'Yoshiki Suzuki', 'Ichiro Takeuchi']","['stat.ML', 'cs.LG']",2015-04-11 13:25:37+00:00
http://arxiv.org/abs/1504.02813v3,Switching nonparametric regression models for multi-curve data,"We develop and apply an approach for analyzing multi-curve data where each
curve is driven by a latent state process. The state at any particular point
determines a smooth function, forcing the individual curve to switch from one
function to another. Thus each curve follows what we call a switching
nonparametric regression model. We develop an EM algorithm to estimate the
model parameters. We also obtain standard errors for the parameter estimates of
the state process. We consider several types of state processes: independent
and identically distributed, independent but depending on a covariate and
Markov. Simulation studies show the frequentist properties of our estimates. We
apply our methods to a data set of a building's power usage.","['Camila P. E. de Souza', 'Nancy E. Heckman', 'Helena Xu']","['stat.ME', 'stat.AP', 'stat.ML']",2015-04-10 23:04:27+00:00
http://arxiv.org/abs/1504.02800v1,High-Dimensional Classification for Brain Decoding,"Brain decoding involves the determination of a subject's cognitive state or
an associated stimulus from functional neuroimaging data measuring brain
activity. In this setting the cognitive state is typically characterized by an
element of a finite set, and the neuroimaging data comprise voluminous amounts
of spatiotemporal data measuring some aspect of the neural signal. The
associated statistical problem is one of classification from high-dimensional
data. We explore the use of functional principal component analysis, mutual
information networks, and persistent homology for examining the data through
exploratory analysis and for constructing features characterizing the neural
signal for brain decoding. We review each approach from this perspective, and
we incorporate the features into a classifier based on symmetric multinomial
logistic regression with elastic net regularization. The approaches are
illustrated in an application where the task is to infer, from brain activity
measured with magnetoencephalography (MEG), the type of video stimulus shown to
a subject.","['Nicole Croteau', 'Farouk S. Nathoo', 'Jiguo Cao', 'Ryan Budney']",['stat.ML'],2015-04-10 21:54:48+00:00
http://arxiv.org/abs/1504.02723v4,A closed-form approach to Bayesian inference in tree-structured graphical models,"We consider the inference of the structure of an undirected graphical model
in an exact Bayesian framework. More specifically we aim at achieving the
inference with close-form posteriors, avoiding any sampling step. This task
would be intractable without any restriction on the considered graphs, so we
limit our exploration to mixtures of spanning trees. We consider the inference
of the structure of an undirected graphical model in a Bayesian framework. To
avoid convergence issues and highly demanding Monte Carlo sampling, we focus on
exact inference. More specifically we aim at achieving the inference with
close-form posteriors, avoiding any sampling step. To this aim, we restrict the
set of considered graphs to mixtures of spanning trees. We investigate under
which conditions on the priors - on both tree structures and parameters - exact
Bayesian inference can be achieved. Under these conditions, we derive a fast an
exact algorithm to compute the posterior probability for an edge to belong to
{the tree model} using an algebraic result called the Matrix-Tree theorem. We
show that the assumption we have made does not prevent our approach to perform
well on synthetic and flow cytometry data.","['Loïc Schwaller', 'Stéphane Robin', 'Michael Stumpf']",['stat.ML'],2015-04-10 16:01:15+00:00
http://arxiv.org/abs/1504.02719v1,Diffusion Component Analysis: Unraveling Functional Topology in Biological Networks,"Complex biological systems have been successfully modeled by biochemical and
genetic interaction networks, typically gathered from high-throughput (HTP)
data. These networks can be used to infer functional relationships between
genes or proteins. Using the intuition that the topological role of a gene in a
network relates to its biological function, local or diffusion based
""guilt-by-association"" and graph-theoretic methods have had success in
inferring gene functions. Here we seek to improve function prediction by
integrating diffusion-based methods with a novel dimensionality reduction
technique to overcome the incomplete and noisy nature of network data. In this
paper, we introduce diffusion component analysis (DCA), a framework that plugs
in a diffusion model and learns a low-dimensional vector representation of each
node to encode the topological properties of a network. As a proof of concept,
we demonstrate DCA's substantial improvement over state-of-the-art
diffusion-based approaches in predicting protein function from molecular
interaction networks. Moreover, our DCA framework can integrate multiple
networks from heterogeneous sources, consisting of genomic information,
biochemical experiments and other resources, to even further improve function
prediction. Yet another layer of performance gain is achieved by integrating
the DCA framework with support vector machines that take our node vector
representations as features. Overall, our DCA framework provides a novel
representation of nodes in a network that can be used as a plug-in architecture
to other machine learning algorithms to decipher topological properties of and
obtain novel insights into interactomes.","['Hyunghoon Cho', 'Bonnie Berger', 'Jian Peng']","['q-bio.MN', 'cs.LG', 'cs.SI', 'stat.ML']",2015-04-10 15:42:11+00:00
http://arxiv.org/abs/1504.02712v1,Gradient of Probability Density Functions based Contrasts for Blind Source Separation (BSS),"The article derives some novel independence measures and contrast functions
for Blind Source Separation (BSS) application. For the $k^{th}$ order
differentiable multivariate functions with equal hyper-volumes (region bounded
by hyper-surfaces) and with a constraint of bounded support for $k>1$, it
proves that equality of any $k^{th}$ order derivatives implies equality of the
functions. The difference between product of marginal Probability Density
Functions (PDFs) and joint PDF of a random vector is defined as Function
Difference (FD) of a random vector. Assuming the PDFs are $k^{th}$ order
differentiable, the results on generalized functions are applied to the
independence condition. This brings new sets of independence measures and BSS
contrasts based on the $L^p$-Norm, $ p \geq 1$ of - FD, gradient of FD (GFD)
and Hessian of FD (HFD). Instead of a conventional two stage indirect
estimation method for joint PDF based BSS contrast estimation, a single stage
direct estimation of the contrasts is desired. The article targets both the
efficient estimation of the proposed contrasts and extension of the potential
theory for an information field. The potential theory has a concept of
reference potential and it is used to derive closed form expression for the
relative analysis of potential field. Analogous to it, there are introduced
concepts of Reference Information Potential (RIP) and Cross Reference
Information Potential (CRIP) based on the potential due to kernel functions
placed at selected sample points as basis in kernel methods. The quantities are
used to derive closed form expressions for information field analysis using
least squares. The expressions are used to estimate $L^2$-Norm of FD and
$L^2$-Norm of GFD based contrasts.",['Dharmani Bhaveshkumar C'],"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML', '94A17']",2015-04-10 15:28:37+00:00
http://arxiv.org/abs/1504.02412v2,Phase Transitions in Spectral Community Detection of Large Noisy Networks,"In this paper, we study the sensitivity of the spectral clustering based
community detection algorithm subject to a Erdos-Renyi type random noise model.
We prove phase transitions in community detectability as a function of the
external edge connection probability and the noisy edge presence probability
under a general network model where two arbitrarily connected communities are
interconnected by random external edges. Specifically, the community detection
performance transitions from almost perfect detectability to low detectability
as the inter-community edge connection probability exceeds some critical value.
We derive upper and lower bounds on the critical value and show that the bounds
are identical when the two communities have the same size. The phase transition
results are validated using network simulations. Using the derived expressions
for the phase transition threshold we propose a method for estimating this
threshold from observed data.","['Pin-Yu Chen', 'Alfred O. Hero III']","['cs.SI', 'physics.data-an', 'physics.soc-ph', 'stat.ML']",2015-04-09 18:23:23+00:00
http://arxiv.org/abs/1504.02406v1,Deciding when to stop: Efficient stopping of active learning guided drug-target prediction,"Active learning has shown to reduce the number of experiments needed to
obtain high-confidence drug-target predictions. However, in order to actually
save experiments using active learning, it is crucial to have a method to
evaluate the quality of the current prediction and decide when to stop the
experimentation process. Only by applying reliable stoping criteria to active
learning, time and costs in the experimental process can be actually saved. We
compute active learning traces on simulated drug-target matrices in order to
learn a regression model for the accuracy of the active learner. By analyzing
the performance of the regression model on simulated data, we design stopping
criteria for previously unseen experimental matrices. We demonstrate on four
previously characterized drug effect data sets that applying the stopping
criteria can result in upto 40% savings of the total experiments for highly
accurate predictions.","['Maja Temerinac-Ott', 'Armaghan W. Naik', 'Robert F. Murphy']","['q-bio.QM', 'cs.LG', 'stat.ML']",2015-04-09 18:10:38+00:00
http://arxiv.org/abs/1504.02382v2,"Robust, scalable and fast bootstrap method for analyzing large scale data","In this paper we address the problem of performing statistical inference for
large scale data sets i.e., Big Data. The volume and dimensionality of the data
may be so high that it cannot be processed or stored in a single computing
node. We propose a scalable, statistically robust and computationally efficient
bootstrap method, compatible with distributed processing and storage systems.
Bootstrap resamples are constructed with smaller number of distinct data points
on multiple disjoint subsets of data, similarly to the bag of little bootstrap
method (BLB) [1]. Then significant savings in computation is achieved by
avoiding the re-computation of the estimator for each bootstrap sample.
Instead, a computationally efficient fixed-point estimation equation is
analytically solved via a smart approximation following the Fast and Robust
Bootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the use
of highly robust statistical methods in analyzing large scale data sets. The
favorable statistical properties of the method are established analytically.
Numerical examples demonstrate scalability, low complexity and robust
statistical performance of the method in analyzing large data sets.","['Shahab Basiri', 'Esa Ollila', 'Visa Koivunen']","['stat.ME', 'cs.IR', 'cs.IT', 'math.IT', 'stat.CO', 'stat.ML']",2015-04-09 16:48:28+00:00
http://arxiv.org/abs/1504.02338v3,Kernel Manifold Alignment,"We introduce a kernel method for manifold alignment (KEMA) and domain
adaptation that can match an arbitrary number of data sources without needing
corresponding pairs, just few labeled examples in all domains. KEMA has
interesting properties: 1) it generalizes other manifold alignment methods, 2)
it can align manifolds of very different complexities, performing a sort of
manifold unfolding plus alignment, 3) it can define a domain-specific metric to
cope with multimodal specificities, 4) it can align data spaces of different
dimensionality, 5) it is robust to strong nonlinear feature deformations, and
6) it is closed-form invertible which allows transfer across-domains and data
synthesis. We also present a reduced-rank version for computational efficiency
and discuss the generalization performance of KEMA under Rademacher principles
of stability. KEMA exhibits very good performance over competing methods in
synthetic examples, visual object recognition and recognition of facial
expressions tasks.","['Devis Tuia', 'Gustau Camps-Valls']","['stat.ML', 'cs.LG']",2015-04-09 14:51:49+00:00
http://arxiv.org/abs/1504.02247v2,Projective simulation with generalization,"The ability to generalize is an important feature of any intelligent agent.
Not only because it may allow the agent to cope with large amounts of data, but
also because in some environments, an agent with no generalization capabilities
cannot learn. In this work we outline several criteria for generalization, and
present a dynamic and autonomous machinery that enables projective simulation
agents to meaningfully generalize. Projective simulation, a novel, physical
approach to artificial intelligence, was recently shown to perform well in
standard reinforcement learning problems, with applications in advanced
robotics as well as quantum experiments. Both the basic projective simulation
model and the presented generalization machinery are based on very simple
principles. This allows us to provide a full analytical analysis of the agent's
performance and to illustrate the benefit the agent gains by generalizing.
Specifically, we show that already in basic (but extreme) environments,
learning without generalization may be impossible, and demonstrate how the
presented generalization machinery enables the projective simulation agent to
learn.","['Alexey A. Melnikov', 'Adi Makmal', 'Vedran Dunjko', 'Hans J. Briegel']","['cs.AI', 'cs.LG', 'stat.ML']",2015-04-09 10:37:11+00:00
http://arxiv.org/abs/1504.02191v1,`local' vs. `global' parameters -- breaking the gaussian complexity barrier,"We show that if $F$ is a convex class of functions that is $L$-subgaussian,
the error rate of learning problems generated by independent noise is
equivalent to a fixed point determined by `local' covering estimates of the
class, rather than by the gaussian averages. To that end, we establish new
sharp upper and lower estimates on the error rate for such problems.",['Shahar Mendelson'],"['stat.ML', 'math.ST', 'stat.TH']",2015-04-09 04:54:29+00:00
http://arxiv.org/abs/1504.02462v3,A Group Theoretic Perspective on Unsupervised Deep Learning,"Why does Deep Learning work? What representations does it capture? How do
higher-order representations emerge? We study these questions from the
perspective of group theory, thereby opening a new approach towards a theory of
Deep learning.
  One factor behind the recent resurgence of the subject is a key algorithmic
step called {\em pretraining}: first search for a good generative model for the
input samples, and repeat the process one layer at a time. We show deeper
implications of this simple principle, by establishing a connection with the
interplay of orbits and stabilizers of group actions. Although the neural
networks themselves may not form groups, we show the existence of {\em shadow}
groups whose elements serve as close approximations.
  Over the shadow groups, the pre-training step, originally introduced as a
mechanism to better initialize a network, becomes equivalent to a search for
features with minimal orbits. Intuitively, these features are in a way the {\em
simplest}. Which explains why a deep learning network learns simple features
first. Next, we show how the same principle, when repeated in the deeper
layers, can capture higher order representations, and why representation
complexity increases as the layers get deeper.","['Arnab Paul', 'Suresh Venkatasubramanian']","['cs.LG', 'cs.NE', 'stat.ML']",2015-04-08 22:39:05+00:00
http://arxiv.org/abs/1504.01823v1,Structured Matrix Completion with Applications to Genomic Data Integration,"Matrix completion has attracted significant recent attention in many fields
including statistics, applied mathematics and electrical engineering. Current
literature on matrix completion focuses primarily on independent sampling
models under which the individual observed entries are sampled independently.
Motivated by applications in genomic data integration, we propose a new
framework of structured matrix completion (SMC) to treat structured missingness
by design. Specifically, our proposed method aims at efficient matrix recovery
when a subset of the rows and columns of an approximately low-rank matrix are
observed. We provide theoretical justification for the proposed SMC method and
derive lower bound for the estimation errors, which together establish the
optimal rate of recovery over certain classes of approximately low-rank
matrices. Simulation studies show that the method performs well in finite
sample under a variety of configurations. The method is applied to integrate
several ovarian cancer genomic studies with different extent of genomic
measurements, which enables us to construct more accurate prediction rules for
ovarian cancer survival.","['Tianxi Cai', 'T. Tony Cai', 'Anru Zhang']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2015-04-08 04:14:07+00:00
http://arxiv.org/abs/1504.01697v1,Tensor machines for learning target-specific polynomial features,"Recent years have demonstrated that using random feature maps can
significantly decrease the training and testing times of kernel-based
algorithms without significantly lowering their accuracy. Regrettably, because
random features are target-agnostic, typically thousands of such features are
necessary to achieve acceptable accuracies. In this work, we consider the
problem of learning a small number of explicit polynomial features. Our
approach, named Tensor Machines, finds a parsimonious set of features by
optimizing over the hypothesis class introduced by Kar and Karnick for random
feature maps in a target-specific manner. Exploiting a natural connection
between polynomials and tensors, we provide bounds on the generalization error
of Tensor Machines. Empirically, Tensor Machines behave favorably on several
real-world datasets compared to other state-of-the-art techniques for learning
polynomial features, and deliver significantly more parsimonious models.","['Jiyan Yang', 'Alex Gittens']","['cs.LG', 'stat.ML']",2015-04-07 18:21:37+00:00
http://arxiv.org/abs/1504.01577v1,"From Averaging to Acceleration, There is Only a Step-size","We show that accelerated gradient descent, averaged gradient descent and the
heavy-ball method for non-strongly-convex problems may be reformulated as
constant parameter second-order difference equation algorithms, where stability
of the system is equivalent to convergence at rate O(1/n 2), where n is the
number of iterations. We provide a detailed analysis of the eigenvalues of the
corresponding linear dynamical system , showing various oscillatory and
non-oscillatory behaviors, together with a sharp stability result with explicit
constants. We also consider the situation where noisy gradients are available,
where we extend our general convergence result, which suggests an alternative
algorithm (i.e., with different step sizes) that exhibits the good aspects of
both averaging and acceleration.","['Nicolas Flammarion', 'Francis Bach']","['stat.ML', 'math.OC']",2015-04-07 12:29:59+00:00
http://arxiv.org/abs/1504.01515v2,Simultaneously sparse and low-rank abundance matrix estimation for hyperspectral image unmixing,"In a plethora of applications dealing with inverse problems, e.g. in image
processing, social networks, compressive sensing, biological data processing
etc., the signal of interest is known to be structured in several ways at the
same time. This premise has recently guided the research to the innovative and
meaningful idea of imposing multiple constraints on the parameters involved in
the problem under study. For instance, when dealing with problems whose
parameters form sparse and low-rank matrices, the adoption of suitably combined
constraints imposing sparsity and low-rankness, is expected to yield
substantially enhanced estimation results. In this paper, we address the
spectral unmixing problem in hyperspectral images. Specifically, two novel
unmixing algorithms are introduced, in an attempt to exploit both spatial
correlation and sparse representation of pixels lying in homogeneous regions of
hyperspectral images. To this end, a novel convex mixed penalty term is first
defined consisting of the sum of the weighted $\ell_1$ and the weighted nuclear
norm of the abundance matrix corresponding to a small area of the image
determined by a sliding square window. This penalty term is then used to
regularize a conventional quadratic cost function and impose simultaneously
sparsity and row-rankness on the abundance matrix. The resulting regularized
cost function is minimized by a) an incremental proximal sparse and low-rank
unmixing algorithm and b) an algorithm based on the alternating minimization
method of multipliers (ADMM). The effectiveness of the proposed algorithms is
illustrated in experiments conducted both on simulated and real data.","['Paris Giampouras', 'Konstantinos Themelis', 'Athanasios Rontogiannis', 'Konstantinos Koutroumbas']","['cs.CV', 'math.OC', 'stat.ML']",2015-04-07 08:23:45+00:00
http://arxiv.org/abs/1504.01492v1,Efficient SDP Inference for Fully-connected CRFs Based on Low-rank Decomposition,"Conditional Random Fields (CRF) have been widely used in a variety of
computer vision tasks. Conventional CRFs typically define edges on neighboring
image pixels, resulting in a sparse graph such that efficient inference can be
performed. However, these CRFs fail to model long-range contextual
relationships. Fully-connected CRFs have thus been proposed. While there are
efficient approximate inference methods for such CRFs, usually they are
sensitive to initialization and make strong assumptions. In this work, we
develop an efficient, yet general algorithm for inference on fully-connected
CRFs. The algorithm is based on a scalable SDP algorithm and the low- rank
approximation of the similarity/kernel matrix. The core of the proposed
algorithm is a tailored quasi-Newton method that takes advantage of the
low-rank matrix approximation when solving the specialized SDP dual problem.
Experiments demonstrate that our method can be applied on fully-connected CRFs
that cannot be solved previously, such as pixel-level image co-segmentation.","['Peng Wang', 'Chunhua Shen', 'Anton van den Hengel']","['cs.CV', 'cs.LG', 'stat.ML']",2015-04-07 06:43:50+00:00
http://arxiv.org/abs/1504.01483v1,Transferring Knowledge from a RNN to a DNN,"Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art
results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent
Neural Network (RNN) models have been shown to outperform DNNs counterparts.
However, state-of-the-art DNN and RNN models tend to be impractical to deploy
on embedded systems with limited computational capacity. Traditionally, the
approach for embedded platforms is to either train a small DNN directly, or to
train a small DNN that learns the output distribution of a large DNN. In this
paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We
use the RNN model to generate soft alignments and minimize the Kullback-Leibler
divergence against the small DNN. The small DNN trained on the soft RNN
alignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task
compared to a baseline 4.54 WER or more than 13% relative improvement.","['William Chan', 'Nan Rosemary Ke', 'Ian Lane']","['cs.LG', 'cs.CL', 'cs.NE', 'stat.ML']",2015-04-07 06:15:44+00:00
http://arxiv.org/abs/1504.01482v1,Deep Recurrent Neural Networks for Acoustic Modelling,"We present a novel deep Recurrent Neural Network (RNN) model for acoustic
modelling in Automatic Speech Recognition (ASR). We term our contribution as a
TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with
Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory
(BLSTM), and a final DNN. The first DNN acts as a feature processor to our
model, the BLSTM then generates a context from the sequence acoustic signal,
and the final DNN takes the context and models the posterior probabilities of
the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)
eval92 task or more than 8% relative improvement over the baseline DNN models.","['William Chan', 'Ian Lane']","['cs.LG', 'cs.CL', 'cs.NE', 'stat.ML']",2015-04-07 06:12:14+00:00
http://arxiv.org/abs/1504.01369v4,Information Recovery from Pairwise Measurements,"This paper is concerned with jointly recovering $n$ node-variables $\left\{
x_{i}\right\}_{1\leq i\leq n}$ from a collection of pairwise difference
measurements. Imagine we acquire a few observations taking the form of
$x_{i}-x_{j}$; the observation pattern is represented by a measurement graph
$\mathcal{G}$ with an edge set $\mathcal{E}$ such that $x_{i}-x_{j}$ is
observed if and only if $(i,j)\in\mathcal{E}$. To account for noisy
measurements in a general manner, we model the data acquisition process by a
set of channels with given input/output transition measures. Employing
information-theoretic tools applied to channel decoding problems, we develop a
\emph{unified} framework to characterize the fundamental recovery criterion,
which accommodates general graph structures, alphabet sizes, and channel
transition measures. In particular, our results isolate a family of
\emph{minimum} \emph{channel divergence measures} to characterize the degree of
measurement corruption, which together with the size of the minimum cut of
$\mathcal{G}$ dictates the feasibility of exact information recovery. For
various homogeneous graphs, the recovery condition depends almost only on the
edge sparsity of the measurement graph irrespective of other graphical metrics;
alternatively, the minimum sample complexity required for these graphs scales
like \[ \text{minimum sample complexity }\asymp\frac{n\log
n}{\mathsf{Hel}_{1/2}^{\min}} \] for certain information metric
$\mathsf{Hel}_{1/2}^{\min}$ defined in the main text, as long as the alphabet
size is not super-polynomial in $n$. We apply our general theory to three
concrete applications, including the stochastic block model, the outlier model,
and the haplotype assembly problem. Our theory leads to order-wise tight
recovery conditions for all these scenarios.","['Yuxin Chen', 'Changho Suh', 'Andrea J. Goldsmith']","['cs.IT', 'cs.DM', 'cs.LG', 'math.IT', 'math.ST', 'stat.ML', 'stat.TH']",2015-04-06 19:47:01+00:00
http://arxiv.org/abs/1504.01362v7,A New Approach to Building the Interindustry Input--Output Table,"We present a new approach to estimating the interdependence of industries in
an economy by applying data science solutions. By exploiting interfirm
buyer--seller network data, we show that the problem of estimating the
interdependence of industries is similar to the problem of uncovering the
latent block structure in network science literature. To estimate the
underlying structure with greater accuracy, we propose an extension of the
sparse block model that incorporates node textual information and an unbounded
number of industries and interactions among them. The latter task is
accomplished by extending the well-known Chinese restaurant process to two
dimensions. Inference is based on collapsed Gibbs sampling, and the model is
evaluated on both synthetic and real-world datasets. We show that the proposed
model improves in predictive accuracy and successfully provides a satisfactory
solution to the motivated problem. We also discuss issues that affect the
future performance of this approach.",['Ryohei Hisano'],['stat.ML'],2015-04-06 19:18:49+00:00
http://arxiv.org/abs/1504.01344v1,Early Stopping is Nonparametric Variational Inference,"We show that unconverged stochastic gradient descent can be interpreted as a
procedure that samples from a nonparametric variational approximate posterior
distribution. This distribution is implicitly defined as the transformation of
an initial distribution by a sequence of optimization updates. By tracking the
change in entropy over this sequence of transformations during optimization, we
form a scalable, unbiased estimate of the variational lower bound on the log
marginal likelihood. We can use this bound to optimize hyperparameters instead
of using cross-validation. This Bayesian interpretation of SGD suggests
improved, overfitting-resistant optimization procedures, and gives a
theoretical foundation for popular tricks such as early stopping and
ensembling. We investigate the properties of this marginal likelihood estimator
on neural network models.","['Dougal Maclaurin', 'David Duvenaud', 'Ryan P. Adams']","['stat.ML', 'cs.LG']",2015-04-06 18:19:45+00:00
http://arxiv.org/abs/1504.01294v2,A Probabilistic $\ell_1$ Method for Clustering High Dimensional Data,"In general, the clustering problem is NP-hard, and global optimality cannot
be established for non-trivial instances. For high-dimensional data,
distance-based methods for clustering or classification face an additional
difficulty, the unreliability of distances in very high-dimensional spaces. We
propose a distance-based iterative method for clustering data in very
high-dimensional space, using the $\ell_1$-metric that is less sensitive to
high dimensionality than the Euclidean distance. For $K$ clusters in
$\mathbb{R}^n$, the problem decomposes to $K$ problems coupled by
probabilities, and an iteration reduces to finding $Kn$ weighted medians of
points on a line. The complexity of the algorithm is linear in the dimension of
the data space, and its performance was observed to improve significantly as
the dimension increases.","['Tsvetan Asamov', 'Adi Ben-Israel']","['math.ST', 'cs.LG', 'math.OC', 'stat.ML', 'stat.TH']",2015-04-06 14:49:13+00:00
http://arxiv.org/abs/1504.01255v3,Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding,"This paper presents a new semi-supervised framework with convolutional neural
networks (CNNs) for text categorization. Unlike the previous approaches that
rely on word embeddings, our method learns embeddings of small text regions
from unlabeled data for integration into a supervised CNN. The proposed scheme
for embedding learning is based on the idea of two-view semi-supervised
learning, which is intended to be useful for the task of interest even though
the training is done on unlabeled data. Our models achieve better results than
previous approaches on sentiment classification and topic classification tasks.","['Rie Johnson', 'Tong Zhang']","['stat.ML', 'cs.CL', 'cs.LG']",2015-04-06 10:42:07+00:00
http://arxiv.org/abs/1504.01169v1,Efficient Dictionary Learning via Very Sparse Random Projections,"Performing signal processing tasks on compressive measurements of data has
received great attention in recent years. In this paper, we extend previous
work on compressive dictionary learning by showing that more general random
projections may be used, including sparse ones. More precisely, we examine
compressive K-means clustering as a special case of compressive dictionary
learning and give theoretical guarantees for its performance for a very general
class of random projections. We then propose a memory and computation efficient
dictionary learning algorithm, specifically designed for analyzing large
volumes of high-dimensional data, which learns the dictionary from very sparse
random projections. Experimental results demonstrate that our approach allows
for reduction of computational complexity and memory/data access, with
controllable loss in accuracy.","['Farhad Pourkamali-Anaraki', 'Stephen Becker', 'Shannon M. Hughes']","['stat.ML', 'cs.LG']",2015-04-05 23:20:47+00:00
http://arxiv.org/abs/1504.01132v3,Recursive Partitioning for Heterogeneous Causal Effects,"In this paper we study the problems of estimating heterogeneity in causal
effects in experimental or observational studies and conducting inference about
the magnitude of the differences in treatment effects across subsets of the
population. In applications, our method provides a data-driven approach to
determine which subpopulations have large or small treatment effects and to
test hypotheses about the differences in these effects. For experiments, our
method allows researchers to identify heterogeneity in treatment effects that
was not specified in a pre-analysis plan, without concern about invalidating
inference due to multiple testing. In most of the literature on supervised
machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal
is to build a model of the relationship between a unit's attributes and an
observed outcome. A prominent role in these methods is played by
cross-validation which compares predictions to actual outcomes in test samples,
in order to select the level of complexity of the model that provides the best
predictive power. Our method is closely related, but it differs in that it is
tailored for predicting causal effects of a treatment rather than a unit's
outcome. The challenge is that the ""ground truth"" for a causal effect is not
observed for any individual unit: we observe the unit with the treatment, or
without the treatment, but not both at the same time. Thus, it is not obvious
how to use cross-validation to determine whether a causal effect has been
accurately predicted. We propose several novel cross-validation criteria for
this problem and demonstrate through simulations the conditions under which
they perform better than standard methods for the problem of causal effects. We
then apply the method to a large-scale field experiment re-ranking results on a
search engine.","['Susan Athey', 'Guido Imbens']","['stat.ML', 'econ.EM']",2015-04-05 16:01:44+00:00
http://arxiv.org/abs/1504.01070v1,"Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via Eigenvector and Semidefinite Programming Synchronization","We consider the classic problem of establishing a statistical ranking of a
set of n items given a set of inconsistent and incomplete pairwise comparisons
between such items. Instantiations of this problem occur in numerous
applications in data analysis (e.g., ranking teams in sports data), computer
vision, and machine learning. We formulate the above problem of ranking with
incomplete noisy information as an instance of the group synchronization
problem over the group SO(2) of planar rotations, whose usefulness has been
demonstrated in numerous applications in recent years. Its least squares
solution can be approximated by either a spectral or a semidefinite programming
(SDP) relaxation, followed by a rounding procedure. We perform extensive
numerical simulations on both synthetic and real-world data sets, showing that
our proposed method compares favorably to other algorithms from the recent
literature. Existing theoretical guarantees on the group synchronization
problem imply lower bounds on the largest amount of noise permissible in the
ranking data while still achieving exact recovery. We propose a similar
synchronization-based algorithm for the rank-aggregation problem, which
integrates in a globally consistent ranking pairwise comparisons given by
different rating systems on the same set of items. We also discuss the problem
of semi-supervised ranking when there is available information on the ground
truth rank of a subset of players, and propose an algorithm based on SDP which
recovers the ranks of the remaining players. Finally, synchronization-based
ranking, combined with a spectral technique for the densest subgraph problem,
allows one to extract locally-consistent partial rankings, in other words, to
identify the rank of a small subset of players whose pairwise comparisons are
less noisy than the rest of the data, which other methods are not able to
identify.",['Mihai Cucuringu'],"['cs.LG', 'cs.SI', 'math.OC', 'stat.ML']",2015-04-05 01:40:35+00:00
http://arxiv.org/abs/1504.01046v2,Graph Connectivity in Noisy Sparse Subspace Clustering,"Subspace clustering is the problem of clustering data points into a union of
low-dimensional linear/affine subspaces. It is the mathematical abstraction of
many important problems in computer vision, image processing and machine
learning. A line of recent work (4, 19, 24, 20) provided strong theoretical
guarantee for sparse subspace clustering (4), the state-of-the-art algorithm
for subspace clustering, on both noiseless and noisy data sets. It was shown
that under mild conditions, with high probability no two points from different
subspaces are clustered together. Such guarantee, however, is not sufficient
for the clustering to be correct, due to the notorious ""graph connectivity
problem"" (15). In this paper, we investigate the graph connectivity problem for
noisy sparse subspace clustering and show that a simple post-processing
procedure is capable of delivering consistent clustering under certain ""general
position"" or ""restricted eigenvalue"" assumptions. We also show that our
condition is almost tight with adversarial noise perturbation by constructing a
counter-example. These results provide the first exact clustering guarantee of
noisy SSC for subspaces of dimension greater then 3.","['Yining Wang', 'Yu-Xiang Wang', 'Aarti Singh']","['stat.ML', 'cs.LG']",2015-04-04 20:05:17+00:00
