id,title,abstract,authors,categories,date
http://arxiv.org/abs/1810.05752v4,Global Convergence of EM Algorithm for Mixtures of Two Component Linear Regression,"The Expectation-Maximization algorithm is perhaps the most broadly used
algorithm for inference of latent variable problems. A theoretical
understanding of its performance, however, largely remains lacking. Recent
results established that EM enjoys global convergence for Gaussian Mixture
Models. For Mixed Linear Regression, however, only local convergence results
have been established, and those only for the high SNR regime. We show here
that EM converges for mixed linear regression with two components (it is known
that it may fail to converge for three or more), and moreover that this
convergence holds for random initialization. Our analysis reveals that EM
exhibits very different behavior in Mixed Linear Regression from its behavior
in Gaussian Mixture Models, and hence our proofs require the development of
several new ideas.","['Jeongyeol Kwon', 'Wei Qian', 'Constantine Caramanis', 'Yudong Chen', 'Damek Davis']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2018-10-12 22:59:30+00:00
http://arxiv.org/abs/1810.05751v2,Policy Transfer with Strategy Optimization,"Computer simulation provides an automatic and safe way for training robotic
control policies to achieve complex tasks such as locomotion. However, a policy
trained in simulation usually does not transfer directly to the real hardware
due to the differences between the two environments. Transfer learning using
domain randomization is a promising approach, but it usually assumes that the
target environment is close to the distribution of the training environments,
thus relying heavily on accurate system identification. In this paper, we
present a different approach that leverages domain randomization for
transferring control policies to unknown environments. The key idea that,
instead of learning a single policy in the simulation, we simultaneously learn
a family of policies that exhibit different behaviors. When tested in the
target environment, we directly search for the best policy in the family based
on the task performance, without the need to identify the dynamic parameters.
We evaluate our method on five simulated robotic control problems with
different discrepancies in the training and testing environment and demonstrate
that our method can overcome larger modeling errors compared to training a
robust policy or an adaptive policy.","['Wenhao Yu', 'C. Karen Liu', 'Greg Turk']","['cs.LG', 'cs.RO', 'stat.ML']",2018-10-12 22:53:30+00:00
http://arxiv.org/abs/1810.05749v3,Graph HyperNetworks for Neural Architecture Search,"Neural architecture search (NAS) automatically finds the best task-specific
neural network topology, outperforming many manual architecture designs.
However, it can be prohibitively expensive as the search requires training
thousands of different networks, while each can last for hours. In this work,
we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an
architecture, it directly generates the weights by running inference on a graph
neural network. GHNs model the topology of an architecture and therefore can
predict network performance more accurately than regular hypernetworks and
premature early stopping. To perform NAS, we randomly sample architectures and
use the validation accuracy of networks with GHN generated weights as the
surrogate search signal. GHNs are fast -- they can search nearly 10 times
faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be
further extended to the anytime prediction setting, where they have found
networks with better speed-accuracy tradeoff than the state-of-the-art manual
designs.","['Chris Zhang', 'Mengye Ren', 'Raquel Urtasun']","['cs.LG', 'cs.CV', 'stat.ML']",2018-10-12 22:21:05+00:00
http://arxiv.org/abs/1810.05741v1,Explaining Black Boxes on Sequential Data using Weighted Automata,"Understanding how a learned black box works is of crucial interest for the
future of Machine Learning. In this paper, we pioneer the question of the
global interpretability of learned black box models that assign numerical
values to symbolic sequential data. To tackle that task, we propose a spectral
algorithm for the extraction of weighted automata (WA) from such black boxes.
This algorithm does not require the access to a dataset or to the inner
representation of the black box: the inferred model can be obtained solely by
querying the black box, feeding it with inputs and analyzing its outputs.
Experiments using Recurrent Neural Networks (RNN) trained on a wide collection
of 48 synthetic datasets and 2 real datasets show that the obtained
approximation is of great quality.","['Stephane Ayache', 'Remi Eyraud', 'Noe Goudian']","['cs.LG', 'stat.ML']",2018-10-12 21:35:23+00:00
http://arxiv.org/abs/1810.05728v4,Estimating Information Flow in Deep Neural Networks,"We study the flow of information and the evolution of internal
representations during deep neural network (DNN) training, aiming to demystify
the compression aspect of the information bottleneck theory. The theory
suggests that DNN training comprises a rapid fitting phase followed by a slower
compression phase, in which the mutual information $I(X;T)$ between the input
$X$ and internal representations $T$ decreases. Several papers observe
compression of estimated mutual information on different DNN models, but the
true $I(X;T)$ over these networks is provably either constant (discrete $X$) or
infinite (continuous $X$). This work explains the discrepancy between theory
and experiments, and clarifies what was actually measured by these past works.
To this end, we introduce an auxiliary (noisy) DNN framework for which $I(X;T)$
is a meaningful quantity that depends on the network's parameters. This noisy
framework is shown to be a good proxy for the original (deterministic) DNN both
in terms of performance and the learned representations. We then develop a
rigorous estimator for $I(X;T)$ in noisy DNNs and observe compression in
various models. By relating $I(X;T)$ in the noisy DNN to an
information-theoretic communication problem, we show that compression is driven
by the progressive clustering of hidden representations of inputs from the same
class. Several methods to directly monitor clustering of hidden
representations, both in noisy and deterministic DNNs, are used to show that
meaningful clusters form in the $T$ space. Finally, we return to the estimator
of $I(X;T)$ employed in past works, and demonstrate that while it fails to
capture the true (vacuous) mutual information, it does serve as a measure for
clustering. This clarifies the past observations of compression and isolates
the geometric clustering of hidden representations as the true phenomenon of
interest.","['Ziv Goldfeld', 'Ewout van den Berg', 'Kristjan Greenewald', 'Igor Melnyk', 'Nam Nguyen', 'Brian Kingsbury', 'Yury Polyanskiy']","['cs.LG', 'stat.ML']",2018-10-12 21:11:30+00:00
http://arxiv.org/abs/1810.05713v1,Improving Generalization of Sequence Encoder-Decoder Networks for Inverse Imaging of Cardiac Transmembrane Potential,"Deep learning models have shown state-of-the-art performance in many inverse
reconstruction problems. However, it is not well understood what properties of
the latent representation may improve the generalization ability of the
network. Furthermore, limited models have been presented for inverse
reconstructions over time sequences. In this paper, we study the generalization
ability of a sequence encoder decoder model for solving inverse reconstructions
on time sequences. Our central hypothesis is that the generalization ability of
the network can be improved by 1) constrained stochasticity and 2) global
aggregation of temporal information in the latent space. First, drawing from
analytical learning theory, we theoretically show that a stochastic latent
space will lead to an improved generalization ability. Second, we consider an
LSTM encoder-decoder architecture that compresses a global latent vector from
all last-layer units in the LSTM encoder. This model is compared with
alternative LSTM encoder-decoder architectures, each in deterministic and
stochastic versions. The results demonstrate that the generalization ability of
an inverse reconstruction network can be improved by constrained stochasticity
combined with global aggregation of temporal information in the latent space.","['Sandesh Ghimire', 'Prashnna Kumar Gyawali', 'John L Sapp', 'Milan Horacek', 'Linwei Wang']","['cs.LG', 'stat.ML']",2018-10-12 20:42:23+00:00
http://arxiv.org/abs/1810.07322v2,Functionality-Oriented Convolutional Filter Pruning,"The sophisticated structure of Convolutional Neural Network (CNN) allows for
outstanding performance, but at the cost of intensive computation. As
significant redundancies inevitably present in such a structure, many works
have been proposed to prune the convolutional filters for computation cost
reduction. Although extremely effective, most works are based only on
quantitative characteristics of the convolutional filters, and highly overlook
the qualitative interpretation of individual filter's specific functionality.
In this work, we interpreted the functionality and redundancy of the
convolutional filters from different perspectives, and proposed a
functionality-oriented filter pruning method. With extensive experiment
results, we proved the convolutional filters' qualitative significance
regardless of magnitude, demonstrated significant neural network redundancy due
to repetitive filter functions, and analyzed the filter functionality defection
under inappropriate retraining process. Such an interpretable pruning approach
not only offers outstanding computation cost optimization over previous filter
pruning methods, but also interprets filter pruning process.","['Zhuwei Qin', 'Fuxun Yu', 'Chenchen Liu', 'Xiang Chen']","['cs.LG', 'cs.CV', 'stat.ML']",2018-10-12 20:39:47+00:00
http://arxiv.org/abs/1810.05691v4,"Faster k-Medoids Clustering: Improving the PAM, CLARA, and CLARANS Algorithms","Clustering non-Euclidean data is difficult, and one of the most used
algorithms besides hierarchical clustering is the popular algorithm
Partitioning Around Medoids (PAM), also simply referred to as k-medoids. In
Euclidean geometry the mean-as used in k-means-is a good estimator for the
cluster center, but this does not hold for arbitrary dissimilarities. PAM uses
the medoid instead, the object with the smallest dissimilarity to all others in
the cluster. This notion of centrality can be used with any (dis-)similarity,
and thus is of high relevance to many domains such as biology that require the
use of Jaccard, Gower, or more complex distances.
  A key issue with PAM is its high run time cost. We propose modifications to
the PAM algorithm to achieve an O(k)-fold speedup in the second SWAP phase of
the algorithm, but will still find the same results as the original PAM
algorithm. If we slightly relax the choice of swaps performed (at comparable
quality), we can further accelerate the algorithm by performing up to k swaps
in each iteration. With the substantially faster SWAP, we can now also explore
alternative strategies for choosing the initial medoids. We also show how the
CLARA and CLARANS algorithms benefit from these modifications. It can easily be
combined with earlier approaches to use PAM and CLARA on big data (some of
which use PAM as a subroutine, hence can immediately benefit from these
improvements), where the performance with high k becomes increasingly
important.
  In experiments on real data with k=100, we observed a 200-fold speedup
compared to the original PAM SWAP algorithm, making PAM applicable to larger
data sets as long as we can afford to compute a distance matrix, and in
particular to higher k (at k=2, the new SWAP was only 1.5 times faster, as the
speedup is expected to increase with k).","['Erich Schubert', 'Peter J. Rousseeuw']","['cs.LG', 'stat.ML']",2018-10-12 19:26:28+00:00
http://arxiv.org/abs/1810.05633v2,"Stochastic (Approximate) Proximal Point Methods: Convergence, Optimality, and Adaptivity","We develop model-based methods for solving stochastic convex optimization
problems, introducing the approximate-proximal point, or aProx, family, which
includes stochastic subgradient, proximal point, and bundle methods. When the
modeling approaches we propose are appropriately accurate, the methods enjoy
stronger convergence and robustness guarantees than classical approaches, even
though the model-based methods typically add little to no computational
overhead over stochastic subgradient methods. For example, we show that
improved models converge with probability 1 and enjoy optimal asymptotic
normality results under weak assumptions; these methods are also adaptive to a
natural class of what we term easy optimization problems, achieving linear
convergence under appropriate strong growth conditions on the objective. Our
substantial experimental investigation shows the advantages of more accurate
modeling over standard subgradient methods across many smooth and non-smooth
optimization problems.","['Hilal Asi', 'John C. Duchi']","['math.OC', 'stat.ML']",2018-10-12 17:56:08+00:00
http://arxiv.org/abs/1810.05598v5,Tuning Fairness by Balancing Target Labels,"The issue of fairness in machine learning models has recently attracted a lot
of attention as ensuring it will ensure continued confidence of the general
public in the deployment of machine learning systems. We focus on mitigating
the harm incurred by a biased machine learning system that offers better
outputs (e.g. loans, job interviews) for certain groups than for others. We
show that bias in the output can naturally be controlled in probabilistic
models by introducing a latent target output. This formulation has several
advantages: first, it is a unified framework for several notions of group
fairness such as Demographic Parity and Equality of Opportunity; second, it is
expressed as a marginalisation instead of a constrained problem; and third, it
allows the encoding of our knowledge of what unbiased outputs should be.
Practically, the second allows us to avoid unstable constrained optimisation
procedures and to reuse off-the-shelf toolboxes. The latter translates to the
ability to control the level of fairness by directly varying fairness target
rates. In contrast, existing approaches rely on intermediate, arguably
unintuitive, control parameters such as covariance thresholds.","['Thomas Kehrenberg', 'Zexun Chen', 'Novi Quadrianto']","['stat.ML', 'cs.LG']",2018-10-12 16:36:23+00:00
http://arxiv.org/abs/1810.05597v3,Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion,"This paper proposes a representational model for grid cells. In this model,
the 2D self-position of the agent is represented by a high-dimensional vector,
and the 2D self-motion or displacement of the agent is represented by a matrix
that transforms the vector. Each component of the vector is a unit or a cell.
The model consists of the following three sub-models. (1) Vector-matrix
multiplication. The movement from the current position to the next position is
modeled by matrix-vector multiplication, i.e., the vector of the next position
is obtained by multiplying the matrix of the motion to the vector of the
current position. (2) Magnified local isometry. The angle between two nearby
vectors equals the Euclidean distance between the two corresponding positions
multiplied by a magnifying factor. (3) Global adjacency kernel. The inner
product between two vectors measures the adjacency between the two
corresponding positions, which is defined by a kernel function of the Euclidean
distance between the two positions. Our representational model has explicit
algebra and geometry. It can learn hexagon patterns of grid cells, and it is
capable of error correction, path integral and path planning.","['Ruiqi Gao', 'Jianwen Xie', 'Song-Chun Zhu', 'Ying Nian Wu']","['stat.ML', 'cs.LG', 'cs.NE']",2018-10-12 16:34:07+00:00
http://arxiv.org/abs/1810.05596v1,Custom Dual Transportation Mode Detection by Smartphone Devices Exploiting Sensor Diversity,"Making applications aware of the mobility experienced by the user can open
the door to a wide range of novel services in different use-cases, from smart
parking to vehicular traffic monitoring. In the literature, there are many
different studies demonstrating the theoretical possibility of performing
Transportation Mode Detection (TMD) by mining smart-phones embedded sensors
data. However, very few of them provide details on the benchmarking process and
on how to implement the detection process in practice. In this study, we
provide guidelines and fundamental results that can be useful for both
researcher and practitioners aiming at implementing a working TMD system. These
guidelines consist of three main contributions. First, we detail the
construction of a training dataset, gathered by heterogeneous users and
including five different transportation modes; the dataset is made available to
the research community as reference benchmark. Second, we provide an in-depth
analysis of the sensor-relevance for the case of Dual TDM, which is required by
most of mobility-aware applications. Third, we investigate the possibility to
perform TMD of unknown users/instances not present in the training set and we
compare with state-of-the-art Android APIs for activity recognition.","['Claudia Carpineti', 'Vincenzo Lomonaco', 'Luca Bedogni', 'Marco Di Felice', 'Luciano Bononi']","['cs.LG', 'stat.ML']",2018-10-12 16:31:43+00:00
http://arxiv.org/abs/1810.05571v1,Facility Locations Utility for Uncovering Classifier Overconfidence,"Assessing the predictive accuracy of black box classifiers is challenging in
the absence of labeled test datasets. In these scenarios we may need to rely on
a human oracle to evaluate individual predictions; presenting the challenge to
create query algorithms to guide the search for points that provide the most
information about the classifier's predictive characteristics. Previous works
have focused on developing utility models and query algorithms for discovering
unknown unknowns --- misclassifications with a predictive confidence above some
arbitrary threshold. However, if misclassifications occur at the rate reflected
by the confidence values, then these search methods reveal nothing more than a
proper assessment of predictive certainty. We are unable to properly mitigate
the risks associated with model deficiency when the model's confidence in
prediction exceeds the actual model accuracy. We propose a facility locations
utility model and corresponding greedy query algorithm that instead searches
for overconfident unknown unknowns. Through robust empirical experiments we
demonstrate that the greedy query algorithm with the facility locations utility
model consistently results in oracle queries with superior performance in
discovering overconfident unknown unknowns than previous methods.","['Karsten Maurer', 'Walter Bennette']","['stat.ML', 'cs.LG']",2018-10-12 15:19:19+00:00
http://arxiv.org/abs/1810.05567v1,Grand Challenge: Real-time Destination and ETA Prediction for Maritime Traffic,"In this paper, we present our approach for solving the DEBS Grand Challenge
2018. The challenge asks to provide a prediction for (i) a destination and the
(ii) arrival time of ships in a streaming-fashion using Geo-spatial data in the
maritime context. Novel aspects of our approach include the use of ensemble
learning based on Random Forest, Gradient Boosting Decision Trees (GBDT),
XGBoost Trees and Extremely Randomized Trees (ERT) in order to provide a
prediction for a destination while for the arrival time, we propose the use of
Feed-forward Neural Networks. In our evaluation, we were able to achieve an
accuracy of 97% for the port destination classification problem and 90% (in
mins) for the ETA prediction.","['Oleh Bodunov', 'Florian Schmidt', 'Andr√© Martin', 'Andrey Brito', 'Christof Fetzer']","['cs.LG', 'stat.ML']",2018-10-12 15:14:00+00:00
http://arxiv.org/abs/1810.05558v2,Variational Bayesian Monte Carlo,"Many probabilistic models of interest in scientific computing and machine
learning have expensive, black-box likelihoods that prevent the application of
standard techniques for Bayesian inference, such as MCMC, which would require
access to the gradient or a large number of likelihood evaluations. We
introduce here a novel sample-efficient inference framework, Variational
Bayesian Monte Carlo (VBMC). VBMC combines variational inference with
Gaussian-process based, active-sampling Bayesian quadrature, using the latter
to efficiently approximate the intractable integral in the variational
objective. Our method produces both a nonparametric approximation of the
posterior distribution and an approximate lower bound of the model evidence,
useful for model selection. We demonstrate VBMC both on several synthetic
likelihoods and on a neuronal model with data from real neurons. Across all
tested problems and dimensions (up to $D = 10$), VBMC performs consistently
well in reconstructing the posterior and the model evidence with a limited
budget of likelihood evaluations, unlike other methods that work only in very
low dimensions. Our framework shows great promise as a novel tool for posterior
and model inference with expensive, black-box likelihoods.",['Luigi Acerbi'],"['stat.ML', 'cs.LG', 'q-bio.NC', 'q-bio.QM']",2018-10-12 14:50:13+00:00
http://arxiv.org/abs/1810.05546v5,Uncertainty in Neural Networks: Approximately Bayesian Ensembling,"Understanding the uncertainty of a neural network's (NN) predictions is
essential for many purposes. The Bayesian framework provides a principled
approach to this, however applying it to NNs is challenging due to large
numbers of parameters and data. Ensembling NNs provides an easily
implementable, scalable method for uncertainty quantification, however, it has
been criticised for not being Bayesian. This work proposes one modification to
the usual process that we argue does result in approximate Bayesian inference;
regularising parameters about values drawn from a distribution which can be set
equal to the prior. A theoretical analysis of the procedure in a simplified
setting suggests the recovered posterior is centred correctly but tends to have
an underestimated marginal variance, and overestimated correlation. However,
two conditions can lead to exact recovery. We argue that these conditions are
partially present in NNs. Empirical evaluations demonstrate it has an advantage
over standard ensembling, and is competitive with variational methods.","['Tim Pearce', 'Felix Leibfried', 'Alexandra Brintrup', 'Mohamed Zaki', 'Andy Neely']","['stat.ML', 'cs.LG']",2018-10-12 14:26:34+00:00
http://arxiv.org/abs/1810.05500v1,Predictive Uncertainty through Quantization,"High-risk domains require reliable confidence estimates from predictive
models. Deep latent variable models provide these, but suffer from the rigid
variational distributions used for tractable inference, which err on the side
of overconfidence. We propose Stochastic Quantized Activation Distributions
(SQUAD), which imposes a flexible yet tractable distribution over discretized
latent variables. The proposed method is scalable, self-normalizing and sample
efficient. We demonstrate that the model fully utilizes the flexible
distribution, learns interesting non-linearities, and provides predictive
uncertainty of competitive quality.","['Bastiaan S. Veeling', 'Rianne van den Berg', 'Max Welling']","['cs.LG', 'stat.ML']",2018-10-12 13:37:43+00:00
http://arxiv.org/abs/1810.12091v1,Embedding Geographic Locations for Modelling the Natural Environment using Flickr Tags and Structured Data,"Meta-data from photo-sharing websites such as Flickr can be used to obtain
rich bag-of-words descriptions of geographic locations, which have proven
valuable, among others, for modelling and predicting ecological features. One
important insight from previous work is that the descriptions obtained from
Flickr tend to be complementary to the structured information that is available
from traditional scientific resources. To better integrate these two diverse
sources of information, in this paper we consider a method for learning vector
space embeddings of geographic locations. We show experimentally that this
method improves on existing approaches, especially in cases where structured
information is available.","['Shelan S. Jeawak', 'Christopher B. Jones', 'Steven Schockaert']","['cs.IR', 'cs.CL', 'cs.CV', 'cs.LG', 'stat.ML']",2018-10-12 12:22:34+00:00
http://arxiv.org/abs/1810.05471v3,Safe Grid Search with Optimal Complexity,"Popular machine learning estimators involve regularization parameters that
can be challenging to tune, and standard strategies rely on grid search for
this task. In this paper, we revisit the techniques of approximating the
regularization path up to predefined tolerance $\epsilon$ in a unified
framework and show that its complexity is $O(1/\sqrt[d]{\epsilon})$ for
uniformly convex loss of order $d \geq 2$ and $O(1/\sqrt{\epsilon})$ for
Generalized Self-Concordant functions. This framework encompasses least-squares
but also logistic regression, a case that as far as we know was not handled as
precisely in previous works. We leverage our technique to provide refined
bounds on the validation error as well as a practical algorithm for
hyperparameter tuning. The latter has global convergence guarantee when
targeting a prescribed accuracy on the validation set. Last but not least, our
approach helps relieving the practitioner from the (often neglected) task of
selecting a stopping criterion when optimizing over the training set: our
method automatically calibrates this criterion based on the targeted accuracy
on the validation set.","['Eugene Ndiaye', 'Tam Le', 'Olivier Fercoq', 'Joseph Salmon', 'Ichiro Takeuchi']","['stat.ML', 'cs.LG', 'math.OC']",2018-10-12 12:16:52+00:00
http://arxiv.org/abs/1810.05466v1,Mode Normalization,"Normalization methods are a central building block in the deep learning
toolbox. They accelerate and stabilize training, while decreasing the
dependence on manually tuned learning rate schedules. When learning from
multi-modal distributions, the effectiveness of batch normalization (BN),
arguably the most prominent normalization method, is reduced. As a remedy, we
propose a more flexible approach: by extending the normalization to more than a
single mean and variance, we detect modes of data on-the-fly, jointly
normalizing samples that share common features. We demonstrate that our method
outperforms BN and other widely used normalization techniques in several
experiments, including single and multi-task datasets.","['Lucas Deecke', 'Iain Murray', 'Hakan Bilen']","['cs.LG', 'stat.ML']",2018-10-12 12:10:10+00:00
http://arxiv.org/abs/1810.05440v2,An algebraic-geometric approach for linear regression without correspondences,"Linear regression without correspondences is the problem of performing a
linear regression fit to a dataset for which the correspondences between the
independent samples and the observations are unknown. Such a problem naturally
arises in diverse domains such as computer vision, data mining, communications
and biology. In its simplest form, it is tantamount to solving a linear system
of equations, for which the entries of the right hand side vector have been
permuted. This type of data corruption renders the linear regression task
considerably harder, even in the absence of other corruptions, such as noise,
outliers or missing entries. Existing methods are either applicable only to
noiseless data or they are very sensitive to initialization or they work only
for partially shuffled data. In this paper we address these issues via an
algebraic geometric approach, which uses symmetric polynomials to extract
permutation-invariant constraints that the parameters $\xi^* \in \Re^n$ of the
linear regression model must satisfy. This naturally leads to a polynomial
system of $n$ equations in $n$ unknowns, which contains $\xi^*$ in its root
locus. Using the machinery of algebraic geometry we prove that as long as the
independent samples are generic, this polynomial system is always consistent
with at most $n!$ complex roots, regardless of any type of corruption inflicted
on the observations. The algorithmic implication of this fact is that one can
always solve this polynomial system and use its most suitable root as
initialization to the Expectation Maximization algorithm. To the best of our
knowledge, the resulting method is the first working solution for small values
of $n$ able to handle thousands of fully shuffled noisy observations in
milliseconds.","['Manolis C. Tsakiris', 'Liangzu Peng', 'Aldo Conca', 'Laurent Kneip', 'Yuanming Shi', 'Hayoung Choi']","['cs.LG', 'stat.ML']",2018-10-12 10:22:05+00:00
http://arxiv.org/abs/1810.05394v1,Sequential Learning of Movement Prediction in Dynamic Environments using LSTM Autoencoder,"Predicting movement of objects while the action of learning agent interacts
with the dynamics of the scene still remains a key challenge in robotics. We
propose a multi-layer Long Short Term Memory (LSTM) autoendocer network that
predicts future frames for a robot navigating in a dynamic environment with
moving obstacles. The autoencoder network is composed of a state and action
conditioned decoder network that reconstructs the future frames of video,
conditioned on the action taken by the agent. The input image frames are first
transformed into low dimensional feature vectors with a pre-trained encoder
network and then reconstructed with the LSTM autoencoder network to generate
the future frames. A virtual environment, based on the OpenAi-Gym framework for
robotics, is used to gather training data and test the proposed network. The
initial experiments show promising results indicating that these predicted
frames can be used by an appropriate reinforcement learning framework in future
to navigate around dynamic obstacles.","['Meenakshi Sarkar', 'Debasish Ghose']","['cs.LG', 'cs.CV', 'cs.RO', 'stat.ML', '68T05']",2018-10-12 08:11:13+00:00
http://arxiv.org/abs/1810.05644v2,Temporal Convolutional Memory Networks for Remaining Useful Life Estimation of Industrial Machinery,"Accurately estimating the remaining useful life (RUL) of industrial machinery
is beneficial in many real-world applications. Estimation techniques have
mainly utilized linear models or neural network based approaches with a focus
on short term time dependencies. This paper, introduces a system model that
incorporates temporal convolutions with both long term and short term time
dependencies. The proposed network learns salient features and complex temporal
variations in sensor values, and predicts the RUL. A data augmentation method
is used for increased accuracy. The proposed method is compared with several
state-of-the-art algorithms on publicly available datasets. It demonstrates
promising results, with superior results for datasets obtained from complex
environments.","['Lahiru Jayasinghe', 'Tharaka Samarasinghe', 'Chau Yuen', 'Jenny Chen Ni Low', 'Shuzhi Sam Ge']","['cs.LG', 'stat.ML']",2018-10-12 08:00:33+00:00
http://arxiv.org/abs/1810.05369v4,Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel,"Recent works have shown that on sufficiently over-parametrized neural nets,
gradient descent with relatively large initialization optimizes a prediction
function in the RKHS of the Neural Tangent Kernel (NTK). This analysis leads to
global convergence results but does not work when there is a standard $\ell_2$
regularizer, which is useful to have in practice. We show that sample
efficiency can indeed depend on the presence of the regularizer: we construct a
simple distribution in d dimensions which the optimal regularized neural net
learns with $O(d)$ samples but the NTK requires $\Omega(d^2)$ samples to learn.
To prove this, we establish two analysis tools: i) for multi-layer feedforward
ReLU nets, we show that the global minimizer of a weakly-regularized
cross-entropy loss is the max normalized margin solution among all neural nets,
which generalizes well; ii) we develop a new technique for proving lower bounds
for kernel methods, which relies on showing that the kernel cannot focus on
informative features. Motivated by our generalization results, we study whether
the regularized global optimum is attainable. We prove that for infinite-width
two-layer nets, noisy gradient descent optimizes the regularized neural net
loss to a global minimum in polynomial iterations.","['Colin Wei', 'Jason D. Lee', 'Qiang Liu', 'Tengyu Ma']","['stat.ML', 'cs.LG']",2018-10-12 06:21:22+00:00
http://arxiv.org/abs/1810.05347v1,Optimal Hierarchical Learning Path Design with Reinforcement Learning,"E-learning systems are capable of providing more adaptive and efficient
learning experiences for students than the traditional classroom setting. A key
component of such systems is the learning strategy, the algorithm that designs
the learning paths for students based on information such as the students'
current progresses, their skills, learning materials, and etc. In this paper,
we address the problem of finding the optimal learning strategy for an
E-learning system. To this end, we first develop a model for students'
hierarchical skills in the E-learning system. Based on the hierarchical skill
model and the classical cognitive diagnosis model, we further develop a
framework to model various proficiency levels of hierarchical skills. The
optimal learning strategy on top of the hierarchical structure is found by
applying a model-free reinforcement learning method, which does not require
information on students' learning transition process. The effectiveness of the
proposed framework is demonstrated via numerical experiments.","['Xiao Li', 'Hanchen Xu', 'Jinming Zhang', 'Hua-hua Chang']","['cs.LG', 'cs.CY', 'stat.ML']",2018-10-12 04:03:20+00:00
http://arxiv.org/abs/1811.10382v2,Heterogeneous multireference alignment for images with application to 2-D classification in single particle reconstruction,"Motivated by the task of 2-D classification in single particle reconstruction
by cryo-electron microscopy (cryo-EM), we consider the problem of heterogeneous
multireference alignment of images. In this problem, the goal is to estimate a
(typically small) set of target images from a (typically large) collection of
observations. Each observation is a rotated, noisy version of one of the target
images. For each individual observation, neither the rotation nor which target
image has been rotated are known. As the noise level in cryo-EM data is high,
clustering the observations and estimating individual rotations is challenging.
We propose a framework to estimate the target images directly from the
observations, completely bypassing the need to cluster or register the images.
The framework consists of two steps. First, we estimate rotation-invariant
features of the images, such as the bispectrum. These features can be estimated
to any desired accuracy, at any noise level, provided sufficiently many
observations are collected. Then, we estimate the images from the invariant
features. Numerical experiments on synthetic cryo-EM datasets demonstrate the
effectiveness of the method. Ultimately, we outline future developments
required to apply this method to experimental data.","['Chao Ma', 'Tamir Bendory', 'Nicolas Boumal', 'Fred Sigworth', 'Amit Singer']","['eess.IV', 'stat.ML']",2018-10-12 02:50:21+00:00
http://arxiv.org/abs/1810.05305v2,Block Stability for MAP Inference,"To understand the empirical success of approximate MAP inference, recent work
(Lang et al., 2018) has shown that some popular approximation algorithms
perform very well when the input instance is stable. The simplest stability
condition assumes that the MAP solution does not change at all when some of the
pairwise potentials are (adversarially) perturbed. Unfortunately, this strong
condition does not seem to be satisfied in practice. In this paper, we
introduce a significantly more relaxed condition that only requires blocks
(portions) of an input instance to be stable. Under this block stability
condition, we prove that the pairwise LP relaxation is persistent on the stable
blocks. We complement our theoretical results with an empirical evaluation of
real-world MAP inference instances from computer vision. We design an algorithm
to find stable blocks, and find that these real instances have large stable
regions. Our work gives a theoretical explanation for the widespread empirical
phenomenon of persistency for this LP relaxation.","['Hunter Lang', 'David Sontag', 'Aravindan Vijayaraghavan']","['stat.ML', 'cs.AI', 'cs.LG']",2018-10-12 01:17:38+00:00
http://arxiv.org/abs/1810.05290v2,Online Multiclass Boosting with Bandit Feedback,"We present online boosting algorithms for multiclass classification with
bandit feedback, where the learner only receives feedback about the correctness
of its prediction. We propose an unbiased estimate of the loss using a
randomized prediction, allowing the model to update its weak learners with
limited information. Using the unbiased estimate, we extend two full
information boosting algorithms (Jung et al., 2017) to the bandit setting. We
prove that the asymptotic error bounds of the bandit algorithms exactly match
their full information counterparts. The cost of restricted feedback is
reflected in the larger sample complexity. Experimental results also support
our theoretical findings, and performance of the proposed models is comparable
to that of an existing bandit boosting algorithm, which is limited to use
binary weak learners.","['Daniel T. Zhang', 'Young Hun Jung', 'Ambuj Tewari']","['stat.ML', 'cs.LG']",2018-10-11 23:47:21+00:00
http://arxiv.org/abs/1810.06640v2,Adversarial Text Generation Without Reinforcement Learning,"Generative Adversarial Networks (GANs) have experienced a recent surge in
popularity, performing competitively in a variety of tasks, especially in
computer vision. However, GAN training has shown limited success in natural
language processing. This is largely because sequences of text are discrete,
and thus gradients cannot propagate from the discriminator to the generator.
Recent solutions use reinforcement learning to propagate approximate gradients
to the generator, but this is inefficient to train. We propose to utilize an
autoencoder to learn a low-dimensional representation of sentences. A GAN is
then trained to generate its own vectors in this space, which decode to
realistic utterances. We report both random and interpolated samples from the
generator. Visualization of sentence vectors indicate our model correctly
learns the latent space of the autoencoder. Both human ratings and BLEU scores
show that our model generates realistic text against competitive baselines.","['David Donahue', 'Anna Rumshisky']","['cs.CL', 'cs.LG', 'stat.ML']",2018-10-11 22:50:38+00:00
http://arxiv.org/abs/1810.05642v1,The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems,"Scenario-based testing for the safety validation of highly automated vehicles
is a promising approach that is being examined in research and industry. This
approach heavily relies on data from real-world scenarios to derive the
necessary scenario information for testing. Measurement data should be
collected at a reasonable effort, contain naturalistic behavior of road users
and include all data relevant for a description of the identified scenarios in
sufficient quality. However, the current measurement methods fail to meet at
least one of the requirements. Thus, we propose a novel method to measure data
from an aerial perspective for scenario-based validation fulfilling the
mentioned requirements. Furthermore, we provide a large-scale naturalistic
vehicle trajectory dataset from German highways called highD. We evaluate the
data in terms of quantity, variety and contained scenarios. Our dataset
consists of 16.5 hours of measurements from six locations with 110 000
vehicles, a total driven distance of 45 000 km and 5600 recorded complete lane
changes. The highD dataset is available online at: http://www.highD-dataset.com","['Robert Krajewski', 'Julian Bock', 'Laurent Kloeker', 'Lutz Eckstein']","['cs.CV', 'cs.AI', 'cs.IR', 'cs.LG', 'stat.ML']",2018-10-11 22:47:33+00:00
http://arxiv.org/abs/1810.05270v2,Rethinking the Value of Network Pruning,"Network pruning is widely used for reducing the heavy inference cost of deep
models in low-resource settings. A typical pruning algorithm is a three-stage
pipeline, i.e., training (a large model), pruning and fine-tuning. During
pruning, according to a certain criterion, redundant weights are pruned and
important weights are kept to best preserve the accuracy. In this work, we make
several surprising observations which contradict common beliefs. For all
state-of-the-art structured pruning algorithms we examined, fine-tuning a
pruned model only gives comparable or worse performance than training that
model with randomly initialized weights. For pruning algorithms which assume a
predefined target network architecture, one can get rid of the full pipeline
and directly train the target network from scratch. Our observations are
consistent for multiple network architectures, datasets, and tasks, which imply
that: 1) training a large, over-parameterized model is often not necessary to
obtain an efficient final model, 2) learned ""important"" weights of the large
model are typically not useful for the small pruned model, 3) the pruned
architecture itself, rather than a set of inherited ""important"" weights, is
more crucial to the efficiency in the final model, which suggests that in some
cases pruning can be useful as an architecture search paradigm. Our results
suggest the need for more careful baseline evaluations in future research on
structured pruning methods. We also compare with the ""Lottery Ticket
Hypothesis"" (Frankle & Carbin 2019), and find that with optimal learning rate,
the ""winning ticket"" initialization as used in Frankle & Carbin (2019) does not
bring improvement over random initialization.","['Zhuang Liu', 'Mingjie Sun', 'Tinghui Zhou', 'Gao Huang', 'Trevor Darrell']","['cs.LG', 'cs.CV', 'stat.ML']",2018-10-11 22:15:28+00:00
http://arxiv.org/abs/1810.05247v2,Real-time Faulted Line Localization and PMU Placement in Power Systems through Convolutional Neural Networks,"Diverse fault types, fast re-closures, and complicated transient states after
a fault event make real-time fault location in power grids challenging.
Existing localization techniques in this area rely on simplistic assumptions,
such as static loads, or require much higher sampling rates or total
measurement availability. This paper proposes a faulted line localization
method based on a Convolutional Neural Network (CNN) classifier using bus
voltages. Unlike prior data-driven methods, the proposed classifier is based on
features with physical interpretations that improve the robustness of the
location performance. The accuracy of our CNN based localization tool is
demonstrably superior to other machine learning classifiers in the literature.
To further improve the location performance, a joint phasor measurement units
(PMU) placement strategy is proposed and validated against other methods. A
significant aspect of our methodology is that under very low observability (7%
of buses), the algorithm is still able to localize the faulted line to a small
neighborhood with high probability. The performance of our scheme is validated
through simulations of faults of various types in the IEEE 39-bus and 68-bus
power systems under varying uncertain conditions, system observability, and
measurement quality.","['Wenting Li', 'Deepjyoti Deka', 'Michael Chertkov', 'Meng Wang']","['cs.SY', 'cs.LG', 'stat.ML']",2018-10-11 21:06:33+00:00
http://arxiv.org/abs/1810.05246v2,Piano Genie,"We present Piano Genie, an intelligent controller which allows non-musicians
to improvise on the piano. With Piano Genie, a user performs on a simple
interface with eight buttons, and their performance is decoded into the space
of plausible piano music in real time. To learn a suitable mapping procedure
for this problem, we train recurrent neural network autoencoders with discrete
bottlenecks: an encoder learns an appropriate sequence of buttons corresponding
to a piano piece, and a decoder learns to map this sequence back to the
original piece. During performance, we substitute a user's input for the
encoder output, and play the decoder's prediction each time the user presses a
button. To improve the intuitiveness of Piano Genie's performance behavior, we
impose musically meaningful constraints over the encoder's outputs.","['Chris Donahue', 'Ian Simon', 'Sander Dieleman']","['cs.LG', 'cs.HC', 'cs.SD', 'eess.AS', 'stat.ML']",2018-10-11 21:00:44+00:00
http://arxiv.org/abs/1810.05236v3,Practical Design Space Exploration,"Multi-objective optimization is a crucial matter in computer systems design
space exploration because real-world applications often rely on a trade-off
between several objectives. Derivatives are usually not available or
impractical to compute and the feasibility of an experiment can not always be
determined in advance. These problems are particularly difficult when the
feasible region is relatively small, and it may be prohibitive to even find a
feasible experiment, let alone an optimal one.
  We introduce a new methodology and corresponding software framework,
HyperMapper 2.0, which handles multi-objective optimization, unknown
feasibility constraints, and categorical/ordinal variables. This new
methodology also supports injection of the user prior knowledge in the search
when available. All of these features are common requirements in computer
systems but rarely exposed in existing design space exploration systems. The
proposed methodology follows a white-box model which is simple to understand
and interpret (unlike, for example, neural networks) and can be used by the
user to better understand the results of the automatic search.
  We apply and evaluate the new methodology to the automatic static tuning of
hardware accelerators within the recently introduced Spatial programming
language, with minimization of design run-time and compute logic under the
constraint of the design fitting in a target field-programmable gate array
chip. Our results show that HyperMapper 2.0 provides better Pareto fronts
compared to state-of-the-art baselines, with better or competitive hypervolume
indicator and with 8x improvement in sampling budget for most of the benchmarks
explored.","['Luigi Nardi', 'David Koeplinger', 'Kunle Olukotun']","['cs.LG', 'math.OC', 'stat.ML']",2018-10-11 20:23:57+00:00
http://arxiv.org/abs/1810.05222v2,Efficient Augmentation via Data Subsampling,"Data augmentation is commonly used to encode invariances in learning methods.
However, this process is often performed in an inefficient manner, as
artificial examples are created by applying a number of transformations to all
points in the training set. The resulting explosion of the dataset size can be
an issue in terms of storage and training costs, as well as in selecting and
tuning the optimal set of transformations to apply. In this work, we
demonstrate that it is possible to significantly reduce the number of data
points included in data augmentation while realizing the same accuracy and
invariance benefits of augmenting the entire dataset. We propose a novel set of
subsampling policies, based on model influence and loss, that can achieve a 90%
reduction in augmentation set size while maintaining the accuracy gains of
standard data augmentation.","['Michael Kuchnik', 'Virginia Smith']","['cs.LG', 'stat.ML']",2018-10-11 19:50:08+00:00
http://arxiv.org/abs/1810.05221v1,MDGAN: Boosting Anomaly Detection Using \\Multi-Discriminator Generative Adversarial Networks,"Anomaly detection is often considered a challenging field of machine learning
due to the difficulty of obtaining anomalous samples for training and the need
to obtain a sufficient amount of training data. In recent years, autoencoders
have been shown to be effective anomaly detectors that train only on ""normal""
data. Generative adversarial networks (GANs) have been used to generate
additional training samples for classifiers, thus making them more accurate and
robust. However, in anomaly detection GANs are only used to reconstruct
existing samples rather than to generate additional ones. This stems both from
the small amount and lack of diversity of anomalous data in most domains. In
this study we propose MDGAN, a novel GAN architecture for improving anomaly
detection through the generation of additional samples. Our approach uses two
discriminators: a dense network for determining whether the generated samples
are of sufficient quality (i.e., valid) and an autoencoder that serves as an
anomaly detector. MDGAN enables us to reconcile two conflicting goals: 1)
generate high-quality samples that can fool the first discriminator, and 2)
generate samples that can eventually be effectively reconstructed by the second
discriminator, thus improving its performance. Empirical evaluation on a
diverse set of datasets demonstrates the merits of our approach.","['Yotam Intrator', 'Gilad Katz', 'Asaf Shabtai']","['cs.LG', 'stat.ML']",2018-10-11 19:45:30+00:00
http://arxiv.org/abs/1810.05640v2,Inventory Balancing with Online Learning,"We study a general problem of allocating limited resources to heterogeneous
customers over time under model uncertainty. Each type of customer can be
serviced using different actions, each of which stochastically consumes some
combination of resources, and returns different rewards for the resources
consumed. We consider a general model where the resource consumption
distribution associated with each (customer type, action)-combination is not
known, but is consistent and can be learned over time. In addition, the
sequence of customer types to arrive over time is arbitrary and completely
unknown.
  We overcome both the challenges of model uncertainty and customer
heterogeneity by judiciously synthesizing two algorithmic frameworks from the
literature: inventory balancing, which ""reserves"" a portion of each resource
for high-reward customer types which could later arrive, and online learning,
which shows how to ""explore"" the resource consumption distributions of each
customer type under different actions. We define an auxiliary problem, which
allows for existing competitive ratio and regret bounds to be seamlessly
integrated. Furthermore, we show that the performance guarantee generated by
our framework is tight, that is, we provide an information-theoretic lower
bound which shows that both the loss from competitive ratio and the loss for
regret are relevant in the combined problem.
  Finally, we demonstrate the efficacy of our algorithms on a publicly
available hotel data set. Our framework is highly practical in that it requires
no historical data (no fitted customer choice models, nor forecasting of
customer arrival patterns) and can be used to initialize allocation strategies
in fast-changing environments.","['Wang Chi Cheung', 'Will Ma', 'David Simchi-Levi', 'Xinshang Wang']","['cs.AI', 'cs.LG', 'stat.ML']",2018-10-11 19:34:13+00:00
http://arxiv.org/abs/1810.05207v3,On Kernel Derivative Approximation with Random Fourier Features,"Random Fourier features (RFF) represent one of the most popular and
wide-spread techniques in machine learning to scale up kernel algorithms.
Despite the numerous successful applications of RFFs, unfortunately, quite
little is understood theoretically on their optimality and limitations of their
performance. Only recently, precise statistical-computational trade-offs have
been established for RFFs in the approximation of kernel values, kernel ridge
regression, kernel PCA and SVM classification. Our goal is to spark the
investigation of optimality of RFF-based approximations in tasks involving not
only function values but derivatives, which naturally lead to optimization
problems with kernel derivatives. Particularly, in this paper, we focus on the
approximation quality of RFFs for kernel derivatives and prove that the
existing finite-sample guarantees can be improved exponentially in terms of the
domain where they hold, using recent tools from unbounded empirical process
theory. Our result implies that the same approximation guarantee is attainable
for kernel derivatives using RFF as achieved for kernel values.","['Zoltan Szabo', 'Bharath K. Sriperumbudur']","['stat.ML', 'cs.LG', 'math.PR', '60E10, 42Bxx, 46E22', 'G.3; I.2.6']",2018-10-11 19:03:11+00:00
http://arxiv.org/abs/1810.05206v2,MeshAdv: Adversarial Meshes for Visual Recognition,"Highly expressive models such as deep neural networks (DNNs) have been widely
applied to various applications. However, recent studies show that DNNs are
vulnerable to adversarial examples, which are carefully crafted inputs aiming
to mislead the predictions. Currently, the majority of these studies have
focused on perturbation added to image pixels, while such manipulation is not
physically realistic. Some works have tried to overcome this limitation by
attaching printable 2D patches or painting patterns onto surfaces, but can be
potentially defended because 3D shape features are intact. In this paper, we
propose meshAdv to generate ""adversarial 3D meshes"" from objects that have rich
shape features but minimal textural variation. To manipulate the shape or
texture of the objects, we make use of a differentiable renderer to compute
accurate shading on the shape and propagate the gradient. Extensive experiments
show that the generated 3D meshes are effective in attacking both classifiers
and object detectors. We evaluate the attack under different viewpoints. In
addition, we design a pipeline to perform black-box attack on a photorealistic
renderer with unknown rendering parameters.","['Chaowei Xiao', 'Dawei Yang', 'Bo Li', 'Jia Deng', 'Mingyan Liu']","['cs.CR', 'cs.CV', 'cs.LG', 'stat.ML']",2018-10-11 19:01:10+00:00
http://arxiv.org/abs/1810.05193v2,Understanding Priors in Bayesian Neural Networks at the Unit Level,"We investigate deep Bayesian neural networks with Gaussian weight priors and
a class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian
priors are well known to induce an L2, ""weight decay"", regularization. Our
results characterize a more intricate regularization effect at the level of the
unit activations. Our main result establishes that the induced prior
distribution on the units before and after activation becomes increasingly
heavy-tailed with the depth of the layer. We show that first layer units are
Gaussian, second layer units are sub-exponential, and units in deeper layers
are characterized by sub-Weibull distributions. Our results provide new
theoretical insight on deep Bayesian neural networks, which we corroborate with
simulation experiments.","['Mariia Vladimirova', 'Jakob Verbeek', 'Pablo Mesejo', 'Julyan Arbel']","['stat.ML', 'cs.LG']",2018-10-11 18:26:50+00:00
http://arxiv.org/abs/1810.05188v2,Fighting Contextual Bandits with Stochastic Smoothing,"We introduce a new stochastic smoothing perspective to study adversarial
contextual bandit problems. We propose a general algorithm template that
represents random perturbation based algorithms and identify several
perturbation distributions that lead to strong regret bounds. Using the idea of
smoothness, we provide an $O(\sqrt{T})$ zero-order bound for the vanilla
algorithm and an $O(L^{*2/3}_{T})$ first-order bound for the clipped version.
These bounds hold when the algorithms use with a variety of distributions that
have a bounded hazard rate. Our algorithm template includes EXP4 as a special
case corresponding to the Gumbel perturbation. Our regret bounds match existing
results for EXP4 without relying on the specific properties of the algorithm.","['Young Hun Jung', 'Ambuj Tewari']","['stat.ML', 'cs.LG']",2018-10-11 18:07:43+00:00
http://arxiv.org/abs/1810.05187v1,The Impact of Annotation Guidelines and Annotated Data on Extracting App Features from App Reviews,"Annotation guidelines used to guide the annotation of training and evaluation
datasets can have a considerable impact on the quality of machine learning
models. In this study, we explore the effects of annotation guidelines on the
quality of app feature extraction models. As a main result, we propose several
changes to the existing annotation guidelines with a goal of making the
extracted app features more useful and informative to the app developers. We
test the proposed changes via simulating the application of the new annotation
guidelines and then evaluating the performance of the supervised machine
learning models trained on datasets annotated with initial and simulated
guidelines. While the overall performance of automatic app feature extraction
remains the same as compared to the model trained on the dataset with initial
annotations, the features extracted by the model trained on the dataset with
simulated new annotations are less noisy and more informative to the app
developers. Secondly, we are interested in what kind of annotated training data
is necessary for training an automatic app feature extraction model. In
particular, we explore whether the training set should contain annotated app
reviews from those apps/app categories on which the model is subsequently
planned to be applied, or is it sufficient to have annotated app reviews from
any app available for training, even when these apps are from very different
categories compared to the test app. Our experiments show that having annotated
training reviews from the test app is not necessary although including them
into training set helps to improve recall. Furthermore, we test whether
augmenting the training set with annotated product reviews helps to improve the
performance of app feature extraction. We find that the models trained on
augmented training set lead to improved recall but at the cost of the drop in
precision.","['Faiz Ali Shah', 'Kairit Sirts', 'Dietmar Pfahl']","['cs.IR', 'cs.LG', 'stat.ML']",2018-10-11 18:07:14+00:00
http://arxiv.org/abs/1810.05186v1,Bilinear Factor Matrix Norm Minimization for Robust PCA: Algorithms and Applications,"The heavy-tailed distributions of corrupted outliers and singular values of
all channels in low-level vision have proven effective priors for many
applications such as background modeling, photometric stereo and image
alignment. And they can be well modeled by a hyper-Laplacian. However, the use
of such distributions generally leads to challenging non-convex, non-smooth and
non-Lipschitz problems, and makes existing algorithms very slow for large-scale
applications. Together with the analytic solutions to lp-norm minimization with
two specific values of p, i.e., p=1/2 and p=2/3, we propose two novel bilinear
factor matrix norm minimization models for robust principal component analysis.
We first define the double nuclear norm and Frobenius/nuclear hybrid norm
penalties, and then prove that they are in essence the Schatten-1/2 and 2/3
quasi-norms, respectively, which lead to much more tractable and scalable
Lipschitz optimization problems. Our experimental analysis shows that both our
methods yield more accurate solutions than original Schatten quasi-norm
minimization, even when the number of observations is very limited. Finally, we
apply our penalties to various low-level vision problems, e.g., text removal,
moving object detection, image alignment and inpainting, and show that our
methods usually outperform the state-of-the-art methods.","['Fanhua Shang', 'James Cheng', 'Yuanyuan Liu', 'Zhi-Quan Luo', 'Zhouchen Lin']","['cs.LG', 'cs.CV', 'math.OC', 'stat.ML']",2018-10-11 18:06:27+00:00
http://arxiv.org/abs/1810.05165v2,Energy Flow Networks: Deep Sets for Particle Jets,"A key question for machine learning approaches in particle physics is how to
best represent and learn from collider events. As an event is intrinsically a
variable-length unordered set of particles, we build upon recent machine
learning efforts to learn directly from sets of features or ""point clouds"".
Adapting and specializing the ""Deep Sets"" framework to particle physics, we
introduce Energy Flow Networks, which respect infrared and collinear safety by
construction. We also develop Particle Flow Networks, which allow for general
energy dependence and the inclusion of additional particle-level information
such as charge and flavor. These networks feature a per-particle internal
(latent) representation, and summing over all particles yields an overall
event-level latent representation. We show how this latent space decomposition
unifies existing event representations based on detector images and radiation
moments. To demonstrate the power and simplicity of this set-based approach, we
apply these networks to the collider task of discriminating quark jets from
gluon jets, finding similar or improved performance compared to existing
methods. We also show how the learned event representation can be directly
visualized, providing insight into the inner workings of the model. These
architectures lend themselves to efficiently processing and analyzing events
for a wide variety of tasks at the Large Hadron Collider. Implementations and
examples of our architectures are available online in our EnergyFlow package.","['Patrick T. Komiske', 'Eric M. Metodiev', 'Jesse Thaler']","['hep-ph', 'hep-ex', 'stat.ML']",2018-10-11 18:00:00+00:00
http://arxiv.org/abs/1810.05157v4,Learning under Misspecified Objective Spaces,"Learning robot objective functions from human input has become increasingly
important, but state-of-the-art techniques assume that the human's desired
objective lies within the robot's hypothesis space. When this is not true, even
methods that keep track of uncertainty over the objective fail because they
reason about which hypothesis might be correct, and not whether any of the
hypotheses are correct. We focus specifically on learning from physical human
corrections during the robot's task execution, where not having a rich enough
hypothesis space leads to the robot updating its objective in ways that the
person did not actually intend. We observe that such corrections appear
irrelevant to the robot, because they are not the best way of achieving any of
the candidate objectives. Instead of naively trusting and learning from every
human interaction, we propose robots learn conservatively by reasoning in real
time about how relevant the human's correction is for the robot's hypothesis
space. We test our inference method in an experiment with human interaction
data, and demonstrate that this alleviates unintended learning in an in-person
user study with a 7DoF robot manipulator.","['Andreea Bobu', 'Andrea Bajcsy', 'Jaime F. Fisac', 'Anca D. Dragan']","['cs.LG', 'cs.AI', 'cs.HC', 'cs.RO', 'stat.ML']",2018-10-11 17:58:27+00:00
http://arxiv.org/abs/1810.05148v4,Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes,"There is a previously identified equivalence between wide fully connected
neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables,
for instance, test set predictions that would have resulted from a fully
Bayesian, infinitely wide trained FCN to be computed without ever instantiating
the FCN, but by instead evaluating the corresponding GP. In this work, we
derive an analogous equivalence for multi-layer convolutional neural networks
(CNNs) both with and without pooling layers, and achieve state of the art
results on CIFAR10 for GPs without trainable kernels. We also introduce a Monte
Carlo method to estimate the GP corresponding to a given neural network
architecture, even in cases where the analytic form has too many terms to be
computationally feasible.
  Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs
with and without weight sharing are identical. As a consequence, translation
equivariance, beneficial in finite channel CNNs trained with stochastic
gradient descent (SGD), is guaranteed to play no role in the Bayesian treatment
of the infinite channel limit - a qualitative difference between the two
regimes that is not present in the FCN case. We confirm experimentally, that
while in some scenarios the performance of SGD-trained finite CNNs approaches
that of the corresponding GPs as the channel count increases, with careful
tuning SGD-trained CNNs can significantly outperform their corresponding GPs,
suggesting advantages from SGD training compared to fully Bayesian parameter
estimation.","['Roman Novak', 'Lechao Xiao', 'Jaehoon Lee', 'Yasaman Bahri', 'Greg Yang', 'Jiri Hron', 'Daniel A. Abolafia', 'Jeffrey Pennington', 'Jascha Sohl-Dickstein']","['stat.ML', 'cs.AI', 'cs.LG', 'cs.NE']",2018-10-11 17:49:41+00:00
http://arxiv.org/abs/1810.05547v2,Physics-Driven Regularization of Deep Neural Networks for Enhanced Engineering Design and Analysis,"In this paper, we introduce a physics-driven regularization method for
training of deep neural networks (DNNs) for use in engineering design and
analysis problems. In particular, we focus on prediction of a physical system,
for which in addition to training data, partial or complete information on a
set of governing laws is also available. These laws often appear in the form of
differential equations, derived from first principles, empirically-validated
laws, or domain expertise, and are usually neglected in data-driven prediction
of engineering systems. We propose a training approach that utilizes the known
governing laws and regularizes data-driven DNN models by penalizing divergence
from those laws. The first two numerical examples are synthetic examples, where
we show that in constructing a DNN model that best fits the measurements from a
physical system, the use of our proposed regularization results in DNNs that
are more interpretable with smaller generalization errors, compared to other
common regularization methods. The last two examples concern metamodeling for a
random Burgers' system and for aerodynamic analysis of passenger vehicles,
where we demonstrate that the proposed regularization provides superior
generalization accuracy compared to other common alternatives.","['Mohammad Amin Nabian', 'Hadi Meidani']","['cs.LG', 'cs.CE', 'cs.NA', 'math.AP', 'math.NA', 'stat.ML']",2018-10-11 17:12:34+00:00
http://arxiv.org/abs/1810.05075v1,Taming the Cross Entropy Loss,"We present the Tamed Cross Entropy (TCE) loss function, a robust derivative
of the standard Cross Entropy (CE) loss used in deep learning for
classification tasks. However, unlike other robust losses, the TCE loss is
designed to exhibit the same training properties than the CE loss in noiseless
scenarios. Therefore, the TCE loss requires no modification on the training
regime compared to the CE loss and, in consequence, can be applied in all
applications where the CE loss is currently used. We evaluate the TCE loss
using the ResNet architecture on four image datasets that we artificially
contaminated with various levels of label noise. The TCE loss outperforms the
CE loss in every tested scenario.","['Manuel Martinez', 'Rainer Stiefelhagen']","['cs.LG', 'stat.ML']",2018-10-11 15:18:19+00:00
http://arxiv.org/abs/1810.05065v2,Regularized Contextual Bandits,"We consider the stochastic contextual bandit problem with additional
regularization. The motivation comes from problems where the policy of the
agent must be close to some baseline policy which is known to perform well on
the task. To tackle this problem we use a nonparametric model and propose an
algorithm splitting the context space into bins, and solving simultaneously -
and independently - regularized multi-armed bandit instances on each bin. We
derive slow and fast rates of convergence, depending on the unknown complexity
of the problem. We also consider a new relevant margin condition to get
problem-independent convergence rates, ending up in intermediate convergence
rates interpolating between the aforementioned slow and fast rates.","['Xavier Fontaine', 'Quentin Berthet', 'Vianney Perchet']","['stat.ML', 'cs.LG', 'math.OC']",2018-10-11 15:00:15+00:00
http://arxiv.org/abs/1810.05064v3,A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice,"In the $k$-nearest neighborhood model ($k$-NN), we are given a set of points
$P$, and we shall answer queries $q$ by returning the $k$ nearest neighbors of
$q$ in $P$ according to some metric. This concept is crucial in many areas of
data analysis and data processing, e.g., computer vision, document retrieval
and machine learning. Many $k$-NN algorithms have been published and
implemented, but often the relation between parameters and accuracy of the
computed $k$-NN is not explicit. We study property testing of $k$-NN graphs in
theory and evaluate it empirically: given a point set $P \subset
\mathbb{R}^\delta$ and a directed graph $G=(P,E)$, is $G$ a $k$-NN graph, i.e.,
every point $p \in P$ has outgoing edges to its $k$ nearest neighbors, or is it
$\epsilon$-far from being a $k$-NN graph? Here, $\epsilon$-far means that one
has to change more than an $\epsilon$-fraction of the edges in order to make
$G$ a $k$-NN graph. We develop a randomized algorithm with one-sided error that
decides this question, i.e., a property tester for the $k$-NN property, with
complexity $O(\sqrt{n} k^2 / \epsilon^2)$ measured in terms of the number of
vertices and edges it inspects, and we prove a lower bound of $\Omega(\sqrt{n /
\epsilon k})$. We evaluate our tester empirically on the $k$-NN models computed
by various algorithms and show that it can be used to detect $k$-NN models with
bad accuracy in significantly less time than the building time of the $k$-NN
model.","['Hendrik Fichtenberger', 'Dennis Rohde']","['cs.LG', 'stat.ML']",2018-10-11 14:56:03+00:00
http://arxiv.org/abs/1810.05057v1,Identification of Invariant Sensorimotor Structures as a Prerequisite for the Discovery of Objects,"Perceiving the surrounding environment in terms of objects is useful for any
general purpose intelligent agent. In this paper, we investigate a fundamental
mechanism making object perception possible, namely the identification of
spatio-temporally invariant structures in the sensorimotor experience of an
agent. We take inspiration from the Sensorimotor Contingencies Theory to define
a computational model of this mechanism through a sensorimotor, unsupervised
and predictive approach. Our model is based on processing the unsupervised
interaction of an artificial agent with its environment. We show how
spatio-temporally invariant structures in the environment induce regularities
in the sensorimotor experience of an agent, and how this agent, while building
a predictive model of its sensorimotor experience, can capture them as densely
connected subgraphs in a graph of sensory states connected by motor commands.
Our approach is focused on elementary mechanisms, and is illustrated with a set
of simple experiments in which an agent interacts with an environment. We show
how the agent can build an internal model of moving but spatio-temporally
invariant structures by performing a Spectral Clustering of the graph modeling
its overall sensorimotor experiences. We systematically examine properties of
the model, shedding light more globally on the specificities of the paradigm
with respect to methods based on the supervised processing of collections of
static images.","['Nicolas Le Hir', 'Olivier Sigaud', 'Alban Laflaqui√®re']","['cs.AI', 'cs.LG', 'cs.RO', 'stat.ML']",2018-10-11 14:47:38+00:00
http://arxiv.org/abs/1810.04996v1,A Simple Way to Deal with Cherry-picking,"Statistical hypothesis testing serves as statistical evidence for scientific
innovation. However, if the reported results are intentionally biased,
hypothesis testing no longer controls the rate of false discovery. In
particular, we study such selection bias in machine learning models where the
reporter is motivated to promote an algorithmic innovation. When the number of
possible configurations (e.g., datasets) is large, we show that the reporter
can falsely report an innovation even if there is no improvement at all. We
propose a `post-reporting' solution to this issue where the bias of the
reported results is verified by another set of results. The theoretical
findings are supported by experimental results with synthetic and real-world
datasets.","['Junpei Komiyama', 'Takanori Maehara']","['stat.ME', 'cs.LG', 'stat.ML']",2018-10-11 13:06:48+00:00
http://arxiv.org/abs/1810.04963v2,The persistence landscape and some of its properties,"Persistence landscapes map persistence diagrams into a function space, which
may often be taken to be a Banach space or even a Hilbert space. In the latter
case, it is a feature map and there is an associated kernel. The main advantage
of this summary is that it allows one to apply tools from statistics and
machine learning. Furthermore, the mapping from persistence diagrams to
persistence landscapes is stable and invertible. We introduce a weighted
version of the persistence landscape and define a one-parameter family of
Poisson-weighted persistence landscape kernels that may be useful for learning.
We also demonstrate some additional properties of the persistence landscape.
First, the persistence landscape may be viewed as a tropical rational function.
Second, in many cases it is possible to exactly reconstruct all of the
component persistence diagrams from an average persistence landscape. It
follows that the persistence landscape kernel is characteristic for certain
generic empirical measures. Finally, the persistence landscape distance may be
arbitrarily small compared to the interleaving distance.",['Peter Bubenik'],"['math.AT', 'stat.ML']",2018-10-11 11:47:48+00:00
http://arxiv.org/abs/1811.12159v1,Systematic Biases in Link Prediction: comparing heuristic and graph embedding based methods,"Link prediction is a popular research topic in network analysis. In the last
few years, new techniques based on graph embedding have emerged as a powerful
alternative to heuristics. In this article, we study the problem of systematic
biases in the prediction, and show that some methods based on graph embedding
offer less biased results than those based on heuristics, despite reaching
lower scores according to usual quality scores. We discuss the relevance of
this finding in the context of the filter bubble problem and the algorithmic
fairness of recommender systems.","['Aakash Sinha', 'R√©my Cazabet', 'R√©mi Vaudaine']","['cs.SI', 'stat.ML']",2018-10-11 09:32:39+00:00
http://arxiv.org/abs/1810.04920v1,Pairwise Augmented GANs with Adversarial Reconstruction Loss,"We propose a novel autoencoding model called Pairwise Augmented GANs. We
train a generator and an encoder jointly and in an adversarial manner. The
generator network learns to sample realistic objects. In turn, the encoder
network at the same time is trained to map the true data distribution to the
prior in latent space. To ensure good reconstructions, we introduce an
augmented adversarial reconstruction loss. Here we train a discriminator to
distinguish two types of pairs: an object with its augmentation and the one
with its reconstruction. We show that such adversarial loss compares objects
based on the content rather than on the exact match. We experimentally
demonstrate that our model generates samples and reconstructions of quality
competitive with state-of-the-art on datasets MNIST, CIFAR10, CelebA and
achieves good quantitative results on CIFAR10.","['Aibek Alanov', 'Max Kochurov', 'Daniil Yashkov', 'Dmitry Vetrov']","['stat.ML', 'cs.LG']",2018-10-11 09:22:36+00:00
http://arxiv.org/abs/1810.04903v2,MOANOFS: Multi-Objective Automated Negotiation based Online Feature Selection System for Big Data Classification,"Feature Selection (FS) plays an important role in learning and classification
tasks. The object of FS is to select the relevant and non-redundant features.
Considering the huge amount number of features in real-world applications, FS
methods using batch learning technique can't resolve big data problem
especially when data arrive sequentially. In this paper, we propose an online
feature selection system which resolves this problem. More specifically, we
treat the problem of online supervised feature selection for binary
classification as a decision-making problem. A philosophical vision to this
problem leads to a hybridization between two important domains: feature
selection using online learning technique (OFS) and automated negotiation (AN).
The proposed OFS system called MOANOFS (Multi-Objective Automated Negotiation
based Online Feature Selection) uses two levels of decision. In the first
level, from n learners (or OFS methods), we decide which are the k trustful
ones (with high confidence or trust value). These elected k learners will
participate in the second level. In this level, we integrate our proposed
Multilateral Automated Negotiation based OFS (MANOFS) method to decide finally
which is the best solution or which are relevant features. We show that MOANOFS
system is applicable to different domains successfully and achieves high
accuracy with several real-world applications.
  Index Terms: Feature selection, online learning, multi-objective automated
negotiation, trust, classification, big data.","['Fatma BenSaid', 'Adel M. Alimi']","['cs.LG', 'cs.AI', 'cs.CV', 'cs.MA', 'stat.ML']",2018-10-11 08:41:30+00:00
http://arxiv.org/abs/1810.04863v1,Classification using margin pursuit,"In this work, we study a new approach to optimizing the margin distribution
realized by binary classifiers. The classical approach to this problem is
simply maximization of the expected margin, while more recent proposals
consider simultaneous variance control and proxy objectives based on robust
location estimates, in the vein of keeping the margin distribution sharply
concentrated in a desirable region. While conceptually appealing, these new
approaches are often computationally unwieldy, and theoretical guarantees are
limited. Given this context, we propose an algorithm which searches the
hypothesis space in such a way that a pre-set ""margin level"" ends up being a
distribution-robust estimator of the margin location. This procedure is easily
implemented using gradient descent, and admits finite-sample bounds on the
excess risk under unbounded inputs. Empirical tests on real-world benchmark
data reinforce the basic principles highlighted by the theory, and are
suggestive of a promising new technique for classification.",['Matthew J. Holland'],"['stat.ML', 'cs.LG']",2018-10-11 06:35:48+00:00
http://arxiv.org/abs/1810.05533v1,Empowerment-driven Exploration using Mutual Information Estimation,"Exploration is a difficult challenge in reinforcement learning and is of
prime importance in sparse reward environments. However, many of the state of
the art deep reinforcement learning algorithms, that rely on epsilon-greedy,
fail on these environments. In such cases, empowerment can serve as an
intrinsic reward signal to enable the agent to maximize the influence it has
over the near future. We formulate empowerment as the channel capacity between
states and actions and is calculated by estimating the mutual information
between the actions and the following states. The mutual information is
estimated using Mutual Information Neural Estimator and a forward dynamics
model. We demonstrate that an empowerment driven agent is able to improve
significantly the score of a baseline DQN agent on the game of Montezuma's
Revenge.",['Navneet Madhu Kumar'],"['cs.LG', 'cs.AI', 'stat.ML']",2018-10-11 06:34:18+00:00
http://arxiv.org/abs/1810.04851v2,PANDA: AdaPtive Noisy Data Augmentation for Regularization of Undirected Graphical Models,"We propose an AdaPtive Noise Augmentation (PANDA) technique to regularize the
estimation and construction of undirected graphical models. PANDA iteratively
optimizes the objective function given the noise augmented data until
convergence to achieve regularization on model parameters. The augmented noises
can be designed to achieve various regularization effects on graph estimation,
such as the bridge (including lasso and ridge), elastic net, adaptive lasso,
and SCAD penalization; it also realizes the group lasso and fused ridge. We
examine the tail bound of the noise-augmented loss function and establish that
the noise-augmented loss function and its minimizer converge almost surely to
the expected penalized loss function and its minimizer, respectively. We derive
the asymptotic distributions for the regularized parameters through PANDA in
generalized linear models, based on which, inferences for the parameters can be
obtained simultaneously with variable selection. We show the non-inferior
performance of PANDA in constructing graphs of different types in simulation
studies and apply PANDA to an autism spectrum disorder data to construct a
mixed-node graph. We also show that the inferences based on the asymptotic
distribution of regularized parameter estimates via PANDA achieve nominal or
near-nominal coverage and are far more efficient, compared to some existing
post-selection procedures. Computationally, PANDA can be easily programmed in
software that implements (GLMs) without resorting to complicated optimization
techniques.","['Yinan Li', 'Xiao Liu', 'Fang Liu']","['stat.ML', 'cs.LG', '62']",2018-10-11 05:54:44+00:00
http://arxiv.org/abs/1810.05732v1,A Novel Domain Adaptation Framework for Medical Image Segmentation,"We propose a segmentation framework that uses deep neural networks and
introduce two innovations. First, we describe a biophysics-based domain
adaptation method. Second, we propose an automatic method to segment white and
gray matter, and cerebrospinal fluid, in addition to tumorous tissue. Regarding
our first innovation, we use a domain adaptation framework that combines a
novel multispecies biophysical tumor growth model with a generative adversarial
model to create realistic looking synthetic multimodal MR images with known
segmentation. Regarding our second innovation, we propose an automatic approach
to enrich available segmentation data by computing the segmentation for healthy
tissues. This segmentation, which is done using diffeomorphic image
registration between the BraTS training data and a set of prelabeled atlases,
provides more information for training and reduces the class imbalance problem.
Our overall approach is not specific to any particular neural network and can
be used in conjunction with existing solutions. We demonstrate the performance
improvement using a 2D U-Net for the BraTS'18 segmentation challenge. Our
biophysics based domain adaptation achieves better results, as compared to the
existing state-of-the-art GAN model used to create synthetic data for training.","['Amir Gholami', 'Shashank Subramanian', 'Varun Shenoy', 'Naveen Himthani', 'Xiangyu Yue', 'Sicheng Zhao', 'Peter Jin', 'George Biros', 'Kurt Keutzer']","['cs.CV', 'cs.LG', 'stat.ML']",2018-10-11 04:03:30+00:00
http://arxiv.org/abs/1810.04826v6,VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking,"In this paper, we present a novel system that separates the voice of a target
speaker from multi-speaker signals, by making use of a reference signal from
the target speaker. We achieve this by training two separate neural networks:
(1) A speaker recognition network that produces speaker-discriminative
embeddings; (2) A spectrogram masking network that takes both noisy spectrogram
and speaker embedding as input, and produces a mask. Our system significantly
reduces the speech recognition WER on multi-speaker signals, with minimal WER
degradation on single-speaker signals.","['Quan Wang', 'Hannah Muckenhirn', 'Kevin Wilson', 'Prashant Sridhar', 'Zelin Wu', 'John Hershey', 'Rif A. Saurous', 'Ron J. Weiss', 'Ye Jia', 'Ignacio Lopez Moreno']","['eess.AS', 'cs.LG', 'eess.SP', 'stat.ML']",2018-10-11 02:57:14+00:00
http://arxiv.org/abs/1810.04824v1,A Blended Deep Learning Approach for Predicting User Intended Actions,"User intended actions are widely seen in many areas. Forecasting these
actions and taking proactive measures to optimize business outcome is a crucial
step towards sustaining the steady business growth. In this work, we focus on
pre- dicting attrition, which is one of typical user intended actions.
Conventional attrition predictive modeling strategies suffer a few inherent
drawbacks. To overcome these limitations, we propose a novel end-to-end
learning scheme to keep track of the evolution of attrition patterns for the
predictive modeling. It integrates user activity logs, dynamic and static user
profiles based on multi-path learning. It exploits historical user records by
establishing a decaying multi-snapshot technique. And finally it employs the
precedent user intentions via guiding them to the subsequent learning
procedure. As a result, it addresses all disadvantages of conventional methods.
We evaluate our methodology on two public data repositories and one private
user usage dataset provided by Adobe Creative Cloud. The extensive experiments
demonstrate that it can offer the appealing performance in comparison with
several existing approaches as rated by different popular metrics. Furthermore,
we introduce an advanced interpretation and visualization strategy to
effectively characterize the periodicity of user activity logs. It can help to
pinpoint important factors that are critical to user attrition and retention
and thus suggests actionable improvement targets for business practice. Our
work will provide useful insights into the prediction and elucidation of other
user intended actions as well.","['Fei Tan', 'Zhi Wei', 'Jun He', 'Xiang Wu', 'Bo Peng', 'Haoran Liu', 'Zhenyu Yan']","['cs.LG', 'cs.HC', 'stat.ML']",2018-10-11 02:48:20+00:00
http://arxiv.org/abs/1810.05497v1,Probabilistic Blocking with An Application to the Syrian Conflict,"Entity resolution seeks to merge databases as to remove duplicate entries
where unique identifiers are typically unknown. We review modern blocking
approaches for entity resolution, focusing on those based upon locality
sensitive hashing (LSH). First, we introduce $k$-means locality sensitive
hashing (KLSH), which is based upon the information retrieval literature and
clusters similar records into blocks using a vector-space representation and
projections. Second, we introduce a subquadratic variant of LSH to the
literature, known as Densified One Permutation Hashing (DOPH). Third, we
propose a weighted variant of DOPH. We illustrate each method on an application
to a subset of the ongoing Syrian conflict, giving a discussion of each method.","['Rebecca C. Steorts', 'Anshumali Shrivastava']","['cs.DB', 'cs.LG', 'stat.AP', 'stat.ML']",2018-10-11 01:16:31+00:00
http://arxiv.org/abs/1810.04778v2,Offline Multi-Action Policy Learning: Generalization and Optimization,"In many settings, a decision-maker wishes to learn a rule, or policy, that
maps from observable characteristics of an individual to an action. Examples
include selecting offers, prices, advertisements, or emails to send to
consumers, as well as the problem of determining which medication to prescribe
to a patient. While there is a growing body of literature devoted to this
problem, most existing results are focused on the case where data comes from a
randomized experiment, and further, there are only two possible actions, such
as giving a drug to a patient or not. In this paper, we study the offline
multi-action policy learning problem with observational data and where the
policy may need to respect budget constraints or belong to a restricted policy
class such as decision trees. We build on the theory of efficient
semi-parametric inference in order to propose and implement a policy learning
algorithm that achieves asymptotically minimax-optimal regret. To the best of
our knowledge, this is the first result of this type in the multi-action setup,
and it provides a substantial performance improvement over the existing
learning algorithms. We then consider additional computational challenges that
arise in implementing our method for the case where the policy is restricted to
take the form of a decision tree. We propose two different approaches, one
using a mixed integer program formulation and the other using a tree-search
based algorithm.","['Zhengyuan Zhou', 'Susan Athey', 'Stefan Wager']","['stat.ML', 'cs.LG', 'econ.EM']",2018-10-10 23:34:37+00:00
http://arxiv.org/abs/1810.04777v3,Rao-Blackwellized Stochastic Gradients for Discrete Distributions,"We wish to compute the gradient of an expectation over a finite or countably
infinite sample space having $K \leq \infty$ categories. When $K$ is indeed
infinite, or finite but very large, the relevant summation is intractable.
Accordingly, various stochastic gradient estimators have been proposed. In this
paper, we describe a technique that can be applied to reduce the variance of
any such estimator, without changing its bias---in particular, unbiasedness is
retained. We show that our technique is an instance of Rao-Blackwellization,
and we demonstrate the improvement it yields on a semi-supervised
classification problem and a pixel attention task.","['Runjing Liu', 'Jeffrey Regier', 'Nilesh Tripuraneni', 'Michael I. Jordan', 'Jon McAuliffe']","['stat.ML', 'cs.LG']",2018-10-10 23:17:11+00:00
http://arxiv.org/abs/1810.04754v2,Efficient Tensor Decomposition with Boolean Factors,"Tensor decomposition has been extensively used as a tool for exploratory
analysis. Motivated by neuroscience applications, we study tensor decomposition
with Boolean factors. The resulting optimization problem is challenging due to
the non-convex objective and the combinatorial constraints. We propose Binary
Matching Pursuit (BMP), a novel generalization of the matching pursuit strategy
to decompose the tensor efficiently. BMP iteratively searches for atoms in a
greedy fashion. The greedy atom search step is solved efficiently via a
MAXCUT-like boolean quadratic program. We prove that BMP is guaranteed to
converge sublinearly to the optimal solution and recover the factors under mild
identifiability conditions. Experiments demonstrate the superior performance of
our method over baselines on synthetic and real datasets. We also showcase the
application of BMP in quantifying neural interactions underlying
high-resolution spatiotemporal ECoG recordings.","['Sung-En Chang', 'Xun Zheng', 'Ian E. H. Yen', 'Pradeep Ravikumar', 'Rose Yu']","['cs.LG', 'stat.ML']",2018-10-10 21:41:52+00:00
http://arxiv.org/abs/1810.04738v1,Probabilistic Clustering Using Maximal Matrix Norm Couplings,"In this paper, we present a local information theoretic approach to
explicitly learn probabilistic clustering of a discrete random variable. Our
formulation yields a convex maximization problem for which it is NP-hard to
find the global optimum. In order to algorithmically solve this optimization
problem, we propose two relaxations that are solved via gradient ascent and
alternating maximization. Experiments on the MSR Sentence Completion Challenge,
MovieLens 100K, and Reuters21578 datasets demonstrate that our approach is
competitive with existing techniques and worthy of further investigation.","['David Qiu', 'Anuran Makur', 'Lizhong Zheng']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2018-10-10 20:26:44+00:00
http://arxiv.org/abs/1810.04719v7,Fully Supervised Speaker Diarization,"In this paper, we propose a fully supervised speaker diarization approach,
named unbounded interleaved-state recurrent neural networks (UIS-RNN). Given
extracted speaker-discriminative embeddings (a.k.a. d-vectors) from input
utterances, each individual speaker is modeled by a parameter-sharing RNN,
while the RNN states for different speakers interleave in the time domain. This
RNN is naturally integrated with a distance-dependent Chinese restaurant
process (ddCRP) to accommodate an unknown number of speakers. Our system is
fully supervised and is able to learn from examples where time-stamped speaker
labels are annotated. We achieved a 7.6% diarization error rate on NIST SRE
2000 CALLHOME, which is better than the state-of-the-art method using spectral
clustering. Moreover, our method decodes in an online fashion while most
state-of-the-art systems rely on offline clustering.","['Aonan Zhang', 'Quan Wang', 'Zhenyao Zhu', 'John Paisley', 'Chong Wang']","['eess.AS', 'cs.LG', 'stat.ML']",2018-10-10 19:21:44+00:00
http://arxiv.org/abs/1810.05731v1,Image Super-Resolution Using VDSR-ResNeXt and SRCGAN,"Over the past decade, many Super Resolution techniques have been developed
using deep learning. Among those, generative adversarial networks (GAN) and
very deep convolutional networks (VDSR) have shown promising results in terms
of HR image quality and computational speed. In this paper, we propose two
approaches based on these two algorithms: VDSR-ResNeXt, which is a deep
multi-branch convolutional network inspired by VDSR and ResNeXt; and SRCGAN,
which is a conditional GAN that explicitly passes class labels as input to the
GAN. The two methods were implemented on common SR benchmark datasets for both
quantitative and qualitative assessment.","['Saifuddin Hitawala', 'Yao Li', 'Xian Wang', 'Dongyang Yang']","['cs.CV', 'cs.LG', 'stat.ML']",2018-10-10 19:20:15+00:00
http://arxiv.org/abs/1810.04714v1,Training Generative Adversarial Networks with Binary Neurons by End-to-end Backpropagation,"We propose the BinaryGAN, a novel generative adversarial network (GAN) that
uses binary neurons at the output layer of the generator. We employ the
sigmoid-adjusted straight-through estimators to estimate the gradients for the
binary neurons and train the whole network by end-to-end backpropogation. The
proposed model is able to directly generate binary-valued predictions at test
time. We implement such a model to generate binarized MNIST digits and
experimentally compare the performance for different types of binary neurons,
GAN objectives and network architectures. Although the results are still
preliminary, we show that it is possible to train a GAN that has binary neurons
and that the use of gradient estimators can be a promising direction for
modeling discrete distributions with GANs. For reproducibility, the source code
is available at https://github.com/salu133445/binarygan .","['Hao-Wen Dong', 'Yi-Hsuan Yang']","['cs.LG', 'stat.ML']",2018-10-10 19:13:59+00:00
http://arxiv.org/abs/1810.05524v1,Introducing a hybrid model of DEA and data mining in evaluating efficiency. Case study: Bank Branches,"The banking industry is very important for an economic cycle of each country
and provides some quality of services for us. With the advancement in
technology and rapidly increasing of the complexity of the business
environment, it has become more competitive than the past so that efficiency
analysis in the banking industry attracts much attention in recent years. From
many aspects, such analyses at the branch level are more desirable. Evaluating
the branch performance with the purpose of eliminating deficiency can be a
crucial issue for branch managers to measure branch efficiency. This work not
only can lead to a better understanding of bank branch performance but also
give further information to enhance managerial decisions to recognize
problematic areas. To achieve this purpose, this study presents an integrated
approach based on Data Envelopment Analysis (DEA), Clustering algorithms and
Polynomial Pattern Classifier for constructing a classifier to identify a class
of bank branches. First, the efficiency estimates of individual branches are
evaluated by using the DEA approach. Next, when the range and number of classes
were identified by experts, the number of clusters is identified by an
agglomerative hierarchical clustering algorithm based on some statistical
methods. Next, we divide our raw data into k clusters By means of
self-organizing map (SOM) neural networks. Finally, all clusters are fed into
the reduced multivariate polynomial model to predict the classes of data.","['Sara Hosseinzadeh Kassani', 'Peyman Hosseinzadeh Kassani', 'Seyed Esmaeel Najafi']","['cs.LG', 'cs.AI', 'stat.ML']",2018-10-10 18:59:29+00:00
http://arxiv.org/abs/1810.04651v3,Principal component-guided sparse regression,"We propose a new method for supervised learning, especially suited to wide
data where the number of features is much greater than the number of
observations. The method combines the lasso ($\ell_1$) sparsity penalty with a
quadratic penalty that shrinks the coefficient vector toward the leading
principal components of the feature matrix. We call the proposed method the
""principal components lasso"" (""pcLasso""). The method can be especially powerful
if the features are pre-assigned to groups (such as cell-pathways, assays or
protein interaction networks). In that case, pcLasso shrinks each group-wise
component of the solution toward the leading principal components of that
group. In the process, it also carries out selection of the feature groups. We
provide some theory for this method and illustrate it on a number of simulated
and real data examples.","['J. Kenneth Tay', 'Jerome Friedman', 'Robert Tibshirani']","['stat.ME', 'stat.ML']",2018-10-10 17:18:46+00:00
http://arxiv.org/abs/1810.04650v2,Multi-Task Learning as Multi-Objective Optimization,"In multi-task learning, multiple tasks are solved jointly, sharing inductive
bias between them. Multi-task learning is inherently a multi-objective problem
because different tasks may conflict, necessitating a trade-off. A common
compromise is to optimize a proxy objective that minimizes a weighted linear
combination of per-task losses. However, this workaround is only valid when the
tasks do not compete, which is rarely the case. In this paper, we explicitly
cast multi-task learning as multi-objective optimization, with the overall
objective of finding a Pareto optimal solution. To this end, we use algorithms
developed in the gradient-based multi-objective optimization literature. These
algorithms are not directly applicable to large-scale learning problems since
they scale poorly with the dimensionality of the gradients and the number of
tasks. We therefore propose an upper bound for the multi-objective loss and
show that it can be optimized efficiently. We further prove that optimizing
this upper bound yields a Pareto optimal solution under realistic assumptions.
We apply our method to a variety of multi-task deep learning problems including
digit classification, scene understanding (joint semantic segmentation,
instance segmentation, and depth estimation), and multi-label classification.
Our method produces higher-performing models than recent multi-task learning
formulations or per-task training.","['Ozan Sener', 'Vladlen Koltun']","['cs.LG', 'stat.ML']",2018-10-10 17:18:09+00:00
http://arxiv.org/abs/1810.04642v1,Virtual Battery Parameter Identification using Transfer Learning based Stacked Autoencoder,"Recent studies have shown that the aggregated dynamic flexibility of an
ensemble of thermostatic loads can be modeled in the form of a virtual battery.
The existing methods for computing the virtual battery parameters require the
knowledge of the first-principle models and parameter values of the loads in
the ensemble. In real-world applications, however, it is likely that the only
available information are end-use measurements such as power consumption, room
temperature, device on/off status, etc., while very little about the individual
load models and parameters are known. We propose a transfer learning based deep
network framework for calculating virtual battery state of a given ensemble of
flexible thermostatic loads, from the available end-use measurements. This
proposed framework extracts first order virtual battery model parameters for
the given ensemble. We illustrate the effectiveness of this novel framework on
different ensembles of ACs and WHs.","['Indrasis Chakraborty', 'Sai Pushpak Nandanoori', 'Soumya Kundu']","['cs.LG', 'stat.ML']",2018-10-10 17:07:53+00:00
http://arxiv.org/abs/1810.04632v2,Non-linear process convolutions for multi-output Gaussian processes,"The paper introduces a non-linear version of the process convolution
formalism for building covariance functions for multi-output Gaussian
processes. The non-linearity is introduced via Volterra series, one series per
each output. We provide closed-form expressions for the mean function and the
covariance function of the approximated Gaussian process at the output of the
Volterra series. The mean function and covariance function for the joint
Gaussian process are derived using formulae for the product moments of Gaussian
variables. We compare the performance of the non-linear model against the
classical process convolution approach in one synthetic dataset and two real
datasets.","['Mauricio A. √Ålvarez', 'Wil O. C. Ward', 'Cristian Guarnizo']","['stat.ML', 'cs.LG']",2018-10-10 16:47:35+00:00
http://arxiv.org/abs/1810.04793v3,Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record,"The wide implementation of electronic health record (EHR) systems facilitates
the collection of large-scale health data from real clinical settings. Despite
the significant increase in adoption of EHR systems, this data remains largely
unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations
and predicting health outcomes. However, the heterogeneity, sparsity, noise,
and bias in this data present many complex challenges. This complexity makes it
difficult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec,
to learn an interpretable deep representation of longitudinal EHR data which is
personalized for each patient. To evaluate this approach, we apply it to the
prediction of future hospitalizations using real EHR data and compare its
predictive performance with baseline methods. Patient2Vec produces a vector
space with meaningful structure and it achieves an AUC around 0.799
outperforming baseline methods. In the end, the learned feature importance can
be visualized and interpreted at both the individual and population levels to
bring clinical insights.","['Jinghe Zhang', 'Kamran Kowsari', 'James H. Harrison', 'Jennifer M. Lobo', 'Laura E. Barnes']","['q-bio.QM', 'cs.AI', 'cs.IR', 'cs.LG', 'stat.ML']",2018-10-10 16:41:05+00:00
http://arxiv.org/abs/1810.04622v3,A Closer Look at Structured Pruning for Neural Network Compression,"Structured pruning is a popular method for compressing a neural network:
given a large trained network, one alternates between removing channel
connections and fine-tuning; reducing the overall width of the network.
However, the efficacy of structured pruning has largely evaded scrutiny. In
this paper, we examine ResNets and DenseNets obtained through structured
pruning-and-tuning and make two interesting observations: (i) reduced
networks---smaller versions of the original network trained from
scratch---consistently outperform pruned networks; (ii) if one takes the
architecture of a pruned network and then trains it from scratch it is
significantly more competitive. Furthermore, these architectures are easy to
approximate: we can prune once and obtain a family of new, scalable network
architectures that can simply be trained from scratch. Finally, we compare the
inference speed of reduced and pruned networks on hardware, and show that
reduced networks are significantly faster. Code is available at
https://github.com/BayesWatch/pytorch-prunes.","['Elliot J. Crowley', 'Jack Turner', 'Amos Storkey', ""Michael O'Boyle""]","['stat.ML', 'cs.CV', 'cs.LG']",2018-10-10 16:30:02+00:00
http://arxiv.org/abs/1810.05041v2,A General Framework for Fair Regression,"Fairness, through its many forms and definitions, has become an important
issue facing the machine learning community. In this work, we consider how to
incorporate group fairness constraints in kernel regression methods, applicable
to Gaussian processes, support vector machines, neural network regression and
decision tree regression. Further, we focus on examining the effect of
incorporating these constraints in decision tree regression, with direct
applications to random forests and boosted trees amongst other widespread
popular inference techniques. We show that the order of complexity of memory
and computation is preserved for such models and tightly bound the expected
perturbations to the model in terms of the number of leaves of the trees.
Importantly, the approach works on trained models and hence can be easily
applied to models in current use and group labels are only required on training
data.","['Jack Fitzsimons', 'AbdulRahman Al Ali', 'Michael Osborne', 'Stephen Roberts']","['cs.LG', 'cs.AI', 'stat.ML']",2018-10-10 16:16:03+00:00
http://arxiv.org/abs/1810.04586v1,The Laplacian in RL: Learning Representations with Efficient Approximations,"The smallest eigenvectors of the graph Laplacian are well-known to provide a
succinct representation of the geometry of a weighted graph. In reinforcement
learning (RL), where the weighted graph may be interpreted as the state
transition process induced by a behavior policy acting on the environment,
approximating the eigenvectors of the Laplacian provides a promising approach
to state representation learning. However, existing methods for performing this
approximation are ill-suited in general RL settings for two main reasons:
First, they are computationally expensive, often requiring operations on large
matrices. Second, these methods lack adequate justification beyond simple,
tabular, finite-state settings. In this paper, we present a fully general and
scalable method for approximating the eigenvectors of the Laplacian in a
model-free RL context. We systematically evaluate our approach and empirically
show that it generalizes beyond the tabular, finite-state setting. Even in
tabular, finite-state settings, its ability to approximate the eigenvectors
outperforms previous proposals. Finally, we show the potential benefits of
using a Laplacian representation learned using our method in goal-achieving RL
tasks, providing evidence that our technique can be used to significantly
improve the performance of an RL agent.","['Yifan Wu', 'George Tucker', 'Ofir Nachum']","['cs.LG', 'stat.ML']",2018-10-10 15:25:49+00:00
http://arxiv.org/abs/1810.04513v2,ET-Lasso: A New Efficient Tuning of Lasso-type Regularization for High-Dimensional Data,"The L1 regularization (Lasso) has proven to be a versatile tool to select
relevant features and estimate the model coefficients simultaneously and has
been widely used in many research areas such as genomes studies, finance, and
biomedical imaging. Despite its popularity, it is very challenging to guarantee
the feature selection consistency of Lasso especially when the dimension of the
data is huge. One way to improve the feature selection consistency is to select
an ideal tuning parameter. Traditional tuning criteria mainly focus on
minimizing the estimated prediction error or maximizing the posterior model
probability, such as cross-validation and BIC, which may either be
time-consuming or fail to control the false discovery rate (FDR) when the
number of features is extremely large. The other way is to introduce
pseudo-features to learn the importance of the original ones. Recently, the
Knockoff filter is proposed to control the FDR when performing feature
selection. However, its performance is sensitive to the choice of the expected
FDR threshold. Motivated by these ideas, we propose a new method using
pseudo-features to obtain an ideal tuning parameter. In particular, we present
the Efficient Tuning of Lasso (ET-Lasso) to separate active and inactive
features by adding permuted features as pseudo-features in linear models. The
pseudo-features are constructed to be inactive by nature, which can be used to
obtain a cutoff to select the tuning parameter that separates active and
inactive features. Experimental studies on both simulations and real-world data
applications are provided to show that ET-Lasso can effectively and efficiently
select active features under a wide range of scenarios","['Songshan Yang', 'Jiawei Wen', 'Xiang Zhan', 'Daniel Kifer']","['stat.ML', 'cs.LG']",2018-10-10 13:25:03+00:00
http://arxiv.org/abs/1810.04491v1,Multi-class Classification Model Inspired by Quantum Detection Theory,"Machine Learning has become very famous currently which assist in identifying
the patterns from the raw data. Technological advancement has led to
substantial improvement in Machine Learning which, thus helping to improve
prediction. Current Machine Learning models are based on Classical Theory,
which can be replaced by Quantum Theory to improve the effectiveness of the
model. In the previous work, we developed binary classifier inspired by Quantum
Detection Theory. In this extended abstract, our main goal is to develop
multi-class classifier. We generally use the terminology multinomial
classification or multi-class classification when we have a classification
problem for classifying observations or instances into one of three or more
classes.","['Prayag Tiwari', 'Massimo Melucci']","['cs.LG', 'stat.ML']",2018-10-10 12:56:06+00:00
http://arxiv.org/abs/1810.04472v3,Domain Confusion with Self Ensembling for Unsupervised Adaptation,"Data collection and annotation are time-consuming in machine learning,
expecially for large scale problem. A common approach for this problem is to
transfer knowledge from a related labeled domain to a target one. There are two
popular ways to achieve this goal: adversarial learning and self training. In
this article, we first analyze the training unstablity problem and the mistaken
confusion issue in adversarial learning process. Then, inspired by domain
confusion and self-ensembling methods, we propose a combined model to learn
feature and class jointly invariant representation, namely Domain Confusion
with Self Ensembling (DCSE). The experiments verified that our proposed
approach can offer better performance than empirical art in a variety of
unsupervised domain adaptation benchmarks.","['Jiawei Wang', 'Zhaoshui He', 'Chengjian Feng', 'Zhouping Zhu', 'Qinzhuang Lin', 'Jun Lv', 'Shengli Xie']","['cs.LG', 'cs.AI', 'stat.ML']",2018-10-10 12:09:36+00:00
http://arxiv.org/abs/1810.04468v2,Decentralized Cooperative Stochastic Bandits,"We study a decentralized cooperative stochastic multi-armed bandit problem
with $K$ arms on a network of $N$ agents. In our model, the reward distribution
of each arm is the same for each agent and rewards are drawn independently
across agents and time steps. In each round, each agent chooses an arm to play
and subsequently sends a message to her neighbors. The goal is to minimize the
overall regret of the entire network. We design a fully decentralized algorithm
that uses an accelerated consensus procedure to compute (delayed) estimates of
the average of rewards obtained by all the agents for each arm, and then uses
an upper confidence bound (UCB) algorithm that accounts for the delay and error
of the estimates. We analyze the regret of our algorithm and also provide a
lower bound. The regret is bounded by the optimal centralized regret plus a
natural and simple term depending on the spectral gap of the communication
matrix. Our algorithm is simpler to analyze than those proposed in prior work
and it achieves better regret bounds, while requiring less information about
the underlying network. It also performs better empirically.","['David Mart√≠nez-Rubio', 'Varun Kanade', 'Patrick Rebeschini']","['cs.LG', 'stat.ML']",2018-10-10 11:46:20+00:00
http://arxiv.org/abs/1810.06695v1,Exploring the Use of Attention within an Neural Machine Translation Decoder States to Translate Idioms,"Idioms pose problems to almost all Machine Translation systems. This type of
language is very frequent in day-to-day language use and cannot be simply
ignored. The recent interest in memory augmented models in the field of
Language Modelling has aided the systems to achieve good results by bridging
long-distance dependencies. In this paper we explore the use of such techniques
into a Neural Machine Translation system to help in translation of idiomatic
language.","['Giancarlo D. Salton', 'Robert J. Ross', 'John D. Kelleher']","['cs.CL', 'cs.LG', 'stat.ML']",2018-10-10 09:57:32+00:00
http://arxiv.org/abs/1810.04437v1,Persistence pays off: Paying Attention to What the LSTM Gating Mechanism Persists,"Language Models (LMs) are important components in several Natural Language
Processing systems. Recurrent Neural Network LMs composed of LSTM units,
especially those augmented with an external memory, have achieved
state-of-the-art results. However, these models still struggle to process long
sequences which are more likely to contain long-distance dependencies because
of information fading and a bias towards more recent information. In this paper
we demonstrate an effective mechanism for retrieving information in a memory
augmented LSTM LM based on attending to information in memory in proportion to
the number of timesteps the LSTM gating mechanism persisted the information.","['Giancarlo D. Salton', 'John D. Kelleher']","['cs.LG', 'stat.ML']",2018-10-10 09:48:20+00:00
http://arxiv.org/abs/1810.04433v3,Lazy-CFR: fast and near optimal regret minimization for extensive games with imperfect information,"Counterfactual regret minimization (CFR) is the most popular algorithm on
solving two-player zero-sum extensive games with imperfect information and
achieves state-of-the-art performance in practice. However, the performance of
CFR is not fully understood, since empirical results on the regret are much
better than the upper bound proved in \cite{zinkevich2008regret}. Another issue
is that CFR has to traverse the whole game tree in each round, which is
time-consuming in large scale games. In this paper, we present a novel
technique, lazy update, which can avoid traversing the whole game tree in CFR,
as well as a novel analysis on the regret of CFR with lazy update. Our analysis
can also be applied to the vanilla CFR, resulting in a much tighter regret
bound than that in \cite{zinkevich2008regret}. Inspired by lazy update, we
further present a novel CFR variant, named Lazy-CFR. Compared to traversing
$O(|\mathcal{I}|)$ information sets in vanilla CFR, Lazy-CFR needs only to
traverse $O(\sqrt{|\mathcal{I}|})$ information sets per round while keeping the
regret bound almost the same, where $\mathcal{I}$ is the class of all
information sets. As a result, Lazy-CFR shows better convergence result
compared with vanilla CFR. Experimental results consistently show that Lazy-CFR
outperforms the vanilla CFR significantly.","['Yichi Zhou', 'Tongzheng Ren', 'Jialian Li', 'Dong Yan', 'Jun Zhu']","['cs.LG', 'stat.ML']",2018-10-10 09:24:39+00:00
http://arxiv.org/abs/1810.05526v1,Automatic Configuration of Deep Neural Networks with EGO,"Designing the architecture for an artificial neural network is a cumbersome
task because of the numerous parameters to configure, including activation
functions, layer types, and hyper-parameters. With the large number of
parameters for most networks nowadays, it is intractable to find a good
configuration for a given task by hand. In this paper an Efficient Global
Optimization (EGO) algorithm is adapted to automatically optimize and configure
convolutional neural network architectures. A configurable neural network
architecture based solely on convolutional layers is proposed for the
optimization. Without using any knowledge on the target problem and not using
any data augmentation techniques, it is shown that on several image
classification tasks this approach is able to find competitive network
architectures in terms of prediction accuracy, compared to the best
hand-crafted ones in literature. In addition, a very small training budget (200
evaluations and 10 epochs in training) is spent on each optimized architectures
in contrast to the usual long training time of hand-crafted networks. Moreover,
instead of the standard sequential evaluation in EGO, several candidate
architectures are proposed and evaluated in parallel, which saves the execution
overheads significantly and leads to an efficient automation for deep neural
network design.","['Bas van Stein', 'Hao Wang', 'Thomas B√§ck']","['cs.LG', 'cs.NE', 'stat.ML']",2018-10-10 09:06:15+00:00
http://arxiv.org/abs/1810.04416v3,Harmonizable mixture kernels with variational Fourier features,"The expressive power of Gaussian processes depends heavily on the choice of
kernel. In this work we propose the novel harmonizable mixture kernel (HMK), a
family of expressive, interpretable, non-stationary kernels derived from
mixture models on the generalized spectral representation. As a theoretically
sound treatment of non-stationary kernels, HMK supports harmonizable
covariances, a wide subset of kernels including all stationary and many
non-stationary covariances. We also propose variational Fourier features, an
inter-domain sparse GP inference framework that offers a representative set of
'inducing frequencies'. We show that harmonizable mixture kernels interpolate
between local patterns, and that variational Fourier features offers a robust
kernel learning framework for the new kernel family.","['Zheyang Shen', 'Markus Heinonen', 'Samuel Kaski']","['stat.ML', 'cs.LG']",2018-10-10 08:41:51+00:00
http://arxiv.org/abs/1810.06394v1,Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space,"Most existing deep reinforcement learning (DRL) frameworks consider either
discrete action space or continuous action space solely. Motivated by
applications in computer games, we consider the scenario with
discrete-continuous hybrid action space. To handle hybrid action space,
previous works either approximate the hybrid space by discretization, or relax
it into a continuous set. In this paper, we propose a parametrized deep
Q-network (P- DQN) framework for the hybrid action space without approximation
or relaxation. Our algorithm combines the spirits of both DQN (dealing with
discrete action space) and DDPG (dealing with continuous action space) by
seamlessly integrating them. Empirical results on a simulation example, scoring
a goal in simulated RoboCup soccer and the solo mode in game King of Glory
(KOG) validate the efficiency and effectiveness of our method.","['Jiechao Xiong', 'Qing Wang', 'Zhuoran Yang', 'Peng Sun', 'Lei Han', 'Yang Zheng', 'Haobo Fu', 'Tong Zhang', 'Ji Liu', 'Han Liu']","['cs.LG', 'cs.AI', 'stat.ML']",2018-10-10 07:38:44+00:00
http://arxiv.org/abs/1810.05724v1,Unpaired High-Resolution and Scalable Style Transfer Using Generative Adversarial Networks,"Neural networks have proven their capabilities by outperforming many other
approaches on regression or classification tasks on various kinds of data.
Other astonishing results have been achieved using neural nets as data
generators, especially in settings of generative adversarial networks (GANs).
One special application is the field of image domain translations. Here, the
goal is to take an image with a certain style (e.g. a photography) and
transform it into another one (e.g. a painting). If such a task is performed
for unpaired training examples, the corresponding GAN setting is complex, the
neural networks are large, and this leads to a high peak memory consumption
during, both, training and evaluation phase. This sets a limit to the highest
processable image size. We address this issue by the idea of not processing the
whole image at once, but to train and evaluate the domain translation on the
level of overlapping image subsamples. This new approach not only enables us to
translate high-resolution images that otherwise cannot be processed by the
neural network at once, but also allows us to work with comparably small neural
networks and with limited hardware resources. Additionally, the number of
images required for the training process is significantly reduced. We present
high-quality results on images with a total resolution of up to over 50
megapixels and emonstrate that our method helps to preserve local image details
while it also keeps global consistency.","['Andrej Junginger', 'Markus Hanselmann', 'Thilo Strauss', 'Sebastian Boblest', 'Jens Buchner', 'Holger Ulmer']","['cs.CV', 'cs.LG', 'stat.ML']",2018-10-10 07:02:47+00:00
http://arxiv.org/abs/1810.04374v3,On the Approximation Properties of Random ReLU Features,"We study the approximation properties of random ReLU features through their
reproducing kernel Hilbert space (RKHS). We first prove a universality theorem
for the RKHS induced by random features whose feature maps are of the form of
nodes in neural networks. The universality result implies that the random ReLU
features method is a universally consistent learning algorithm. We prove that
despite the universality of the RKHS induced by the random ReLU features,
composition of functions in it generates substantially more complicated
functions that are harder to approximate than those functions simply in the
RKHS. We also prove that such composite functions can be efficiently
approximated by multi-layer ReLU networks with bounded weights. This depth
separation result shows that the random ReLU features models suffer from the
same weakness as that of shallow models. We show in experiments that the
performance of random ReLU features is comparable to that of random Fourier
features and, in general, has a lower computational cost. We also demonstrate
that when the target function is the composite function as described in the
depth separation theorem, 3-layer neural networks indeed outperform both random
ReLU features and 2-layer neural networks.","['Yitong Sun', 'Anna Gilbert', 'Ambuj Tewari']","['stat.ML', 'cs.LG']",2018-10-10 04:58:45+00:00
http://arxiv.org/abs/1810.04361v1,Semi-supervised clustering for de-duplication,"Data de-duplication is the task of detecting multiple records that correspond
to the same real-world entity in a database. In this work, we view
de-duplication as a clustering problem where the goal is to put records
corresponding to the same physical entity in the same cluster and putting
records corresponding to different physical entities into different clusters.
  We introduce a framework which we call promise correlation clustering. Given
a complete graph $G$ with the edges labelled $0$ and $1$, the goal is to find a
clustering that minimizes the number of $0$ edges within a cluster plus the
number of $1$ edges across different clusters (or correlation loss). The
optimal clustering can also be viewed as a complete graph $G^*$ with edges
corresponding to points in the same cluster being labelled $0$ and other edges
being labelled $1$. Under the promise that the edge difference between $G$ and
$G^*$ is ""small"", we prove that finding the optimal clustering (or $G^*$) is
still NP-Hard. [Ashtiani et. al, 2016] introduced the framework of
semi-supervised clustering, where the learning algorithm has access to an
oracle, which answers whether two points belong to the same or different
clusters. We further prove that even with access to a same-cluster oracle, the
promise version is NP-Hard as long as the number queries to the oracle is not
too large ($o(n)$ where $n$ is the number of vertices).
  Given these negative results, we consider a restricted version of correlation
clustering. As before, the goal is to find a clustering that minimizes the
correlation loss. However, we restrict ourselves to a given class $\mathcal F$
of clusterings. We offer a semi-supervised algorithmic approach to solve the
restricted variant with success guarantees.","['Shrinu Kushagra', 'Shai Ben-David', 'Ihab Ilyas']","['cs.LG', 'stat.ML']",2018-10-10 04:12:50+00:00
http://arxiv.org/abs/1810.04336v2,Combining Bayesian Optimization and Lipschitz Optimization,"Bayesian optimization and Lipschitz optimization have developed alternative
techniques for optimizing black-box functions. They each exploit a different
form of prior about the function. In this work, we explore strategies to
combine these techniques for better global optimization. In particular, we
propose ways to use the Lipschitz continuity assumption within traditional BO
algorithms, which we call Lipschitz Bayesian optimization (LBO). This approach
does not increase the asymptotic runtime and in some cases drastically improves
the performance (while in the worst-case the performance is similar). Indeed,
in a particular setting, we prove that using the Lipschitz information yields
the same or a better bound on the regret compared to using Bayesian
optimization on its own. Moreover, we propose a simple heuristics to estimate
the Lipschitz constant, and prove that a growing estimate of the Lipschitz
constant is in some sense ``harmless''. Our experiments on 15 datasets with 4
acquisition functions show that in the worst case LBO performs similar to the
underlying BO method while in some cases it performs substantially better.
Thompson sampling in particular typically saw drastic improvements (as the
Lipschitz information corrected for its well-known ``over-exploration''
phenomenon) and its LBO variant often outperformed other acquisition functions.","['Mohamed Osama Ahmed', 'Sharan Vaswani', 'Mark Schmidt']","['cs.LG', 'stat.ML']",2018-10-10 02:26:02+00:00
http://arxiv.org/abs/1810.04327v4,Complementary-Label Learning for Arbitrary Losses and Models,"In contrast to the standard classification paradigm where the true class is
given to each training pattern, complementary-label learning only uses training
patterns each equipped with a complementary label, which only specifies one of
the classes that the pattern does not belong to. The goal of this paper is to
derive a novel framework of complementary-label learning with an unbiased
estimator of the classification risk, for arbitrary losses and models---all
existing methods have failed to achieve this goal. Not only is this beneficial
for the learning stage, it also makes model/hyper-parameter selection (through
cross-validation) possible without the need of any ordinarily labeled
validation data, while using any linear/non-linear models or convex/non-convex
loss functions. We further improve the risk estimator by a non-negative
correction and gradient ascent trick, and demonstrate its superiority through
experiments.","['Takashi Ishida', 'Gang Niu', 'Aditya Krishna Menon', 'Masashi Sugiyama']","['stat.ML', 'cs.LG']",2018-10-10 01:52:43+00:00
http://arxiv.org/abs/1810.05665v2,Is PGD-Adversarial Training Necessary? Alternative Training via a Soft-Quantization Network with Noisy-Natural Samples Only,"Recent work on adversarial attack and defense suggests that PGD is a
universal $l_\infty$ first-order attack, and PGD adversarial training can
significantly improve network robustness against a wide range of first-order
$l_\infty$-bounded attacks, represented as the state-of-the-art defense method.
However, an obvious weakness of PGD adversarial training is its
highly-computational cost in generating adversarial samples, making it
computationally infeasible for large and high-resolution real datasets such as
the ImageNet dataset. In addition, recent work also has suggested a simple
""close-form"" solution to a robust model on MNIST. Therefore, a natural question
raised is that is PGD adversarial training really necessary for robust defense?
In this paper, we give a negative answer by proposing a training paradigm that
is comparable to PGD adversarial training on several standard datasets, while
only using noisy-natural samples. Specifically, we reformulate the min-max
objective in PGD adversarial training by a problem to minimize the original
network loss plus $l_1$ norms of its gradients w.r.t. the inputs. For the
$l_1$-norm loss, we propose a computationally-feasible solution by embedding a
differentiable soft-quantization layer after the network input layer. We show
formally that the soft-quantization layer trained with noisy-natural samples is
an alternative approach to minimizing the $l_1$-gradient norms as in PGD
adversarial training. Extensive empirical evaluations on standard datasets show
that our proposed models are comparable to PGD-adversarially-trained models
under PGD and BPDA attacks. Remarkably, our method achieves a 24X speed-up on
MNIST while maintaining a comparable defensive ability, and for the first time
fine-tunes a robust Imagenet model within only two days. Code is provided on
\url{https://github.com/tianzheng4/Noisy-Training-Soft-Quantization}","['Tianhang Zheng', 'Changyou Chen', 'Kui Ren']","['cs.LG', 'stat.ML']",2018-10-10 01:06:05+00:00
http://arxiv.org/abs/1810.04304v2,Multi-Institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation,"Deep learning models for semantic segmentation of images require large
amounts of data. In the medical imaging domain, acquiring sufficient data is a
significant challenge. Labeling medical image data requires expert knowledge.
Collaboration between institutions could address this challenge, but sharing
medical data to a centralized location faces various legal, privacy, technical,
and data-ownership challenges, especially among international institutions. In
this study, we introduce the first use of federated learning for
multi-institutional collaboration, enabling deep learning modeling without
sharing patient data. Our quantitative results demonstrate that the performance
of federated semantic segmentation models (Dice=0.852) on multimodal brain
scans is similar to that of models trained by sharing data (Dice=0.862). We
compare federated learning with two alternative collaborative learning methods
and find that they fail to match the performance of federated learning.","['Micah J Sheller', 'G Anthony Reina', 'Brandon Edwards', 'Jason Martin', 'Spyridon Bakas']","['cs.LG', 'stat.ML']",2018-10-10 00:05:44+00:00
http://arxiv.org/abs/1810.04303v1,Batch Active Preference-Based Learning of Reward Functions,"Data generation and labeling are usually an expensive part of learning for
robotics. While active learning methods are commonly used to tackle the former
problem, preference-based learning is a concept that attempts to solve the
latter by querying users with preference questions. In this paper, we will
develop a new algorithm, batch active preference-based learning, that enables
efficient learning of reward functions using as few data samples as possible
while still having short query generation times. We introduce several
approximations to the batch active learning problem, and provide theoretical
guarantees for the convergence of our algorithms. Finally, we present our
experimental results for a variety of robotics tasks in simulation. Our results
suggest that our batch active learning algorithm requires only a few queries
that are computed in a short amount of time. We then showcase our algorithm in
a study to learn human users' preferences.","['Erdem Bƒ±yƒ±k', 'Dorsa Sadigh']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",2018-10-10 00:02:55+00:00
http://arxiv.org/abs/1810.04261v2,"A Tale of Three Probabilistic Families: Discriminative, Descriptive and Generative Models","The pattern theory of Grenander is a mathematical framework where patterns
are represented by probability models on random variables of algebraic
structures. In this paper, we review three families of probability models,
namely, the discriminative models, the descriptive models, and the generative
models. A discriminative model is in the form of a classifier. It specifies the
conditional probability of the class label given the input signal. A
descriptive model specifies the probability distribution of the signal, based
on an energy function defined on the signal. A generative model assumes that
the signal is generated by some latent variables via a transformation. We shall
review these models within a common framework and explore their connections. We
shall also review the recent developments that take advantage of the high
approximation capacities of deep neural networks.","['Ying Nian Wu', 'Ruiqi Gao', 'Tian Han', 'Song-Chun Zhu']","['stat.ML', 'cs.CV', 'cs.LG']",2018-10-09 21:54:54+00:00
http://arxiv.org/abs/1810.04249v2,Data-dependent compression of random features for large-scale kernel approximation,"Kernel methods offer the flexibility to learn complex relationships in
modern, large data sets while enjoying strong theoretical guarantees on
quality. Unfortunately, these methods typically require cubic running time in
the data set size, a prohibitive cost in the large-data setting. Random feature
maps (RFMs) and the Nystrom method both consider low-rank approximations to the
kernel matrix as a potential solution. But, in order to achieve desirable
theoretical guarantees, the former may require a prohibitively large number of
features J+, and the latter may be prohibitively expensive for high-dimensional
problems. We propose to combine the simplicity and generality of RFMs with a
data-dependent feature selection scheme to achieve desirable theoretical
approximation properties of Nystrom with just O(log J+) features. Our key
insight is to begin with a large set of random features, then reduce them to a
small number of weighted features in a data-dependent, computationally
efficient way, while preserving the statistical guarantees of using the
original large set of features. We demonstrate the efficacy of our method with
theory and experiments--including on a data set with over 50 million
observations. In particular, we show that our method achieves small kernel
matrix approximation error and better test set accuracy with provably fewer
random features than state-of-the-art methods.","['Raj Agrawal', 'Trevor Campbell', 'Jonathan H. Huggins', 'Tamara Broderick']","['stat.ML', 'cs.LG']",2018-10-09 21:20:41+00:00
http://arxiv.org/abs/1810.04247v7,Feature Selection using Stochastic Gates,"Feature selection problems have been extensively studied for linear
estimation, for instance, Lasso, but less emphasis has been placed on feature
selection for non-linear functions. In this study, we propose a method for
feature selection in high-dimensional non-linear function estimation problems.
The new procedure is based on minimizing the $\ell_0$ norm of the vector of
indicator variables that represent if a feature is selected or not. Our
approach relies on the continuous relaxation of Bernoulli distributions, which
allows our model to learn the parameters of the approximate Bernoulli
distributions via gradient descent. This general framework simultaneously
minimizes a loss function while selecting relevant features. Furthermore, we
provide an information-theoretic justification of incorporating Bernoulli
distribution into our approach and demonstrate the potential of the approach on
synthetic and real-life applications.","['Yutaro Yamada', 'Ofir Lindenbaum', 'Sahand Negahban', 'Yuval Kluger']","['cs.LG', 'stat.ML']",2018-10-09 21:17:37+00:00
