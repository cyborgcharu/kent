id,title,abstract,authors,categories,date
http://arxiv.org/abs/1610.07104v1,Independent Component Analysis by Entropy Maximization with Kernels,"Independent component analysis (ICA) is the most popular method for blind
source separation (BSS) with a diverse set of applications, such as biomedical
signal processing, video and image analysis, and communications. Maximum
likelihood (ML), an optimal theoretical framework for ICA, requires knowledge
of the true underlying probability density function (PDF) of the latent
sources, which, in many applications, is unknown. ICA algorithms cast in the ML
framework often deviate from its theoretical optimality properties due to poor
estimation of the source PDF. Therefore, accurate estimation of source PDFs is
critical in order to avoid model mismatch and poor ICA performance. In this
paper, we propose a new and efficient ICA algorithm based on entropy
maximization with kernels, (ICA-EMK), which uses both global and local
measuring functions as constraints to dynamically estimate the PDF of the
sources with reasonable complexity. In addition, the new algorithm performs
optimization with respect to each of the cost function gradient directions
separately, enabling parallel implementations on multi-core computers. We
demonstrate the superior performance of ICA-EMK over competing ICA algorithms
using simulated as well as real-world data.","['Zois Boukouvalas', 'Rami Mowakeaa', 'Geng-Shen Fu', 'Tulay Adali']",['stat.ML'],2016-10-22 23:08:01+00:00
http://arxiv.org/abs/1610.06972v1,Learning Cost-Effective Treatment Regimes using Markov Decision Processes,"Decision makers, such as doctors and judges, make crucial decisions such as
recommending treatments to patients, and granting bails to defendants on a
daily basis. Such decisions typically involve weighting the potential benefits
of taking an action against the costs involved. In this work, we aim to
automate this task of learning \emph{cost-effective, interpretable and
actionable treatment regimes}. We formulate this as a problem of learning a
decision list -- a sequence of if-then-else rules -- which maps characteristics
of subjects (eg., diagnostic test results of patients) to treatments. We
propose a novel objective to construct a decision list which maximizes outcomes
for the population, and minimizes overall costs. We model the problem of
learning such a list as a Markov Decision Process (MDP) and employ a variant of
the Upper Confidence Bound for Trees (UCT) strategy which leverages customized
checks for pruning the search space effectively. Experimental results on real
world observational data capturing judicial bail decisions and treatment
recommendations for asthma patients demonstrate the effectiveness of our
approach.","['Himabindu Lakkaraju', 'Cynthia Rudin']","['cs.AI', 'cs.LG', 'stat.ML']",2016-10-21 23:17:03+00:00
http://arxiv.org/abs/1610.06949v1,Mean-Field Variational Inference for Gradient Matching with Gaussian Processes,"Gradient matching with Gaussian processes is a promising tool for learning
parameters of ordinary differential equations (ODE's). The essence of gradient
matching is to model the prior over state variables as a Gaussian process which
implies that the joint distribution given the ODE's and GP kernels is also
Gaussian distributed. The state-derivatives are integrated out analytically
since they are modelled as latent variables. However, the state variables
themselves are also latent variables because they are contaminated by noise.
Previous work sampled the state variables since integrating them out is
\textit{not} analytically tractable. In this paper we use mean-field
approximation to establish tight variational lower bounds that decouple state
variables and are therefore, in contrast to the integral over state variables,
analytically tractable and even concave for a restricted family of ODE's,
including nonlinear and periodic ODE's. Such variational lower bounds
facilitate ""hill climbing"" to determine the maximum a posteriori estimate of
ODE parameters. An additional advantage of our approach over sampling methods
is the determination of a proxy to the intractable posterior distribution over
state variables given observations and the ODE's.","['Nico S. Gorbach', 'Stefan Bauer', 'Joachim M. Buhmann']",['stat.ML'],2016-10-21 20:50:47+00:00
http://arxiv.org/abs/1610.06940v3,Safety Verification of Deep Neural Networks,"Deep neural networks have achieved impressive experimental results in image
classification, but can surprisingly be unstable with respect to adversarial
perturbations, that is, minimal changes to the input image that cause the
network to misclassify it. With potential applications including perception
modules and end-to-end controllers for self-driving cars, this raises concerns
about their safety. We develop a novel automated verification framework for
feed-forward multi-layer neural networks based on Satisfiability Modulo Theory
(SMT). We focus on safety of image classification decisions with respect to
image manipulations, such as scratches or changes to camera angle or lighting
conditions that would result in the same class being assigned by a human, and
define safety for an individual decision in terms of invariance of the
classification within a small neighbourhood of the original image. We enable
exhaustive search of the region by employing discretisation, and propagate the
analysis layer by layer. Our method works directly with the network code and,
in contrast to existing methods, can guarantee that adversarial examples, if
they exist, are found for the given region and family of manipulations. If
found, adversarial examples can be shown to human testers and/or used to
fine-tune the network. We implement the techniques using Z3 and evaluate them
on state-of-the-art networks, including regularised and deep learning networks.
We also compare against existing techniques to search for adversarial examples
and estimate network robustness.","['Xiaowei Huang', 'Marta Kwiatkowska', 'Sen Wang', 'Min Wu']","['cs.AI', 'cs.LG', 'stat.ML']",2016-10-21 20:16:16+00:00
http://arxiv.org/abs/1610.06902v1,Dictionary Learning Strategies for Compressed Fiber Sensing Using a Probabilistic Sparse Model,"We present a sparse estimation and dictionary learning framework for
compressed fiber sensing based on a probabilistic hierarchical sparse model. To
handle severe dictionary coherence, selective shrinkage is achieved using a
Weibull prior, which can be related to non-convex optimization with $p$-norm
constraints for $0 < p < 1$. In addition, we leverage the specific dictionary
structure to promote collective shrinkage based on a local similarity model.
This is incorporated in form of a kernel function in the joint prior density of
the sparse coefficients, thereby establishing a Markov random field-relation.
Approximate inference is accomplished using a hybrid technique that combines
Hamilton Monte Carlo and Gibbs sampling. To estimate the dictionary parameter,
we pursue two strategies, relying on either a deterministic or a probabilistic
model for the dictionary parameter. In the first strategy, the parameter is
estimated based on alternating estimation. In the second strategy, it is
jointly estimated along with the sparse coefficients. The performance is
evaluated in comparison to an existing method in various scenarios using
simulations and experimental data.","['Christian Weiss', 'Abdelhak M. Zoubir']",['stat.ML'],2016-10-21 19:27:48+00:00
http://arxiv.org/abs/1610.06811v1,Convex Formulation for Kernel PCA and its Use in Semi-Supervised Learning,"In this paper, Kernel PCA is reinterpreted as the solution to a convex
optimization problem. Actually, there is a constrained convex problem for each
principal component, so that the constraints guarantee that the principal
component is indeed a solution, and not a mere saddle point. Although these
insights do not imply any algorithmic improvement, they can be used to further
understand the method, formulate possible extensions and properly address them.
As an example, a new convex optimization problem for semi-supervised
classification is proposed, which seems particularly well-suited whenever the
number of known labels is small. Our formulation resembles a Least Squares SVM
problem with a regularization parameter multiplied by a negative sign, combined
with a variational principle for Kernel PCA. Our primal optimization principle
for semi-supervised learning is solved in terms of the Lagrange multipliers.
Numerical experiments in several classification tasks illustrate the
performance of the proposed model in problems with only a few labeled data.","['Carlos M. Alaíz', 'Michaël Fanuel', 'Johan A. K. Suykens']","['cs.LG', 'stat.ML']",2016-10-21 14:55:48+00:00
http://arxiv.org/abs/1610.06806v1,Robust training on approximated minimal-entropy set,"In this paper, we propose a general framework to learn a robust large-margin
binary classifier when corrupt measurements, called anomalies, caused by sensor
failure might be present in the training set. The goal is to minimize the
generalization error of the classifier on non-corrupted measurements while
controlling the false alarm rate associated with anomalous samples. By
incorporating a non-parametric regularizer based on an empirical entropy
estimator, we propose a Geometric-Entropy-Minimization regularized Maximum
Entropy Discrimination (GEM-MED) method to learn to classify and detect
anomalies in a joint manner. We demonstrate using simulated data and a real
multimodal data set. Our GEM-MED method can yield improved performance over
previous robust classification methods in terms of both classification accuracy
and anomaly detection rate.","['Tianpei Xie', 'Nasser. M. Narabadi', 'Alfred O. Hero']","['cs.LG', 'stat.ML', 'I.1.2; H.1.1']",2016-10-21 14:38:38+00:00
http://arxiv.org/abs/1610.06761v1,Maximally Divergent Intervals for Anomaly Detection,"We present new methods for batch anomaly detection in multivariate time
series. Our methods are based on maximizing the Kullback-Leibler divergence
between the data distribution within and outside an interval of the time
series. An empirical analysis shows the benefits of our algorithms compared to
methods that treat each time step independently from each other without
optimizing with respect to all possible intervals.","['Erik Rodner', 'Björn Barz', 'Yanira Guanche', 'Milan Flach', 'Miguel Mahecha', 'Paul Bodesheim', 'Markus Reichstein', 'Joachim Denzler']","['stat.ML', 'cs.LG']",2016-10-21 12:30:30+00:00
http://arxiv.org/abs/1610.06731v3,Minimax Error of Interpolation and Optimal Design of Experiments for Variable Fidelity Data,"Engineering problems often involve data sources of variable fidelity with
different costs of obtaining an observation. In particular, one can use both a
cheap low fidelity function (e.g. a computational experiment with a CFD code)
and an expensive high fidelity function (e.g. a wind tunnel experiment) to
generate a data sample in order to construct a regression model of a high
fidelity function. The key question in this setting is how the sizes of the
high and low fidelity data samples should be selected in order to stay within a
given computational budget and maximize accuracy of the regression model prior
to committing resources on data acquisition.
  In this paper we obtain minimax interpolation errors for single and variable
fidelity scenarios for a multivariate Gaussian process regression. Evaluation
of the minimax errors allows us to identify cases when the variable fidelity
data provides better interpolation accuracy than the exclusively high fidelity
data for the same computational budget.
  These results allow us to calculate the optimal shares of variable fidelity
data samples under the given computational budget constraint. Real and
synthetic data experiments suggest that using the obtained optimal shares often
outperforms natural heuristics in terms of the regression accuracy.","['Alexey Zaytsev', 'Evgeny Burnaev']","['stat.ML', 'math.ST', 'stat.AP', 'stat.TH']",2016-10-21 10:24:08+00:00
http://arxiv.org/abs/1610.07857v1,Hybrid clustering-classification neural network in the medical diagnostics of reactive arthritis,"The hybrid clustering-classification neural network is proposed. This network
allows increasing a quality of information processing under the condition of
overlapping classes due to the rational choice of a learning rate parameter and
introducing a special procedure of fuzzy reasoning in the clustering process,
which occurs both with an external learning signal (supervised) and without the
one (unsupervised). As similarity measure neighborhood function or membership
one, cosine structures are used, which allow to provide a high flexibility due
to self-learning-learning process and to provide some new useful properties.
Many realized experiments have confirmed the efficiency of proposed hybrid
clustering-classification neural network; also, this network was used for
solving diagnostics task of reactive arthritis.","['Yevgeniy Bodyanskiy', 'Olena Vynokurova', 'Volodymyr Savvo', 'Tatiana Tverdokhlib', 'Pavlo Mulesa']","['cs.LG', 'cs.NE', 'stat.ML']",2016-10-21 09:11:53+00:00
http://arxiv.org/abs/1610.06700v1,End-to-End Training Approaches for Discriminative Segmental Models,"Recent work on discriminative segmental models has shown that they can
achieve competitive speech recognition performance, using features based on
deep neural frame classifiers. However, segmental models can be more
challenging to train than standard frame-based approaches. While some segmental
models have been successfully trained end to end, there is a lack of
understanding of their training under different settings and with different
losses.
  We investigate a model class based on recent successful approaches,
consisting of a linear model that combines segmental features based on an LSTM
frame classifier. Similarly to hybrid HMM-neural network models, segmental
models of this class can be trained in two stages (frame classifier training
followed by linear segmental model weight training), end to end (joint training
of both frame classifier and linear weights), or with end-to-end fine-tuning
after two-stage training.
  We study segmental models trained end to end with hinge loss, log loss,
latent hinge loss, and marginal log loss. We consider several losses for the
case where training alignments are available as well as where they are not.
  We find that in general, marginal log loss provides the most consistent
strong performance without requiring ground-truth alignments. We also find that
training with dropout is very important in obtaining good performance with
end-to-end training. Finally, the best results are typically obtained by a
combination of two-stage training and fine-tuning.","['Hao Tang', 'Weiran Wang', 'Kevin Gimpel', 'Karen Livescu']","['cs.CL', 'cs.LG', 'stat.ML']",2016-10-21 08:45:35+00:00
http://arxiv.org/abs/1610.06665v1,On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators,"Recent advances in Bayesian learning with large-scale data have witnessed
emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic
gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC
(SGHMC), and the stochastic gradient thermostat. While finite-time convergence
properties of the SGLD with a 1st-order Euler integrator have recently been
studied, corresponding theory for general SG-MCMCs has not been explored. In
this paper we consider general SG-MCMCs with high-order integrators, and
develop theory to analyze finite-time convergence properties and their
asymptotic invariant measures. Our theoretical results show faster convergence
rates and more accurate invariant measures for SG-MCMCs with higher-order
integrators. For example, with the proposed efficient 2nd-order symmetric
splitting integrator, the {\em mean square error} (MSE) of the posterior
average for the SGHMC achieves an optimal convergence rate of $L^{-4/5}$ at $L$
iterations, compared to $L^{-2/3}$ for the SGHMC and SGLD with 1st-order Euler
integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs
are also developed, with the same convergence rates as their fixed-step-size
counterparts for a specific decreasing sequence. Experiments on both synthetic
and real datasets verify our theory, and show advantages of the proposed method
in two large-scale real applications.","['Changyou Chen', 'Nan Ding', 'Lawrence Carin']",['stat.ML'],2016-10-21 04:28:15+00:00
http://arxiv.org/abs/1610.06664v1,Stochastic Gradient MCMC with Stale Gradients,"Stochastic gradient MCMC (SG-MCMC) has played an important role in
large-scale Bayesian learning, with well-developed theoretical convergence
properties. In such applications of SG-MCMC, it is becoming increasingly
popular to employ distributed systems, where stochastic gradients are computed
based on some outdated parameters, yielding what are termed stale gradients.
While stale gradients could be directly used in SG-MCMC, their impact on
convergence properties has not been well studied. In this paper we develop
theory to show that while the bias and MSE of an SG-MCMC algorithm depend on
the staleness of stochastic gradients, its estimation variance (relative to the
expected estimate, based on a prescribed number of samples) is independent of
it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients
are computed asynchronously by a set of workers, our theory indicates a linear
speedup on the decrease of estimation variance w.r.t. the number of workers.
Experiments on synthetic data and deep neural networks validate our theory,
demonstrating the effectiveness and scalability of SG-MCMC with stale
gradients.","['Changyou Chen', 'Nan Ding', 'Chunyuan Li', 'Yizhe Zhang', 'Lawrence Carin']","['stat.ML', 'cs.LG']",2016-10-21 04:18:11+00:00
http://arxiv.org/abs/1610.06656v2,Single Pass PCA of Matrix Products,"In this paper we present a new algorithm for computing a low rank
approximation of the product $A^TB$ by taking only a single pass of the two
matrices $A$ and $B$. The straightforward way to do this is to (a) first sketch
$A$ and $B$ individually, and then (b) find the top components using PCA on the
sketch. Our algorithm in contrast retains additional summary information about
$A,B$ (e.g. row and column norms etc.) and uses this additional information to
obtain an improved approximation from the sketches. Our main analytical result
establishes a comparable spectral norm guarantee to existing two-pass methods;
in addition we also provide results from an Apache Spark implementation that
shows better computational and statistical performance on real-world and
synthetic evaluation datasets.","['Shanshan Wu', 'Srinadh Bhojanapalli', 'Sujay Sanghavi', 'Alexandros G. Dimakis']","['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']",2016-10-21 02:45:46+00:00
http://arxiv.org/abs/1610.06603v4,Combinatorial Multi-Armed Bandit with General Reward Functions,"In this paper, we study the stochastic combinatorial multi-armed bandit
(CMAB) framework that allows a general nonlinear reward function, whose
expected value may not depend only on the means of the input random variables
but possibly on the entire distributions of these variables. Our framework
enables a much larger class of reward functions such as the $\max()$ function
and nonlinear utility functions. Existing techniques relying on accurate
estimations of the means of random variables, such as the upper confidence
bound (UCB) technique, do not work directly on these functions. We propose a
new algorithm called stochastically dominant confidence bound (SDCB), which
estimates the distributions of underlying random variables and their
stochastically dominant confidence bounds. We prove that SDCB can achieve
$O(\log{T})$ distribution-dependent regret and $\tilde{O}(\sqrt{T})$
distribution-independent regret, where $T$ is the time horizon. We apply our
results to the $K$-MAX problem and expected utility maximization problems. In
particular, for $K$-MAX, we provide the first polynomial-time approximation
scheme (PTAS) for its offline problem, and give the first $\tilde{O}(\sqrt T)$
bound on the $(1-\epsilon)$-approximation regret of its online problem, for any
$\epsilon>0$.","['Wei Chen', 'Wei Hu', 'Fu Li', 'Jian Li', 'Yu Liu', 'Pinyan Lu']","['cs.LG', 'cs.DS', 'stat.ML']",2016-10-20 20:54:41+00:00
http://arxiv.org/abs/1610.06551v1,Nonlinear Structural Vector Autoregressive Models for Inferring Effective Brain Network Connectivity,"Structural equation models (SEMs) and vector autoregressive models (VARMs)
are two broad families of approaches that have been shown useful in effective
brain connectivity studies. While VARMs postulate that a given region of
interest in the brain is directionally connected to another one by virtue of
time-lagged influences, SEMs assert that causal dependencies arise due to
contemporaneous effects, and may even be adopted when nodal measurements are
not necessarily multivariate time series. To unify these complementary
perspectives, linear structural vector autoregressive models (SVARMs) that
leverage both contemporaneous and time-lagged nodal data have recently been put
forth. Albeit simple and tractable, linear SVARMs are quite limited since they
are incapable of modeling nonlinear dependencies between neuronal time series.
To this end, the overarching goal of the present paper is to considerably
broaden the span of linear SVARMs by capturing nonlinearities through kernels,
which have recently emerged as a powerful nonlinear modeling framework in
canonical machine learning tasks, e.g., regression, classification, and
dimensionality reduction. The merits of kernel-based methods are extended here
to the task of learning the effective brain connectivity, and an efficient
regularized estimator is put forth to leverage the edge sparsity inherent to
real-world complex networks. Judicious kernel choice from a preselected
dictionary of kernels is also addressed using a data-driven approach. Extensive
numerical tests on ECoG data captured through a study on epileptic seizures
demonstrate that it is possible to unveil previously unknown causal links
between brain regions of interest.","['Yanning Shen', 'Brian Baingana', 'Georgios B. Giannakis']","['stat.AP', 'stat.ML']",2016-10-20 19:37:46+00:00
http://arxiv.org/abs/1610.06545v4,Revisiting Classifier Two-Sample Tests,"The goal of two-sample tests is to assess whether two samples, $S_P \sim P^n$
and $S_Q \sim Q^m$, are drawn from the same distribution. Perhaps intriguingly,
one relatively unexplored method to build two-sample tests is the use of binary
classifiers. In particular, construct a dataset by pairing the $n$ examples in
$S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a
negative label. If the null hypothesis ""$P = Q$"" is true, then the
classification accuracy of a binary classifier on a held-out subset of this
dataset should remain near chance-level. As we will show, such Classifier
Two-Sample Tests (C2ST) learn a suitable representation of the data on the fly,
return test statistics in interpretable units, have a simple null distribution,
and their predictive uncertainty allow to interpret where $P$ and $Q$ differ.
The goal of this paper is to establish the properties, performance, and uses of
C2ST. First, we analyze their main theoretical properties. Second, we compare
their performance against a variety of state-of-the-art alternatives. Third, we
propose their use to evaluate the sample quality of generative models with
intractable likelihoods, such as Generative Adversarial Networks (GANs).
Fourth, we showcase the novel application of GANs together with C2ST for causal
discovery.","['David Lopez-Paz', 'Maxime Oquab']",['stat.ML'],2016-10-20 19:16:10+00:00
http://arxiv.org/abs/1610.06525v2,ChoiceRank: Identifying Preferences from Node Traffic in Networks,"Understanding how users navigate in a network is of high interest in many
applications. We consider a setting where only aggregate node-level traffic is
observed and tackle the task of learning edge transition probabilities. We cast
it as a preference learning problem, and we study a model where choices follow
Luce's axiom. In this case, the $O(n)$ marginal counts of node visits are a
sufficient statistic for the $O(n^2)$ transition probabilities. We show how to
make the inference problem well-posed regardless of the network's structure,
and we present ChoiceRank, an iterative algorithm that scales to networks that
contains billions of nodes and edges. We apply the model to two clickstream
datasets and show that it successfully recovers the transition probabilities
using only the network structure and marginal (node-level) traffic data.
Finally, we also consider an application to mobility networks and apply the
model to one year of rides on New York City's bicycle-sharing system.","['Lucas Maystre', 'Matthias Grossglauser']","['stat.ML', 'cs.LG', 'cs.SI']",2016-10-20 18:19:07+00:00
http://arxiv.org/abs/1610.06773v2,Variational Koopman models: slow collective variables and molecular kinetics from short off-equilibrium simulations,"Markov state models (MSMs) and Master equation models are popular approaches
to approximate molecular kinetics, equilibria, metastable states, and reaction
coordinates in terms of a state space discretization usually obtained by
clustering. Recently, a powerful generalization of MSMs has been introduced,
the variational approach (VA) of molecular kinetics and its special case the
time-lagged independent component analysis (TICA), which allow us to
approximate slow collective variables and molecular kinetics by linear
combinations of smooth basis functions or order parameters. While it is known
how to estimate MSMs from trajectories whose starting points are not sampled
from an equilibrium ensemble, this has not yet been the case for TICA and the
VA. Previous estimates from short trajectories, have been strongly biased and
thus not variationally optimal. Here, we employ Koopman operator theory and
ideas from dynamic mode decomposition (DMD) to extend the VA and TICA to
non-equilibrium data. The main insight is that the VA and TICA provide a
coefficient matrix that we call Koopman model, as it approximates the
underlying dynamical (Koopman) operator in conjunction with the basis set used.
This Koopman model can be used to compute a stationary vector to reweight the
data to equilibrium. From such a Koopman-reweighted sample, equilibrium
expectation values and variationally optimal reversible Koopman models can be
constructed even with short simulations. The Koopman model can be used to
propagate densities, and its eigenvalue decomposition provide estimates of
relaxation timescales and slow collective variables for dimension reduction.
Koopman models are generalizations of Markov state models, TICA and the linear
VA and allow molecular kinetics to be described without a cluster
discretization.","['Hao Wu', 'Feliks Nüske', 'Fabian Paul', 'Stefan Klus', 'Peter Koltai', 'Frank Noé']","['stat.ML', 'physics.bio-ph', 'physics.chem-ph', 'q-bio.BM']",2016-10-20 16:15:09+00:00
http://arxiv.org/abs/1610.06462v3,Gaussian process modeling in approximate Bayesian computation to estimate horizontal gene transfer in bacteria,"Approximate Bayesian computation (ABC) can be used for model fitting when the
likelihood function is intractable but simulating from the model is feasible.
However, even a single evaluation of a complex model may take several hours,
limiting the number of model evaluations available. Modelling the discrepancy
between the simulated and observed data using a Gaussian process (GP) can be
used to reduce the number of model evaluations required by ABC, but the
sensitivity of this approach to a specific GP formulation has not yet been
thoroughly investigated. We begin with a comprehensive empirical evaluation of
using GPs in ABC, including various transformations of the discrepancies and
two novel GP formulations. Our results indicate the choice of GP may
significantly affect the accuracy of the estimated posterior distribution.
Selection of an appropriate GP model is thus important. We formulate expected
utility to measure the accuracy of classifying discrepancies below or above the
ABC threshold, and show that it can be used to automate the GP model selection
step. Finally, based on the understanding gained with toy examples, we fit a
population genetic model for bacteria, providing insight into horizontal gene
transfer events within the population and from external origins.","['Marko Järvenpää', 'Michael Gutmann', 'Aki Vehtari', 'Pekka Marttinen']","['stat.ML', 'stat.AP', 'stat.ME']",2016-10-20 15:39:15+00:00
http://arxiv.org/abs/1610.06461v1,Efficient Estimation of Compressible State-Space Models with Application to Calcium Signal Deconvolution,"In this paper, we consider linear state-space models with compressible
innovations and convergent transition matrices in order to model
spatiotemporally sparse transient events. We perform parameter and state
estimation using a dynamic compressed sensing framework and develop an
efficient solution consisting of two nested Expectation-Maximization (EM)
algorithms. Under suitable sparsity assumptions on the innovations, we prove
recovery guarantees and derive confidence bounds for the state estimates. We
provide simulation studies as well as application to spike deconvolution from
calcium imaging data which verify our theoretical results and show significant
improvement over existing algorithms.","['Abbas Kazemipour', 'Ji Liu', 'Patrick Kanold', 'Min Wu', 'Behtash Babadi']","['stat.ML', 'cs.CV', 'cs.IT', 'math.DS', 'math.IT', 'math.ST', 'stat.TH']",2016-10-20 15:37:53+00:00
http://arxiv.org/abs/1610.06454v2,Reasoning with Memory Augmented Neural Networks for Language Comprehension,"Hypothesis testing is an important cognitive process that supports human
reasoning. In this paper, we introduce a computational hypothesis testing
approach based on memory augmented neural networks. Our approach involves a
hypothesis testing loop that reconsiders and progressively refines a previously
formed hypothesis in order to generate new hypotheses to test. We apply the
proposed approach to language comprehension task by using Neural Semantic
Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an
absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by
single and ensemble systems on standard machine comprehension benchmarks such
as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.","['Tsendsuren Munkhdalai', 'Hong Yu']","['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']",2016-10-20 15:17:04+00:00
http://arxiv.org/abs/1610.06453v1,Change-point Detection Methods for Body-Worn Video,"Body-worn video (BWV) cameras are increasingly utilized by police departments
to provide a record of police-public interactions. However, large-scale BWV
deployment produces terabytes of data per week, necessitating the development
of effective computational methods to identify salient changes in video. In
work carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel
two-stage framework for video change-point detection. First, we employ
state-of-the-art machine learning methods including convolutional neural
networks and support vector machines for scene classification. We then develop
and compare change-point detection algorithms utilizing mean squared-error
minimization, forecasting methods, hidden Markov models, and maximum likelihood
estimation to identify noteworthy changes. We test our framework on detection
of vehicle exits and entrances in a BWV data set provided by the Los Angeles
Police Department and achieve over 90% recall and nearly 70% precision --
demonstrating robustness to rapid scene changes, extreme luminance differences,
and frequent camera occlusions.","['Stephanie Allen', 'David Madras', 'Ye Ye', 'Greg Zanotti']","['cs.CV', 'cs.LG', 'stat.ML']",2016-10-20 15:11:42+00:00
http://arxiv.org/abs/1610.06447v4,Regularized Optimal Transport and the Rot Mover's Distance,"This paper presents a unified framework for smooth convex regularization of
discrete optimal transport problems. In this context, the regularized optimal
transport turns out to be equivalent to a matrix nearness problem with respect
to Bregman divergences. Our framework thus naturally generalizes a previously
proposed regularization based on the Boltzmann-Shannon entropy related to the
Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We
call the regularized optimal transport distance the rot mover's distance in
reference to the classical earth mover's distance. We develop two generic
schemes that we respectively call the alternate scaling algorithm and the
non-negative alternate scaling algorithm, to compute efficiently the
regularized optimal plans depending on whether the domain of the regularizer
lies within the non-negative orthant or not. These schemes are based on
Dykstra's algorithm with alternate Bregman projections, and further exploit the
Newton-Raphson method when applied to separable divergences. We enhance the
separable case with a sparse extension to deal with high data dimensions. We
also instantiate our proposed framework and discuss the inherent specificities
for well-known regularizers and statistical divergences in the machine learning
and information geometry communities. Finally, we demonstrate the merits of our
methods with experiments using synthetic data to illustrate the effect of
different regularizers and penalties on the solutions, as well as real-world
data for a pattern recognition application to audio scene classification.","['Arnaud Dessein', 'Nicolas Papadakis', 'Jean-Luc Rouas']","['stat.ML', 'cs.LG']",2016-10-20 14:49:36+00:00
http://arxiv.org/abs/1610.06434v1,Kernel Alignment for Unsupervised Transfer Learning,"The ability of a human being to extrapolate previously gained knowledge to
other domains inspired a new family of methods in machine learning called
transfer learning. Transfer learning is often based on the assumption that
objects in both target and source domains share some common feature and/or data
space. In this paper, we propose a simple and intuitive approach that minimizes
iteratively the distance between source and target task distributions by
optimizing the kernel target alignment (KTA). We show that this procedure is
suitable for transfer learning by relating it to Hilbert-Schmidt Independence
Criterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run
our method on benchmark computer vision data sets and show that it can
outperform some state-of-art methods.","['Ievgen Redko', 'Younès Bennani']","['stat.ML', 'cs.LG']",2016-10-20 14:37:46+00:00
http://arxiv.org/abs/1610.06258v3,Using Fast Weights to Attend to the Recent Past,"Until recently, research on artificial neural networks was largely restricted
to systems with only two types of variable: Neural activities that represent
the current or recent input and weights that learn to capture regularities
among inputs, outputs and payoffs. There is no good reason for this
restriction. Synapses have dynamics at many different time-scales and this
suggests that artificial neural networks might benefit from variables that
change slower than activities but much faster than the standard weights. These
""fast weights"" can be used to store temporary memories of the recent past and
they provide a neurally plausible way of implementing the type of attention to
the past that has recently proved very helpful in sequence-to-sequence models.
By using fast weights we can avoid the need to store copies of neural activity
patterns.","['Jimmy Ba', 'Geoffrey Hinton', 'Volodymyr Mnih', 'Joel Z. Leibo', 'Catalin Ionescu']","['stat.ML', 'cs.LG', 'cs.NE']",2016-10-20 01:03:20+00:00
http://arxiv.org/abs/1610.06235v1,Enhancing ICA Performance by Exploiting Sparsity: Application to FMRI Analysis,"Independent component analysis (ICA) is a powerful method for blind source
separation based on the assumption that sources are statistically independent.
Though ICA has proven useful and has been employed in many applications,
complete statistical independence can be too restrictive an assumption in
practice. Additionally, important prior information about the data, such as
sparsity, is usually available. Sparsity is a natural property of the data, a
form of diversity, which, if incorporated into the ICA model, can relax the
independence assumption, resulting in an improvement in the overall separation
performance. In this work, we propose a new variant of ICA by entropy bound
minimization (ICA-EBM)-a flexible, yet parameter-free algorithm-through the
direct exploitation of sparsity. Using this new SparseICA-EBM algorithm, we
study the synergy of independence and sparsity through simulations on synthetic
as well as functional magnetic resonance imaging (fMRI)-like data.","['Zois Boukouvalas', 'Yuri Levin-Schwartz', 'Tulay Adali']",['stat.ML'],2016-10-19 21:53:07+00:00
http://arxiv.org/abs/1610.06194v3,Robust and Parallel Bayesian Model Selection,"Effective and accurate model selection is an important problem in modern data
analysis. One of the major challenges is the computational burden required to
handle large data sets that cannot be stored or processed on one machine.
Another challenge one may encounter is the presence of outliers and
contaminations that damage the inference quality. The parallel ""divide and
conquer"" model selection strategy divides the observations of the full data set
into roughly equal subsets and perform inference and model selection
independently on each subset. After local subset inference, this method
aggregates the posterior model probabilities or other model/variable selection
criteria to obtain a final model by using the notion of geometric median. This
approach leads to improved concentration in finding the ""correct"" model and
model parameters and also is provably robust to outliers and data
contamination.","['Michael Minyi Zhang', 'Henry Lam', 'Lizhen Lin']",['stat.ML'],2016-10-19 20:09:51+00:00
http://arxiv.org/abs/1610.06145v2,A global optimization algorithm for sparse mixed membership matrix factorization,"Mixed membership factorization is a popular approach for analyzing data sets
that have within-sample heterogeneity. In recent years, several algorithms have
been developed for mixed membership matrix factorization, but they only
guarantee estimates from a local optimum. Here, we derive a global optimization
(GOP) algorithm that provides a guaranteed $\epsilon$-global optimum for a
sparse mixed membership matrix factorization problem. We test the algorithm on
simulated data and find the algorithm always bounds the global optimum across
random initializations and explores multiple modes efficiently.","['Fan Zhang', 'Chuangqi Wang', 'Andrew Trapp', 'Patrick Flaherty']","['stat.ME', 'math.OC', 'stat.ML']",2016-10-19 18:39:41+00:00
http://arxiv.org/abs/1610.06072v1,Learning to Learn Neural Networks,"Meta-learning consists in learning learning algorithms. We use a Long Short
Term Memory (LSTM) based network to learn to compute on-line updates of the
parameters of another neural network. These parameters are stored in the cell
state of the LSTM. Our framework allows to compare learned algorithms to
hand-made algorithms within the traditional train and test methodology. In an
experiment, we learn a learning algorithm for a one-hidden layer Multi-Layer
Perceptron (MLP) on non-linearly separable datasets. The learned algorithm is
able to update parameters of both layers and generalise well on similar
datasets.",['Tom Bosc'],"['cs.LG', 'stat.ML']",2016-10-19 15:46:30+00:00
http://arxiv.org/abs/1610.05956v1,Clustering by connection center evolution,"The determination of cluster centers generally depends on the scale that we
use to analyze the data to be clustered. Inappropriate scale usually leads to
unreasonable cluster centers and thus unreasonable results. In this study, we
first consider the similarity of elements in the data as the connectivity of
nodes in an undirected graph, then present the concept of a connection center
and regard it as the cluster center of the data. Based on this definition, the
determination of cluster centers and the assignment of class are very simple,
natural and effective. One more crucial finding is that the cluster centers of
different scales can be obtained easily by the different powers of a similarity
matrix and the change of power from small to large leads to the dynamic
evolution of cluster centers from local (microscopic) to global (microscopic).
Further, in this process of evolution, the number of categories changes
discontinuously, which means that the presented method can automatically skip
the unreasonable number of clusters, suggest appropriate observation scales and
provide corresponding cluster results.","['Xiurui Geng', 'Hairong Tang']",['stat.ML'],2016-10-19 10:52:07+00:00
http://arxiv.org/abs/1610.05950v1,Consistent Kernel Mean Estimation for Functions of Random Variables,"We provide a theoretical foundation for non-parametric estimation of
functions of random variables using kernel mean embeddings. We show that for
any continuous function $f$, consistent estimators of the mean embedding of a
random variable $X$ lead to consistent estimators of the mean embedding of
$f(X)$. For Mat\'ern kernels and sufficiently smooth functions we also provide
rates of convergence. Our results extend to functions of multiple random
variables. If the variables are dependent, we require an estimator of the mean
embedding of their joint distribution as a starting point; if they are
independent, it is sufficient to have separate estimators of the mean
embeddings of their marginal distributions. In either case, our results cover
both mean embeddings based on i.i.d. samples as well as ""reduced set""
expansions in terms of dependent expansion points. The latter serves as a
justification for using such expansions to limit memory resources when applying
the approach as a basis for probabilistic programming.","['Carl-Johann Simon-Gabriel', 'Adam Ścibior', 'Ilya Tolstikhin', 'Bernhard Schölkopf']",['stat.ML'],2016-10-19 10:23:55+00:00
http://arxiv.org/abs/1610.05925v1,Learning Determinantal Point Processes in Sublinear Time,"We propose a new class of determinantal point processes (DPPs) which can be
manipulated for inference and parameter learning in potentially sublinear time
in the number of items. This class, based on a specific low-rank factorization
of the marginal kernel, is particularly suited to a subclass of continuous DPPs
and DPPs defined on exponentially many items. We apply this new class to
modelling text documents as sampling a DPP of sentences, and propose a
conditional maximum likelihood formulation to model topic proportions, which is
made possible with no approximation for our class of DPPs. We present an
application to document summarization with a DPP on $2^{500}$ items.","['Christophe Dupuy', 'Francis Bach']","['stat.ML', 'cs.LG']",2016-10-19 09:18:10+00:00
http://arxiv.org/abs/1610.05872v1,Making brain-machine interfaces robust to future neural variability,"A major hurdle to clinical translation of brain-machine interfaces (BMIs) is
that current decoders, which are trained from a small quantity of recent data,
become ineffective when neural recording conditions subsequently change. We
tested whether a decoder could be made more robust to future neural variability
by training it to handle a variety of recording conditions sampled from months
of previously collected data as well as synthetic training data perturbations.
We developed a new multiplicative recurrent neural network BMI decoder that
successfully learned a large variety of neural-to- kinematic mappings and
became more robust with larger training datasets. When tested with a non-human
primate preclinical BMI model, this decoder was robust under conditions that
disabled a state-of-the-art Kalman filter based decoder. These results validate
a new BMI strategy in which accumulated data history is effectively harnessed,
and may facilitate reliable daily BMI use by reducing decoder retraining
downtime.","['David Sussillo', 'Sergey D. Stavisky', 'Jonathan C. Kao', 'Stephen I. Ryu', 'Krishna V. Shenoy']","['q-bio.NC', 'stat.ML']",2016-10-19 05:32:32+00:00
http://arxiv.org/abs/1610.06848v3,An Efficient Minibatch Acceptance Test for Metropolis-Hastings,"We present a novel Metropolis-Hastings method for large datasets that uses
small expected-size minibatches of data. Previous work on reducing the cost of
Metropolis-Hastings tests yield variable data consumed per sample, with only
constant factor reductions versus using the full dataset for each sample. Here
we present a method that can be tuned to provide arbitrarily small batch sizes,
by adjusting either proposal step size or temperature. Our test uses the
noise-tolerant Barker acceptance test with a novel additive correction
variable. The resulting test has similar cost to a normal SGD update. Our
experiments demonstrate several order-of-magnitude speedups over previous work.","['Daniel Seita', 'Xinlei Pan', 'Haoyu Chen', 'John Canny']","['cs.LG', 'stat.ML']",2016-10-19 00:19:25+00:00
http://arxiv.org/abs/1610.05820v2,Membership Inference Attacks against Machine Learning Models,"We quantitatively investigate how machine learning models leak information
about the individual data records on which they were trained. We focus on the
basic membership inference attack: given a data record and black-box access to
a model, determine if the record was in the model's training dataset. To
perform membership inference against a target model, we make adversarial use of
machine learning and train our own inference model to recognize differences in
the target model's predictions on the inputs that it trained on versus the
inputs that it did not train on.
  We empirically evaluate our inference techniques on classification models
trained by commercial ""machine learning as a service"" providers such as Google
and Amazon. Using realistic datasets and classification tasks, including a
hospital discharge dataset whose membership is sensitive from the privacy
perspective, we show that these models can be vulnerable to membership
inference attacks. We then investigate the factors that influence this leakage
and evaluate mitigation strategies.","['Reza Shokri', 'Marco Stronati', 'Congzheng Song', 'Vitaly Shmatikov']","['cs.CR', 'cs.LG', 'stat.ML']",2016-10-18 22:38:33+00:00
http://arxiv.org/abs/1610.05792v4,Big Batch SGD: Automated Inference using Adaptive Batch Sizes,"Classical stochastic gradient methods for optimization rely on noisy gradient
approximations that become progressively less accurate as iterates approach a
solution. The large noise and small signal in the resulting gradients makes it
difficult to use them for adaptive stepsize selection and automatic stopping.
We propose alternative ""big batch"" SGD schemes that adaptively grow the batch
size over time to maintain a nearly constant signal-to-noise ratio in the
gradient approximation. The resulting methods have similar convergence rates to
classical SGD, and do not require convexity of the objective. The high fidelity
gradients enable automated learning rate selection and do not require stepsize
decay. Big batch methods are thus easily automated and can run with little or
no oversight.","['Soham De', 'Abhay Yadav', 'David Jacobs', 'Tom Goldstein']","['cs.LG', 'math.NA', 'math.OC', 'stat.ML']",2016-10-18 20:24:10+00:00
http://arxiv.org/abs/1610.05775v1,Modeling the Dynamics of Online Learning Activity,"People are increasingly relying on the Web and social media to find solutions
to their problems in a wide range of domains. In this online setting, closely
related problems often lead to the same characteristic learning pattern, in
which people sharing these problems visit related pieces of information,
perform almost identical queries or, more generally, take a series of similar
actions. In this paper, we introduce a novel modeling framework for clustering
continuous-time grouped streaming data, the hierarchical Dirichlet Hawkes
process (HDHP), which allows us to automatically uncover a wide variety of
learning patterns from detailed traces of learning activity. Our model allows
for efficient inference, scaling to millions of actions taken by thousands of
users. Experiments on real data gathered from Stack Overflow reveal that our
framework can recover meaningful learning patterns in terms of both content and
temporal dynamics, as well as accurately track users' interests and goals over
time.","['Charalampos Mavroforakis', 'Isabel Valera', 'Manuel Gomez Rodriguez']","['stat.ML', 'cs.LG', 'cs.SI']",2016-10-18 20:00:09+00:00
http://arxiv.org/abs/1610.05773v1,RedQueen: An Online Algorithm for Smart Broadcasting in Social Networks,"Users in social networks whose posts stay at the top of their followers'{}
feeds the longest time are more likely to be noticed. Can we design an online
algorithm to help them decide when to post to stay at the top? In this paper,
we address this question as a novel optimal control problem for jump stochastic
differential equations. For a wide variety of feed dynamics, we show that the
optimal broadcasting intensity for any user is surprisingly simple -- it is
given by the position of her most recent post on each of her follower's feeds.
As a consequence, we are able to develop a simple and highly efficient online
algorithm, RedQueen, to sample the optimal times for the user to post.
Experiments on both synthetic and real data gathered from Twitter show that our
algorithm is able to consistently make a user's posts more visible over time,
is robust to volume changes on her followers' feeds, and significantly
outperforms the state of the art.","['Ali Zarezade', 'Utkarsh Upadhyay', 'Hamid Rabiee', 'Manuel Gomez Rodriguez']","['stat.ML', 'cs.DS', 'cs.LG', 'cs.SI']",2016-10-18 20:00:05+00:00
http://arxiv.org/abs/1610.05756v2,Modeling community structure and topics in dynamic text networks,"The last decade has seen great progress in both dynamic network modeling and
topic modeling. This paper draws upon both areas to create a Bayesian method
that allows topic discovery to inform the latent network model and the network
structure to facilitate topic identification. We apply this method to the 467
top political blogs of 2012. Our results find complex community structure
within this set of blogs, where community membership depends strongly upon the
set of topics in which the blogger is interested.","['Teague Henry', 'David Banks', 'Christine Chai', 'Derek Owens-Oas']","['cs.SI', 'physics.soc-ph', 'stat.ML']",2016-10-18 19:39:44+00:00
http://arxiv.org/abs/1610.05755v4,Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data,"Some machine learning applications involve training data that is sensitive,
such as the medical histories of patients in a clinical trial. A model may
inadvertently and implicitly store some of its training data; careful analysis
of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to
providing strong privacy guarantees for training data: Private Aggregation of
Teacher Ensembles (PATE). The approach combines, in a black-box fashion,
multiple models trained with disjoint datasets, such as records from different
subsets of users. Because they rely directly on sensitive data, these models
are not published, but instead used as ""teachers"" for a ""student"" model. The
student learns to predict an output chosen by noisy voting among all of the
teachers, and cannot directly access an individual teacher or the underlying
data or parameters. The student's privacy properties can be understood both
intuitively (since no single teacher and thus no single dataset dictates the
student's training) and formally, in terms of differential privacy. These
properties hold even if an adversary can not only query the student but also
inspect its internal workings.
  Compared with previous work, the approach imposes only weak assumptions on
how teachers are trained: it applies to any model, including non-convex models
like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and
SVHN thanks to an improved privacy analysis and semi-supervised learning.","['Nicolas Papernot', 'Martín Abadi', 'Úlfar Erlingsson', 'Ian Goodfellow', 'Kunal Talwar']","['stat.ML', 'cs.CR', 'cs.LG']",2016-10-18 19:37:37+00:00
http://arxiv.org/abs/1610.05735v1,Deep Amortized Inference for Probabilistic Programs,"Probabilistic programming languages (PPLs) are a powerful modeling tool, able
to represent any computable probability distribution. Unfortunately,
probabilistic program inference is often intractable, and existing PPLs mostly
rely on expensive, approximate sampling-based methods. To alleviate this
problem, one could try to learn from past inferences, so that future inferences
run faster. This strategy is known as amortized inference; it has recently been
applied to Bayesian networks and deep generative models. This paper proposes a
system for amortized inference in PPLs. In our system, amortization comes in
the form of a parameterized guide program. Guide programs have similar
structure to the original program, but can have richer data flow, including
neural network components. These networks can be optimized so that the guide
approximately samples from the posterior distribution defined by the original
program. We present a flexible interface for defining guide programs and a
stochastic gradient-based scheme for optimizing guide parameters, as well as
some preliminary results on automatically deriving guide programs. We explore
in detail the common machine learning pattern in which a 'local' model is
specified by 'global' random values and used to generate independent observed
data points; this gives rise to amortized local inference supporting global
model learning.","['Daniel Ritchie', 'Paul Horsfall', 'Noah D. Goodman']","['cs.AI', 'cs.LG', 'stat.ML']",2016-10-18 18:35:09+00:00
http://arxiv.org/abs/1610.05683v3,Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms,"Variational inference using the reparameterization trick has enabled
large-scale approximate Bayesian inference in complex probabilistic models,
leveraging stochastic optimization to sidestep intractable expectations. The
reparameterization trick is applicable when we can simulate a random variable
by applying a differentiable deterministic function on an auxiliary random
variable whose distribution is fixed. For many distributions of interest (such
as the gamma or Dirichlet), simulation of random variables relies on
acceptance-rejection sampling. The discontinuity introduced by the
accept-reject step means that standard reparameterization tricks are not
applicable. We propose a new method that lets us leverage reparameterization
gradients even when variables are outputs of a acceptance-rejection sampling
algorithm. Our approach enables reparameterization on a larger class of
variational distributions. In several studies of real and synthetic data, we
show that the variance of the estimator of the gradient is significantly lower
than other state-of-the-art methods. This leads to faster convergence of
stochastic gradient variational inference.","['Christian A. Naesseth', 'Francisco J. R. Ruiz', 'Scott W. Linderman', 'David M. Blei']","['stat.ML', 'stat.ME']",2016-10-18 15:55:08+00:00
http://arxiv.org/abs/1610.05604v3,Dynamic Assortment Personalization in High Dimensions,"We study the problem of dynamic assortment personalization with large,
heterogeneous populations and wide arrays of products, and demonstrate the
importance of structural priors for effective, efficient large-scale
personalization. Assortment personalization is the problem of choosing, for
each individual (type), a best assortment of products, ads, or other offerings
(items) so as to maximize revenue. This problem is central to revenue
management in e-commerce and online advertising where both items and types can
number in the millions.
  We formulate the dynamic assortment personalization problem as a
discrete-contextual bandit with $m$ contexts (types) and exponentially many
arms (assortments of the $n$ items). We assume that each type's preferences
follow a simple parametric model with $n$ parameters. In all, there are $mn$
parameters, and existing literature suggests that order optimal regret scales
as $mn$. However, the data required to estimate so many parameters is orders of
magnitude larger than the data available in most revenue management
applications; and the optimal regret under these models is unacceptably high.
  In this paper, we impose a natural structure on the problem -- a small latent
dimension, or low rank. In the static setting, we show that this model can be
efficiently learned from surprisingly few interactions, using a time- and
memory-efficient optimization algorithm that converges globally whenever the
model is learnable. In the dynamic setting, we show that structure-aware
dynamic assortment personalization can have regret that is an order of
magnitude smaller than structure-ignorant approaches. We validate our
theoretical results empirically.","['Nathan Kallus', 'Madeleine Udell']","['stat.ML', 'math.OC', 'stat.ME']",2016-10-18 13:32:36+00:00
http://arxiv.org/abs/1610.05507v1,Analysis and Implementation of an Asynchronous Optimization Algorithm for the Parameter Server,"This paper presents an asynchronous incremental aggregated gradient algorithm
and its implementation in a parameter server framework for solving regularized
optimization problems. The algorithm can handle both general convex (possibly
non-smooth) regularizers and general convex constraints. When the empirical
data loss is strongly convex, we establish linear convergence rate, give
explicit expressions for step-size choices that guarantee convergence to the
optimum, and bound the associated convergence factors. The expressions have an
explicit dependence on the degree of asynchrony and recover classical results
under synchronous operation. Simulations and implementations on commercial
compute clouds validate our findings.","['Arda Aytekin', 'Hamid Reza Feyzmahdavian', 'Mikael Johansson']","['math.OC', 'cs.DC', 'cs.LG', 'stat.ML']",2016-10-18 09:48:51+00:00
http://arxiv.org/abs/1610.05448v1,Generalization error minimization: a new approach to model evaluation and selection with an application to penalized regression,"We study model evaluation and model selection from the perspective of
generalization ability (GA): the ability of a model to predict outcomes in new
samples from the same population. We believe that GA is one way formally to
address concerns about the external validity of a model. The GA of a model
estimated on a sample can be measured by its empirical out-of-sample errors,
called the generalization errors (GE). We derive upper bounds for the GE, which
depend on sample sizes, model complexity and the distribution of the loss
function. The upper bounds can be used to evaluate the GA of a model, ex ante.
We propose using generalization error minimization (GEM) as a framework for
model selection. Using GEM, we are able to unify a big class of penalized
regression estimators, including lasso, ridge and bridge, under the same set of
assumptions. We establish finite-sample and asymptotic properties (including
$\mathcal{L}_2$-consistency) of the GEM estimator for both the $n \geqslant p$
and the $n < p$ cases. We also derive the $\mathcal{L}_2$-distance between the
penalized and corresponding unpenalized regression estimates. In practice, GEM
can be implemented by validation or cross-validation. We show that the GE
bounds can be used for selecting the optimal number of folds in $K$-fold
cross-validation. We propose a variant of $R^2$, the $GR^2$, as a measure of
GA, which considers both both in-sample and out-of-sample goodness of fit.
Simulations are used to demonstrate our key results.","['Ning Xu', 'Jian Hong', 'Timothy C. G. Fisher']","['stat.ML', 'math.ST', 'q-fin.EC', 'stat.TH']",2016-10-18 06:26:47+00:00
http://arxiv.org/abs/1610.05400v2,Going off the Grid: Iterative Model Selection for Biclustered Matrix Completion,"We consider the problem of performing matrix completion with side information
on row-by-row and column-by-column similarities. We build upon recent proposals
for matrix estimation with smoothness constraints with respect to row and
column graphs. We present a novel iterative procedure for directly minimizing
an information criterion in order to select an appropriate amount row and
column smoothing, namely perform model selection. We also discuss how to
exploit the special structure of the problem to scale up the estimation and
model selection procedure via the Hutchinson estimator. We present simulation
results and an application to predicting associations in imaging-genomics
studies.","['Eric Chi', 'Liuiyi Hu', 'Arvind K. Saibaba', 'Arvind U. K. Rao']","['stat.CO', 'stat.ML']",2016-10-18 01:50:52+00:00
http://arxiv.org/abs/1610.05392v3,AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models,"We investigate the capabilities and limitations of Gaussian process models by
jointly exploring three complementary directions: (i) scalable and
statistically efficient inference; (ii) flexible kernels; and (iii) objective
functions for hyperparameter learning alternative to the marginal likelihood.
Our approach outperforms all previously reported GP methods on the standard
MNIST dataset; performs comparatively to previous kernel-based methods using
the RECTANGLES-IMAGE dataset; and breaks the 1% error-rate barrier in GP models
using the MNIST8M dataset, showing along the way the scalability of our method
at unprecedented scale for GP models (8 million observations) in classification
problems. Overall, our approach represents a significant breakthrough in kernel
methods and GP models, bridging the gap between deep learning approaches and
kernel machines.","['Karl Krauth', 'Edwin V. Bonilla', 'Kurt Cutajar', 'Maurizio Filippone']",['stat.ML'],2016-10-18 01:09:19+00:00
http://arxiv.org/abs/1610.05350v1,How Well Do Local Algorithms Solve Semidefinite Programs?,"Several probabilistic models from high-dimensional statistics and machine
learning reveal an intriguing --and yet poorly understood-- dichotomy. Either
simple local algorithms succeed in estimating the object of interest, or even
sophisticated semi-definite programming (SDP) relaxations fail.
  In order to explore this phenomenon, we study a classical SDP relaxation of
the minimum graph bisection problem, when applied to Erd\H{o}s-Renyi random
graphs with bounded average degree $d>1$, and obtain several types of results.
First, we use a dual witness construction (using the so-called non-backtracking
matrix of the graph) to upper bound the SDP value. Second, we prove that a
simple local algorithm approximately solves the SDP to within a factor
$2d^2/(2d^2+d-1)$ of the upper bound. In particular, the local algorithm is at
most $8/9$ suboptimal, and $1+O(1/d)$ suboptimal for large degree.
  We then analyze a more sophisticated local algorithm, which aggregates
information according to the harmonic measure on the limiting Galton-Watson
(GW) tree. The resulting lower bound is expressed in terms of the conductance
of the GW tree and matches surprisingly well the empirically determined SDP
values on large-scale Erd\H{o}s-Renyi graphs.
  We finally consider the planted partition model. In this case, purely local
algorithms are known to fail, but they do succeed if a small amount of side
information is available. Our results imply quantitative bounds on the
threshold for partial recovery using SDP in this model.","['Zhou Fan', 'Andrea Montanari']","['cs.DM', 'math.OC', 'stat.ML']",2016-10-17 20:45:11+00:00
http://arxiv.org/abs/1610.05275v1,A Unified Computational and Statistical Framework for Nonconvex Low-Rank Matrix Estimation,"We propose a unified framework for estimating low-rank matrices through
nonconvex optimization based on gradient descent algorithm. Our framework is
quite general and can be applied to both noisy and noiseless observations. In
the general case with noisy observations, we show that our algorithm is
guaranteed to linearly converge to the unknown low-rank matrix up to minimax
optimal statistical error, provided an appropriate initial estimator. While in
the generic noiseless setting, our algorithm converges to the unknown low-rank
matrix at a linear rate and enables exact recovery with optimal sample
complexity. In addition, we develop a new initialization algorithm to provide a
desired initial estimator, which outperforms existing initialization algorithms
for nonconvex low-rank matrix estimation. We illustrate the superiority of our
framework through three examples: matrix regression, matrix completion, and
one-bit matrix completion. We also corroborate our theory through extensive
experiments on synthetic data.","['Lingxiao Wang', 'Xiao Zhang', 'Quanquan Gu']",['stat.ML'],2016-10-17 19:16:39+00:00
http://arxiv.org/abs/1610.05261v3,A probabilistic model for the numerical solution of initial value problems,"Like many numerical methods, solvers for initial value problems (IVPs) on
ordinary differential equations estimate an analytically intractable quantity,
using the results of tractable computations as inputs. This structure is
closely connected to the notion of inference on latent variables in statistics.
We describe a class of algorithms that formulate the solution to an IVP as
inference on a latent path that is a draw from a Gaussian process probability
measure (or equivalently, the solution of a linear stochastic differential
equation). We then show that certain members of this class are connected
precisely to generalized linear methods for ODEs, a number of Runge--Kutta
methods, and Nordsieck methods. This probabilistic formulation of classic
methods is valuable in two ways: analytically, it highlights implicit prior
assumptions favoring certain approximate solutions to the IVP over others, and
gives a precise meaning to the old observation that these methods act like
filters. Practically, it endows the classic solvers with `docking points' for
notions of uncertainty and prior information about the initial value, the value
of the ODE itself, and the solution of the problem.","['Michael Schober', 'Simo Särkkä', 'Philipp Hennig']","['math.NA', 'cs.LG', 'stat.ML']",2016-10-17 18:50:35+00:00
http://arxiv.org/abs/1610.05247v1,Black-box Importance Sampling,"Importance sampling is widely used in machine learning and statistics, but
its power is limited by the restriction of using simple proposals for which the
importance weights can be tractably calculated. We address this problem by
studying black-box importance sampling methods that calculate importance
weights for samples generated from any unknown proposal or black-box mechanism.
Our method allows us to use better and richer proposals to solve difficult
problems, and (somewhat counter-intuitively) also has the additional benefit of
improving the estimation accuracy beyond typical importance sampling. Both
theoretical and empirical analyses are provided.","['Qiang Liu', 'Jason D. Lee']",['stat.ML'],2016-10-17 18:24:30+00:00
http://arxiv.org/abs/1610.05246v7,BET on Independence,"We study the problem of nonparametric dependence detection. Many existing
methods may suffer severe power loss due to non-uniform consistency, which we
illustrate with a paradox. To avoid such power loss, we approach the
nonparametric test of independence through the new framework of binary
expansion statistics (BEStat) and binary expansion testing (BET), which examine
dependence through a novel binary expansion filtration approximation of the
copula. Through a Hadamard transform, we find that the symmetry statistics in
the filtration are complete sufficient statistics for dependence. These
statistics are also uncorrelated under the null. By utilizing symmetry
statistics, the BET avoids the problem of non-uniform consistency and improves
upon a wide class of commonly used methods (a) by achieving the minimax rate in
sample size requirement for reliable power and (b) by providing clear
interpretations of global relationships upon rejection of independence. The
binary expansion approach also connects the symmetry statistics with the
current computing system to facilitate efficient bitwise implementation. We
illustrate the BET with a study of the distribution of stars in the night sky
and with an exploratory data analysis of the TCGA breast cancer data.",['Kai Zhang'],"['math.ST', 'cs.LG', 'stat.CO', 'stat.ME', 'stat.ML', 'stat.TH']",2016-10-17 18:19:49+00:00
http://arxiv.org/abs/1610.05214v2,A polynomial-time relaxation of the Gromov-Hausdorff distance,"The Gromov-Hausdorff distance provides a metric on the set of isometry
classes of compact metric spaces. Unfortunately, computing this metric directly
is believed to be computationally intractable. Motivated by applications in
shape matching and point-cloud comparison, we study a semidefinite programming
relaxation of the Gromov-Hausdorff metric. This relaxation can be computed in
polynomial time, and somewhat surprisingly is itself a pseudometric. We
describe the induced topology on the set of compact metric spaces. Finally, we
demonstrate the numerical performance of various algorithms for computing the
relaxed distance and apply these algorithms to several relevant data sets. In
particular we propose a greedy algorithm for finding the best correspondence
between finite metric spaces that can handle hundreds of points.","['Soledad Villar', 'Afonso S. Bandeira', 'Andrew J. Blumberg', 'Rachel Ward']","['math.GT', 'cs.CG', 'math.OC', 'stat.ML']",2016-10-17 17:18:45+00:00
http://arxiv.org/abs/1610.05202v2,Decentralized Collaborative Learning of Personalized Models over Networks,"We consider a set of learning agents in a collaborative peer-to-peer network,
where each agent learns a personalized model according to its own learning
objective. The question addressed in this paper is: how can agents improve upon
their locally trained model by communicating with other agents that have
similar objectives? We introduce and analyze two asynchronous gossip algorithms
running in a fully decentralized manner. Our first approach, inspired from
label propagation, aims to smooth pre-trained local models over the network
while accounting for the confidence that each agent has in its initial model.
In our second approach, agents jointly learn and propagate their model by
making iterative updates based on both their local dataset and the behavior of
their neighbors. To optimize this challenging objective, our decentralized
algorithm is based on ADMM.","['Paul Vanhaesebrouck', 'Aurélien Bellet', 'Marc Tommasi']","['cs.LG', 'cs.AI', 'cs.DC', 'cs.SY', 'stat.ML']",2016-10-17 16:51:49+00:00
http://arxiv.org/abs/1610.05163v1,Spatio-temporal Gaussian processes modeling of dynamical systems in systems biology,"Quantitative modeling of post-transcriptional regulation process is a
challenging problem in systems biology. A mechanical model of the regulatory
process needs to be able to describe the available spatio-temporal protein
concentration and mRNA expression data and recover the continuous
spatio-temporal fields. Rigorous methods are required to identify model
parameters. A promising approach to deal with these difficulties is proposed
using Gaussian process as a prior distribution over the latent function of
protein concentration and mRNA expression. In this study, we consider a partial
differential equation mechanical model with differential operators and latent
function. Since the operators at stake are linear, the information from the
physical model can be encoded into the kernel function. Hybrid Monte Carlo
methods are employed to carry out Bayesian inference of the partial
differential equation parameters and Gaussian process kernel parameters. The
spatio-temporal field of protein concentration and mRNA expression are
reconstructed without explicitly solving the partial differential equation.","['Mu Niu', 'Zhenwen Dai', 'Neil Lawrence', 'Kolja Becker']",['stat.ML'],2016-10-17 15:25:56+00:00
http://arxiv.org/abs/1610.05160v1,The Peaking Phenomenon in Semi-supervised Learning,"For the supervised least squares classifier, when the number of training
objects is smaller than the dimensionality of the data, adding more data to the
training set may first increase the error rate before decreasing it. This,
possibly counterintuitive, phenomenon is known as peaking. In this work, we
observe that a similar but more pronounced version of this phenomenon also
occurs in the semi-supervised setting, where instead of labeled objects,
unlabeled objects are added to the training set. We explain why the learning
curve has a more steep incline and a more gradual decline in this setting
through simulation studies and by applying an approximation of the learning
curve based on the work by Raudys & Duin.","['Jesse H. Krijthe', 'Marco Loog']","['stat.ML', 'cs.LG']",2016-10-17 15:22:43+00:00
http://arxiv.org/abs/1610.05129v1,Risk-Aware Algorithms for Adversarial Contextual Bandits,"In this work we consider adversarial contextual bandits with risk
constraints. At each round, nature prepares a context, a cost for each arm, and
additionally a risk for each arm. The learner leverages the context to pull an
arm and then receives the corresponding cost and risk associated with the
pulled arm. In addition to minimizing the cumulative cost, the learner also
needs to satisfy long-term risk constraints -- the average of the cumulative
risk from all pulled arms should not be larger than a pre-defined threshold. To
address this problem, we first study the full information setting where in each
round the learner receives an adversarial convex loss and a convex constraint.
We develop a meta algorithm leveraging online mirror descent for the full
information setting and extend it to contextual bandit with risk constraints
setting using expert advice. Our algorithms can achieve near-optimal regret in
terms of minimizing the total cost, while successfully maintaining a sublinear
growth of cumulative risk constraint violation.","['Wen Sun', 'Debadeepta Dey', 'Ashish Kapoor']","['cs.LG', 'stat.ML']",2016-10-17 14:14:43+00:00
http://arxiv.org/abs/1610.05108v4,The xyz algorithm for fast interaction search in high-dimensional data,"When performing regression on a dataset with $p$ variables, it is often of
interest to go beyond using main linear effects and include interactions as
products between individual variables. For small-scale problems, these
interactions can be computed explicitly but this leads to a computational
complexity of at least $\mathcal{O}(p^2)$ if done naively. This cost can be
prohibitive if $p$ is very large. We introduce a new randomised algorithm that
is able to discover interactions with high probability and under mild
conditions has a runtime that is subquadratic in $p$. We show that strong
interactions can be discovered in almost linear time, whilst finding weaker
interactions requires $\mathcal{O}(p^\alpha)$ operations for $1 < \alpha < 2$
depending on their strength. The underlying idea is to transform interaction
search into a closestpair problem which can be solved efficiently in
subquadratic time. The algorithm is called $\mathit{xyz}$ and is implemented in
the language R. We demonstrate its efficiency for application to genome-wide
association studies, where more than $10^{11}$ interactions can be screened in
under $280$ seconds with a single-core $1.2$ GHz CPU.","['Gian-Andrea Thanei', 'Nicolai Meinshausen', 'Rajen D. Shah']","['stat.ML', 'stat.CO', '62-04']",2016-10-17 13:42:22+00:00
http://arxiv.org/abs/1610.05083v3,Efficient Metric Learning for the Analysis of Motion Data,"We investigate metric learning in the context of dynamic time warping (DTW),
the by far most popular dissimilarity measure used for the comparison and
analysis of motion capture data. While metric learning enables a
problem-adapted representation of data, the majority of methods has been
proposed for vectorial data only. In this contribution, we extend the popular
principle offered by the large margin nearest neighbors learner (LMNN) to DTW
by treating the resulting component-wise dissimilarity values as features. We
demonstrate that this principle greatly enhances the classification accuracy in
several benchmarks. Further, we show that recent auxiliary concepts such as
metric regularization can be transferred from the vectorial case to
component-wise DTW in a similar way. We illustrate that metric regularization
constitutes a crucial prerequisite for the interpretation of the resulting
relevance profiles.","['Babak Hosseini', 'Barbara Hammer']","['cs.LG', 'stat.ML']",2016-10-17 12:47:20+00:00
http://arxiv.org/abs/1610.04929v1,Probabilistic Dimensionality Reduction via Structure Learning,"We propose a novel probabilistic dimensionality reduction framework that can
naturally integrate the generative model and the locality information of data.
Based on this framework, we present a new model, which is able to learn a
smooth skeleton of embedding points in a low-dimensional space from
high-dimensional noisy data. The formulation of the new model can be
equivalently interpreted as two coupled learning problem, i.e., structure
learning and the learning of projection matrix. This interpretation motivates
the learning of the embedding points that can directly form an explicit graph
structure. We develop a new method to learn the embedding points that form a
spanning tree, which is further extended to obtain a discriminative and compact
feature representation for clustering problems. Unlike traditional clustering
methods, we assume that centers of clusters should be close to each other if
they are connected in a learned graph, and other cluster centers should be
distant. This can greatly facilitate data visualization and scientific
discovery in downstream analysis. Extensive experiments are performed that
demonstrate that the proposed framework is able to obtain discriminative
feature representations, and correctly recover the intrinsic structures of
various real-world datasets.",['Li Wang'],"['stat.ML', 'cs.LG']",2016-10-16 23:37:26+00:00
http://arxiv.org/abs/1610.04811v2,Estimation of low rank density matrices by Pauli measurements,"Density matrices are positively semi-definite Hermitian matrices with unit
trace that describe the states of quantum systems. Many quantum systems of
physical interest can be represented as high-dimensional low rank density
matrices. A popular problem in {\it quantum state tomography} (QST) is to
estimate the unknown low rank density matrix of a quantum system by conducting
Pauli measurements. Our main contribution is twofold. First, we establish the
minimax lower bounds in Schatten $p$-norms with $1\leq p\leq +\infty$ for low
rank density matrices estimation by Pauli measurements. In our previous paper,
these minimax lower bounds are proved under the trace regression model with
Gaussian noise and the noise is assumed to have common variance. In this paper,
we prove these bounds under the Binomial observation model which meets the
actual model in QST.
  Second, we study the Dantzig estimator (DE) for estimating the unknown low
rank density matrix under the Binomial observation model by using Pauli
measurements. In our previous papers, we studied the least squares estimator
and the projection estimator, where we proved the optimal convergence rates for
the least squares estimator in Schatten $p$-norms with $1\leq p\leq 2$ and,
under a stronger condition, the optimal convergence rates for the projection
estimator in Schatten $p$-norms with $1\leq p\leq +\infty$. In this paper, we
show that the results of these two distinct estimators can be simultaneously
obtained by the Dantzig estimator. Moreover, better convergence rates in
Schatten norm distances can be proved for Dantzig estimator under conditions
weaker than those needed in previous papers. When the objective function of DE
is replaced by the negative von Neumann entropy, we obtain sharp convergence
rate in Kullback-Leibler divergence.",['Dong Xia'],"['stat.ML', '62J99, 81P50, 62H12']",2016-10-16 02:28:08+00:00
http://arxiv.org/abs/1610.04804v1,Dynamic Stacked Generalization for Node Classification on Networks,"We propose a novel stacked generalization (stacking) method as a dynamic
ensemble technique using a pool of heterogeneous classifiers for node label
classification on networks. The proposed method assigns component models a set
of functional coefficients, which can vary smoothly with certain topological
features of a node. Compared to the traditional stacking model, the proposed
method can dynamically adjust the weights of individual models as we move
across the graph and provide a more versatile and significantly more accurate
stacking model for label prediction on a network. We demonstrate the benefits
of the proposed model using both a simulation study and real data analysis.","['Zhen Han', 'Alyson Wilson']","['stat.ML', 'cs.LG', 'cs.SI', 'stat.AP']",2016-10-16 00:47:21+00:00
http://arxiv.org/abs/1610.04798v1,Communication-efficient Distributed Sparse Linear Discriminant Analysis,"We propose a communication-efficient distributed estimation method for sparse
linear discriminant analysis (LDA) in the high dimensional regime. Our method
distributes the data of size $N$ into $m$ machines, and estimates a local
sparse LDA estimator on each machine using the data subset of size $N/m$. After
the distributed estimation, our method aggregates the debiased local estimators
from $m$ machines, and sparsifies the aggregated estimator. We show that the
aggregated estimator attains the same statistical rate as the centralized
estimation method, as long as the number of machines $m$ is chosen
appropriately. Moreover, we prove that our method can attain the model
selection consistency under a milder condition than the centralized method.
Experiments on both synthetic and real datasets corroborate our theory.","['Lu Tian', 'Quanquan Gu']",['stat.ML'],2016-10-15 23:39:45+00:00
http://arxiv.org/abs/1610.04782v1,An Adaptive Test of Independence with Analytic Kernel Embeddings,"A new computationally efficient dependence measure, and an adaptive
statistical test of independence, are proposed. The dependence measure is the
difference between analytic embeddings of the joint distribution and the
product of the marginals, evaluated at a finite set of locations (features).
These features are chosen so as to maximize a lower bound on the test power,
resulting in a test that is data-efficient, and that runs in linear time (with
respect to the sample size n). The optimized features can be interpreted as
evidence to reject the null hypothesis, indicating regions in the joint domain
where the joint distribution and the product of the marginals differ most.
Consistency of the independence test is established, for an appropriate choice
of features. In real-world benchmarks, independence tests using the optimized
features perform comparably to the state-of-the-art quadratic-time HSIC test,
and outperform competing O(n) and O(n log n) tests.","['Wittawat Jitkrittum', 'Zoltan Szabo', 'Arthur Gretton']","['stat.ML', 'cs.LG', '46E22, 62G10', 'G.3; I.2.6']",2016-10-15 20:19:48+00:00
http://arxiv.org/abs/1610.05672v2,Markov Chain Truncation for Doubly-Intractable Inference,"Computing partition functions, the normalizing constants of probability
distributions, is often hard. Variants of importance sampling give unbiased
estimates of a normalizer Z, however, unbiased estimates of the reciprocal 1/Z
are harder to obtain. Unbiased estimates of 1/Z allow Markov chain Monte Carlo
sampling of ""doubly-intractable"" distributions, such as the parameter posterior
for Markov Random Fields or Exponential Random Graphs. We demonstrate how to
construct unbiased estimates for 1/Z given access to black-box importance
sampling estimators for Z. We adapt recent work on random series truncation and
Markov chain coupling, producing estimators with lower variance and a higher
percentage of positive estimates than before. Our debiasing algorithms are
simple to implement, and have some theoretical and empirical advantages over
existing methods.","['Colin Wei', 'Iain Murray']","['stat.ML', 'cs.LG']",2016-10-15 20:14:52+00:00
http://arxiv.org/abs/1610.04751v1,Unsupervised clustering under the Union of Polyhedral Cones (UOPC) model,"In this paper, we consider clustering data that is assumed to come from one
of finitely many pointed convex polyhedral cones. This model is referred to as
the Union of Polyhedral Cones (UOPC) model. Similar to the Union of Subspaces
(UOS) model where each data from each subspace is generated from a (unknown)
basis, in the UOPC model each data from each cone is assumed to be generated
from a finite number of (unknown) \emph{extreme rays}.To cluster data under
this model, we consider several algorithms - (a) Sparse Subspace Clustering by
Non-negative constraints Lasso (NCL), (b) Least squares approximation (LSA),
and (c) K-nearest neighbor (KNN) algorithm to arrive at affinity between data
points. Spectral Clustering (SC) is then applied on the resulting affinity
matrix to cluster data into different polyhedral cones. We show that on an
average KNN outperforms both NCL and LSA and for this algorithm we provide the
deterministic conditions for correct clustering. For an affinity measure
between the cones it is shown that as long as the cones are not very coherent
and as long as the density of data within each cone exceeds a threshold, KNN
leads to accurate clustering. Finally, simulation results on real datasets
(MNIST and YaleFace datasets) depict that the proposed algorithm works well on
real data indicating the utility of the UOPC model and the proposed algorithm.","['Wenqi Wang', 'Vaneet Aggarwal', 'Shuchin Aeron']",['stat.ML'],2016-10-15 16:04:49+00:00
http://arxiv.org/abs/1610.04658v2,Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation,"We consider multi-class classification where the predictor has a hierarchical
structure that allows for a very large number of labels both at train and test
time. The predictive power of such models can heavily depend on the structure
of the tree, and although past work showed how to learn the tree structure, it
expected that the feature vectors remained static. We provide a novel algorithm
to simultaneously perform representation learning for the input data and
learning of the hierarchi- cal predictor. Our approach optimizes an objec- tive
function which favors balanced and easily- separable multi-way node partitions.
We theoret- ically analyze this objective, showing that it gives rise to a
boosting style property and a bound on classification error. We next show how
to extend the algorithm to conditional density estimation. We empirically
validate both variants of the al- gorithm on text classification and language
mod- eling, respectively, and show that they compare favorably to common
baselines in terms of accu- racy and running time.","['Yacine Jernite', 'Anna Choromanska', 'David Sontag']","['stat.ML', 'cs.CL', 'cs.LG']",2016-10-14 22:03:15+00:00
http://arxiv.org/abs/1610.04599v1,"Data-Driven Threshold Machine: Scan Statistics, Change-Point Detection, and Extreme Bandits","We present a novel distribution-free approach, the data-driven threshold
machine (DTM), for a fundamental problem at the core of many learning tasks:
choose a threshold for a given pre-specified level that bounds the tail
probability of the maximum of a (possibly dependent but stationary) random
sequence. We do not assume data distribution, but rather relying on the
asymptotic distribution of extremal values, and reduce the problem to estimate
three parameters of the extreme value distributions and the extremal index. We
specially take care of data dependence via estimating extremal index since in
many settings, such as scan statistics, change-point detection, and extreme
bandits, where dependence in the sequence of statistics can be significant. Key
features of our DTM also include robustness and the computational efficiency,
and it only requires one sample path to form a reliable estimate of the
threshold, in contrast to the Monte Carlo sampling approach which requires
drawing a large number of sample paths. We demonstrate the good performance of
DTM via numerical examples in various dependent settings.","['Shuang Li', 'Yao Xie', 'Le Song']","['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']",2016-10-14 19:43:16+00:00
http://arxiv.org/abs/1610.04583v1,Message-passing algorithms for synchronization problems over compact groups,"Various alignment problems arising in cryo-electron microscopy, community
detection, time synchronization, computer vision, and other fields fall into a
common framework of synchronization problems over compact groups such as Z/L,
U(1), or SO(3). The goal of such problems is to estimate an unknown vector of
group elements given noisy relative observations. We present an efficient
iterative algorithm to solve a large class of these problems, allowing for any
compact group, with measurements on multiple 'frequency channels' (Fourier
modes, or more generally, irreducible representations of the group). Our
algorithm is a highly efficient iterative method following the blueprint of
approximate message passing (AMP), which has recently arisen as a central
technique for inference problems such as structured low-rank estimation and
compressed sensing. We augment the standard ideas of AMP with ideas from
representation theory so that the algorithm can work with distributions over
compact groups. Using standard but non-rigorous methods from statistical
physics we analyze the behavior of our algorithm on a Gaussian noise model,
identifying phases where the problem is easy, (computationally) hard, and
(statistically) impossible. In particular, such evidence predicts that our
algorithm is information-theoretically optimal in many cases, and that the
remaining cases show evidence of statistical-to-computational gaps.","['Amelia Perry', 'Alexander S. Wein', 'Afonso S. Bandeira', 'Ankur Moitra']","['cs.IT', 'cs.CV', 'cs.DS', 'math.IT', 'math.OC', 'stat.ML']",2016-10-14 19:05:32+00:00
http://arxiv.org/abs/1610.04580v1,Two-sample testing in non-sparse high-dimensional linear models,"In analyzing high-dimensional models, sparsity of the model parameter is a
common but often undesirable assumption. In this paper, we study the following
two-sample testing problem: given two samples generated by two high-dimensional
linear models, we aim to test whether the regression coefficients of the two
linear models are identical. We propose a framework named TIERS (short for
TestIng Equality of Regression Slopes), which solves the two-sample testing
problem without making any assumptions on the sparsity of the regression
parameters. TIERS builds a new model by convolving the two samples in such a
way that the original hypothesis translates into a new moment condition. A
self-normalization construction is then developed to form a moment test. We
provide rigorous theory for the developed framework. Under very weak conditions
of the feature covariance, we show that the accuracy of the proposed test in
controlling Type I errors is robust both to the lack of sparsity in the
features and to the heavy tails in the error distribution, even when the sample
size is much smaller than the feature dimension. Moreover, we discuss minimax
optimality and efficiency properties of the proposed test. Simulation analysis
demonstrates excellent finite-sample performance of our test. In deriving the
test, we also develop tools that are of independent interest. The test is built
upon a novel estimator, called Auto-aDaptive Dantzig Selector (ADDS), which not
only automatically chooses an appropriate scale of the error term but also
incorporates prior information. To effectively approximate the critical value
of the test statistic, we develop a novel high-dimensional plug-in approach
that complements the recent advances in Gaussian approximation theory.","['Yinchu Zhu', 'Jelena Bradic']","['math.ST', 'stat.ME', 'stat.ML', 'stat.TH']",2016-10-14 18:51:34+00:00
http://arxiv.org/abs/1610.04578v3,Improved Strongly Adaptive Online Learning using Coin Betting,"This paper describes a new parameter-free online learning algorithm for
changing environments. In comparing against algorithms with the same time
complexity as ours, we obtain a strongly adaptive regret bound that is a factor
of at least $\sqrt{\log(T)}$ better, where $T$ is the time horizon. Empirical
results show that our algorithm outperforms state-of-the-art methods in
learning with expert advice and metric learning scenarios.","['Kwang-Sung Jun', 'Francesco Orabona', 'Rebecca Willett', 'Stephen Wright']","['stat.ML', 'cs.LG']",2016-10-14 18:51:25+00:00
http://arxiv.org/abs/1610.04576v1,Kernel Alignment Inspired Linear Discriminant Analysis,"Kernel alignment measures the degree of similarity between two kernels. In
this paper, inspired from kernel alignment, we propose a new Linear
Discriminant Analysis (LDA) formulation, kernel alignment LDA (kaLDA). We first
define two kernels, data kernel and class indicator kernel. The problem is to
find a subspace to maximize the alignment between subspace-transformed data
kernel and class indicator kernel. Surprisingly, the kernel alignment induced
kaLDA objective function is very similar to classical LDA and can be expressed
using between-class and total scatter matrices. This can be extended to
multi-label data. We use a Stiefel-manifold gradient descent algorithm to solve
this problem. We perform experiments on 8 single-label and 6 multi-label data
sets. Results show that kaLDA has very good performance on many single-label
and multi-label problems.","['Shuai Zheng', 'Chris Ding']","['cs.LG', 'stat.ML']",2016-10-14 18:48:03+00:00
http://arxiv.org/abs/1610.04574v3,Generalization Error of Invariant Classifiers,"This paper studies the generalization error of invariant classifiers. In
particular, we consider the common scenario where the classification task is
invariant to certain transformations of the input, and that the classifier is
constructed (or learned) to be invariant to these transformations. Our approach
relies on factoring the input space into a product of a base space and a set of
transformations. We show that whereas the generalization error of a
non-invariant classifier is proportional to the complexity of the input space,
the generalization error of an invariant classifier is proportional to the
complexity of the base space. We also derive a set of sufficient conditions on
the geometry of the base space and the set of transformations that ensure that
the complexity of the base space is much smaller than the complexity of the
input space. Our analysis applies to general classifiers such as convolutional
neural networks. We demonstrate the implications of the developed theory for
such classifiers with experiments on the MNIST and CIFAR-10 datasets.","['Jure Sokolic', 'Raja Giryes', 'Guillermo Sapiro', 'Miguel R. D. Rodrigues']","['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']",2016-10-14 18:40:52+00:00
http://arxiv.org/abs/1610.04490v3,Amortised MAP Inference for Image Super-resolution,"Image super-resolution (SR) is an underdetermined inverse problem, where a
large number of plausible high-resolution images can explain the same
downsampled image. Most current single image SR methods use empirical risk
minimisation, often with a pixel-wise mean squared error (MSE) loss. However,
the outputs from such methods tend to be blurry, over-smoothed and generally
appear implausible. A more desirable approach would employ Maximum a Posteriori
(MAP) inference, preferring solutions that always have a high probability under
the image prior, and thus appear more plausible. Direct MAP estimation for SR
is non-trivial, as it requires us to build a model for the image prior from
samples. Furthermore, MAP inference is often performed via optimisation-based
iterative algorithms which don't compare well with the efficiency of
neural-network-based alternatives. Here we introduce new methods for amortised
MAP inference whereby we calculate the MAP estimate directly using a
convolutional neural network. We first introduce a novel neural network
architecture that performs a projection to the affine subspace of valid SR
solutions ensuring that the high resolution output of the network is always
consistent with the low resolution input. We show that, using this
architecture, the amortised MAP inference problem reduces to minimising the
cross-entropy between two distributions, similar to training generative models.
We propose three methods to solve this optimisation problem: (1) Generative
Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates
gradient-estimates from denoising to train the network, and (3) a baseline
method using a maximum-likelihood-trained image prior. Our experiments show
that the GAN based approach performs best on real image data. Lastly, we
establish a connection between GANs and amortised variational inference as in
e.g. variational autoencoders.","['Casper Kaae Sønderby', 'Jose Caballero', 'Lucas Theis', 'Wenzhe Shi', 'Ferenc Huszár']","['cs.CV', 'cs.LG', 'stat.ML']",2016-10-14 14:58:44+00:00
http://arxiv.org/abs/1610.04491v1,The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits,"Stochastic linear bandits are a natural and simple generalisation of
finite-armed bandits with numerous practical applications. Current approaches
focus on generalising existing techniques for finite-armed bandits, notably the
optimism principle and Thompson sampling. While prior work has mostly been in
the worst-case setting, we analyse the asymptotic instance-dependent regret and
show matching upper and lower bounds on what is achievable. Surprisingly, our
results show that no algorithm based on optimism or Thompson sampling will ever
achieve the optimal rate, and indeed, can be arbitrarily far from optimal, even
in very simple cases. This is a disturbing result because these techniques are
standard tools that are widely used for sequential optimisation. For example,
for generalised linear bandits and reinforcement learning.","['Tor Lattimore', 'Csaba Szepesvari']","['stat.ML', 'cs.LG']",2016-10-14 14:58:44+00:00
http://arxiv.org/abs/1610.04460v3,On the Existence of a Sample Mean in Dynamic Time Warping Spaces,"The concept of sample mean in dynamic time warping (DTW) spaces has been
successfully applied to improve pattern recognition systems and generalize
centroid-based clustering algorithms. Its existence has neither been proved nor
challenged. This article presents sufficient conditions for existence of a
sample mean in DTW spaces. The proposed result justifies prior work on
approximate mean algorithms, sets the stage for constructing exact mean
algorithms, and is a first step towards a statistical theory of DTW spaces.","['Brijnesh J. Jain', 'David Schultz']","['cs.CV', 'math.OC', 'stat.ML']",2016-10-14 13:42:47+00:00
http://arxiv.org/abs/1610.04420v4,Theoretical Analysis of Domain Adaptation with Optimal Transport,"Domain adaptation (DA) is an important and emerging field of machine learning
that tackles the problem occurring when the distributions of training (source
domain) and test (target domain) data are similar but different. Current
theoretical results show that the efficiency of DA algorithms depends on their
capacity of minimizing the divergence between source and target probability
distributions. In this paper, we provide a theoretical study on the advantages
that concepts borrowed from optimal transportation theory can bring to DA. In
particular, we show that the Wasserstein metric can be used as a divergence
measure between distributions to obtain generalization guarantees for three
different learning settings: (i) classic DA with unsupervised target data (ii)
DA combining source and target labeled data, (iii) multiple source DA. Based on
the obtained results, we provide some insights showing when this analysis can
be tighter than other existing frameworks.","['Ievgen Redko', 'Amaury Habrard', 'Marc Sebban']","['stat.ML', 'cs.LG']",2016-10-14 11:59:28+00:00
http://arxiv.org/abs/1610.04386v2,Random Feature Expansions for Deep Gaussian Processes,"The composition of multiple Gaussian Processes as a Deep Gaussian Process
(DGP) enables a deep probabilistic nonparametric approach to flexibly tackle
complex machine learning problems with sound quantification of uncertainty.
Existing inference approaches for DGP models have limited scalability and are
notoriously cumbersome to construct. In this work, we introduce a novel
formulation of DGPs based on random feature expansions that we train using
stochastic variational inference. This yields a practical learning framework
which significantly advances the state-of-the-art in inference for DGPs, and
enables accurate quantification of uncertainty. We extensively showcase the
scalability and performance of our proposal on several datasets with up to 8
million observations, and various DGP architectures with up to 30 hidden
layers.","['Kurt Cutajar', 'Edwin V. Bonilla', 'Pietro Michiardi', 'Maurizio Filippone']","['stat.ML', 'stat.CO']",2016-10-14 09:56:17+00:00
http://arxiv.org/abs/1610.04371v1,"Aboveground biomass mapping in French Guiana by combining remote sensing, forest inventories and environmental data","Mapping forest aboveground biomass (AGB) has become an important task,
particularly for the reporting of carbon stocks and changes. AGB can be mapped
using synthetic aperture radar data (SAR) or passive optical data. However,
these data are insensitive to high AGB levels (\textgreater{}150 Mg/ha, and
\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical
forests. Studies have mapped the rough variations in AGB by combining optical
and environmental data at regional and global scales. Nevertheless, these maps
cannot represent local variations in AGB in tropical forests. In this paper, we
hypothesize that the problem of misrepresenting local variations in AGB and AGB
estimation with good precision occurs because of both methodological limits
(signal saturation or dilution bias) and a lack of adequate calibration data in
this range of AGB values. We test this hypothesis by developing a calibrated
regression model to predict variations in high AGB values (mean
\textgreater{}300 Mg/ha) in French Guiana by a methodological approach for
spatial extrapolation with data from the optical geoscience laser altimeter
system (GLAS), forest inventories, radar, optics, and environmental variables
for spatial inter-and extrapolation. Given their higher point count, GLAS data
allow a wider coverage of AGB values. We find that the metrics from GLAS
footprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3
Mg/ha) with no bias for high values. First, predictive models, including
remote-sensing, environmental variables and spatial correlation functions,
allow us to obtain ""wall-to-wall"" AGB maps over French Guiana with an RMSE for
the in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We
conclude that a calibrated regression model based on GLAS with dependent
environmental data can produce good AGB predictions even for high AGB values if
the calibration data fit the AGB range. We also demonstrate that small temporal
and spatial mismatches between field data and GLAS footprints are not a problem
for regional and global calibrated regression models because field data aim to
predict large and deep tendencies in AGB variations from environmental
gradients and do not aim to represent high but stochastic and temporally
limited variations from forest dynamics. Thus, we advocate including a greater
variety of data, even if less precise and shifted, to better represent high AGB
values in global models and to improve the fitting of these models for high
values.","['Ibrahim Fayad', 'Nicolas Baghdadi', 'Stéphane Guitet', 'Jean-Stéphane Bailly', 'Bruno Hérault', 'Valéry Gond', 'Mahmoud Hajj', 'Dinh Ho Tong Minh']","['stat.ML', 'stat.AP']",2016-10-14 08:53:48+00:00
http://arxiv.org/abs/1610.04351v1,Semi-supervised Graph Embedding Approach to Dynamic Link Prediction,"We propose a simple discrete time semi-supervised graph embedding approach to
link prediction in dynamic networks. The learned embedding reflects information
from both the temporal and cross-sectional network structures, which is
performed by defining the loss function as a weighted sum of the supervised
loss from past dynamics and the unsupervised loss of predicting the
neighborhood context in the current network. Our model is also capable of
learning different embeddings for both formation and dissolution dynamics.
These key aspects contributes to the predictive performance of our model and we
provide experiments with three real--world dynamic networks showing that our
method is comparable to state of the art methods in link formation prediction
and outperforms state of the art baseline methods in link dissolution
prediction.",['Ryohei Hisano'],"['stat.ML', 'cs.LG', 'cs.SI', 'physics.soc-ph']",2016-10-14 07:44:33+00:00
http://arxiv.org/abs/1610.04345v1,A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts,"Many methods have been used to recognize author personality traits from text,
typically combining linguistic feature engineering with shallow learning
models, e.g. linear regression or Support Vector Machines. This work uses
deep-learning-based models and atomic features of text, the characters, to
build hierarchical, vectorial word and sentence representations for trait
inference. This method, applied to a corpus of tweets, shows state-of-the-art
performance across five traits and three languages (English, Spanish and
Italian) compared with prior work in author profiling. The results, supported
by preliminary visualisation work, are encouraging for the ability to detect
complex human traits.","['Fei Liu', 'Julien Perez', 'Scott Nowson']","['cs.CL', 'stat.ML']",2016-10-14 07:14:44+00:00
http://arxiv.org/abs/1610.04336v5,MML is not consistent for Neyman-Scott,"Strict Minimum Message Length (SMML) is an information-theoretic statistical
inference method widely cited (but only with informal arguments) as providing
estimations that are consistent for general estimation problems. It is,
however, almost invariably intractable to compute, for which reason only
approximations of it (known as MML algorithms) are ever used in practice. Using
novel techniques that allow for the first time direct, non-approximated
analysis of SMML solutions, we investigate the Neyman-Scott estimation problem,
an oft-cited showcase for the consistency of MML, and show that even with a
natural choice of prior neither SMML nor its popular approximations are
consistent for it, thereby providing a counterexample to the general claim.
This is the first known explicit construction of an SMML solution for a
natural, high-dimensional problem.",['Michael Brand'],"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH', '62G05', 'I.2.6']",2016-10-14 06:07:45+00:00
http://arxiv.org/abs/1610.04211v2,Gated End-to-End Memory Networks,"Machine reading using differentiable reasoning models has recently shown
remarkable progress. In this context, End-to-End trainable Memory Networks,
MemN2N, have demonstrated promising performance on simple natural language
based reasoning tasks such as factual reasoning and basic deduction. However,
other tasks, namely multi-fact question-answering, positional reasoning or
dialog related tasks, remain challenging particularly due to the necessity of
more complex interactions between the memory and controller modules composing
this family of models. In this paper, we introduce a novel end-to-end memory
access regulation mechanism inspired by the current progress on the connection
short-cutting principle in the field of computer vision. Concretely, we develop
a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the
machine learning perspective, this new capability is learned in an end-to-end
fashion without the use of any additional supervision signal which is, as far
as our knowledge goes, the first of its kind. Our experiments show significant
improvements on the most challenging tasks in the 20 bAbI dataset, without the
use of any domain knowledge. Then, we show improvements on the dialog bAbI
tasks including the real human-bot conversion-based Dialog State Tracking
Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state
of the art.","['Julien Perez', 'Fei Liu']","['cs.CL', 'stat.ML']",2016-10-13 19:38:03+00:00
http://arxiv.org/abs/1610.04210v2,Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex Relaxation,"We propose a flexible convex relaxation for the phase retrieval problem that
operates in the natural domain of the signal. Therefore, we avoid the
prohibitive computational cost associated with ""lifting"" and semidefinite
programming (SDP) in methods such as PhaseLift and compete with recently
developed non-convex techniques for phase retrieval. We relax the quadratic
equations for phaseless measurements to inequality constraints each of which
representing a symmetric ""slab"". Through a simple convex program, our proposed
estimator finds an extreme point of the intersection of these slabs that is
best aligned with a given anchor vector. We characterize geometric conditions
that certify success of the proposed estimator. Furthermore, using classic
results in statistical learning theory, we show that for random measurements
the geometric certificates hold with high probability at an optimal sample
complexity. Phase transition of our estimator is evaluated through simulations.
Our numerical experiments also suggest that the proposed method can solve phase
retrieval problems with coded diffraction measurements as well.","['Sohail Bahmani', 'Justin Romberg']","['cs.IT', 'cs.LG', 'math.FA', 'math.IT', 'math.OC', 'stat.ML']",2016-10-13 19:35:28+00:00
http://arxiv.org/abs/1610.08087v2,Infinite-dimensional Log-Determinant divergences II: Alpha-Beta divergences,"This work presents a parametrized family of divergences, namely Alpha-Beta
Log- Determinant (Log-Det) divergences, between positive definite unitized
trace class operators on a Hilbert space. This is a generalization of the
Alpha-Beta Log-Determinant divergences between symmetric, positive definite
matrices to the infinite-dimensional setting. The family of Alpha-Beta Log-Det
divergences is highly general and contains many divergences as special cases,
including the recently formulated infinite dimensional affine-invariant
Riemannian distance and the infinite-dimensional Alpha Log-Det divergences
between positive definite unitized trace class operators. In particular, it
includes a parametrized family of metrics between positive definite trace class
operators, with the affine-invariant Riemannian distance and the square root of
the symmetric Stein divergence being special cases. For the Alpha-Beta Log-Det
divergences between covariance operators on a Reproducing Kernel Hilbert Space
(RKHS), we obtain closed form formulas via the corresponding Gram matrices.",['Minh Ha Quang'],"['math.FA', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",2016-10-13 17:58:58+00:00
http://arxiv.org/abs/1610.04181v6,Removal of Batch Effects using Distribution-Matching Residual Networks,"Sources of variability in experimentally derived data include measurement
error in addition to the physical phenomena of interest. This measurement error
is a combination of systematic components, originating from the measuring
instrument, and random measurement errors. Several novel biological
technologies, such as mass cytometry and single-cell RNA-seq, are plagued with
systematic errors that may severely affect statistical analysis if the data is
not properly calibrated. We propose a novel deep learning approach for removing
systematic batch effects. Our method is based on a residual network, trained to
minimize the Maximum Mean Discrepancy (MMD) between the multivariate
distributions of two replicates, measured in different batches. We apply our
method to mass cytometry and single-cell RNA-seq datasets, and demonstrate that
it effectively attenuates batch effects.","['Uri Shaham', 'Kelly P. Stanton', 'Jun Zhao', 'Huamin Li', 'Khadir Raddassi', 'Ruth Montgomery', 'Yuval Kluger']",['stat.ML'],2016-10-13 17:14:33+00:00
http://arxiv.org/abs/1610.04167v5,Tensorial Mixture Models,"Casting neural networks in generative frameworks is a highly sought-after
endeavor these days. Contemporary methods, such as Generative Adversarial
Networks, capture some of the generative capabilities, but not all. In
particular, they lack the ability of tractable marginalization, and thus are
not suitable for many tasks. Other methods, based on arithmetic circuits and
sum-product networks, do allow tractable marginalization, but their performance
is challenged by the need to learn the structure of a circuit. Building on the
tractability of arithmetic circuits, we leverage concepts from tensor analysis,
and derive a family of generative models we call Tensorial Mixture Models
(TMMs). TMMs assume a simple convolutional network structure, and in addition,
lend themselves to theoretical analyses that allow comprehensive understanding
of the relation between their structure and their expressive properties. We
thus obtain a generative model that is tractable on one hand, and on the other
hand, allows effective representation of rich distributions in an easily
controlled manner. These two capabilities are brought together in the task of
classification under missing data, where TMMs deliver state of the art
accuracies with seamless implementation and design.","['Or Sharir', 'Ronen Tamari', 'Nadav Cohen', 'Amnon Shashua']","['cs.LG', 'cs.NE', 'stat.ML']",2016-10-13 16:43:32+00:00
http://arxiv.org/abs/1610.04079v1,Towards end-to-end optimisation of functional image analysis pipelines,"The study of neurocognitive tasks requiring accurate localisation of activity
often rely on functional Magnetic Resonance Imaging, a widely adopted technique
that makes use of a pipeline of data processing modules, each involving a
variety of parameters. These parameters are frequently set according to the
local goal of each specific module, not accounting for the rest of the
pipeline. Given recent success of neural network research in many different
domains, we propose to convert the whole data pipeline into a deep neural
network, where the parameters involved are jointly optimised by the network to
best serve a common global goal. As a proof of concept, we develop a module
able to adaptively apply the most suitable spatial smoothing to every brain
volume for each specific neuroimaging task, and we validate its results in a
standard brain decoding experiment.","['Albert Vilamala', 'Kristoffer Hougaard Madsen', 'Lars Kai Hansen']","['cs.CV', 'q-bio.NC', 'stat.ML']",2016-10-13 13:57:55+00:00
http://arxiv.org/abs/1610.04019v1,Voice Conversion from Non-parallel Corpora Using Variational Auto-encoder,"We propose a flexible framework for spectral conversion (SC) that facilitates
training with unaligned corpora. Many SC frameworks require parallel corpora,
phonetic alignments, or explicit frame-wise correspondence for learning
conversion functions or for synthesizing a target spectrum with the aid of
alignments. However, these requirements gravely limit the scope of practical
applications of SC due to scarcity or even unavailability of parallel corpora.
We propose an SC framework based on variational auto-encoder which enables us
to exploit non-parallel corpora. The framework comprises an encoder that learns
speaker-independent phonetic representations and a decoder that learns to
reconstruct the designated speaker. It removes the requirement of parallel
corpora or phonetic alignments to train a spectral conversion system. We report
objective and subjective evaluations to validate our proposed method and
compare it to SC methods that have access to aligned corpora.","['Chin-Cheng Hsu', 'Hsin-Te Hwang', 'Yi-Chiao Wu', 'Yu Tsao', 'Hsin-Min Wang']","['stat.ML', 'cs.LG', 'cs.SD']",2016-10-13 10:52:25+00:00
http://arxiv.org/abs/1610.03995v2,Semi-Supervised Active Learning for Support Vector Machines: A Novel Approach that Exploits Structure Information in Data,"In our today's information society more and more data emerges, e.g.~in social
networks, technical applications, or business applications. Companies try to
commercialize these data using data mining or machine learning methods. For
this purpose, the data are categorized or classified, but often at high
(monetary or temporal) costs. An effective approach to reduce these costs is to
apply any kind of active learning (AL) methods, as AL controls the training
process of a classifier by specific querying individual data points (samples),
which are then labeled (e.g., provided with class memberships) by a domain
expert. However, an analysis of current AL research shows that AL still has
some shortcomings. In particular, the structure information given by the
spatial pattern of the (un)labeled data in the input space of a classification
model (e.g.,~cluster information), is used in an insufficient way. In addition,
many existing AL techniques pay too little attention to their practical
applicability. To meet these challenges, this article presents several
techniques that together build a new approach for combining AL and
semi-supervised learning (SSL) for support vector machines (SVM) in
classification tasks. Structure information is captured by means of
probabilistic models that are iteratively improved at runtime when label
information becomes available. The probabilistic models are considered in a
selection strategy based on distance, density, diversity, and distribution (4DS
strategy) information for AL and in a kernel function (Responsibility Weighted
Mahalanobis kernel) for SVM. The approach fuses generative and discriminative
modeling techniques. With 20 benchmark data sets and with the MNIST data set it
is shown that our new solution yields significantly better results than
state-of-the-art methods.","['Tobias Reitmaier', 'Adrian Calma', 'Bernhard Sick']","['stat.ML', 'cs.LG']",2016-10-13 09:36:56+00:00
http://arxiv.org/abs/1610.03988v1,Dictionary Update for NMF-based Voice Conversion Using an Encoder-Decoder Network,"In this paper, we propose a dictionary update method for Nonnegative Matrix
Factorization (NMF) with high dimensional data in a spectral conversion (SC)
task. Voice conversion has been widely studied due to its potential
applications such as personalized speech synthesis and speech enhancement.
Exemplar-based NMF (ENMF) emerges as an effective and probably the simplest
choice among all techniques for SC, as long as a source-target parallel speech
corpus is given. ENMF-based SC systems usually need a large amount of bases
(exemplars) to ensure the quality of the converted speech. However, a small and
effective dictionary is desirable but hard to obtain via dictionary update, in
particular when high-dimensional features such as STRAIGHT spectra are used.
Therefore, we propose a dictionary update framework for NMF by means of an
encoder-decoder reformulation. Regarding NMF as an encoder-decoder network
makes it possible to exploit the whole parallel corpus more effectively and
efficiently when applied to SC. Our experiments demonstrate significant gains
of the proposed system with small dictionaries over conventional ENMF-based
systems with dictionaries of same or much larger size.","['Chin-Cheng Hsu', 'Hsin-Te Hwang', 'Yi-Chiao Wu', 'Yu Tsao', 'Hsin-Min Wang']","['stat.ML', 'cs.LG', 'cs.SD']",2016-10-13 09:18:53+00:00
http://arxiv.org/abs/1610.03934v1,A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder,"Speech Translation has always been about giving source text or audio input
and waiting for system to give translated output in desired form. In this
paper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice
ear-piece translation device. We introduce and survey the recent advances made
in the field of Speech Engineering, to employ in the ADD, particularly focusing
on the three major processing steps of Recognition, Translation and Synthesis.
We tackle the problem of machine understanding of natural language by designing
a recognition unit for source audio to text, a translation unit for source
language text to target language text, and a synthesis unit for target language
text to target language speech. Speech from the surroundings will be recorded
by the recognition unit present on the ear-piece and translation will start as
soon as one sentence is successfully read. This way, we hope to give translated
output as and when input is being read. The recognition unit will use Hidden
Markov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory
cells, and the synthesis unit, HMM based speech synthesis system HTS. This
system will initially be built as an English to Tamil translation device.","['Hans Krupakar', 'Keerthika Rajvel', 'Bharathi B', 'Angel Deborah S', 'Vallidevi Krishnamurthy']","['cs.CL', 'cs.NE', 'cs.SD', 'stat.ML']",2016-10-13 04:10:58+00:00
http://arxiv.org/abs/1610.03927v1,Statistical Inference Using Mean Shift Denoising,"In this paper, we study how the mean shift algorithm can be used to denoise a
dataset. We introduce a new framework to analyze the mean shift algorithm as a
denoising approach by viewing the algorithm as an operator on a distribution
function. We investigate how the mean shift algorithm changes the distribution
and show that data points shifted by the mean shift concentrate around high
density regions of the underlying density function. By using the mean shift as
a denoising method, we enhance the performance of several clustering
techniques, improve the power of two-sample tests, and obtain a new method for
anomaly detection.","['Yunhua Xiang', 'Yen-Chi Chen']","['stat.ME', 'stat.ML']",2016-10-13 03:36:55+00:00
http://arxiv.org/abs/1610.03899v2,Generalization bound for kernel similarity learning,"Similarity learning has received a large amount of interest and is an
important tool for many scientific and industrial applications. In this
framework, we wish to infer the distance (similarity) between points with
respect to an arbitrary distance function $d$. Here, we formulate the problem
as a regression from a feature space $\mathcal{X}$ to an arbitrary vector space
$\mathcal{Y}$, where the Euclidean distance is proportional to $d$. We then
give Rademacher complexity bounds on the generalization error. We find that
with high probability, the complexity is bounded by the maximum of the radius
of $\mathcal{X}$ and the radius of $\mathcal{Y}$.",['Michael Rabadi'],"['stat.ML', 'cs.LG']",2016-10-12 23:35:45+00:00
http://arxiv.org/abs/1610.03774v4,"Parallelizing Stochastic Gradient Descent for Least Squares Regression: mini-batching, averaging, and model misspecification","This work characterizes the benefits of averaging schemes widely used in
conjunction with stochastic gradient descent (SGD). In particular, this work
provides a sharp analysis of: (1) mini-batching, a method of averaging many
samples of a stochastic gradient to both reduce the variance of the stochastic
gradient estimate and for parallelizing SGD and (2) tail-averaging, a method
involving averaging the final few iterates of SGD to decrease the variance in
SGD's final iterate. This work presents non-asymptotic excess risk bounds for
these schemes for the stochastic approximation problem of least squares
regression.
  Furthermore, this work establishes a precise problem-dependent extent to
which mini-batch SGD yields provable near-linear parallelization speedups over
SGD with batch size one. This allows for understanding learning rate versus
batch size tradeoffs for the final iterate of an SGD method. These results are
then utilized in providing a highly parallelizable SGD method that obtains the
minimax risk with nearly the same number of serial updates as batch gradient
descent, improving significantly over existing SGD methods. A non-asymptotic
analysis of communication efficient parallelization schemes such as
model-averaging/parameter mixing methods is then provided.
  Finally, this work sheds light on some fundamental differences in SGD's
behavior when dealing with agnostic noise in the (non-realizable) least squares
regression problem. In particular, the work shows that the stepsizes that
ensure minimax risk for the agnostic case must be a function of the noise
properties.
  This paper builds on the operator view of analyzing SGD methods, introduced
by Defossez and Bach (2015), followed by developing a novel analysis in
bounding these operators to characterize the excess risk. These techniques are
of broader interest in analyzing computational aspects of stochastic
approximation.","['Prateek Jain', 'Sham M. Kakade', 'Rahul Kidambi', 'Praneeth Netrapalli', 'Aaron Sidford']","['stat.ML', 'cs.DS', 'cs.LG']",2016-10-12 16:30:11+00:00
http://arxiv.org/abs/1610.03761v3,Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble of Autoencoders,"A fall is an abnormal activity that occurs rarely, so it is hard to collect
real data for falls. It is, therefore, difficult to use supervised learning
methods to automatically detect falls. Another challenge in using machine
learning methods to automatically detect falls is the choice of engineered
features. In this paper, we propose to use an ensemble of autoencoders to
extract features from different channels of wearable sensor data trained only
on normal activities. We show that the traditional approach of choosing a
threshold as the maximum of the reconstruction error on the training normal
data is not the right way to identify unseen falls. We propose two methods for
automatic tightening of reconstruction error from only the normal activities
for better identification of unseen falls. We present our results on two
activity recognition datasets and show the efficacy of our proposed method
against traditional autoencoder models and two standard one-class
classification methods.","['Shehroz S. Khan', 'Babak Taati']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']",2016-10-12 15:55:06+00:00
http://arxiv.org/abs/1610.03738v1,Exploring the Entire Regularization Path for the Asymmetric Cost Linear Support Vector Machine,"We propose an algorithm for exploring the entire regularization path of
asymmetric-cost linear support vector machines. Empirical evidence suggests the
predictive power of support vector machines depends on the regularization
parameters of the training algorithms. The algorithms exploring the entire
regularization paths have been proposed for single-cost support vector machines
thereby providing the complete knowledge on the behavior of the trained model
over the hyperparameter space. Considering the problem in two-dimensional
hyperparameter space though enables our algorithm to maintain greater
flexibility in dealing with special cases and sheds light on problems
encountered by algorithms building the paths in one-dimensional spaces. We
demonstrate two-dimensional regularization paths for linear support vector
machines that we train on synthetic and real data.",['Daniel Wesierski'],"['cs.LG', 'stat.ML']",2016-10-12 14:57:10+00:00
http://arxiv.org/abs/1610.03725v2,Post Selection Inference with Kernels,"We propose a novel kernel based post selection inference (PSI) algorithm,
which can not only handle non-linearity in data but also structured output such
as multi-dimensional and multi-label outputs. Specifically, we develop a PSI
algorithm for independence measures, and propose the Hilbert-Schmidt
Independence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the
proposed algorithm is that it can handle non-linearity and/or structured data
through kernels. Namely, the proposed algorithm can be used for wider range of
applications including nonlinear multi-class classification and multi-variate
regressions, while existing PSI algorithms cannot handle them. Through
synthetic experiments, we show that the proposed approach can find a set of
statistically significant features for both regression and classification
problems. Moreover, we apply the hsicInf algorithm to a real-world data, and
show that hsicInf can successfully identify important features.","['Makoto Yamada', 'Yuta Umezu', 'Kenji Fukumizu', 'Ichiro Takeuchi']","['stat.ML', 'stat.ME']",2016-10-12 14:23:09+00:00
http://arxiv.org/abs/1610.03724v2,Decision trees unearth return sign correlation in the S&P 500,"Technical trading rules and linear regressive models are often used by
practitioners to find trends in financial data. However, these models are
unsuited to find non-linearly separable patterns. We propose a decision tree
forecasting model that has the flexibility to capture arbitrary patterns. To
illustrate, we construct a binary Markov process with a deterministic component
that cannot be predicted with an autoregressive process. A simulation study
confirms the robustness of the trees and limitation of the autoregressive
model. Finally, adjusting for multiple testing, we show that some tree based
strategies achieve trading performance significant at the 99% confidence level
on the S&P 500 over the past 20 years. The best strategy breaks even with the
buy-and-hold strategy at 21 bps in transaction costs per round trip. A
four-factor regression analysis shows significant intercept and correlation
with the market. The return anomalies are strongest during the bursts of the
dotcom bubble, financial crisis, and European debt crisis. The correlation of
the return signs during these periods confirms the theoretical model.","['Lucas Fievet', 'Didier Sornette']","['stat.AP', 'stat.ML']",2016-10-12 14:22:05+00:00
