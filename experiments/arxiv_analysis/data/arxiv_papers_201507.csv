id,title,abstract,authors,categories,date
http://arxiv.org/abs/1508.07384v2,Generalized Uniformly Optimal Methods for Nonlinear Programming,"In this paper, we present a generic framework to extend existing uniformly
optimal convex programming algorithms to solve more general nonlinear, possibly
nonconvex, optimization problems. The basic idea is to incorporate a local
search step (gradient descent or Quasi-Newton iteration) into these uniformly
optimal convex programming methods, and then enforce a monotone decreasing
property of the function values computed along the trajectory. Algorithms of
these types will then achieve the best known complexity for nonconvex problems,
and the optimal complexity for convex ones without requiring any problem
parameters. As a consequence, we can have a unified treatment for a general
class of nonlinear programming problems regardless of their convexity and
smoothness level. In particular, we show that the accelerated gradient and
level methods, both originally designed for solving convex optimization
problems only, can be used for solving both convex and nonconvex problems
uniformly. In a similar vein, we show that some well-studied techniques for
nonlinear programming, e.g., Quasi-Newton iteration, can be embedded into
optimal convex optimization algorithms to possibly further enhance their
numerical performance. Our theoretical and algorithmic developments are
complemented by some promising numerical results obtained for solving a few
important nonconvex and nonlinear data analysis problems in the literature.","['Saeed Ghadimi', 'Guanghui Lan', 'Hongchao Zhang']","['math.OC', 'stat.ML']",2015-08-29 01:03:47+00:00
http://arxiv.org/abs/1508.07192v2,Varying-coefficient models with isotropic Gaussian process priors,"We study learning problems in which the conditional distribution of the
output given the input varies as a function of additional task variables. In
varying-coefficient models with Gaussian process priors, a Gaussian process
generates the functional relationship between the task variables and the
parameters of this conditional. Varying-coefficient models subsume hierarchical
Bayesian multitask models, but also generalizations in which the conditional
varies continuously, for instance, in time or space. However, Bayesian
inference in varying-coefficient models is generally intractable. We show that
inference for varying-coefficient models with isotropic Gaussian process priors
resolves to standard inference for a Gaussian process that can be solved
efficiently. MAP inference in this model resolves to multitask learning using
task and instance kernels, and inference for hierarchical Bayesian multitask
models can be carried out efficiently using graph-Laplacian kernels. We report
on experiments for geospatial prediction.","['Matthias Bussas', 'Christoph Sawade', 'Tobias Scheffer', 'Niels Landwehr']","['cs.LG', 'stat.ML', 'I.2.6']",2015-08-28 13:13:49+00:00
http://arxiv.org/abs/1508.07103v1,Regularized Kernel Recursive Least Square Algoirthm,"In most adaptive signal processing applications, system linearity is assumed
and adaptive linear filters are thus used. The traditional class of supervised
adaptive filters rely on error-correction learning for their adaptive
capability. The kernel method is a powerful nonparametric modeling tool for
pattern analysis and statistical signal processing. Through a nonlinear
mapping, kernel methods transform the data into a set of points in a
Reproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fast
convergence rate in stationary scenario. However the good performance is
obtained at a cost of high computation complexity. Sparsification in kernel
methods is know to related to less computational complexity and memory
consumption.",['Songlin Zhao'],"['cs.LG', 'stat.ML']",2015-08-28 06:04:37+00:00
http://arxiv.org/abs/1508.07096v1,Partitioning Large Scale Deep Belief Networks Using Dropout,"Deep learning methods have shown great promise in many practical
applications, ranging from speech recognition, visual object recognition, to
text processing. However, most of the current deep learning methods suffer from
scalability problems for large-scale applications, forcing researchers or users
to focus on small-scale problems with fewer parameters.
  In this paper, we consider a well-known machine learning model, deep belief
networks (DBNs) that have yielded impressive classification performance on a
large number of benchmark machine learning tasks. To scale up DBN, we propose
an approach that can use the computing clusters in a distributed environment to
train large models, while the dense matrix computations within a single machine
are sped up using graphics processors (GPU). When training a DBN, each machine
randomly drops out a portion of neurons in each hidden layer, for each training
case, making the remaining neurons only learn to detect features that are
generally helpful for producing the correct answer. Within our approach, we
have developed four methods to combine outcomes from each machine to form a
unified model. Our preliminary experiment on the mnst handwritten digit
database demonstrates that our approach outperforms the state of the art test
error rate.","['Yanping Huang', 'Sai Zhang']","['stat.ML', 'cs.LG', 'cs.NE']",2015-08-28 05:24:06+00:00
http://arxiv.org/abs/1508.06916v4,Nucleosome positioning: resources and tools online,"Nucleosome positioning is an important process required for proper genome
packing and its accessibility to execute the genetic program in a
cell-specific, timely manner. In the recent years hundreds of papers have been
devoted to the bioinformatics, physics and biology of nucleosome positioning.
The purpose of this review is to cover a practical aspect of this field, namely
to provide a guide to the multitude of nucleosome positioning resources
available online. These include almost 300 experimental datasets of genome-wide
nucleosome occupancy profiles determined in different cell types and more than
40 computational tools for the analysis of experimental nucleosome positioning
data and prediction of intrinsic nucleosome formation probabilities from the
DNA sequence. A manually curated, up to date list of these resources will be
maintained at http://generegulation.info.",['Vladimir B. Teif'],"['q-bio.GN', 'physics.bio-ph', 'q-bio.BM', 'stat.ML']",2015-08-27 16:09:30+00:00
http://arxiv.org/abs/1508.06901v1,Compressive Sensing via Low-Rank Gaussian Mixture Models,"We develop a new compressive sensing (CS) inversion algorithm by utilizing
the Gaussian mixture model (GMM). While the compressive sensing is performed
globally on the entire image as implemented in our lensless camera, a low-rank
GMM is imposed on the local image patches. This low-rank GMM is derived via
eigenvalue thresholding of the GMM trained on the projection of the measurement
data, thus learned {\em in situ}. The GMM and the projection of the measurement
data are updated iteratively during the reconstruction. Our GMM algorithm
degrades to the piecewise linear estimator (PLE) if each patch is represented
by a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also
developed for CS inversion, constituting an additional contribution of this
paper. Extensive results on both simulation data and real data captured by the
lensless camera demonstrate the efficacy of the proposed algorithm.
Furthermore, we compare the CS reconstruction results using our algorithm with
the JPEG compression. Simulation results demonstrate that when limited
bandwidth is available (a small number of measurements), our algorithm can
achieve comparable results as JPEG.","['Xin Yuan', 'Hong Jiang', 'Gang Huang', 'Paul A. Wilford']","['stat.ML', 'cs.LG', 'stat.AP']",2015-08-27 15:35:11+00:00
http://arxiv.org/abs/1508.06845v1,Encrypted statistical machine learning: new privacy preserving methods,"We present two new statistical machine learning methods designed to learn on
fully homomorphic encrypted (FHE) data. The introduction of FHE schemes
following Gentry (2009) opens up the prospect of privacy preserving statistical
machine learning analysis and modelling of encrypted data without compromising
security constraints. We propose tailored algorithms for applying extremely
random forests, involving a new cryptographic stochastic fraction estimator,
and na\""{i}ve Bayes, involving a semi-parametric model for the class decision
boundary, and show how they can be used to learn and predict from encrypted
data. We demonstrate that these techniques perform competitively on a variety
of classification data sets and provide detailed information about the
computational practicalities of these and other FHE methods.","['Louis J. M. Aslett', 'Pedro M. Esperan√ßa', 'Chris C. Holmes']","['stat.ML', 'cs.CR', 'cs.LG', 'stat.ME']",2015-08-27 13:06:55+00:00
http://arxiv.org/abs/1508.06615v4,Character-Aware Neural Language Models,"We describe a simple neural language model that relies only on
character-level inputs. Predictions are still made at the word-level. Our model
employs a convolutional neural network (CNN) and a highway network over
characters, whose output is given to a long short-term memory (LSTM) recurrent
neural network language model (RNN-LM). On the English Penn Treebank the model
is on par with the existing state-of-the-art despite having 60% fewer
parameters. On languages with rich morphology (Arabic, Czech, French, German,
Spanish, Russian), the model outperforms word-level/morpheme-level LSTM
baselines, again with fewer parameters. The results suggest that on many
languages, character inputs are sufficient for language modeling. Analysis of
word representations obtained from the character composition part of the model
reveals that the model is able to encode, from characters only, both semantic
and orthographic information.","['Yoon Kim', 'Yacine Jernite', 'David Sontag', 'Alexander M. Rush']","['cs.CL', 'cs.NE', 'stat.ML']",2015-08-26 19:25:34+00:00
http://arxiv.org/abs/1508.06574v1,A review of homomorphic encryption and software tools for encrypted statistical machine learning,"Recent advances in cryptography promise to enable secure statistical
computation on encrypted data, whereby a limited set of operations can be
carried out without the need to first decrypt. We review these homomorphic
encryption schemes in a manner accessible to statisticians and machine
learners, focusing on pertinent limitations inherent in the current state of
the art. These limitations restrict the kind of statistics and machine learning
algorithms which can be implemented and we review those which have been
successfully applied in the literature. Finally, we document a high performance
R package implementing a recent homomorphic scheme in a general framework.","['Louis J. M. Aslett', 'Pedro M. Esperan√ßa', 'Chris C. Holmes']","['stat.ML', 'cs.CR', 'cs.LG']",2015-08-26 17:11:12+00:00
http://arxiv.org/abs/1508.06446v2,Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric Admixture Modeling,"Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixture
modeling, where the number of mixture components grows with the number of data
items. The Hierarchical Dirichlet Process (HDP), is an extension of DP for
grouped data, often used for non-parametric topic modeling, where each group is
a mixture over shared mixture densities. The Nested Dirichlet Process (nDP), on
the other hand, is an extension of the DP for learning group level
distributions from data, simultaneously clustering the groups. It allows group
level distributions to be shared across groups in a non-parametric setting,
leading to a non-parametric mixture of mixtures. The nCRF extends the nDP for
multilevel non-parametric mixture modeling, enabling modeling topic
hierarchies. However, the nDP and nCRF do not allow sharing of distributions as
required in many applications, motivating the need for multi-level
non-parametric admixture modeling. We address this gap by proposing multi-level
nested HDPs (nHDP) where the base distribution of the HDP is itself a HDP at
each level thereby leading to admixtures of admixtures at each level. Because
of couplings between various HDP levels, scaling up is naturally a challenge
during inference. We propose a multi-level nested Chinese Restaurant Franchise
(nCRF) representation for the nested HDP, with which we outline an inference
algorithm based on Gibbs Sampling. We evaluate our model with the two level
nHDP for non-parametric entity topic modeling where an inner HDP creates a
countably infinite set of topic mixtures and associates them with author
entities, while an outer HDP associates documents with these author entities.
In our experiments on two real world research corpora, the nHDP is able to
generalize significantly better than existing models and detect missing author
entities with a reasonable level of accuracy.","['Lavanya Sita Tekumalla', 'Priyanka Agrawal', 'Indrajit Bhattacharya']","['stat.ML', 'cs.LG']",2015-08-26 11:24:36+00:00
http://arxiv.org/abs/1508.06388v1,Gaussian Mixture Models with Component Means Constrained in Pre-selected Subspaces,"We investigate a Gaussian mixture model (GMM) with component means
constrained in a pre-selected subspace. Applications to classification and
clustering are explored. An EM-type estimation algorithm is derived. We prove
that the subspace containing the component means of a GMM with a common
covariance matrix also contains the modes of the density and the class means.
This motivates us to find a subspace by applying weighted principal component
analysis to the modes of a kernel density and the class means. To circumvent
the difficulty of deciding the kernel bandwidth, we acquire multiple subspaces
from the kernel densities based on a sequence of bandwidths. The GMM
constrained by each subspace is estimated; and the model yielding the maximum
likelihood is chosen. A dimension reduction property is proved in the sense of
being informative for classification or clustering. Experiments on real and
simulated data sets are conducted to examine several ways of determining the
subspace and to compare with the reduced rank mixture discriminant analysis
(MDA). Our new method with the simple technique of spanning the subspace only
by class means often outperforms the reduced rank MDA when the subspace
dimension is very low, making it particularly appealing for visualization.","['Mu Qiao', 'Jia Li']","['stat.ML', 'cs.LG']",2015-08-26 07:25:22+00:00
http://arxiv.org/abs/1508.06235v4,Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm,"In this paper, we propose a model-based clustering method (TVClust) that
robustly incorporates noisy side information as soft-constraints and aims to
seek a consensus between side information and the observed data. Our method is
based on a nonparametric Bayesian hierarchical model that combines the
probabilistic model for the data instance and the one for the side-information.
An efficient Gibbs sampling algorithm is proposed for posterior inference.
Using the small-variance asymptotics of our probabilistic model, we then derive
a new deterministic clustering algorithm (RDP-means). It can be viewed as an
extension of K-means that allows for the inclusion of side information and has
the additional property that the number of clusters does not need to be
specified a priori. Empirical studies have been carried out to compare our work
with many constrained clustering algorithms from the literature on both a
variety of data sets and under a variety of conditions such as using noisy side
information and erroneous k values. The results of our experiments show strong
results for our probabilistic and deterministic approaches under these
conditions when compared to other algorithms in the literature.","['Daniel Khashabi', 'John Wieting', 'Jeffrey Yufei Liu', 'Feng Liang']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.CO']",2015-08-25 18:13:27+00:00
http://arxiv.org/abs/1509.01199v1,Inferring Passenger Type from Commuter Eigentravel Matrices,"A sufficient knowledge of the demographics of a commuting public is essential
in formulating and implementing more targeted transportation policies, as
commuters exhibit different ways of traveling. With the advent of the Automated
Fare Collection system (AFC), probing the travel patterns of commuters has
become less invasive and more accessible. Consequently, numerous transport
studies related to human mobility have shown that these observed patterns allow
one to pair individuals with locations and/or activities at certain times of
the day. However, classifying commuters using their travel signatures is yet to
be thoroughly examined.
  Here, we contribute to the literature by demonstrating a procedure to
characterize passenger types (Adult, Child/Student, and Senior Citizen) based
on their three-month travel patterns taken from a smart fare card system. We
first establish a method to construct distinct commuter matrices, which we
refer to as eigentravel matrices, that capture the characteristic travel
routines of individuals. From the eigentravel matrices, we build classification
models that predict the type of passengers traveling. Among the models
explored, the gradient boosting method (GBM) gives the best prediction accuracy
at 76%, which is 84% better than the minimum model accuracy (41%) required
vis-\`a-vis the proportional chance criterion. In addition, we find that travel
features generated during weekdays have greater predictive power than those on
weekends. This work should not only be useful for transport planners, but for
market researchers as well. With the awareness of which commuter types are
traveling, ads, service announcements, and surveys, among others, can be made
more targeted spatiotemporally. Finally, our framework should be effective in
creating synthetic populations for use in real-world simulations that involve a
metropolitan's public transport system.","['Erika Fille Legara', 'Christopher Monterola']","['physics.soc-ph', 'cs.CY', 'physics.data-an', 'stat.AP', 'stat.ML']",2015-08-25 16:10:08+00:00
http://arxiv.org/abs/1508.06095v1,OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based Neural Training,"In this paper we consider the training of single hidden layer neural networks
by pseudoinversion, which, in spite of its popularity, is sometimes affected by
numerical instability issues. Regularization is known to be effective in such
cases, so that we introduce, in the framework of Tikhonov regularization, a
matricial reformulation of the problem which allows us to use the condition
number as a diagnostic tool for identification of instability. By imposing
well-conditioning requirements on the relevant matrices, our theoretical
analysis allows the identification of an optimal value for the regularization
parameter from the standpoint of stability. We compare with the value derived
by cross-validation for overfitting control and optimisation of the
generalization performance. We test our method for both regression and
classification tasks. The proposed method is quite effective in terms of
predictivity, often with some improvement on performance with respect to the
reference cases considered. This approach, due to analytical determination of
the regularization parameter, dramatically reduces the computational load
required by many other techniques.","['Rossella Cancelliere', 'Mario Gai', 'Patrick Gallinari', 'Luca Rubini']","['cs.NE', 'cs.LG', 'stat.ML']",2015-08-25 10:09:31+00:00
http://arxiv.org/abs/1508.06091v1,AUC Optimisation and Collaborative Filtering,"In recommendation systems, one is interested in the ranking of the predicted
items as opposed to other losses such as the mean squared error. Although a
variety of ways to evaluate rankings exist in the literature, here we focus on
the Area Under the ROC Curve (AUC) as it widely used and has a strong
theoretical underpinning. In practical recommendation, only items at the top of
the ranked list are presented to the users. With this in mind, we propose a
class of objective functions over matrix factorisations which primarily
represent a smooth surrogate for the real AUC, and in a special case we show
how to prioritise the top of the list. The objectives are differentiable and
optimised through a carefully designed stochastic gradient-descent-based
algorithm which scales linearly with the size of the data. In the special case
of square loss we show how to improve computational complexity by leveraging
previously computed measures. To understand theoretically the underlying matrix
factorisation approaches we study both the consistency of the loss functions
with respect to AUC, and generalisation using Rademacher theory. The resulting
generalisation analysis gives strong motivation for the optimisation under
study. Finally, we provide computation results as to the efficacy of the
proposed method using synthetic and real data.","['Charanpal Dhanjal', 'Romaric Gaudel', 'Stephan Clemencon']","['stat.ML', 'cs.LG']",2015-08-25 09:46:09+00:00
http://arxiv.org/abs/1508.05913v1,Another Look at DWD: Thrifty Algorithm and Bayes Risk Consistency in RKHS,"Distance weighted discrimination (DWD) is a margin-based classifier with an
interesting geometric motivation. DWD was originally proposed as a superior
alternative to the support vector machine (SVM), however DWD is yet to be
popular compared with the SVM. The main reasons are twofold. First, the
state-of-the-art algorithm for solving DWD is based on the second-order-cone
programming (SOCP), while the SVM is a quadratic programming problem which is
much more efficient to solve. Second, the current statistical theory of DWD
mainly focuses on the linear DWD for the high-dimension-low-sample-size setting
and data-piling, while the learning theory for the SVM mainly focuses on the
Bayes risk consistency of the kernel SVM. In fact, the Bayes risk consistency
of DWD is presented as an open problem in the original DWD paper. In this work,
we advance the current understanding of DWD from both computational and
theoretical perspectives. We propose a novel efficient algorithm for solving
DWD, and our algorithm can be several hundred times faster than the existing
state-of-the-art algorithm based on the SOCP. In addition, our algorithm can
handle the generalized DWD, while the SOCP algorithm only works well for a
special DWD but not the generalized DWD. Furthermore, we consider a natural
kernel DWD in a reproducing kernel Hilbert space and then establish the Bayes
risk consistency of the kernel DWD. We compare DWD and the SVM on several
benchmark data sets and show that the two have comparable classification
accuracy, but DWD equipped with our new algorithm can be much faster to compute
than the SVM.","['Boxiang Wang', 'Hui Zou']",['stat.ML'],2015-08-24 18:59:03+00:00
http://arxiv.org/abs/1508.05803v1,Searching for significant patterns in stratified data,"Significant pattern mining, the problem of finding itemsets that are
significantly enriched in one class of objects, is statistically challenging,
as the large space of candidate patterns leads to an enormous multiple testing
problem. Recently, the concept of testability was proposed as one approach to
correct for multiple testing in pattern mining while retaining statistical
power. Still, these strategies based on testability do not allow one to
condition the test of significance on the observed covariates, which severely
limits its utility in biomedical applications. Here we propose a strategy and
an efficient algorithm to perform significant pattern mining in the presence of
categorical covariates with K states.","['Felipe Llinares-Lopez', 'Laetitia Papaxanthos', 'Dean Bodenham', 'Karsten Borgwardt']","['stat.ML', 'cs.LG']",2015-08-24 13:53:06+00:00
http://arxiv.org/abs/1508.05711v1,Fast Asynchronous Parallel Stochastic Gradient Decent,"Stochastic gradient descent~(SGD) and its variants have become more and more
popular in machine learning due to their efficiency and effectiveness. To
handle large-scale problems, researchers have recently proposed several
parallel SGD methods for multicore systems. However, existing parallel SGD
methods cannot achieve satisfactory performance in real applications. In this
paper, we propose a fast asynchronous parallel SGD method, called AsySVRG, by
designing an asynchronous strategy to parallelize the recently proposed SGD
variant called stochastic variance reduced gradient~(SVRG). Both theoretical
and empirical results show that AsySVRG can outperform existing
state-of-the-art parallel SGD methods like Hogwild! in terms of convergence
rate and computation cost.","['Shen-Yi Zhao', 'Wu-Jun Li']","['stat.ML', 'cs.LG']",2015-08-24 07:51:09+00:00
http://arxiv.org/abs/1508.05608v1,The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms,"We consider the Max $K$-Armed Bandit problem, where a learning agent is faced
with several sources (arms) of items (rewards), and interested in finding the
best item overall. At each time step the agent chooses an arm, and obtains a
random real valued reward. The rewards of each arm are assumed to be i.i.d.,
with an unknown probability distribution that generally differs among the arms.
Under the PAC framework, we provide lower bounds on the sample complexity of
any $(\epsilon,\delta)$-correct algorithm, and propose algorithms that attain
this bound up to logarithmic factors. We compare the performance of this
multi-arm algorithms to the variant in which the arms are not distinguishable
by the agent and are chosen randomly at each stage. Interestingly, when the
maximal rewards of the arms happen to be similar, the latter approach may
provide better performance.","['Yahel David', 'Nahum Shimkin']","['stat.ML', 'cs.AI', 'cs.LG']",2015-08-23 13:38:15+00:00
http://arxiv.org/abs/1508.05565v2,Necessary and Sufficient Conditions and a Provably Efficient Algorithm for Separable Topic Discovery,"We develop necessary and sufficient conditions and a novel provably
consistent and efficient algorithm for discovering topics (latent factors) from
observations (documents) that are realized from a probabilistic mixture of
shared latent factors that have certain properties. Our focus is on the class
of topic models in which each shared latent factor contains a novel word that
is unique to that factor, a property that has come to be known as separability.
Our algorithm is based on the key insight that the novel words correspond to
the extreme points of the convex hull formed by the row-vectors of a suitably
normalized word co-occurrence matrix. We leverage this geometric insight to
establish polynomial computation and sample complexity bounds based on a few
isotropic random projections of the rows of the normalized word co-occurrence
matrix. Our proposed random-projections-based algorithm is naturally amenable
to an efficient distributed implementation and is attractive for modern
web-scale distributed data mining applications.","['Weicong Ding', 'Prakash Ishwar', 'Venkatesh Saligrama']","['cs.LG', 'cs.CL', 'cs.IR', 'stat.ML']",2015-08-23 03:44:26+00:00
http://arxiv.org/abs/1508.05550v7,MultiView Diffusion Maps,"In this paper, we address the challenging task of achieving multi-view
dimensionality reduction. The goal is to effectively use the availability of
multiple views for extracting a coherent low-dimensional representation of the
data. The proposed method exploits the intrinsic relation within each view, as
well as the mutual relations between views. The multi-view dimensionality
reduction is achieved by defining a cross-view model in which an implied random
walk process is restrained to hop between objects in the different views. The
method is robust to scaling and insensitive to small structural changes in the
data. We define new diffusion distances and analyze the spectra of the proposed
kernel. We show that the proposed framework is useful for various machine
learning applications such as clustering, classification, and manifold
learning. Finally, by fusing multi-sensor seismic data we present a method for
automatic identification of seismic events.","['Ofir Lindenbaum', 'Arie Yeredor', 'Moshe Salhov', 'Amir Averbuch']","['cs.LG', 'stat.ML']",2015-08-23 00:20:17+00:00
http://arxiv.org/abs/1508.05514v1,Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence,"We propose a greedy mixture reduction algorithm which is capable of pruning
mixture components as well as merging them based on the Kullback-Leibler
divergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD
based method since it is not restricted to merging operations. The capability
of pruning (in addition to merging) gives the algorithm the ability of
preserving the peaks of the original mixture during the reduction. Analytical
approximations are derived to circumvent the computational intractability of
the KLD which results in a computationally efficient method. The proposed
algorithm is compared with Runnalls' and Williams' methods in two numerical
examples, using both simulated and real world data. The results indicate that
the performance and computational complexity of the proposed approach make it
an efficient alternative to existing mixture reduction methods.","['Tohid Ardeshiri', 'Umut Orguner', 'Emre √ñzkan']","['stat.ML', 'cs.CV', 'cs.LG', 'cs.RO', 'cs.SY']",2015-08-22 13:41:17+00:00
http://arxiv.org/abs/1508.05495v1,Bayesian Hypothesis Testing for Block Sparse Signal Recovery,"This letter presents a novel Block Bayesian Hypothesis Testing Algorithm
(Block-BHTA) for reconstructing block sparse signals with unknown block
structures. The Block-BHTA comprises the detection and recovery of the
supports, and the estimation of the amplitudes of the block sparse signal. The
support detection and recovery is performed using a Bayesian hypothesis
testing. Then, based on the detected and reconstructed supports, the nonzero
amplitudes are estimated by linear MMSE. The effectiveness of Block-BHTA is
demonstrated by numerical experiments.","['Mehdi Korki', 'Hadi Zayyani', 'Jingxin Zhang']","['stat.ML', 'cs.IT', 'math.IT']",2015-08-22 10:58:25+00:00
http://arxiv.org/abs/1508.05249v1,Representation of Quasi-Monotone Functionals by Families of Separating Hyperplanes,"We characterize when the level sets of a continuous quasi-monotone functional
defined on a suitable convex subset of a normed space can be uniquely
represented by a family of bounded continuous functionals. Furthermore, we
investigate how regularly these functionals depend on the parameterizing level.
Finally, we show how this question relates to the recent problem of property
elicitation that simultaneously attracted interest in machine learning,
statistical evaluation of forecasts, and finance.",['Ingo Steinwart'],"['math.OC', 'math.FA', 'math.ST', 'stat.ML', 'stat.TH']",2015-08-21 11:58:42+00:00
http://arxiv.org/abs/1508.05243v2,Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures,"Coresets are efficient representations of data sets such that models trained
on the coreset are provably competitive with models trained on the original
data set. As such, they have been successfully used to scale up clustering
models such as K-Means and Gaussian mixture models to massive data sets.
However, until now, the algorithms and the corresponding theory were usually
specific to each clustering problem.
  We propose a single, practical algorithm to construct strong coresets for a
large class of hard and soft clustering problems based on Bregman divergences.
This class includes hard clustering with popular distortion measures such as
the Squared Euclidean distance, the Mahalanobis distance, KL-divergence and
Itakura-Saito distance. The corresponding soft clustering problems are directly
related to popular mixture models due to a dual relationship between Bregman
divergences and Exponential family distributions. Our theoretical results
further imply a randomized polynomial-time approximation scheme for hard
clustering. We demonstrate the practicality of the proposed algorithm in an
empirical evaluation.","['Mario Lucic', 'Olivier Bachem', 'Andreas Krause']","['stat.ML', 'cs.LG']",2015-08-21 11:31:04+00:00
http://arxiv.org/abs/1508.05170v2,Adaptive Online Learning,"We propose a general framework for studying adaptive regret bounds in the
online learning framework, including model selection bounds and data-dependent
bounds. Given a data- or model-dependent bound we ask, ""Does there exist some
algorithm achieving this bound?"" We show that modifications to recently
introduced sequential complexity measures can be used to answer this question
by providing sufficient conditions under which adaptive rates can be achieved.
In particular each adaptive rate induces a set of so-called offset complexity
measures, and obtaining small upper bounds on these quantities is sufficient to
demonstrate achievability. A cornerstone of our analysis technique is the use
of one-sided tail inequalities to bound suprema of offset random processes.
  Our framework recovers and improves a wide variety of adaptive bounds
including quantile bounds, second-order data-dependent bounds, and small loss
bounds. In addition we derive a new type of adaptive bound for online linear
optimization based on the spectral norm, as well as a new online PAC-Bayes
theorem that holds for countably infinite sets.","['Dylan J. Foster', 'Alexander Rakhlin', 'Karthik Sridharan']","['cs.LG', 'stat.ML']",2015-08-21 03:44:43+00:00
http://arxiv.org/abs/1508.05383v1,On Monotonicity of the Optimal Transmission Policy in Cross-layer Adaptive m-QAM Modulation,"This paper considers a cross-layer adaptive modulation system that is modeled
as a Markov decision process (MDP). We study how to utilize the monotonicity of
the optimal transmission policy to relieve the computational complexity of
dynamic programming (DP). In this system, a scheduler controls the bit rate of
the m-quadrature amplitude modulation (m-QAM) in order to minimize the
long-term losses incurred by the queue overflow in the data link layer and the
transmission power consumption in the physical layer. The work is done in two
steps. Firstly, we observe the L-natural-convexity and submodularity of DP to
prove that the optimal policy is always nondecreasing in queue occupancy/state
and derive the sufficient condition for it to be nondecreasing in both queue
and channel states. We also show that, due to the L-natural-convexity of DP,
the variation of the optimal policy in queue state is restricted by a bounded
marginal effect: The increment of the optimal policy between adjacent queue
states is no greater than one. Secondly, we use the monotonicity results to
present two low complexity algorithms: monotonic policy iteration (MPI) based
on L-natural-convexity and discrete simultaneous perturbation stochastic
approximation (DSPSA). We run experiments to show that the time complexity of
MPI based on L-natural-convexity is much lower than that of DP and the
conventional MPI that is based on submodularity and DSPSA is able to adaptively
track the optimal policy when the system parameters change.","['Ni Ding', 'Parastoo Sadeghi', 'Rodney A. Kennedy']","['stat.ML', 'cs.IT', 'math.IT']",2015-08-21 03:23:21+00:00
http://arxiv.org/abs/1508.05003v1,AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization,"We study distributed stochastic convex optimization under the delayed
gradient model where the server nodes perform parameter updates, while the
worker nodes compute stochastic gradients. We discuss, analyze, and experiment
with a setup motivated by the behavior of real-world distributed computation
networks, where the machines are differently slow at different time. Therefore,
we allow the parameter updates to be sensitive to the actual delays
experienced, rather than to worst-case bounds on the maximum delay. This
sensitivity leads to larger stepsizes, that can help gain rapid initial
convergence without having to wait too long for slower machines, while
maintaining the same asymptotic complexity. We obtain encouraging improvements
to overall convergence for distributed experiments on real datasets with up to
billions of examples and features.","['Suvrit Sra', 'Adams Wei Yu', 'Mu Li', 'Alexander J. Smola']","['stat.ML', 'cs.LG', 'math.OC']",2015-08-20 15:11:11+00:00
http://arxiv.org/abs/1508.04999v3,A Deep Bag-of-Features Model for Music Auto-Tagging,"Feature learning and deep learning have drawn great attention in recent years
as a way of transforming input data into more effective representations using
learning algorithms. Such interest has grown in the area of music information
retrieval (MIR) as well, particularly in music audio classification tasks such
as auto-tagging. In this paper, we present a two-stage learning model to
effectively predict multiple labels from music audio. The first stage learns to
project local spectral patterns of an audio track onto a high-dimensional
sparse space in an unsupervised manner and summarizes the audio track as a
bag-of-features. The second stage successively performs the unsupervised
learning on the bag-of-features in a layer-by-layer manner to initialize a deep
neural network and finally fine-tunes it with the tag labels. Through the
experiment, we rigorously examine training choices and tuning parameters, and
show that the model achieves high performance on Magnatagatune, a popularly
used dataset in music auto-tagging.","['Juhan Nam', 'Jorge Herrera', 'Kyogu Lee']","['cs.LG', 'cs.SD', 'stat.ML']",2015-08-20 14:38:56+00:00
http://arxiv.org/abs/1508.04945v2,DeepWriterID: An End-to-end Online Text-independent Writer Identification System,"Owing to the rapid growth of touchscreen mobile terminals and pen-based
interfaces, handwriting-based writer identification systems are attracting
increasing attention for personal authentication, digital forensics, and other
applications. However, most studies on writer identification have not been
satisfying because of the insufficiency of data and difficulty of designing
good features under various conditions of handwritings. Hence, we introduce an
end-to-end system, namely DeepWriterID, employed a deep convolutional neural
network (CNN) to address these problems. A key feature of DeepWriterID is a new
method we are proposing, called DropSegment. It designs to achieve data
augmentation and improve the generalized applicability of CNN. For sufficient
feature representation, we further introduce path signature feature maps to
improve performance. Experiments were conducted on the NLPR handwriting
database. Even though we only use pen-position information in the pen-down
state of the given handwriting samples, we achieved new state-of-the-art
identification rates of 95.72% for Chinese text and 98.51% for English text.","['Weixin Yang', 'Lianwen Jin', 'Manfei Liu']","['cs.CV', 'cs.LG', 'stat.ML']",2015-08-20 10:39:19+00:00
http://arxiv.org/abs/1508.04912v1,The ABACOC Algorithm: a Novel Approach for Nonparametric Classification of Data Streams,"Stream mining poses unique challenges to machine learning: predictive models
are required to be scalable, incrementally trainable, must remain bounded in
size (even when the data stream is arbitrarily long), and be nonparametric in
order to achieve high accuracy even in complex and dynamic environments.
Moreover, the learning system must be parameterless ---traditional tuning
methods are problematic in streaming settings--- and avoid requiring prior
knowledge of the number of distinct class labels occurring in the stream. In
this paper, we introduce a new algorithmic approach for nonparametric learning
in data streams. Our approach addresses all above mentioned challenges by
learning a model that covers the input space using simple local classifiers.
The distribution of these classifiers dynamically adapts to the local (unknown)
complexity of the classification problem, thus achieving a good balance between
model complexity and predictive accuracy. We design four variants of our
approach of increasing adaptivity. By means of an extensive empirical
evaluation against standard nonparametric baselines, we show state-of-the-art
results in terms of accuracy versus model size. For the variant that imposes a
strict bound on the model size, we show better performance against all other
methods measured at the same model size value. Our empirical analysis is
complemented by a theoretical performance guarantee which does not rely on any
stochastic assumption on the source generating the stream.","['Rocco De Rosa', 'Francesco Orabona', 'Nicol√≤ Cesa-Bianchi']","['stat.ML', 'cs.LG']",2015-08-20 08:15:08+00:00
http://arxiv.org/abs/1508.04904v1,Review and Perspective for Distance Based Trajectory Clustering,"In this paper we tackle the issue of clustering trajectories of geolocalized
observations. Using clustering technics based on the choice of a distance
between the observations, we first provide a comprehensive review of the
different distances used in the literature to compare trajectories. Then based
on the limitations of these methods, we introduce a new distance : Symmetrized
Segment-Path Distance (SSPD). We finally compare this new distance to the
others according to their corresponding clustering results obtained using both
hierarchical clustering and affinity propagation methods.","['Philippe Besse', 'Brendan Guillouet', 'Jean-Michel Loubes', 'Royer Fran√ßois']","['stat.ML', 'cs.LG', 'stat.AP']",2015-08-20 07:46:15+00:00
http://arxiv.org/abs/1508.04887v1,Multi-criteria Similarity-based Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit
anomalous behavior, often referred to as anomaly detection. Similarity-based
anomaly detection algorithms detect abnormally large amounts of similarity or
dissimilarity, e.g.~as measured by nearest neighbor Euclidean distances between
a test sample and the training samples. In many application domains there may
not exist a single dissimilarity measure that captures all possible anomalous
patterns. In such cases, multiple dissimilarity measures can be defined,
including non-metric measures, and one can test for anomalies by scalarizing
using a non-negative linear combination of them. If the relative importance of
the different dissimilarity measures are not known in advance, as in many
anomaly detection applications, the anomaly detection algorithm may need to be
executed multiple times with different choices of weights in the linear
combination. In this paper, we propose a method for similarity-based anomaly
detection using a novel multi-criteria dissimilarity measure, the Pareto depth.
The proposed Pareto depth analysis (PDA) anomaly detection algorithm uses the
concept of Pareto optimality to detect anomalies under multiple criteria
without having to run an algorithm multiple times with different choices of
weights. The proposed PDA approach is provably better than using linear
combinations of the criteria and shows superior performance on experiments with
synthetic and real data sets.","['Ko-Jen Hsiao', 'Kevin S. Xu', 'Jeff Calder', 'Alfred O. Hero III']","['cs.CV', 'cs.LG', 'stat.ML']",2015-08-20 06:25:52+00:00
http://arxiv.org/abs/1508.04757v1,Time Series Clustering via Community Detection in Networks,"In this paper, we propose a technique for time series clustering using
community detection in complex networks. Firstly, we present a method to
transform a set of time series into a network using different distance
functions, where each time series is represented by a vertex and the most
similar ones are connected. Then, we apply community detection algorithms to
identify groups of strongly connected vertices (called a community) and,
consequently, identify time series clusters. Still in this paper, we make a
comprehensive analysis on the influence of various combinations of time series
distance functions, network generation methods and community detection
techniques on clustering results. Experimental study shows that the proposed
network-based approach achieves better results than various classic or
up-to-date clustering techniques under consideration. Statistical tests confirm
that the proposed method outperforms some classic clustering algorithms, such
as $k$-medoids, diana, median-linkage and centroid-linkage in various data
sets. Interestingly, the proposed method can effectively detect shape patterns
presented in time series due to the topological structure of the underlying
network constructed in the clustering process. At the same time, other
techniques fail to identify such patterns. Moreover, the proposed method is
robust enough to group time series presenting similar pattern but with time
shifts and/or amplitude variations. In summary, the main point of the proposed
method is the transformation of time series from time-space domain to
topological domain. Therefore, we hope that our approach contributes not only
for time series clustering, but also for general time series analysis tasks.","['Leonardo N. Ferreira', 'Liang Zhao']","['stat.ML', 'cs.LG', 'cs.SI']",2015-08-19 19:55:08+00:00
http://arxiv.org/abs/1508.04559v1,Introduction to Cross-Entropy Clustering The R Package CEC,"The R Package CEC performs clustering based on the cross-entropy clustering
(CEC) method, which was recently developed with the use of information theory.
The main advantage of CEC is that it combines the speed and simplicity of
$k$-means with the ability to use various Gaussian mixture models and reduce
unnecessary clusters. In this work we present a practical tutorial to CEC based
on the R Package CEC. Functions are provided to encompass the whole process of
clustering.","['Jacek Tabor', 'Przemys≈Çaw Spurek', 'Konrad Kamieniecki', 'Marek ≈ömieja', 'Krzysztof Misztal']","['cs.LG', 'stat.ME', 'stat.ML']",2015-08-19 08:13:56+00:00
http://arxiv.org/abs/1508.04556v1,Spatio-temporal Spike and Slab Priors for Multiple Measurement Vector Problems,"We are interested in solving the multiple measurement vector (MMV) problem
for instances, where the underlying sparsity pattern exhibit spatio-temporal
structure motivated by the electroencephalogram (EEG) source localization
problem. We propose a probabilistic model that takes this structure into
account by generalizing the structured spike and slab prior and the associated
Expectation Propagation inference scheme. Based on numerical experiments, we
demonstrate the viability of the model and the approximate inference scheme.","['Michael Riis Andersen', 'Ole Winther', 'Lars Kai Hansen']",['stat.ML'],2015-08-19 08:09:06+00:00
http://arxiv.org/abs/1508.04554v1,Mining Brain Networks using Multiple Side Views for Neurological Disorder Identification,"Mining discriminative subgraph patterns from graph data has attracted great
interest in recent years. It has a wide variety of applications in disease
diagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the
graph representation alone. However, in many real-world applications, the side
information is available along with the graph data. For example, for
neurological disorder identification, in addition to the brain networks derived
from neuroimaging data, hundreds of clinical, immunologic, serologic and
cognitive measures may also be documented for each subject. These measures
compose multiple side views encoding a tremendous amount of supplemental
information for diagnostic purposes, yet are often ignored. In this paper, we
study the problem of discriminative subgraph selection using multiple side
views and propose a novel solution to find an optimal set of subgraph features
for graph classification by exploring a plurality of side views. We derive a
feature evaluation criterion, named gSide, to estimate the usefulness of
subgraph patterns based upon side views. Then we develop a branch-and-bound
algorithm, called gMSV, to efficiently search for optimal subgraph features by
integrating the subgraph mining process and the procedure of discriminative
feature selection. Empirical studies on graph classification tasks for
neurological disorders using brain networks demonstrate that subgraph patterns
selected by the multi-side-view guided subgraph selection approach can
effectively boost graph classification performances and are relevant to disease
diagnosis.","['Bokai Cao', 'Xiangnan Kong', 'Jingyuan Zhang', 'Philip S. Yu', 'Ann B. Ragin']","['cs.LG', 'cs.CV', 'cs.CY', 'stat.AP', 'stat.ML']",2015-08-19 07:51:14+00:00
http://arxiv.org/abs/1508.04486v1,A Dictionary Learning Approach for Factorial Gaussian Models,"In this paper, we develop a parameter estimation method for factorially
parametrized models such as Factorial Gaussian Mixture Model and Factorial
Hidden Markov Model. Our contributions are two-fold. First, we show that the
emission matrix of the standard Factorial Model is unidentifiable even if the
true assignment matrix is known. Secondly, we address the issue of
identifiability by making a one component sharing assumption and derive a
parameter learning algorithm for this case. Our approach is based on a
dictionary learning problem of the form $X = O R$, where the goal is to learn
the dictionary $O$ given the data matrix $X$. We argue that due to the specific
structure of the activation matrix $R$ in the shared component factorial
mixture model, and an incoherence assumption on the shared component, it is
possible to extract the columns of the $O$ matrix without the need for
alternating between the estimation of $O$ and $R$.","['Y. Cem Subakan', 'Johannes Traa', 'Paris Smaragdis', 'Noah Stein']","['cs.LG', 'stat.ML']",2015-08-18 23:47:28+00:00
http://arxiv.org/abs/1508.04467v1,Robust Subspace Clustering via Smoothed Rank Approximation,"Matrix rank minimizing subject to affine constraints arises in many
application areas, ranging from signal processing to machine learning. Nuclear
norm is a convex relaxation for this problem which can recover the rank exactly
under some restricted and theoretically interesting conditions. However, for
many real-world applications, nuclear norm approximation to the rank function
can only produce a result far from the optimum. To seek a solution of higher
accuracy than the nuclear norm, in this paper, we propose a rank approximation
based on Logarithm-Determinant. We consider using this rank approximation for
subspace clustering application. Our framework can model different kinds of
errors and noise. Effective optimization strategy is developed with theoretical
guarantee to converge to a stationary point. The proposed method gives
promising results on face clustering and motion segmentation tasks compared to
the state-of-the-art subspace clustering algorithms.","['Zhao Kang', 'Chong Peng', 'Qiang Cheng']","['cs.CV', 'cs.IT', 'cs.LG', 'cs.NA', 'math.IT', 'stat.ML']",2015-08-18 21:54:03+00:00
http://arxiv.org/abs/1508.04422v3,Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural Networks,"Several popular graph embedding techniques for representation learning and
dimensionality reduction rely on performing computationally expensive
eigendecompositions to derive a nonlinear transformation of the input data
space. The resulting eigenvectors encode the embedding coordinates for the
training samples only, and so the embedding of novel data samples requires
further costly computation. In this paper, we present a method for the
out-of-sample extension of graph embeddings using deep neural networks (DNN) to
parametrically approximate these nonlinear maps. Compared with traditional
nonparametric out-of-sample extension methods, we demonstrate that the DNNs can
generalize with equal or better fidelity and require orders of magnitude less
computation at test time. Moreover, we find that unsupervised pretraining of
the DNNs improves optimization for larger network sizes, thus removing
sensitivity to model selection.","['Aren Jansen', 'Gregory Sell', 'Vince Lyzinski']","['stat.ML', 'cs.LG', 'cs.NE', 'stat.ME']",2015-08-18 19:47:31+00:00
http://arxiv.org/abs/1508.04409v2,ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R,"We introduce the C++ application and R package ranger. The software is a fast
implementation of random forests for high dimensional data. Ensembles of
classification, regression and survival trees are supported. We describe the
implementation, provide examples, validate the package with a reference
implementation, and compare runtime and memory usage with other
implementations. The new software proves to scale best with the number of
features, samples, trees, and features tried for splitting. Finally, we show
that ranger is the fastest and most memory efficient implementation of random
forests to analyze data on the scale of a genome-wide association study.","['Marvin N. Wright', 'Andreas Ziegler']","['stat.ML', 'stat.CO']",2015-08-18 18:47:10+00:00
http://arxiv.org/abs/1508.04319v1,Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo,"We present a novel approach for fully non-stationary Gaussian process
regression (GPR), where all three key parameters -- noise variance, signal
variance and lengthscale -- can be simultaneously input-dependent. We develop
gradient-based inference methods to learn the unknown function and the
non-stationary model parameters, without requiring any model approximations. We
propose to infer full parameter posterior with Hamiltonian Monte Carlo (HMC),
which conveniently extends the analytical gradient-based GPR learning by
guiding the sampling with model gradients. We also learn the MAP solution from
the posterior by gradient ascent. In experiments on several synthetic datasets
and in modelling of temporal gene expression, the nonstationary GPR is shown to
be necessary for modeling realistic input-dependent dynamics, while it performs
comparably to conventional stationary or previous non-stationary GPR models
otherwise.","['Markus Heinonen', 'Henrik Mannerstr√∂m', 'Juho Rousu', 'Samuel Kaski', 'Harri L√§hdesm√§ki']",['stat.ML'],2015-08-18 13:48:02+00:00
http://arxiv.org/abs/1508.04306v1,Deep clustering: Discriminative embeddings for segmentation and separation,"We address the problem of acoustic source separation in a deep learning
framework we call ""deep clustering."" Rather than directly estimating signals or
masking functions, we train a deep network to produce spectrogram embeddings
that are discriminative for partition labels given in training data. Previous
deep network approaches provide great advantages in terms of learning power and
speed, but previously it has been unclear how to use them to separate signals
in a class-independent way. In contrast, spectral clustering approaches are
flexible with respect to the classes and number of items to be segmented, but
it has been unclear how to leverage the learning power and speed of deep
networks. To obtain the best of both worlds, we use an objective function that
to train embeddings that yield a low-rank approximation to an ideal pairwise
affinity matrix, in a class-independent way. This avoids the high cost of
spectral factorization and instead produces compact clusters that are amenable
to simple clustering methods. The segmentations are therefore implicitly
encoded in the embeddings, and can be ""decoded"" by clustering. Preliminary
experiments show that the proposed method can separate speech: when trained on
spectrogram features containing mixtures of two speakers, and tested on
mixtures of a held-out set of speakers, it can infer masking functions that
improve signal quality by around 6dB. We show that the model can generalize to
three-speaker mixtures despite training only on two-speaker mixtures. The
framework can be used without class labels, and therefore has the potential to
be trained on a diverse set of sound types, and to generalize to novel sources.
We hope that future work will lead to segmentation of arbitrary sounds, with
extensions to microphone array methods as well as image segmentation and other
domains.","['John R. Hershey', 'Zhuo Chen', 'Jonathan Le Roux', 'Shinji Watanabe']","['cs.NE', 'cs.LG', 'stat.ML']",2015-08-18 13:12:34+00:00
http://arxiv.org/abs/1508.04211v1,Scalable Bayesian Non-Negative Tensor Factorization for Massive Count Data,"We present a Bayesian non-negative tensor factorization model for
count-valued tensor data, and develop scalable inference algorithms (both batch
and online) for dealing with massive tensors. Our generative model can handle
overdispersed counts as well as infer the rank of the decomposition. Moreover,
leveraging a reparameterization of the Poisson distribution as a multinomial
facilitates conjugacy in the model and enables simple and efficient Gibbs
sampling and variational Bayes (VB) inference updates, with a computational
cost that only depends on the number of nonzeros in the tensor. The model also
provides a nice interpretability for the factors; in our model, each factor
corresponds to a ""topic"". We develop a set of online inference algorithms that
allow further scaling up the model to massive tensors, for which batch
inference methods may be infeasible. We apply our framework on diverse
real-world applications, such as \emph{multiway} topic modeling on a scientific
publications database, analyzing a political science data set, and analyzing a
massive household transactions data set.","['Changwei Hu', 'Piyush Rai', 'Changyou Chen', 'Matthew Harding', 'Lawrence Carin']","['stat.ML', 'cs.LG']",2015-08-18 04:28:56+00:00
http://arxiv.org/abs/1508.04210v1,Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors,"We present a scalable Bayesian model for low-rank factorization of massive
tensors with binary observations. The proposed model has the following key
properties: (1) in contrast to the models based on the logistic or probit
likelihood, using a zero-truncated Poisson likelihood for binary data allows
our model to scale up in the number of \emph{ones} in the tensor, which is
especially appealing for massive but sparse binary tensors; (2)
side-information in form of binary pairwise relationships (e.g., an adjacency
network) between objects in any tensor mode can also be leveraged, which can be
especially useful in ""cold-start"" settings; and (3) the model admits simple
Bayesian inference via batch, as well as \emph{online} MCMC; the latter allows
scaling up even for \emph{dense} binary data (i.e., when the number of ones in
the tensor/network is also massive). In addition, non-negative factor matrices
in our model provide easy interpretability, and the tensor rank can be inferred
from the data. We evaluate our model on several large-scale real-world binary
tensors, achieving excellent computational scalability, and also demonstrate
its usefulness in leveraging side-information provided in form of
mode-network(s).","['Changwei Hu', 'Piyush Rai', 'Lawrence Carin']","['stat.ML', 'cs.LG']",2015-08-18 04:24:24+00:00
http://arxiv.org/abs/1508.04065v1,A Deep Learning Approach to Structured Signal Recovery,"In this paper, we develop a new framework for sensing and recovering
structured signals. In contrast to compressive sensing (CS) systems that employ
linear measurements, sparse representations, and computationally complex
convex/greedy algorithms, we introduce a deep learning framework that supports
both linear and mildly nonlinear measurements, that learns a structured
representation from training data, and that efficiently computes a signal
estimate. In particular, we apply a stacked denoising autoencoder (SDA), as an
unsupervised feature learner. SDA enables us to capture statistical
dependencies between the different elements of certain signals and improve
signal recovery performance as compared to the CS approach.","['Ali Mousavi', 'Ankit B. Patel', 'Richard G. Baraniuk']","['cs.LG', 'stat.ML']",2015-08-17 15:46:09+00:00
http://arxiv.org/abs/1508.04035v1,A Generative Model for Multi-Dialect Representation,"In the era of deep learning several unsupervised models have been developed
to capture the key features in unlabeled handwritten data. Popular among them
is the Restricted Boltzmann Machines RBM. However, due to the novelty in
handwritten multidialect data, the RBM may fail to generate an efficient
representation. In this paper we propose a generative model, the Mode
Synthesizing Machine MSM for on-line representation of real life handwritten
multidialect language data. The MSM takes advantage of the hierarchical
representation of the modes of a data distribution using a two-point error
update to learn a sequence of representative multidialects in a generative way.
Experiments were performed to evaluate the performance of the MSM over the RBM
with the former attaining much lower error values than the latter on both
independent and mixed data set.",['Emmanuel N. Osegi'],"['cs.CV', 'cs.LG', 'stat.ML']",2015-08-17 14:05:44+00:00
http://arxiv.org/abs/1508.03826v1,A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution,"Most existing word embedding methods can be categorized into Neural Embedding
Models and Matrix Factorization (MF)-based methods. However some models are
opaque to probabilistic interpretation, and MF-based methods, typically solved
using Singular Value Decomposition (SVD), may incur loss of corpus information.
In addition, it is desirable to incorporate global latent factors, such as
topics, sentiments or writing styles, into the word embedding model. Since
generative models provide a principled way to incorporate latent factors, we
propose a generative word embedding model, which is easy to interpret, and can
serve as a basis of more sophisticated latent factor models. The model
inference reduces to a low rank weighted positive semidefinite approximation
problem. Its optimization is approached by eigendecomposition on a submatrix,
followed by online blockwise regression, which is scalable and avoids the
information loss in SVD. In experiments on 7 common benchmark datasets, our
vectors are competitive to word2vec, and better than other MF-based methods.","['Shaohua Li', 'Jun Zhu', 'Chunyan Miao']","['cs.CL', 'cs.LG', 'stat.ML']",2015-08-16 14:12:17+00:00
http://arxiv.org/abs/1508.03712v1,Towards an Axiomatic Approach to Hierarchical Clustering of Measures,"We propose some axioms for hierarchical clustering of probability measures
and investigate their ramifications. The basic idea is to let the user
stipulate the clusters for some elementary measures. This is done without the
need of any notion of metric, similarity or dissimilarity. Our main results
then show that for each suitable choice of user-defined clustering on
elementary measures we obtain a unique notion of clustering on a large set of
distributions satisfying a set of additivity and continuity axioms. We
illustrate the developed theory by numerous examples including some with and
some without a density.","['Philipp Thomann', 'Ingo Steinwart', 'Nico Schmid']","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH', 'Primary 62H30, Secondary 91C20, 62G07']",2015-08-15 09:07:01+00:00
http://arxiv.org/abs/1508.03666v1,Unbounded Bayesian Optimization via Regularization,"Bayesian optimization has recently emerged as a popular and efficient tool
for global optimization and hyperparameter tuning. Currently, the established
Bayesian optimization practice requires a user-defined bounding box which is
assumed to contain the optimizer. However, when little is known about the
probed objective function, it can be difficult to prescribe such bounds. In
this work we modify the standard Bayesian optimization framework in a
principled way to allow automatic resizing of the search space. We introduce
two alternative methods and compare them on two common synthetic benchmarking
test functions as well as the tasks of tuning the stochastic gradient descent
optimizer of a multi-layered perceptron and a convolutional neural network on
MNIST.","['Bobak Shahriari', 'Alexandre Bouchard-C√¥t√©', 'Nando de Freitas']",['stat.ML'],2015-08-14 21:10:46+00:00
http://arxiv.org/abs/1508.03411v2,Emphatic TD Bellman Operator is a Contraction,"Recently, \citet{SuttonMW15} introduced the emphatic temporal differences
(ETD) algorithm for off-policy evaluation in Markov decision processes. In this
short note, we show that the projected fixed-point equation that underlies ETD
involves a contraction operator, with a $\sqrt{\gamma}$-contraction modulus
(where $\gamma$ is the discount factor). This allows us to provide error bounds
on the approximation error of ETD. To our knowledge, these are the first error
bounds for an off-policy evaluation algorithm under general target and behavior
policies.","['Assaf Hallak', 'Aviv Tamar', 'Shie Mannor']","['stat.ML', 'cs.LG']",2015-08-14 03:34:10+00:00
http://arxiv.org/abs/1508.03395v1,Information-theoretic Bounds on Matrix Completion under Union of Subspaces Model,"In this short note we extend some of the recent results on matrix completion
under the assumption that the columns of the matrix can be grouped (clustered)
into subspaces (not necessarily disjoint or independent). This model deviates
from the typical assumption prevalent in the literature dealing with
compression and recovery for big-data applications. The results have a direct
bearing on the problem of subspace clustering under missing or incomplete
information.","['Vaneet Aggarwal', 'Shuchin Aeron']","['cs.IT', 'math.IT', 'stat.ML']",2015-08-14 01:09:00+00:00
http://arxiv.org/abs/1508.03390v3,Doubly Stochastic Primal-Dual Coordinate Method for Bilinear Saddle-Point Problem,"We propose a doubly stochastic primal-dual coordinate optimization algorithm
for empirical risk minimization, which can be formulated as a bilinear
saddle-point problem. In each iteration, our method randomly samples a block of
coordinates of the primal and dual solutions to update. The linear convergence
of our method could be established in terms of 1) the distance from the current
iterate to the optimal solution and 2) the primal-dual objective gap. We show
that the proposed method has a lower overall complexity than existing
coordinate methods when either the data matrix has a factorized structure or
the proximal mapping on each block is computationally expensive, e.g.,
involving an eigenvalue decomposition. The efficiency of the proposed method is
confirmed by empirical studies on several real applications, such as the
multi-task large margin nearest neighbor problem.","['Adams Wei Yu', 'Qihang Lin', 'Tianbao Yang']","['cs.LG', 'stat.ML']",2015-08-14 00:22:45+00:00
http://arxiv.org/abs/1508.03332v1,Dimensionality Reduction of Collective Motion by Principal Manifolds,"While the existence of low-dimensional embedding manifolds has been shown in
patterns of collective motion, the current battery of nonlinear dimensionality
reduction methods are not amenable to the analysis of such manifolds. This is
mainly due to the necessary spectral decomposition step, which limits control
over the mapping from the original high-dimensional space to the embedding
space. Here, we propose an alternative approach that demands a two-dimensional
embedding which topologically summarizes the high-dimensional data. In this
sense, our approach is closely related to the construction of one-dimensional
principal curves that minimize orthogonal error to data points subject to
smoothness constraints. Specifically, we construct a two-dimensional principal
manifold directly in the high-dimensional space using cubic smoothing splines,
and define the embedding coordinates in terms of geodesic distances. Thus, the
mapping from the high-dimensional data to the manifold is defined in terms of
local coordinates. Through representative examples, we show that compared to
existing nonlinear dimensionality reduction methods, the principal manifold
retains the original structure even in noisy and sparse datasets. The principal
manifold finding algorithm is applied to configurations obtained from a
dynamical system of multiple agents simulating a complex maneuver called
predator mobbing, and the resulting two-dimensional embedding is compared with
that of a well-established nonlinear dimensionality reduction method.","['Kelum Gajamannage', 'Sachit Butail', 'Maurizio Porfiri', 'Erik M. Bollt']","['math.NA', 'cs.LG', 'cs.MA', 'math.DS', 'stat.ML', '57M60 (Primary), 37E99, 57N35 (Secondary)']",2015-08-13 21:07:51+00:00
http://arxiv.org/abs/1508.03337v5,A Randomized Rounding Algorithm for Sparse PCA,"We present and analyze a simple, two-step algorithm to approximate the
optimal solution of the sparse PCA problem. Our approach first solves a L1
penalized version of the NP-hard sparse PCA optimization problem and then uses
a randomized rounding strategy to sparsify the resulting dense solution. Our
main theoretical result guarantees an additive error approximation and provides
a tradeoff between sparsity and accuracy. Our experimental evaluation indicates
that our approach is competitive in practice, even compared to state-of-the-art
toolboxes such as Spasm.","['Kimon Fountoulakis', 'Abhisek Kundu', 'Eugenia-Maria Kontopoulou', 'Petros Drineas']","['cs.DS', 'cs.LG', 'stat.ML']",2015-08-13 20:06:59+00:00
http://arxiv.org/abs/1508.03106v2,Neyman-Pearson Classification under High-Dimensional Settings,"Most existing binary classification methods target on the optimization of the
overall classification risk and may fail to serve some real-world applications
such as cancer diagnosis, where users are more concerned with the risk of
misclassifying one specific class than the other. Neyman-Pearson (NP) paradigm
was introduced in this context as a novel statistical framework for handling
asymmetric type I/II error priorities. It seeks classifiers with a minimal type
II error and a constrained type I error under a user specified level. This
article is the first attempt to construct classifiers with guaranteed
theoretical performance under the NP paradigm in high-dimensional settings.
Based on the fundamental Neyman-Pearson Lemma, we used a plug-in approach to
construct NP-type classifiers for Naive Bayes models. The proposed classifiers
satisfy the NP oracle inequalities, which are natural NP paradigm counterparts
of the oracle inequalities in classical binary classification. Besides their
desirable theoretical properties, we also demonstrated their numerical
advantages in prioritized error control via both simulation and real data
studies.","['Anqi Zhao', 'Yang Feng', 'Lie Wang', 'Xin Tong']",['stat.ML'],2015-08-13 02:47:53+00:00
http://arxiv.org/abs/1508.02933v3,No Regret Bound for Extreme Bandits,"Algorithms for hyperparameter optimization abound, all of which work well
under different and often unverifiable assumptions. Motivated by the general
challenge of sequentially choosing which algorithm to use, we study the more
specific task of choosing among distributions to use for random hyperparameter
optimization. This work is naturally framed in the extreme bandit setting,
which deals with sequentially choosing which distribution from a collection to
sample in order to minimize (maximize) the single best cost (reward). Whereas
the distributions in the standard bandit setting are primarily characterized by
their means, a number of subtleties arise when we care about the minimal cost
as opposed to the average cost. For example, there may not be a well-defined
""best"" distribution as there is in the standard bandit setting. The best
distribution depends on the rewards that have been obtained and on the
remaining time horizon. Whereas in the standard bandit setting, it is sensible
to compare policies with an oracle which plays the single best arm, in the
extreme bandit setting, there are multiple sensible oracle models. We define a
sensible notion of ""extreme regret"" in the extreme bandit setting, which
parallels the concept of regret in the standard bandit setting. We then prove
that no policy can asymptotically achieve no extreme regret.","['Robert Nishihara', 'David Lopez-Paz', 'L√©on Bottou']","['stat.ML', 'cs.LG', 'math.OC', 'math.ST', 'stat.TH']",2015-08-12 14:31:49+00:00
http://arxiv.org/abs/1508.02925v1,RCR: Robust Compound Regression for Robust Estimation of Errors-in-Variables Model,"The errors-in-variables (EIV) regression model, being more realistic by
accounting for measurement errors in both the dependent and the independent
variables, is widely adopted in applied sciences. The traditional EIV model
estimators, however, can be highly biased by outliers and other departures from
the underlying assumptions. In this paper, we develop a novel nonparametric
regression approach - the robust compound regression (RCR) analysis method for
the robust estimation of EIV models. We first introduce a robust and efficient
estimator called least sine squares (LSS). Taking full advantage of both the
new LSS method and the compound regression analysis method developed in our own
group, we subsequently propose the RCR approach as a generalization of those
two, which provides a robust counterpart of the entire class of the maximum
likelihood estimation (MLE) solutions of the EIV model, in a 1-1 mapping.
Technically, our approach gives users the flexibility to select from a class of
RCR estimates the optimal one with a predefined regression efficiency criterion
satisfied. Simulation studies and real-life examples are provided to illustrate
the effectiveness of the RCR approach.","['Hao Han', 'Wei Zhu']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",2015-08-12 14:19:24+00:00
http://arxiv.org/abs/1508.02905v2,Bayesian Dropout,"Dropout has recently emerged as a powerful and simple method for training
neural networks preventing co-adaptation by stochastically omitting neurons.
Dropout is currently not grounded in explicit modelling assumptions which so
far has precluded its adoption in Bayesian modelling. Using Bayesian entropic
reasoning we show that dropout can be interpreted as optimal inference under
constraints. We demonstrate this on an analytically tractable regression model
providing a Bayesian interpretation of its mechanism for regularizing and
preventing co-adaptation as well as its connection to other Bayesian
techniques. We also discuss two general approximate techniques for applying
Bayesian dropout for general models, one based on an analytical approximation
and the other on stochastic variational techniques. These techniques are then
applied to a Baysian logistic regression problem and are shown to improve
performance as the model become more misspecified. Our framework roots dropout
as a theoretically justified and practical tool for statistical modelling
allowing Bayesians to tap into the benefits of dropout training.","['Tue Herlau', 'Morten M√∏rup', 'Mikkel N. Schmidt']","['stat.ML', '68T37', 'I.5.1']",2015-08-12 13:09:19+00:00
http://arxiv.org/abs/1508.02884v2,Towards Real-time Customer Experience Prediction for Telecommunication Operators,"Telecommunications operators (telcos) traditional sources of income, voice
and SMS, are shrinking due to customers using over-the-top (OTT) applications
such as WhatsApp or Viber. In this challenging environment it is critical for
telcos to maintain or grow their market share, by providing users with as good
an experience as possible on their network.
  But the task of extracting customer insights from the vast amounts of data
collected by telcos is growing in complexity and scale everey day. How can we
measure and predict the quality of a user's experience on a telco network in
real-time? That is the problem that we address in this paper.
  We present an approach to capture, in (near) real-time, the mobile customer
experience in order to assess which conditions lead the user to place a call to
a telco's customer care center. To this end, we follow a supervised learning
approach for prediction and train our 'Restricted Random Forest' model using,
as a proxy for bad experience, the observed customer transactions in the telco
data feed before the user places a call to a customer care center.
  We evaluate our approach using a rich dataset provided by a major African
telecommunication's company and a novel big data architecture for both the
training and scoring of predictive models. Our empirical study shows our
solution to be effective at predicting user experience by inferring if a
customer will place a call based on his current context.
  These promising results open new possibilities for improved customer service,
which will help telcos to reduce churn rates and improve customer experience,
both factors that directly impact their revenue growth.","['Ernesto Diaz-Aviles', 'Fabio Pinelli', 'Karol Lynch', 'Zubair Nabi', 'Yiannis Gkoufas', 'Eric Bouillet', 'Francesco Calabrese', 'Eoin Coughlan', 'Peter Holland', 'Jason Salzwedel']","['cs.CY', 'cs.IR', 'stat.ML', 'I.2.6; K.4.0; H.3.3']",2015-08-12 11:43:11+00:00
http://arxiv.org/abs/1508.02865v2,Maximum Entropy Vector Kernels for MIMO system identification,"Recent contributions have framed linear system identification as a
nonparametric regularized inverse problem. Relying on $\ell_2$-type
regularization which accounts for the stability and smoothness of the impulse
response to be estimated, these approaches have been shown to be competitive
w.r.t classical parametric methods. In this paper, adopting Maximum Entropy
arguments, we derive a new $\ell_2$ penalty deriving from a vector-valued
kernel; to do so we exploit the structure of the Hankel matrix, thus
controlling at the same time complexity, measured by the McMillan degree,
stability and smoothness of the identified models. As a special case we recover
the nuclear norm penalty on the squared block Hankel matrix. In contrast with
previous literature on reweighted nuclear norm penalties, our kernel is
described by a small number of hyper-parameters, which are iteratively updated
through marginal likelihood maximization; constraining the structure of the
kernel acts as a (hyper)regularizer which helps controlling the effective
degrees of freedom of our estimator. To optimize the marginal likelihood we
adapt a Scaled Gradient Projection (SGP) algorithm which is proved to be
significantly computationally cheaper than other first and second order
off-the-shelf optimization methods. The paper also contains an extensive
comparison with many state-of-the-art methods on several Monte-Carlo studies,
which confirms the effectiveness of our procedure.","['Giulia Prando', 'Gianluigi Pillonetto', 'Alessandro Chiuso']","['cs.SY', 'stat.ML']",2015-08-12 09:59:09+00:00
http://arxiv.org/abs/1508.02810v2,Convergence rates of sub-sampled Newton methods,"We consider the problem of minimizing a sum of $n$ functions over a convex
parameter set $\mathcal{C} \subset \mathbb{R}^p$ where $n\gg p\gg 1$. In this
regime, algorithms which utilize sub-sampling techniques are known to be
effective. In this paper, we use sub-sampling techniques together with low-rank
approximation to design a new randomized batch algorithm which possesses
comparable convergence rate to Newton's method, yet has much smaller
per-iteration cost. The proposed algorithm is robust in terms of starting point
and step size, and enjoys a composite convergence rate, namely, quadratic
convergence at start and linear convergence when the iterate is close to the
minimizer. We develop its theoretical analysis which also allows us to select
near-optimal algorithm parameters. Our theoretical results can be used to
obtain convergence rates of previously proposed sub-sampling based algorithms
as well. We demonstrate how our results apply to well-known machine learning
problems. Lastly, we evaluate the performance of our algorithm on several
datasets under various scenarios.","['Murat A. Erdogdu', 'Andrea Montanari']",['stat.ML'],2015-08-12 04:52:58+00:00
http://arxiv.org/abs/1508.02809v1,Identifying manifolds underlying group motion in Vicsek agents,"Collective motion of animal groups often undergoes changes due to
perturbations. In a topological sense, we describe these changes as switching
between low-dimensional embedding manifolds underlying a group of evolving
agents. To characterize such manifolds, first we introduce a simple mapping of
agents between time-steps. Then, we construct a novel metric which is
susceptible to variations in the collective motion, thus revealing distinct
underlying manifolds. The method is validated through three sample scenarios
simulated using a Vicsek model, namely switching of speed, coordination, and
structure of a group. Combined with a dimensionality reduction technique that
is used to infer the dimensionality of the embedding manifold, this approach
provides an effective model-free framework for the analysis of collective
behavior across animal species.","['Kelum Gajamannage', 'Sachit Butail', 'Maurizio Porfiri', 'Erik M. Bollt']","['math.DS', 'cs.MA', 'math-ph', 'math.GT', 'math.MP', 'stat.ML', '57M60 (Primary), 37E99, 57N35 (Secondary)']",2015-08-12 04:51:59+00:00
http://arxiv.org/abs/1508.02765v2,Are Slepian-Wolf Rates Necessary for Distributed Parameter Estimation?,"We consider a distributed parameter estimation problem, in which multiple
terminals send messages related to their local observations using limited rates
to a fusion center who will obtain an estimate of a parameter related to
observations of all terminals. It is well known that if the transmission rates
are in the Slepian-Wolf region, the fusion center can fully recover all
observations and hence can construct an estimator having the same performance
as that of the centralized case. One natural question is whether Slepian-Wolf
rates are necessary to achieve the same estimation performance as that of the
centralized case. In this paper, we show that the answer to this question is
negative. We establish our result by explicitly constructing an asymptotically
minimum variance unbiased estimator (MVUE) that has the same performance as
that of the optimal estimator in the centralized case while requiring
information rates less than the conditions required in the Slepian-Wolf rate
region.","['Mostafa El Gamal', 'Lifeng Lai']","['cs.IT', 'math.IT', 'stat.ML']",2015-08-11 22:29:23+00:00
http://arxiv.org/abs/1508.02757v3,De-biasing the Lasso: Optimal Sample Size for Gaussian Designs,"Performing statistical inference in high-dimension is an outstanding
challenge. A major source of difficulty is the absence of precise information
on the distribution of high-dimensional estimators. Here, we consider linear
regression in the high-dimensional regime $p\gg n$. In this context, we would
like to perform inference on a high-dimensional parameters vector
$\theta^*\in{\mathbb R}^p$. Important progress has been achieved in computing
confidence intervals for single coordinates $\theta^*_i$. A key role in these
new methods is played by a certain debiased estimator $\hat{\theta}^{\rm d}$
that is constructed from the Lasso. Earlier work establishes that, under
suitable assumptions on the design matrix, the coordinates of
$\hat{\theta}^{\rm d}$ are asymptotically Gaussian provided $\theta^*$ is
$s_0$-sparse with $s_0 = o(\sqrt{n}/\log p )$. The condition $s_0 = o(\sqrt{n}/
\log p )$ is stronger than the one for consistent estimation, namely $s_0 =
o(n/ \log p)$. We study Gaussian designs with known or unknown population
covariance. When the covariance is known, we prove that the debiased estimator
is asymptotically Gaussian under the nearly optimal condition $s_0 = o(n/ (\log
p)^2)$. Note that earlier work was limited to $s_0 = o(\sqrt{n}/\log p)$ even
for perfectly known covariance. The same conclusion holds if the population
covariance is unknown but can be estimated sufficiently well, e.g. under the
same sparsity conditions on the inverse covariance as assumed by earlier work.
For intermediate regimes, we describe the trade-off between sparsity in the
coefficients and in the inverse covariance of the design. We further discuss
several applications of our results to high-dimensional inference. In
particular, we propose a new estimator that is minimax optimal up to a factor
$1+o_n(1)$ for i.i.d. Gaussian designs.","['Adel Javanmard', 'Andrea Montanari']","['math.ST', 'stat.ML', 'stat.TH']",2015-08-11 21:38:13+00:00
http://arxiv.org/abs/1508.02473v4,Bridging AIC and BIC: a new criterion for autoregression,"We introduce a new criterion to determine the order of an autoregressive
model fitted to time series data. It has the benefits of the two well-known
model selection techniques, the Akaike information criterion and the Bayesian
information criterion. When the data is generated from a finite order
autoregression, the Bayesian information criterion is known to be consistent,
and so is the new criterion. When the true order is infinity or suitably high
with respect to the sample size, the Akaike information criterion is known to
be efficient in the sense that its prediction performance is asymptotically
equivalent to the best offered by the candidate models; in this case, the new
criterion behaves in a similar manner. Different from the two classical
criteria, the proposed criterion adaptively achieves either consistency or
efficiency depending on the underlying true model. In practice where the
observed time series is given without any prior information about the model
specification, the proposed order selection criterion is more flexible and
robust compared with classical approaches. Numerical results are presented
demonstrating the adaptivity of the proposed technique when applied to various
datasets.","['Jie Ding', 'Vahid Tarokh', 'Yuhong Yang']","['math.ST', 'q-fin.EC', 'stat.ML', 'stat.TH']",2015-08-11 02:49:45+00:00
http://arxiv.org/abs/1508.02344v1,Local Algorithms for Block Models with Side Information,"There has been a recent interest in understanding the power of local
algorithms for optimization and inference problems on sparse graphs. Gamarnik
and Sudan (2014) showed that local algorithms are weaker than global algorithms
for finding large independent sets in sparse random regular graphs. Montanari
(2015) showed that local algorithms are suboptimal for finding a community with
high connectivity in the sparse Erd\H{o}s-R\'enyi random graphs. For the
symmetric planted partition problem (also named community detection for the
block models) on sparse graphs, a simple observation is that local algorithms
cannot have non-trivial performance.
  In this work we consider the effect of side information on local algorithms
for community detection under the binary symmetric stochastic block model. In
the block model with side information each of the $n$ vertices is labeled $+$
or $-$ independently and uniformly at random; each pair of vertices is
connected independently with probability $a/n$ if both of them have the same
label or $b/n$ otherwise. The goal is to estimate the underlying vertex
labeling given 1) the graph structure and 2) side information in the form of a
vertex labeling positively correlated with the true one. Assuming that the
ratio between in and out degree $a/b$ is $\Theta(1)$ and the average degree $
(a+b) / 2 = n^{o(1)}$, we characterize three different regimes under which a
local algorithm, namely, belief propagation run on the local neighborhoods,
maximizes the expected fraction of vertices labeled correctly. Thus, in
contrast to the case of symmetric block models without side information, we
show that local algorithms can achieve optimal performance for the block model
with side information.","['Elchanan Mossel', 'Jiaming Xu']","['stat.ML', 'cs.CC', 'cs.DC', 'math.PR']",2015-08-10 18:23:27+00:00
http://arxiv.org/abs/1508.02324v2,Adaptive Sampling of RF Fingerprints for Fine-grained Indoor Localization,"Indoor localization is a supporting technology for a broadening range of
pervasive wireless applications. One promis- ing approach is to locate users
with radio frequency fingerprints. However, its wide adoption in real-world
systems is challenged by the time- and manpower-consuming site survey process,
which builds a fingerprint database a priori for localization. To address this
problem, we visualize the 3-D RF fingerprint data as a function of locations
(x-y) and indices of access points (fingerprint), as a tensor and use tensor
algebraic methods for an adaptive tubal-sampling of this fingerprint space. In
particular using a recently proposed tensor algebraic framework in [1] we
capture the complexity of the fingerprint space as a low-dimensional
tensor-column space. In this formulation the proposed scheme exploits
adaptivity to identify reference points which are highly informative for
learning this low-dimensional space. Further, under certain incoherency
conditions we prove that the proposed scheme achieves bounded recovery error
and near-optimal sampling complexity. In contrast to several existing work that
rely on random sampling, this paper shows that adaptivity in sampling can lead
to significant improvements in localization accuracy. The approach is validated
on both data generated by the ray-tracing indoor model which accounts for the
floor plan and the impact of walls and the real world data. Simulation results
show that, while maintaining the same localization accuracy of existing
approaches, the amount of samples can be cut down by 71% for the high SNR case
and 55% for the low SNR case.","['Xiao-Yang Liu', 'Shuchin Aeron', 'Vaneet Aggarwal', 'Xiaodong Wang', 'Min-You Wu']","['cs.IT', 'math.IT', 'math.OC', 'stat.ML']",2015-08-10 16:57:54+00:00
http://arxiv.org/abs/1508.02186v1,Model-based SIR for dimension reduction,"A new dimension reduction method based on Gaussian finite mixtures is
proposed as an extension to sliced inverse regression (SIR). The model-based
SIR (MSIR) approach allows the main limitation of SIR to be overcome, i.e.,
failure in the presence of regression symmetric relationships, without the need
to impose further assumptions. Extensive numerical studies are presented to
compare the new method with some of most popular dimension reduction methods,
such as SIR, sliced average variance estimation, principal Hessian direction,
and directional regression. MSIR appears sufficiently flexible to accommodate
various regression functions, and its performance is comparable with or better,
particularly as sample size grows, than other available methods. Lastly, MSIR
is illustrated with two real data examples about ozone concentration
regression, and hand-written digit classification.",['Luca Scrucca'],"['stat.ME', 'stat.ML']",2015-08-10 09:31:01+00:00
http://arxiv.org/abs/1508.02171v1,Automatic Extraction of the Passing Strategies of Soccer Teams,"Technology offers new ways to measure the locations of the players and of the
ball in sports. This translates to the trajectories the ball takes on the field
as a result of the tactics the team applies. The challenge professionals in
soccer are facing is to take the reverse path: given the trajectories of the
ball is it possible to infer the underlying strategy/tactic of a team? We
propose a method based on Dynamic Time Warping to reveal the tactics of a team
through the analysis of repeating series of events. Based on the analysis of an
entire season, we derive insights such as passing strategies for maintaining
ball possession or counter attacks, and passing styles with a focus on the team
or on the capabilities of the individual players.","['Laszlo Gyarmati', 'Xavier Anguera']","['cs.CV', 'stat.ML']",2015-08-10 09:00:33+00:00
http://arxiv.org/abs/1508.02087v2,A Linearly-Convergent Stochastic L-BFGS Algorithm,"We propose a new stochastic L-BFGS algorithm and prove a linear convergence
rate for strongly convex and smooth functions. Our algorithm draws heavily from
a recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as
a recent approach to variance reduction for stochastic gradient descent from
Johnson and Zhang (2013). We demonstrate experimentally that our algorithm
performs well on large-scale convex and non-convex optimization problems,
exhibiting linear convergence and rapidly solving the optimization problems to
high levels of precision. Furthermore, we show that our algorithm performs well
for a wide-range of step sizes, often differing by several orders of magnitude.","['Philipp Moritz', 'Robert Nishihara', 'Michael I. Jordan']","['math.OC', 'cs.LG', 'math.NA', 'stat.CO', 'stat.ML']",2015-08-09 21:40:33+00:00
http://arxiv.org/abs/1508.01993v2,Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures,"Decision analytics commonly focuses on the text mining of financial news
sources in order to provide managerial decision support and to predict stock
market movements. Existing predictive frameworks almost exclusively apply
traditional machine learning methods, whereas recent research indicates that
traditional machine learning methods are not sufficiently capable of extracting
suitable features and capturing the non-linear nature of complex tasks. As a
remedy, novel deep learning models aim to overcome this issue by extending
traditional neural network models with additional hidden layers. Indeed, deep
learning has been shown to outperform traditional methods in terms of
predictive performance. In this paper, we adapt the novel deep learning
technique to financial decision support. In this instance, we aim to predict
the direction of stock movements following financial disclosures. As a result,
we show how deep learning can outperform the accuracy of random forests as a
benchmark for machine learning by 5.66%.","['Stefan Feuerriegel', 'Ralph Fehrer']","['stat.ML', 'cs.CL', 'cs.LG']",2015-08-09 07:39:24+00:00
http://arxiv.org/abs/1508.01939v5,Model Assisted Variable Clustering: Minimax-optimal Recovery and Algorithms,"Model-based clustering defines population level clusters relative to a model
that embeds notions of similarity. Algorithms tailored to such models yield
estimated clusters with a clear statistical interpretation. We take this view
here and introduce the class of G-block covariance models as a background model
for variable clustering. In such models, two variables in a cluster are deemed
similar if they have similar associations will all other variables. This can
arise, for instance, when groups of variables are noise corrupted versions of
the same latent factor. We quantify the difficulty of clustering data generated
from a G-block covariance model in terms of cluster proximity, measured with
respect to two related, but different, cluster separation metrics. We derive
minimax cluster separation thresholds, which are the metric values below which
no algorithm can recover the model-defined clusters exactly, and show that they
are different for the two metrics. We therefore develop two algorithms, COD and
PECOK, tailored to G-block covariance models, and study their
minimax-optimality with respect to each metric. Of independent interest is the
fact that the analysis of the PECOK algorithm, which is based on a corrected
convex relaxation of the popular K-means algorithm, provides the first
statistical analysis of such algorithms for variable clustering. Additionally,
we contrast our methods with another popular clustering method, spectral
clustering, specialized to variable clustering, and show that ensuring exact
cluster recovery via this method requires clusters to have a higher separation,
relative to the minimax threshold. Extensive simulation studies, as well as our
data analyses, confirm the applicability of our approach.","['Florentina Bunea', 'Christophe Giraud', 'Xi Luo', 'Martin Royer', 'Nicolas Verzelen']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH', '62H30, 62C20']",2015-08-08 18:25:16+00:00
http://arxiv.org/abs/1508.01928v1,A variational approach to the consistency of spectral clustering,"This paper establishes the consistency of spectral approaches to data
clustering. We consider clustering of point clouds obtained as samples of a
ground-truth measure. A graph representing the point cloud is obtained by
assigning weights to edges based on the distance between the points they
connect. We investigate the spectral convergence of both unnormalized and
normalized graph Laplacians towards the appropriate operators in the continuum
domain. We obtain sharp conditions on how the connectivity radius can be scaled
with respect to the number of sample points for the spectral convergence to
hold.
  We also show that the discrete clusters obtained via spectral clustering
converge towards a continuum partition of the ground truth measure. Such
continuum partition minimizes a functional describing the continuum analogue of
the graph-based spectral partitioning. Our approach, based on variational
convergence, is general and flexible.","['Nicol√°s Garc√≠a Trillos', 'Dejan Slepƒçev']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH', '49J55, 49J45, 60D05, 68R10, 62G20']",2015-08-08 17:14:51+00:00
http://arxiv.org/abs/1508.01922v3,The Discrete Dantzig Selector: Estimating Sparse Linear Models via Mixed Integer Linear Optimization,"We propose a novel high-dimensional linear regression estimator: the Discrete
Dantzig Selector, which minimizes the number of nonzero regression coefficients
subject to a budget on the maximal absolute correlation between the features
and residuals. Motivated by the significant advances in integer optimization
over the past 10-15 years, we present a Mixed Integer Linear Optimization
(MILO) approach to obtain certifiably optimal global solutions to this
nonconvex optimization problem. The current state of algorithmics in integer
optimization makes our proposal substantially more computationally attractive
than the least squares subset selection framework based on integer quadratic
optimization, recently proposed in [8] and the continuous nonconvex quadratic
optimization framework of [33]. We propose new discrete first-order methods,
which when paired with state-of-the-art MILO solvers, lead to good solutions
for the Discrete Dantzig Selector problem for a given computational budget. We
illustrate that our integrated approach provides globally optimal solutions in
significantly shorter computation times, when compared to off-the-shelf MILO
solvers. We demonstrate both theoretically and empirically that in a wide range
of regimes the statistical properties of the Discrete Dantzig Selector are
superior to those of popular $\ell_{1}$-based approaches. We illustrate that
our approach can handle problem instances with p = 10,000 features with
certifiable optimality making it a highly scalable combinatorial variable
selection approach in sparse linear modeling.","['Rahul Mazumder', 'Peter Radchenko']","['stat.ME', 'math.OC', 'math.ST', 'stat.CO', 'stat.ML', 'stat.TH']",2015-08-08 16:13:07+00:00
http://arxiv.org/abs/1508.01903v2,Diffusion Maximum Correntropy Criterion Algorithms for Robust Distributed Estimation,"Robust diffusion adaptive estimation algorithms based on the maximum
correntropy criterion (MCC), including adaptation to combination MCC and
combination to adaptation MCC, are developed to deal with the distributed
estimation over network in impulsive (long-tailed) noise environments. The cost
functions used in distributed estimation are in general based on the mean
square error (MSE) criterion, which is desirable when the measurement noise is
Gaussian. In non-Gaussian situations, such as the impulsive-noise case, MCC
based methods may achieve much better performance than the MSE methods as they
take into account higher order statistics of error distribution. The proposed
methods can also outperform the robust diffusion least mean p-power(DLMP) and
diffusion minimum error entropy (DMEE) algorithms. The mean and mean square
convergence analysis of the new algorithms are also carried out.","['Wentao Ma', 'Badong Chen', 'Jiandong Duan', 'Haiquan Zhao']","['stat.ML', 'cs.LG']",2015-08-08 13:38:41+00:00
http://arxiv.org/abs/1508.01819v1,Spectral Clustering and Block Models: A Review And A New Algorithm,"We focus on spectral clustering of unlabeled graphs and review some results
on clustering methods which achieve weak or strong consistent identification in
data generated by such models. We also present a new algorithm which appears to
perform optimally both theoretically using asymptotic theory and empirically.","['Sharmodeep Bhattacharyya', 'Peter J. Bickel']","['math.ST', 'cs.SI', 'stat.ML', 'stat.TH', '62F12, 68R10, 05C12, 62M99, 60J80']",2015-08-07 21:11:41+00:00
http://arxiv.org/abs/1508.01774v2,An End-to-End Neural Network for Polyphonic Piano Music Transcription,"We present a supervised neural network model for polyphonic piano music
transcription. The architecture of the proposed model is analogous to speech
recognition systems and comprises an acoustic model and a music language model.
The acoustic model is a neural network used for estimating the probabilities of
pitches in a frame of audio. The language model is a recurrent neural network
that models the correlations between pitch combinations over time. The proposed
model is general and can be used to transcribe polyphonic music without
imposing any constraints on the polyphony. The acoustic and language model
predictions are combined using a probabilistic graphical model. Inference over
the output variables is performed using the beam search algorithm. We perform
two sets of experiments. We investigate various neural network architectures
for the acoustic models and also investigate the effect of combining acoustic
and music language model predictions using the proposed architecture. We
compare performance of the neural network based acoustic models with two
popular unsupervised acoustic models. Results show that convolutional neural
network acoustic models yields the best performance across all evaluation
metrics. We also observe improved performance with the application of the music
language models. Finally, we present an efficient variant of beam search that
improves performance and reduces run-times by an order of magnitude, making the
model suitable for real-time applications.","['Siddharth Sigtia', 'Emmanouil Benetos', 'Simon Dixon']","['stat.ML', 'cs.LG', 'cs.SD']",2015-08-07 18:16:32+00:00
http://arxiv.org/abs/1508.01746v2,Using Deep Learning for Detecting Spoofing Attacks on Speech Signals,"It is well known that speaker verification systems are subject to spoofing
attacks. The Automatic Speaker Verification Spoofing and Countermeasures
Challenge -- ASVSpoof2015 -- provides a standard spoofing database, containing
attacks based on synthetic speech, along with a protocol for experiments. This
paper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based
on deep neural networks, working both as a classifier and as a feature
extraction module for a GMM and a SVM classifier. Results show the validity of
this approach, achieving less than 0.5\% EER for known attacks.","['Alan Godoy', 'Fl√°vio Sim√µes', 'Jos√© Augusto Stuchi', 'Marcus de Assis Angeloni', 'M√°rio Uliani', 'Ricardo Violato']","['cs.SD', 'cs.CL', 'cs.CR', 'cs.LG', 'stat.ML']",2015-08-07 16:20:52+00:00
http://arxiv.org/abs/1508.01720v2,Mismatch in the Classification of Linear Subspaces: Sufficient Conditions for Reliable Classification,"This paper considers the classification of linear subspaces with mismatched
classifiers. In particular, we assume a model where one observes signals in the
presence of isotropic Gaussian noise and the distribution of the signals
conditioned on a given class is Gaussian with a zero mean and a low-rank
covariance matrix. We also assume that the classifier knows only a mismatched
version of the parameters of input distribution in lieu of the true parameters.
By constructing an asymptotic low-noise expansion of an upper bound to the
error probability of such a mismatched classifier, we provide sufficient
conditions for reliable classification in the low-noise regime that are able to
sharply predict the absence of a classification error floor. Such conditions
are a function of the geometry of the true signal distribution, the geometry of
the mismatched signal distributions as well as the interplay between such
geometries, namely, the principal angles and the overlap between the true and
the mismatched signal subspaces. Numerical results demonstrate that our
conditions for reliable classification can sharply predict the behavior of a
mismatched classifier both with synthetic data and in a motion segmentation and
a hand-written digit classification applications.","['Jure Sokolic', 'Francesco Renna', 'Robert Calderbank', 'Miguel R. D. Rodrigues']","['cs.IT', 'cs.CV', 'math.IT', 'stat.ML']",2015-08-07 15:16:39+00:00
http://arxiv.org/abs/1508.01717v4,Distributional Equivalence and Structure Learning for Bow-free Acyclic Path Diagrams,"We consider the problem of structure learning for bow-free acyclic path
diagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG
models that allow for certain hidden variables. We present a first method for
this problem using a greedy score-based search algorithm. We also prove some
necessary and some sufficient conditions for distributional equivalence of BAPs
which are used in an algorithmic ap- proach to compute (nearly) equivalent
model structures. This allows us to infer lower bounds of causal effects. We
also present applications to real and simulated datasets using our publicly
available R-package.","['Christopher Nowzohour', 'Marloes H. Maathuis', 'Robin J. Evans', 'Peter B√ºhlmann']",['stat.ML'],2015-08-07 15:06:04+00:00
http://arxiv.org/abs/1508.01713v1,Dimension reduction for model-based clustering,"We introduce a dimension reduction method for visualizing the clustering
structure obtained from a finite mixture of Gaussian densities. Information on
the dimension reduction subspace is obtained from the variation on group means
and, depending on the estimated mixture model, on the variation on group
covariances. The proposed method aims at reducing the dimensionality by
identifying a set of linear combinations, ordered by importance as quantified
by the associated eigenvalues, of the original features which capture most of
the cluster structure contained in the data. Observations may then be projected
onto such a reduced subspace, thus providing summary plots which help to
visualize the clustering structure. These plots can be particularly appealing
in the case of high-dimensional data and noisy structure. The new constructed
variables capture most of the clustering information available in the data, and
they can be further reduced to improve clustering performance. We illustrate
the approach on both simulated and real data sets.",['Luca Scrucca'],"['stat.ME', 'stat.ML']",2015-08-07 14:54:03+00:00
http://arxiv.org/abs/1508.01609v1,The Contribution of Internal and Model Variabilities to the Uncertainty in CMIP5 Decadal Climate Predictions,"Decadal climate predictions, which are initialized with observed conditions,
are characterized by two main sources of uncertainties--internal and model
variabilities. Using an ensemble of climate model simulations from the CMIP5
decadal experiments, we quantified the total uncertainty associated with these
predictions and the relative importance of each source. Annual and monthly
averages of the surface temperature and wind components were considered. We
show that different definitions of the anomaly results in different conclusions
regarding the variance of the ensemble members. However, some features of the
uncertainty are common to all the measures we considered. We found that over
decadal time scales, there is no considerable increase in the uncertainty with
time. The model variability is more sensitive to the annual cycle than the
internal variability. This, in turn, results in a maximal uncertainty during
the winter in the northern hemisphere. The uncertainty of the surface
temperature prediction is dominated by the model variability, whereas the
uncertainty of the wind components is determined by both sources. Analysis of
the spatial distribution of the uncertainty reveals that the surface
temperature has higher variability over land and in high latitudes, whereas the
surface zonal wind has higher variability over the ocean. The relative
importance of the internal and model variabilities depends on the averaging
period, the definition of the anomaly, and the location. These findings suggest
that several methods should be combined in order to assess future climate
prediction uncertainties and that weighting schemes of the ensemble members may
reduce the uncertainties.","['Ehud Strobach', 'Golan Bel']","['physics.ao-ph', 'physics.data-an', 'stat.ML']",2015-08-07 05:36:04+00:00
http://arxiv.org/abs/1508.01596v1,Sublinear Partition Estimation,"The output scores of a neural network classifier are converted to
probabilities via normalizing over the scores of all competing categories.
Computing this partition function, $Z$, is then linear in the number of
categories, which is problematic as real-world problem sets continue to grow in
categorical types, such as in visual object recognition or discriminative
language modeling. We propose three approaches for sublinear estimation of the
partition function, based on approximate nearest neighbor search and kernel
feature maps and compare the performance of the proposed approaches
empirically.","['Pushpendre Rastogi', 'Benjamin Van Durme']","['stat.ML', 'cs.LG']",2015-08-07 03:45:21+00:00
http://arxiv.org/abs/1508.01551v1,A Knowledge Gradient Policy for Sequencing Experiments to Identify the Structure of RNA Molecules Using a Sparse Additive Belief Model,"We present a sparse knowledge gradient (SpKG) algorithm for adaptively
selecting the targeted regions within a large RNA molecule to identify which
regions are most amenable to interactions with other molecules. Experimentally,
such regions can be inferred from fluorescence measurements obtained by binding
a complementary probe with fluorescence markers to the targeted regions. We use
a biophysical model which shows that the fluorescence ratio under the log scale
has a sparse linear relationship with the coefficients describing the
accessibility of each nucleotide, since not all sites are accessible (due to
the folding of the molecule). The SpKG algorithm uniquely combines the Bayesian
ranking and selection problem with the frequentist $\ell_1$ regularized
regression approach Lasso. We use this algorithm to identify the sparsity
pattern of the linear model as well as sequentially decide the best regions to
test before experimental budget is exhausted. Besides, we also develop two
other new algorithms: batch SpKG algorithm, which generates more suggestions
sequentially to run parallel experiments; and batch SpKG with a procedure which
we call length mutagenesis. It dynamically adds in new alternatives, in the
form of types of probes, are created by inserting, deleting or mutating
nucleotides within existing probes. In simulation, we demonstrate these
algorithms on the Group I intron (a mid-size RNA molecule), showing that they
efficiently learn the correct sparsity pattern, identify the most accessible
region, and outperform several other policies.","['Yan Li', 'Kristofer G. Reyes', 'Jorge Vazquez-Anderson', 'Yingfei Wang', 'Lydia M. Contreras', 'Warren B. Powell']","['math.OC', 'stat.AP', 'stat.ML']",2015-08-06 22:03:34+00:00
http://arxiv.org/abs/1508.01340v1,Universal Approximation of Edge Density in Large Graphs,"In this paper, we present a novel way to summarize the structure of large
graphs, based on non-parametric estimation of edge density in directed
multigraphs. Following coclustering approach, we use a clustering of the
vertices, with a piecewise constant estimation of the density of the edges
across the clusters, and address the problem of automatically and reliably
inferring the number of clusters, which is the granularity of the coclustering.
We use a model selection technique with data-dependent prior and obtain an
exact evaluation criterion for the posterior probability of edge density
estimation models. We demonstrate, both theoretically and empirically, that our
data-dependent modeling technique is consistent, resilient to noise, valid non
asymptotically and asymptotically behaves as an universal approximator of the
true edge density in directed multigraphs. We evaluate our method using
artificial graphs and present its practical interest on real world graphs. The
method is both robust and scalable. It is able to extract insightful patterns
in the unsupervised learning setting and to provide state of the art accuracy
when used as a preparation step for supervised learning.",['Marc Boull√©'],"['cs.SI', 'cs.DB', 'stat.ML', 'H.2.8; I.5.3; G.3']",2015-08-06 09:40:28+00:00
http://arxiv.org/abs/1508.01248v4,Sparse Pseudo-input Local Kriging for Large Spatial Datasets with Exogenous Variables,"We study large-scale spatial systems that contain exogenous variables, e.g.
environmental factors that are significant predictors in spatial processes.
Building predictive models for such processes is challenging because the large
numbers of observations present makes it inefficient to apply full Kriging. In
order to reduce computational complexity, this paper proposes Sparse
Pseudo-input Local Kriging (SPLK), which utilizes hyperplanes to partition a
domain into smaller subdomains and then applies a sparse approximation of the
full Kriging to each subdomain. We also develop an optimization procedure to
find the desired hyperplanes. To alleviate the problem of discontinuity in the
global predictor, we impose continuity constraints on the boundaries of the
neighboring subdomains. Furthermore, partitioning the domain into smaller
subdomains makes it possible to use different parameter values for the
covariance function in each region and, therefore, the heterogeneity in the
data structure can be effectively captured. Numerical experiments demonstrate
that SPLK outperforms, or is comparable to, the algorithms commonly applied to
spatial datasets.","['Babak Farmanesh', 'Arash Pourhabib']",['stat.ML'],2015-08-05 23:44:56+00:00
http://arxiv.org/abs/1508.01240v4,A Bayesian framework for functional calibration of expensive computational models through non-isometric matching,"We study statistical calibration, i.e., adjusting features of a computational
model that are not observable or controllable in its associated physical
system. We focus on functional calibration, which arises in many manufacturing
processes where the unobservable features, called calibration variables, are a
function of the input variables. A major challenge in many applications is that
computational models are expensive and can only be evaluated a limited number
of times. Furthermore, without making strong assumptions, the calibration
variables are not identifiable. We propose Bayesian non-isometric matching
calibration (BNMC) that allows calibration of expensive computational models
with only a limited number of samples taken from a computational model and its
associated physical system. BNMC replaces the computational model with a
dynamic Gaussian process (GP) whose parameters are trained in the calibration
procedure. To resolve the identifiability issue, we present the calibration
problem from a geometric perspective of non-isometric curve to surface
matching, which enables us to take advantage of combinatorial optimization
techniques to extract necessary information for constructing prior
distributions. Our numerical experiments demonstrate that in terms of
prediction accuracy BNMC outperforms, or is comparable to, other existing
calibration frameworks.","['Babak Farmanesh', 'Arash Pourhabib', 'Balabhaskar Balasundaram', 'Austin Buchanan']",['stat.ML'],2015-08-05 22:17:56+00:00
http://arxiv.org/abs/1508.01235v2,Empirical Similarity for Absent Data Generation in Imbalanced Classification,"When the training data in a two-class classification problem is overwhelmed
by one class, most classification techniques fail to correctly identify the
data points belonging to the underrepresented class. We propose
Similarity-based Imbalanced Classification (SBIC) that learns patterns in the
training data based on an empirical similarity function. To take the imbalanced
structure of the training data into account, SBIC utilizes the concept of
absent data, i.e. data from the minority class which can help better find the
boundary between the two classes. SBIC simultaneously optimizes the weights of
the empirical similarity function and finds the locations of absent data
points. As such, SBIC uses an embedded mechanism for synthetic data generation
which does not modify the training dataset, but alters the algorithm to suit
imbalanced datasets. Therefore, SBIC uses the ideas of both major schools of
thoughts in imbalanced classification: Like cost-sensitive approaches SBIC
operates on an algorithm level to handle imbalanced structures; and similar to
synthetic data generation approaches, it utilizes the properties of unobserved
data points from the minority class. The application of SBIC to imbalanced
datasets suggests it is comparable to, and in some cases outperforms, other
commonly used classification techniques for imbalanced datasets.",['Arash Pourhabib'],"['stat.ML', 'cs.LG']",2015-08-05 21:43:32+00:00
http://arxiv.org/abs/1508.01217v4,Bayesian Approximate Kernel Regression with Variable Selection,"Nonlinear kernel regression models are often used in statistics and machine
learning because they are more accurate than linear models. Variable selection
for kernel regression models is a challenge partly because, unlike the linear
regression setting, there is no clear concept of an effect size for regression
coefficients. In this paper, we propose a novel framework that provides an
effect size analog of each explanatory variable for Bayesian kernel regression
models when the kernel is shift-invariant --- for example, the Gaussian kernel.
We use function analytic properties of shift-invariant reproducing kernel
Hilbert spaces (RKHS) to define a linear vector space that: (i) captures
nonlinear structure, and (ii) can be projected onto the original explanatory
variables. The projection onto the original explanatory variables serves as an
analog of effect sizes. The specific function analytic property we use is that
shift-invariant kernel functions can be approximated via random Fourier bases.
Based on the random Fourier expansion we propose a computationally efficient
class of Bayesian approximate kernel regression (BAKR) models for both
nonlinear regression and binary classification for which one can compute an
analog of effect sizes. We illustrate the utility of BAKR by examining two
important problems in statistical genetics: genomic selection (i.e. phenotypic
prediction) and association mapping (i.e. inference of significant variants or
loci). State-of-the-art methods for genomic selection and association mapping
are based on kernel regression and linear models, respectively. BAKR is the
first method that is competitive in both settings.","['Lorin Crawford', 'Kris C. Wood', 'Xiang Zhou', 'Sayan Mukherjee']","['stat.ME', 'q-bio.QM', 'stat.AP', 'stat.ML']",2015-08-05 20:40:11+00:00
http://arxiv.org/abs/1508.01211v2,"Listen, Attend and Spell","We present Listen, Attend and Spell (LAS), a neural network that learns to
transcribe speech utterances to characters. Unlike traditional DNN-HMM models,
this model learns all the components of a speech recognizer jointly. Our system
has two components: a listener and a speller. The listener is a pyramidal
recurrent network encoder that accepts filter bank spectra as inputs. The
speller is an attention-based recurrent network decoder that emits characters
as outputs. The network produces character sequences without making any
independence assumptions between the characters. This is the key improvement of
LAS over previous end-to-end CTC models. On a subset of the Google voice search
task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a
language model, and 10.3% with language model rescoring over the top 32 beams.
By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.","['William Chan', 'Navdeep Jaitly', 'Quoc V. Le', 'Oriol Vinyals']","['cs.CL', 'cs.LG', 'cs.NE', 'stat.ML']",2015-08-05 20:17:58+00:00
http://arxiv.org/abs/1508.01071v1,A MAP approach for $\ell_q$-norm regularized sparse parameter estimation using the EM algorithm,"In this paper, Bayesian parameter estimation through the consideration of the
Maximum A Posteriori (MAP) criterion is revisited under the prism of the
Expectation-Maximization (EM) algorithm. By incorporating a sparsity-promoting
penalty term in the cost function of the estimation problem through the use of
an appropriate prior distribution, we show how the EM algorithm can be used to
efficiently solve the corresponding optimization problem. To this end, we rely
on variance-mean Gaussian mixtures (VMGM) to describe the prior distribution,
while we incorporate many nice features of these mixtures to our estimation
problem. The corresponding MAP estimation problem is completely expressed in
terms of the EM algorithm, which allows for handling nonlinearities and hidden
variables that cannot be easily handled with traditional methods. For
comparison purposes, we also develop a Coordinate Descent algorithm for the
$\ell_q$-norm penalized problem and present the performance results via
simulations.","['Rodrigo Carvajal', 'Juan C. Ag√ºero', 'Boris I. Godoy', 'Dimitrios Katselis']","['cs.SY', 'stat.ML']",2015-08-05 13:24:15+00:00
http://arxiv.org/abs/1508.01019v1,Direct Estimation of the Derivative of Quadratic Mutual Information with Application in Supervised Dimension Reduction,"A typical goal of supervised dimension reduction is to find a low-dimensional
subspace of the input space such that the projected input variables preserve
maximal information about the output variables. The dependence maximization
approach solves the supervised dimension reduction problem through maximizing a
statistical dependence between projected input variables and output variables.
A well-known statistical dependence measure is mutual information (MI) which is
based on the Kullback-Leibler (KL) divergence. However, it is known that the KL
divergence is sensitive to outliers. On the other hand, quadratic MI (QMI) is a
variant of MI based on the $L_2$ distance which is more robust against outliers
than the KL divergence, and a computationally efficient method to estimate QMI
from data, called least-squares QMI (LSQMI), has been proposed recently. For
these reasons, developing a supervised dimension reduction method based on
LSQMI seems promising. However, not QMI itself, but the derivative of QMI is
needed for subspace search in supervised dimension reduction, and the
derivative of an accurate QMI estimator is not necessarily a good estimator of
the derivative of QMI. In this paper, we propose to directly estimate the
derivative of QMI without estimating QMI itself. We show that the direct
estimation of the derivative of QMI is more accurate than the derivative of the
estimated QMI. Finally, we develop a supervised dimension reduction algorithm
which efficiently uses the proposed derivative estimator, and demonstrate
through experiments that the proposed method is more robust against outliers
than existing methods.","['Voot Tangkaratt', 'Hiroaki Sasaki', 'Masashi Sugiyama']",['stat.ML'],2015-08-05 09:44:32+00:00
http://arxiv.org/abs/1508.00973v1,Progressive EM for Latent Tree Models and Hierarchical Topic Detection,"Hierarchical latent tree analysis (HLTA) is recently proposed as a new method
for topic detection. It differs fundamentally from the LDA-based methods in
terms of topic definition, topic-document relationship, and learning method. It
has been shown to discover significantly more coherent topics and better topic
hierarchies. However, HLTA relies on the Expectation-Maximization (EM)
algorithm for parameter estimation and hence is not efficient enough to deal
with large datasets. In this paper, we propose a method to drastically speed up
HLTA using a technique inspired by recent advances in the moments method.
Empirical experiments show that our method greatly improves the efficiency of
HLTA. It is as efficient as the state-of-the-art LDA-based method for
hierarchical topic detection and finds substantially better topics and topic
hierarchies.","['Peixian Chen', 'Nevin L. Zhang', 'Leonard K. M. Poon', 'Zhourong Chen']","['cs.LG', 'cs.CL', 'cs.IR', 'stat.ML']",2015-08-05 05:00:32+00:00
http://arxiv.org/abs/1508.00945v4,Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms,"Margin-based structured prediction commonly uses a maximum loss over all
possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural
language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of
the maximum loss over random structured outputs sampled independently from some
proposal distribution. This method is linear-time in the number of random
structured outputs and trivially parallelizable. We study this family of loss
functions in the PAC-Bayes framework under Gaussian perturbations
\cite{McAllester07}. Under some technical conditions and up to statistical
accuracy, we show that this family of loss functions produces a tighter upper
bound of the Gibbs decoder distortion than commonly used methods. Thus, using
the maximum loss over random structured outputs is a principled way of learning
the parameter of structured prediction models. Besides explaining the
experimental success of \cite{Zhang14,Zhang15}, our theoretical results show
that more general techniques are possible.","['Jean Honorio', 'Tommi Jaakkola']","['stat.ML', 'cs.LG']",2015-08-05 00:22:39+00:00
http://arxiv.org/abs/1508.00882v1,Asynchronous stochastic convex optimization,"We show that asymptotically, completely asynchronous stochastic gradient
procedures achieve optimal (even to constant factors) convergence rates for the
solution of convex optimization problems under nearly the same conditions
required for asymptotic optimality of standard stochastic gradient procedures.
Roughly, the noise inherent to the stochastic approximation scheme dominates
any noise from asynchrony. We also give empirical evidence demonstrating the
strong performance of asynchronous, parallel stochastic optimization schemes,
demonstrating that the robustness inherent to stochastic approximation problems
allows substantially faster parallel and asynchronous solution methods.","['John C. Duchi', 'Sorathan Chaturapruek', 'Christopher R√©']","['math.OC', 'stat.ML']",2015-08-04 19:35:49+00:00
http://arxiv.org/abs/1508.00842v4,Perceptron like Algorithms for Online Learning to Rank,"Perceptron is a classic online algorithm for learning a classification
function. In this paper, we provide a novel extension of the perceptron
algorithm to the learning to rank problem in information retrieval. We consider
popular listwise performance measures such as Normalized Discounted Cumulative
Gain (NDCG) and Average Precision (AP). A modern perspective on perceptron for
classification is that it is simply an instance of online gradient descent
(OGD), during mistake rounds, using the hinge loss function. Motivated by this
interpretation, we propose a novel family of listwise, large margin ranking
surrogates. Members of this family can be thought of as analogs of the hinge
loss. Exploiting a certain self-bounding property of the proposed family, we
provide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our
perceptron-like algorithm. We show that, if there exists a perfect oracle
ranker which can correctly rank each instance in an online sequence of ranking
data, with some margin, the cumulative loss of perceptron algorithm on that
sequence is bounded by a constant, irrespective of the length of the sequence.
This result is reminiscent of Novikoff's convergence theorem for the
classification perceptron. Moreover, we prove a lower bound on the cumulative
loss achievable by any deterministic algorithm, under the assumption of
existence of perfect oracle ranker. The lower bound shows that our perceptron
bound is not tight, and we propose another, \emph{purely online}, algorithm
which achieves the lower bound. We provide empirical results on simulated and
large commercial datasets to corroborate our theoretical results.","['Sougata Chaudhuri', 'Ambuj Tewari']","['cs.LG', 'stat.ML']",2015-08-04 17:23:46+00:00
http://arxiv.org/abs/1508.00655v1,Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance based High Dimensional Two Sample Testing,"Nonparametric two sample testing is a decision theoretic problem that
involves identifying differences between two random variables without making
parametric assumptions about their underlying distributions. We refer to the
most common settings as mean difference alternatives (MDA), for testing
differences only in first moments, and general difference alternatives (GDA),
which is about testing for any difference in distributions. A large number of
test statistics have been proposed for both these settings. This paper connects
three classes of statistics - high dimensional variants of Hotelling's t-test,
statistics based on Reproducing Kernel Hilbert Spaces, and energy statistics
based on pairwise distances. We ask the question: how much statistical power do
popular kernel and distance based tests for GDA have when the unknown
distributions differ in their means, compared to specialized tests for MDA?
  We formally characterize the power of popular tests for GDA like the Maximum
Mean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent
variants of the Energy Distance with the Euclidean norm (eED) in the
high-dimensional MDA regime. Some practically important properties include (a)
eED and gMMD have asymptotically equal power; furthermore they enjoy a free
lunch because, while they are additionally consistent for GDA, they also have
the same power as specialized high-dimensional t-test variants for MDA. All
these tests are asymptotically optimal (including matching constants) under MDA
for spherical covariances, according to simple lower bounds, (b) The power of
gMMD is independent of the kernel bandwidth, as long as it is larger than the
choice made by the median heuristic, (c) There is a clear and smooth
computation-statistics tradeoff for linear-time, subquadratic-time and
quadratic-time versions of these tests, with more computation resulting in
higher power.","['Aaditya Ramdas', 'Sashank J. Reddi', 'Barnabas Poczos', 'Aarti Singh', 'Larry Wasserman']","['math.ST', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ML', 'stat.TH']",2015-08-04 04:10:05+00:00
http://arxiv.org/abs/1508.00641v4,Episodic Multi-armed Bandits,"We introduce a new class of reinforcement learning methods referred to as
{\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\em
episodes}, each composed of several {\em steps}, in which it chooses an action
and observes a feedback signal. Moreover, in each step, it can take a special
action, called the $stop$ action, that ends the current episode. After the
$stop$ action is taken, the learner collects a terminal reward, and observes
the costs and terminal rewards associated with each step of the episode. The
goal of the learner is to maximize its cumulative gain (i.e., the terminal
reward minus costs) over all episodes by learning to choose the best sequence
of actions based on the feedback. First, we define an {\em oracle} benchmark,
which sequentially selects the actions that maximize the expected immediate
gain. Then, we propose our online learning algorithm, named {\em FeedBack
Adaptive Learning} (FeedBAL), and prove that its regret with respect to the
benchmark is bounded with high probability and increases logarithmically in
expectation. Moreover, the regret only has polynomial dependence on the number
of steps, actions and states. eMAB can be used to model applications that
involve humans in the loop, ranging from personalized medical screening to
personalized web-based education, where sequences of actions are taken in each
episode, and optimal behavior requires adapting the chosen actions based on the
feedback.","['Cem Tekin', 'Mihaela van der Schaar']","['cs.LG', 'stat.ML']",2015-08-04 01:52:42+00:00
http://arxiv.org/abs/1508.00635v1,Bayesian mixtures of spatial spline regressions,"This work relates the framework of model-based clustering for spatial
functional data where the data are surfaces. We first introduce a Bayesian
spatial spline regression model with mixed-effects (BSSR) for modeling spatial
function data. The BSSR model is based on Nodal basis functions for spatial
regression and accommodates both common mean behavior for the data through a
fixed-effects part, and variability inter-individuals thanks to a
random-effects part. Then, in order to model populations of spatial functional
data issued from heterogeneous groups, we integrate the BSSR model into a
mixture framework. The resulting model is a Bayesian mixture of spatial spline
regressions with mixed-effects (BMSSR) used for density estimation and
model-based surface clustering. The models, through their Bayesian formulation,
allow to integrate possible prior knowledge on the data structure and
constitute a good alternative to recent mixture of spatial spline regressions
model estimated in a maximum likelihood framework via the
expectation-maximization (EM) algorithm. The Bayesian model inference is
performed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs
sampler to infer the BSSR and the BMSSR models and apply them on simulated
surfaces and a real problem of handwritten digit recognition using the MNIST
data set. The obtained results highlight the potential benefit of the proposed
Bayesian approaches for modeling surfaces possibly dispersed in particular in
clusters.",['Faicel Chamroukhi'],"['stat.ME', 'cs.LG', 'stat.CO', 'stat.ML', '62-XX, 62Fxx, 62F15, 62H30']",2015-08-04 01:29:49+00:00
