id,title,abstract,authors,categories,date
http://arxiv.org/abs/1801.05062v2,Multi-Label Learning from Medical Plain Text with Convolutional Residual Models,"Predicting diagnoses from Electronic Health Records (EHRs) is an important
medical application of multi-label learning. We propose a convolutional
residual model for multi-label classification from doctor notes in EHR data. A
given patient may have multiple diagnoses, and therefore multi-label learning
is required. We employ a Convolutional Neural Network (CNN) to encode plain
text into a fixed-length sentence embedding vector. Since diagnoses are
typically correlated, a deep residual network is employed on top of the CNN
encoder, to capture label (diagnosis) dependencies and incorporate information
directly from the encoded sentence vector. A real EHR dataset is considered,
and we compare the proposed model with several well-known baselines, to predict
diagnoses based on doctor notes. Experimental results demonstrate the
superiority of the proposed convolutional residual model.","['Xinyuan Zhang', 'Ricardo Henao', 'Zhe Gan', 'Yitong Li', 'Lawrence Carin']","['stat.ML', 'cs.LG', 'stat.AP']",2018-01-15 22:59:17+00:00
http://arxiv.org/abs/1801.05039v3,Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator,"Direct policy gradient methods for reinforcement learning and continuous
control problems are a popular approach for a variety of reasons: 1) they are
easy to implement without explicit knowledge of the underlying model 2) they
are an ""end-to-end"" approach, directly optimizing the performance metric of
interest 3) they inherently allow for richly parameterized policies. A notable
drawback is that even in the most basic continuous control problem (that of
linear quadratic regulators), these methods must solve a non-convex
optimization problem, where little is understood about their efficiency from
both computational and statistical perspectives. In contrast, system
identification and model based planning in optimal control theory have a much
more solid theoretical footing, where much is known with regards to their
computational and statistical properties. This work bridges this gap showing
that (model free) policy gradient methods globally converge to the optimal
solution and are efficient (polynomially so in relevant problem dependent
quantities) with regards to their sample and computational complexities.","['Maryam Fazel', 'Rong Ge', 'Sham M. Kakade', 'Mehran Mesbahi']","['cs.LG', 'stat.ML']",2018-01-15 21:40:50+00:00
http://arxiv.org/abs/1801.05007v1,Divide and Recombine for Large and Complex Data: Model Likelihood Functions using MCMC,"In Divide & Recombine (D&R), big data are divided into subsets, each analytic
method is applied to subsets, and the outputs are recombined. This enables deep
analysis and practical computational performance. An innovate D\&R procedure is
proposed to compute likelihood functions of data-model (DM) parameters for big
data. The likelihood-model (LM) is a parametric probability density function of
the DM parameters. The density parameters are estimated by fitting the density
to MCMC draws from each subset DM likelihood function, and then the fitted
densities are recombined. The procedure is illustrated using normal and
skew-normal LMs for the logistic regression DM.","['Qi Liu', 'Anindya Bhadra', 'William S. Cleveland']","['stat.ME', 'stat.ML']",2018-01-15 20:53:52+00:00
http://arxiv.org/abs/1801.04987v3,On the Complexity of the Weighted Fused Lasso,"The solution path of the 1D fused lasso for an $n$-dimensional input is
piecewise linear with $\mathcal{O}(n)$ segments (Hoefling et al. 2010 and
Tibshirani et al 2011). However, existing proofs of this bound do not hold for
the weighted fused lasso. At the same time, results for the generalized lasso,
of which the weighted fused lasso is a special case, allow $\Omega(3^n)$
segments (Mairal et al. 2012). In this paper, we prove that the number of
segments in the solution path of the weighted fused lasso is
$\mathcal{O}(n^2)$, and that, for some instances, it is $\Omega(n^2)$. We also
give a new, very simple, proof of the $\mathcal{O}(n)$ bound for the fused
lasso.","['Jose Bento', 'Ralph Furmaniak', 'Surjyendu Ray']","['cs.LG', 'cs.CC', 'math.OC', 'stat.ML']",2018-01-15 20:20:53+00:00
http://arxiv.org/abs/1801.04958v1,Topic Modeling on Health Journals with Regularized Variational Inference,"Topic modeling enables exploration and compact representation of a corpus.
The CaringBridge (CB) dataset is a massive collection of journals written by
patients and caregivers during a health crisis. Topic modeling on the CB
dataset, however, is challenging due to the asynchronous nature of multiple
authors writing about their health journeys. To overcome this challenge we
introduce the Dynamic Author-Persona topic model (DAP), a probabilistic
graphical model designed for temporal corpora with multiple authors. The
novelty of the DAP model lies in its representation of authors by a persona ---
where personas capture the propensity to write about certain topics over time.
Further, we present a regularized variational inference algorithm, which we use
to encourage the DAP model's personas to be distinct. Our results show
significant improvements over competing topic models --- particularly after
regularization, and highlight the DAP model's unique ability to capture common
journeys shared by different authors.","['Robert Giaquinto', 'Arindam Banerjee']","['cs.CL', 'cs.LG', 'stat.ML']",2018-01-15 19:23:21+00:00
http://arxiv.org/abs/1801.04929v1,"Generalizing, Decoding, and Optimizing Support Vector Machine Classification","The classification of complex data usually requires the composition of
processing steps. Here, a major challenge is the selection of optimal
algorithms for preprocessing and classification (including parameterizations).
Nowadays, parts of the optimization process are automized but expert knowledge
and manual work are still required. We present three steps to face this process
and ease the optimization. Namely, we take a theoretical view on classical
classifiers, provide an approach to interpret the classifier together with the
preprocessing, and integrate both into one framework which enables a
semiautomatic optimization of the processing chain and which interfaces
numerous algorithms.",['Mario Michael Krell'],"['cs.LG', 'cs.CV', 'stat.ML']",2018-01-15 16:49:52+00:00
http://arxiv.org/abs/1801.04856v1,Improving Orbit Prediction Accuracy through Supervised Machine Learning,"Due to the lack of information such as the space environment condition and
resident space objects' (RSOs') body characteristics, current orbit predictions
that are solely grounded on physics-based models may fail to achieve required
accuracy for collision avoidance and have led to satellite collisions already.
This paper presents a methodology to predict RSOs' trajectories with higher
accuracy than that of the current methods. Inspired by the machine learning
(ML) theory through which the models are learned based on large amounts of
observed data and the prediction is conducted without explicitly modeling space
objects and space environment, the proposed ML approach integrates
physics-based orbit prediction algorithms with a learning-based process that
focuses on reducing the prediction errors. Using a simulation-based space
catalog environment as the test bed, the paper demonstrates three types of
generalization capability for the proposed ML approach: 1) the ML model can be
used to improve the same RSO's orbit information that is not available during
the learning process but shares the same time interval as the training data; 2)
the ML model can be used to improve predictions of the same RSO at future
epochs; and 3) the ML model based on a RSO can be applied to other RSOs that
share some common features.","['Hao Peng', 'Xiaoli Bai']","['astro-ph.EP', 'cs.CE', 'cs.LG', 'stat.ML']",2018-01-15 15:56:36+00:00
http://arxiv.org/abs/1801.04813v1,Predicting Movie Genres Based on Plot Summaries,"This project explores several Machine Learning methods to predict movie
genres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent
Neural Networks are used for text classification, while K-binary
transformation, rank method and probabilistic classification with learned
probability threshold are employed for the multi-label problem involved in the
genre tagging task.Experiments with more than 250,000 movies show that
employing the Gated Recurrent Units (GRU) neural networks for the probabilistic
classification with learned probability threshold approach achieves the best
result on the test set. The model attains a Jaccard Index of 50.0%, a F-score
of 0.56, and a hit rate of 80.5%.",['Quan Hoang'],"['cs.CL', 'cs.LG', 'stat.ML']",2018-01-15 14:11:57+00:00
http://arxiv.org/abs/1801.04701v1,tau-FPL: Tolerance-Constrained Learning in Linear Time,"Learning a classifier with control on the false-positive rate plays a
critical role in many machine learning applications. Existing approaches either
introduce prior knowledge dependent label cost or tune parameters based on
traditional classifiers, which lack consistency in methodology because they do
not strictly adhere to the false-positive rate constraint. In this paper, we
propose a novel scoring-thresholding approach, tau-False Positive Learning
(tau-FPL) to address this problem. We show the scoring problem which takes the
false-positive rate tolerance into accounts can be efficiently solved in linear
time, also an out-of-bootstrap thresholding method can transform the learned
ranking function into a low false-positive classifier. Both theoretical
analysis and experimental results show superior performance of the proposed
tau-FPL over existing approaches.","['Ao Zhang', 'Nan Li', 'Jian Pu', 'Jun Wang', 'Junchi Yan', 'Hongyuan Zha']","['cs.LG', 'cs.AI', 'stat.ML']",2018-01-15 08:56:49+00:00
http://arxiv.org/abs/1801.04695v3,Sparsity-based Defense against Adversarial Attacks on Linear Classifiers,"Deep neural networks represent the state of the art in machine learning in a
growing number of fields, including vision, speech and natural language
processing. However, recent work raises important questions about the
robustness of such architectures, by showing that it is possible to induce
classification errors through tiny, almost imperceptible, perturbations.
Vulnerability to such ""adversarial attacks"", or ""adversarial examples"", has
been conjectured to be due to the excessive linearity of deep networks. In this
paper, we study this phenomenon in the setting of a linear classifier, and show
that it is possible to exploit sparsity in natural data to combat
$\ell_{\infty}$-bounded adversarial perturbations. Specifically, we demonstrate
the efficacy of a sparsifying front end via an ensemble averaged analysis, and
experimental results for the MNIST handwritten digit database. To the best of
our knowledge, this is the first work to show that sparsity provides a
theoretically rigorous framework for defense against adversarial attacks.","['Zhinus Marzi', 'Soorya Gopalakrishnan', 'Upamanyu Madhow', 'Ramtin Pedarsani']","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2018-01-15 08:18:33+00:00
http://arxiv.org/abs/1801.04693v1,Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks,"Machine learning systems based on deep neural networks, being able to produce
state-of-the-art results on various perception tasks, have gained mainstream
adoption in many applications. However, they are shown to be vulnerable to
adversarial example attack, which generates malicious output by adding slight
perturbations to the input. Previous adversarial example crafting methods,
however, use simple metrics to evaluate the distances between the original
examples and the adversarial ones, which could be easily detected by human
eyes. In addition, these attacks are often not robust due to the inevitable
noises and deviation in the physical world. In this work, we present a new
adversarial example attack crafting method, which takes the human perceptual
system into consideration and maximizes the noise tolerance of the crafted
adversarial example. Experimental results demonstrate the efficacy of the
proposed technique.","['Bo Luo', 'Yannan Liu', 'Lingxiao Wei', 'Qiang Xu']","['cs.LG', 'cs.CR', 'stat.ML']",2018-01-15 08:15:33+00:00
http://arxiv.org/abs/1801.04590v4,Frame-Recurrent Video Super-Resolution,"Recent advances in video super-resolution have shown that convolutional
neural networks combined with motion compensation are able to merge information
from multiple low-resolution (LR) frames to generate high-quality images.
Current state-of-the-art methods process a batch of LR frames to generate a
single high-resolution (HR) frame and run this scheme in a sliding window
fashion over the entire video, effectively treating the problem as a large
number of separate multi-frame super-resolution tasks. This approach has two
main weaknesses: 1) Each input frame is processed and warped multiple times,
increasing the computational cost, and 2) each output frame is estimated
independently conditioned on the input frames, limiting the system's ability to
produce temporally consistent results.
  In this work, we propose an end-to-end trainable frame-recurrent video
super-resolution framework that uses the previously inferred HR estimate to
super-resolve the subsequent frame. This naturally encourages temporally
consistent results and reduces the computational cost by warping only one image
in each step. Furthermore, due to its recurrent nature, the proposed method has
the ability to assimilate a large number of previous frames without increased
computational demands. Extensive evaluations and comparisons with previous
methods validate the strengths of our approach and demonstrate that the
proposed framework is able to significantly outperform the current state of the
art.","['Mehdi S. M. Sajjadi', 'Raviteja Vemulapalli', 'Matthew Brown']","['cs.CV', 'cs.AI', 'stat.ML']",2018-01-14 17:53:53+00:00
http://arxiv.org/abs/1801.04540v2,Fix your classifier: the marginal value of training the last weight layer,"Neural networks are commonly used as models for classification for a wide
variety of tasks. Typically, a learned affine transformation is placed at the
end of such models, yielding a per-class value used for classification. This
classifier can have a vast number of parameters, which grows linearly with the
number of possible classes, thus requiring increasingly more resources. In this
work we argue that this classifier can be fixed, up to a global scale constant,
with little or no loss of accuracy for most tasks, allowing memory and
computational benefits. Moreover, we show that by initializing the classifier
with a Hadamard matrix we can speed up inference as well. We discuss the
implications for current understanding of neural network models.","['Elad Hoffer', 'Itay Hubara', 'Daniel Soudry']","['cs.LG', 'cs.CV', 'stat.ML']",2018-01-14 12:00:43+00:00
http://arxiv.org/abs/1801.04510v2,Brain EEG Time Series Selection: A Novel Graph-Based Approach for Classification,"Brain Electroencephalography (EEG) classification is widely applied to
analyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs
degrade the diagnosis performance and most previously developed methods ignore
the necessity of EEG selection for classification. To this end, this paper
proposes a novel maximum weight clique-based EEG selection approach, named
mwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques
from an improved Fr\'{e}chet distance-weighted undirected EEG graph
simultaneously considering edge weights and vertex weights. Our mwcEEGs
improves the classification performance by selecting intra-clique pairwise
similar and inter-clique discriminative EEGs with similarity threshold
$\delta$. Experimental results demonstrate the algorithm effectiveness compared
with the state-of-the-art time series selection algorithms on real-world EEG
datasets.","['Chenglong Dai', 'Jia Wu', 'Dechang Pi', 'Lin Cui']","['cs.LG', 'q-bio.NC', 'stat.ML']",2018-01-14 04:51:22+00:00
http://arxiv.org/abs/1801.04503v2,Multivariate LSTM-FCNs for Time Series Classification,"Over the past decade, multivariate time series classification has received
great attention. We propose transforming the existing univariate time series
classification models, the Long Short Term Memory Fully Convolutional Network
(LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN), into a multivariate time series
classification model by augmenting the fully convolutional block with a
squeeze-and-excitation block to further improve accuracy. Our proposed models
outperform most state-of-the-art models while requiring minimum preprocessing.
The proposed models work efficiently on various complex multivariate time
series classification tasks such as activity recognition or action recognition.
Furthermore, the proposed models are highly efficient at test time and small
enough to deploy on memory constrained systems.","['Fazle Karim', 'Somshubra Majumdar', 'Houshang Darabi', 'Samuel Harford']","['cs.LG', 'stat.ML']",2018-01-14 03:00:53+00:00
http://arxiv.org/abs/1801.04378v2,Fairness in Supervised Learning: An Information Theoretic Approach,"Automated decision making systems are increasingly being used in real-world
applications. In these systems for the most part, the decision rules are
derived by minimizing the training error on the available historical data.
Therefore, if there is a bias related to a sensitive attribute such as gender,
race, religion, etc. in the data, say, due to cultural/historical
discriminatory practices against a certain demographic, the system could
continue discrimination in decisions by including the said bias in its decision
rule. We present an information theoretic framework for designing fair
predictors from data, which aim to prevent discrimination against a specified
sensitive attribute in a supervised learning setting. We use equalized odds as
the criterion for discrimination, which demands that the prediction should be
independent of the protected attribute conditioned on the actual label. To
ensure fairness and generalization simultaneously, we compress the data to an
auxiliary variable, which is used for the prediction task. This auxiliary
variable is chosen such that it is decontaminated from the discriminatory
attribute in the sense of equalized odds. The final predictor is obtained by
applying a Bayesian decision rule to the auxiliary variable.","['AmirEmad Ghassami', 'Sajad Khodadadian', 'Negar Kiyavash']","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",2018-01-13 04:03:04+00:00
http://arxiv.org/abs/1801.04342v3,Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs,"Neural programming involves training neural networks to learn programs,
mathematics, or logic from data. Previous works have failed to achieve good
generalization performance, especially on problems and programs with high
complexity or on large domains. This is because they mostly rely either on
black-box function evaluations that do not capture the structure of the
program, or on detailed execution traces that are expensive to obtain, and
hence the training data has poor coverage of the domain under consideration. We
present a novel framework that utilizes black-box function evaluations, in
conjunction with symbolic expressions that define relationships between the
given functions. We employ tree LSTMs to incorporate the structure of the
symbolic expression trees. We use tree encoding for numbers present in function
evaluation data, based on their decimal representation. We present an
evaluation benchmark for this task to demonstrate our proposed model combines
symbolic reasoning and function evaluation in a fruitful manner, obtaining high
accuracies in our experiments. Our framework generalizes significantly better
to expressions of higher depth and is able to fill partial equations with valid
completions.","['Forough Arabshahi', 'Sameer Singh', 'Animashree Anandkumar']","['cs.LG', 'cs.AI', 'stat.ML']",2018-01-12 22:24:42+00:00
http://arxiv.org/abs/1801.04339v3,Estimating the Number of Connected Components in a Graph via Subgraph Sampling,"Learning properties of large graphs from samples has been an important
problem in statistical network analysis since the early work of Goodman
\cite{Goodman1949} and Frank \cite{Frank1978}. We revisit a problem formulated
by Frank \cite{Frank1978} of estimating the number of connected components in a
large graph based on the subgraph sampling model, in which we randomly sample a
subset of the vertices and observe the induced subgraph. The key question is
whether accurate estimation is achievable in the \emph{sublinear} regime where
only a vanishing fraction of the vertices are sampled. We show that it is
impossible if the parent graph is allowed to contain high-degree vertices or
long induced cycles. For the class of chordal graphs, where induced cycles of
length four or above are forbidden, we characterize the optimal sample
complexity within constant factors and construct linear-time estimators that
provably achieve these bounds. This significantly expands the scope of previous
results which have focused on unbiased estimators and special classes of graphs
such as forests or cliques.
  Both the construction and the analysis of the proposed methodology rely on
combinatorial properties of chordal graphs and identities of induced subgraph
counts. They, in turn, also play a key role in proving minimax lower bounds
based on construction of random instances of graphs with matching structures of
small subgraphs.","['Jason M. Klusowski', 'Yihong Wu']","['math.ST', 'cs.DM', 'cs.LG', 'stat.ML', 'stat.TH', '62D05, 62C20']",2018-01-12 22:13:48+00:00
http://arxiv.org/abs/1801.04295v1,"Generalization Error Bounds for Noisy, Iterative Algorithms","In statistical learning theory, generalization error is used to quantify the
degree to which a supervised machine learning algorithm may overfit to training
data. Recent work [Xu and Raginsky (2017)] has established a bound on the
generalization error of empirical risk minimization based on the mutual
information $I(S;W)$ between the algorithm input $S$ and the algorithm output
$W$, when the loss function is sub-Gaussian. We leverage these results to
derive generalization error bounds for a broad class of iterative algorithms
that are characterized by bounded, noisy updates with Markovian structure. Our
bounds are very general and are applicable to numerous settings of interest,
including stochastic gradient Langevin dynamics (SGLD) and variants of the
stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm. Furthermore, our
error bounds hold for any output function computed over the path of iterates,
including the last iterate of the algorithm or the average of subsets of
iterates, and also allow for non-uniform sampling of data in successive updates
of the algorithm.","['Ankit Pensia', 'Varun Jog', 'Po-Ling Loh']","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",2018-01-12 19:26:35+00:00
http://arxiv.org/abs/1801.04289v1,Asynchronous Stochastic Variational Inference,"Stochastic variational inference (SVI) employs stochastic optimization to
scale up Bayesian computation to massive data. Since SVI is at its core a
stochastic gradient-based algorithm, horizontal parallelism can be harnessed to
allow larger scale inference. We propose a lock-free parallel implementation
for SVI which allows distributed computations over multiple slaves in an
asynchronous style. We show that our implementation leads to linear speed-up
while guaranteeing an asymptotic ergodic convergence rate $O(1/\sqrt(T)$ )
given that the number of slaves is bounded by $\sqrt(T)$ ($T$ is the total
number of iterations). The implementation is done in a high-performance
computing (HPC) environment using message passing interface (MPI) for python
(MPI4py). The extensive empirical evaluation shows that our parallel SVI is
lossless, performing comparably well to its counterpart serial SVI with linear
speed-up.","['Saad Mohamad', 'Abdelhamid Bouchachia', 'Moamar Sayed-Mouchaweh']","['stat.ML', 'cs.DC', 'cs.LG']",2018-01-12 19:05:09+00:00
http://arxiv.org/abs/1801.05055v1,Toward Metric Indexes for Incremental Insertion and Querying,"In this work we explore the use of metric index structures, which accelerate
nearest neighbor queries, in the scenario where we need to interleave
insertions and queries during deployment. This use-case is inspired by a
real-life need in malware analysis triage, and is surprisingly understudied.
Existing literature tends to either focus on only final query efficiency, often
does not support incremental insertion, or does not support arbitrary distance
metrics. We modify and improve three algorithms to support our scenario of
incremental insertion and querying with arbitrary metrics, and evaluate them on
multiple datasets and distance metrics while varying the value of $k$ for the
desired number of nearest neighbors. In doing so we determine that our improved
Vantage-Point tree of Minimum-Variance performs best for this scenario.","['Edward Raff', 'Charles Nicholas']","['cs.DS', 'cs.DB', 'stat.ML']",2018-01-12 16:25:16+00:00
http://arxiv.org/abs/1801.04212v2,Multinomial logistic model for coinfection diagnosis between arbovirus and malaria in Kedougou,"In tropical regions, populations continue to suffer morbidity and mortality
from malaria and arboviral diseases. In Kedougou (Senegal), these illnesses are
all endemic due to the climate and its geographical position. The
co-circulation of malaria parasites and arboviruses can explain the observation
of coinfected cases. Indeed there is strong resemblance in symptoms between
these diseases making problematic targeted medical care of coinfected cases.
This is due to the fact that the origin of illness is not obviously known. Some
cases could be immunized against one or the other of the pathogens, immunity
typically acquired with factors like age and exposure as usual for endemic
area. Then, coinfection needs to be better diagnosed. Using data collected from
patients in Kedougou region, from 2009 to 2013, we adjusted a multinomial
logistic model and selected relevant variables in explaining coinfection
status. We observed specific sets of variables explaining each of the diseases
exclusively and the coinfection. We tested the independence between arboviral
and malaria infections and derived coinfection probabilities from the model
fitting. In case of a coinfection probability greater than a threshold value to
be calibrated on the data, duration of illness above 3 days and age above 10
years-old are mostly indicative of arboviral disease while body temperature
higher than 40{\textdegree}C and presence of nausea or vomiting symptoms during
the rainy season are mostly indicative of malaria disease.","['Mor Absa Loum', 'Marie-Anne Poursat', 'Abdourahmane Sow', 'Amadou Sall', 'Cheikh Loucoubar', 'Elisabeth Gassiat']","['stat.AP', 'math.ST', 'stat.ML', 'stat.TH']",2018-01-12 16:05:56+00:00
http://arxiv.org/abs/1801.04211v2,Towards Arbitrary Noise Augmentation - Deep Learning for Sampling from Arbitrary Probability Distributions,"Accurate noise modelling is important for training of deep learning
reconstruction algorithms. While noise models are well known for traditional
imaging techniques, the noise distribution of a novel sensor may be difficult
to determine a priori. Therefore, we propose learning arbitrary noise
distributions. To do so, this paper proposes a fully connected neural network
model to map samples from a uniform distribution to samples of any explicitly
known probability density function. During the training, the Jensen-Shannon
divergence between the distribution of the model's output and the target
distribution is minimized. We experimentally demonstrate that our model
converges towards the desired state. It provides an alternative to existing
sampling methods such as inversion sampling, rejection sampling, Gaussian
mixture models and Markov-Chain-Monte-Carlo. Our model has high sampling
efficiency and is easily applied to any probability distribution, without the
need of further analytical or numerical calculations.","['Felix Horger', 'Tobias Würfl', 'Vincent Christlein', 'Andreas Maier']","['cs.LG', 'stat.ML']",2018-01-12 16:03:21+00:00
http://arxiv.org/abs/1801.04159v2,Can Who-Edits-What Predict Edit Survival?,"As the number of contributors to online peer-production systems grows, it
becomes increasingly important to predict whether the edits that users make
will eventually be beneficial to the project. Existing solutions either rely on
a user reputation system or consist of a highly specialized predictor that is
tailored to a specific peer-production system. In this work, we explore a
different point in the solution space that goes beyond user reputation but does
not involve any content-based feature of the edits. We view each edit as a game
between the editor and the component of the project. We posit that the
probability that an edit is accepted is a function of the editor's skill, of
the difficulty of editing the component and of a user-component interaction
term. Our model is broadly applicable, as it only requires observing data about
who makes an edit, what the edit affects and whether the edit survives or not.
We apply our model on Wikipedia and the Linux kernel, two examples of
large-scale peer-production systems, and we seek to understand whether it can
effectively predict edit survival: in both cases, we provide a positive answer.
Our approach significantly outperforms those based solely on user reputation
and bridges the gap with specialized predictors that use content-based
features. It is simple to implement, computationally inexpensive, and in
addition it enables us to discover interesting structure in the data.","['Ali Batuhan Yardım', 'Victor Kristof', 'Lucas Maystre', 'Matthias Grossglauser']","['stat.AP', 'cs.SI', 'stat.ML']",2018-01-12 13:26:57+00:00
http://arxiv.org/abs/1801.04153v7,Bayesian Quadrature for Multiple Related Integrals,"Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithful manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then prove convergence rates for the method in the well-specified and
misspecified cases, and demonstrate its efficiency in the context of
multi-fidelity models for complex engineering systems and a problem of global
illumination in computer graphics.","['Xiaoyue Xi', 'François-Xavier Briol', 'Mark Girolami']","['stat.CO', 'cs.NA', 'math.NA', 'stat.ML']",2018-01-12 12:49:32+00:00
http://arxiv.org/abs/1801.04140v1,Cosmic String Detection with Tree-Based Machine Learning,"We explore the use of random forest and gradient boosting, two powerful
tree-based machine learning algorithms, for the detection of cosmic strings in
maps of the cosmic microwave background (CMB), through their unique
Gott-Kaiser-Stebbins effect on the temperature anisotropies.The information in
the maps is compressed into feature vectors before being passed to the learning
units. The feature vectors contain various statistical measures of processed
CMB maps that boost the cosmic string detectability. Our proposed classifiers,
after training, give results improved over or similar to the claimed
detectability levels of the existing methods for string tension, $G\mu$. They
can make $3\sigma$ detection of strings with $G\mu \gtrsim 2.1\times 10^{-10}$
for noise-free, $0.9'$-resolution CMB observations. The minimum detectable
tension increases to $G\mu \gtrsim 3.0\times 10^{-8}$ for a more realistic, CMB
S4-like (II) strategy, still a significant improvement over the previous
results.","['A. Vafaei Sadr', 'M. Farhang', 'S. M. S. Movahed', 'B. Bassett', 'M. Kunz']","['astro-ph.CO', 'astro-ph.IM', 'physics.data-an', 'stat.ML']",2018-01-12 11:57:22+00:00
http://arxiv.org/abs/1801.04062v5,MINE: Mutual Information Neural Estimation,"We argue that the estimation of mutual information between high dimensional
continuous random variables can be achieved by gradient descent over neural
networks. We present a Mutual Information Neural Estimator (MINE) that is
linearly scalable in dimensionality as well as in sample size, trainable
through back-prop, and strongly consistent. We present a handful of
applications on which MINE can be used to minimize or maximize mutual
information. We apply MINE to improve adversarially trained generative models.
We also use MINE to implement Information Bottleneck, applying it to supervised
classification; our results demonstrate substantial improvement in flexibility
and performance in these settings.","['Mohamed Ishmael Belghazi', 'Aristide Baratin', 'Sai Rajeswar', 'Sherjil Ozair', 'Yoshua Bengio', 'Aaron Courville', 'R Devon Hjelm']","['cs.LG', 'stat.ML']",2018-01-12 05:42:58+00:00
http://arxiv.org/abs/1801.04055v1,A3T: Adversarially Augmented Adversarial Training,"Recent research showed that deep neural networks are highly sensitive to
so-called adversarial perturbations, which are tiny perturbations of the input
data purposely designed to fool a machine learning classifier. Most
classification models, including deep learning models, are highly vulnerable to
adversarial attacks. In this work, we investigate a procedure to improve
adversarial robustness of deep neural networks through enforcing representation
invariance. The idea is to train the classifier jointly with a discriminator
attached to one of its hidden layer and trained to filter the adversarial
noise. We perform preliminary experiments to test the viability of the approach
and to compare it to other standard adversarial training methods.","['Akram Erraqabi', 'Aristide Baratin', 'Yoshua Bengio', 'Simon Lacoste-Julien']","['cs.LG', 'stat.ML']",2018-01-12 04:34:17+00:00
http://arxiv.org/abs/1801.04053v1,Noisy Expectation-Maximization: Applications and Generalizations,"We present a noise-injected version of the Expectation-Maximization (EM)
algorithm: the Noisy Expectation Maximization (NEM) algorithm. The NEM
algorithm uses noise to speed up the convergence of the EM algorithm. The NEM
theorem shows that injected noise speeds up the average convergence of the EM
algorithm to a local maximum of the likelihood surface if a positivity
condition holds. The generalized form of the noisy expectation-maximization
(NEM) algorithm allow for arbitrary modes of noise injection including adding
and multiplying noise to the data.
  We demonstrate these noise benefits on EM algorithms for the Gaussian mixture
model (GMM) with both additive and multiplicative NEM noise injection. A
separate theorem (not presented here) shows that the noise benefit for
independent identically distributed additive noise decreases with sample size
in mixture models. This theorem implies that the noise benefit is most
pronounced if the data is sparse. Injecting blind noise only slowed
convergence.","['Osonde Osoba', 'Bart Kosko']","['stat.ML', 'cs.LG']",2018-01-12 04:09:48+00:00
http://arxiv.org/abs/1801.04016v1,Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution,"Current machine learning systems operate, almost exclusively, in a
statistical, or model-free mode, which entails severe theoretical limits on
their power and performance. Such systems cannot reason about interventions and
retrospection and, therefore, cannot serve as the basis for strong AI. To
achieve human level intelligence, learning machines need the guidance of a
model of reality, similar to the ones used in causal inference tasks. To
demonstrate the essential role of such models, I will present a summary of
seven tasks which are beyond reach of current machine learning systems and
which have been accomplished using the tools of causal modeling.",['Judea Pearl'],"['cs.LG', 'cs.AI', 'stat.ML']",2018-01-11 23:37:48+00:00
http://arxiv.org/abs/1801.03911v2,Stochastic Learning of Nonstationary Kernels for Natural Language Modeling,"Natural language processing often involves computations with semantic or
syntactic graphs to facilitate sophisticated reasoning based on structural
relationships. While convolution kernels provide a powerful tool for comparing
graph structure based on node (word) level relationships, they are difficult to
customize and can be computationally expensive. We propose a generalization of
convolution kernels, with a nonstationary model, for better expressibility of
natural languages in supervised settings. For a scalable learning of the
parameters introduced with our model, we propose a novel algorithm that
leverages stochastic sampling on k-nearest neighbor graphs, along with
approximations based on locality-sensitive hashing. We demonstrate the
advantages of our approach on a challenging real-world (structured inference)
problem of automatically extracting biological models from the text of
scientific papers.","['Sahil Garg', 'Greg Ver Steeg', 'Aram Galstyan']","['cs.CL', 'cs.IR', 'cs.LG', 'stat.ML']",2018-01-11 18:24:02+00:00
http://arxiv.org/abs/1801.03851v3,Autoencoders and Probabilistic Inference with Missing Data: An Exact Solution for The Factor Analysis Case,"Latent variable models can be used to probabilistically ""fill-in"" missing
data entries. The variational autoencoder architecture (Kingma and Welling,
2014; Rezende et al., 2014) includes a ""recognition"" or ""encoder"" network that
infers the latent variables given the data variables. However, it is not clear
how to handle missing data variables in this network. The factor analysis (FA)
model is a basic autoencoder, using linear encoder and decoder networks. We
show how to calculate exactly the latent posterior distribution for the factor
analysis (FA) model in the presence of missing data, and note that this
solution implies that a different encoder network is required for each pattern
of missingness. We also discuss various approximations to the exact solution.
Experiments compare the effectiveness of various approaches to filling in the
missing data.","['Christopher K. I. Williams', 'Charlie Nash', 'Alfredo Nazábal']","['cs.LG', 'stat.ML']",2018-01-11 16:22:07+00:00
http://arxiv.org/abs/1801.03765v2,Non-stationary Douglas-Rachford and alternating direction method of multipliers: adaptive stepsizes and convergence,"We revisit the classical Douglas-Rachford (DR) method for finding a zero of
the sum of two maximal monotone operators. Since the practical performance of
the DR method crucially depends on the stepsizes, we aim at developing an
adaptive stepsize rule. To that end, we take a closer look at a linear case of
the problem and use our findings to develop a stepsize strategy that eliminates
the need for stepsize tuning. We analyze a general non-stationary DR scheme and
prove its convergence for a convergent sequence of stepsizes with summable
increments. This, in turn, proves the convergence of the method with the new
adaptive stepsize rule. We also derive the related non-stationary alternating
direction method of multipliers (ADMM) from such a non-stationary DR method. We
illustrate the efficiency of the proposed methods on several numerical
examples.","['Dirk A. Lorenz', 'Quoc Tran-Dinh']","['math.OC', 'math.NA', 'stat.ML', '90C25, 65K05, 65J15, 47H05']",2018-01-11 14:13:16+00:00
http://arxiv.org/abs/1801.03749v3,Improved asynchronous parallel optimization analysis for stochastic incremental methods,"As datasets continue to increase in size and multi-core computer
architectures are developed, asynchronous parallel optimization algorithms
become more and more essential to the field of Machine Learning. Unfortunately,
conducting the theoretical analysis asynchronous methods is difficult, notably
due to the introduction of delay and inconsistency in inherently sequential
algorithms. Handling these issues often requires resorting to simplifying but
unrealistic assumptions. Through a novel perspective, we revisit and clarify a
subtle but important technical issue present in a large fraction of the recent
convergence rate proofs for asynchronous parallel optimization algorithms, and
propose a simplification of the recently introduced ""perturbed iterate""
framework that resolves it. We demonstrate the usefulness of our new framework
by analyzing three distinct asynchronous parallel incremental optimization
algorithms: Hogwild (asynchronous SGD), KROMAGNON (asynchronous SVRG) and
ASAGA, a novel asynchronous parallel version of the incremental gradient
algorithm SAGA that enjoys fast linear convergence rates. We are able to both
remove problematic assumptions and obtain better theoretical results. Notably,
we prove that ASAGA and KROMAGNON can obtain a theoretical linear speedup on
multi-core systems even without sparsity assumptions. We present results of an
implementation on a 40-core architecture illustrating the practical speedups as
well as the hardware overhead. Finally, we investigate the overlap constant, an
ill-understood but central quantity for the theoretical analysis of
asynchronous parallel algorithms. We find that it encompasses much more
complexity than suggested in previous work, and often is order-of-magnitude
bigger than traditionally thought.","['Rémi Leblond', 'Fabian Pedregosa', 'Simon Lacoste-Julien']","['math.OC', 'cs.LG', 'stat.ML']",2018-01-11 13:31:33+00:00
http://arxiv.org/abs/1801.03744v3,Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?,"We give a rigorous analysis of the statistical behavior of gradients in a
randomly initialized fully connected network N with ReLU activations. Our
results show that the empirical variance of the squares of the entries in the
input-output Jacobian of N is exponential in a simple architecture-dependent
constant beta, given by the sum of the reciprocals of the hidden layer widths.
When beta is large, the gradients computed by N at initialization vary wildly.
Our approach complements the mean field theory analysis of random networks.
From this point of view, we rigorously compute finite width corrections to the
statistics of gradients at the edge of chaos.",['Boris Hanin'],"['stat.ML', 'cs.LG', 'math.PR', 'math.ST', 'stat.TH']",2018-01-11 13:17:22+00:00
http://arxiv.org/abs/1801.03714v1,Multi-Band Covariance Interpolation with Applications in Massive MIMO,"In this paper, we study the problem of multi-band (frequency-variant)
covariance interpolation with a particular emphasis towards massive MIMO
applications. In a massive MIMO system, the communication between each BS with
$M \gg 1$ antennas and each single-antenna user occurs through a collection of
scatterers in the environment, where the channel vector of each user at BS
antennas consists in a weighted linear combination of the array responses of
the scatterers, where each scatterer has its own angle of arrival (AoA) and
complex channel gain. The array response at a given AoA depends on the
wavelength of the incoming planar wave and is naturally frequency dependent.
This results in a frequency-dependent distortion where the second order
statistics, i.e., the covariance matrix, of the channel vectors varies with
frequency. In this paper, we show that although this effect is generally
negligible for a small number of antennas $M$, it results in a considerable
distortion of the covariance matrix and especially its dominant signal subspace
in the massive MIMO regime where $M \to \infty$, and can generally incur a
serious degradation of the performance especially in frequency division
duplexing (FDD) massive MIMO systems where the uplink (UL) and the downlink
(DL) communication occur over different frequency bands. We propose a novel
UL-DL covariance interpolation technique that is able to recover the covariance
matrix in the DL from an estimate of the covariance matrix in the UL under a
mild reciprocity condition on the angular power spread function (PSF) of the
users. We analyze the performance of our proposed scheme mathematically and
prove its robustness under a sufficiently large spatial oversampling of the
array. We also propose several simple off-the-shelf algorithms for UL-DL
covariance interpolation and evaluate their performance via numerical
simulations.","['Saeid Haghighatshoar', 'Mahdi Barzegar Khalilsarai', 'Giuseppe Caire']","['cs.IT', 'math.IT', 'stat.ML']",2018-01-11 11:21:03+00:00
http://arxiv.org/abs/1801.05856v2,Active Community Detection with Maximal Expected Model Change,"We present a novel active learning algorithm for community detection on
networks. Our proposed algorithm uses a Maximal Expected Model Change (MEMC)
criterion for querying network nodes label assignments. MEMC detects nodes that
maximally change the community assignment likelihood model following a query.
Our method is inspired by detection in the benchmark Stochastic Block Model
(SBM), where we provide sample complexity analysis and empirical study with SBM
and real network data for binary as well as for the multi-class settings. The
analysis also covers the most challenging case of sparse degree and
below-detection-threshold SBMs, where we observe a super-linear error
reduction. MEMC is shown to be superior to the random selection baseline and
other state-of-the-art active learners.","['Dan Kushnir', 'Benjamin Mirabelli']","['cs.SI', 'cs.LG', 'stat.ML']",2018-01-11 03:26:16+00:00
http://arxiv.org/abs/1801.03558v3,Inference Suboptimality in Variational Autoencoders,"Amortized inference allows latent-variable models trained via variational
learning to scale to large datasets. The quality of approximate inference is
determined by two factors: a) the capacity of the variational distribution to
match the true posterior and b) the ability of the recognition network to
produce good variational parameters for each datapoint. We examine approximate
inference in variational autoencoders in terms of these factors. We find that
divergence from the true posterior is often due to imperfect recognition
networks, rather than the limited complexity of the approximating distribution.
We show that this is due partly to the generator learning to accommodate the
choice of approximation. Furthermore, we show that the parameters used to
increase the expressiveness of the approximation play a role in generalizing
inference rather than simply improving the complexity of the approximation.","['Chris Cremer', 'Xuechen Li', 'David Duvenaud']","['cs.LG', 'stat.ML']",2018-01-10 21:24:59+00:00
http://arxiv.org/abs/1801.03454v2,Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks,"In an effort to understand the meaning of the intermediate representations
captured by deep networks, recent papers have tried to associate specific
semantic concepts to individual neural network filter responses, where
interesting correlations are often found, largely by focusing on extremal
filter responses. In this paper, we show that this approach can favor
easy-to-interpret cases that are not necessarily representative of the average
behavior of a representation.
  A more realistic but harder-to-study hypothesis is that semantic
representations are distributed, and thus filters must be studied in
conjunction. In order to investigate this idea while enabling systematic
visualization and quantification of multiple filter responses, we introduce the
Net2Vec framework, in which semantic concepts are mapped to vectorial
embeddings based on corresponding filter responses. By studying such
embeddings, we are able to show that 1., in most cases, multiple filters are
required to code for a concept, that 2., often filters are not concept specific
and help encode multiple concepts, and that 3., compared to single filter
activations, filter embeddings are able to better characterize the meaning of a
representation and its relationship to other concepts.","['Ruth Fong', 'Andrea Vedaldi']","['cs.CV', 'cs.AI', 'stat.ML']",2018-01-10 17:01:36+00:00
http://arxiv.org/abs/1801.03437v2,Approximation beats concentration? An approximation view on inference with smooth radial kernels,"Positive definite kernels and their associated Reproducing Kernel Hilbert
Spaces provide a mathematically compelling and practically competitive
framework for learning from data.
  In this paper we take the approximation theory point of view to explore
various aspects of smooth kernels related to their inferential properties. We
analyze eigenvalue decay of kernels operators and matrices, properties of
eigenfunctions/eigenvectors and ""Fourier"" coefficients of functions in the
kernel space restricted to a discrete set of data points. We also investigate
the fitting capacity of kernels, giving explicit bounds on the fat shattering
dimension of the balls in Reproducing Kernel Hilbert spaces. Interestingly, the
same properties that make kernels very effective approximators for functions in
their ""native"" kernel space, also limit their capacity to represent arbitrary
functions. We discuss various implications, including those for gradient
descent type methods.
  It is important to note that most of our bounds are measure independent.
Moreover, at least in moderate dimension, the bounds for eigenvalues are much
tighter than the bounds which can be obtained from the usual matrix
concentration results. For example, we see that the eigenvalues of kernel
matrices show nearly exponential decay with constants depending only on the
kernel and the domain. We call this ""approximation beats concentration""
phenomenon as even when the data are sampled from a probability distribution,
some of their aspects are better understood in terms of approximation theory.",['Mikhail Belkin'],"['cs.LG', 'stat.ML']",2018-01-10 16:13:15+00:00
http://arxiv.org/abs/1801.06490v1,Worst-case Optimal Submodular Extensions for Marginal Estimation,"Submodular extensions of an energy function can be used to efficiently
compute approximate marginals via variational inference. The accuracy of the
marginals depends crucially on the quality of the submodular extension. To
identify the best possible extension, we show an equivalence between the
submodular extensions of the energy and the objective functions of linear
programming (LP) relaxations for the corresponding MAP estimation problem. This
allows us to (i) establish the worst-case optimality of the submodular
extension for Potts model used in the literature; (ii) identify the worst-case
optimal submodular extension for the more general class of metric labeling; and
(iii) efficiently compute the marginals for the widely used dense CRF model
with the help of a recently proposed Gaussian filtering method. Using synthetic
and real data, we show that our approach provides comparable upper bounds on
the log-partition function to those obtained using tree-reweighted message
passing (TRW) in cases where the latter is computationally feasible.
Importantly, unlike TRW, our approach provides the first practical algorithm to
compute an upper bound on the dense CRF model.","['Pankaj Pansari', 'Chris Russell', 'M. Pawan Kumar']","['cs.LG', 'cs.CV', 'stat.ML']",2018-01-10 14:36:57+00:00
http://arxiv.org/abs/1801.03329v3,Weakly Supervised One-Shot Detection with Attention Similarity Networks,"Neural network models that are not conditioned on class identities were shown
to facilitate knowledge transfer between classes and to be well-suited for
one-shot learning tasks. Following this motivation, we further explore and
establish such models and present a novel neural network architecture for the
task of weakly supervised one-shot detection. Our model is only conditioned on
a single exemplar of an unseen class and a larger target example that may or
may not contain an instance of the same class as the exemplar. By pairing a
Siamese similarity network with an attention mechanism, we design a model that
manages to simultaneously identify and localise instances of classes unseen at
training time. In experiments with datasets from the computer vision and audio
domains, the proposed method considerably outperforms the baseline methods for
the weakly supervised one-shot detection task.","['Gil Keren', 'Maximilian Schmitt', 'Thomas Kehrenberg', 'Björn Schuller']","['stat.ML', 'cs.LG']",2018-01-10 12:10:24+00:00
http://arxiv.org/abs/1801.03326v2,Expected Policy Gradients for Reinforcement Learning,"We propose expected policy gradients (EPG), which unify stochastic policy
gradients (SPG) and deterministic policy gradients (DPG) for reinforcement
learning. Inspired by expected sarsa, EPG integrates (or sums) across actions
when estimating the gradient, instead of relying only on the action in the
sampled trajectory. For continuous action spaces, we first derive a practical
result for Gaussian policies and quadratic critics and then extend it to a
universal analytical method, covering a broad class of actors and critics,
including Gaussian, exponential families, and policies with bounded support.
For Gaussian policies, we introduce an exploration method that uses covariance
proportional to the matrix exponential of the scaled Hessian of the critic with
respect to the actions. For discrete action spaces, we derive a variant of EPG
based on softmax policies. We also establish a new general policy gradient
theorem, of which the stochastic and deterministic policy gradient theorems are
special cases. Furthermore, we prove that EPG reduces the variance of the
gradient estimates without requiring deterministic policies and with little
computational overhead. Finally, we provide an extensive experimental
evaluation of EPG and show that it outperforms existing approaches on multiple
challenging control domains.","['Kamil Ciosek', 'Shimon Whiteson']","['stat.ML', 'cs.AI', 'I.2.8; G.3']",2018-01-10 11:59:59+00:00
http://arxiv.org/abs/1801.07756v5,Deep Learning for Electromyographic Hand Gesture Signal Classification Using Transfer Learning,"In recent years, deep learning algorithms have become increasingly more
prominent for their unparalleled ability to automatically learn discriminant
features from large amounts of data. However, within the field of
electromyography-based gesture recognition, deep learning algorithms are seldom
employed as they require an unreasonable amount of effort from a single person,
to generate tens of thousands of examples.
  This work's hypothesis is that general, informative features can be learned
from the large amounts of data generated by aggregating the signals of multiple
users, thus reducing the recording burden while enhancing gesture recognition.
Consequently, this paper proposes applying transfer learning on aggregated data
from multiple users, while leveraging the capacity of deep learning algorithms
to learn discriminant features from large datasets. Two datasets comprised of
19 and 17 able-bodied participants respectively (the first one is employed for
pre-training) were recorded for this work, using the Myo Armband. A third Myo
Armband dataset was taken from the NinaPro database and is comprised of 10
able-bodied participants. Three different deep learning networks employing
three different modalities as input (raw EMG, Spectrograms and Continuous
Wavelet Transform (CWT)) are tested on the second and third dataset. The
proposed transfer learning scheme is shown to systematically and significantly
enhance the performance for all three networks on the two datasets, achieving
an offline accuracy of 98.31% for 7 gestures over 17 participants for the
CWT-based ConvNet and 68.98% for 18 gestures over 10 participants for the raw
EMG-based ConvNet. Finally, a use-case study employing eight able-bodied
participants suggests that real-time feedback allows users to adapt their
muscle activation strategy which reduces the degradation in accuracy normally
experienced over time.","['Ulysse Côté-Allard', 'Cheikh Latyr Fall', 'Alexandre Drouin', 'Alexandre Campeau-Lecours', 'Clément Gosselin', 'Kyrre Glette', 'François Laviolette', 'Benoit Gosselin']","['cs.LG', 'stat.ML']",2018-01-10 11:42:30+00:00
http://arxiv.org/abs/1801.03265v3,More Adaptive Algorithms for Adversarial Bandits,"We develop a novel and generic algorithm for the adversarial multi-armed
bandit problem (or more generally the combinatorial semi-bandit problem). When
instantiated differently, our algorithm achieves various new data-dependent
regret bounds improving previous work. Examples include: 1) a regret bound
depending on the variance of only the best arm; 2) a regret bound depending on
the first-order path-length of only the best arm; 3) a regret bound depending
on the sum of first-order path-lengths of all arms as well as an important
negative term, which together lead to faster convergence rates for some normal
form games with partial feedback; 4) a regret bound that simultaneously implies
small regret when the best arm has small loss and logarithmic regret when there
exists an arm whose expected loss is always smaller than those of others by a
fixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last
two results, our algorithm is completely parameter-free.
  The main idea of our algorithm is to apply the optimism and adaptivity
techniques to the well-known Online Mirror Descent framework with a special
log-barrier regularizer. The challenges are to come up with appropriate
optimistic predictions and correction terms in this framework. Some of our
results also crucially rely on using a sophisticated increasing learning rate
schedule.","['Chen-Yu Wei', 'Haipeng Luo']","['cs.LG', 'stat.ML']",2018-01-10 08:28:00+00:00
http://arxiv.org/abs/1801.03226v1,Adaptive Graph Convolutional Neural Networks,"Graph Convolutional Neural Networks (Graph CNNs) are generalizations of
classical CNNs to handle graph data such as molecular data, point could and
social networks. Current filters in graph CNNs are built for fixed and shared
graph structure. However, for most real data, the graph structures varies in
both size and connectivity. The paper proposes a generalized and flexible graph
CNN taking data of arbitrary graph structure as input. In that way a
task-driven adaptive graph is learned for each graph data while training. To
efficiently learn the graph, a distance metric learning is proposed. Extensive
experiments on nine graph-structured datasets have demonstrated the superior
performance improvement on both convergence speed and predictive accuracy.","['Ruoyu Li', 'Sheng Wang', 'Feiyun Zhu', 'Junzhou Huang']","['cs.LG', 'stat.ML']",2018-01-10 03:17:45+00:00
http://arxiv.org/abs/1801.03222v2,Multivariate Bayesian Structural Time Series Model,"This paper deals with inference and prediction for multiple correlated time
series, where one has also the choice of using a candidate pool of
contemporaneous predictors for each target series. Starting with a structural
model for the time-series, Bayesian tools are used for model fitting,
prediction, and feature selection, thus extending some recent work along these
lines for the univariate case. The Bayesian paradigm in this multivariate
setting helps the model avoid overfitting as well as capture correlations among
the multiple time series with the various state components. The model provides
needed flexibility to choose a different set of components and available
predictors for each target series. The cyclical component in the model can
handle large variations in the short term, which may be caused by external
shocks. We run extensive simulations to investigate properties such as
estimation accuracy and performance in forecasting. We then run an empirical
study with one-step-ahead prediction on the max log return of a portfolio of
stocks that involve four leading financial institutions. Both the simulation
studies and the extensive empirical study confirm that this multivariate model
outperforms three other benchmark models, viz. a model that treats each target
series as independent, the autoregressive integrated moving average model with
regression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model.","['S. Rao Jammalamadaka', 'Jinwen Qiu', 'Ning Ning']",['stat.ML'],2018-01-10 02:48:34+00:00
http://arxiv.org/abs/1801.03164v1,Paranom: A Parallel Anomaly Dataset Generator,"In this paper, we present Paranom, a parallel anomaly dataset generator. We
discuss its design and provide brief experimental results demonstrating its
usefulness in improving the classification correctness of LSTM-AD, a
state-of-the-art anomaly detection model.",['Justin Gottschlich'],"['cs.LG', 'cs.AI', 'stat.ML']",2018-01-09 22:33:51+00:00
http://arxiv.org/abs/1801.03143v1,Comparing heterogeneous entities using artificial neural networks of trainable weighted structural components and machine-learned activation functions,"To compare entities of differing types and structural components, the
artificial neural network paradigm was used to cross-compare structural
components between heterogeneous documents. Trainable weighted structural
components were input into machine-learned activation functions of the neurons.
The model was used for matching news articles and videos, where the inputs and
activation functions respectively consisted of term vectors and cosine
similarity measures between the weighted structural components. The model was
tested with different weights, achieving as high as 59.2% accuracy for matching
videos to news articles. A mobile application user interface for recommending
related videos for news articles was developed to demonstrate consumer value,
including its potential usefulness for cross-selling products from unrelated
categories.","['Artit Wangperawong', 'Kettip Kriangchaivech', 'Austin Lanari', 'Supui Lam', 'Panthong Wangperawong']","['stat.ML', 'cs.AI', 'cs.IR', 'cs.LG', 'cs.NE']",2018-01-09 21:20:08+00:00
http://arxiv.org/abs/1801.03137v1,Convergence Analysis of Gradient Descent Algorithms with Proportional Updates,"The rise of deep learning in recent years has brought with it increasingly
clever optimization methods to deal with complex, non-linear loss functions.
These methods are often designed with convex optimization in mind, but have
been shown to work well in practice even for the highly non-convex optimization
associated with neural networks. However, one significant drawback of these
methods when they are applied to deep learning is that the magnitude of the
update step is sometimes disproportionate to the magnitude of the weights (much
smaller or larger), leading to training instabilities such as vanishing and
exploding gradients. An idea to combat this issue is gradient descent with
proportional updates. Gradient descent with proportional updates was introduced
in 2017. It was independently developed by You et al (Layer-wise Adaptive Rate
Scaling (LARS) algorithm) and by Abu-El-Haija (PercentDelta algorithm). The
basic idea of both of these algorithms is to make each step of the gradient
descent proportional to the current weight norm and independent of the gradient
magnitude. It is common in the context of new optimization methods to prove
convergence or derive regret bounds under the assumption of Lipschitz
continuity and convexity. However, even though LARS and PercentDelta were shown
to work well in practice, there is no theoretical analysis of the convergence
properties of these algorithms. Thus it is not clear if the idea of gradient
descent with proportional updates is used in the optimal way, or if it could be
improved by using a different norm or specific learning rate schedule, for
example. Moreover, it is not clear if these algorithms can be extended to other
problems, besides neural networks. We attempt to answer these questions by
establishing the theoretical analysis of gradient descent with proportional
updates, and verifying this analysis with empirical examples.","['Igor Gitman', 'Deepak Dilipkumar', 'Ben Parr']","['cs.LG', 'cs.AI', 'stat.ML']",2018-01-09 20:51:28+00:00
http://arxiv.org/abs/1801.03132v1,Robust Propensity Score Computation Method based on Machine Learning with Label-corrupted Data,"In biostatistics, propensity score is a common approach to analyze the
imbalance of covariate and process confounding covariates to eliminate
differences between groups. While there are an abundant amount of methods to
compute propensity score, a common issue of them is the corrupted labels in the
dataset. For example, the data collected from the patients could contain
samples that are treated mistakenly, and the computing methods could
incorporate them as a misleading information. In this paper, we propose a
Machine Learning-based method to handle the problem. Specifically, we utilize
the fact that the majority of sample should be labeled with the correct
instance and design an approach to first cluster the data with spectral
clustering and then sample a new dataset with a distribution processed from the
clustering results. The propensity score is computed by Xgboost, and a
mathematical justification of our method is provided in this paper. The
experimental results illustrate that xgboost propensity scores computing with
the data processed by our method could outperform the same method with original
data, and the advantages of our method increases as we add some artificial
corruptions to the dataset. Meanwhile, the implementation of xgboost to compute
propensity score for multiple treatments is also a pioneering work in the area.","['Chen Wang', 'Suzhen Wang', 'Fuyan Shi', 'Zaixiang Wang']","['stat.ME', 'cs.AI', 'stat.ML']",2018-01-09 20:35:31+00:00
http://arxiv.org/abs/1801.03050v3,Assessing the effect of advertising expenditures upon sales: a Bayesian structural time series model,"We propose a robust implementation of the Nerlove--Arrow model using a
Bayesian structural time series model to explain the relationship between
advertising expenditures of a country-wide fast-food franchise network with its
weekly sales. Thanks to the flexibility and modularity of the model, it is well
suited to generalization to other markets or situations. Its Bayesian nature
facilitates incorporating \emph{a priori} information (the manager's views),
which can be updated with relevant data. This aspect of the model will be used
to present a strategy of budget scheduling across time and channels.","['Víctor Gallego', 'Pablo Suárez-García', 'Pablo Angulo', 'David Gómez-Ullate']","['stat.ML', 'econ.EM', 'q-fin.RM', 'stat.AP']",2018-01-09 17:39:51+00:00
http://arxiv.org/abs/1801.02950v3,Adversarial Deep Learning for Robust Detection of Binary Encoded Malware,"Malware is constantly adapting in order to avoid detection. Model based
malware detectors, such as SVM and neural networks, are vulnerable to so-called
adversarial examples which are modest changes to detectable malware that allows
the resulting malware to evade detection. Continuous-valued methods that are
robust to adversarial examples of images have been developed using saddle-point
optimization formulations. We are inspired by them to develop similar methods
for the discrete, e.g. binary, domain which characterizes the features of
malware. A specific extra challenge of malware is that the adversarial examples
must be generated in a way that preserves their malicious functionality. We
introduce methods capable of generating functionally preserved adversarial
malware examples in the binary domain. Using the saddle-point formulation, we
incorporate the adversarial examples into the training of models that are
robust to them. We evaluate the effectiveness of the methods and others in the
literature on a set of Portable Execution~(PE) files. Comparison prompts our
introduction of an online measure computed during training to assess general
expectation of robustness.","['Abdullah Al-Dujaili', 'Alex Huang', 'Erik Hemberg', ""Una-May O'Reilly""]","['cs.CR', 'cs.LG', 'stat.ML']",2018-01-09 14:32:30+00:00
http://arxiv.org/abs/1801.02949v1,An efficient K -means clustering algorithm for massive data,"The analysis of continously larger datasets is a task of major importance in
a wide variety of scientific fields. In this sense, cluster analysis algorithms
are a key element of exploratory data analysis, due to their easiness in the
implementation and relatively low computational cost. Among these algorithms,
the K -means algorithm stands out as the most popular approach, besides its
high dependency on the initial conditions, as well as to the fact that it might
not scale well on massive datasets. In this article, we propose a recursive and
parallel approximation to the K -means algorithm that scales well on both the
number of instances and dimensionality of the problem, without affecting the
quality of the approximation. In order to achieve this, instead of analyzing
the entire dataset, we work on small weighted sets of points that mostly intend
to extract information from those regions where it is harder to determine the
correct cluster assignment of the original instances. In addition to different
theoretical properties, which deduce the reasoning behind the algorithm,
experimental results indicate that our method outperforms the state-of-the-art
in terms of the trade-off between number of distance computations and the
quality of the solution obtained.","['Marco Capó', 'Aritz Pérez', 'Jose A. Lozano']","['stat.ML', 'cs.LG']",2018-01-09 14:32:06+00:00
http://arxiv.org/abs/1801.02939v1,Deep Gaussian Processes with Decoupled Inducing Inputs,"Deep Gaussian Processes (DGP) are hierarchical generalizations of Gaussian
Processes (GP) that have proven to work effectively on a multiple supervised
regression tasks. They combine the well calibrated uncertainty estimates of GPs
with the great flexibility of multilayer models. In DGPs, given the inputs, the
outputs of the layers are Gaussian distributions parameterized by their means
and covariances. These layers are realized as Sparse GPs where the training
data is approximated using a small set of pseudo points. In this work, we show
that the computational cost of DGPs can be reduced with no loss in performance
by using a separate, smaller set of pseudo points when calculating the
layerwise variance while using a larger set of pseudo points when calculating
the layerwise mean. This enabled us to train larger models that have lower cost
and better predictive performance.","['Marton Havasi', 'José Miguel Hernández-Lobato', 'Juan José Murillo-Fuentes']",['stat.ML'],2018-01-09 14:01:53+00:00
http://arxiv.org/abs/1801.02929v2,Data Augmentation by Pairing Samples for Images Classification,"Data augmentation is a widely used technique in many machine learning tasks,
such as image classification, to virtually enlarge the training dataset size
and avoid overfitting. Traditional data augmentation techniques for image
classification tasks create new samples from the original training data by, for
example, flipping, distorting, adding a small amount of noise to, or cropping a
patch from an original image. In this paper, we introduce a simple but
surprisingly effective data augmentation technique for image classification
tasks. With our technique, named SamplePairing, we synthesize a new sample from
one image by overlaying another image randomly chosen from the training data
(i.e., taking an average of two images for each pixel). By using two images
randomly selected from the training set, we can generate $N^2$ new samples from
$N$ training samples. This simple data augmentation technique significantly
improved classification accuracy for all the tested datasets; for example, the
top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset
with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show
that our SamplePairing technique largely improved accuracy when the number of
samples in the training set was very small. Therefore, our technique is more
valuable for tasks with a limited amount of training data, such as medical
imaging tasks.",['Hiroshi Inoue'],"['cs.LG', 'cs.CV', 'stat.ML']",2018-01-09 13:37:11+00:00
http://arxiv.org/abs/1801.02901v2,Convexification of Neural Graph,"Traditionally, most complex intelligence architectures are extremely
non-convex, which could not be well performed by convex optimization. However,
this paper decomposes complex structures into three types of nodes: operators,
algorithms and functions. Iteratively, propagating from node to node along
edge, we prove that ""regarding the tree-structured neural graph, it is nearly
convex in each variable, when the other variables are fixed."" In fact, the
non-convex properties stem from circles and functions, which could be
transformed to be convex with our proposed \textit{\textbf{scale mechanism}}.
Experimentally, we justify our theoretical analysis by two practical
applications.",['Han Xiao'],"['cs.LG', 'stat.ML']",2018-01-09 11:57:41+00:00
http://arxiv.org/abs/1801.02858v4,"Scalable high-resolution forecasting of sparse spatiotemporal events with kernel methods: a winning solution to the NIJ ""Real-Time Crime Forecasting Challenge""","We propose a generic spatiotemporal event forecasting method, which we
developed for the National Institute of Justice's (NIJ) Real-Time Crime
Forecasting Challenge. Our method is a spatiotemporal forecasting model
combining scalable randomized Reproducing Kernel Hilbert Space (RKHS) methods
for approximating Gaussian processes with autoregressive smoothing kernels in a
regularized supervised learning framework. While the smoothing kernels capture
the two main approaches in current use in the field of crime forecasting,
kernel density estimation (KDE) and self-exciting point process (SEPP) models,
the RKHS component of the model can be understood as an approximation to the
popular log-Gaussian Cox Process model. For inference, we discretize the
spatiotemporal point pattern and learn a log-intensity function using the
Poisson likelihood and highly efficient gradient-based optimization methods.
Model hyperparameters including quality of RKHS approximation, spatial and
temporal kernel lengthscales, number of autoregressive lags, bandwidths for
smoothing kernels, as well as cell shape, size, and rotation, were learned
using crossvalidation. Resulting predictions significantly exceeded baseline
KDE estimates and SEPP models for sparse events.","['Seth Flaxman', 'Michael Chirico', 'Pau Pereira', 'Charles Loeffler']","['stat.ML', 'stat.AP']",2018-01-09 10:04:17+00:00
http://arxiv.org/abs/1801.02850v2,Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks,"Deep neural networks are vulnerable to adversarial examples. Prior defenses
attempted to make deep networks more robust by either changing the network
architecture or augmenting the training set with adversarial examples, but both
have inherent limitations. Motivated by recent research that shows outliers in
the training set have a high negative influence on the trained model, we
studied the relationship between model robustness and the quality of the
training set. We first show that outliers give the model better generalization
ability but weaker robustness. Next, we propose an adversarial example
detection framework, in which we design two methods for removing outliers from
training set to obtain the sanitized model and then detect adversarial example
by calculating the difference of outputs between the original and the sanitized
model. We evaluated the framework on both MNIST and SVHN. Based on the
difference measured by Kullback-Leibler divergence, we could detect adversarial
examples with accuracy between 94.67% to 99.89%.","['Yongshuai Liu', 'Jiyu Chen', 'Hao Chen']","['cs.CR', 'cs.LG', 'stat.ML']",2018-01-09 09:36:56+00:00
http://arxiv.org/abs/1801.02788v1,Sequential Preference-Based Optimization,"Many real-world engineering problems rely on human preferences to guide their
design and optimization. We present PrefOpt, an open source package to simplify
sequential optimization tasks that incorporate human preference feedback. Our
approach extends an existing latent variable model for binary preferences to
allow for observations of equivalent preference from users.","['Ian Dewancker', 'Jakob Bauer', 'Michael McCourt']","['cs.LG', 'cs.CE', 'cs.HC', 'stat.ML']",2018-01-09 04:13:11+00:00
http://arxiv.org/abs/1801.03523v1,Generative Models for Stochastic Processes Using Convolutional Neural Networks,"The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.",['Fernando Fernandes Neto'],"['stat.ML', 'cs.NE', 'physics.comp-ph', 'q-fin.CP']",2018-01-09 03:35:20+00:00
http://arxiv.org/abs/1801.02736v1,Modeling sepsis progression using hidden Markov models,"Characterizing a patient's progression through stages of sepsis is critical
for enabling risk stratification and adaptive, personalized treatment. However,
commonly used sepsis diagnostic criteria fail to account for significant
underlying heterogeneity, both between patients as well as over time in a
single patient. We introduce a hidden Markov model of sepsis progression that
explicitly accounts for patient heterogeneity. Benchmarked against two sepsis
diagnostic criteria, the model provides a useful tool to uncover a patient's
latent sepsis trajectory and to identify high-risk patients in whom more
aggressive therapy may be indicated.","['Brenden K. Petersen', 'Michael B. Mayhew', 'Kalvin O. E. Ogbuefi', 'John D. Greene', 'Vincent X. Liu', 'Priyadip Ray']","['stat.ML', 'q-bio.QM']",2018-01-09 01:02:32+00:00
http://arxiv.org/abs/1801.03815v1,Informed Group-Sparse Representation for Singing Voice Separation,"Singing voice separation attempts to separate the vocal and instrumental
parts of a music recording, which is a fundamental problem in music information
retrieval. Recent work on singing voice separation has shown that the low-rank
representation and informed separation approaches are both able to improve
separation quality. However, low-rank optimizations are computationally
inefficient due to the use of singular value decompositions. Therefore, in this
paper, we propose a new linear-time algorithm called informed group-sparse
representation, and use it to separate the vocals from music using pitch
annotations as side information. Experimental results on the iKala dataset
confirm the efficacy of our approach, suggesting that the music accompaniment
follows a group-sparse structure given a pre-trained instrumental dictionary.
We also show how our work can be easily extended to accommodate multiple
dictionaries using the DSD100 dataset.","['Tak-Shing T. Chan', 'Yi-Hsuan Yang']","['eess.AS', 'cs.IR', 'cs.SD', 'eess.SP', 'stat.ML']",2018-01-09 00:45:30+00:00
http://arxiv.org/abs/1801.03773v1,Polar $n$-Complex and $n$-Bicomplex Singular Value Decomposition and Principal Component Pursuit,"Informed by recent work on tensor singular value decomposition and circulant
algebra matrices, this paper presents a new theoretical bridge that unifies the
hypercomplex and tensor-based approaches to singular value decomposition and
robust principal component analysis. We begin our work by extending the
principal component pursuit to Olariu's polar $n$-complex numbers as well as
their bicomplex counterparts. In so doing, we have derived the polar
$n$-complex and $n$-bicomplex proximity operators for both the $\ell_1$- and
trace-norm regularizers, which can be used by proximal optimization methods
such as the alternating direction method of multipliers. Experimental results
on two sets of audio data show that our algebraically-informed formulation
outperforms tensor robust principal component analysis. We conclude with the
message that an informed definition of the trace norm can bridge the gap
between the hypercomplex and tensor-based approaches. Our approach can be seen
as a general methodology for generating other principal component pursuit
algorithms with proper algebraic structures.","['Tak-Shing T. Chan', 'Yi-Hsuan Yang']","['eess.SP', 'cs.MM', 'cs.SD', 'eess.AS', 'stat.ML']",2018-01-09 00:45:01+00:00
http://arxiv.org/abs/1801.03816v1,Complex and Quaternionic Principal Component Pursuit and Its Application to Audio Separation,"Recently, the principal component pursuit has received increasing attention
in signal processing research ranging from source separation to video
surveillance. So far, all existing formulations are real-valued and lack the
concept of phase, which is inherent in inputs such as complex spectrograms or
color images. Thus, in this letter, we extend principal component pursuit to
the complex and quaternionic cases to account for the missing phase
information. Specifically, we present both complex and quaternionic proximity
operators for the $\ell_1$- and trace-norm regularizers. These operators can be
used in conjunction with proximal minimization methods such as the inexact
augmented Lagrange multiplier algorithm. The new algorithms are then applied to
the singing voice separation problem, which aims to separate the singing voice
from the instrumental accompaniment. Results on the iKala and MSD100 datasets
confirmed the usefulness of phase information in principal component pursuit.","['Tak-Shing T. Chan', 'Yi-Hsuan Yang']","['eess.SP', 'cs.MM', 'cs.SD', 'eess.AS', 'stat.ML']",2018-01-09 00:39:50+00:00
http://arxiv.org/abs/1801.02642v3,Boundary Optimizing Network (BON),"Despite all the success that deep neural networks have seen in classifying
certain datasets, the challenge of finding optimal solutions that generalize
still remains. In this paper, we propose the Boundary Optimizing Network (BON),
a new approach to generalization for deep neural networks when used for
supervised learning. Given a classification network, we propose to use a
collaborative generative network that produces new synthetic data points in the
form of perturbations of original data points. In this way, we create a data
support around each original data point which prevents decision boundaries from
passing too close to the original data points, i.e. prevents overfitting. We
show that BON improves convergence on CIFAR-10 using the state-of-the-art
Densenet. We do however observe that the generative network suffers from
catastrophic forgetting during training, and we therefore propose to use a
variation of Memory Aware Synapses to optimize the generative network (called
BON++). On the Iris dataset, we visualize the effect of BON++ when the
generator does not suffer from catastrophic forgetting and conclude that the
approach has the potential to create better boundaries in a higher dimensional
space.","['Marco Singh', 'Akshay Pai']","['cs.LG', 'cs.CV', 'stat.ML']",2018-01-08 19:02:44+00:00
http://arxiv.org/abs/1801.02612v2,Spatially Transformed Adversarial Examples,"Recent studies show that widely used deep neural networks (DNNs) are
vulnerable to carefully crafted adversarial examples. Many advanced algorithms
have been proposed to generate adversarial examples by leveraging the
$\mathcal{L}_p$ distance for penalizing perturbations. Researchers have
explored different defense methods to defend against such adversarial attacks.
While the effectiveness of $\mathcal{L}_p$ distance as a metric of perceptual
quality remains an active research area, in this paper we will instead focus on
a different type of perturbation, namely spatial transformation, as opposed to
manipulating the pixel values directly as in prior works. Perturbations
generated through spatial transformation could result in large $\mathcal{L}_p$
distance measures, but our extensive experiments show that such spatially
transformed adversarial examples are perceptually realistic and more difficult
to defend against with existing defense systems. This potentially provides a
new direction in adversarial example generation and the design of corresponding
defenses. We visualize the spatial transformation based perturbation for
different examples and show that our technique can produce realistic
adversarial examples with smooth image deformation. Finally, we visualize the
attention of deep networks with different types of adversarial examples to
better understand how these examples are interpreted.","['Chaowei Xiao', 'Jun-Yan Zhu', 'Bo Li', 'Warren He', 'Mingyan Liu', 'Dawn Song']","['cs.CR', 'cs.CV', 'stat.ML']",2018-01-08 18:51:59+00:00
http://arxiv.org/abs/1801.02610v5,Generating Adversarial Examples with Adversarial Networks,"Deep neural networks (DNNs) have been found to be vulnerable to adversarial
examples resulting from adding small-magnitude perturbations to inputs. Such
adversarial examples can mislead DNNs to produce adversary-selected results.
Different attack strategies have been proposed to generate adversarial
examples, but how to produce them with high perceptual quality and more
efficiently requires more research efforts. In this paper, we propose AdvGAN to
generate adversarial examples with generative adversarial networks (GANs),
which can learn and approximate the distribution of original instances. For
AdvGAN, once the generator is trained, it can generate adversarial
perturbations efficiently for any instance, so as to potentially accelerate
adversarial training as defenses. We apply AdvGAN in both semi-whitebox and
black-box attack settings. In semi-whitebox attacks, there is no need to access
the original target model after the generator is trained, in contrast to
traditional white-box attacks. In black-box attacks, we dynamically train a
distilled model for the black-box model and optimize the generator accordingly.
Adversarial examples generated by AdvGAN on different target models have high
attack success rate under state-of-the-art defenses compared to other attacks.
Our attack has placed the first with 92.76% accuracy on a public MNIST
black-box attack challenge.","['Chaowei Xiao', 'Bo Li', 'Jun-Yan Zhu', 'Warren He', 'Mingyan Liu', 'Dawn Song']","['cs.CR', 'cs.CV', 'stat.ML']",2018-01-08 18:50:13+00:00
http://arxiv.org/abs/1801.02937v1,Online Cluster Validity Indices for Streaming Data,"Cluster analysis is used to explore structure in unlabeled data sets in a
wide range of applications. An important part of cluster analysis is validating
the quality of computationally obtained clusters. A large number of different
internal indices have been developed for validation in the offline setting.
However, this concept has not been extended to the online setting. A key
challenge is to find an efficient incremental formulation of an index that can
capture both cohesion and separation of the clusters over potentially infinite
data streams. In this paper, we develop two online versions (with and without
forgetting factors) of the Xie-Beni and Davies-Bouldin internal validity
indices, and analyze their characteristics, using two streaming clustering
algorithms (sk-means and online ellipsoidal clustering), and illustrate their
use in monitoring evolving clusters in streaming data. We also show that
incremental cluster validity indices are capable of sending a distress signal
to online monitors when evolving clusters go awry. Our numerical examples
indicate that the incremental Xie-Beni index with forgetting factor is superior
to the other three indices tested.","['Masud Moshtaghi', 'James C. Bezdek', 'Sarah M. Erfani', 'Christopher Leckie', 'James Bailey']","['stat.ML', 'cs.LG']",2018-01-08 18:43:00+00:00
http://arxiv.org/abs/1801.02567v2,Weighted Contrastive Divergence,"Learning algorithms for energy based Boltzmann architectures that rely on
gradient descent are in general computationally prohibitive, typically due to
the exponential number of terms involved in computing the partition function.
In this way one has to resort to approximation schemes for the evaluation of
the gradient. This is the case of Restricted Boltzmann Machines (RBM) and its
learning algorithm Contrastive Divergence (CD). It is well-known that CD has a
number of shortcomings, and its approximation to the gradient has several
drawbacks. Overcoming these defects has been the basis of much research and new
algorithms have been devised, such as persistent CD. In this manuscript we
propose a new algorithm that we call Weighted CD (WCD), built from small
modifications of the negative phase in standard CD. However small these
modifications may be, experimental work reported in this paper suggest that WCD
provides a significant improvement over standard CD and persistent CD at a
small additional computational cost.","['Enrique Romero Merino', 'Ferran Mazzanti Castrillejo', 'Jordi Delgado Pin', 'David Buchaca Prats']","['cs.LG', 'cs.NE', 'stat.ML']",2018-01-08 17:20:17+00:00
http://arxiv.org/abs/1801.02982v3,How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD,"Stochastic gradient descent (SGD) gives an optimal convergence rate when
minimizing convex stochastic objectives $f(x)$. However, in terms of making the
gradients small, the original SGD does not give an optimal rate, even when
$f(x)$ is convex.
  If $f(x)$ is convex, to find a point with gradient norm $\varepsilon$, we
design an algorithm SGD3 with a near-optimal rate
$\tilde{O}(\varepsilon^{-2})$, improving the best known rate
$O(\varepsilon^{-8/3})$ of [18].
  If $f(x)$ is nonconvex, to find its $\varepsilon$-approximate local minimum,
we design an algorithm SGD5 with rate $\tilde{O}(\varepsilon^{-3.5})$, where
previously SGD variants only achieve $\tilde{O}(\varepsilon^{-4})$ [6, 15, 33].
This is no slower than the best known stochastic version of Newton's method in
all parameter regimes [30].",['Zeyuan Allen-Zhu'],"['cs.LG', 'cs.DS', 'math.OC', 'stat.ML']",2018-01-08 10:26:50+00:00
http://arxiv.org/abs/1801.02328v2,Deep Nearest Class Mean Model for Incremental Odor Classification,"In recent years, more machine learning algorithms have been applied to odor
classification. These odor classification algorithms usually assume that the
training datasets are static. However, for some odor recognition tasks, new
odor classes continually emerge. That is, the odor datasets are dynamically
growing while both training samples and number of classes are increasing over
time. Motivated by this concern, this paper proposes a Deep Nearest Class Mean
(DNCM) model based on the deep learning framework and nearest class mean
method. The proposed model not only leverages deep neural network to extract
deep features, but is also able to dynamically integrate new classes over time.
In our experiments, the DNCM model was initially trained with 10 classes, then
25 new classes are integrated. Experiment results demonstrate that the proposed
model is very efficient for incremental odor classification, especially for new
classes with only a small number of training examples.","['Yu Cheng', 'Angus Wong', 'Kevin Hung', 'Zhizhong Li', 'Weitong Li', 'Jun Zhang']","['cs.LG', 'stat.ML']",2018-01-08 07:46:31+00:00
http://arxiv.org/abs/1801.02309v4,Log-concave sampling: Metropolis-Hastings algorithms are fast,"We consider the problem of sampling from a strongly log-concave density in
$\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of
the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by
simulating a Markov chain obtained from the discretization of an appropriate
Langevin diffusion, combined with an accept-reject step. Relative to known
guarantees for the unadjusted Langevin algorithm (ULA), our bounds show that
the use of an accept-reject step in MALA leads to an exponentially improved
dependence on the error-tolerance. Concretely, in order to obtain samples with
TV error at most $\delta$ for a density with condition number $\kappa$, we show
that MALA requires $\mathcal{O} \big(\kappa d \log(1/\delta) \big)$ steps, as
compared to the $\mathcal{O} \big(\kappa^2 d/\delta^2 \big)$ steps established
in past work on ULA. We also demonstrate the gains of MALA over ULA for weakly
log-concave densities. Furthermore, we derive mixing time bounds for the
Metropolized random walk (MRW) and obtain $\mathcal{O}(\kappa)$ mixing time
slower than MALA. We provide numerical examples that support our theoretical
findings, and demonstrate the benefits of Metropolis-Hastings adjustment for
Langevin-type sampling algorithms.","['Raaz Dwivedi', 'Yuansi Chen', 'Martin J. Wainwright', 'Bin Yu']","['stat.ML', 'stat.CO']",2018-01-08 05:39:14+00:00
http://arxiv.org/abs/1801.02294v5,Learning Tree-based Deep Model for Recommender Systems,"Model-based methods for recommender systems have been studied extensively in
recent years. In systems with large corpus, however, the calculation cost for
the learnt model to predict all user-item preferences is tremendous, which
makes full corpus retrieval extremely difficult. To overcome the calculation
barriers, models such as matrix factorization resort to inner product form
(i.e., model user-item preference as the inner product of user, item latent
factors) and indexes to facilitate efficient approximate k-nearest neighbor
searches. However, it still remains challenging to incorporate more expressive
interaction forms between user and item features, e.g., interactions through
deep neural networks, because of the calculation cost.
  In this paper, we focus on the problem of introducing arbitrary advanced
models to recommender systems with large corpus. We propose a novel tree-based
method which can provide logarithmic complexity w.r.t. corpus size even with
more expressive models such as deep neural networks. Our main idea is to
predict user interests from coarse to fine by traversing tree nodes in a
top-down fashion and making decisions for each user-node pair. We also show
that the tree structure can be jointly learnt towards better compatibility with
users' interest distribution and hence facilitate both training and prediction.
Experimental evaluations with two large-scale real-world datasets show that the
proposed method significantly outperforms traditional methods. Online A/B test
results in Taobao display advertising platform also demonstrate the
effectiveness of the proposed method in production environments.","['Han Zhu', 'Xiang Li', 'Pengye Zhang', 'Guozheng Li', 'Jie He', 'Han Li', 'Kun Gai']","['stat.ML', 'cs.IR', 'cs.LG']",2018-01-08 02:52:20+00:00
http://arxiv.org/abs/1801.02261v1,Anatomical Data Augmentation For CNN based Pixel-wise Classification,"In this work we propose a method for anatomical data augmentation that is
based on using slices of computed tomography (CT) examinations that are
adjacent to labeled slices as another resource of labeled data for training the
network. The extended labeled data is used to train a U-net network for a
pixel-wise classification into different hepatic lesions and normal liver
tissues. Our dataset contains CT examinations from 140 patients with 333 CT
images annotated by an expert radiologist. We tested our approach and compared
it to the conventional training process. Results indicate superiority of our
method. Using the anatomical data augmentation we achieved an improvement of 3%
in the success rate, 5% in the classification accuracy, and 4% in Dice.","['Avi Ben-Cohen', 'Eyal Klang', 'Michal Marianne Amitai', 'Jacob Goldberger', 'Hayit Greenspan']","['cs.CV', 'cs.LG', 'stat.ML']",2018-01-07 23:00:02+00:00
http://arxiv.org/abs/1801.02257v1,Denoising Dictionary Learning Against Adversarial Perturbations,"We propose denoising dictionary learning (DDL), a simple yet effective
technique as a protection measure against adversarial perturbations. We
examined denoising dictionary learning on MNIST and CIFAR10 perturbed under two
different perturbation techniques, fast gradient sign (FGSM) and jacobian
saliency maps (JSMA). We evaluated it against five different deep neural
networks (DNN) representing the building blocks of most recent architectures
indicating a successive progression of model complexity of each other. We show
that each model tends to capture different representations based on their
architecture. For each model we recorded its accuracy both on the perturbed
test data previously misclassified with high confidence and on the denoised one
after the reconstruction using dictionary learning. The reconstruction quality
of each data point is assessed by means of PSNR (Peak Signal to Noise Ratio)
and Structure Similarity Index (SSI). We show that after applying (DDL) the
reconstruction of the original data point from a noisy","['John Mitro', 'Derek Bridge', 'Steven Prestwich']","['stat.ML', 'cs.LG']",2018-01-07 22:03:20+00:00
http://arxiv.org/abs/1801.02251v2,Graph Autoencoder-Based Unsupervised Feature Selection with Broad and Local Data Structure Preservation,"Feature selection is a dimensionality reduction technique that selects a
subset of representative features from high dimensional data by eliminating
irrelevant and redundant features. Recently, feature selection combined with
sparse learning has attracted significant attention due to its outstanding
performance compared with traditional feature selection methods that ignores
correlation between features. These works first map data onto a low-dimensional
subspace and then select features by posing a sparsity constraint on the
transformation matrix. However, they are restricted by design to linear data
transformation, a potential drawback given that the underlying correlation
structures of data are often non-linear. To leverage a more sophisticated
embedding, we propose an autoencoder-based unsupervised feature selection
approach that leverages a single-layer autoencoder for a joint framework of
feature selection and manifold learning. More specifically, we enforce column
sparsity on the weight matrix connecting the input layer and the hidden layer,
as in previous work. Additionally, we include spectral graph analysis on the
projected data into the learning process to achieve local data geometry
preservation from the original data space to the low-dimensional feature space.
Extensive experiments are conducted on image, audio, text, and biological data.
The promising experimental results validate the superiority of the proposed
method.","['Siwei Feng', 'Marco F. Duarte']","['cs.CV', 'cs.LG', 'stat.ML']",2018-01-07 21:14:01+00:00
http://arxiv.org/abs/1801.02227v2,Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models,"We propose a new technique that boosts the convergence of training generative
adversarial networks. Generally, the rate of training deep models reduces
severely after multiple iterations. A key reason for this phenomenon is that a
deep network is expressed using a highly non-convex finite-dimensional model,
and thus the parameter gets stuck in a local optimum. Because of this, methods
often suffer not only from degeneration of the convergence speed but also from
limitations in the representational power of the trained network. To overcome
this issue, we propose an additional layer called the gradient layer to seek a
descent direction in an infinite-dimensional space. Because the layer is
constructed in the infinite-dimensional space, we are not restricted by the
specific model structure of finite-dimensional models. As a result, we can get
out of the local optima in finite-dimensional models and move towards the
global optimal function more directly. In this paper, this phenomenon is
explained from the functional gradient method perspective of the gradient
layer. Interestingly, the optimization procedure using the gradient layer
naturally constructs the deep structure of the network. Moreover, we
demonstrate that this procedure can be regarded as a discretization method of
the gradient flow that naturally reduces the objective function. Finally, the
method is tested using several numerical experiments, which show its fast
convergence.","['Atsushi Nitanda', 'Taiji Suzuki']","['stat.ML', 'cs.LG']",2018-01-07 18:44:10+00:00
http://arxiv.org/abs/1801.02171v1,Detection and segmentation of the Left Ventricle in Cardiac MRI using Deep Learning,"Manual segmentation of the Left Ventricle (LV) is a tedious and meticulous
task that can vary depending on the patient, the Magnetic Resonance Images
(MRI) cuts and the experts. Still today, we consider manual delineation done by
experts as being the ground truth for cardiac diagnosticians. Thus, we are
reviewing the paper - written by Avendi and al. - who presents a combined
approach with Convolutional Neural Networks, Stacked Auto-Encoders and
Deformable Models, to try and automate the segmentation while performing more
accurately. Furthermore, we have implemented parts of the paper (around three
quarts) and experimented both the original method and slightly modified
versions when changing the architecture and the parameters.","['Alexandre Attia', 'Sharone Dayan']","['cs.CV', 'stat.ML']",2018-01-07 11:22:12+00:00
http://arxiv.org/abs/1801.02149v1,Applying an Ensemble Learning Method for Improving Multi-label Classification Performance,"In recent years, multi-label classification problem has become a
controversial issue. In this kind of classification, each sample is associated
with a set of class labels. Ensemble approaches are supervised learning
algorithms in which an operator takes a number of learning algorithms, namely
base-level algorithms and combines their outcomes to make an estimation. The
simplest form of ensemble learning is to train the base-level algorithms on
random subsets of data and then let them vote for the most popular
classifications or average the predictions of the base-level algorithms. In
this study, an ensemble learning method is proposed for improving multi-label
classification evaluation criteria. We have compared our method with well-known
base-level algorithms on some data sets. Experiment results show the proposed
approach outperforms the base well-known classifiers for the multi-label
classification problem.","['Amirreza Mahdavi-Shahri', 'Mahboobeh Houshmand', 'Mahdi Yaghoobi', 'Mehrdad Jalali']","['cs.LG', 'stat.ML']",2018-01-07 06:43:46+00:00
http://arxiv.org/abs/1801.02125v2,Threshold Auto-Tuning Metric Learning,"It has been reported repeatedly that discriminative learning of distance
metric boosts the pattern recognition performance. A weak point of ITML-based
methods is that the distance threshold for similarity/dissimilarity constraints
must be determined manually and it is sensitive to generalization performance,
although the ITML-based methods enjoy an advantage that the Bregman projection
framework can be applied for optimization of distance metric. In this paper, we
present a new formulation of metric learning algorithm in which the distance
threshold is optimized together. Since the optimization is still in the Bregman
projection framework, the Dykstra algorithm can be applied for optimization. A
nonlinear equation has to be solved to project the solution onto a half-space
in each iteration. Na\""{i}ve method takes $O(LMn^{3})$ computational time to
solve the nonlinear equation. In this study, an efficient technique that can
solve the nonlinear equation in $O(Mn^{3})$ has been discovered. We have proved
that the root exists and is unique. We empirically show that the accuracy of
pattern recognition for the proposed metric learning algorithm is comparable to
the existing metric learning methods, yet the distance threshold is
automatically tuned for the proposed metric learning algorithm.","['Yuya Onuma', 'Rachelle Rivero', 'Tsuyoshi Kato']","['cs.LG', 'stat.ML']",2018-01-07 04:01:24+00:00
http://arxiv.org/abs/1801.02124v2,Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations,"This paper considers the problem of inverse reinforcement learning in
zero-sum stochastic games when expert demonstrations are known to be not
optimal. Compared to previous works that decouple agents in the game by
assuming optimality in expert strategies, we introduce a new objective function
that directly pits experts against Nash Equilibrium strategies, and we design
an algorithm to solve for the reward function in the context of inverse
reinforcement learning with deep neural networks as model approximations. In
our setting the model and algorithm do not decouple by agent. In order to find
Nash Equilibrium in large-scale games, we also propose an adversarial training
algorithm for zero-sum stochastic games, and show the theoretical appeal of
non-existence of local optima in its objective function. In our numerical
experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement
learning algorithms address games that are not amenable to previous approaches
using tabular representations. Moreover, with sub-optimal expert demonstrations
our algorithms recover both reward functions and strategies with good quality.","['Xingyu Wang', 'Diego Klabjan']","['stat.ML', 'cs.LG']",2018-01-07 03:53:30+00:00
http://arxiv.org/abs/1801.02961v2,Representation Learning with Autoencoders for Electronic Health Records: A Comparative Study,"Increasing volume of Electronic Health Records (EHR) in recent years provides
great opportunities for data scientists to collaborate on different aspects of
healthcare research by applying advanced analytics to these EHR clinical data.
A key requirement however is obtaining meaningful insights from high
dimensional, sparse and complex clinical data. Data science approaches
typically address this challenge by performing feature learning in order to
build more reliable and informative feature representations from clinical data
followed by supervised learning. In this paper, we propose a predictive
modeling approach based on deep learning based feature representations and word
embedding techniques. Our method uses different deep architectures (stacked
sparse autoencoders, deep belief network, adversarial autoencoders and
variational autoencoders) for feature representation in higher-level
abstraction to obtain effective and robust features from EHRs, and then build
prediction models on top of them. Our approach is particularly useful when the
unlabeled data is abundant whereas labeled data is scarce. We investigate the
performance of representation learning through a supervised learning approach.
Our focus is to present a comparative study to evaluate the performance of
different deep architectures through supervised learning and provide insights
in the choice of deep feature representation techniques. Our experiments
demonstrate that for small data sets, stacked sparse autoencoder demonstrates a
superior generality performance in prediction due to sparsity regularization
whereas variational autoencoders outperform the competing approaches for large
data sets due to its capability of learning the representation distribution.","['Najibesadat Sadati', 'Milad Zafar Nezhad', 'Ratna Babu Chinnam', 'Dongxiao Zhu']","['cs.LG', 'stat.ML']",2018-01-06 23:17:24+00:00
http://arxiv.org/abs/1801.02013v3,Multiscale Sparse Microcanonical Models,"We study approximations of non-Gaussian stationary processes having long
range correlations with microcanonical models. These models are conditioned by
the empirical value of an energy vector, evaluated on a single realization.
Asymptotic properties of maximum entropy microcanonical and macrocanonical
processes and their convergence to Gibbs measures are reviewed. We show that
the Jacobian of the energy vector controls the entropy rate of microcanonical
processes.
  Sampling maximum entropy processes through MCMC algorithms require too many
operations when the number of constraints is large. We define microcanonical
gradient descent processes by transporting a maximum entropy measure with a
gradient descent algorithm which enforces the energy conditions. Convergence
and symmetries are analyzed. Approximations of non-Gaussian processes with long
range interactions are defined with multiscale energy vectors computed with
wavelet and scattering transforms. Sparsity properties are captured with $\bf
l^1$ norms. Approximations of Gaussian, Ising and point processes are studied,
as well as image and audio texture synthesis.","['Joan Bruna', 'Stephane Mallat']","['math-ph', 'math.MP', 'stat.ML']",2018-01-06 13:18:46+00:00
http://arxiv.org/abs/1801.01973v2,A Note on the Inception Score,"Deep generative models are powerful tools that have produced impressive
results in recent years. These advances have been for the most part empirically
driven, making it essential that we use high quality evaluation metrics. In
this paper, we provide new insights into the Inception Score, a recently
proposed and widely used evaluation metric for generative models, and
demonstrate that it fails to provide useful guidance when comparing models. We
discuss both suboptimalities of the metric itself and issues with its
application. Finally, we call for researchers to be more systematic and careful
when evaluating and comparing generative models, as the advancement of the
field depends upon it.","['Shane Barratt', 'Rishi Sharma']","['stat.ML', 'cs.LG']",2018-01-06 05:44:29+00:00
http://arxiv.org/abs/1801.01961v2,Compressive sensing adaptation for polynomial chaos expansions,"Basis adaptation in Homogeneous Chaos spaces rely on a suitable rotation of
the underlying Gaussian germ. Several rotations have been proposed in the
literature resulting in adaptations with different convergence properties. In
this paper we present a new adaptation mechanism that builds on compressive
sensing algorithms, resulting in a reduced polynomial chaos approximation with
optimal sparsity. The developed adaptation algorithm consists of a two-step
optimization procedure that computes the optimal coefficients and the input
projection matrix of a low dimensional chaos expansion with respect to an
optimally rotated basis. We demonstrate the attractive features of our
algorithm through several numerical examples including the application on
Large-Eddy Simulation (LES) calculations of turbulent combustion in a HIFiRE
scramjet engine.","['Panagiotis Tsilifis', 'Xun Huan', 'Cosmin Safta', 'Khachik Sargsyan', 'Guilhem Lacaze', 'Joseph C. Oefelein', 'Habib N. Najm', 'Roger G. Ghanem']",['stat.ML'],2018-01-06 03:52:52+00:00
http://arxiv.org/abs/1801.01953v1,Adversarial Perturbation Intensity Achieving Chosen Intra-Technique Transferability Level for Logistic Regression,"Machine Learning models have been shown to be vulnerable to adversarial
examples, ie. the manipulation of data by a attacker to defeat a defender's
classifier at test time. We present a novel probabilistic definition of
adversarial examples in perfect or limited knowledge setting using prior
probability distributions on the defender's classifier. Using the asymptotic
properties of the logistic regression, we derive a closed-form expression of
the intensity of any adversarial perturbation, in order to achieve a given
expected misclassification rate. This technique is relevant in a threat model
of known model specifications and unknown training data. To our knowledge, this
is the first method that allows an attacker to directly choose the probability
of attack success. We evaluate our approach on two real-world datasets.",['Martin Gubri'],"['stat.ML', 'cs.LG']",2018-01-06 01:37:30+00:00
http://arxiv.org/abs/1801.01952v4,Generating Neural Networks with Neural Networks,"Hypernetworks are neural networks that generate weights for another neural
network. We formulate the hypernetwork training objective as a compromise
between accuracy and diversity, where the diversity takes into account trivial
symmetry transformations of the target network. We explain how this simple
formulation generalizes variational inference. We use multi-layered perceptrons
to form the mapping from the low dimensional input random vector to the high
dimensional weight space, and demonstrate how to reduce the number of
parameters in this mapping by parameter sharing. We perform experiments and
show that the generated weights are diverse and lie on a non-trivial manifold.",['Lior Deutsch'],"['stat.ML', 'cs.LG']",2018-01-06 01:27:16+00:00
http://arxiv.org/abs/1801.01899v2,Clustering with Outlier Removal,"Cluster analysis and outlier detection are strongly coupled tasks in data
mining area. Cluster structure can be easily destroyed by few outliers; on the
contrary, outliers are defined by the concept of cluster, which are recognized
as the points belonging to none of the clusters. Unfortunately, most existing
studies do not notice the coupled relationship between these two task and
handle them separately. In light of this, we consider the joint cluster
analysis and outlier detection problem, and propose the Clustering with Outlier
Removal (COR) algorithm. Generally speaking, the original space is transformed
into the binary space via generating basic partitions in order to define
clusters. Then an objective function based Holoentropy is designed to enhance
the compactness of each cluster with a few outliers removed. With further
analyses on the objective function, only partial of the problem can be handled
by K-means optimization. To provide an integrated solution, an auxiliary binary
matrix is nontrivally introduced so that COR completely and efficiently solves
the challenging problem via a unified K-means-- with theoretical supports.
Extensive experimental results on numerous data sets in various domains
demonstrate the effectiveness and efficiency of COR significantly over
state-of-the-art methods in terms of cluster validity and outlier detection.
Some key factors in COR are further analyzed for practical use. Finally, an
application on flight trajectory is provided to demonstrate the effectiveness
of COR in the real-world scenario.","['Hongfu Liu', 'Jun Li', 'Yue Wu', 'Yun Fu']","['cs.LG', 'stat.ML']",2018-01-05 19:15:17+00:00
http://arxiv.org/abs/1801.01799v2,Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization,"We present novel understandings of the Gamma-Poisson (GaP) model, a
probabilistic matrix factorization model for count data. We show that GaP can
be rewritten free of the score/activation matrix. This gives us new insights
about the estimation of the topic/dictionary matrix by maximum marginal
likelihood estimation. In particular, this explains the robustness of this
estimator to over-specified values of the factorization rank, especially its
ability to automatically prune irrelevant dictionary columns, as empirically
observed in previous work. The marginalization of the activation matrix leads
in turn to a new Monte Carlo Expectation-Maximization algorithm with favorable
properties.","['Louis Filstroff', 'Alberto Lumbreras', 'Cédric Févotte']","['stat.ML', 'cs.LG']",2018-01-05 15:50:39+00:00
http://arxiv.org/abs/1801.01750v1,Nonparametric Stochastic Contextual Bandits,"We analyze the $K$-armed bandit problem where the reward for each arm is a
noisy realization based on an observed context under mild nonparametric
assumptions. We attain tight results for top-arm identification and a sublinear
regret of $\widetilde{O}\Big(T^{\frac{1+D}{2+D}}\Big)$, where $D$ is the
context dimension, for a modified UCB algorithm that is simple to implement
($k$NN-UCB). We then give global intrinsic dimension dependent and ambient
dimension independent regret bounds. We also discuss recovering topological
structures within the context space based on expected bandit performance and
provide an extension to infinite-armed contextual bandits. Finally, we
experimentally show the improvement of our algorithm over existing multi-armed
bandit approaches for both simulated tasks and MNIST image classification.","['Melody Y. Guan', 'Heinrich Jiang']","['cs.LG', 'stat.ML']",2018-01-05 13:27:42+00:00
http://arxiv.org/abs/1801.01743v1,A relativistic extension of Hopfield neural networks via the mechanical analogy,"We propose a modification of the cost function of the Hopfield model whose
salient features shine in its Taylor expansion and result in more than pairwise
interactions with alternate signs, suggesting a unified framework for handling
both with deep learning and network pruning. In our analysis, we heavily rely
on the Hamilton-Jacobi correspondence relating the statistical model with a
mechanical system. In this picture, our model is nothing but the relativistic
extension of the original Hopfield model (whose cost function is a quadratic
form in the Mattis magnetization which mimics the non-relativistic Hamiltonian
for a free particle). We focus on the low-storage regime and solve the model
analytically by taking advantage of the mechanical analogy, thus obtaining a
complete characterization of the free energy and the associated
self-consistency equations in the thermodynamic limit. On the numerical side,
we test the performances of our proposal with MC simulations, showing that the
stability of spurious states (limiting the capabilities of the standard Hebbian
construction) is sensibly reduced due to presence of unlearning contributions
in this extended framework.","['Adriano Barra', 'Matteo Beccaria', 'Alberto Fachechi']","['cond-mat.dis-nn', 'cs.NE', 'stat.ML']",2018-01-05 12:57:18+00:00
http://arxiv.org/abs/1801.01708v1,Negative Binomial Matrix Factorization for Recommender Systems,"We introduce negative binomial matrix factorization (NBMF), a matrix
factorization technique specially designed for analyzing over-dispersed count
data. It can be viewed as an extension of Poisson matrix factorization (PF)
perturbed by a multiplicative term which models exposure. This term brings a
degree of freedom for controlling the dispersion, making NBMF more robust to
outliers. We show that NBMF allows to skip traditional pre-processing stages,
such as binarization, which lead to loss of information. Two estimation
approaches are presented: maximum likelihood and variational Bayes inference.
We test our model with a recommendation task and show its ability to predict
user tastes with better precision than PF.","['Olivier Gouvert', 'Thomas Oberlin', 'Cédric Févotte']","['cs.LG', 'cs.IR', 'stat.ML']",2018-01-05 11:00:46+00:00
http://arxiv.org/abs/1801.01649v2,Gauged Mini-Bucket Elimination for Approximate Inference,"Computing the partition function $Z$ of a discrete graphical model is a
fundamental inference challenge. Since this is computationally intractable,
variational approximations are often used in practice. Recently, so-called
gauge transformations were used to improve variational lower bounds on $Z$. In
this paper, we propose a new gauge-variational approach, termed WMBE-G, which
combines gauge transformations with the weighted mini-bucket elimination (WMBE)
method. WMBE-G can provide both upper and lower bounds on $Z$, and is easier to
optimize than the prior gauge-variational algorithm. We show that WMBE-G
strictly improves the earlier WMBE approximation for symmetric models including
Ising models with no magnetic field. Our experimental results demonstrate the
effectiveness of WMBE-G even for generic, nonsymmetric models.","['Sungsoo Ahn', 'Michael Chertkov', 'Jinwoo Shin', 'Adrian Weller']",['stat.ML'],2018-01-05 07:26:39+00:00
http://arxiv.org/abs/1801.01587v6,SpectralNet: Spectral Clustering using Deep Neural Networks,"Spectral clustering is a leading and popular technique in unsupervised data
analysis. Two of its major limitations are scalability and generalization of
the spectral embedding (i.e., out-of-sample-extension). In this paper we
introduce a deep learning approach to spectral clustering that overcomes the
above shortcomings. Our network, which we call SpectralNet, learns a map that
embeds input data points into the eigenspace of their associated graph
Laplacian matrix and subsequently clusters them. We train SpectralNet using a
procedure that involves constrained stochastic optimization. Stochastic
optimization allows it to scale to large datasets, while the constraints, which
are implemented using a special-purpose output layer, allow us to keep the
network output orthogonal. Moreover, the map learned by SpectralNet naturally
generalizes the spectral embedding to unseen data points. To further improve
the quality of the clustering, we replace the standard pairwise Gaussian
affinities with affinities leaned from unlabeled data using a Siamese network.
Additional improvement can be achieved by applying the network to code
representations produced, e.g., by standard autoencoders. Our end-to-end
learning procedure is fully unsupervised. In addition, we apply VC dimension
theory to derive a lower bound on the size of SpectralNet. State-of-the-art
clustering results are reported on the Reuters dataset. Our implementation is
publicly available at https://github.com/kstant0725/SpectralNet .","['Uri Shaham', 'Kelly Stanton', 'Henry Li', 'Boaz Nadler', 'Ronen Basri', 'Yuval Kluger']","['stat.ML', 'cs.LG']",2018-01-04 23:56:36+00:00
http://arxiv.org/abs/1801.01469v1,PHOENICS: A universal deep Bayesian optimizer,"In this work we introduce PHOENICS, a probabilistic global optimization
algorithm combining ideas from Bayesian optimization with concepts from
Bayesian kernel density estimation. We propose an inexpensive acquisition
function balancing the explorative and exploitative behavior of the algorithm.
This acquisition function enables intuitive sampling strategies for an
efficient parallel search of global minima. The performance of PHOENICS is
assessed via an exhaustive benchmark study on a set of 15 discrete,
quasi-discrete and continuous multidimensional functions. Unlike optimization
methods based on Gaussian processes (GP) and random forests (RF), we show that
PHOENICS is less sensitive to the nature of the co-domain, and outperforms GP
and RF optimizations. We illustrate the performance of PHOENICS on the
Oregonator, a difficult case-study describing a complex chemical reaction
network. We demonstrate that only PHOENICS was able to reproduce qualitatively
and quantitatively the target dynamic behavior of this nonlinear reaction
dynamics. We recommend PHOENICS for rapid optimization of scalar, possibly
non-convex, black-box unknown objective functions.","['Florian Häse', 'Loïc M. Roch', 'Christoph Kreisbeck', 'Alán Aspuru-Guzik']","['stat.ML', 'physics.chem-ph']",2018-01-04 17:40:31+00:00
http://arxiv.org/abs/1801.01467v1,Deep Reinforcement Learning based Optimal Control of Hot Water Systems,"Energy consumption for hot water production is a major draw in high
efficiency buildings. Optimizing this has typically been approached from a
thermodynamics perspective, decoupled from occupant influence. Furthermore,
optimization usually presupposes existence of a detailed dynamics model for the
hot water system. These assumptions lead to suboptimal energy efficiency in the
real world. In this paper, we present a novel reinforcement learning based
methodology which optimizes hot water production. The proposed methodology is
completely generalizable, and does not require an offline step or human domain
knowledge to build a model for the hot water vessel or the heating element.
Occupant preferences too are learnt on the fly. The proposed system is applied
to a set of 32 houses in the Netherlands where it reduces energy consumption
for hot water production by roughly 20% with no loss of occupant comfort.
Extrapolating, this translates to absolute savings of roughly 200 kWh for a
single household on an annual basis. This performance can be replicated to any
domestic hot water system and optimization objective, given that the fairly
minimal requirements on sensor data are met. With millions of hot water systems
operational worldwide, the proposed framework has the potential to reduce
energy consumption in existing and new systems on a multi Gigawatt-hour scale
in the years to come.","['Hussain Kazmi', 'Fahad Mehmood', 'Stefan Lodeweyckx', 'Johan Driesen']","['cs.SY', 'stat.AP', 'stat.ML']",2018-01-04 17:37:50+00:00
http://arxiv.org/abs/1801.01423v3,Overcoming catastrophic forgetting with hard attention to the task,"Catastrophic forgetting occurs when a neural network loses the information
learned in a previous task after training on subsequent tasks. This problem
remains a hurdle for artificial intelligence systems with sequential learning
capabilities. In this paper, we propose a task-based hard attention mechanism
that preserves previous tasks' information without affecting the current task's
learning. A hard attention mask is learned concurrently to every task, through
stochastic gradient descent, and previous masks are exploited to condition such
learning. We show that the proposed mechanism is effective for reducing
catastrophic forgetting, cutting current rates by 45 to 80%. We also show that
it is robust to different hyperparameter choices, and that it offers a number
of monitoring capabilities. The approach features the possibility to control
both the stability and compactness of the learned knowledge, which we believe
makes it also attractive for online learning or network compression
applications.","['Joan Serrà', 'Dídac Surís', 'Marius Miron', 'Alexandros Karatzoglou']","['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",2018-01-04 16:22:22+00:00
http://arxiv.org/abs/1801.01401v5,Demystifying MMD GANs,"We investigate the training and performance of generative adversarial
networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs.
As our main theoretical contribution, we clarify the situation with bias in GAN
loss functions raised by recent work: we show that gradient estimators used in
the optimization process for both MMD GANs and Wasserstein GANs are unbiased,
but learning a discriminator based on samples leads to biased gradients for the
generator parameters. We also discuss the issue of kernel choice for the MMD
critic, and characterize the kernel corresponding to the energy distance used
for the Cramer GAN critic. Being an integral probability metric, the MMD
benefits from training strategies recently developed for Wasserstein GANs. In
experiments, the MMD GAN is able to employ a smaller critic network than the
Wasserstein GAN, resulting in a simpler and faster-training algorithm with
matching performance. We also propose an improved measure of GAN convergence,
the Kernel Inception Distance, and show how to use it to dynamically adapt
learning rates during GAN training.","['Mikołaj Bińkowski', 'Danica J. Sutherland', 'Michael Arbel', 'Arthur Gretton']","['stat.ML', 'cs.LG']",2018-01-04 15:25:26+00:00
http://arxiv.org/abs/1801.01290v2,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,"Model-free deep reinforcement learning (RL) algorithms have been demonstrated
on a range of challenging decision making and control tasks. However, these
methods typically suffer from two major challenges: very high sample complexity
and brittle convergence properties, which necessitate meticulous hyperparameter
tuning. Both of these challenges severely limit the applicability of such
methods to complex, real-world domains. In this paper, we propose soft
actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum
entropy reinforcement learning framework. In this framework, the actor aims to
maximize expected reward while also maximizing entropy. That is, to succeed at
the task while acting as randomly as possible. Prior deep RL methods based on
this framework have been formulated as Q-learning methods. By combining
off-policy updates with a stable stochastic actor-critic formulation, our
method achieves state-of-the-art performance on a range of continuous control
benchmark tasks, outperforming prior on-policy and off-policy methods.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving very similar performance across
different random seeds.","['Tuomas Haarnoja', 'Aurick Zhou', 'Pieter Abbeel', 'Sergey Levine']","['cs.LG', 'cs.AI', 'stat.ML']",2018-01-04 09:50:50+00:00
